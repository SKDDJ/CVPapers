# Arxiv Papers in cs.CV on 2020-10-02
### LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar Fusion
- **Arxiv ID**: http://arxiv.org/abs/2010.00731v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.00731v3)
- **Published**: 2020-10-02 00:13:00+00:00
- **Updated**: 2020-11-12 22:29:07+00:00
- **Authors**: Meet Shah, Zhiling Huang, Ankit Laddha, Matthew Langford, Blake Barber, Sidney Zhang, Carlos Vallespi-Gonzalez, Raquel Urtasun
- **Comment**: Accepted to Conference on Robot Learning (CoRL) 2020
- **Journal**: None
- **Summary**: In this paper, we present LiRaNet, a novel end-to-end trajectory prediction method which utilizes radar sensor information along with widely used lidar and high definition (HD) maps. Automotive radar provides rich, complementary information, allowing for longer range vehicle detection as well as instantaneous radial velocity measurements. However, there are factors that make the fusion of lidar and radar information challenging, such as the relatively low angular resolution of radar measurements, their sparsity and the lack of exact time synchronization with lidar. To overcome these challenges, we propose an efficient spatio-temporal radar feature extraction scheme which achieves state-of-the-art performance on multiple large-scale datasets.Further, by incorporating radar information, we show a 52% reduction in prediction error for objects with high acceleration and a 16% reduction in prediction error for objects at longer range.



### Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.00735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00735v1)
- **Published**: 2020-10-02 00:43:39+00:00
- **Updated**: 2020-10-02 00:43:39+00:00
- **Authors**: Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu Xu
- **Comment**: COLING 2020
- **Journal**: None
- **Summary**: Unsupervised text style transfer is full of challenges due to the lack of parallel data and difficulties in content preservation. In this paper, we propose a novel neural approach to unsupervised text style transfer, which we refer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from non-parallel data. CAE consists of three essential components: (1) LSTM autoencoders that encode a text in one style into its latent representation and decode an encoded representation into its original text or a transferred representation into a style-transferred text, (2) adversarial style transfer networks that use an adversarially trained generator to transform a latent representation in one style into a representation in another style, and (3) a cycle-consistent constraint that enhances the capacity of the adversarial style transfer networks in content preservation. The entire CAE with these three components can be trained end-to-end. Extensive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation.



### Smart-Inspect: Micro Scale Localization and Classification of Smartphone Glass Defects for Industrial Automation
- **Arxiv ID**: http://arxiv.org/abs/2010.00741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.00741v1)
- **Published**: 2020-10-02 01:15:00+00:00
- **Updated**: 2020-10-02 01:15:00+00:00
- **Authors**: M Usman Maqbool Bhutta, Shoaib Aslam, Peng Yun, Jianhao Jiao, Ming Liu
- **Comment**: IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS 2020)
- **Journal**: None
- **Summary**: The presence of any type of defect on the glass screen of smart devices has a great impact on their quality. We present a robust semi-supervised learning framework for intelligent micro-scaled localization and classification of defects on a 16K pixel image of smartphone glass. Our model features the efficient recognition and labeling of three types of defects: scratches, light leakage due to cracks, and pits. Our method also differentiates between the defects and light reflections due to dust particles and sensor regions, which are classified as non-defect areas. We use a partially labeled dataset to achieve high robustness and excellent classification of defect and non-defect areas as compared to principal components analysis (PCA), multi-resolution and information-fusion-based algorithms. In addition, we incorporated two classifiers at different stages of our inspection framework for labeling and refining the unlabeled defects. We successfully enhanced the inspection depth-limit up to 5 microns. The experimental results show that our method outperforms manual inspection in testing the quality of glass screen samples by identifying defects on samples that have been marked as good by human inspection.



### Cell Complex Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.00743v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV, math.AT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00743v4)
- **Published**: 2020-10-02 01:38:12+00:00
- **Updated**: 2021-03-02 03:50:54+00:00
- **Authors**: Mustafa Hajij, Kyle Istvan, Ghada Zamzmi
- **Comment**: None
- **Journal**: None
- **Summary**: Cell complexes are topological spaces constructed from simple blocks called cells. They generalize graphs, simplicial complexes, and polyhedral complexes that form important domains for practical applications. They also provide a combinatorial formalism that allows the inclusion of complicated relationships of restrictive structures such as graphs and meshes. In this paper, we propose \textbf{Cell Complexes Neural Networks (CXNs)}, a general, combinatorial and unifying construction for performing neural network-type computations on cell complexes. We introduce an inter-cellular message passing scheme on cell complexes that takes the topology of the underlying space into account and generalizes message passing scheme to graphs. Finally, we introduce a unified cell complex encoder-decoder framework that enables learning representation of cells for a given complex inside the Euclidean spaces. In particular, we show how our cell complex autoencoder construction can give, in the special case \textbf{cell2vec}, a generalization for node2vec.



### Contrastive Learning of Medical Visual Representations from Paired Images and Text
- **Arxiv ID**: http://arxiv.org/abs/2010.00747v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00747v2)
- **Published**: 2020-10-02 02:10:18+00:00
- **Updated**: 2022-09-19 20:20:23+00:00
- **Authors**: Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, Curtis P. Langlotz
- **Comment**: First published in 2020. Accepted at Machine Learning for Healthcare
  (MLHC) 2022
- **Journal**: None
- **Summary**: Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose ConVIRT, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test ConVIRT by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.



### Deep Learning for Earth Image Segmentation based on Imperfect Polyline Labels with Annotation Errors
- **Arxiv ID**: http://arxiv.org/abs/2010.00757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00757v1)
- **Published**: 2020-10-02 02:54:06+00:00
- **Updated**: 2020-10-02 02:54:06+00:00
- **Authors**: Zhe Jiang, Marcus Stephen Kirby, Wenchong He, Arpan Man Sainju
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning techniques (e.g., U-Net, DeepLab) have achieved tremendous success in image segmentation. The performance of these models heavily relies on high-quality ground truth segment labels. Unfortunately, in many real-world problems, ground truth segment labels often have geometric annotation errors due to manual annotation mistakes, GPS errors, or visually interpreting background imagery at a coarse resolution. Such location errors will significantly impact the training performance of existing deep learning algorithms. Existing research on label errors either models ground truth errors in label semantics (assuming label locations to be correct) or models label location errors with simple square patch shifting. These methods cannot fully incorporate the geometric properties of label location errors. To fill the gap, this paper proposes a generic learning framework based on the EM algorithm to update deep learning model parameters and infer hidden true label locations simultaneously. Evaluations on a real-world hydrological dataset in the streamline refinement application show that the proposed framework outperforms baseline methods in classification accuracy (reducing the number of false positives by 67% and reducing the number of false negatives by 55%).



### Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2010.00763v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00763v4)
- **Published**: 2020-10-02 03:19:46+00:00
- **Updated**: 2021-01-04 21:50:06+00:00
- **Authors**: Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, Animashree Anandkumar
- **Comment**: 22 pages, NeurIPS 2020
- **Journal**: None
- **Summary**: Humans have an inherent ability to learn novel concepts from only a few samples and generalize these concepts to different situations. Even though today's machine learning models excel with a plethora of training data on standard recognition tasks, a considerable gap exists between machine-level pattern recognition and human-level concept learning. To narrow this gap, the Bongard problems (BPs) were introduced as an inspirational challenge for visual cognition in intelligent systems. Despite new advances in representation learning and learning to learn, BPs remain a daunting challenge for modern AI. Inspired by the original one hundred BPs, we propose a new benchmark Bongard-LOGO for human-level concept learning and reasoning. We develop a program-guided generation technique to produce a large set of human-interpretable visual cognition problems in action-oriented LOGO language. Our benchmark captures three core properties of human cognition: 1) context-dependent perception, in which the same object may have disparate interpretations given different contexts; 2) analogy-making perception, in which some meaningful concepts are traded off for other meaningful concepts; and 3) perception with a few samples but infinite vocabulary. In experiments, we show that the state-of-the-art deep learning methods perform substantially worse than human subjects, implying that they fail to capture core human cognition properties. Finally, we discuss research directions towards a general architecture for visual reasoning to tackle this benchmark.



### A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2010.00793v1
- **DOI**: 10.1016/j.neucom.2020.05.108
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00793v1)
- **Published**: 2020-10-02 05:27:57+00:00
- **Updated**: 2020-10-02 05:27:57+00:00
- **Authors**: Chongyi Li, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, Yao Zhao
- **Comment**: Neurocomputing, 415, pp. 411-420, 2020
- **Journal**: None
- **Summary**: The diverse spatial resolutions, various object types, scales and orientations, and cluttered backgrounds in optical remote sensing images (RSIs) challenge the current salient object detection (SOD) approaches. It is commonly unsatisfactory to directly employ the SOD approaches designed for nature scene images (NSIs) to RSIs. In this paper, we propose a novel Parallel Down-up Fusion network (PDF-Net) for SOD in optical RSIs, which takes full advantage of the in-path low- and high-level features and cross-path multi-resolution features to distinguish diversely scaled salient objects and suppress the cluttered backgrounds. To be specific, keeping a key observation that the salient objects still are salient no matter the resolutions of images are in mind, the PDF-Net takes successive down-sampling to form five parallel paths and perceive scaled salient objects that are commonly existed in optical RSIs. Meanwhile, we adopt the dense connections to take advantage of both low- and high-level information in the same path and build up the relations of cross paths, which explicitly yield strong feature representations. At last, we fuse the multiple-resolution features in parallel paths to combine the benefits of the features with different resolutions, i.e., the high-resolution feature consisting of complete structure and clear details while the low-resolution features highlighting the scaled salient objects. Extensive experiments on the ORSSD dataset demonstrate that the proposed network is superior to the state-of-the-art approaches both qualitatively and quantitatively.



### Online Knowledge Distillation via Multi-branch Diversity Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2010.00795v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00795v3)
- **Published**: 2020-10-02 05:52:12+00:00
- **Updated**: 2020-11-13 14:18:39+00:00
- **Authors**: Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: Knowledge distillation is an effective method to transfer the knowledge from the cumbersome teacher model to the lightweight student model. Online knowledge distillation uses the ensembled prediction results of multiple student models as soft targets to train each student model. However, the homogenization problem will lead to difficulty in further improving model performance. In this work, we propose a new distillation method to enhance the diversity among multiple student models. We introduce Feature Fusion Module (FFM), which improves the performance of the attention mechanism in the network by integrating rich semantic information contained in the last block of multiple student models. Furthermore, we use the Classifier Diversification(CD) loss function to strengthen the differences between the student models and deliver a better ensemble result. Extensive experiments proved that our method significantly enhances the diversity among student models and brings better distillation performance. We evaluate our method on three image classification datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves state-of-the-art performance on these datasets.



### Block-wise Image Transformation with Secret Key for Adversarially Robust Defense
- **Arxiv ID**: http://arxiv.org/abs/2010.00801v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2010.00801v1)
- **Published**: 2020-10-02 06:07:12+00:00
- **Updated**: 2020-10-02 06:07:12+00:00
- **Authors**: MaungMaung AprilPyone, Hitoshi Kiya
- **Comment**: Under review
- **Journal**: None
- **Summary**: In this paper, we propose a novel defensive transformation that enables us to maintain a high classification accuracy under the use of both clean images and adversarial examples for adversarially robust defense. The proposed transformation is a block-wise preprocessing technique with a secret key to input images. We developed three algorithms to realize the proposed transformation: Pixel Shuffling, Bit Flipping, and FFX Encryption. Experiments were carried out on the CIFAR-10 and ImageNet datasets by using both black-box and white-box attacks with various metrics including adaptive ones. The results show that the proposed defense achieves high accuracy close to that of using clean images even under adaptive attacks for the first time. In the best-case scenario, a model trained by using images transformed by FFX Encryption (block size of 4) yielded an accuracy of 92.30% on clean images and 91.48% under PGD attack with a noise distance of 8/255, which is close to the non-robust accuracy (95.45%) for the CIFAR-10 dataset, and it yielded an accuracy of 72.18% on clean images and 71.43% under the same attack, which is also close to the standard accuracy (73.70%) for the ImageNet dataset. Overall, all three proposed algorithms are demonstrated to outperform state-of-the-art defenses including adversarial training whether or not a model is under attack.



### PrognoseNet: A Generative Probabilistic Framework for Multimodal Position Prediction given Context Information
- **Arxiv ID**: http://arxiv.org/abs/2010.00802v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2010.00802v1)
- **Published**: 2020-10-02 06:13:41+00:00
- **Updated**: 2020-10-02 06:13:41+00:00
- **Authors**: Thomas Kurbiel, Akash Sachdeva, Kun Zhao, Markus Buehren
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: The ability to predict multiple possible future positions of the ego-vehicle given the surrounding context while also estimating their probabilities is key to safe autonomous driving. Most of the current state-of-the-art Deep Learning approaches are trained on trajectory data to achieve this task. However trajectory data captured by sensor systems is highly imbalanced, since by far most of the trajectories follow straight lines with an approximately constant velocity. This poses a huge challenge for the task of predicting future positions, which is inherently a regression problem. Current state-of-the-art approaches alleviate this problem only by major preprocessing of the training data, e.g. resampling, clustering into anchors etc. In this paper we propose an approach which reformulates the prediction problem as a classification task, allowing for powerful tools, e.g. focal loss, to combat the imbalance. To this end we design a generative probabilistic model consisting of a deep neural network with a Mixture of Gaussian head. A smart choice of the latent variable allows for the reformulation of the log-likelihood function as a combination of a classification problem and a much simplified regression problem. The output of our model is an estimate of the probability density function of future positions, hence allowing for prediction of multiple possible positions while also estimating their probabilities. The proposed approach can easily incorporate context information and does not require any preprocessing of the data.



### Deep4Air: A Novel Deep Learning Framework for Airport Airside Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2010.00806v2
- **DOI**: 10.1109/ICMEW53276.2021.9456005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00806v2)
- **Published**: 2020-10-02 06:33:21+00:00
- **Updated**: 2021-07-21 14:58:10+00:00
- **Authors**: Phat Thai, Sameer Alam, Nimrod Lilith, Phu N. Tran, Binh Nguyen Thanh
- **Comment**: None
- **Journal**: None
- **Summary**: An airport runway and taxiway (airside) area is a highly dynamic and complex environment featuring interactions between different types of vehicles (speed and dimension), under varying visibility and traffic conditions. Airport ground movements are deemed safety-critical activities, and safe-separation procedures must be maintained by Air Traffic Controllers (ATCs). Large airports with complicated runway-taxiway systems use advanced ground surveillance systems. However, these systems have inherent limitations and a lack of real-time analytics. In this paper, we propose a novel computer-vision based framework, namely "Deep4Air", which can not only augment the ground surveillance systems via the automated visual monitoring of runways and taxiways for aircraft location, but also provide real-time speed and distance analytics for aircraft on runways and taxiways. The proposed framework includes an adaptive deep neural network for efficiently detecting and tracking aircraft. The experimental results show an average precision of detection and tracking of up to 99.8% on simulated data with validations on surveillance videos from the digital tower at George Bush Intercontinental Airport. The results also demonstrate that "Deep4Air" can locate aircraft positions relative to the airport runway and taxiway infrastructure with high accuracy. Furthermore, aircraft speed and separation distance are monitored in real-time, providing enhanced safety management.



### Discriminative and Generative Models for Anatomical Shape Analysison Point Clouds with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.00820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07(Primary), I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2010.00820v1)
- **Published**: 2020-10-02 07:37:40+00:00
- **Updated**: 2020-10-02 07:37:40+00:00
- **Authors**: Benjamin Gutierrez Becker, Ignacio Sarasua, Christian Wachinger
- **Comment**: Accepted for publication in the Medical Image Analysis journal
- **Journal**: None
- **Summary**: We introduce deep neural networks for the analysis of anatomical shapes that learn a low-dimensional shape representation from the given task, instead of relying on hand-engineered representations. Our framework is modular and consists of several computing blocks that perform fundamental shape processing tasks. The networks operate on unordered point clouds and provide invariance to similarity transformations, avoiding the need to identify point correspondences between shapes. Based on the framework, we assemble a discriminative model for disease classification and age regression, as well as a generative model for the accruate reconstruction of shapes. In particular, we propose a conditional generative model, where the condition vector provides a mechanism to control the generative process. instance, it enables to assess shape variations specific to a particular diagnosis, when passing it as side information. Next to working on single shapes, we introduce an extension for the joint analysis of multiple anatomical structures, where the simultaneous modeling of multiple structures can lead to a more compact encoding and a better understanding of disorders. We demonstrate the advantages of our framework in comprehensive experiments on real and synthetic data. The key insights are that (i) learning a shape representation specific to the given task yields higher performance than alternative shape descriptors, (ii) multi-structure analysis is both more efficient and more accurate than single-structure analysis, and (iii) point clouds generated by our model capture morphological differences associated to Alzheimers disease, to the point that they can be used to train a discriminative model for disease classification. Our framework naturally scales to the analysis of large datasets, giving it the potential to learn characteristic variations in large populations.



### Explainable Online Validation of Machine Learning Models for Practical Applications
- **Arxiv ID**: http://arxiv.org/abs/2010.00821v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00821v3)
- **Published**: 2020-10-02 07:38:31+00:00
- **Updated**: 2021-01-17 12:26:28+00:00
- **Authors**: Wolfgang Fuhl, Yao Rong, Thomas Motz, Michael Scheidt, Andreas Hartel, Andreas Koch, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: We present a reformulation of the regression and classification, which aims to validate the result of a machine learning algorithm. Our reformulation simplifies the original problem and validates the result of the machine learning algorithm using the training data. Since the validation of machine learning algorithms must always be explainable, we perform our experiments with the kNN algorithm as well as with an algorithm based on conditional probabilities, which is proposed in this work. For the evaluation of our approach, three publicly available data sets were used and three classification and two regression problems were evaluated. The presented algorithm based on conditional probabilities is also online capable and requires only a fraction of memory compared to the kNN algorithm.



### Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.00824v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00824v4)
- **Published**: 2020-10-02 07:42:00+00:00
- **Updated**: 2021-07-01 00:59:01+00:00
- **Authors**: Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, Dieter Fox
- **Comment**: None
- **Journal**: The Conference on Robot Learning (CoRL 2021)
- **Summary**: 6D robotic grasping beyond top-down bin-picking scenarios is a challenging task. Previous solutions based on 6D grasp synthesis with robot motion planning usually operate in an open-loop setting, which are sensitive to grasp synthesis errors. In this work, we propose a new method for learning closed-loop control policies for 6D grasping. Our policy takes a segmented point cloud of an object from an egocentric camera as input, and outputs continuous 6D control actions of the robot gripper for grasping the object. We combine imitation learning and reinforcement learning and introduce a goal-auxiliary actor-critic algorithm for policy learning. We demonstrate that our learned policy can be integrated into a tabletop 6D grasping system and a human-robot handover system to improve the grasping performance of unseen objects. Our videos and code can be found at https://sites.google.com/view/gaddpg .



### Continuous close-range 3D object pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.00829v1
- **DOI**: 10.1109/IROS40897.2019.8967580
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00829v1)
- **Published**: 2020-10-02 07:48:17+00:00
- **Updated**: 2020-10-02 07:48:17+00:00
- **Authors**: Bjarne Grossmann, Francesco Rovida, Volker Krueger
- **Comment**: None
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Summary**: In the context of future manufacturing lines, removing fixtures will be a fundamental step to increase the flexibility of autonomous systems in assembly and logistic operations. Vision-based 3D pose estimation is a necessity to accurately handle objects that might not be placed at fixed positions during the robot task execution. Industrial tasks bring multiple challenges for the robust pose estimation of objects such as difficult object properties, tight cycle times and constraints on camera views. In particular, when interacting with objects, we have to work with close-range partial views of objects that pose a new challenge for typical view-based pose estimation methods. In this paper, we present a 3D pose estimation method based on a gradient-ascend particle filter that integrates new observations on-the-fly to improve the pose estimate. Thereby, we can apply this method online during task execution to save valuable cycle time. In contrast to other view-based pose estimation methods, we model potential views in full 6- dimensional space that allows us to cope with close-range partial objects views. We demonstrate the approach on a real assembly task, in which the algorithm usually converges to the correct pose within 10-15 iterations with an average accuracy of less than 8mm.



### CAPTION: Correction by Analyses, POS-Tagging and Interpretation of Objects using only Nouns
- **Arxiv ID**: http://arxiv.org/abs/2010.00839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00839v1)
- **Published**: 2020-10-02 08:06:42+00:00
- **Updated**: 2020-10-02 08:06:42+00:00
- **Authors**: Leonardo Anjoletto Ferreira, Douglas De Rizzo Meneghetti, Paulo Eduardo Santos
- **Comment**: Published at the First Annual International Workshop on
  Interpretability: Methodologies and algorithms (IMA 2019)
- **Journal**: None
- **Summary**: Recently, Deep Learning (DL) methods have shown an excellent performance in image captioning and visual question answering. However, despite their performance, DL methods do not learn the semantics of the words that are being used to describe a scene, making it difficult to spot incorrect words used in captions or to interchange words that have similar meanings. This work proposes a combination of DL methods for object detection and natural language processing to validate image's captions. We test our method in the FOIL-COCO data set, since it provides correct and incorrect captions for various images using only objects represented in the MS-COCO image data set. Results show that our method has a good overall performance, in some cases similar to the human performance.



### Morphological segmentation of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2010.00853v1
- **DOI**: 10.5566/ias.v26.p101-109
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00853v1)
- **Published**: 2020-10-02 08:32:52+00:00
- **Updated**: 2020-10-02 08:32:52+00:00
- **Authors**: Guillaume Noyel, Jesus Angulo, Dominique Jeulin
- **Comment**: None
- **Journal**: Image Analysis and Stereology, International Society for
  Stereology, 2007, 26 (3), pp.101-109
- **Summary**: The present paper develops a general methodology for the morphological segmentation of hyperspectral images, i.e., with an important number of channels. This approach, based on watershed, is composed of a spectral classification to obtain the markers and a vectorial gradient which gives the spatial information. Several alternative gradients are adapted to the different hyperspectral functions. Data reduction is performed either by Factor Analysis or by model fitting. Image segmentation is done on different spaces: factor space, parameters space, etc. On all these spaces the spatial/spectral segmentation approach is applied, leading to relevant results on the image.



### Weight and Gradient Centralization in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.00866v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00866v3)
- **Published**: 2020-10-02 08:50:04+00:00
- **Updated**: 2021-01-17 12:05:14+00:00
- **Authors**: Wolfgang Fuhl, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Batch normalization is currently the most widely used variant of internal normalization for deep neural networks. Additional work has shown that the normalization of weights and additional conditioning as well as the normalization of gradients further improve the generalization. In this work, we combine several of these methods and thereby increase the generalization of the networks. The advantage of the newer methods compared to the batch normalization is not only increased generalization, but also that these methods only have to be applied during training and, therefore, do not influence the running time during use. Link to CUDA code https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/



### Rotated Ring, Radial and Depth Wise Separable Radial Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2010.00873v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00873v3)
- **Published**: 2020-10-02 09:01:51+00:00
- **Updated**: 2021-01-17 12:08:07+00:00
- **Authors**: Wolfgang Fuhl, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Simple image rotations significantly reduce the accuracy of deep neural networks. Moreover, training with all possible rotations increases the data set, which also increases the training duration. In this work, we address trainable rotation invariant convolutions as well as the construction of nets, since fully connected layers can only be rotation invariant with a one-dimensional input. On the one hand, we show that our approach is rotationally invariant for different models and on different public data sets. We also discuss the influence of purely rotational invariant features on accuracy. The rotationally adaptive convolution models presented in this work are more computationally intensive than normal convolution models. Therefore, we also present a depth wise separable approach with radial convolution. Link to CUDA code https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/



### Remote Sensing Image Scene Classification with Self-Supervised Paradigm under Limited Labeled Samples
- **Arxiv ID**: http://arxiv.org/abs/2010.00882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00882v1)
- **Published**: 2020-10-02 09:27:19+00:00
- **Updated**: 2020-10-02 09:27:19+00:00
- **Authors**: Chao Tao, Ji Qi, Weipeng Lu, Hao Wang, Haifeng Li
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: With the development of deep learning, supervised learning methods perform well in remote sensing images (RSIs) scene classification. However, supervised learning requires a huge number of annotated data for training. When labeled samples are not sufficient, the most common solution is to fine-tune the pre-training models using a large natural image dataset (e.g. ImageNet). However, this learning paradigm is not a panacea, especially when the target remote sensing images (e.g. multispectral and hyperspectral data) have different imaging mechanisms from RGB natural images. To solve this problem, we introduce new self-supervised learning (SSL) mechanism to obtain the high-performance pre-training model for RSIs scene classification from large unlabeled data. Experiments on three commonly used RSIs scene classification datasets demonstrated that this new learning paradigm outperforms the traditional dominant ImageNet pre-trained model. Moreover, we analyze the impacts of several factors in SSL on RSIs scene classification tasks, including the choice of self-supervised signals, the domain difference between the source and target dataset, and the amount of pre-training data. The insights distilled from our studies can help to foster the development of SSL in the remote sensing community. Since SSL could learn from unlabeled massive RSIs which are extremely easy to obtain, it will be a potentially promising way to alleviate dependence on labeled samples and thus efficiently solve many problems, such as global mapping.



### OpenTraj: Assessing Prediction Complexity in Human Trajectories Datasets
- **Arxiv ID**: http://arxiv.org/abs/2010.00890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00890v2)
- **Published**: 2020-10-02 09:37:18+00:00
- **Updated**: 2020-11-02 21:24:33+00:00
- **Authors**: Javad Amirian, Bingqing Zhang, Francisco Valente Castro, Juan Jose Baldelomar, Jean-Bernard Hayet, Julien Pettre
- **Comment**: ACCV2020
- **Journal**: None
- **Summary**: Human Trajectory Prediction (HTP) has gained much momentum in the last years and many solutions have been proposed to solve it. Proper benchmarking being a key issue for comparing methods, this paper addresses the question of evaluating how complex is a given dataset with respect to the prediction problem. For assessing a dataset complexity, we define a series of indicators around three concepts: Trajectory predictability; Trajectory regularity; Context complexity. We compare the most common datasets used in HTP in the light of these indicators and discuss what this may imply on benchmarking of HTP algorithms. Our source code is released on Github.



### Weight Encode Reconstruction Network for Computed Tomography in a Semi-Case-Wise and Learning-Based Way
- **Arxiv ID**: http://arxiv.org/abs/2010.00893v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00893v1)
- **Published**: 2020-10-02 09:46:35+00:00
- **Updated**: 2020-10-02 09:46:35+00:00
- **Authors**: Hujie Pan, Xuesong Li, Min Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Classic algebraic reconstruction technology (ART) for computed tomography requires pre-determined weights of the voxels for projecting pixel values. However, such weight cannot be accurately obtained due to the limitation of the physical understanding and computation resources. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to tackle the issues mentioned above. The model is trained in a self-supervised manner without the label of a voxel set. It contains two branches, including the voxel weight encoder and the voxel attention part. Using gradient normalization, we are able to co-train the encoder and voxel set numerically stably. With WERNet, the reconstructed result was obtained with a cosine similarity greater than 0.999 with the ground truth. Moreover, the model shows the extraordinary capability of denoising comparing to the classic ART method. In the generalization test of the model, the encoder is transferable from a voxel set with complex structure to the unseen cases without the deduction of the accuracy.



### Tubular Shape Aware Data Generation for Semantic Segmentation in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.00907v2
- **DOI**: 10.1007/s11548-022-02621-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00907v2)
- **Published**: 2020-10-02 10:28:25+00:00
- **Updated**: 2020-12-07 15:11:33+00:00
- **Authors**: Ilyas Sirazitdinov, Heinrich Schulz, Axel Saalbach, Steffen Renisch, Dmitry V. Dylov
- **Comment**: None
- **Journal**: International Journal of Computer Assisted Radiology and Surgery,
  V. 17, pp.1091-1099, 2022
- **Summary**: Chest X-ray is one of the most widespread examinations of the human body. In interventional radiology, its use is frequently associated with the need to visualize various tube-like objects, such as puncture needles, guiding sheaths, wires, and catheters. Detection and precise localization of these tube-like objects in the X-ray images is, therefore, of utmost value, catalyzing the development of accurate target-specific segmentation algorithms. Similar to the other medical imaging tasks, the manual pixel-wise annotation of the tubes is a resource-consuming process. In this work, we aim to alleviate the lack of the annotated images by using artificial data. Specifically, we present an approach for synthetic data generation of the tube-shaped objects, with a generative adversarial network being regularized with a prior-shape constraint. Our method eliminates the need for paired image--mask data and requires only a weakly-labeled dataset (10--20 images) to reach the accuracy of the fully-supervised models. We report the applicability of the approach for the task of segmenting tubes and catheters in the X-ray images, whereas the results should also hold for the other imaging modalities.



### Self-Play Reinforcement Learning for Fast Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/2010.00909v1
- **DOI**: 10.1145/3394171.3413857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00909v1)
- **Published**: 2020-10-02 10:31:27+00:00
- **Updated**: 2020-10-02 10:31:27+00:00
- **Authors**: Nobukatsu Kajiura, Satoshi Kosugi, Xueting Wang, Toshihiko Yamasaki
- **Comment**: Accepted to ACM Multimedia 2020
- **Journal**: None
- **Summary**: In this study, we address image retargeting, which is a task that adjusts input images to arbitrary sizes. In one of the best-performing methods called MULTIOP, multiple retargeting operators were combined and retargeted images at each stage were generated to find the optimal sequence of operators that minimized the distance between original and retargeted images. The limitation of this method is in its tremendous processing time, which severely prohibits its practical use. Therefore, the purpose of this study is to find the optimal combination of operators within a reasonable processing time; we propose a method of predicting the optimal operator for each step using a reinforcement learning agent. The technical contributions of this study are as follows. Firstly, we propose a reward based on self-play, which will be insensitive to the large variance in the content-dependent distance measured in MULTIOP. Secondly, we propose to dynamically change the loss weight for each action to prevent the algorithm from falling into a local optimum and from choosing only the most frequently used operator in its training. Our experiments showed that we achieved multi-operator image retargeting with less processing time by three orders of magnitude and the same quality as the original multi-operator-based method, which was the best-performing algorithm in retargeting tasks.



### Multiple Infrared Small Targets Detection based on Hierarchical Maximal Entropy Random Walk
- **Arxiv ID**: http://arxiv.org/abs/2010.00923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00923v1)
- **Published**: 2020-10-02 11:11:34+00:00
- **Updated**: 2020-10-02 11:11:34+00:00
- **Authors**: Chaoqun Xia, Xiaorun Li, Liaoying Zhao, Shuhan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The technique of detecting multiple dim and small targets with low signal-to-clutter ratios (SCR) is very important for infrared search and tracking systems. In this paper, we establish a detection method derived from maximal entropy random walk (MERW) to robustly detect multiple small targets. Initially, we introduce the primal MERW and analyze the feasibility of applying it to small target detection. However, the original weight matrix of the MERW is sensitive to interferences. Therefore, a specific weight matrix is designed for the MERW in principle of enhancing characteristics of small targets and suppressing strong clutters. Moreover, the primal MERW has a critical limitation of strong bias to the most salient small target. To achieve multiple small targets detection, we develop a hierarchical version of the MERW method. Based on the hierarchical MERW (HMERW), we propose a small target detection method as follows. First, filtering technique is used to smooth the infrared image. Second, an output map is obtained by importing the filtered image into the HMERW. Then, a coefficient map is constructed to fuse the stationary dirtribution map of the HMERW. Finally, an adaptive threshold is used to segment multiple small targets from the fusion map. Extensive experiments on practical data sets demonstrate that the proposed method is superior to the state-of-the-art methods in terms of target enhancement, background suppression and multiple small targets detection.



### Multi-Resolution 3D Convolutional Neural Networks for Automatic Coronary Centerline Extraction in Cardiac CT Angiography Scans
- **Arxiv ID**: http://arxiv.org/abs/2010.00925v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00925v2)
- **Published**: 2020-10-02 11:20:25+00:00
- **Updated**: 2020-12-07 06:09:17+00:00
- **Authors**: Zohaib Salahuddin, Matthias Lenga, Hannes Nickisch
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep learning-based automatic coronary artery tree centerline tracker (AuCoTrack) extending the vessel tracker by Wolterink (arXiv:1810.03143). A dual pathway Convolutional Neural Network (CNN) operating on multi-scale 3D inputs predicts the direction of the coronary arteries as well as the presence of a bifurcation. A similar multi-scale dual pathway 3D CNN is trained to identify coronary artery endpoints for terminating the tracking process. Two or more continuation directions are derived based on the bifurcation detection. The iterative tracker detects the entire left and right coronary artery trees based on only two ostium landmarks derived from a model-based segmentation of the heart.   The 3D CNNs were trained on a proprietary dataset consisting of 43 CCTA scans. An average sensitivity of 87.1% and clinically relevant overlap of 89.1% was obtained relative to a refined manual segmentation. In addition, the MICCAI 2008 Coronary Artery Tracking Challenge (CAT08) training and test datasets were used to benchmark the algorithm and to assess its generalization. An average overlap of 93.6% and a clinically relevant overlap of 96.4% were obtained. The proposed method achieved better overlap scores than the current state-of-the-art automatic centerline extraction techniques on the CAT08 dataset with a vessel detection rate of 95%.



### Image-based underwater 3D reconstruction for Cultural Heritage: from image collection to 3D. Critical steps and considerations
- **Arxiv ID**: http://arxiv.org/abs/2010.00928v1
- **DOI**: 10.1007/978-3-030-37191-3_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00928v1)
- **Published**: 2020-10-02 11:32:33+00:00
- **Updated**: 2020-10-02 11:32:33+00:00
- **Authors**: Dimitrios Skarlatos, Panagiotis Agrafiotis
- **Comment**: Pre-submission version of the manuscript
- **Journal**: None
- **Summary**: Underwater Cultural Heritage (CH) sites are widely spread; from ruins in coastlines up to shipwrecks in deep. The documentation and preservation of this heritage is an obligation of the mankind, dictated also by the international treaties like the Convention on the Protection of the Underwater Cultural Her-itage which fosters the use of "non-destructive techniques and survey meth-ods in preference over the recovery of objects". However, submerged CH lacks in protection and monitoring in regards to the land CH and nowadays recording and documenting, for digital preservation as well as dissemination through VR to wide public, is of most importance. At the same time, it is most difficult to document it, due to inherent restrictions posed by the environ-ment. In order to create high detailed textured 3D models, optical sensors and photogrammetric techniques seems to be the best solution. This chapter dis-cusses critical aspects of all phases of image based underwater 3D reconstruc-tion process, from data acquisition and data preparation using colour restora-tion and colour enhancement algorithms to Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques to produce an accurate, precise and complete 3D model for a number of applications.



### A Deep-Unfolded Reference-Based RPCA Network For Video Foreground-Background Separation
- **Arxiv ID**: http://arxiv.org/abs/2010.00929v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00929v1)
- **Published**: 2020-10-02 11:40:09+00:00
- **Updated**: 2020-10-02 11:40:09+00:00
- **Authors**: Huynh Van Luong, Boris Joukovsky, Yonina C. Eldar, Nikos Deligiannis
- **Comment**: 5 pages, accepted for publication
- **Journal**: None
- **Summary**: Deep unfolded neural networks are designed by unrolling the iterations of optimization algorithms. They can be shown to achieve faster convergence and higher accuracy than their optimization counterparts. This paper proposes a new deep-unfolding-based network design for the problem of Robust Principal Component Analysis (RPCA) with application to video foreground-background separation. Unlike existing designs, our approach focuses on modeling the temporal correlation between the sparse representations of consecutive video frames. To this end, we perform the unfolding of an iterative algorithm for solving reweighted $\ell_1$-$\ell_1$ minimization; this unfolding leads to a different proximal operator (a.k.a. different activation function) adaptively learned per neuron. Experimentation using the moving MNIST dataset shows that the proposed network outperforms a recently proposed state-of-the-art RPCA network in the task of video foreground-background separation.



### MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2010.00947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00947v1)
- **Published**: 2020-10-02 12:24:48+00:00
- **Updated**: 2020-10-02 12:24:48+00:00
- **Authors**: Shengyu Zhang, Donghui Wang, Zhou Zhao, Siliang Tang, Di Xie, Fei Wu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of text-to-pedestrian synthesis, which has many potential applications in art, design, and video surveillance. Existing methods for text-to-bird/flower synthesis are still far from solving this fine-grained image generation problem, due to the complex structure and heterogeneous appearance that the pedestrians naturally take on. To this end, we propose the Multi-Grained Discrimination enhanced Generative Adversarial Network, that capitalizes a human-part-based Discriminator (HPD) and a self-cross-attended (SCA) global Discriminator in order to capture the coherence of the complex body structure. A fined-grained word-level attention mechanism is employed in the HPD module to enforce diversified appearance and vivid details. In addition, two pedestrian generation metrics, named Pose Score and Pose Variance, are devised to evaluate the generation quality and diversity, respectively. We conduct extensive experiments and ablation studies on the caption-annotated pedestrian dataset, CUHK Person Description Dataset. The substantial improvement over the various metrics demonstrates the efficacy of MGD-GAN on the text-to-pedestrian synthesis scenario.



### DOTS: Decoupling Operation and Topology in Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2010.00969v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00969v3)
- **Published**: 2020-10-02 13:00:18+00:00
- **Updated**: 2021-04-08 07:36:23+00:00
- **Authors**: Yu-Chao Gu, Li-Juan Wang, Yun Liu, Yi Yang, Yu-Huan Wu, Shao-Ping Lu, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable Architecture Search (DARTS) has attracted extensive attention due to its efficiency in searching for cell structures. DARTS mainly focuses on the operation search and derives the cell topology from the operation weights. However, the operation weights can not indicate the importance of cell topology and result in poor topology rating correctness. To tackle this, we propose to Decouple the Operation and Topology Search (DOTS), which decouples the topology representation from operation weights and makes an explicit topology search. DOTS is achieved by introducing a topology search space that contains combinations of candidate edges. The proposed search space directly reflects the search objective and can be easily extended to support a flexible number of edges in the searched cell. Existing gradient-based NAS methods can be incorporated into DOTS for further improvement by the topology search. Considering that some operations (e.g., Skip-Connection) can affect the topology, we propose a group operation search scheme to preserve topology-related operations for a better topology search. The experiments on CIFAR10/100 and ImageNet demonstrate that DOTS is an effective solution for differentiable NAS.



### RISA-Net: Rotation-Invariant Structure-Aware Network for Fine-Grained 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2010.00973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00973v1)
- **Published**: 2020-10-02 13:06:12+00:00
- **Updated**: 2020-10-02 13:06:12+00:00
- **Authors**: Rao Fu, Jie Yang, Jiawei Sun, Fang-Lue Zhang, Yu-Kun Lai, Lin Gao
- **Comment**: The code and dataset are available at:
  https://github.com/IGLICT/RisaNET
- **Journal**: None
- **Summary**: Fine-grained 3D shape retrieval aims to retrieve 3D shapes similar to a query shape in a repository with models belonging to the same class, which requires shape descriptors to be capable of representing detailed geometric information to discriminate shapes with globally similar structures. Moreover, 3D objects can be placed with arbitrary position and orientation in real-world applications, which further requires shape descriptors to be robust to rigid transformations. The shape descriptions used in existing 3D shape retrieval systems fail to meet the above two criteria. In this paper, we introduce a novel deep architecture, RISA-Net, which learns rotation invariant 3D shape descriptors that are capable of encoding fine-grained geometric information and structural information, and thus achieve accurate results on the task of fine-grained 3D object retrieval. RISA-Net extracts a set of compact and detailed geometric features part-wisely and discriminatively estimates the contribution of each semantic part to shape representation. Furthermore, our method is able to learn the importance of geometric and structural information of all the parts when generating the final compact latent feature of a 3D shape for fine-grained retrieval. We also build and publish a new 3D shape dataset with sub-class labels for validating the performance of fine-grained 3D shape retrieval methods. Qualitative and quantitative experiments show that our RISA-Net outperforms state-of-the-art methods on the fine-grained object retrieval task, demonstrating its capability in geometric detail extraction. The code and dataset are available at: https://github.com/IGLICT/RisaNET.



### Taking Modality-free Human Identification as Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.00975v2
- **DOI**: 10.1109/TCSVT.2021.3137216
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00975v2)
- **Published**: 2020-10-02 13:08:27+00:00
- **Updated**: 2021-12-30 08:35:12+00:00
- **Authors**: Zhizhe Liu, Xingxing Zhang, Zhenfeng Zhu, Shuai Zheng, Yao Zhao, Jian Cheng
- **Comment**: This manuscript has been accepted by IEEE Transactions on Circuits
  and Systems for Video Technology
- **Journal**: None
- **Summary**: Human identification is an important topic in event detection, person tracking, and public security. There have been numerous methods proposed for human identification, such as face identification, person re-identification, and gait identification. Typically, existing methods predominantly classify a queried image to a specific identity in an image gallery set (I2I). This is seriously limited for the scenario where only a textual description of the query or an attribute gallery set is available in a wide range of video surveillance applications (A2I or I2A). However, very few efforts have been devoted towards modality-free identification, i.e., identifying a query in a gallery set in a scalable way. In this work, we take an initial attempt, and formulate such a novel Modality-Free Human Identification (named MFHI) task as a generic zero-shot learning model in a scalable way. Meanwhile, it is capable of bridging the visual and semantic modalities by learning a discriminative prototype of each identity. In addition, the semantics-guided spatial attention is enforced on visual modality to obtain representations with both high global category-level and local attribute-level discrimination. Finally, we design and conduct an extensive group of experiments on two common challenging identification tasks, including face identification and person re-identification, demonstrating that our method outperforms a wide variety of state-of-the-art methods on modality-free human identification.



### Group Equivariant Stand-Alone Self-Attention For Vision
- **Arxiv ID**: http://arxiv.org/abs/2010.00977v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00977v2)
- **Published**: 2020-10-02 13:16:00+00:00
- **Updated**: 2021-03-18 19:19:38+00:00
- **Authors**: David W. Romero, Jean-Baptiste Cordonnier
- **Comment**: Proceedings of the 9th International Conference on Learning
  Representations (ICLR), 2021
- **Journal**: Proceedings of the International Conference on Learning
  Representations, 2021
- **Summary**: We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.



### An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders
- **Arxiv ID**: http://arxiv.org/abs/2010.00984v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.00984v1)
- **Published**: 2020-10-02 13:29:41+00:00
- **Updated**: 2020-10-02 13:29:41+00:00
- **Authors**: Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio Merra
- **Comment**: 9 pages, 1 figure
- **Journal**: None
- **Summary**: Visual-based recommender systems (VRSs) enhance recommendation performance by integrating users' feedback with the visual features of product images extracted from a deep neural network (DNN). Recently, human-imperceptible images perturbations, defined \textit{adversarial attacks}, have been demonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking category of products. However, since adversarial training techniques have proven to successfully robustify DNNs in preserving classification accuracy, to the best of our knowledge, two important questions have not been investigated yet: 1) How well can these defensive mechanisms protect the VRSs performance? 2) What are the reasons behind ineffective/effective defenses? To answer these questions, we define a set of defense and attack settings, as well as recommender models, to empirically investigate the efficacy of defensive mechanisms. The results indicate alarming risks in protecting a VRS through the DNN robustification. Our experiments shed light on the importance of visual features in very effective attack scenarios. Given the financial impact of VRSs on many companies, we believe this work might rise the need to investigate how to successfully protect visual-based recommenders. Source code and data are available at https://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.



### Uncertainty driven probabilistic voxel selection for image registration
- **Arxiv ID**: http://arxiv.org/abs/2010.00988v1
- **DOI**: 10.1109/TMI.2013.2264467
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00988v1)
- **Published**: 2020-10-02 13:33:54+00:00
- **Updated**: 2020-10-02 13:33:54+00:00
- **Authors**: Boris N. Oreshkin, Tal Arbel
- **Comment**: None
- **Journal**: in IEEE Transactions on Medical Imaging, vol. 32, no. 10, pp.
  1777-1790, Oct. 2013
- **Summary**: This paper presents a novel probabilistic voxel selection strategy for medical image registration in time-sensitive contexts, where the goal is aggressive voxel sampling (e.g. using less than 1% of the total number) while maintaining registration accuracy and low failure rate. We develop a Bayesian framework whereby, first, a voxel sampling probability field (VSPF) is built based on the uncertainty on the transformation parameters. We then describe a practical, multi-scale registration algorithm, where, at each optimization iteration, different voxel subsets are sampled based on the VSPF. The approach maximizes accuracy without committing to a particular fixed subset of voxels. The probabilistic sampling scheme developed is shown to manage the tradeoff between the robustness of traditional random voxel selection (by permitting more exploration) and the accuracy of fixed voxel selection (by permitting a greater proportion of informative voxels).



### RDCNet: Instance segmentation with a minimalist recurrent residual network
- **Arxiv ID**: http://arxiv.org/abs/2010.00991v1
- **DOI**: 10.1007/978-3-030-59861-7
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00991v1)
- **Published**: 2020-10-02 13:36:45+00:00
- **Updated**: 2020-10-02 13:36:45+00:00
- **Authors**: Raphael Ortiz, Gustavo de Medeiros, Antoine H. F. M. Peters, Prisca Liberali, Markus Rempfler
- **Comment**: Accepted at MICCAI-MLMI 2020 workshop
- **Journal**: None
- **Summary**: Instance segmentation is a key step for quantitative microscopy. While several machine learning based methods have been proposed for this problem, most of them rely on computationally complex models that are trained on surrogate tasks. Building on recent developments towards end-to-end trainable instance segmentation, we propose a minimalist recurrent network called recurrent dilated convolutional network (RDCNet), consisting of a shared stacked dilated convolution (sSDC) layer that iteratively refines its output and thereby generates interpretable intermediate predictions. It is light-weight and has few critical hyperparameters, which can be related to physical aspects such as object size or density.We perform a sensitivity analysis of its main parameters and we demonstrate its versatility on 3 tasks with different imaging modalities: nuclear segmentation of H&E slides, of 3D anisotropic stacks from light-sheet fluorescence microscopy and leaf segmentation of top-view images of plants. It achieves state-of-the-art on 2 of the 3 datasets.



### Understanding the Predictability of Gesture Parameters from Speech and their Perceptual Importance
- **Arxiv ID**: http://arxiv.org/abs/2010.00995v1
- **DOI**: 10.1145/3383652.3423882
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00995v1)
- **Published**: 2020-10-02 13:43:33+00:00
- **Updated**: 2020-10-02 13:43:33+00:00
- **Authors**: Ylva Ferstl, Michael Neff, Rachel McDonnell
- **Comment**: To be published in the Proceedings of the 20th ACM International
  Conference on Intelligent Virtual Agents (IVA 20)
- **Journal**: None
- **Summary**: Gesture behavior is a natural part of human conversation. Much work has focused on removing the need for tedious hand-animation to create embodied conversational agents by designing speech-driven gesture generators. However, these generators often work in a black-box manner, assuming a general relationship between input speech and output motion. As their success remains limited, we investigate in more detail how speech may relate to different aspects of gesture motion. We determine a number of parameters characterizing gesture, such as speed and gesture size, and explore their relationship to the speech signal in a two-fold manner. First, we train multiple recurrent networks to predict the gesture parameters from speech to understand how well gesture attributes can be modeled from speech alone. We find that gesture parameters can be partially predicted from speech, and some parameters, such as path length, being predicted more accurately than others, like velocity. Second, we design a perceptual study to assess the importance of each gesture parameter for producing motion that people perceive as appropriate for the speech. Results show that a degradation in any parameter was viewed negatively, but some changes, such as hand shape, are more impactful than others. A video summarization can be found at https://youtu.be/aw6-_5kmLjY.



### Optimization over Random and Gradient Probabilistic Pixel Sampling for Fast, Robust Multi-Resolution Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2010.02505v1
- **DOI**: 10.1007/978-3-642-31340-0_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02505v1)
- **Published**: 2020-10-02 13:43:50+00:00
- **Updated**: 2020-10-02 13:43:50+00:00
- **Authors**: Boris N. Oreshkin, Tal Arbel
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2010.00988
- **Journal**: WBIR 2012. Lecture Notes in Computer Science, vol 7359. Springer,
  Berlin, Heidelberg
- **Summary**: This paper presents an approach to fast image registration through probabilistic pixel sampling. We propose a practical scheme to leverage the benefits of two state-of-the-art pixel sampling approaches: gradient magnitude based pixel sampling and uniformly random sampling. Our framework involves learning the optimal balance between the two sampling schemes off-line during training, based on a small training dataset, using particle swarm optimization. We then test the proposed sampling approach on 3D rigid registration against two state-of-the-art approaches based on the popular, publicly available, Vanderbilt RIRE dataset. Our results indicate that the proposed sampling approach yields much faster, accurate and robust registration results when compared against the state-of-the-art.



### DIRV: Dense Interaction Region Voting for End-to-End Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.01005v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01005v2)
- **Published**: 2020-10-02 13:57:58+00:00
- **Updated**: 2021-01-19 16:48:22+00:00
- **Authors**: Hao-Shu Fang, Yichen Xie, Dian Shao, Cewu Lu
- **Comment**: Paper is accepted. Code available at:
  https://github.com/MVIG-SJTU/DIRV
- **Journal**: None
- **Summary**: Recent years, human-object interaction (HOI) detection has achieved impressive advances. However, conventional two-stage methods are usually slow in inference. On the other hand, existing one-stage methods mainly focus on the union regions of interactions, which introduce unnecessary visual information as disturbances to HOI detection. To tackle the problems above, we propose a novel one-stage HOI detection approach DIRV in this paper, based on a new concept called interaction region for the HOI problem. Unlike previous methods, our approach concentrates on the densely sampled interaction regions across different scales for each human-object pair, so as to capture the subtle visual features that is most essential to the interaction. Moreover, in order to compensate for the detection flaws of a single interaction region, we introduce a novel voting strategy that makes full use of those overlapped interaction regions in place of conventional Non-Maximal Suppression (NMS). Extensive experiments on two popular benchmarks: V-COCO and HICO-DET show that our approach outperforms existing state-of-the-arts by a large margin with the highest inference speed and lightest network architecture. We achieved 56.1 mAP on V-COCO without addtional input. Our code is publicly available at: https://github.com/MVIG-SJTU/DIRV



### DecAug: Augmenting HOI Detection via Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2010.01007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01007v1)
- **Published**: 2020-10-02 13:59:05+00:00
- **Updated**: 2020-10-02 13:59:05+00:00
- **Authors**: Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Human-object interaction (HOI) detection requires a large amount of annotated data. Current algorithms suffer from insufficient training samples and category imbalance within datasets. To increase data efficiency, in this paper, we propose an efficient and effective data augmentation method called DecAug for HOI detection. Based on our proposed object state similarity metric, object patterns across different HOIs are shared to augment local object appearance features without changing their state. Further, we shift spatial correlation between humans and objects to other feasible configurations with the aid of a pose-guided Gaussian Mixture Model while preserving their interactions. Experiments show that our method brings up to 3.3 mAP and 1.6 mAP improvements on V-COCO and HICODET dataset for two advanced models. Specifically, interactions with fewer samples enjoy more notable improvement. Our method can be easily integrated into various HOI detection models with negligible extra computational consumption. Our code will be made publicly available.



### AVECL-UMONS database for audio-visual event classification and localization
- **Arxiv ID**: http://arxiv.org/abs/2011.01018v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2011.01018v1)
- **Published**: 2020-10-02 14:26:02+00:00
- **Updated**: 2020-10-02 14:26:02+00:00
- **Authors**: Mathilde Brousmiche, Stéphane Dupont, Jean Rouat
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the AVECL-UMons dataset for audio-visual event classification and localization in the context of office environments. The audio-visual dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. Two types of sequences are recorded according to the number of events in the sequence. The dataset comprises 2662 unilabel sequences and 2724 multilabel sequences corresponding to a total of 5.24 hours. The dataset is publicly accessible online : https://zenodo.org/record/3965492#.X09wsobgrCI.



### Hard Negative Mixing for Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01028v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01028v2)
- **Published**: 2020-10-02 14:34:58+00:00
- **Updated**: 2020-12-04 11:46:48+00:00
- **Authors**: Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, Diane Larlus
- **Comment**: Accepted at NeurIPS 2020. Project page with pretrained models:
  https://europe.naverlabs.com/mochi
- **Journal**: None
- **Summary**: Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.



### Encoded Prior Sliced Wasserstein AutoEncoder for learning latent manifold representations
- **Arxiv ID**: http://arxiv.org/abs/2010.01037v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01037v2)
- **Published**: 2020-10-02 14:58:54+00:00
- **Updated**: 2021-12-10 20:40:02+00:00
- **Authors**: Sanjukta Krishnagopal, Jacob Bedrossian
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: While variational autoencoders have been successful in several tasks, the use of conventional priors are limited in their ability to encode the underlying structure of input data. We introduce an Encoded Prior Sliced Wasserstein AutoEncoder wherein an additional prior-encoder network learns an embedding of the data manifold which preserves topological and geometric properties of the data, thus improving the structure of latent space. The autoencoder and prior-encoder networks are iteratively trained using the Sliced Wasserstein distance. The effectiveness of the learned manifold encoding is explored by traversing latent space through interpolations along geodesics which generate samples that lie on the data manifold and hence are more realistic compared to Euclidean interpolation. To this end, we introduce a graph-based algorithm for exploring the data manifold and interpolating along network-geodesics in latent space by maximizing the density of samples along the path while minimizing total energy. We use the 3D-spiral data to show that the prior encodes the geometry underlying the data unlike conventional autoencoders, and to demonstrate the exploration of the embedded data manifold through the network algorithm. We apply our framework to benchmarked image datasets to demonstrate the advantages of learning data representations in outlier generation, latent structure, and geodesic interpolation.



### Attention-Based Clustering: Learning a Kernel from Context
- **Arxiv ID**: http://arxiv.org/abs/2010.01040v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01040v1)
- **Published**: 2020-10-02 15:06:06+00:00
- **Updated**: 2020-10-02 15:06:06+00:00
- **Authors**: Samuel Coward, Erik Visse-Martindale, Chithrupa Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning, no data point stands alone. We believe that context is an underappreciated concept in many machine learning methods. We propose Attention-Based Clustering (ABC), a neural architecture based on the attention mechanism, which is designed to learn latent representations that adapt to context within an input set, and which is inherently agnostic to input sizes and number of clusters. By learning a similarity kernel, our method directly combines with any out-of-the-box kernel-based clustering approach. We present competitive results for clustering Omniglot characters and include analytical evidence of the effectiveness of an attention-based approach for clustering.



### Homography Estimation with Convolutional Neural Networks Under Conditions of Variance
- **Arxiv ID**: http://arxiv.org/abs/2010.01041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01041v2)
- **Published**: 2020-10-02 15:11:25+00:00
- **Updated**: 2020-10-22 16:05:37+00:00
- **Authors**: David Niblick, Avinash Kak
- **Comment**: 9 pages, 16 figures, submitted to 2020 Computer Vision and Pattern
  Recognition Conference
- **Journal**: None
- **Summary**: Planar homography estimation is foundational to many computer vision problems, such as Simultaneous Localization and Mapping (SLAM) and Augmented Reality (AR). However, conditions of high variance confound even the state-of-the-art algorithms. In this report, we analyze the performance of two recently published methods using Convolutional Neural Networks (CNNs) that are meant to replace the more traditional feature-matching based approaches to the estimation of homography. Our evaluation of the CNN based methods focuses particularly on measuring the performance under conditions of significant noise, illumination shift, and occlusion. We also measure the benefits of training CNNs to varying degrees of noise. Additionally, we compare the effect of using color images instead of grayscale images for inputs to CNNs. Finally, we compare the results against baseline feature-matching based homography estimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained to be more robust against noise, but at a small cost to accuracy in the noiseless case. Additionally, CNNs perform significantly better in conditions of extreme variance than their feature-matching based counterparts. With regard to color inputs, we conclude that with no change in the CNN architecture to take advantage of the additional information in the color planes, the difference in performance using color inputs or grayscale inputs is negligible. About the CNNs trained with noise-corrupted inputs, we show that training a CNN to a specific magnitude of noise leads to a "Goldilocks Zone" with regard to the noise levels where that CNN performs best.



### Semantics through Time: Semi-supervised Segmentation of Aerial Videos with Iterative Label Propagation
- **Arxiv ID**: http://arxiv.org/abs/2010.01910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01910v1)
- **Published**: 2020-10-02 15:15:50+00:00
- **Updated**: 2020-10-02 15:15:50+00:00
- **Authors**: Alina Marcu, Vlad Licaret, Dragos Costea, Marius Leordeanu
- **Comment**: Accepted as oral presentation at Asian Conference on Computer Vision
  (ACCV), 2020. arXiv admin note: text overlap with arXiv:1910.10026
- **Journal**: None
- **Summary**: Semantic segmentation is a crucial task for robot navigation and safety. However, current supervised methods require a large amount of pixelwise annotations to yield accurate results. Labeling is a tedious and time consuming process that has hampered progress in low altitude UAV applications. This paper makes an important step towards automatic annotation by introducing SegProp, a novel iterative flow-based method, with a direct connection to spectral clustering in space and time, to propagate the semantic labels to frames that lack human annotations. The labels are further used in semi-supervised learning scenarios. Motivated by the lack of a large video aerial dataset, we also introduce Ruralscapes, a new dataset with high resolution (4K) images and manually-annotated dense labels every 50 frames - the largest of its kind, to the best of our knowledge. Our novel SegProp automatically annotates the remaining unlabeled 98% of frames with an accuracy exceeding 90% (F-measure), significantly outperforming other state-of-the-art label propagation methods. Moreover, when integrating other methods as modules inside SegProp's iterative label propagation loop, we achieve a significant boost over the baseline labels. Finally, we test SegProp in a full semi-supervised setting: we train several state-of-the-art deep neural networks on the SegProp-automatically-labeled training frames and test them on completely novel videos. We convincingly demonstrate, every time, a significant improvement over the supervised scenario.



### Efficient Image Super-Resolution Using Pixel Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.01073v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01073v1)
- **Published**: 2020-10-02 16:04:33+00:00
- **Updated**: 2020-10-02 16:04:33+00:00
- **Authors**: Hengyuan Zhao, Xiangtao Kong, Jingwen He, Yu Qiao, Chao Dong
- **Comment**: 17 pages, 5 figures, conference, accpeted by ECCVW (AIM2020 ESR
  Challenge)
- **Journal**: None
- **Summary**: This work aims at designing a lightweight convolutional neural network for image super resolution (SR). With simplicity bare in mind, we construct a pretty concise and effective network with a newly proposed pixel attention scheme. Pixel attention (PA) is similar as channel attention and spatial attention in formulation. The difference is that PA produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results. On the basis of PA, we propose two building blocks for the main branch and the reconstruction branch, respectively. The first one - SC-PA block has the same structure as the Self-Calibrated convolution but with our PA layer. This block is much more efficient than conventional residual/dense blocks, for its twobranch architecture and attention scheme. While the second one - UPA block combines the nearest-neighbor upsampling, convolution and PA layers. It improves the final reconstruction quality with little parameter cost. Our final model- PAN could achieve similar performance as the lightweight networks - SRResNet and CARN, but with only 272K parameters (17.92% of SRResNet and 17.09% of CARN). The effectiveness of each proposed component is also validated by ablation study. The code is available at https://github.com/zhaohengyuan1/PAN.



### Semi-Supervised Learning for Multi-Task Scene Understanding by Neural Graph Consensus
- **Arxiv ID**: http://arxiv.org/abs/2010.01086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01086v2)
- **Published**: 2020-10-02 16:30:49+00:00
- **Updated**: 2020-12-03 15:31:41+00:00
- **Authors**: Marius Leordeanu, Mihai Pirvu, Dragos Costea, Alina Marcu, Emil Slusanschi, Rahul Sukthankar
- **Comment**: Accepted at the 35th AAAI Conference on Artificial Intelligence (AAAI
  2021)
- **Journal**: None
- **Summary**: We address the challenging problem of semi-supervised learning in the context of multiple visual interpretations of the world by finding consensus in a graph of neural networks. Each graph node is a scene interpretation layer, while each edge is a deep net that transforms one layer at one node into another from a different node. During the supervised phase edge networks are trained independently. During the next unsupervised stage edge nets are trained on the pseudo-ground truth provided by consensus among multiple paths that reach the nets' start and end nodes. These paths act as ensemble teachers for any given edge and strong consensus is used for high-confidence supervisory signal. The unsupervised learning process is repeated over several generations, in which each edge becomes a "student" and also part of different ensemble "teachers" for training other students. By optimizing such consensus between different paths, the graph reaches consistency and robustness over multiple interpretations and generations, in the face of unknown labels. We give theoretical justifications of the proposed idea and validate it on a large dataset. We show how prediction of different representations such as depth, semantic segmentation, surface normals and pose from RGB input could be effectively learned through self-supervised consensus in our graph. We also compare to state-of-the-art methods for multi-task and semi-supervised learning and show superior performance.



### Unsupervised Point Cloud Pre-Training via Occlusion Completion
- **Arxiv ID**: http://arxiv.org/abs/2010.01089v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01089v3)
- **Published**: 2020-10-02 16:43:14+00:00
- **Updated**: 2021-10-13 23:15:38+00:00
- **Authors**: Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, Matthew J. Kusner
- **Comment**: sync with ICCV camera ready
- **Journal**: None
- **Summary**: We describe a simple pre-training approach for point clouds. It works in three steps: 1. Mask all points occluded in a camera view; 2. Learn an encoder-decoder model to reconstruct the occluded points; 3. Use the encoder weights as initialisation for downstream point cloud tasks. We find that even when we construct a single pre-training dataset (from ModelNet40), this pre-training method improves accuracy across different datasets and encoders, on a wide range of downstream tasks. Specifically, we show that our method outperforms previous pre-training methods in object classification, and both part-based and semantic segmentation tasks. We study the pre-trained features and find that they lead to wide downstream minima, have high transformation invariance, and have activations that are highly correlated with part labels. Code and data are available at: https://github.com/hansen7/OcCo



### Efficient Colon Cancer Grading with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01091v1)
- **Published**: 2020-10-02 16:43:33+00:00
- **Updated**: 2020-10-02 16:43:33+00:00
- **Authors**: Franziska Lippoldt
- **Comment**: 10 pages, draft version
- **Journal**: None
- **Summary**: Dealing with the application of grading colorectal cancer images, this work proposes a 3 step pipeline for prediction of cancer levels from a histopathology image. The overall model performs better compared to other state of the art methods on the colorectal cancer grading data set and shows excellent performance for the extended colorectal cancer grading set. The performance improvements can be attributed to two main factors: The feature selection and graph augmentation method described here are spatially aware, but overall pixel position independent. Further, the graph size in terms of nodes becomes stable with respect to the model's prediction and accuracy for sufficiently large models. The graph neural network itself consists of three convolutional blocks and linear layers, which is a rather simple design compared to other networks for this application.



### Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01097v1)
- **Published**: 2020-10-02 16:50:26+00:00
- **Updated**: 2020-10-02 16:50:26+00:00
- **Authors**: Kun Yuan, Quanquan Li, Dapeng Chen, Aojun Zhou, Junjie Yan
- **Comment**: 13 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass. We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.



### AIM 2020 Challenge on Image Extreme Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2010.01110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01110v1)
- **Published**: 2020-10-02 17:11:17+00:00
- **Updated**: 2020-10-02 17:11:17+00:00
- **Authors**: Evangelos Ntavelis, Andrés Romero, Siavash Bigdeli, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the AIM 2020 challenge on extreme image inpainting. This report focuses on proposed solutions and results for two different tracks on extreme image inpainting: classical image inpainting and semantically guided image inpainting. The goal of track 1 is to inpaint considerably large part of the image using no supervision but the context. Similarly, the goal of track 2 is to inpaint the image by having access to the entire semantic segmentation map of the image to inpaint. The challenge had 88 and 74 participants, respectively. 11 and 6 teams competed in the final phase of the challenge, respectively. This report gauges current solutions and set a benchmark for future extreme image inpainting methods.



### Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.01114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01114v1)
- **Published**: 2020-10-02 17:17:45+00:00
- **Updated**: 2020-10-02 17:17:45+00:00
- **Authors**: Patrick Dendorfer, Aljoša Ošep, Laura Leal-Taixé
- **Comment**: Oral presentation at ACCV 2020
- **Journal**: None
- **Summary**: In this paper, we present Goal-GAN, an interpretable and end-to-end trainable model for human trajectory prediction. Inspired by human navigation, we model the task of trajectory prediction as an intuitive two-stage process: (i) goal estimation, which predicts the most likely target positions of the agent, followed by a (ii) routing module which estimates a set of plausible trajectories that route towards the estimated goal. We leverage information about the past trajectory and visual context of the scene to estimate a multi-modal probability distribution over the possible goal positions, which is used to sample a potential goal during the inference. The routing is governed by a recurrent neural network that reacts to physical constraints in the nearby surroundings and generates feasible paths that route towards the sampled goal. Our extensive experimental evaluation shows that our method establishes a new state-of-the-art on several benchmarks while being able to generate a realistic and diverse set of trajectories that conform to physical constraints.



### Embedded Systems and Computer Vision Techniques utilized in Spray Painting Robots: A Review
- **Arxiv ID**: http://arxiv.org/abs/2010.01131v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0; I.4.3; I.4.6; I.4.9; I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/2010.01131v1)
- **Published**: 2020-10-02 17:59:03+00:00
- **Updated**: 2020-10-02 17:59:03+00:00
- **Authors**: Soham Shah, Siddhi Vinayak Pandey, Archit Sorathiya, Raj Sheth, Alok Kumar Singh, Jignesh Thaker
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: The advent of the era of machines has limited human interaction and this has increased their presence in the last decade. The requirement to increase the effectiveness, durability and reliability in the robots has also risen quite drastically too. Present paper covers the various embedded system and computer vision methodologies, techniques and innovations used in the field of spray painting robots. There have been many advancements in the sphere of painting robots utilized for high rise buildings, wall painting, road marking paintings, etc. Review focuses on image processing, computational and computer vision techniques that can be applied in the product to increase efficiency of the performance drastically. Image analysis, filtering, enhancement, object detection, edge detection methods, path and localization methods and fine tuning of parameters are being discussed in depth to use while developing such products. Dynamic system design is being deliberated by using which results in reduction of human interaction, environment sustainability and better quality of work in detail. Embedded systems involving the micro-controllers, processors, communicating devices, sensors and actuators, soft-ware to use them; is being explained for end-to-end development and enhancement of accuracy and precision in Spray Painting Robots.



### Semantics-Guided Clustering with Deep Progressive Learning for Semi-Supervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2010.01148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01148v1)
- **Published**: 2020-10-02 18:02:35+00:00
- **Updated**: 2020-10-02 18:02:35+00:00
- **Authors**: Chih-Ting Liu, Yu-Jhe Li, Shao-Yi Chien, Yu-Chiang Frank Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-ID) requires one to match images of the same person across camera views. As a more challenging task, semi-supervised re-ID tackles the problem that only a number of identities in training data are fully labeled, while the remaining are unlabeled. Assuming that such labeled and unlabeled training data share disjoint identity labels, we propose a novel framework of Semantics-Guided Clustering with Deep Progressive Learning (SGC-DPL) to jointly exploit the above data. By advancing the proposed Semantics-Guided Affinity Propagation (SG-AP), we are able to assign pseudo-labels to selected unlabeled data in a progressive fashion, under the semantics guidance from the labeled ones. As a result, our approach is able to augment the labeled training data in the semi-supervised setting. Our experiments on two large-scale person re-ID benchmarks demonstrate the superiority of our SGC-DPL over state-of-the-art methods across different degrees of supervision. In extension, the generalization ability of our SGC-DPL is also verified in other tasks like vehicle re-ID or image retrieval with the semi-supervised setting.



### MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand Pose Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.01158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.01158v1)
- **Published**: 2020-10-02 18:27:34+00:00
- **Updated**: 2020-10-02 18:27:34+00:00
- **Authors**: Zhenyu Wu, Duc Hoang, Shih-Yao Lin, Yusheng Xie, Liangjian Chen, Yen-Yu Lin, Zhangyang Wang, Wei Fan
- **Comment**: Accepted by ACM Multimedia 2020
- **Journal**: None
- **Summary**: Estimating the 3D hand pose from a monocular RGB image is important but challenging. A solution is training on large-scale RGB hand images with accurate 3D hand keypoint annotations. However, it is too expensive in practice. Instead, we have developed a learning-based approach to synthesize realistic, diverse, and 3D pose-preserving hand images under the guidance of 3D pose information. We propose a 3D-aware multi-modal guided hand generative network (MM-Hand), together with a novel geometry-based curriculum learning strategy. Our extensive experimental results demonstrate that the 3D-annotated images generated by MM-Hand qualitatively and quantitatively outperform existing options. Moreover, the augmented data can consistently improve the quantitative performance of the state-of-the-art 3D hand pose estimators on two benchmark datasets. The code will be available at https://github.com/ScottHoang/mm-hand.



### Machine learning approach to force reconstruction in photoelastic materials
- **Arxiv ID**: http://arxiv.org/abs/2010.01163v3
- **DOI**: None
- **Categories**: **cs.CV**, physics.app-ph, J.2; I.5.4; I.2.10; I.6.6
- **Links**: [PDF](http://arxiv.org/pdf/2010.01163v3)
- **Published**: 2020-10-02 18:50:20+00:00
- **Updated**: 2021-08-03 02:17:52+00:00
- **Authors**: Renat Sergazinov, Miroslav Kramar
- **Comment**: 20 pages, 6 figures, 2 tables; changed formatting of the tables;
  reduced picture resolutions
- **Journal**: None
- **Summary**: Photoelastic techniques have a long tradition in both qualitative and quantitative analysis of the stresses in granular materials. Over the last two decades, computational methods for reconstructing forces between particles from their photoelastic response have been developed by many different experimental teams. Unfortunately, all of these methods are computationally expensive. This limits their use for processing extensive data sets that capture the time evolution of granular ensembles consisting of a large number of particles. In this paper, we present a novel approach to this problem which leverages the power of convolutional neural networks to recognize complex spatial patterns. The main drawback of using neural networks is that training them usually requires a large labeled data set which is hard to obtain experimentally. We show that this problem can be successfully circumvented by pretraining the networks on a large synthetic data set and then fine-tuning them on much smaller experimental data sets. Due to our current lack of experimental data, we demonstrate the potential of our method by changing the size of the considered particles which alters the exhibited photoelastic patterns more than typical experimental errors.



### Deep Expectation-Maximization for Semi-Supervised Lung Cancer Screening
- **Arxiv ID**: http://arxiv.org/abs/2010.01173v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01173v1)
- **Published**: 2020-10-02 19:17:07+00:00
- **Updated**: 2020-10-02 19:17:07+00:00
- **Authors**: Sumeet Menon, David Chapman, Phuong Nguyen, Yelena Yesha, Michael Morris, Babak Saboury
- **Comment**: This paper has been accepted at the ACM SIGKDD Workshop DCCL 2019.
  https://sites.google.com/view/kdd-workshop-2019/accepted-papers
  https://drive.google.com/file/d/0B8FX-5qN3tbjM3c4SVZDYWxjbGhCekhjUV9PUC11b3dOSXRR/view
- **Journal**: None
- **Summary**: We present a semi-supervised algorithm for lung cancer screening in which a 3D Convolutional Neural Network (CNN) is trained using the Expectation-Maximization (EM) meta-algorithm. Semi-supervised learning allows a smaller labelled data-set to be combined with an unlabeled data-set in order to provide a larger and more diverse training sample. EM allows the algorithm to simultaneously calculate a maximum likelihood estimate of the CNN training coefficients along with the labels for the unlabeled training set which are defined as a latent variable space. We evaluate the model performance of the Semi-Supervised EM algorithm for CNNs through cross-domain training of the Kaggle Data Science Bowl 2017 (Kaggle17) data-set with the National Lung Screening Trial (NLST) data-set. Our results show that the Semi-Supervised EM algorithm greatly improves the classification accuracy of the cross-domain lung cancer screening, although results are lower than a fully supervised approach with the advantage of additional labelled data from the unsupervised sample. As such, we demonstrate that Semi-Supervised EM is a valuable technique to improve the accuracy of lung cancer screening models using 3D CNNs.



### Global Adaptive Filtering Layer for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2010.01177v4
- **DOI**: 10.1016/j.cviu.2022.103519
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01177v4)
- **Published**: 2020-10-02 19:43:49+00:00
- **Updated**: 2021-08-04 15:52:46+00:00
- **Authors**: Viktor Shipitsin, Iaroslav Bespalov, Dmitry V. Dylov
- **Comment**: The manuscript is under consideration at Computer Vision and Image
  Understanding. 28 pages, 25 figures (main article and supplementary
  material). V.S. and I.B contributed equally, D.V.D is Corresponding author
- **Journal**: Computer Vision and Image Understanding, V. 223, 103519, 2022
- **Summary**: We devise a universal adaptive neural layer to "learn" optimal frequency filter for each image together with the weights of the base neural network that performs some computer vision task. The proposed approach takes the source image in the spatial domain, automatically selects the best frequencies from the frequency domain, and transmits the inverse-transform image to the main neural network. Remarkably, such a simple add-on layer dramatically improves the performance of the main network regardless of its design. We observe that the light networks gain a noticeable boost in the performance metrics; whereas, the training of the heavy ones converges faster when our adaptive layer is allowed to "learn" alongside the main architecture. We validate the idea in four classical computer vision tasks: classification, segmentation, denoising, and erasing, considering popular natural and medical data benchmarks.



### Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views
- **Arxiv ID**: http://arxiv.org/abs/2010.01191v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01191v3)
- **Published**: 2020-10-02 20:44:46+00:00
- **Updated**: 2021-03-11 00:26:51+00:00
- **Authors**: Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map ("what is where?") from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length x width x feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations build by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour("Find chair") or answering questions about the space ("How many chairs did you see in the house?"). Project page: https://vincentcartillier.github.io/smnet.html.



### Background Adaptive Faster R-CNN for Semi-Supervised Convolutional Object Detection of Threats in X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2010.01202v1
- **DOI**: 10.1117/12.2558542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01202v1)
- **Published**: 2020-10-02 21:05:13+00:00
- **Updated**: 2020-10-02 21:05:13+00:00
- **Authors**: John B. Sigman, Gregory P. Spell, Kevin J Liang, Lawrence Carin
- **Comment**: None
- **Journal**: Proc. SPIE 11404, Anomaly Detection and Imaging with X-Rays (ADIX)
  V, 1140404 (26 May 2020)
- **Summary**: Recently, progress has been made in the supervised training of Convolutional Object Detectors (e.g. Faster R-CNN) for threat recognition in carry-on luggage using X-ray images. This is part of the Transportation Security Administration's (TSA's) mission to protect air travelers in the United States. While more training data with threats may reliably improve performance for this class of deep algorithm, it is expensive to stage in realistic contexts. By contrast, data from the real world can be collected quickly with minimal cost. In this paper, we present a semi-supervised approach for threat recognition which we call Background Adaptive Faster R-CNN. This approach is a training method for two-stage object detectors which uses Domain Adaptation methods from the field of deep learning. The data sources described earlier make two "domains": a hand-collected data domain of images with threats, and a real-world domain of images assumed without threats. Two domain discriminators, one for discriminating object proposals and one for image features, are adversarially trained to prevent encoding domain-specific information. Without this penalty a Convolutional Neural Network (CNN) can learn to identify domains based on superficial characteristics, and minimize a supervised loss function without improving its ability to recognize objects. For the hand-collected data, only object proposals and image features from backgrounds are used. The losses for these domain-adaptive discriminators are added to the Faster R-CNN losses of images from both domains. This can reduce threat detection false alarm rates by matching the statistics of extracted features from hand-collected backgrounds to real world data. Performance improvements are demonstrated on two independently-collected datasets of labeled threats.



### Leveraging Tacit Information Embedded in CNN Layers for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.01204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01204v1)
- **Published**: 2020-10-02 21:16:26+00:00
- **Updated**: 2020-10-02 21:16:26+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba
- **Comment**: ACCV 2020 Camera Ready Extended Version
- **Journal**: None
- **Summary**: Different layers in CNNs provide not only different levels of abstraction for describing the objects in the input but also encode various implicit information about them. The activation patterns of different features contain valuable information about the stream of incoming images: spatial relations, temporal patterns, and co-occurrence of spatial and spatiotemporal (ST) features. The studies in visual tracking literature, so far, utilized only one of the CNN layers, a pre-fixed combination of them, or an ensemble of trackers built upon individual layers. In this study, we employ an adaptive combination of several CNN layers in a single DCF tracker to address variations of the target appearances and propose the use of style statistics on both spatial and temporal properties of the target, directly extracted from CNN layers for visual tracking. Experiments demonstrate that using the additional implicit data of CNNs significantly improves the performance of the tracker. Results demonstrate the effectiveness of using style similarity and activation consistency regularization in improving its localization and scale accuracy.



### Artificial Intelligence Enabled Traffic Monitoring System
- **Arxiv ID**: http://arxiv.org/abs/2010.01217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01217v1)
- **Published**: 2020-10-02 22:28:02+00:00
- **Updated**: 2020-10-02 22:28:02+00:00
- **Authors**: Vishal Mandal, Abdul Rashid Mussah, Peng Jin, Yaw Adu-Gyamfi
- **Comment**: None
- **Journal**: None
- **Summary**: Manual traffic surveillance can be a daunting task as Traffic Management Centers operate a myriad of cameras installed over a network. Injecting some level of automation could help lighten the workload of human operators performing manual surveillance and facilitate making proactive decisions which would reduce the impact of incidents and recurring congestion on roadways. This article presents a novel approach to automatically monitor real time traffic footage using deep convolutional neural networks and a stand-alone graphical user interface. The authors describe the results of research received in the process of developing models that serve as an integrated framework for an artificial intelligence enabled traffic monitoring system. The proposed system deploys several state-of-the-art deep learning algorithms to automate different traffic monitoring needs. Taking advantage of a large database of annotated video surveillance data, deep learning-based models are trained to detect queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level segmentation approach is applied to detect traffic queues and predict severity. Real-time object detection algorithms coupled with different tracking systems are deployed to automatically detect stranded vehicles as well as perform vehicular counts. At each stages of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system. Overall, the results demonstrate that the proposed framework performs satisfactorily under varied conditions without being immensely impacted by environmental hazards such as blurry camera views, low illumination, rain, or snow.



### Hierarchical Domain-Adapted Feature Learning for Video Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.01220v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01220v4)
- **Published**: 2020-10-02 23:00:00+00:00
- **Updated**: 2021-05-06 08:09:36+00:00
- **Authors**: Giovanni Bellitto, Federica Proietto Salanitri, Simone Palazzo, Francesco Rundo, Daniela Giordano, Concetto Spampinato
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a 3D fully convolutional architecture for video saliency prediction that employs hierarchical supervision on intermediate maps (referred to as conspicuity maps) generated using features extracted at different abstraction levels. We provide the base hierarchical learning mechanism with two techniques for domain adaptation and domain-specific learning. For the former, we encourage the model to unsupervisedly learn hierarchical general features using gradient reversal at multiple scales, to enhance generalization capabilities on datasets for which no annotations are provided during training. As for domain specialization, we employ domain-specific operations (namely, priors, smoothing and batch normalization) by specializing the learned features on individual datasets in order to maximize performance. The results of our experiments show that the proposed model yields state-of-the-art accuracy on supervised saliency prediction. When the base hierarchical model is empowered with domain-specific modules, performance improves, outperforming state-of-the-art models on three out of five metrics on the DHF1K benchmark and reaching the second-best results on the other two. When, instead, we test it in an unsupervised domain adaptation setting, by enabling hierarchical gradient reversal layers, we obtain performance comparable to supervised state-of-the-art.



### Stuttering Speech Disfluency Prediction using Explainable Attribution Vectors of Facial Muscle Movements
- **Arxiv ID**: http://arxiv.org/abs/2010.01231v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2010.01231v1)
- **Published**: 2020-10-02 23:45:41+00:00
- **Updated**: 2020-10-02 23:45:41+00:00
- **Authors**: Arun Das, Jeffrey Mock, Henry Chacon, Farzan Irani, Edward Golob, Peyman Najafirad
- **Comment**: Submitting to IEEE Trans. 10 pages, 7 figures. Final Manuscript
- **Journal**: None
- **Summary**: Speech disorders such as stuttering disrupt the normal fluency of speech by involuntary repetitions, prolongations and blocking of sounds and syllables. In addition to these disruptions to speech fluency, most adults who stutter (AWS) also experience numerous observable secondary behaviors before, during, and after a stuttering moment, often involving the facial muscles. Recent studies have explored automatic detection of stuttering using Artificial Intelligence (AI) based algorithm from respiratory rate, audio, etc. during speech utterance. However, most methods require controlled environments and/or invasive wearable sensors, and are unable explain why a decision (fluent vs stuttered) was made. We hypothesize that pre-speech facial activity in AWS, which can be captured non-invasively, contains enough information to accurately classify the upcoming utterance as either fluent or stuttered. Towards this end, this paper proposes a novel explainable AI (XAI) assisted convolutional neural network (CNN) classifier to predict near future stuttering by learning temporal facial muscle movement patterns of AWS and explains the important facial muscles and actions involved. Statistical analyses reveal significantly high prevalence of cheek muscles (p<0.005) and lip muscles (p<0.005) to predict stuttering and shows a behavior conducive of arousal and anticipation to speak. The temporal study of these upper and lower facial muscles may facilitate early detection of stuttering, promote automated assessment of stuttering and have application in behavioral therapies by providing automatic non-invasive feedback in realtime.



