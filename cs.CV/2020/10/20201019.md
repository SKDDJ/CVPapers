# Arxiv Papers in cs.CV on 2020-10-19
### Evidential Sparsification of Multimodal Latent Spaces in Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2010.09164v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, I.2.10; I.2.9; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2010.09164v3)
- **Published**: 2020-10-19 01:27:21+00:00
- **Updated**: 2021-01-18 18:34:32+00:00
- **Authors**: Masha Itkina, Boris Ivanovic, Ransalu Senanayake, Mykel J. Kochenderfer, Marco Pavone
- **Comment**: 21 pages, 15 figures, 34th Conference on Neural Information
  Processing Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: Discrete latent spaces in variational autoencoders have been shown to effectively capture the data distribution for many real-world problems such as natural language understanding, human intent prediction, and visual scene representation. However, discrete latent spaces need to be sufficiently large to capture the complexities of real-world data, rendering downstream tasks computationally challenging. For instance, performing motion planning in a high-dimensional latent representation of the environment could be intractable. We consider the problem of sparsifying the discrete latent space of a trained conditional variational autoencoder, while preserving its learned multimodality. As a post hoc latent space reduction technique, we use evidential theory to identify the latent classes that receive direct evidence from a particular input condition and filter out those that do not. Experiments on diverse tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique at reducing the discrete latent sample space size of a model while maintaining its learned multimodality.



### Gaussian Constrained Attention Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.09169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09169v1)
- **Published**: 2020-10-19 01:55:30+00:00
- **Updated**: 2020-10-19 01:55:30+00:00
- **Authors**: Zhi Qiao, Xugong Qin, Yu Zhou, Fei Yang, Weiping Wang
- **Comment**: ICPR2020
- **Journal**: None
- **Summary**: Scene text recognition has been a hot topic in computer vision. Recent methods adopt the attention mechanism for sequence prediction which achieve convincing results. However, we argue that the existing attention mechanism faces the problem of attention diffusion, in which the model may not focus on a certain character area. In this paper, we propose Gaussian Constrained Attention Network to deal with this problem. It is a 2D attention-based method integrated with a novel Gaussian Constrained Refinement Module, which predicts an additional Gaussian mask to refine the attention weights. Different from adopting an additional supervision on the attention weights simply, our proposed method introduces an explicit refinement. In this way, the attention weights will be more concentrated and the attention-based recognition network achieves better performance. The proposed Gaussian Constrained Refinement Module is flexible and can be applied to existing attention-based methods directly. The experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. Our code has been available at https://github.com/Pay20Y/GCAN.



### MaskNet: A Fully-Convolutional Network to Estimate Inlier Points
- **Arxiv ID**: http://arxiv.org/abs/2010.09185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.09185v1)
- **Published**: 2020-10-19 03:18:35+00:00
- **Updated**: 2020-10-19 03:18:35+00:00
- **Authors**: Vinit Sarode, Animesh Dhagat, Rangaprasad Arun Srivatsan, Nicolas Zevallos, Simon Lucey, Howie Choset
- **Comment**: Accepted at International Conference on 3D Vision (3DV, 2020)
- **Journal**: None
- **Summary**: Point clouds have grown in importance in the way computers perceive the world. From LIDAR sensors in autonomous cars and drones to the time of flight and stereo vision systems in our phones, point clouds are everywhere. Despite their ubiquity, point clouds in the real world are often missing points because of sensor limitations or occlusions, or contain extraneous points from sensor noise or artifacts. These problems challenge algorithms that require computing correspondences between a pair of point clouds. Therefore, this paper presents a fully-convolutional neural network that identifies which points in one point cloud are most similar (inliers) to the points in another. We show improvements in learning-based and classical point cloud registration approaches when retrofitted with our network. We demonstrate these improvements on synthetic and real-world datasets. Finally, our network produces impressive results on test datasets that were unseen during training, thus exhibiting generalizability. Code and videos are available at https://github.com/vinits5/masknet



### Rotation Invariant Aerial Image Retrieval with Group Convolutional Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09202v1)
- **Published**: 2020-10-19 04:12:36+00:00
- **Updated**: 2020-10-19 04:12:36+00:00
- **Authors**: Hyunseung Chung, Woo-Jeoung Nam, Seong-Whan Lee
- **Comment**: 8 pages, 5 figures, Accepted in ICPR 2020
- **Journal**: None
- **Summary**: Remote sensing image retrieval (RSIR) is the process of ranking database images depending on the degree of similarity compared to the query image. As the complexity of RSIR increases due to the diversity in shooting range, angle, and location of remote sensors, there is an increasing demand for methods to address these issues and improve retrieval performance. In this work, we introduce a novel method for retrieving aerial images by merging group convolution with attention mechanism and metric learning, resulting in robustness to rotational variations. For refinement and emphasis on important features, we applied channel attention in each group convolution stage. By utilizing the characteristics of group convolution and channel-wise attention, it is possible to acknowledge the equality among rotated but identically located images. The training procedure has two main steps: (i) training the network with Aerial Image Dataset (AID) for classification, (ii) fine-tuning the network with triplet-loss for retrieval with Google Earth South Korea and NWPU-RESISC45 datasets. Results show that the proposed method performance exceeds other state-of-the-art retrieval methods in both rotated and original environments. Furthermore, we utilize class activation maps (CAM) to visualize the distinct difference of main features between our method and baseline, resulting in better adaptability in rotated environments.



### Unsupervised Domain Adaptation for Spatio-Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2010.09211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09211v1)
- **Published**: 2020-10-19 04:25:10+00:00
- **Updated**: 2020-10-19 04:25:10+00:00
- **Authors**: Nakul Agarwal, Yi-Ting Chen, Behzad Dariush, Ming-Hsuan Yang
- **Comment**: Accepted in BMVC 2020
- **Journal**: None
- **Summary**: Spatio-temporal action localization is an important problem in computer vision that involves detecting where and when activities occur, and therefore requires modeling of both spatial and temporal features. This problem is typically formulated in the context of supervised learning, where the learned classifiers operate on the premise that both training and test data are sampled from the same underlying distribution. However, this assumption does not hold when there is a significant domain shift, leading to poor generalization performance on the test data. To address this, we focus on the hard and novel task of generalizing training models to test samples without access to any labels from the latter for spatio-temporal action localization by proposing an end-to-end unsupervised domain adaptation algorithm. We extend the state-of-the-art object detection framework to localize and classify actions. In order to minimize the domain shift, three domain adaptation modules at image level (temporal and spatial) and instance level (temporal) are designed and integrated. We design a new experimental setup and evaluate the proposed method and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark datasets. We show that significant performance gain can be achieved when spatial and temporal features are adapted separately, or jointly for the most effective results.



### Self-supervised Geometric Features Discovery via Interpretable Attention for Vehicle Re-Identification and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2010.09221v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09221v3)
- **Published**: 2020-10-19 04:43:56+00:00
- **Updated**: 2023-01-06 11:56:18+00:00
- **Authors**: Ming Li, Xinming Huang, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: To learn distinguishable patterns, most of recent works in vehicle re-identification (ReID) struggled to redevelop official benchmarks to provide various supervisions, which requires prohibitive human labors. In this paper, we seek to achieve the similar goal but do not involve more human efforts. To this end, we introduce a novel framework, which successfully encodes both geometric local features and global representations to distinguish vehicle instances, optimized only by the supervision from official ID labels. Specifically, given our insight that objects in ReID share similar geometric characteristics, we propose to borrow self-supervised representation learning to facilitate geometric features discovery. To condense these features, we introduce an interpretable attention module, with the core of local maxima aggregation instead of fully automatic learning, whose mechanism is completely understandable and whose response map is physically reasonable. To the best of our knowledge, we are the first that perform self-supervised learning to discover geometric features. We conduct comprehensive experiments on three most popular datasets for vehicle ReID, i.e., VeRi-776, CityFlow-ReID, and VehicleID. We report our state-of-the-art (SOTA) performances and promising visualization results. We also show the excellent scalability of our approach on other ReID related tasks, i.e., person ReID and multi-target multi-camera (MTMC) vehicle tracking.



### Intelligent Reference Curation for Visual Place Recognition via Bayesian Selective Fusion
- **Arxiv ID**: http://arxiv.org/abs/2010.09228v2
- **DOI**: 10.1109/LRA.2020.3047791
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09228v2)
- **Published**: 2020-10-19 05:17:35+00:00
- **Updated**: 2021-01-03 22:28:28+00:00
- **Authors**: Timothy L. Molloy, Tobias Fischer, Michael Milford, Girish N. Nair
- **Comment**: 8 pages, 10 figures, accepted in the IEEE Robotics and Automation
  Letters
- **Journal**: IEEE Robotics and Automation Letters 6(2):588-595, 2021
- **Summary**: A key challenge in visual place recognition (VPR) is recognizing places despite drastic visual appearance changes due to factors such as time of day, season, weather or lighting conditions. Numerous approaches based on deep-learnt image descriptors, sequence matching, domain translation, and probabilistic localization have had success in addressing this challenge, but most rely on the availability of carefully curated representative reference images of the possible places. In this paper, we propose a novel approach, dubbed Bayesian Selective Fusion, for actively selecting and fusing informative reference images to determine the best place match for a given query image. The selective element of our approach avoids the counterproductive fusion of every reference image and enables the dynamic selection of informative reference images in environments with changing visual conditions (such as indoors with flickering lights, outdoors during sunshowers or over the day-night cycle). The probabilistic element of our approach provides a means of fusing multiple reference images that accounts for their varying uncertainty via a novel training-free likelihood function for VPR. On difficult query images from two benchmark datasets, we demonstrate that our approach matches and exceeds the performance of several alternative fusion approaches along with state-of-the-art techniques that are provided with prior (unfair) knowledge of the best reference images. Our approach is well suited for long-term robot autonomy where dynamic visual environments are commonplace since it is training-free, descriptor-agnostic, and complements existing techniques such as sequence matching.



### Continual Unsupervised Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09236v2)
- **Published**: 2020-10-19 05:59:48+00:00
- **Updated**: 2021-10-11 05:58:11+00:00
- **Authors**: Joonhyuk Kim, Sahng-Min Yoo, Gyeong-Moon Park, Jong-Hwan Kim
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) for semantic segmentation has been favorably applied to real-world scenarios in which pixel-level labels are hard to be obtained. In most of the existing UDA methods, all target data are assumed to be introduced simultaneously. Yet, the data are usually presented sequentially in the real world. Moreover, Continual UDA, which deals with more practical scenarios with multiple target domains in the continual learning setting, has not been actively explored. In this light, we propose Continual UDA for semantic segmentation based on a newly designed Expanding Target-specific Memory (ETM) framework. Our novel ETM framework contains Target-specific Memory (TM) for each target domain to alleviate catastrophic forgetting. Furthermore, a proposed Double Hinge Adversarial (DHA) loss leads the network to produce better UDA performance overall. Our design of the TM and training objectives let the semantic segmentation network adapt to the current target domain while preserving the knowledge learned on previous target domains. The model with the proposed framework outperforms other state-of-the-art models in continual learning settings on standard benchmarks such as GTA5, SYNTHIA, CityScapes, IDD, and Cross-City datasets. The source code is available at https://github.com/joonh-kim/ETM.



### MCGKT-Net: Multi-level Context Gating Knowledge Transfer Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2010.09241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09241v1)
- **Published**: 2020-10-19 06:21:07+00:00
- **Updated**: 2020-10-19 06:21:07+00:00
- **Authors**: Kohei Yamamichi, Xian-Hua Han
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: Rain streak removal in a single image is a very challenging task due to its ill-posed nature in essence. Recently, the end-to-end learning techniques with deep convolutional neural networks (DCNN) have made great progress in this task. However, the conventional DCNN-based deraining methods have struggled to exploit deeper and more complex network architectures for pursuing better performance. This study proposes a novel MCGKT-Net for boosting deraining performance, which is a naturally multi-scale learning framework being capable of exploring multi-scale attributes of rain streaks and different semantic structures of the clear images. In order to obtain high representative features inside MCGKT-Net, we explore internal knowledge transfer module using ConvLSTM unit for conducting interaction learning between different layers and investigate external knowledge transfer module for leveraging the knowledge already learned in other task domains. Furthermore, to dynamically select useful features in learning procedure, we propose a multi-scale context gating module in the MCGKT-Net using squeeze-and-excitation block. Experiments on three benchmark datasets: Rain100H, Rain100L, and Rain800, manifest impressive performance compared with state-of-the-art methods.



### Extraction of Discrete Spectra Modes from Video Data Using a Deep Convolutional Koopman Network
- **Arxiv ID**: http://arxiv.org/abs/2010.09245v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DS, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2010.09245v1)
- **Published**: 2020-10-19 06:26:29+00:00
- **Updated**: 2020-10-19 06:26:29+00:00
- **Authors**: Scott Leask, Vincent McDonell
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Recent deep learning extensions in Koopman theory have enabled compact, interpretable representations of nonlinear dynamical systems which are amenable to linear analysis. Deep Koopman networks attempt to learn the Koopman eigenfunctions which capture the coordinate transformation to globally linearize system dynamics. These eigenfunctions can be linked to underlying system modes which govern the dynamical behavior of the system. While many related techniques have demonstrated their efficacy on canonical systems and their associated state variables, in this work the system dynamics are observed optically (i.e. in video format). We demonstrate the ability of a deep convolutional Koopman network (CKN) in automatically identifying independent modes for dynamical systems with discrete spectra. Practically, this affords flexibility in system data collection as the data are easily obtainable observable variables. The learned models are able to successfully and robustly identify the underlying modes governing the system, even with a redundantly large embedding space. Modal disaggregation is encouraged using a simple masking procedure. All of the systems analyzed in this work use an identical network architecture.



### DeepReflecs: Deep Learning for Automotive Object Classification with Radar Reflections
- **Arxiv ID**: http://arxiv.org/abs/2010.09273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09273v1)
- **Published**: 2020-10-19 07:35:51+00:00
- **Updated**: 2020-10-19 07:35:51+00:00
- **Authors**: Michael Ulrich, Claudius Gläser, Fabian Timm
- **Comment**: preprint, under review
- **Journal**: None
- **Summary**: This paper presents an novel object type classification method for automotive applications which uses deep learning with radar reflections. The method provides object class information such as pedestrian, cyclist, car, or non-obstacle. The method is both powerful and efficient, by using a light-weight deep learning approach on reflection level radar data. It fills the gap between low-performant methods of handcrafted features and high-performant methods with convolutional neural networks. The proposed network exploits the specific characteristics of radar reflection data: It handles unordered lists of arbitrary length as input and it combines both extraction of local and global features. In experiments with real data the proposed network outperforms existing methods of handcrafted or learned features. An ablation study analyzes the impact of the proposed global context layer.



### Modality-Pairing Learning for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09277v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09277v2)
- **Published**: 2020-10-19 07:42:10+00:00
- **Updated**: 2020-12-29 02:59:57+00:00
- **Authors**: Yixin Wang, Yao Zhang, Feng Hou, Yang Liu, Jiang Tian, Cheng Zhong, Yang Zhang, Zhiqiang He
- **Comment**: Second place of BraTS 2020 Challenge
- **Journal**: None
- **Summary**: Automatic brain tumor segmentation from multi-modality Magnetic Resonance Images (MRI) using deep learning methods plays an important role in assisting the diagnosis and treatment of brain tumor. However, previous methods mostly ignore the latent relationship among different modalities. In this work, we propose a novel end-to-end Modality-Pairing learning method for brain tumor segmentation. Paralleled branches are designed to exploit different modality features and a series of layer connections are utilized to capture complex relationships and abundant information among modalities. We also use a consistency loss to minimize the prediction variance between two branches. Besides, learning rate warmup strategy is adopted to solve the problem of the training instability and early over-fitting. Lastly, we use average ensemble of multiple models and some post-processing techniques to get final results. Our method is tested on the BraTS 2020 online testing dataset, obtaining promising segmentation performance, with average dice scores of 0.891, 0.842, 0.816 for the whole tumor, tumor core and enhancing tumor, respectively. We won the second place of the BraTS 2020 Challenge for the tumor segmentation task.



### MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2010.09278v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09278v2)
- **Published**: 2020-10-19 07:42:41+00:00
- **Updated**: 2020-10-24 01:50:11+00:00
- **Authors**: Wen Fei, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. The last BN layer provides autotuned learning rates and also improves accuracy. Experimental results show that MimicNorm achieves similar accuracy for various network structures, including ResNets and lightweight networks like ShuffleNet, with a reduction of about 20% memory consumption. The code is publicly available at https://github.com/Kid-key/MimicNorm.



### The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results
- **Arxiv ID**: http://arxiv.org/abs/2010.10298v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10298v2)
- **Published**: 2020-10-19 07:57:27+00:00
- **Updated**: 2020-10-22 03:42:29+00:00
- **Authors**: Jie Lian, Jingyu Liu, Yizhou Yu, Mengyuan Ding, Yaoci Lu, Yi Lu, Jie Cai, Deshou Lin, Miao Zhang, Zhe Wang, Kai He, Yijie Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of thoracic abnormalities challenge is organized by the Deepwise AI Lab. The challenge is divided into two rounds. In this paper, we present the results of 6 teams which reach the second round. The challenge adopts the ChestX-Det10 dateset proposed by the Deepwise AI Lab. ChestX-Det10 is the first chest X-Ray dataset with instance-level annotations, including 10 categories of disease/abnormality of 3,543 images. The annotations are located at https://github.com/Deepwise-AILab/ChestX-Det10-Dataset. In the challenge, we randomly split all data into 3001 images for training and 542 images for testing.



### Frame Aggregation and Multi-Modal Fusion Framework for Video-Based Person Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.09290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.09290v2)
- **Published**: 2020-10-19 08:06:40+00:00
- **Updated**: 2020-12-30 09:01:06+00:00
- **Authors**: Fangtao Li, Wenzhe Wang, Zihe Liu, Haoran Wang, Chenghao Yan, Bin Wu
- **Comment**: Accepted by MMM 2021
- **Journal**: None
- **Summary**: Video-based person recognition is challenging due to persons being blocked and blurred, and the variation of shooting angle. Previous research always focused on person recognition on still images, ignoring similarity and continuity between video frames. To tackle the challenges above, we propose a novel Frame Aggregation and Multi-Modal Fusion (FAMF) framework for video-based person recognition, which aggregates face features and incorporates them with multi-modal information to identify persons in videos. For frame aggregation, we propose a novel trainable layer based on NetVLAD (named AttentionVLAD), which takes arbitrary number of features as input and computes a fixed-length aggregation feature based on feature quality. We show that introducing an attention mechanism to NetVLAD can effectively decrease the impact of low-quality frames. For the multi-model information of videos, we propose a Multi-Layer Multi-Modal Attention (MLMA) module to learn the correlation of multi-modality by adaptively updating Gram matrix. Experimental results on iQIYI-VID-2019 dataset show that our framework outperforms other state-of-the-art methods.



### Meta-learning the Learning Trends Shared Across Tasks
- **Arxiv ID**: http://arxiv.org/abs/2010.09291v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09291v1)
- **Published**: 2020-10-19 08:06:47+00:00
- **Updated**: 2020-10-19 08:06:47+00:00
- **Authors**: Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah
- **Comment**: Code will be released at https://github.com/brjathu/PAMELA
- **Journal**: None
- **Summary**: Meta-learning stands for 'learning to learn' such that generalization to new tasks is achieved. Among these methods, Gradient-based meta-learning algorithms are a specific sub-class that excel at quick adaptation to new tasks with limited data. This demonstrates their ability to acquire transferable knowledge, a capability that is central to human learning. However, the existing meta-learning approaches only depend on the current task information during the adaptation, and do not share the meta-knowledge of how a similar task has been adapted before. To address this gap, we propose a 'Path-aware' model-agnostic meta-learning approach. Specifically, our approach not only learns a good initialization for adaptation, it also learns an optimal way to adapt these parameters to a set of task-specific parameters, with learnable update directions, learning rates and, most importantly, the way updates evolve over different time-steps. Compared to the existing meta-learning methods, our approach offers: (a) The ability to learn gradient-preconditioning at different time-steps of the inner-loop, thereby modeling the dynamic learning behavior shared across tasks, and (b) The capability of aggregating the learning context through the provision of direct gradient-skip connections from the old time-steps, thus avoiding overfitting and improving generalization. In essence, our approach not only learns a transferable initialization, but also models the optimal update directions, learning rates, and task-specific learning trends. Specifically, in terms of learning trends, our approach determines the way update directions shape up as the task-specific learning progresses and how the previous update history helps in the current update. Our approach is simple to implement and demonstrates faster convergence. We report significant performance improvements on a number of FSL datasets.



### FTBNN: Rethinking Non-linearity for 1-bit CNNs and Going Beyond
- **Arxiv ID**: http://arxiv.org/abs/2010.09294v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09294v4)
- **Published**: 2020-10-19 08:11:48+00:00
- **Updated**: 2020-12-30 09:48:00+00:00
- **Authors**: Zhuo Su, Linpu Fang, Deke Guo, Dewen Hu, Matti Pietikäinen, Li Liu
- **Comment**: Openreview: https://openreview.net/forum?id=9wHe4F-lpp
- **Journal**: None
- **Summary**: Binary neural networks (BNNs), where both weights and activations are binarized into 1 bit, have been widely studied in recent years due to its great benefit of highly accelerated computation and substantially reduced memory footprint that appeal to the development of resource constrained devices. In contrast to previous methods tending to reduce the quantization error for training BNN structures, we argue that the binarized convolution process owns an increasing linearity towards the target of minimizing such error, which in turn hampers BNN's discriminative ability. In this paper, we re-investigate and tune proper non-linear modules to fix that contradiction, leading to a strong baseline which achieves state-of-the-art performance on the large-scale ImageNet dataset in terms of accuracy and training efficiency. To go further, we find that the proposed BNN model still has much potential to be compressed by making a better use of the efficient binary operations, without losing accuracy. In addition, the limited capacity of the BNN model can also be increased with the help of group execution. Based on these insights, we are able to improve the baseline with an additional 4~5% top-1 accuracy gain even with less computational cost. Our code will be made public at https://github.com/zhuogege1943/ftbnn.



### Semantic Histogram Based Graph Matching for Real-Time Multi-Robot Global Localization in Large Scale Environment
- **Arxiv ID**: http://arxiv.org/abs/2010.09297v2
- **DOI**: 10.1109/LRA.2021.3058935
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09297v2)
- **Published**: 2020-10-19 08:18:42+00:00
- **Updated**: 2021-02-24 09:51:22+00:00
- **Authors**: Xiyue Guo, Junjie Hu, Junfeng Chen, Fuqin Deng, Tin Lun Lam
- **Comment**: None
- **Journal**: None
- **Summary**: The core problem of visual multi-robot simultaneous localization and mapping (MR-SLAM) is how to efficiently and accurately perform multi-robot global localization (MR-GL). The difficulties are two-fold. The first is the difficulty of global localization for significant viewpoint difference. Appearance-based localization methods tend to fail under large viewpoint changes. Recently, semantic graphs have been utilized to overcome the viewpoint variation problem. However, the methods are highly time-consuming, especially in large-scale environments. This leads to the second difficulty, which is how to perform real-time global localization. In this paper, we propose a semantic histogram-based graph matching method that is robust to viewpoint variation and can achieve real-time global localization. Based on that, we develop a system that can accurately and efficiently perform MR-GL for both homogeneous and heterogeneous robots. The experimental results show that our approach is about 30 times faster than Random Walk based semantic descriptors. Moreover, it achieves an accuracy of 95% for global localization, while the accuracy of the state-of-the-art method is 85%.



### Double-Uncertainty Weighted Method for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09298v1)
- **Published**: 2020-10-19 08:20:18+00:00
- **Updated**: 2020-10-19 08:20:18+00:00
- **Authors**: Yixin Wang, Yao Zhang, Jiang Tian, Cheng Zhong, Zhongchao Shi, Yang Zhang, Zhiqiang He
- **Comment**: accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Though deep learning has achieved advanced performance recently, it remains a challenging task in the field of medical imaging, as obtaining reliable labeled training data is time-consuming and expensive. In this paper, we propose a double-uncertainty weighted method for semi-supervised segmentation based on the teacher-student model. The teacher model provides guidance for the student model by penalizing their inconsistent prediction on both labeled and unlabeled data. We train the teacher model using Bayesian deep learning to obtain double-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It is the first to extend segmentation uncertainty estimation to feature uncertainty, which reveals the capability to capture information among channels. A learnable uncertainty consistency loss is designed for the unsupervised learning process in an interactive manner between prediction and uncertainty. With no ground-truth for supervision, it can still incentivize more accurate teacher's predictions and facilitate the model to reduce uncertain estimations. Furthermore, our proposed double-uncertainty serves as a weight on each inconsistency penalty to balance and harmonize supervised and unsupervised training processes. We validate the proposed feature uncertainty and loss function through qualitative and quantitative analyses. Experimental results show that our method outperforms the state-of-the-art uncertainty-based semi-supervised methods on two public medical datasets.



### Language and Visual Entity Relationship Graph for Agent Navigation
- **Arxiv ID**: http://arxiv.org/abs/2010.09304v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09304v2)
- **Published**: 2020-10-19 08:25:55+00:00
- **Updated**: 2020-12-25 02:43:43+00:00
- **Authors**: Yicong Hong, Cristian Rodriguez-Opazo, Yuankai Qi, Qi Wu, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) requires an agent to navigate in a real-world environment following natural language instructions. From both the textual and visual perspectives, we find that the relationships among the scene, its objects,and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52%. On the Room-for-Room (R4R) dataset, our method significantly improves the previous best from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW). Code is available at: https://github.com/YicongHong/Entity-Graph-VLN.



### A Two-stage Unsupervised Approach for Low light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2010.09316v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09316v2)
- **Published**: 2020-10-19 08:51:32+00:00
- **Updated**: 2020-10-20 01:39:00+00:00
- **Authors**: Junjie Hu, Xiyue Guo, Junfeng Chen, Guanqi Liang, Fuqin Deng, Tin lun Lam
- **Comment**: None
- **Journal**: None
- **Summary**: As vision based perception methods are usually built on the normal light assumption, there will be a serious safety issue when deploying them into low light environments. Recently, deep learning based methods have been proposed to enhance low light images by penalizing the pixel-wise loss of low light and normal light images. However, most of them suffer from the following problems: 1) the need of pairs of low light and normal light images for training, 2) the poor performance for dark images, 3) the amplification of noise. To alleviate these problems, in this paper, we propose a two-stage unsupervised method that decomposes the low light image enhancement into a pre-enhancement and a post-refinement problem. In the first stage, we pre-enhance a low light image with a conventional Retinex based method. In the second stage, we use a refinement network learned with adversarial training for further improvement of the image quality. The experimental results show that our method outperforms previous methods on four benchmark datasets. In addition, we show that our method can significantly improve feature points matching and simultaneous localization and mapping in low light conditions.



### Semantic-Guided Inpainting Network for Complex Urban Scenes Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2010.09334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09334v1)
- **Published**: 2020-10-19 09:17:17+00:00
- **Updated**: 2020-10-19 09:17:17+00:00
- **Authors**: Pierfrancesco Ardino, Yahui Liu, Elisa Ricci, Bruno Lepri, Marco De Nadai
- **Comment**: To appear in the Proceedings of IEEE ICPR 2020
- **Journal**: None
- **Summary**: Manipulating images of complex scenes to reconstruct, insert and/or remove specific object instances is a challenging task. Complex scenes contain multiple semantics and objects, which are frequently cluttered or ambiguous, thus hampering the performance of inpainting models. Conventional techniques often rely on structural information such as object contours in multi-stage approaches that generate unreliable results and boundaries. In this work, we propose a novel deep learning model to alter a complex urban scene by removing a user-specified portion of the image and coherently inserting a new object (e.g. car or pedestrian) in that scene. Inspired by recent works on image inpainting, our proposed method leverages the semantic segmentation to model the content and structure of the image, and learn the best shape and location of the object to insert. To generate reliable results, we design a new decoder block that combines the semantic segmentation and generation task to guide better the generation of new objects and scenes, which have to be semantically consistent with the image. Our experiments, conducted on two large-scale datasets of urban scenes (Cityscapes and Indian Driving), show that our proposed approach successfully address the problem of semantically-guided inpainting of complex urban scene.



### SMA-STN: Segmented Movement-Attending Spatiotemporal Network forMicro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.09342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.09342v1)
- **Published**: 2020-10-19 09:23:24+00:00
- **Updated**: 2020-10-19 09:23:24+00:00
- **Authors**: Jiateng Liu, Wenming Zheng, Yuan Zong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Correctly perceiving micro-expression is difficult since micro-expression is an involuntary, repressed, and subtle facial expression, and efficiently revealing the subtle movement changes and capturing the significant segments in a micro-expression sequence is the key to micro-expression recognition (MER). To handle the crucial issue, in this paper, we firstly propose a dynamic segmented sparse imaging module (DSSI) to compute dynamic images as local-global spatiotemporal descriptors under a unique sampling protocol, which reveals the subtle movement changes visually in an efficient way. Secondly, a segmented movement-attending spatiotemporal network (SMA-STN) is proposed to further unveil imperceptible small movement changes, which utilizes a spatiotemporal movement-attending module (STMA) to capture long-distance spatial relation for facial expression and weigh temporal segments. Besides, a deviation enhancement loss (DE-Loss) is embedded in the SMA-STN to enhance the robustness of SMA-STN to subtle movement changes in feature level. Extensive experiments on three widely used benchmarks, i.e., CASME II, SAMM, and SHIC, show that the proposed SMA-STN achieves better MER performance than other state-of-the-art methods, which proves that the proposed method is effective to handle the challenging MER problem.



### SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.09343v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09343v3)
- **Published**: 2020-10-19 09:23:39+00:00
- **Updated**: 2022-02-09 04:59:30+00:00
- **Authors**: Yan Xu, Zhaoyang Huang, Kwan-Yee Lin, Xinge Zhu, Jianping Shi, Hujun Bao, Guofeng Zhang, Hongsheng Li
- **Comment**: Accepted to CoRL 2020
- **Journal**: None
- **Summary**: Recent learning-based LiDAR odometry methods have demonstrated their competitiveness. However, most methods still face two substantial challenges: 1) the 2D projection representation of LiDAR data cannot effectively encode 3D structures from the point clouds; 2) the needs for a large amount of labeled data for training limit the application scope of these methods. In this paper, we propose a self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to tackle these two difficulties. Specifically, we propose a 3D convolution network to process the raw LiDAR data directly, which extracts features that better encode the 3D geometric patterns. To suit our network to self-supervised learning, we design several novel loss functions that utilize the inherent properties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is incorporated in the loss functions to alleviate the interference of moving objects/noises. We evaluate our method's performances on two large-scale datasets, i.e., KITTI and Apollo-SouthBay. Our method outperforms state-of-the-art unsupervised methods by 27%/32% in terms of translational/rotational errors on the KITTI dataset and also performs well on the Apollo-SouthBay dataset. By including more unlabelled training data, our method can further improve performance comparable to the supervised methods.



### The efficacy of Neural Planning Metrics: A meta-analysis of PKL on nuScenes
- **Arxiv ID**: http://arxiv.org/abs/2010.09350v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09350v3)
- **Published**: 2020-10-19 09:32:48+00:00
- **Updated**: 2021-07-13 03:53:53+00:00
- **Authors**: Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion, Sanja Fidler
- **Comment**: IROS 2020 Workshop on Benchmarking Progress in Autonomous Driving
- **Journal**: None
- **Summary**: A high-performing object detection system plays a crucial role in autonomous driving (AD). The performance, typically evaluated in terms of mean Average Precision, does not take into account orientation and distance of the actors in the scene, which are important for the safe AD. It also ignores environmental context. Recently, Philion et al. proposed a neural planning metric (PKL), based on the KL divergence of a planner's trajectory and the groundtruth route, to accommodate these requirements. In this paper, we use this neural planning metric to score all submissions of the nuScenes detection challenge and analyze the results. We find that while somewhat correlated with mAP, the PKL metric shows different behavior to increased traffic density, ego velocity, road curvature and intersections. Finally, we propose ideas to extend the neural planning metric.



### Neural Architecture Performance Prediction Using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.10024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10024v1)
- **Published**: 2020-10-19 09:33:57+00:00
- **Updated**: 2020-10-19 09:33:57+00:00
- **Authors**: Jovita Lukasik, David Friede, Heiner Stuckenschmidt, Margret Keuper
- **Comment**: camera ready version for DAGM GCPR 2020. arXiv admin note:
  substantial text overlap with arXiv:1912.05317
- **Journal**: None
- **Summary**: In computer vision research, the process of automating architecture engineering, Neural Architecture Search (NAS), has gained substantial interest. Due to the high computational costs, most recent approaches to NAS as well as the few available benchmarks only provide limited search spaces. In this paper we propose a surrogate model for neural architecture performance prediction built upon Graph Neural Networks (GNN). We demonstrate the effectiveness of this surrogate model on neural architecture performance prediction for structurally unknown architectures (i.e. zero shot prediction) by evaluating the GNN on several experiments on the NAS-Bench-101 dataset.



### SHREC 2020 track: 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.09355v1
- **DOI**: 10.2312/3dor.20201164
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09355v1)
- **Published**: 2020-10-19 09:45:42+00:00
- **Updated**: 2020-10-19 09:45:42+00:00
- **Authors**: Honglin Yuan, Remco C. Veltkamp, Georgios Albanis, Nikolaos Zioulis, Dimitrios Zarpalas, Petros Daras
- **Comment**: None
- **Journal**: Eurographics Workshop on 3D Object Retrieval (2020)
- **Summary**: 6D pose estimation is crucial for augmented reality, virtual reality, robotic manipulation and visual navigation. However, the problem is challenging due to the variety of objects in the real world. They have varying 3D shape and their appearances in captured images are affected by sensor noise, changing lighting conditions and occlusions between objects. Different pose estimation methods have different strengths and weaknesses, depending on feature representations and scene contents. At the same time, existing 3D datasets that are used for data-driven methods to estimate 6D poses have limited view angles and low resolution. To address these issues, we organize the Shape Retrieval Challenge benchmark on 6D pose estimation and create a physically accurate simulator that is able to generate photo-realistic color-and-depth image pairs with corresponding ground truth 6D poses. From captured color and depth images, we use this simulator to generate a 3D dataset which has 400 photo-realistic synthesized color-and-depth image pairs with various view angles for training, and another 100 captured and synthetic images for testing. Five research groups register in this track and two of them submitted their results. Data-driven methods are the current trend in 6D object pose estimation and our evaluation results show that approaches which fully exploit the color and geometric features are more robust for 6D pose estimation of reflective and texture-less objects and occlusion. This benchmark and comparative evaluation results have the potential to further enrich and boost the research of 6D object pose estimation and its applications.



### A combined full-reference image quality assessment approach based on convolutional activation maps
- **Arxiv ID**: http://arxiv.org/abs/2010.09361v3
- **DOI**: 10.3390/a13120313
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09361v3)
- **Published**: 2020-10-19 10:00:29+00:00
- **Updated**: 2020-12-03 05:01:40+00:00
- **Authors**: Domonkos Varga
- **Comment**: None
- **Journal**: Algorithms 2020
- **Summary**: The goal of full-reference image quality assessment (FR-IQA) is to predict the quality of an image as perceived by human observers with using its pristine, reference counterpart. In this study, we explore a novel, combined approach which predicts the perceptual quality of a distorted image by compiling a feature vector from convolutional activation maps. More specifically, a reference-distorted image pair is run through a pretrained convolutional neural network and the activation maps are compared with a traditional image similarity metric. Subsequently, the resulted feature vector is mapped onto perceptual quality scores with the help of a trained support vector regressor. A detailed parameter study is also presented in which the design choices of the proposed method is reasoned. Furthermore, we study the relationship between the amount of training images and the prediction performance. Specifically, it is demonstrated that the proposed method can be trained with few amount of data to reach high prediction performance. Our best proposal - ActMapFeat - is compared to the state-of-the-art on six publicly available benchmark IQA databases, such as KADID-10k, TID2013, TID2008, MDID, CSIQ, and VCL-FER. Specifically, our method is able to significantly outperform the state-of-the-art on these benchmark databases.



### Fisheye lens distortion correction
- **Arxiv ID**: http://arxiv.org/abs/2010.10295v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10295v1)
- **Published**: 2020-10-19 11:11:54+00:00
- **Updated**: 2020-10-19 11:11:54+00:00
- **Authors**: Dmitry Pozdnyakov
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: A new distortion correction algorithm for fisheye lens with equidistant mapping function is considered in the present study. The algorithm is much more data lossless and accurate than such a classical approach like Brown-Conrady model



### Measuring breathing induced oesophageal motion and its dosimetric impact
- **Arxiv ID**: http://arxiv.org/abs/2010.09391v3
- **DOI**: 10.1016/j.ejmp.2021.06.007
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09391v3)
- **Published**: 2020-10-19 11:25:05+00:00
- **Updated**: 2021-06-21 09:32:46+00:00
- **Authors**: Tobias Fechter, Sonja Adebahr, Anca-Ligia Grosu, Dimos Baltas
- **Comment**: The paper got accepted for publication in Physica Medica
- **Journal**: None
- **Summary**: Stereotactic body radiation therapy allows for a precise and accurate dose delivery. Organ motion during treatment bears the risk of undetected high dose healthy tissue exposure. An organ very susceptible to high dose is the oesophagus. Its low contrast on CT and the oblong shape renders motion estimation difficult. We tackle this issue by modern algorithms to measure the oesophageal motion voxel-wise and to estimate motion related dosimetric impact. Oesophageal motion was measured using deformable image registration and 4DCT of 11 internal and 5 public datasets. Current clinical practice of contouring the organ on 3DCT was compared to timely resolved 4DCT contours. The dosimetric impact of the motion was estimated by analysing the trajectory of each voxel in the 4D dose distribution. Finally an organ motion model was built, allowing for easier patient-wise comparisons. Motion analysis showed mean absolute maximal motion amplitudes of 4.55 +/- 1.81 mm left-right, 5.29 +/- 2.67 mm anterior-posterior and 10.78 +/- 5.30 mm superior-inferior. Motion between the cohorts differed significantly. In around 50 % of the cases the dosimetric passing criteria was violated. Contours created on 3DCT did not cover 14 % of the organ for 50 % of the respiratory cycle and the 3D contour is around 38 % smaller than the union of all 4D contours. The motion model revealed that the maximal motion is not limited to the lower part of the organ. Our results showed motion amplitudes higher than most reported values in the literature and that motion is very heterogeneous across patients. Therefore, individual motion information should be considered in contouring and planning.



### SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.09409v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5; I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2010.09409v1)
- **Published**: 2020-10-19 12:07:07+00:00
- **Updated**: 2020-10-19 12:07:07+00:00
- **Authors**: Juan J. Gómez Rodríguez, José Lamarca, Javier Morlana, Juan D. Tardós, José M. M. Montiel
- **Comment**: 10 pages, 8 figures. Submitted to RA-L with option to ICRA 2021.
  Associated video: https://youtu.be/gkcC0IR3X6A
- **Journal**: None
- **Summary**: Conventional SLAM techniques strongly rely on scene rigidity to solve data association, ignoring dynamic parts of the scene. In this work we present Semi-Direct DefSLAM (SD-DefSLAM), a novel monocular deformable SLAM method able to map highly deforming environments, built on top of DefSLAM. To robustly solve data association in challenging deforming scenes, SD-DefSLAM combines direct and indirect methods: an enhanced illumination-invariant Lucas-Kanade tracker for data association, geometric Bundle Adjustment for pose and deformable map estimation, and bag-of-words based on feature descriptors for camera relocation. Dynamic objects are detected and segmented-out using a CNN trained for the specific application domain. We thoroughly evaluate our system in two public datasets. The mandala dataset is a SLAM benchmark with increasingly aggressive deformations. The Hamlyn dataset contains intracorporeal sequences that pose serious real-life challenges beyond deformation like weak texture, specular reflections, surgical tools and occlusions. Our results show that SD-DefSLAM outperforms DefSLAM in point tracking, reconstruction accuracy and scale drift thanks to the improvement in all the data association steps, being the first system able to robustly perform SLAM inside the human body.



### An Investigation of Feature Selection and Transfer Learning for Writer-Independent Offline Handwritten Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/2010.10025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10025v1)
- **Published**: 2020-10-19 12:18:51+00:00
- **Updated**: 2020-10-19 12:18:51+00:00
- **Authors**: Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz, Robert Sabourin
- **Comment**: arXiv admin note: text overlap with arXiv:2004.03373
- **Journal**: None
- **Summary**: SigNet is a state of the art model for feature representation used for handwritten signature verification (HSV). This representation is based on a Deep Convolutional Neural Network (DCNN) and contains 2048 dimensions. When transposed to a dissimilarity space generated by the dichotomy transformation (DT), related to the writer-independent (WI) approach, these features may include redundant information. This paper investigates the presence of overfitting when using Binary Particle Swarm Optimization (BPSO) to perform the feature selection in a wrapper mode. We proposed a method based on a global validation strategy with an external archive to control overfitting during the search for the most discriminant representation. Moreover, an investigation is also carried out to evaluate the use of the selected features in a transfer learning context. The analysis is carried out on a writer-independent approach on the CEDAR, MCYT and GPDS datasets. The experimental results showed the presence of overfitting when no validation is used during the optimization process and the improvement when the global validation strategy with an external archive is used. Also, the space generated after feature selection can be used in a transfer learning context.



### Image Captioning with Visual Object Representations Grounded in the Textual Modality
- **Arxiv ID**: http://arxiv.org/abs/2010.09413v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.09413v2)
- **Published**: 2020-10-19 12:21:38+00:00
- **Updated**: 2020-10-20 12:24:39+00:00
- **Authors**: Dušan Variš, Katsuhito Sudoh, Satoshi Nakamura
- **Comment**: None
- **Journal**: None
- **Summary**: We present our work in progress exploring the possibilities of a shared embedding space between textual and visual modality. Leveraging the textual nature of object detection labels and the hypothetical expressiveness of extracted visual object representations, we propose an approach opposite to the current trend, grounding of the representations in the word embedding space of the captioning system instead of grounding words or sentences in their associated images. Based on the previous work, we apply additional grounding losses to the image captioning training objective aiming to force visual object representations to create more heterogeneous clusters based on their class label and copy a semantic structure of the word embedding space. In addition, we provide an analysis of the learned object vector space projection and its impact on the IC system performance. With only slight change in performance, grounded models reach the stopping criterion during training faster than the unconstrained model, needing about two to three times less training updates. Additionally, an improvement in structural correlation between the word embeddings and both original and projected object vectors suggests that the grounding is actually mutual.



### Comprehensive evaluation of no-reference image quality assessment algorithms on KADID-10k database
- **Arxiv ID**: http://arxiv.org/abs/2010.09414v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09414v2)
- **Published**: 2020-10-19 12:23:06+00:00
- **Updated**: 2020-11-09 10:07:54+00:00
- **Authors**: Domonkos Varga
- **Comment**: None
- **Journal**: None
- **Summary**: The main goal of objective image quality assessment is to devise computational, mathematical models which are able to predict perceptual image quality consistently with subjective evaluations. The evaluation of objective image quality assessment algorithms is based on experiments conducted on publicly available benchmark databases. In this study, our goal is to give a comprehensive evaluation about no-reference image quality assessment algorithms, whose original source codes are available online, using the recently published KADID-10k database which is one of the largest available benchmark databases. Specifically, average PLCC, SROCC, and KROCC are reported which were measured over 100 random train-test splits. Furthermore, the database was divided into a train (appx. 80\% of images) and a test set (appx. 20% of images) with respect to the reference images. So no semantic content overlap was between these two sets. Our evaluation results may be helpful to obtain a clear understanding about the status of state-of-the-art no-reference image quality assessment methods.



### Synthesizing the Unseen for Zero-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.09425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09425v1)
- **Published**: 2020-10-19 12:36:11+00:00
- **Updated**: 2020-10-19 12:36:11+00:00
- **Authors**: Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, Fahad Shahbaz Khan
- **Comment**: Accepted for publication at ACCV 2020
- **Journal**: None
- **Summary**: The existing zero-shot detection approaches project visual features to the semantic domain for seen objects, hoping to map unseen objects to their corresponding semantics during inference. However, since the unseen objects are never visualized during training, the detection model is skewed towards seen content, thereby labeling unseen as background or a seen class. In this work, we propose to synthesize visual features for unseen classes, so that the model learns both seen and unseen objects in the visual domain. Consequently, the major challenge becomes, how to accurately synthesize unseen objects merely using their class semantics? Towards this ambitious goal, we propose a novel generative model that uses class-semantics to not only generate the features but also to discriminatively separate them. Further, using a unified model, we ensure the synthesized features have high diversity that represents the intra-class differences and variable localization precision in the detected bounding boxes. We test our approach on three object detection benchmarks, PASCAL VOC, MSCOCO, and ILSVRC detection, under both conventional and generalized settings, showing impressive gains over the state-of-the-art methods. Our codes are available at https://github.com/nasir6/zero_shot_detection.



### GASNet: Weakly-supervised Framework for COVID-19 Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09456v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09456v1)
- **Published**: 2020-10-19 13:06:23+00:00
- **Updated**: 2020-10-19 13:06:23+00:00
- **Authors**: Zhanwei Xu, Yukun Cao, Cheng Jin, Guozhu Shao, Xiaoqing Liu, Jie Zhou, Heshui Shi, Jianjiang Feng
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Segmentation of infected areas in chest CT volumes is of great significance for further diagnosis and treatment of COVID-19 patients. Due to the complex shapes and varied appearances of lesions, a large number of voxel-level labeled samples are generally required to train a lesion segmentation network, which is a main bottleneck for developing deep learning based medical image segmentation algorithms. In this paper, we propose a weakly-supervised lesion segmentation framework by embedding the Generative Adversarial training process into the Segmentation Network, which is called GASNet. GASNet is optimized to segment the lesion areas of a COVID-19 CT by the segmenter, and to replace the abnormal appearance with a generated normal appearance by the generator, so that the restored CT volumes are indistinguishable from healthy CT volumes by the discriminator. GASNet is supervised by chest CT volumes of many healthy and COVID-19 subjects without voxel-level annotations. Experiments on three public databases show that when using as few as one voxel-level labeled sample, the performance of GASNet is comparable to fully-supervised segmentation algorithms trained on dozens of voxel-level labeled samples.



### Noisy-LSTM: Improving Temporal Awareness for Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09466v1)
- **Published**: 2020-10-19 13:08:15+00:00
- **Updated**: 2020-10-19 13:08:15+00:00
- **Authors**: Bowen Wang, Liangzhi Li, Yuta Nakashima, Ryo Kawasaki, Hajime Nagahara, Yasushi Yagi
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic video segmentation is a key challenge for various applications. This paper presents a new model named Noisy-LSTM, which is trainable in an end-to-end manner, with convolutional LSTMs (ConvLSTMs) to leverage the temporal coherency in video frames. We also present a simple yet effective training strategy, which replaces a frame in video sequence with noises. This strategy spoils the temporal coherency in video frames during training and thus makes the temporal links in ConvLSTMs unreliable, which may consequently improve feature extraction from video frames, as well as serve as a regularizer to avoid overfitting, without requiring extra data annotation or computational costs. Experimental results demonstrate that the proposed model can achieve state-of-the-art performances in both the CityScapes and EndoVis2018 datasets.



### Softer Pruning, Incremental Regularization
- **Arxiv ID**: http://arxiv.org/abs/2010.09498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.09498v1)
- **Published**: 2020-10-19 13:37:19+00:00
- **Updated**: 2020-10-19 13:37:19+00:00
- **Authors**: Linhang Cai, Zhulin An, Chuanguang Yang, Yongjun Xu
- **Comment**: 7 pages, ICPR2020
- **Journal**: None
- **Summary**: Network pruning is widely used to compress Deep Neural Networks (DNNs). The Soft Filter Pruning (SFP) method zeroizes the pruned filters during training while updating them in the next training epoch. Thus the trained information of the pruned filters is completely dropped. To utilize the trained pruned filters, we proposed a SofteR Filter Pruning (SRFP) method and its variant, Asymptotic SofteR Filter Pruning (ASRFP), simply decaying the pruned weights with a monotonic decreasing parameter. Our methods perform well across various networks, datasets and pruning rates, also transferable to weight pruning. On ILSVRC-2012, ASRFP prunes 40% of the parameters on ResNet-34 with 1.63% top-1 and 0.68% top-5 accuracy improvement. In theory, SRFP and ASRFP are an incremental regularization of the pruned filters. Besides, We note that SRFP and ASRFP pursue better results while slowing down the speed of convergence.



### A Backbone Replaceable Fine-tuning Framework for Stable Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/2010.09501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09501v2)
- **Published**: 2020-10-19 13:40:39+00:00
- **Updated**: 2020-11-13 02:33:01+00:00
- **Authors**: Xu Sun, Zhenfeng Fan, Zihao Zhang, Yingjie Guo, Shihong Xia
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Heatmap regression based face alignment has achieved prominent performance on static images. However, the stability and accuracy are remarkably discounted when applying the existing methods on dynamic videos. We attribute the degradation to random noise and motion blur, which are common in videos. The temporal information is critical to address this issue yet not fully considered in the existing works. In this paper, we visit the video-oriented face alignment problem in two perspectives: detection accuracy prefers lower error for a single frame, and detection consistency forces better stability between adjacent frames. On this basis, we propose a Jitter loss function that leverages temporal information to suppress inaccurate as well as jittered landmarks. The Jitter loss is involved in a novel framework with a fine-tuning ConvLSTM structure over a backbone replaceable network. We further demonstrate that accurate and stable landmarks are associated with different regions with overlaps in a canonical coordinate, based on which the proposed Jitter loss facilitates the optimization process during training. The proposed framework achieves at least 40% improvement on stability evaluation metrics while enhancing detection accuracy versus state-of-the-art methods. Generally, it can swiftly convert a landmark detector for facial images to a better-performing one for videos without retraining the entire model.



### Multimodal Research in Vision and Language: A Review of Current and Emerging Trends
- **Arxiv ID**: http://arxiv.org/abs/2010.09522v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.09522v2)
- **Published**: 2020-10-19 13:55:10+00:00
- **Updated**: 2020-12-22 04:43:20+00:00
- **Authors**: Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya Poria, Roger Zimmermann, Amir Zadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning and its applications have cascaded impactful research and development with a diverse range of modalities present in the real-world data. More recently, this has enhanced research interests in the intersection of the Vision and Language arena with its numerous applications and fast-paced growth. In this paper, we present a detailed overview of the latest trends in research pertaining to visual and language modalities. We look at its applications in their task formulations and how to solve various problems related to semantic perception and content generation. We also address task-specific trends, along with their evaluation strategies and upcoming challenges. Moreover, we shed some light on multi-disciplinary patterns and insights that have emerged in the recent past, directing this field towards more modular and transparent intelligent systems. This survey identifies key trends gravitating recent literature in VisLang research and attempts to unearth directions that the field is heading towards.



### Deep Multi-path Network Integrating Incomplete Biomarker and Chest CT Data for Evaluating Lung Cancer Risk
- **Arxiv ID**: http://arxiv.org/abs/2010.09524v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09524v2)
- **Published**: 2020-10-19 13:55:40+00:00
- **Updated**: 2021-02-10 03:17:15+00:00
- **Authors**: Riqiang Gao, Yucheng Tang, Kaiwen Xu, Michael N. Kammer, Sanja L. Antic, Steve Deppen, Kim L. Sandler, Pierre P. Massion, Yuankai Huo, Bennett A. Landman
- **Comment**: RFW all-conference best paper finalist, SPIE2021 Medical Imaging
- **Journal**: None
- **Summary**: Clinical data elements (CDEs) (e.g., age, smoking history), blood markers and chest computed tomography (CT) structural features have been regarded as effective means for assessing lung cancer risk. These independent variables can provide complementary information and we hypothesize that combining them will improve the prediction accuracy. In practice, not all patients have all these variables available. In this paper, we propose a new network design, termed as multi-path multi-modal missing network (M3Net), to integrate the multi-modal data (i.e., CDEs, biomarker and CT image) considering missing modality with multiple paths neural network. Each path learns discriminative features of one modality, and different modalities are fused in a second stage for an integrated prediction. The network can be trained end-to-end with both medical image features and CDEs/biomarkers, or make a prediction with single modality. We evaluate M3Net with datasets including three sites from the Consortium for Molecular and Cellular Characterization of Screen-Detected Lesions (MCL) project. Our method is cross validated within a cohort of 1291 subjects (383 subjects with complete CDEs/biomarkers and CT images), and externally validated with a cohort of 99 subjects (99 with complete CDEs/biomarkers and CT images). Both cross-validation and external-validation results show that combining multiple modality significantly improves the predicting performance of single modality. The results suggest that integrating subjects with missing either CDEs/biomarker or CT imaging features can contribute to the discriminatory power of our model (p < 0.05, bootstrap two-tailed test). In summary, the proposed M3Net framework provides an effective way to integrate image and non-image data in the context of missing information.



### Weakly-supervised Learning For Catheter Segmentation in 3D Frustum Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2010.09525v1
- **DOI**: 10.1016/j.compmedimag.2022.102037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09525v1)
- **Published**: 2020-10-19 13:56:22+00:00
- **Updated**: 2020-10-19 13:56:22+00:00
- **Authors**: Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and efficient catheter segmentation in 3D ultrasound (US) is essential for cardiac intervention. Currently, the state-of-the-art segmentation algorithms are based on convolutional neural networks (CNNs), which achieved remarkable performances in a standard Cartesian volumetric data. Nevertheless, these approaches suffer the challenges of low efficiency and GPU unfriendly image size. Therefore, such difficulties and expensive hardware requirements become a bottleneck to build accurate and efficient segmentation models for real clinical application. In this paper, we propose a novel Frustum ultrasound based catheter segmentation method. Specifically, Frustum ultrasound is a polar coordinate based image, which includes same information of standard Cartesian image but has much smaller size, which overcomes the bottleneck of efficiency than conventional Cartesian images. Nevertheless, the irregular and deformed Frustum images lead to more efforts for accurate voxel-level annotation. To address this limitation, a weakly supervised learning framework is proposed, which only needs 3D bounding box annotations overlaying the region-of-interest to training the CNNs. Although the bounding box annotation includes noise and inaccurate annotation to mislead to model, it is addressed by the proposed pseudo label generated scheme. The labels of training voxels are generated by incorporating class activation maps with line filtering, which is iteratively updated during the training. Our experimental results show the proposed method achieved the state-of-the-art performance with an efficiency of 0.25 second per volume. More crucially, the Frustum image segmentation provides a much faster and cheaper solution for segmentation in 3D US image, which meet the demands of clinical applications.



### RONELD: Robust Neural Network Output Enhancement for Active Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.09548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09548v2)
- **Published**: 2020-10-19 14:22:47+00:00
- **Updated**: 2020-11-03 02:16:21+00:00
- **Authors**: Zhe Ming Chng, Joseph Mun Hung Lew, Jimmy Addison Lee
- **Comment**: Fixed typos; Accepted at ICPR 2020, 8 pages, 6 figures, code to be
  published at http://github.com/czming/RONELD-Lane-Detection
- **Journal**: None
- **Summary**: Accurate lane detection is critical for navigation in autonomous vehicles, particularly the active lane which demarcates the single road space that the vehicle is currently traveling on. Recent state-of-the-art lane detection algorithms utilize convolutional neural networks (CNNs) to train deep learning models on popular benchmarks such as TuSimple and CULane. While each of these models works particularly well on train and test inputs obtained from the same dataset, the performance drops significantly on unseen datasets of different environments. In this paper, we present a real-time robust neural network output enhancement for active lane detection (RONELD) method to identify, track, and optimize active lanes from deep learning probability map outputs. We first adaptively extract lane points from the probability map outputs, followed by detecting curved and straight lanes before using weighted least squares linear regression on straight lanes to fix broken lane edges resulting from fragmentation of edge maps in real images. Lastly, we hypothesize true active lanes through tracking preceding frames. Experimental results demonstrate an up to two-fold increase in accuracy using RONELD on cross-dataset validation tests.



### A Versatile Crack Inspection Portable System based on Classifier Ensemble and Controlled Illumination
- **Arxiv ID**: http://arxiv.org/abs/2010.09557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09557v1)
- **Published**: 2020-10-19 14:39:03+00:00
- **Updated**: 2020-10-19 14:39:03+00:00
- **Authors**: Milind G. Padalkar, Carlos Beltrán-González, Matteo Bustreo, Alessio Del Bue, Vittorio Murino
- **Comment**: Accepted in ICPR 2020
- **Journal**: None
- **Summary**: This paper presents a novel setup for automatic visual inspection of cracks in ceramic tile as well as studies the effect of various classifiers and height-varying illumination conditions for this task. The intuition behind this setup is that cracks can be better visualized under specific lighting conditions than others. Our setup, which is designed for field work with constraints in its maximum dimensions, can acquire images for crack detection with multiple lighting conditions using the illumination sources placed at multiple heights. Crack detection is then performed by classifying patches extracted from the acquired images in a sliding window fashion. We study the effect of lights placed at various heights by training classifiers both on customized as well as state-of-the-art architectures and evaluate their performance both at patch-level and image-level, demonstrating the effectiveness of our setup. More importantly, ours is the first study that demonstrates how height-varying illumination conditions can affect crack detection with the use of existing state-of-the-art classifiers. We provide an insight about the illumination conditions that can help in improving crack detection in a challenging real-world industrial environment.



### Domain Generalized Person Re-Identification via Cross-Domain Episodic Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09561v1)
- **Published**: 2020-10-19 14:42:29+00:00
- **Updated**: 2020-10-19 14:42:29+00:00
- **Authors**: Ci-Siang Lin, Yuan-Chia Cheng, Yu-Chiang Frank Wang
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Aiming at recognizing images of the same person across distinct camera views, person re-identification (re-ID) has been among active research topics in computer vision. Most existing re-ID works require collection of a large amount of labeled image data from the scenes of interest. When the data to be recognized are different from the source-domain training ones, a number of domain adaptation approaches have been proposed. Nevertheless, one still needs to collect labeled or unlabelled target-domain data during training. In this paper, we tackle an even more challenging and practical setting, domain generalized (DG) person re-ID. That is, while a number of labeled source-domain datasets are available, we do not have access to any target-domain training data. In order to learn domain-invariant features without knowing the target domain of interest, we present an episodic learning scheme which advances meta learning strategies to exploit the observed source-domain labeled data. The learned features would exhibit sufficient domain-invariant properties while not overfitting the source-domain data or ID labels. Our experiments on four benchmark datasets confirm the superiority of our method over the state-of-the-arts.



### On the Generalisation Capabilities of Fingerprint Presentation Attack Detection Methods in the Short Wave Infrared Domain
- **Arxiv ID**: http://arxiv.org/abs/2010.09566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09566v2)
- **Published**: 2020-10-19 14:50:24+00:00
- **Updated**: 2021-01-11 16:45:36+00:00
- **Authors**: Jascha Kolberg, Marta Gomez-Barrero, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, fingerprint-based biometric recognition systems are becoming increasingly popular. However, in spite of their numerous advantages, biometric capture devices are usually exposed to the public and thus vulnerable to presentation attacks (PAs). Therefore, presentation attack detection (PAD) methods are of utmost importance in order to distinguish between bona fide and attack presentations. Due to the nearly unlimited possibilities to create new presentation attack instruments (PAIs), unknown attacks are a threat to existing PAD algorithms. This fact motivates research on generalisation capabilities in order to find PAD methods that are resilient to new attacks. In this context, we evaluate the generalisability of multiple PAD algorithms on a dataset of 19,711 bona fide and 4,339 PA samples, including 45 different PAI species. The PAD data is captured in the short wave infrared domain and the results discuss the advantages and drawbacks of this PAD technique regarding unknown attacks.



### Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions
- **Arxiv ID**: http://arxiv.org/abs/2010.09567v5
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09567v5)
- **Published**: 2020-10-19 14:51:28+00:00
- **Updated**: 2023-04-25 03:33:49+00:00
- **Authors**: Vladimir Kolmogorov
- **Comment**: to appear at CVPR 2023
- **Journal**: None
- **Summary**: We consider the problem of solving LP relaxations of MAP-MRF inference problems, and in particular the method proposed recently in (Swoboda, Kolmogorov 2019; Kolmogorov, Pock 2021). As a key computational subroutine, it uses a variant of the Frank-Wolfe (FW) method to minimize a smooth convex function over a combinatorial polytope. We propose an efficient implementation of this subproutine based on in-face Frank-Wolfe directions, introduced in (Freund et al. 2017) in a different context. More generally, we define an abstract data structure for a combinatorial subproblem that enables in-face FW directions, and describe its specialization for tree-structured MAP-MRF inference subproblems. Experimental results indicate that the resulting method is the current state-of-art LP solver for some classes of problems. Our code is available at https://pub.ist.ac.at/~vnk/papers/IN-FACE-FW.html.



### Teacher-Student Competition for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.09572v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09572v2)
- **Published**: 2020-10-19 14:58:29+00:00
- **Updated**: 2020-10-20 03:37:22+00:00
- **Authors**: Ruixin Xiao, Zhilei Liu, Baoyuan Wu
- **Comment**: Accepted by ICPR 2020
- **Journal**: None
- **Summary**: With the supervision from source domain only in class-level, existing unsupervised domain adaptation (UDA) methods mainly learn the domain-invariant representations from a shared feature extractor, which causes the source-bias problem. This paper proposes an unsupervised domain adaptation approach with Teacher-Student Competition (TSC). In particular, a student network is introduced to learn the target-specific feature space, and we design a novel competition mechanism to select more credible pseudo-labels for the training of student network. We introduce a teacher network with the structure of existing conventional UDA method, and both teacher and student networks compete to provide target pseudo-labels to constrain every target sample's training in student network. Extensive experiments demonstrate that our proposed TSC framework significantly outperforms the state-of-the-art domain adaptation methods on Office-31 and ImageCLEF-DA benchmarks.



### Learning to Reconstruct and Segment 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2010.09582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09582v1)
- **Published**: 2020-10-19 15:09:04+00:00
- **Updated**: 2020-10-19 15:09:04+00:00
- **Authors**: Bo Yang
- **Comment**: DPhil (PhD) Thesis 2020, University of Oxford
  https://ora.ox.ac.uk/objects/uuid:5f9cd30d-0ee7-412d-ba49-44f5fd76bf28
- **Journal**: None
- **Summary**: To endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approaches usually leverage hand-crafted features to estimate the shape and semantics of objects or scenes. However, they are difficult to generalize to novel objects and scenarios, and struggle to overcome critical issues caused by visual occlusions. By contrast, we aim to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. To achieve these aims, this thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding.



### Brain Atlas Guided Attention U-Net for White Matter Hyperintensity Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09586v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09586v2)
- **Published**: 2020-10-19 15:10:50+00:00
- **Updated**: 2020-12-22 01:44:10+00:00
- **Authors**: Zicong Zhang, Kimerly Powell, Changchang Yin, Shilei Cao, Dani Gonzalez, Yousef Hannawi, Ping Zhang
- **Comment**: Accepted by AMIA 2021 Virtual Informatics Summit
- **Journal**: None
- **Summary**: White Matter Hyperintensities (WMH) are the most common manifestation of cerebral small vessel disease (cSVD) on the brain MRI. Accurate WMH segmentation algorithms are important to determine cSVD burden and its clinical consequences. Most of existing WMH segmentation algorithms require both fluid attenuated inversion recovery (FLAIR) images and T1-weighted images as inputs. However, T1-weighted images are typically not part of standard clinicalscans which are acquired for patients with acute stroke. In this paper, we propose a novel brain atlas guided attention U-Net (BAGAU-Net) that leverages only FLAIR images with a spatially-registered white matter (WM) brain atlas to yield competitive WMH segmentation performance. Specifically, we designed a dual-path segmentation model with two novel connecting mechanisms, namely multi-input attention module (MAM) and attention fusion module (AFM) to fuse the information from two paths for accurate results. Experiments on two publicly available datasets show the effectiveness of the proposed BAGAU-Net. With only FLAIR images and WM brain atlas, BAGAU-Net outperforms the state-of-the-art method with T1-weighted images, paving the way for effective development of WMH segmentation. Availability:https://github.com/Ericzhang1/BAGAU-Net



### Multiclass Wound Image Classification using an Ensemble Deep CNN-based Classifier
- **Arxiv ID**: http://arxiv.org/abs/2010.09593v1
- **DOI**: 10.1016/j.compbiomed.2021.104536
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09593v1)
- **Published**: 2020-10-19 15:20:12+00:00
- **Updated**: 2020-10-19 15:20:12+00:00
- **Authors**: Behrouz Rostami, D. M. Anisuzzaman, Chuanbo Wang, Sandeep Gopalakrishnan, Jeffrey Niezgoda, Zeyun Yu
- **Comment**: None
- **Journal**: Computers in Biology and Medicine (2021)
- **Summary**: Acute and chronic wounds are a challenge to healthcare systems around the world and affect many people's lives annually. Wound classification is a key step in wound diagnosis that would help clinicians to identify an optimal treatment procedure. Hence, having a high-performance classifier assists the specialists in the field to classify the wounds with less financial and time costs. Different machine learning and deep learning-based wound classification methods have been proposed in the literature. In this study, we have developed an ensemble Deep Convolutional Neural Network-based classifier to classify wound images including surgical, diabetic, and venous ulcers, into multi-classes. The output classification scores of two classifiers (patch-wise and image-wise) are fed into a Multi-Layer Perceptron to provide a superior classification performance. A 5-fold cross-validation approach is used to evaluate the proposed method. We obtained maximum and average classification accuracy values of 96.4% and 94.28% for binary and 91.9\% and 87.7\% for 3-class classification problems. The results show that our proposed method can be used effectively as a decision support system in classification of wound images or other related clinical applications.



### Multi-Modal Super Resolution for Dense Microscopic Particle Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.09594v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.9; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2010.09594v1)
- **Published**: 2020-10-19 15:20:40+00:00
- **Updated**: 2020-10-19 15:20:40+00:00
- **Authors**: Sarvesh Patil, Chava Y P D Phani Rajanish, Naveen Margankunte
- **Comment**: 11 pages, 10 figures, 6 tables, submitted to IEEE-TPAMI
- **Journal**: None
- **Summary**: Particle Size Analysis (PSA) is an important process carried out in a number of industries, which can significantly influence the properties of the final product. A ubiquitous instrument for this purpose is the Optical Microscope (OM). However, OMs are often prone to drawbacks like low resolution, small focal depth, and edge features being masked due to diffraction. We propose a powerful application of a combination of two Conditional Generative Adversarial Networks (cGANs) that Super Resolve OM images to look like Scanning Electron Microscope (SEM) images. We further demonstrate the use of a custom object detection module that can perform efficient PSA of the super-resolved particles on both, densely and sparsely packed images. The PSA results obtained from the super-resolved images have been benchmarked against human annotators, and results obtained from the corresponding SEM images. The proposed models show a generalizable way of multi-modal image translation and super-resolution for accurate particle size estimation.



### Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2010.09624v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09624v2)
- **Published**: 2020-10-19 16:03:46+00:00
- **Updated**: 2021-01-28 17:47:48+00:00
- **Authors**: Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: 24 pages, 14 figures
- **Journal**: None
- **Summary**: Driven by massive amounts of data and important advances in computational resources, new deep learning systems have achieved outstanding results in a large spectrum of applications. Nevertheless, our current theoretical understanding on the mathematical foundations of deep learning lags far behind its empirical success. Towards solving the vulnerability of neural networks, however, the field of adversarial robustness has recently become one of the main sources of explanations of our deep models. In this article, we provide an in-depth review of the field of adversarial robustness in deep learning, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails. We highlight the intuitive connection between adversarial examples and the geometry of deep neural networks, and eventually explore how the geometric study of adversarial examples can serve as a powerful tool to understand deep learning. Furthermore, we demonstrate the broad applicability of adversarial robustness, providing an overview of the main emerging applications of adversarial robustness beyond security. The goal of this article is to provide readers with a set of new perspectives to understand deep learning, and to supply them with intuitive tools and insights on how to use adversarial robustness to improve it.



### SWIPENET: Object detection in noisy underwater images
- **Arxiv ID**: http://arxiv.org/abs/2010.10006v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10006v3)
- **Published**: 2020-10-19 16:41:20+00:00
- **Updated**: 2022-03-13 04:45:54+00:00
- **Authors**: Long Chen, Feixiang Zhou, Shengke Wang, Junyu Dong, Ning Li, Haiping Ma, Xin Wang, Huiyu Zhou
- **Comment**: arXiv admin note: text overlap with arXiv:2005.11552
- **Journal**: None
- **Summary**: In recent years, deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a novel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, a novel sample-weighted detection loss function is designed for SWIPENET, which focuses on learning high weight samples and ignore learning low weight samples. Moreover, inspired by the human education process that drives the learning from easy to hard concepts, we here propose the CMA training paradigm that first trains a clean detector which is free from the influence of noisy data. Then, based on the clean detector, multiple detectors focusing on learning diverse noisy data are trained and incorporated into a unified deep ensemble of strong noise immunity. Experiments on two underwater robot picking contest datasets (URPC2017 and URPC2018) show that the proposed SWIPENET+CMA framework achieves better accuracy in object detection against several state-of-the-art approaches.



### Attention Augmented ConvLSTM for Environment Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.09662v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2010.09662v3)
- **Published**: 2020-10-19 16:57:24+00:00
- **Updated**: 2021-09-10 19:02:03+00:00
- **Authors**: Bernard Lange, Masha Itkina, Mykel J. Kochenderfer
- **Comment**: Accepted to be published on 2021 International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Safe and proactive planning in robotic systems generally requires accurate predictions of the environment. Prior work on environment prediction applied video frame prediction techniques to bird's-eye view environment representations, such as occupancy grids. ConvLSTM-based frameworks used previously often result in significant blurring and vanishing of moving objects, thus hindering their applicability for use in safety-critical applications. In this work, we propose two extensions to the ConvLSTM to address these issues. We present the Temporal Attention Augmented ConvLSTM (TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks for spatiotemporal occupancy prediction, and demonstrate improved performance over baseline architectures on the real-world KITTI and Waymo datasets.



### RobustBench: a standardized adversarial robustness benchmark
- **Arxiv ID**: http://arxiv.org/abs/2010.09670v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.09670v3)
- **Published**: 2020-10-19 17:06:18+00:00
- **Updated**: 2021-10-31 20:03:39+00:00
- **Authors**: Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein
- **Comment**: The camera-ready version accepted at the NeurIPS'21 Datasets and
  Benchmarks Track: 120+ evaluations, 80+ models, 7 leaderboards (Linf, L2,
  common corruptions; CIFAR-10, CIFAR-100, ImageNet), significantly expanded
  analysis part (calibration, fairness, privacy leakage, smoothness,
  transferability)
- **Journal**: None
- **Summary**: As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.



### Multi-Stage Fusion for One-Click Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09672v2)
- **Published**: 2020-10-19 17:07:40+00:00
- **Updated**: 2020-10-20 12:52:55+00:00
- **Authors**: Soumajit Majumder, Ansh Khurana, Abhinav Rai, Angela Yao
- **Comment**: A preprint of the accepted paper at GCPR 2020
- **Journal**: None
- **Summary**: Segmenting objects of interest in an image is an essential building block of applications such as photo-editing and image analysis. Under interactive settings, one should achieve good segmentations while minimizing user input. Current deep learning-based interactive segmentation approaches use early fusion and incorporate user cues at the image input layer. Since segmentation CNNs have many layers, early fusion may weaken the influence of user interactions on the final prediction results. As such, we propose a new multi-stage guidance framework for interactive segmentation. By incorporating user cues at different stages of the network, we allow user interactions to impact the final segmentation output in a more direct way. Our proposed framework has a negligible increase in parameter count compared to early-fusion frameworks. We perform extensive experimentation on the standard interactive instance segmentation and one-click segmentation benchmarks and report state-of-the-art performance.



### Detecting Hands and Recognizing Physical Contact in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2010.09676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09676v1)
- **Published**: 2020-10-19 17:11:41+00:00
- **Updated**: 2020-10-19 17:11:41+00:00
- **Authors**: Supreeth Narasimhaswamy, Trung Nguyen, Minh Hoai
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We investigate a new problem of detecting hands and recognizing their physical contact state in unconstrained conditions. This is a challenging inference task given the need to reason beyond the local appearance of hands. The lack of training annotations indicating which object or parts of an object the hand is in contact with further complicates the task. We propose a novel convolutional network based on Mask-RCNN that can jointly learn to localize hands and predict their physical contact to address this problem. The network uses outputs from another object detector to obtain locations of objects present in the scene. It uses these outputs and hand locations to recognize the hand's contact state using two attention mechanisms. The first attention mechanism is based on the hand and a region's affinity, enclosing the hand and the object, and densely pools features from this region to the hand region. The second attention module adaptively selects salient features from this plausible region of contact. To develop and evaluate our method's performance, we introduce a large-scale dataset called ContactHands, containing unconstrained images annotated with hand locations and contact states. The proposed network, including the parameters of attention modules, is end-to-end trainable. This network achieves approximately 7\% relative improvement over a baseline network that was built on the vanilla Mask-RCNN architecture and trained for recognizing hand contact states.



### A Survey of Machine Learning Techniques in Adversarial Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/2010.09680v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09680v1)
- **Published**: 2020-10-19 17:16:38+00:00
- **Updated**: 2020-10-19 17:16:38+00:00
- **Authors**: Ehsan Nowroozi, Ali Dehghantanha, Reza M. Parizi, Kim-Kwang Raymond Choo
- **Comment**: 37 pages, 24 figures, Accepted to the Journal Computer and Security
  (Elsevier)
- **Journal**: 2020
- **Summary**: Image forensic plays a crucial role in both criminal investigations (e.g., dissemination of fake images to spread racial hate or false narratives about specific ethnicity groups) and civil litigation (e.g., defamation). Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial (image) examples, with real-world consequences (e.g., inadmissible evidence, or wrongful conviction). Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.



### Multiple Pedestrians and Vehicles Tracking in Aerial Imagery: A Comprehensive Study
- **Arxiv ID**: http://arxiv.org/abs/2010.09689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09689v1)
- **Published**: 2020-10-19 17:26:09+00:00
- **Updated**: 2020-10-19 17:26:09+00:00
- **Authors**: Seyed Majid Azimi, Maximilian Kraus, Reza Bahmanyar, Peter Reinartz
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address various challenges in multi-pedestrian and vehicle tracking in high-resolution aerial imagery by intensive evaluation of a number of traditional and Deep Learning based Single- and Multi-Object Tracking methods. We also describe our proposed Deep Learning based Multi-Object Tracking method AerialMPTNet that fuses appearance, temporal, and graphical information using a Siamese Neural Network, a Long Short-Term Memory, and a Graph Convolutional Neural Network module for a more accurate and stable tracking. Moreover, we investigate the influence of the Squeeze-and-Excitation layers and Online Hard Example Mining on the performance of AerialMPTNet. To the best of our knowledge, we are the first in using these two for a regression-based Multi-Object Tracking. Additionally, we studied and compared the L1 and Huber loss functions. In our experiments, we extensively evaluate AerialMPTNet on three aerial Multi-Object Tracking datasets, namely AerialMPT and KIT AIS pedestrian and vehicle datasets. Qualitative and quantitative results show that AerialMPTNet outperforms all previous methods for the pedestrian datasets and achieves competitive results for the vehicle dataset. In addition, Long Short-Term Memory and Graph Convolutional Neural Network modules enhance the tracking performance. Moreover, using Squeeze-and-Excitation and Online Hard Example Mining significantly helps for some cases while degrades the results for other cases. In addition, according to the results, L1 yields better results with respect to Huber loss for most of the scenarios. The presented results provide a deep insight into challenges and opportunities of the aerial Multi-Object Tracking domain, paving the way for future research.



### Self-supervised Co-training for Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09709v2)
- **Published**: 2020-10-19 17:59:01+00:00
- **Updated**: 2021-01-11 20:53:18+00:00
- **Authors**: Tengda Han, Weidi Xie, Andrew Zisserman
- **Comment**: NeurIPS2020
- **Journal**: None
- **Summary**: The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.



### PseudoSeg: Designing Pseudo Labels for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09713v2)
- **Published**: 2020-10-19 17:59:30+00:00
- **Updated**: 2021-03-30 17:54:47+00:00
- **Authors**: Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, Tomas Pfister
- **Comment**: ICLR 2021. Project page: https://yuliang.vision/pseudo_seg/ Code:
  https://github.com/googleinterns/wss
- **Journal**: None
- **Summary**: Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code is available at https://github.com/googleinterns/wss.



### Investigating and Simplifying Masking-based Saliency Methods for Model Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2010.09750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09750v1)
- **Published**: 2020-10-19 18:00:36+00:00
- **Updated**: 2020-10-19 18:00:36+00:00
- **Authors**: Jason Phang, Jungkyu Park, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency maps that identify the most informative regions of an image for a classifier are valuable for model interpretability. A common approach to creating saliency maps involves generating input masks that mask out portions of an image to maximally deteriorate classification performance, or mask in an image to preserve classification performance. Many variants of this approach have been proposed in the literature, such as counterfactual generation and optimizing over a Gumbel-Softmax distribution. Using a general formulation of masking-based saliency methods, we conduct an extensive evaluation study of a number of recently proposed variants to understand which elements of these methods meaningfully improve performance. Surprisingly, we find that a well-tuned, relatively simple formulation of a masking-based saliency model outperforms many more complex approaches. We find that the most important ingredients for high quality saliency map generation are (1) using both masked-in and masked-out objectives and (2) training the classifier alongside the masking model. Strikingly, we show that a masking model can be trained with as few as 10 examples per class and still generate saliency maps with only a 0.7-point increase in localization error.



### GAMesh: Guided and Augmented Meshing for Deep Point Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.09774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09774v1)
- **Published**: 2020-10-19 18:23:53+00:00
- **Updated**: 2020-10-19 18:23:53+00:00
- **Authors**: Nitin Agarwal, M Gopi
- **Comment**: Accepted to 3DV 2020
- **Journal**: None
- **Summary**: We present a new meshing algorithm called guided and augmented meshing, GAMesh, which uses a mesh prior to generate a surface for the output points of a point network. By projecting the output points onto this prior and simplifying the resulting mesh, GAMesh ensures a surface with the same topology as the mesh prior but whose geometric fidelity is controlled by the point network. This makes GAMesh independent of both the density and distribution of the output points, a common artifact in traditional surface reconstruction algorithms. We show that such a separation of geometry from topology can have several advantages especially in single-view shape prediction, fair evaluation of point networks and reconstructing surfaces for networks which output sparse point clouds. We further show that by training point networks with GAMesh, we can directly optimize the vertex positions to generate adaptive meshes with arbitrary topologies.



### Lung Nodule Classification Using Biomarkers, Volumetric Radiomics and 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/2010.11682v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11682v1)
- **Published**: 2020-10-19 18:57:26+00:00
- **Updated**: 2020-10-19 18:57:26+00:00
- **Authors**: Kushal Mehta, Arshita Jain, Jayalakshmi Mangalagiri, Sumeet Menon, Phuong Nguyen, David R. Chapman
- **Comment**: This paper has been submitted to the Journal of Digital Imaging (JDI
  2020). The poster of this paper has received the 2nd prize for the Research
  Poster Award. Link: https://siim.org/page/20m_p_lung_node_malignancy
- **Journal**: None
- **Summary**: We present a hybrid algorithm to estimate lung nodule malignancy that combines imaging biomarkers from Radiologist's annotation with image classification of CT scans. Our algorithm employs a 3D Convolutional Neural Network (CNN) as well as a Random Forest in order to combine CT imagery with biomarker annotation and volumetric radiomic features. We analyze and compare the performance of the algorithm using only imagery, only biomarkers, combined imagery + biomarkers, combined imagery + volumetric radiomic features and finally the combination of imagery + biomarkers + volumetric features in order to classify the suspicion level of nodule malignancy. The National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to train and evaluate the classification task. We show that the incorporation of semi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the available training sample size of the LIDC-IDRI thereby further improving the accuracy of malignancy estimation of most of the models tested although there is no significant improvement with the use of KNN semi-supervised learning if image classification with CNNs and volumetric features are combined with descriptive biomarkers. Unexpectedly, we also show that a model using image biomarkers alone is more accurate than one that combines biomarkers with volumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the possibility that this result may be influenced by cognitive bias in LIDC-IDRI because malignancy estimates were recorded by the same radiologist panel as biomarkers, as well as future work to incorporate pathology information over a subset of study participants.



### Facial Emotion Recognition with Noisy Multi-task Annotations
- **Arxiv ID**: http://arxiv.org/abs/2010.09849v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09849v2)
- **Published**: 2020-10-19 20:39:37+00:00
- **Updated**: 2020-11-24 17:43:01+00:00
- **Authors**: Siwei Zhang, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool
- **Comment**: Accepted by 2021 WACV, camera-ready version with appendix
- **Journal**: None
- **Summary**: Human emotions can be inferred from facial expressions. However, the annotations of facial expressions are often highly noisy in common emotion coding models, including categorical and dimensional ones. To reduce human labelling effort on multi-task labels, we introduce a new problem of facial emotion recognition with noisy multi-task annotations. For this new problem, we suggest a formulation from the point of joint distribution match view, which aims at learning more reliable correlations among raw facial images and multi-task labels, resulting in the reduction of noise influence. In our formulation, we exploit a new method to enable the emotion prediction and the joint distribution learning in a unified adversarial learning game. Evaluation throughout extensive experiments studies the real setups of the suggested new problem, as well as the clear superiority of the proposed method over the state-of-the-art competing methods on either the synthetic noisy labeled CIFAR-10 or practical noisy multi-task labeled RAF and AffectNet. The code is available at https://github.com/sanweiliti/noisyFER.



### Anomaly Detection on X-Rays Using Self-Supervised Aggregation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09856v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09856v1)
- **Published**: 2020-10-19 20:49:34+00:00
- **Updated**: 2020-10-19 20:49:34+00:00
- **Authors**: Behzad Bozorgtabar, Dwarikanath Mahapatra, Guillaume Vray, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: None
- **Summary**: Deep anomaly detection models using a supervised mode of learning usually work under a closed set assumption and suffer from overfitting to previously seen rare anomalies at training, which hinders their applicability in a real scenario. In addition, obtaining annotations for X-rays is very time consuming and requires extensive training of radiologists. Hence, training anomaly detection in a fully unsupervised or self-supervised fashion would be advantageous, allowing a significant reduction of time spent on the report by radiologists. In this paper, we present SALAD, an end-to-end deep self-supervised methodology for anomaly detection on X-Ray images. The proposed method is based on an optimization strategy in which a deep neural network is encouraged to represent prototypical local patterns of the normal data in the embedding space. During training, we record the prototypical patterns of normal training samples via a memory bank. Our anomaly score is then derived by measuring similarity to a weighted combination of normal prototypical patterns within a memory bank without using any anomalous patterns. We present extensive experiments on the challenging NIH Chest X-rays and MURA dataset, which indicate that our algorithm improves state-of-the-art methods by a wide margin.



### Failure Prediction by Confidence Estimation of Uncertainty-Aware Dirichlet Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.09865v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09865v1)
- **Published**: 2020-10-19 21:06:45+00:00
- **Updated**: 2020-10-19 21:06:45+00:00
- **Authors**: Theodoros Tsiligkaridis
- **Comment**: preliminary version presented at ICML 2020 Workshop on Uncertainty
  and Robustness in Deep Learning, submitted
- **Journal**: None
- **Summary**: Reliably assessing model confidence in deep learning and predicting errors likely to be made are key elements in providing safety for model deployment, in particular for applications with dire consequences. In this paper, it is first shown that uncertainty-aware deep Dirichlet neural networks provide an improved separation between the confidence of correct and incorrect predictions in the true class probability (TCP) metric. Second, as the true class is unknown at test time, a new criterion is proposed for learning the true class probability by matching prediction confidence scores while taking imbalance and TCP constraints into account for correct predictions and failures. Experimental results show our method improves upon the maximum class probability (MCP) baseline and predicted TCP for standard networks on several image classification tasks with various network architectures.



### DeepApple: Deep Learning-based Apple Detection using a Suppression Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2010.09870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09870v1)
- **Published**: 2020-10-19 21:07:46+00:00
- **Updated**: 2020-10-19 21:07:46+00:00
- **Authors**: Pengyu Chu, Zhaojian Li, Kyle Lammers, Renfu Lu, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic apple harvesting has received much research attention in the past few years due to growing shortage and rising cost in labor. One key enabling technology towards automated harvesting is accurate and robust apple detection, which poses great challenges as a result of the complex orchard environment that involves varying lighting conditions and foliage/branch occlusions. This letter reports on the development of a novel deep learning-based apple detection framework named DeepApple. Specifically, we first collect a comprehensive apple orchard dataset for 'Gala' and 'Blondee' apples, using a color camera, under different lighting conditions (sunny vs. overcast and front lighting vs. back lighting). We then develop a novel suppression Mask R-CNN for apple detection, in which a suppression branch is added to the standard Mask R-CNN to suppress non-apple features generated by the original network. Comprehensive evaluations are performed, which show that the developed suppression Mask R-CNN network outperforms state-of-the-art models with a higher F1-score of 0.905 and a detection time of 0.25 second per frame on a standard desktop computer.



### LT-GAN: Self-Supervised GAN with Latent Transformation Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.09893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09893v1)
- **Published**: 2020-10-19 22:09:45+00:00
- **Updated**: 2020-10-19 22:09:45+00:00
- **Authors**: Parth Patel, Nupur Kumari, Mayank Singh, Balaji Krishnamurthy
- **Comment**: Accepted at WACV2021
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) coupled with self-supervised tasks have shown promising results in unconditional and semi-supervised image generation. We propose a self-supervised approach (LT-GAN) to improve the generation quality and diversity of images by estimating the GAN-induced transformation (i.e. transformation induced in the generated images by perturbing the latent space of generator). Specifically, given two pairs of images where each pair comprises of a generated image and its transformed version, the self-supervision task aims to identify whether the latent transformation applied in the given pair is same to that of the other pair. Hence, this auxiliary loss encourages the generator to produce images that are distinguishable by the auxiliary network, which in turn promotes the synthesis of semantically consistent images with respect to latent transformations. We show the efficacy of this pretext task by improving the image generation quality in terms of FID on state-of-the-art models for both conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover, we empirically show that LT-GAN helps in improving controlled image editing for CelebA-HQ and ImageNet over baseline models. We experimentally demonstrate that our proposed LT self-supervision task can be effectively combined with other state-of-the-art training techniques for added benefits. Consequently, we show that our approach achieves the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation.



### Color Image Segmentation Metrics
- **Arxiv ID**: http://arxiv.org/abs/2010.09907v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV, I.4.6; I.2.10; I.5.0; I.3.0; E.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.09907v1)
- **Published**: 2020-10-19 22:47:32+00:00
- **Updated**: 2020-10-19 22:47:32+00:00
- **Authors**: Majid Harouni, Hadi Yazdani Baghmaleki
- **Comment**: 19 pages, 11 figures, 6 tables, 29 equations, book chapter, 2 authors
- **Journal**: None
- **Summary**: An automatic image segmentation procedure is an inevitable part of many image analyses and computer vision which deeply affect the rest of the system; therefore, a set of interactive segmentation evaluation methods can substantially simplify the system development process. This entry presents the state of the art of quantitative evaluation metrics for color image segmentation methods by performing an analytical and comparative review of the measures. The decision-making process in selecting a suitable evaluation metric is still very serious because each metric tends to favor a different segmentation method for each benchmark dataset. Furthermore, a conceptual comparison of these metrics is provided at a high level of abstraction and is discussed for understanding the quantitative changes in different image segmentation results.



### Product Manifold Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09908v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10 (Primary), 42-08, 57Z25 (Secondary), I.5.0; I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.09908v1)
- **Published**: 2020-10-19 22:51:06+00:00
- **Updated**: 2020-10-19 22:51:06+00:00
- **Authors**: Sharon Zhang, Amit Moscovich, Amit Singer
- **Comment**: 10 pages, 4 figures
- **Journal**: Proceedings of The 24th International Conference on Artificial
  Intelligence and Statistics. 130 (2021) 3241-3249
- **Summary**: We consider problems of dimensionality reduction and learning data representations for continuous spaces with two or more independent degrees of freedom. Such problems occur, for example, when observing shapes with several components that move independently. Mathematically, if the parameter space of each continuous independent motion is a manifold, then their combination is known as a product manifold. In this paper, we present a new paradigm for non-linear independent component analysis called manifold factorization. Our factorization algorithm is based on spectral graph methods for manifold learning and the separability of the Laplacian operator on product spaces. Recovering the factors of a manifold yields meaningful lower-dimensional representations and provides a new way to focus on particular aspects of the data space while ignoring others. We demonstrate the potential use of our method for an important and challenging problem in structural biology: mapping the motions of proteins and other large molecules using cryo-electron microscopy datasets.



### Hierarchical Paired Channel Fusion Network for Street Scene Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.09925v1
- **DOI**: 10.1109/TIP.2020.3031173
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.09925v1)
- **Published**: 2020-10-19 23:51:28+00:00
- **Updated**: 2020-10-19 23:51:28+00:00
- **Authors**: Yinjie Lei, Duo Peng, Pingping Zhang, Qiuhong Ke, Haifeng Li
- **Comment**: To appear in Transactions on Image Processing, including 13 pages, 13
  figures, 9 tables
- **Journal**: None
- **Summary**: Street Scene Change Detection (SSCD) aims to locate the changed regions between a given street-view image pair captured at different times, which is an important yet challenging task in the computer vision community. The intuitive way to solve the SSCD task is to fuse the extracted image feature pairs, and then directly measure the dissimilarity parts for producing a change map. Therefore, the key for the SSCD task is to design an effective feature fusion method that can improve the accuracy of the corresponding change maps. To this end, we present a novel Hierarchical Paired Channel Fusion Network (HPCFNet), which utilizes the adaptive fusion of paired feature channels. Specifically, the features of a given image pair are jointly extracted by a Siamese Convolutional Neural Network (SCNN) and hierarchically combined by exploring the fusion of channel pairs at multiple feature levels. In addition, based on the observation that the distribution of scene changes is diverse, we further propose a Multi-Part Feature Learning (MPFL) strategy to detect diverse changes. Based on the MPFL strategy, our framework achieves a novel approach to adapt to the scale and location diversities of the scene change regions. Extensive experiments on three public datasets (i.e., PCD, VL-CMU-CD and CDnet2014) demonstrate that the proposed framework achieves superior performance which outperforms other state-of-the-art methods with a considerable margin.



