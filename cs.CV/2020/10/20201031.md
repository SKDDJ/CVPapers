# Arxiv Papers in cs.CV on 2020-10-31
### Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.00147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00147v1)
- **Published**: 2020-10-31 00:11:36+00:00
- **Updated**: 2020-10-31 00:11:36+00:00
- **Authors**: Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, Alexander G. Hauptmann
- **Comment**: Accepted by NeurIPS 2020 (oral). Code:
  https://github.com/kgl-prml/Pixel- Level-Cycle-Association
- **Journal**: None
- **Summary**: Domain adaptive semantic segmentation aims to train a model performing satisfactory pixel-level predictions on the target with only out-of-domain (source) annotations. The conventional solution to this task is to minimize the discrepancy between source and target to enable effective knowledge transfer. Previous domain discrepancy minimization methods are mainly based on the adversarial training. They tend to consider the domain discrepancy globally, which ignore the pixel-wise relationships and are less discriminative. In this paper, we propose to build the pixel-level cycle association between source and target pixel pairs and contrastively strengthen their connections to diminish the domain gap and make the features more discriminative. To the best of our knowledge, this is a new perspective for tackling such a challenging task. Experiment results on two representative domain adaptation benchmarks, i.e. GTAV $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes, verify the effectiveness of our proposed method and demonstrate that our method performs favorably against previous state-of-the-arts. Our method can be trained end-to-end in one stage and introduces no additional parameters, which is expected to serve as a general framework and help ease future research in domain adaptive semantic segmentation. Code is available at https://github.com/kgl-prml/Pixel- Level-Cycle-Association.



### Leveraging Adaptive Color Augmentation in Convolutional Neural Networks for Deep Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.00148v1
- **DOI**: 10.1109/ISBI45749.2020.9098344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00148v1)
- **Published**: 2020-10-31 00:16:23+00:00
- **Updated**: 2020-10-31 00:16:23+00:00
- **Authors**: Anindo Saha, Prem Prasad, Abdullah Thabit
- **Comment**: Accepted to 2020 17th IEEE International Symposium on Biomedical
  Imaging (ISBI) [oral presentation]
- **Journal**: None
- **Summary**: Fully automatic detection of skin lesions in dermatoscopic images can facilitate early diagnosis and repression of malignant melanoma and non-melanoma skin cancer. Although convolutional neural networks are a powerful solution, they are limited by the illumination spectrum of annotated dermatoscopic screening images, where color is an important discriminative feature. In this paper, we propose an adaptive color augmentation technique to amplify data expression and model performance, while regulating color difference and saturation to minimize the risks of using synthetic data. Through deep visualization, we qualitatively identify and verify the semantic structural features learned by the network for discriminating skin lesions against normal skin tissue. The overall system achieves a Dice Ratio of 0.891 with 0.943 sensitivity and 0.932 specificity on the ISIC 2018 Testing Set for segmentation.



### Weakly Supervised 3D Classification of Chest CT using Aggregated Multi-Resolution Deep Segmentation Features
- **Arxiv ID**: http://arxiv.org/abs/2011.00149v1
- **DOI**: 10.1117/12.2550857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00149v1)
- **Published**: 2020-10-31 00:16:53+00:00
- **Updated**: 2020-10-31 00:16:53+00:00
- **Authors**: Anindo Saha, Fakrul I. Tushar, Khrystyna Faryna, Vincent M. D'Anniballe, Rui Hou, Maciej A. Mazurowski, Geoffrey D. Rubin, Joseph Y. Lo
- **Comment**: Accepted to 2020 SPIE Medical Imaging: Computer-Aided Diagnosis [oral
  presentation]
- **Journal**: None
- **Summary**: Weakly supervised disease classification of CT imaging suffers from poor localization owing to case-level annotations, where even a positive scan can hold hundreds to thousands of negative slices along multiple planes. Furthermore, although deep learning segmentation and classification models extract distinctly unique combinations of anatomical features from the same target class(es), they are typically seen as two independent processes in a computer-aided diagnosis (CAD) pipeline, with little to no feature reuse. In this research, we propose a medical classifier that leverages the semantic structural concepts learned via multi-resolution segmentation feature maps, to guide weakly supervised 3D classification of chest CT volumes. Additionally, a comparative analysis is drawn across two different types of feature aggregation to explore the vast possibilities surrounding feature fusion. Using a dataset of 1593 scans labeled on a case-level basis via rule-based model, we train a dual-stage convolutional neural network (CNN) to perform organ segmentation and binary classification of four representative diseases (emphysema, pneumonia/atelectasis, mass and nodules) in lungs. The baseline model, with separate stages for segmentation and classification, results in AUC of 0.791. Using identical hyperparameters, the connected architecture using static and dynamic feature aggregation improves performance to AUC of 0.832 and 0.851, respectively. This study advances the field in two key ways. First, case-level report data is used to weakly supervise a 3D CT classifier of multiple, simultaneous diseases for an organ. Second, segmentation and classification models are connected with two different feature aggregation strategies to enhance the classification performance.



### Automatic Chronic Degenerative Diseases Identification Using Enteric Nervous System Images
- **Arxiv ID**: http://arxiv.org/abs/2011.00160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.00160v1)
- **Published**: 2020-10-31 01:04:46+00:00
- **Updated**: 2020-10-31 01:04:46+00:00
- **Authors**: Gustavo Z. Felipe, Jacqueline N. Zanoni, Camila C. Sehaber-Sierakowski, Gleison D. P. Bossolani, Sara R. G. Souza, Franklin C. Flores, Luiz E. S. Oliveira, Rodolfo M. Pereira, Yandre M. G. Costa
- **Comment**: None
- **Journal**: None
- **Summary**: Studies recently accomplished on the Enteric Nervous System have shown that chronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus, the development of recognition methods able to identify whether or not the EGC are affected by these type of diseases may be helpful in its diagnoses. In this work, we propose the use of pattern recognition and machine learning techniques to evaluate if a given animal EGC image was obtained from a healthy individual or one affect by a chronic degenerative disease. In the proposed approach, we have performed the classification task with handcrafted features and deep learning based techniques, also known as non-handcrafted features. The handcrafted features were obtained from the textural content of the ECG images using texture descriptors, such as the Local Binary Pattern (LBP). Moreover, the representation learning techniques employed in the approach are based on different Convolutional Neural Network (CNN) architectures, such as AlexNet and VGG16, with and without transfer learning. The complementarity between the handcrafted and non-handcrafted features was also evaluated with late fusion techniques. The datasets of EGC images used in the experiments, which are also contributions of this paper, are composed of three different chronic degenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The experimental results, supported by statistical analysis, shown that the proposed approach can distinguish healthy cells from the sick ones with a recognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13% (Diabetes Mellitus), being achieved by combining classifiers obtained both feature scenarios.



### Multimodal and self-supervised representation learning for automatic gesture recognition in surgical robotics
- **Arxiv ID**: http://arxiv.org/abs/2011.00168v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.00168v1)
- **Published**: 2020-10-31 02:20:32+00:00
- **Updated**: 2020-10-31 02:20:32+00:00
- **Authors**: Aniruddha Tamhane, Jie Ying Wu, Mathias Unberath
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Self-supervised, multi-modal learning has been successful in holistic representation of complex scenarios. This can be useful to consolidate information from multiple modalities which have multiple, versatile uses. Its application in surgical robotics can lead to simultaneously developing a generalised machine understanding of the surgical process and reduce the dependency on quality, expert annotations which are generally difficult to obtain. We develop a self-supervised, multi-modal representation learning paradigm that learns representations for surgical gestures from video and kinematics. We use an encoder-decoder network configuration that encodes representations from surgical videos and decodes them to yield kinematics. We quantitatively demonstrate the efficacy of our learnt representations for gesture recognition (with accuracy between 69.6 % and 77.8 %), transfer learning across multiple tasks (with accuracy between 44.6 % and 64.8 %) and surgeon skill classification (with accuracy between 76.8 % and 81.2 %). Further, we qualitatively demonstrate that our self-supervised representations cluster in semantically meaningful properties (surgeon skill and gestures).



### A Study of Image Pre-processing for Faster Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.06928v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06928v1)
- **Published**: 2020-10-31 02:55:17+00:00
- **Updated**: 2020-10-31 02:55:17+00:00
- **Authors**: Md Tanzil Shahriar, Huyue Li
- **Comment**: None
- **Journal**: None
- **Summary**: Quality of image always plays a vital role in in-creasing object recognition or classification rate. A good quality image gives better recognition or classification rate than any unprocessed noisy images. It is more difficult to extract features from such unprocessed images which in-turn reduces object recognition or classification rate. To overcome problems occurred due to low quality image, typically pre-processing is done before extracting features from the image. Our project proposes an image pre-processing method, so that the performance of selected Machine Learning algorithms or Deep Learning algorithms increases in terms of increased accuracy or reduced the number of training images. In the later part, we compare the performance results by using our method with the previous used approaches.



### Dense Pixel-wise Micro-motion Estimation of Object Surface by using Low Dimensional Embedding of Laser Speckle Pattern
- **Arxiv ID**: http://arxiv.org/abs/2011.00174v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00174v1)
- **Published**: 2020-10-31 03:03:00+00:00
- **Updated**: 2020-10-31 03:03:00+00:00
- **Authors**: Ryusuke Sagawa, Yusuke Higuchi, Hiroshi Kawasaki, Ryo Furukawa, Takahiro Ito
- **Comment**: to be published in ACCV2020
- **Journal**: None
- **Summary**: This paper proposes a method of estimating micro-motion of an object at each pixel that is too small to detect under a common setup of camera and illumination. The method introduces an active-lighting approach to make the motion visually detectable. The approach is based on speckle pattern, which is produced by the mutual interference of laser light on object's surface and continuously changes its appearance according to the out-of-plane motion of the surface. In addition, speckle pattern becomes uncorrelated with large motion. To compensate such micro- and large motion, the method estimates the motion parameters up to scale at each pixel by nonlinear embedding of the speckle pattern into low-dimensional space. The out-of-plane motion is calculated by making the motion parameters spatially consistent across the image. In the experiments, the proposed method is compared with other measuring devices to prove the effectiveness of the method.



### Evaluation of Inference Attack Models for Deep Learning on Medical Data
- **Arxiv ID**: http://arxiv.org/abs/2011.00177v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00177v1)
- **Published**: 2020-10-31 03:18:36+00:00
- **Updated**: 2020-10-31 03:18:36+00:00
- **Authors**: Maoqiang Wu, Xinyue Zhang, Jiahao Ding, Hien Nguyen, Rong Yu, Miao Pan, Stephen T. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has attracted broad interest in healthcare and medical communities. However, there has been little research into the privacy issues created by deep networks trained for medical applications. Recently developed inference attack algorithms indicate that images and text records can be reconstructed by malicious parties that have the ability to query deep networks. This gives rise to the concern that medical images and electronic health records containing sensitive patient information are vulnerable to these attacks. This paper aims to attract interest from researchers in the medical deep learning community to this important problem. We evaluate two prominent inference attack models, namely, attribute inference attack and model inversion attack. We show that they can reconstruct real-world medical images and clinical reports with high fidelity. We then investigate how to protect patients' privacy using defense mechanisms, such as label perturbation and model perturbation. We provide a comparison of attack results between the original and the medical deep learning models with defenses. The experimental evaluations show that our proposed defense approaches can effectively reduce the potential privacy leakage of medical deep learning from the inference attacks.



### Learning Open Set Network with Discriminative Reciprocal Points
- **Arxiv ID**: http://arxiv.org/abs/2011.00178v1
- **DOI**: 10.1007/978-3-030-58580-8_30
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.00178v1)
- **Published**: 2020-10-31 03:20:31+00:00
- **Updated**: 2020-10-31 03:20:31+00:00
- **Authors**: Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun Huang, Shiliang Pu, Yonghong Tian
- **Comment**: ECCV 2020 (spotlight)
- **Journal**: ECCV 2020
- **Summary**: Open set recognition is an emerging research area that aims to simultaneously classify samples from predefined classes and identify the rest as 'unknown'. In this process, one of the key challenges is to reduce the risk of generalizing the inherent characteristics of numerous unknown samples learned from a small amount of known data. In this paper, we propose a new concept, Reciprocal Point, which is the potential representation of the extra-class space corresponding to each known category. The sample can be classified to known or unknown by the otherness with reciprocal points. To tackle the open set problem, we offer a novel open space risk regularization term. Based on the bounded space constructed by reciprocal points, the risk of unknown is reduced through multi-category interaction. The novel learning framework called Reciprocal Point Learning (RPL), which can indirectly introduce the unknown information into the learner with only known classes, so as to learn more compact and discriminative representations. Moreover, we further construct a new large-scale challenging aircraft dataset for open set recognition: Aircraft 300 (Air-300). Extensive experiments on multiple benchmark datasets indicate that our framework is significantly superior to other existing approaches and achieves state-of-the-art performance on standard open set benchmarks.



### Combining Domain-Specific Meta-Learners in the Parameter Space for Cross-Domain Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.00179v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.00179v1)
- **Published**: 2020-10-31 03:33:39+00:00
- **Updated**: 2020-10-31 03:33:39+00:00
- **Authors**: Shuman Peng, Weilian Song, Martin Ester
- **Comment**: Code coming soon at https://github.com/shumanpng/CosML
- **Journal**: None
- **Summary**: The goal of few-shot classification is to learn a model that can classify novel classes using only a few training examples. Despite the promising results shown by existing meta-learning algorithms in solving the few-shot classification problem, there still remains an important challenge: how to generalize to unseen domains while meta-learning on multiple seen domains? In this paper, we propose an optimization-based meta-learning method, called Combining Domain-Specific Meta-Learners (CosML), that addresses the cross-domain few-shot classification problem. CosML first trains a set of meta-learners, one for each training domain, to learn prior knowledge (i.e., meta-parameters) specific to each domain. The domain-specific meta-learners are then combined in the \emph{parameter space}, by taking a weighted average of their meta-parameters, which is used as the initialization parameters of a task network that is quickly adapted to novel few-shot classification tasks in an unseen domain. Our experiments show that CosML outperforms a range of state-of-the-art methods and achieves strong cross-domain generalization ability.



### Exploring Severe Occlusion: Multi-Person 3D Pose Estimation with Gated Convolution
- **Arxiv ID**: http://arxiv.org/abs/2011.00184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00184v1)
- **Published**: 2020-10-31 04:35:24+00:00
- **Updated**: 2020-10-31 04:35:24+00:00
- **Authors**: Renshu Gu, Gaoang Wang, Jenq-Neng Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D human pose estimation (HPE) is crucial in many fields, such as human behavior analysis, augmented reality/virtual reality (AR/VR) applications, and self-driving industry. Videos that contain multiple potentially occluded people captured from freely moving monocular cameras are very common in real-world scenarios, while 3D HPE for such scenarios is quite challenging, partially because there is a lack of such data with accurate 3D ground truth labels in existing datasets. In this paper, we propose a temporal regression network with a gated convolution module to transform 2D joints to 3D and recover the missing occluded joints in the meantime. A simple yet effective localization approach is further conducted to transform the normalized pose to the global trajectory. To verify the effectiveness of our approach, we also collect a new moving camera multi-human (MMHuman) dataset that includes multiple people with heavy occlusion captured by moving cameras. The 3D ground truth joints are provided by accurate motion capture (MoCap) system. From the experiments on static-camera based Human3.6M data and our own collected moving-camera based data, we show that our proposed method outperforms most state-of-the-art 2D-to-3D pose estimation methods, especially for the scenarios with heavy occlusions.



### Self-supervised Representation Learning for Evolutionary Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2011.00186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00186v1)
- **Published**: 2020-10-31 04:57:16+00:00
- **Updated**: 2020-10-31 04:57:16+00:00
- **Authors**: Chen Wei, Yiping Tang, Chuang Niu, Haihong Hu, Yue Wang, Jimin Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently proposed neural architecture search (NAS) algorithms adopt neural predictors to accelerate the architecture search. The capability of neural predictors to accurately predict the performance metrics of neural architecture is critical to NAS, and the acquisition of training datasets for neural predictors is time-consuming. How to obtain a neural predictor with high prediction accuracy using a small amount of training data is a central problem to neural predictor-based NAS. Here, we firstly design a new architecture encoding scheme that overcomes the drawbacks of existing vector-based architecture encoding schemes to calculate the graph edit distance of neural architectures. To enhance the predictive performance of neural predictors, we devise two self-supervised learning methods from different perspectives to pre-train the architecture embedding part of neural predictors to generate a meaningful representation of neural architectures. The first one is to train a carefully designed two branch graph neural network model to predict the graph edit distance of two input neural architectures. The second method is inspired by the prevalently contrastive learning, and we present a new contrastive learning algorithm that utilizes a central feature vector as a proxy to contrast positive pairs against negative pairs. Experimental results illustrate that the pre-trained neural predictors can achieve comparable or superior performance compared with their supervised counterparts with several times less training samples. We achieve state-of-the-art performance on the NASBench-101 and NASBench201 benchmarks when integrating the pre-trained neural predictors with an evolutionary NAS algorithm.



### Enhanced Balancing GAN: Minority-class Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.00189v1
- **DOI**: 10.1007/s00521-021-06163-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00189v1)
- **Published**: 2020-10-31 05:03:47+00:00
- **Updated**: 2020-10-31 05:03:47+00:00
- **Authors**: Gaofeng Huang, Amir H. Jafari
- **Comment**: None
- **Journal**: Neural Computing and Applications, 2021
- **Summary**: Generative adversarial networks (GANs) are one of the most powerful generative models, but always require a large and balanced dataset to train. Traditional GANs are not applicable to generate minority-class images in a highly imbalanced dataset. Balancing GAN (BAGAN) is proposed to mitigate this problem, but it is unstable when images in different classes look similar, e.g. flowers and cells. In this work, we propose a supervised autoencoder with an intermediate embedding model to disperse the labeled latent vectors. With the improved autoencoder initialization, we also build an architecture of BAGAN with gradient penalty (BAGAN-GP). Our proposed model overcomes the unstable issue in original BAGAN and converges faster to high quality generations. Our model achieves high performance on the imbalanced scale-down version of MNIST Fashion, CIFAR-10, and one small-scale medical image dataset.



### Meta-Learning with Adaptive Hyperparameters
- **Arxiv ID**: http://arxiv.org/abs/2011.00209v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00209v2)
- **Published**: 2020-10-31 08:05:34+00:00
- **Updated**: 2020-12-08 06:53:01+00:00
- **Authors**: Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, Kyoung Mu Lee
- **Comment**: NeurIPS 2020. Code at https://github.com/baiksung/alfa. Typo fix in
  the updated version
- **Journal**: None
- **Summary**: Despite its popularity, several recent works question the effectiveness of MAML when test tasks are different from training tasks, thus suggesting various task-conditioned methodology to improve the initialization. Instead of searching for better task-aware initialization, we focus on a complementary factor in MAML framework, inner-loop optimization (or fast adaptation). Consequently, we propose a new weight update rule that greatly enhances the fast adaptation process. Specifically, we introduce a small meta-network that can adaptively generate per-step hyperparameters: learning rate and weight decay coefficients. The experimental results validate that the Adaptive Learning of hyperparameters for Fast Adaptation (ALFA) is the equally important ingredient that was often neglected in the recent few-shot learning approaches. Surprisingly, fast adaptation from random initialization with ALFA can already outperform MAML.



### Temporal Smoothing for 3D Human Pose Estimation and Localization for Occluded People
- **Arxiv ID**: http://arxiv.org/abs/2011.00250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00250v1)
- **Published**: 2020-10-31 11:48:31+00:00
- **Updated**: 2020-10-31 11:48:31+00:00
- **Authors**: Marton Veges, Andras Lorincz
- **Comment**: ICONIP 2020
- **Journal**: None
- **Summary**: In multi-person pose estimation actors can be heavily occluded, even become fully invisible behind another person. While temporal methods can still predict a reasonable estimation for a temporarily disappeared pose using past and future frames, they exhibit large errors nevertheless. We present an energy minimization approach to generate smooth, valid trajectories in time, bridging gaps in visibility. We show that it is better than other interpolation based approaches and achieves state of the art results. In addition, we present the synthetic MuCo-Temp dataset, a temporal extension of the MuCo-3DHP dataset. Our code is made publicly available.



### Encoding Clinical Priori in 3D Convolutional Neural Networks for Prostate Cancer Detection in bpMRI
- **Arxiv ID**: http://arxiv.org/abs/2011.00263v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00263v4)
- **Published**: 2020-10-31 13:10:58+00:00
- **Updated**: 2021-09-21 11:25:26+00:00
- **Authors**: Anindo Saha, Matin Hosseinzadeh, Henkjan Huisman
- **Comment**: Accepted to Medical Imaging Meets NeurIPS Workshop of the 34th
  Conference on Neural Information Processing Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: We hypothesize that anatomical priors can be viable mediums to infuse domain-specific clinical knowledge into state-of-the-art convolutional neural networks (CNN) based on the U-Net architecture. We introduce a probabilistic population prior which captures the spatial prevalence and zonal distinction of clinically significant prostate cancer (csPCa), in order to improve its computer-aided detection (CAD) in bi-parametric MR imaging (bpMRI). To evaluate performance, we train 3D adaptations of the U-Net, U-SEResNet, UNet++ and Attention U-Net using 800 institutional training-validation scans, paired with radiologically-estimated annotations and our computed prior. For 200 independent testing bpMRI scans with histologically-confirmed delineations of csPCa, our proposed method of encoding clinical priori demonstrates a strong ability to improve patient-based diagnosis (upto 8.70% increase in AUROC) and lesion-level detection (average increase of 1.08 pAUC between 0.1-10 false positives per patient) across all four architectures.



### ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.00265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00265v1)
- **Published**: 2020-10-31 13:14:34+00:00
- **Updated**: 2020-10-31 13:14:34+00:00
- **Authors**: Weidong Shi, Guanghui Ren, Yunpeng Chen, Shuicheng Yan
- **Comment**: 10pages, 3figures
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher's behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as large margin constraints (e.g. margin-based softmax). We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition. We conduct extensive experiments on standard face recognition benchmarks, and the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.



### LandmarkGAN: Synthesizing Faces from Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2011.00269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00269v2)
- **Published**: 2020-10-31 13:27:21+00:00
- **Updated**: 2021-02-06 04:19:53+00:00
- **Authors**: Pu Sun, Yuezun Li, Honggang Qi, Siwei Lyu
- **Comment**: Under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Face synthesis is an important problem in computer vision with many applications. In this work, we describe a new method, namely LandmarkGAN, to synthesize faces based on facial landmarks as input. Facial landmarks are a natural, intuitive, and effective representation for facial expressions and orientations, which are independent from the target's texture or color and background scene. Our method is able to transform a set of facial landmarks into new faces of different subjects, while retains the same facial expression and orientation. Experimental results on face synthesis and reenactments demonstrate the effectiveness of our method.



### PREGAN: Pose Randomization and Estimation for Weakly Paired Image Style Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.00301v2
- **DOI**: 10.1109/LRA.2021.3061359
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.00301v2)
- **Published**: 2020-10-31 16:11:11+00:00
- **Updated**: 2021-01-17 07:18:56+00:00
- **Authors**: Zexi Chen, Jiaxin Guo, Xuecheng Xu, Yunkai Wang, Yue Wang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Utilizing the trained model under different conditions without data annotation is attractive for robot applications. Towards this goal, one class of methods is to translate the image style from another environment to the one on which models are trained. In this paper, we propose a weakly-paired setting for the style translation, where the content in the two images is aligned with errors in poses. These images could be acquired by different sensors in different conditions that share an overlapping region, e.g. with LiDAR or stereo cameras, from sunny days or foggy nights. We consider this setting to be more practical with: (i) easier labeling than the paired data; (ii) better interpretability and detail retrieval than the unpaired data. To translate across such images, we propose PREGAN to train a style translator by intentionally transforming the two images with a random pose, and to estimate the given random pose by differentiable non-trainable pose estimator given that the more aligned in style, the better the estimated result is. Such adversarial training enforces the network to learn the style translation, avoiding being entangled with other variations. Finally, PREGAN is validated on both simulated and real-world collected data to show the effectiveness. Results on down-stream tasks, classification, road segmentation, object detection, and feature matching show its potential for real applications. https://github.com/wrld/PRoGAN



### General Data Analytics with Applications to Visual Information Analysis: A Provable Backward-Compatible Semisimple Paradigm over T-Algebra
- **Arxiv ID**: http://arxiv.org/abs/2011.00307v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, math.RA
- **Links**: [PDF](http://arxiv.org/pdf/2011.00307v8)
- **Published**: 2020-10-31 16:41:09+00:00
- **Updated**: 2021-05-02 15:45:32+00:00
- **Authors**: Liang Liao, Stephen John Maybank
- **Comment**: 38 page, 12 figures. two typos are removed. Official code repository:
  https://github.com/liaoliang2020/talgebra
- **Journal**: None
- **Summary**: We consider a novel backward-compatible paradigm of general data analytics over a recently-reported semisimple algebra (called t-algebra). We study the abstract algebraic framework over the t-algebra by representing the elements of t-algebra by fix-sized multi-way arrays of complex numbers and the algebraic structure over the t-algebra by a collection of direct-product constituents. Over the t-algebra, many algorithms are generalized in a straightforward manner using this new semisimple paradigm. To demonstrate the new paradigm's performance and its backward-compatibility, we generalize some canonical algorithms for visual pattern analysis. Experiments on public datasets show that the generalized algorithms compare favorably with their canonical counterparts.



### Scene Flow from Point Clouds with or without Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.00320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.00320v1)
- **Published**: 2020-10-31 17:24:48+00:00
- **Updated**: 2020-10-31 17:24:48+00:00
- **Authors**: Jhony Kaesemodel Pontes, James Hays, Simon Lucey
- **Comment**: International Conference on 3D Vision (3DV 2020)
- **Journal**: None
- **Summary**: Scene flow is the three-dimensional (3D) motion field of a scene. It provides information about the spatial arrangement and rate of change of objects in dynamic environments. Current learning-based approaches seek to estimate the scene flow directly from point clouds and have achieved state-of-the-art performance. However, supervised learning methods are inherently domain specific and require a large amount of labeled data. Annotation of scene flow on real-world point clouds is expensive and challenging, and the lack of such datasets has recently sparked interest in self-supervised learning methods. How to accurately and robustly learn scene flow representations without labeled real-world data is still an open problem. Here we present a simple and interpretable objective function to recover the scene flow from point clouds. We use the graph Laplacian of a point cloud to regularize the scene flow to be "as-rigid-as-possible". Our proposed objective function can be used with or without learning---as a self-supervisory signal to learn scene flow representations, or as a non-learning-based method in which the scene flow is optimized during runtime. Our approach outperforms related works in many datasets. We also show the immediate applications of our proposed method for two applications: motion segmentation and point cloud densification.



### Self-paced and self-consistent co-training for semi-supervised image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.00325v4
- **DOI**: 10.1016/j.media.2021.102146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00325v4)
- **Published**: 2020-10-31 17:41:03+00:00
- **Updated**: 2021-02-10 16:46:23+00:00
- **Authors**: Ping Wang, Jizong Peng, Marco Pedersoli, Yuanfeng Zhou, Caiming Zhang, Christian Desrosiers
- **Comment**: None
- **Journal**: Medical Image Analysis, 2021
- **Summary**: Deep co-training has recently been proposed as an effective approach for image segmentation when annotated data is scarce. In this paper, we improve existing approaches for semi-supervised segmentation with a self-paced and self-consistent co-training method. To help distillate information from unlabeled images, we first design a self-paced learning strategy for co-training that lets jointly-trained neural networks focus on easier-to-segment regions first, and then gradually consider harder ones.This is achieved via an end-to-end differentiable loss inthe form of a generalized Jensen Shannon Divergence(JSD). Moreover, to encourage predictions from different networks to be both consistent and confident, we enhance this generalized JSD loss with an uncertainty regularizer based on entropy. The robustness of individual models is further improved using a self-ensembling loss that enforces their prediction to be consistent across different training iterations. We demonstrate the potential of our method on three challenging image segmentation problems with different image modalities, using small fraction of labeled data. Results show clear advantages in terms of performance compared to the standard co-training baselines and recently proposed state-of-the-art approaches for semi-supervised segmentation



### Deep learning in the ultrasound evaluation of neonatal respiratory status
- **Arxiv ID**: http://arxiv.org/abs/2011.00337v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00337v1)
- **Published**: 2020-10-31 18:57:55+00:00
- **Updated**: 2020-10-31 18:57:55+00:00
- **Authors**: Michela Gravina, Diego Gragnaniello, Luisa Verdoliva, Giovanni Poggi, Iuri Corsini, Carlo Dani, Fabio Meneghin, Gianluca Lista, Salvatore Aversa, Francesco Raimondi, Fiorella Migliaro, Carlo Sansone
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Lung ultrasound imaging is reaching growing interest from the scientific community. On one side, thanks to its harmlessness and high descriptive power, this kind of diagnostic imaging has been largely adopted in sensitive applications, like the diagnosis and follow-up of preterm newborns in neonatal intensive care units. On the other side, state-of-the-art image analysis and pattern recognition approaches have recently proven their ability to fully exploit the rich information contained in these data, making them attractive for the research community. In this work, we present a thorough analysis of recent deep learning networks and training strategies carried out on a vast and challenging multicenter dataset comprising 87 patients with different diseases and gestational ages. These approaches are employed to assess the lung respiratory status from ultrasound images and are evaluated against a reference marker. The conducted analysis sheds some light on this problem by showing the critical points that can mislead the training procedure and proposes some adaptations to the specific data and task. The achieved results sensibly outperform those obtained by a previous work, which is based on textural features, and narrow the gap with the visual score predicted by the human experts.



### Unsupervised Deep Persistent Monocular Visual Odometry and Depth Estimation in Extreme Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.00341v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.00341v1)
- **Published**: 2020-10-31 19:10:27+00:00
- **Updated**: 2020-10-31 19:10:27+00:00
- **Authors**: Yasin Almalioglu, Angel Santamaria-Navarro, Benjamin Morrell, Ali-akbar Agha-mohammadi
- **Comment**: Submitted to ICRA2021
- **Journal**: None
- **Summary**: In recent years, unsupervised deep learning approaches have received significant attention to estimate the depth and visual odometry (VO) from unlabelled monocular image sequences. However, their performance is limited in challenging environments due to perceptual degradation, occlusions and rapid motions. Moreover, the existing unsupervised methods suffer from the lack of scale-consistency constraints across frames, which causes that the VO estimators fail to provide persistent trajectories over long sequences. In this study, we propose an unsupervised monocular deep VO framework that predicts six-degrees-of-freedom pose camera motion and depth map of the scene from unlabelled RGB image sequences. We provide detailed quantitative and qualitative evaluations of the proposed framework on a) a challenging dataset collected during the DARPA Subterranean challenge; and b) the benchmark KITTI and Cityscapes datasets. The proposed approach outperforms both traditional and state-of-the-art unsupervised deep VO methods providing better results for both pose estimation and depth recovery. The presented approach is part of the solution used by the COSTAR team participating at the DARPA Subterranean Challenge.



### TartanVO: A Generalizable Learning-based VO
- **Arxiv ID**: http://arxiv.org/abs/2011.00359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.00359v1)
- **Published**: 2020-10-31 20:49:33+00:00
- **Updated**: 2020-10-31 20:49:33+00:00
- **Authors**: Wenshan Wang, Yaoyu Hu, Sebastian Scherer
- **Comment**: None
- **Journal**: The Conference on Robot Learning 2020
- **Summary**: We present the first learning-based visual odometry (VO) model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes. We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments. Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model. Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories. Our code is available at https://github.com/castacks/tartanvo.



### A Survey on Contrastive Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.00362v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00362v3)
- **Published**: 2020-10-31 21:05:04+00:00
- **Updated**: 2021-02-07 19:11:55+00:00
- **Authors**: Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, Fillia Makedon
- **Comment**: 20 pages, 18 figures, 6 tables
- **Journal**: None
- **Summary**: Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.



### DL-Reg: A Deep Learning Regularization Technique using Linear Regression
- **Arxiv ID**: http://arxiv.org/abs/2011.00368v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00368v2)
- **Published**: 2020-10-31 21:53:24+00:00
- **Updated**: 2020-11-03 23:22:48+00:00
- **Authors**: Maryam Dialameh, Ali Hamzeh, Hossein Rahmani
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization plays a vital role in the context of deep learning by preventing deep neural networks from the danger of overfitting. This paper proposes a novel deep learning regularization method named as DL-Reg, which carefully reduces the nonlinearity of deep networks to a certain extent by explicitly enforcing the network to behave as much linear as possible. The key idea is to add a linear constraint to the objective function of the deep neural networks, which is simply the error of a linear mapping from the inputs to the outputs of the model. More precisely, the proposed DL-Reg carefully forces the network to behave in a linear manner. This linear constraint, which is further adjusted by a regularization factor, prevents the network from the risk of overfitting. The performance of DL-Reg is evaluated by training state-of-the-art deep network models on several benchmark datasets. The experimental results show that the proposed regularization method: 1) gives major improvements over the existing regularization techniques, and 2) significantly improves the performance of deep neural networks, especially in the case of small-sized training datasets.



### Pose Estimation of Specular and Symmetrical Objects
- **Arxiv ID**: http://arxiv.org/abs/2011.00372v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00372v1)
- **Published**: 2020-10-31 22:08:46+00:00
- **Updated**: 2020-10-31 22:08:46+00:00
- **Authors**: Jiaming Hu, Hongyi Ling, Priyam Parashar, Aayush Naik, Henrik Christensen
- **Comment**: submitted to ICRA 2021
- **Journal**: None
- **Summary**: In the robotic industry, specular and textureless metallic components are ubiquitous. The 6D pose estimation of such objects with only a monocular RGB camera is difficult because of the absence of rich texture features. Furthermore, the appearance of specularity heavily depends on the camera viewpoint and environmental light conditions making traditional methods, like template matching, fail. In the last 30 years, pose estimation of the specular object has been a consistent challenge, and most related works require massive knowledge modeling effort for light setups, environment, or the object surface. On the other hand, recent works exhibit the feasibility of 6D pose estimation on a monocular camera with convolutional neural networks(CNNs) however they mostly use opaque objects for evaluation. This paper provides a data-driven solution to estimate the 6D pose of specular objects for grasping them, proposes a cost function for handling symmetry, and demonstrates experimental results showing the system's feasibility.



### Segmentation of Infrared Breast Images Using MultiResUnet Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2011.00376v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00376v1)
- **Published**: 2020-10-31 22:15:28+00:00
- **Updated**: 2020-10-31 22:15:28+00:00
- **Authors**: Ange Lou, Shuyue Guan, Nada Kamona, Murray Loew
- **Comment**: 6 pages. Accepted by IEEE AIPR 2019 (Oral)
- **Journal**: None
- **Summary**: Breast cancer is the second leading cause of death for women in the U.S. Early detection of breast cancer is key to higher survival rates of breast cancer patients. We are investigating infrared (IR) thermography as a noninvasive adjunct to mammography for breast cancer screening. IR imaging is radiation-free, pain-free, and non-contact. Automatic segmentation of the breast area from the acquired full-size breast IR images will help limit the area for tumor search, as well as reduce the time and effort costs of manual segmentation. Autoencoder-like convolutional and deconvolutional neural networks (C-DCNN) had been applied to automatically segment the breast area in IR images in previous studies. In this study, we applied a state-of-the-art deep-learning segmentation model, MultiResUnet, which consists of an encoder part to capture features and a decoder part for precise localization. It was used to segment the breast area by using a set of breast IR images, collected in our pilot study by imaging breast cancer patients and normal volunteers with a thermal infrared camera (N2 Imager). The database we used has 450 images, acquired from 14 patients and 16 volunteers. We used a thresholding method to remove interference in the raw images and remapped them from the original 16-bit to 8-bit, and then cropped and segmented the 8-bit images manually. Experiments using leave-one-out cross-validation (LOOCV) and comparison with the ground-truth images by using Tanimoto similarity show that the average accuracy of MultiResUnet is 91.47%, which is about 2% higher than that of the autoencoder. MultiResUnet offers a better approach to segment breast IR images than our previous model.



### Real-Time Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.00380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00380v1)
- **Published**: 2020-10-31 22:36:20+00:00
- **Updated**: 2020-10-31 22:36:20+00:00
- **Authors**: Shuonan Pei, Mingzhi Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Inrecentyears,ConvolutionalNeuralNet-work(CNN) is quite a popular topic, as it is a powerful andintelligent technique that can be applied in various fields.The YOLO is a technique that uses the algorithms for real-time text detection tasks. However, issues like, photometricdistortion and geometric distortion, could affect the systemYOLO accuracy and cause system failure. Therefore, thereare improvements that can make the system work better. Inthis paper, we are going to present our solution - a potentialsolution of a fast and accurate real-time text direction andrecognition system. The paper covers the topic of Real-TimeText detection and recognition in three major areas: 1. videoand image preprocess, 2. Text detection, 3. Text recognition. Asa mature technique, there are many existing methods that canpotentially improve the solution. We will go through some ofthose existing methods in the literature review session. In thisway, we are presenting an industrial strength, high-accuracy,Real-Time Text Detection and recognition tool.



