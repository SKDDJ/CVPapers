# Arxiv Papers in cs.CV on 2020-10-08
### A Critique of Self-Expressive Deep Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2010.03697v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03697v2)
- **Published**: 2020-10-08 00:14:59+00:00
- **Updated**: 2021-03-19 20:33:37+00:00
- **Authors**: Benjamin D. Haeffele, Chong You, Ren√© Vidal
- **Comment**: Published as a conference paper at the International Conference on
  Learning Representations (ICLR) 2021
- **Journal**: None
- **Summary**: Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model.



### Deep Tiered Image Segmentation For Detecting Internal Ice Layers in Radar Imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.03712v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03712v3)
- **Published**: 2020-10-08 01:13:03+00:00
- **Updated**: 2021-04-06 04:31:32+00:00
- **Authors**: Yuchen Wang, Mingze Xu, John Paden, Lora Koenig, Geoffrey Fox, David Crandall
- **Comment**: ICME version
- **Journal**: None
- **Summary**: Understanding the structure of Earth's polar ice sheets is important for modeling how global warming will impact polar ice and, in turn, the Earth's climate. Ground-penetrating radar is able to collect observations of the internal structure of snow and ice, but the process of manually labeling these observations is slow and laborious. Recent work has developed automatic techniques for finding the boundaries between the ice and the bedrock, but finding internal layers - the subtle boundaries that indicate where one year's ice accumulation ended and the next began - is much more challenging because the number of layers varies and the boundaries often merge and split. In this paper, we propose a novel deep neural network for solving a general class of tiered segmentation problems. We then apply it to detecting internal layers in polar ice, evaluating on a large-scale dataset of polar ice radar data with human-labeled annotations as ground truth.



### Distributionally Robust Learning for Uncertainty Calibration under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2010.05784v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05784v3)
- **Published**: 2020-10-08 02:10:54+00:00
- **Updated**: 2021-12-26 23:21:37+00:00
- **Authors**: Haoxuan Wang, Anqi Liu, Zhiding Yu, Junchi Yan, Yisong Yue, Anima Anandkumar
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We propose a framework for learning calibrated uncertainties under domain shifts. We consider the case where the source (training) distribution differs from the target (test) distribution. We detect such domain shifts through the use of a binary domain classifier and integrate it with the task network and train them jointly end-to-end. The binary domain classifier yields a density ratio that reflects the closeness of a target (test) sample to the source (training) distribution. We employ it to adjust the uncertainty of prediction in the task network. This idea of using the density ratio is based on the distributionally robust learning (DRL) framework, which accounts for the domain shift through adversarial risk minimization. We demonstrate that our method generates calibrated uncertainties that benefit many downstream tasks, such as unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). In these tasks, methods like self-training and FixMatch use uncertainties to select confident pseudo-labels for re-training. Our experiments show that the introduction of DRL leads to significant improvements in cross-domain performance. We also demonstrate that the estimated density ratios show agreement with the human selection frequencies, suggesting a positive correlation with a proxy of human perceived uncertainties.



### Decamouflage: A Framework to Detect Image-Scaling Attacks on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.03735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03735v1)
- **Published**: 2020-10-08 02:30:55+00:00
- **Updated**: 2020-10-08 02:30:55+00:00
- **Authors**: Bedeuro Kim, Alsharif Abuadbba, Yansong Gao, Yifeng Zheng, Muhammad Ejaz Ahmed, Hyoungshick Kim, Surya Nepal
- **Comment**: None
- **Journal**: None
- **Summary**: As an essential processing step in computer vision applications, image resizing or scaling, more specifically downsampling, has to be applied before feeding a normally large image into a convolutional neural network (CNN) model because CNN models typically take small fixed-size images as inputs. However, image scaling functions could be adversarially abused to perform a newly revealed attack called image-scaling attack, which can affect a wide range of computer vision applications building upon image-scaling functions.   This work presents an image-scaling attack detection framework, termed as Decamouflage. Decamouflage consists of three independent detection methods: (1) rescaling, (2) filtering/pooling, and (3) steganalysis. While each of these three methods is efficient standalone, they can work in an ensemble manner not only to improve the detection accuracy but also to harden potential adaptive attacks. Decamouflage has a pre-determined detection threshold that is generic. More precisely, as we have validated, the threshold determined from one dataset is also applicable to other different datasets. Extensive experiments show that Decamouflage achieves detection accuracy of 99.9\% and 99.8\% in the white-box (with the knowledge of attack algorithms) and the black-box (without the knowledge of attack algorithms) settings, respectively. To corroborate the efficiency of Decamouflage, we have also measured its run-time overhead on a personal PC with an i5 CPU and found that Decamouflage can detect image-scaling attacks in milliseconds. Overall, Decamouflage can accurately detect image scaling attacks in both white-box and black-box settings with acceptable run-time overhead.



### 3D Convolutional Sequence to Sequence Model for Vertebral Compression Fractures Identification in CT
- **Arxiv ID**: http://arxiv.org/abs/2010.03739v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03739v1)
- **Published**: 2020-10-08 02:39:40+00:00
- **Updated**: 2020-10-08 02:39:40+00:00
- **Authors**: David Chettrit, Tomer Meir, Hila Lebel, Mila Orlovsky, Ronen Gordon, Ayelet Akselrod-Ballin, Amir Bar
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: An osteoporosis-related fracture occurs every three seconds worldwide, affecting one in three women and one in five men aged over 50. The early detection of at-risk patients facilitates effective and well-evidenced preventative interventions, reducing the incidence of major osteoporotic fractures. In this study, we present an automatic system for identification of vertebral compression fractures on Computed Tomography images, which are often an undiagnosed precursor to major osteoporosis-related fractures. The system integrates a compact 3D representation of the spine, utilizing a Convolutional Neural Network (CNN) for spinal cord detection and a novel end-to-end sequence to sequence 3D architecture. We evaluate several model variants that exploit different representation and classification approaches and present a framework combining an ensemble of models that achieves state of the art results, validated on a large data set, with a patient-level fracture identification of 0.955 Area Under the Curve (AUC). The system proposed has the potential to support osteoporosis clinical management, improve treatment pathways, and to change the course of one of the most burdensome diseases of our generation.



### Bone Feature Segmentation in Ultrasound Spine Image with Robustness to Speckle and Regular Occlusion Noise
- **Arxiv ID**: http://arxiv.org/abs/2010.03740v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03740v1)
- **Published**: 2020-10-08 02:44:39+00:00
- **Updated**: 2020-10-08 02:44:39+00:00
- **Authors**: Zixun Huang, Li-Wen Wang, Frank H. F. Leung, Sunetra Banerjee, De Yang, Timothy Lee, Juan Lyu, Sai Ho Ling, Yong-Ping Zheng
- **Comment**: SMC2020
- **Journal**: None
- **Summary**: 3D ultrasound imaging shows great promise for scoliosis diagnosis thanks to its low-costing, radiation-free and real-time characteristics. The key to accessing scoliosis by ultrasound imaging is to accurately segment the bone area and measure the scoliosis degree based on the symmetry of the bone features. The ultrasound images tend to contain many speckles and regular occlusion noise which is difficult, tedious and time-consuming for experts to find out the bony feature. In this paper, we propose a robust bone feature segmentation method based on the U-net structure for ultrasound spine Volume Projection Imaging (VPI) images. The proposed segmentation method introduces a total variance loss to reduce the sensitivity of the model to small-scale and regular occlusion noise. The proposed approach improves 2.3% of Dice score and 1% of AUC score as compared with the u-net model and shows high robustness to speckle and regular occlusion noise.



### Visual News: Benchmark and Challenges in News Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2010.03743v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03743v3)
- **Published**: 2020-10-08 03:07:00+00:00
- **Updated**: 2021-09-13 18:53:35+00:00
- **Authors**: Fuxiao Liu, Yinghan Wang, Tianlu Wang, Vicente Ordonez
- **Comment**: 9 pages, 5 figures, accepted to EMNLP2021
- **Journal**: None
- **Summary**: We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as events and entities. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our method utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.



### Characterizing Datasets for Social Visual Question Answering, and the New TinySocial Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.11997v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CL, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11997v1)
- **Published**: 2020-10-08 03:20:23+00:00
- **Updated**: 2020-10-08 03:20:23+00:00
- **Authors**: Zhanwen Chen, Shiyao Li, Roxanne Rashedi, Xiaoman Zi, Morgan Elrod-Erickson, Bryan Hollis, Angela Maliakal, Xinyu Shen, Simeng Zhao, Maithilee Kunda
- **Comment**: To appear in the Joint IEEE International Conference on Development
  and Learning and on Epigenetic Robotics (ICDL), 2020
- **Journal**: None
- **Summary**: Modern social intelligence includes the ability to watch videos and answer questions about social and theory-of-mind-related content, e.g., for a scene in Harry Potter, "Is the father really upset about the boys flying the car?" Social visual question answering (social VQA) is emerging as a valuable methodology for studying social reasoning in both humans (e.g., children with autism) and AI agents. However, this problem space spans enormous variations in both videos and questions. We discuss methods for creating and characterizing social VQA datasets, including 1) crowdsourcing versus in-house authoring, including sample comparisons of two new datasets that we created (TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ dataset; 2) a new rubric for characterizing the difficulty and content of a given video; and 3) a new rubric for characterizing question types. We close by describing how having well-characterized social VQA datasets will enhance the explainability of AI agents and can also inform assessments and educational interventions for people.



### Generative Autoregressive Ensembles for Satellite Imagery Manipulation Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.03758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03758v1)
- **Published**: 2020-10-08 04:41:30+00:00
- **Updated**: 2020-10-08 04:41:30+00:00
- **Authors**: Daniel Mas Montserrat, J√°nos Horv√°th, S. K. Yarlagadda, Fengqing Zhu, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite imagery is becoming increasingly accessible due to the growing number of orbiting commercial satellites. Many applications make use of such images: agricultural management, meteorological prediction, damage assessment from natural disasters, or cartography are some of the examples. Unfortunately, these images can be easily tampered and modified with image manipulation tools damaging downstream applications. Because the nature of the manipulation applied to the image is typically unknown, unsupervised methods that don't require prior knowledge of the tampering techniques used are preferred. In this paper, we use ensembles of generative autoregressive models to model the distribution of the pixels of the image in order to detect potential manipulations. We evaluate the performance of the presented approach obtaining accurate localization results compared to previously presented approaches.



### ALFWorld: Aligning Text and Embodied Environments for Interactive Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.03768v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.03768v2)
- **Published**: 2020-10-08 05:13:36+00:00
- **Updated**: 2021-03-14 22:44:38+00:00
- **Authors**: Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, Matthew Hausknecht
- **Comment**: ICLR 2021; Data, code, and videos are available at alfworld.github.io
- **Journal**: None
- **Summary**: Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (C\^ot\'e et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).



### DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.03771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03771v1)
- **Published**: 2020-10-08 05:22:47+00:00
- **Updated**: 2020-10-08 05:22:47+00:00
- **Authors**: Ha Le, Ioannis A. Kakadiaris
- **Comment**: accepted to IJCB20
- **Journal**: None
- **Summary**: Deep learning-based domain-invariant feature learning methods are advancing in near-infrared and visible (NIR-VIS) heterogeneous face recognition. However, these methods are prone to overfitting due to the large intra-class variation and the lack of NIR images for training. In this paper, we introduce Domain-Based Label Face (DBLFace), a learning approach based on the assumption that a subject is not represented by a single label but by a set of labels. Each label represents images of a specific domain. In particular, a set of two labels per subject, one for the NIR images and one for the VIS images, are used for training a NIR-VIS face recognition model. The classification of images into different domains reduces the intra-class variation and lessens the negative impact of data imbalance in training. To train a network with sets of labels, we introduce a domain-based angular margin loss and a maximum angular loss to maintain the inter-class discrepancy and to enforce the close relationship of labels in a set. Quantitative experiments confirm that DBLFace significantly improves the rank-1 identification rate by 6.7% on the EDGE20 dataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0 dataset.



### Age and Gender Prediction From Face Images Using Attentional Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2010.03791v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03791v2)
- **Published**: 2020-10-08 06:33:55+00:00
- **Updated**: 2020-12-07 20:06:33+00:00
- **Authors**: Amirali Abdolrashidi, Mehdi Minaei, Elham Azimi, Shervin Minaee
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic prediction of age and gender from face images has drawn a lot of attention recently, due it is wide applications in various facial analysis problems. However, due to the large intra-class variation of face images (such as variation in lighting, pose, scale, occlusion), the existing models are still behind the desired accuracy level, which is necessary for the use of these models in real-world applications. In this work, we propose a deep learning framework, based on the ensemble of attentional and residual convolutional networks, to predict gender and age group of facial images with high accuracy rate. Using attention mechanism enables our model to focus on the important and informative parts of the face, which can help it to make a more accurate prediction. We train our model in a multi-task learning fashion, and augment the feature embedding of the age classifier, with the predicted gender, and show that doing so can further increase the accuracy of age prediction. Our model is trained on a popular face age and gender dataset, and achieved promising results. Through visualization of the attention maps of the train model, we show that our model has learned to become sensitive to the right regions of the face.



### A Comparative Study on Effects of Original and Pseudo Labels for Weakly Supervised Learning for Car Localization Problem
- **Arxiv ID**: http://arxiv.org/abs/2010.03815v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03815v2)
- **Published**: 2020-10-08 07:41:40+00:00
- **Updated**: 2022-11-21 08:27:51+00:00
- **Authors**: Cenk Bircanoglu
- **Comment**: Need to improve the results
- **Journal**: None
- **Summary**: In this study, the effects of different class labels created as a result of multiple conceptual meanings on localization using Weakly Supervised Learning presented on Car Dataset. In addition, the generated labels are included in the comparison, and the solution turned into Unsupervised Learning. This paper investigates multiple setups for car localization in the images with other approaches rather than Supervised Learning. To predict localization labels, Class Activation Mapping (CAM) is implemented and from the results, the bounding boxes are extracted by using morphological edge detection. Besides the original class labels, generated class labels also employed to train CAM on which turn to a solution to Unsupervised Learning example. In the experiments, we first analyze the effects of class labels in Weakly Supervised localization on the Compcars dataset. We then show that the proposed Unsupervised approach outperforms the Weakly Supervised method in this particular dataset by approximately %6.



### Improve Adversarial Robustness via Weight Penalization on Classification Layer
- **Arxiv ID**: http://arxiv.org/abs/2010.03844v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03844v1)
- **Published**: 2020-10-08 08:57:57+00:00
- **Updated**: 2020-10-08 08:57:57+00:00
- **Authors**: Cong Xu, Dan Li, Min Yang
- **Comment**: 22 pages, 10 figures, 43 references
- **Journal**: None
- **Summary**: It is well-known that deep neural networks are vulnerable to adversarial attacks. Recent studies show that well-designed classification parts can lead to better robustness. However, there is still much space for improvement along this line. In this paper, we first prove that, from a geometric point of view, the robustness of a neural network is equivalent to some angular margin condition of the classifier weights. We then explain why ReLU type function is not a good choice for activation under this framework. These findings reveal the limitations of the existing approaches and lead us to develop a novel light-weight-penalized defensive method, which is simple and has a good scalability. Empirical results on multiple benchmark datasets demonstrate that our method can effectively improve the robustness of the network without requiring too much additional computation, while maintaining a high classification precision for clean data.



### Dense Relational Image Captioning via Multi-task Triple-Stream Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.03855v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.03855v3)
- **Published**: 2020-10-08 09:17:55+00:00
- **Updated**: 2021-10-11 08:49:57+00:00
- **Authors**: Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon
- **Comment**: IEEE TPAMI accepted. Journal extension of our CVPR 2019 paper (
  arXiv:1903.05942 ). Source code :
  https://github.com/Dong-JinKim/DenseRelationalCaptioning
- **Journal**: None
- **Summary**: We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions for each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS; i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to learn not only to generate captions but also to understand the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. Then, we present applications of our framework to holistic image captioning, scene graph generation, and retrieval tasks.



### Clinically Verified Hybrid Deep Learning System for Retinal Ganglion Cells Aware Grading of Glaucomatous Progression
- **Arxiv ID**: http://arxiv.org/abs/2010.03872v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03872v1)
- **Published**: 2020-10-08 10:01:48+00:00
- **Updated**: 2020-10-08 10:01:48+00:00
- **Authors**: Hina Raja, Taimur Hassan, Muhammad Usman Akram, Naoufel Werghi
- **Comment**: Accepted in IEEE Transactions on Biomedical Engineering, Source Code:
  https://github.com/taimurhassan/rag-net-v2
- **Journal**: None
- **Summary**: Objective: Glaucoma is the second leading cause of blindness worldwide. Glaucomatous progression can be easily monitored by analyzing the degeneration of retinal ganglion cells (RGCs). Many researchers have screened glaucoma by measuring cup-to-disc ratios from fundus and optical coherence tomography scans. However, this paper presents a novel strategy that pays attention to the RGC atrophy for screening glaucomatous pathologies and grading their severity. Methods: The proposed framework encompasses a hybrid convolutional network that extracts the retinal nerve fiber layer, ganglion cell with the inner plexiform layer and ganglion cell complex regions, allowing thus a quantitative screening of glaucomatous subjects. Furthermore, the severity of glaucoma in screened cases is objectively graded by analyzing the thickness of these regions. Results: The proposed framework is rigorously tested on publicly available Armed Forces Institute of Ophthalmology (AFIO) dataset, where it achieved the F1 score of 0.9577 for diagnosing glaucoma, a mean dice coefficient score of 0.8697 for extracting the RGC regions and an accuracy of 0.9117 for grading glaucomatous progression. Furthermore, the performance of the proposed framework is clinically verified with the markings of four expert ophthalmologists, achieving a statistically significant Pearson correlation coefficient of 0.9236. Conclusion: An automated assessment of RGC degeneration yields better glaucomatous screening and grading as compared to the state-of-the-art solutions. Significance: An RGC-aware system not only screens glaucoma but can also grade its severity and here we present an end-to-end solution that is thoroughly evaluated on a standardized dataset and is clinically validated for analyzing glaucomatous pathologies.



### BGM: Building a Dynamic Guidance Map without Visual Images for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.03897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03897v1)
- **Published**: 2020-10-08 10:48:47+00:00
- **Updated**: 2020-10-08 10:48:47+00:00
- **Authors**: Beihao Xia, Conghao Wong, Heng Li, Shiming Chen, Qinmu Peng, Xinge You
- **Comment**: None
- **Journal**: None
- **Summary**: Visual images usually contain the informative context of the environment, thereby helping to predict agents' behaviors. However, they hardly impose the dynamic effects on agents' actual behaviors due to the respectively fixed semantics. To solve this problem, we propose a deterministic model named BGM to construct a guidance map to represent the dynamic semantics, which circumvents to use visual images for each agent to reflect the difference of activities in different periods. We first record all agents' activities in the scene within a period close to the current to construct a guidance map and then feed it to a Context CNN to obtain their context features. We adopt a Historical Trajectory Encoder to extract the trajectory features and then combine them with the context feature as the input of the social energy based trajectory decoder, thus obtaining the prediction that meets the social rules. Experiments demonstrate that BGM achieves state-of-the-art prediction accuracy on the two widely used ETH and UCY datasets and handles more complex scenarios.



### IRX-1D: A Simple Deep Learning Architecture for Remote Sensing Classifications
- **Arxiv ID**: http://arxiv.org/abs/2010.03902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03902v2)
- **Published**: 2020-10-08 11:07:02+00:00
- **Updated**: 2023-06-19 05:51:05+00:00
- **Authors**: Mahesh Pal, Akshay, B. Charan Teja
- **Comment**: Want to improve this manuscript as it is not accepted by journal in
  present form
- **Journal**: None
- **Summary**: We proposes a simple deep learning architecture combining elements of Inception, ResNet and Xception networks. Four new datasets were used for classification with both small and large training samples. Results in terms of classification accuracy suggests improved performance by proposed architecture in comparison to Bayesian optimised 2D-CNN with small training samples. Comparison of results using small training sample with Indiana Pines hyperspectral dataset suggests comparable or better performance by proposed architecture than nine reported works using different deep learning architectures. In spite of achieving high classification accuracy with limited training samples, comparison of classified image suggests different land cover classes are assigned to same area when compared with the classified image provided by the model trained using large training samples with all datasets.



### Free annotated data for deep learning in microscopy? A hitchhiker's guide
- **Arxiv ID**: http://arxiv.org/abs/2010.03988v1
- **DOI**: 10.1051/photon/202010430
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03988v1)
- **Published**: 2020-10-08 14:00:46+00:00
- **Updated**: 2020-10-08 14:00:46+00:00
- **Authors**: Adrian Shajkofci, Michael Liebling
- **Comment**: Accepted in Photoniques 104
- **Journal**: Photoniques 104 2020
- **Summary**: In microscopy, the time burden and cost of acquiring and annotating large datasets that many deep learning models take as a prerequisite, often appears to make these methods impractical. Can this requirement for annotated data be relaxed? Is it possible to borrow the knowledge gathered from datasets in other application fields and leverage it for microscopy? Here, we aim to provide an overview of methods that have recently emerged to successfully train learning-based methods in bio-microscopy.



### UESegNet: Context Aware Unconstrained ROI Segmentation Networks for Ear Biometric
- **Arxiv ID**: http://arxiv.org/abs/2010.03990v1
- **DOI**: 10.1007/s10044-020-00914-4
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03990v1)
- **Published**: 2020-10-08 14:05:15+00:00
- **Updated**: 2020-10-08 14:05:15+00:00
- **Authors**: Aman Kamboj, Rajneesh Rani, Aditya Nigam, Ranjeet Ranjan Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric-based personal authentication systems have seen a strong demand mainly due to the increasing concern in various privacy and security applications. Although the use of each biometric trait is problem dependent, the human ear has been found to have enough discriminating characteristics to allow its use as a strong biometric measure. To locate an ear in a 2D side face image is a challenging task, numerous existing approaches have achieved significant performance, but the majority of studies are based on the constrained environment. However, ear biometrics possess a great level of difficulties in the unconstrained environment, where pose, scale, occlusion, illuminations, background clutter etc. varies to a great extent. To address the problem of ear localization in the wild, we have proposed two high-performance region of interest (ROI) segmentation models UESegNet-1 and UESegNet-2, which are fundamentally based on deep convolutional neural networks and primarily uses contextual information to localize ear in the unconstrained environment. Additionally, we have applied state-of-the-art deep learning models viz; FRCNN (Faster Region Proposal Network) and SSD (Single Shot MultiBox Detecor) for ear localization task. To test the model's generalization, they are evaluated on six different benchmark datasets viz; IITD, IITK, USTB-DB3, UND-E, UND-J2 and UBEAR, all of which contain challenging images. The performance of the models is compared on the basis of object detection performance measure parameters such as IOU (Intersection Over Union), Accuracy, Precision, Recall, and F1-Score. It has been observed that the proposed models UESegNet-1 and UESegNet-2 outperformed the FRCNN and SSD at higher values of IOUs i.e. an accuracy of 100\% is achieved at IOU 0.5 on majority of the databases.



### Watch, read and lookup: learning to spot signs from multiple supervisors
- **Arxiv ID**: http://arxiv.org/abs/2010.04002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04002v1)
- **Published**: 2020-10-08 14:12:56+00:00
- **Updated**: 2020-10-08 14:12:56+00:00
- **Authors**: Liliane Momeni, G√ºl Varol, Samuel Albanie, Triantafyllos Afouras, Andrew Zisserman
- **Comment**: Appears in: Asian Conference on Computer Vision 2020 (ACCV 2020) -
  Oral presentation. 29 pages
- **Journal**: None
- **Summary**: The focus of this work is sign spotting - given a video of an isolated sign, our task is to identify whether and where it has been signed in a continuous, co-articulated sign language video. To achieve this sign spotting task, we train a model using multiple types of available supervision by: (1) watching existing sparsely labelled footage; (2) reading associated subtitles (readily available translations of the signed content) which provide additional weak-supervision; (3) looking up words (for which no co-articulated labelled examples are available) in visual sign language dictionaries to enable novel sign spotting. These three tasks are integrated into a unified learning framework using the principles of Noise Contrastive Estimation and Multiple Instance Learning. We validate the effectiveness of our approach on low-shot sign spotting benchmarks. In addition, we contribute a machine-readable British Sign Language (BSL) dictionary dataset of isolated signs, BSLDict, to facilitate study of this task. The dataset, models and code are available at our project page.



### Estimation of Camera Response Function using Prediction Consistency and Gradual Refinement with an Extension to Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.04009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04009v2)
- **Published**: 2020-10-08 14:19:48+00:00
- **Updated**: 2021-02-23 03:05:56+00:00
- **Authors**: Aashish Sharma, Robby T. Tan, Loong-Fah Cheong
- **Comment**: 11 Pages, 12 Figures
- **Journal**: None
- **Summary**: Most existing methods for CRF estimation from a single image fail to handle general real images. For instance, EdgeCRF based on colour patches extracted from edges works effectively only when the presence of noise is insignificant, which is not the case for many real images; and, CRFNet, a recent method based on fully supervised deep learning works only for the CRFs that are in the training data, and hence fail to deal with other possible CRFs beyond the training data. To address these problems, we introduce a non-deep-learning method using prediction consistency and gradual refinement. First, we rely more on the patches of the input image that provide more consistent predictions. If the predictions from a patch are more consistent, it means that the patch is likely to be less affected by noise or any inferior colour combinations, and hence, it can be more reliable for CRF estimation. Second, we employ a gradual refinement scheme in which we start from a simple CRF model to generate a result which is more robust to noise but less accurate, and then we gradually increase the model's complexity to improve the result. This is because a simple model, while being less accurate, overfits less to noise than a complex model does. Our experiments show that our method outperforms the existing single-image methods for daytime and nighttime real images. We further propose a more efficient deep learning extension that performs test-time training (based on unsupervised losses) on the test input image. This provides our method better generalization performance than CRFNet making it more practically applicable for CRF estimation for general real images.



### Spatially-Variant CNN-based Point Spread Function Estimation for Blind Deconvolution and Depth Estimation in Optical Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.04011v2
- **DOI**: 10.1109/TIP.2020.2986880
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04011v2)
- **Published**: 2020-10-08 14:20:16+00:00
- **Updated**: 2020-10-13 09:39:50+00:00
- **Authors**: Adrian Shajkofci, Michael Liebling
- **Comment**: Preprint
- **Journal**: IEEE Transactions on Image Processing, vol. 29, pp. 5848-5861,
  2020
- **Summary**: Optical microscopy is an essential tool in biology and medicine. Imaging thin, yet non-flat objects in a single shot (without relying on more sophisticated sectioning setups) remains challenging as the shallow depth of field that comes with high-resolution microscopes leads to unsharp image regions and makes depth localization and quantitative image interpretation difficult.   Here, we present a method that improves the resolution of light microscopy images of such objects by locally estimating image distortion while jointly estimating object distance to the focal plane. Specifically, we estimate the parameters of a spatially-variant Point-Spread function (PSF) model using a Convolutional Neural Network (CNN), which does not require instrument- or object-specific calibration. Our method recovers PSF parameters from the image itself with up to a squared Pearson correlation coefficient of 0.99 in ideal conditions, while remaining robust to object rotation, illumination variations, or photon noise. When the recovered PSFs are used with a spatially-variant and regularized Richardson-Lucy deconvolution algorithm, we observed up to 2.1 dB better signal-to-noise ratio compared to other blind deconvolution techniques. Following microscope-specific calibration, we further demonstrate that the recovered PSF model parameters permit estimating surface depth with a precision of 2 micrometers and over an extended range when using engineered PSFs. Our method opens up multiple possibilities for enhancing images of non-flat objects with minimal need for a priori knowledge about the optical setup.



### Frequency and Spatial domain based Saliency for Pigmented Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.04022v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04022v1)
- **Published**: 2020-10-08 14:38:42+00:00
- **Updated**: 2020-10-08 14:38:42+00:00
- **Authors**: Zanobya N. Khan
- **Comment**: 9 pages, 9 figures and 2 tables
- **Journal**: None
- **Summary**: Skin lesion segmentation can be rather a challenging task owing to the presence of artifacts, low contrast between lesion and boundary, color variegation, fuzzy skin lesion borders and heterogeneous background in dermoscopy images. In this paper, we propose a simple yet effective saliency-based approach derived in the frequency and spatial domain to detect pigmented skin lesion. Two color models are utilized for the construction of these maps. We suggest a different metric for each color model to design map in the spatial domain via color features. The map in the frequency domain is generated from aggregated images. We adopt a separate fusion scheme to combine salient features in their respective domains. Finally, two-phase saliency integration scheme is devised to combine these maps using pixelwise multiplication. Performance of the proposed method is assessed on PH2 and ISIC 2016 datasets. The outcome of the experiments suggests that the proposed scheme generate better segmentation result as compared to state-of-the-art methods.



### Weakly Supervised Learning of Multi-Object 3D Scene Decompositions Using Deep Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2010.04030v5
- **DOI**: 10.1016/j.cviu.2022.103440
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04030v5)
- **Published**: 2020-10-08 14:49:23+00:00
- **Updated**: 2022-05-03 08:54:37+00:00
- **Authors**: Cathrin Elich, Martin R. Oswald, Marc Pollefeys, Joerg Stueckler
- **Comment**: Preprint accepted to Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Representing scenes at the granularity of objects is a prerequisite for scene understanding and decision making. We propose PriSMONet, a novel approach based on Prior Shape knowledge for learning Multi-Object 3D scene decomposition and representations from single images. Our approach learns to decompose images of synthetic scenes with multiple objects on a planar surface into its constituent scene objects and to infer their 3D properties from a single view. A recurrent encoder regresses a latent representation of 3D shape, pose and texture of each object from an input RGB image. By differentiable rendering, we train our model to decompose scenes from RGB-D images in a self-supervised way. The 3D shapes are represented continuously in function-space as signed distance functions which we pre-train from example shapes in a supervised way. These shape priors provide weak supervision signals to better condition the challenging overall learning task. We evaluate the accuracy of our model in inferring 3D scene layout, demonstrate its generative capabilities, assess its generalization to real images, and point out benefits of the learned representation.



### Texture-based Presentation Attack Detection for Automatic Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/2010.04038v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2010.04038v1)
- **Published**: 2020-10-08 15:03:29+00:00
- **Updated**: 2020-10-08 15:03:29+00:00
- **Authors**: Lazaro J. Gonzalez-Soler, Jose Patino, Marta Gomez-Barrero, Massimiliano Todisco, Christoph Busch, Nicholas Evans
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric systems are nowadays employed across a broad range of applications. They provide high security and efficiency and, in many cases, are user friendly. Despite these and other advantages, biometric systems in general and Automatic speaker verification (ASV) systems in particular can be vulnerable to attack presentations. The most recent ASVSpoof 2019 competition showed that most forms of attacks can be detected reliably with ensemble classifier-based presentation attack detection (PAD) approaches. These, though, depend fundamentally upon the complementarity of systems in the ensemble. With the motivation to increase the generalisability of PAD solutions, this paper reports our exploration of texture descriptors applied to the analysis of speech spectrogram images. In particular, we propose a common fisher vector feature space based on a generative model. Experimental results show the soundness of our approach: at most, 16 in 100 bona fide presentations are rejected whereas only one in 100 attack presentations are accepted.



### Hierarchical Classification of Pulmonary Lesions: A Large-Scale Radio-Pathomics Study
- **Arxiv ID**: http://arxiv.org/abs/2010.04049v1
- **DOI**: 10.1007/978-3-030-59725-2_48
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04049v1)
- **Published**: 2020-10-08 15:14:34+00:00
- **Updated**: 2020-10-08 15:14:34+00:00
- **Authors**: Jiancheng Yang, Mingze Gao, Kaiming Kuang, Bingbing Ni, Yunlang She, Dong Xie, Chang Chen
- **Comment**: MICCAI 2020 (Early Accepted)
- **Journal**: None
- **Summary**: Diagnosis of pulmonary lesions from computed tomography (CT) is important but challenging for clinical decision making in lung cancer related diseases. Deep learning has achieved great success in computer aided diagnosis (CADx) area for lung cancer, whereas it suffers from label ambiguity due to the difficulty in the radiological diagnosis. Considering that invasive pathological analysis serves as the clinical golden standard of lung cancer diagnosis, in this study, we solve the label ambiguity issue via a large-scale radio-pathomics dataset containing 5,134 radiological CT images with pathologically confirmed labels, including cancers (e.g., invasive/non-invasive adenocarcinoma, squamous carcinoma) and non-cancer diseases (e.g., tuberculosis, hamartoma). This retrospective dataset, named Pulmonary-RadPath, enables development and validation of accurate deep learning systems to predict invasive pathological labels with a non-invasive procedure, i.e., radiological CT scans. A three-level hierarchical classification system for pulmonary lesions is developed, which covers most diseases in cancer-related diagnosis. We explore several techniques for hierarchical classification on this dataset, and propose a Leaky Dense Hierarchy approach with proven effectiveness in experiments. Our study significantly outperforms prior arts in terms of data scales (6x larger), disease comprehensiveness and hierarchies. The promising results suggest the potentials to facilitate precision medicine.



### A Unified Approach to Interpreting and Boosting Adversarial Transferability
- **Arxiv ID**: http://arxiv.org/abs/2010.04055v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04055v1)
- **Published**: 2020-10-08 15:19:22+00:00
- **Updated**: 2020-10-08 15:19:22+00:00
- **Authors**: Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we use the interaction inside adversarial perturbations to explain and boost the adversarial transferability. We discover and prove the negative correlation between the adversarial transferability and the interaction inside adversarial perturbations. The negative correlation is further verified through different DNNs with various inputs. Moreover, this negative correlation can be regarded as a unified perspective to understand current transferability-boosting methods. To this end, we prove that some classic methods of enhancing the transferability essentially decease interactions inside adversarial perturbations. Based on this, we propose to directly penalize interactions during the attacking process, which significantly improves the adversarial transferability.



### Regularized Compression of MRI Data: Modular Optimization of Joint Reconstruction and Coding
- **Arxiv ID**: http://arxiv.org/abs/2010.04065v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04065v2)
- **Published**: 2020-10-08 15:32:52+00:00
- **Updated**: 2020-11-09 15:01:34+00:00
- **Authors**: Veronica Corona, Yehuda Dar, Guy Williams, Carola-Bibiane Sch√∂nlieb
- **Comment**: None
- **Journal**: None
- **Summary**: The Magnetic Resonance Imaging (MRI) processing chain starts with a critical acquisition stage that provides raw data for reconstruction of images for medical diagnosis. This flow usually includes a near-lossless data compression stage that enables digital storage and/or transmission in binary formats. In this work we propose a framework for joint optimization of the MRI reconstruction and lossy compression, producing compressed representations of medical images that achieve improved trade-offs between quality and bit-rate. Moreover, we demonstrate that lossy compression can even improve the reconstruction quality compared to settings based on lossless compression. Our method has a modular optimization structure, implemented using the alternating direction method of multipliers (ADMM) technique and the state-of-the-art image compression technique (BPG) as a black-box module iteratively applied. This establishes a medical data compression approach compatible with a lossy compression standard of choice. A main novelty of the proposed algorithm is in the total-variation regularization added to the modular compression process, leading to decompressed images of higher quality without any additional processing at/after the decompression stage. Our experiments show that our regularization-based approach for joint MRI reconstruction and compression often achieves significant PSNR gains between 4 to 9 dB at high bit-rates compared to non-regularized solutions of the joint task. Compared to regularization-based solutions, our optimization method provides PSNR gains between 0.5 to 1 dB at high bit-rates, which is the range of interest for medical image compression.



### Are Adaptive Face Recognition Systems still Necessary? Experiments on the APE Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.04072v2
- **DOI**: 10.1109/IPAS50080.2020.9334946
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04072v2)
- **Published**: 2020-10-08 15:45:55+00:00
- **Updated**: 2020-10-17 14:36:11+00:00
- **Authors**: Giulia Orr√π, Marco Micheletto, Julian Fierrez, Gian Luca Marcialis
- **Comment**: Preprint version of a paper accepted at IPAS 2020 (Fourth IEEE
  International Conference on Image Processing, Applications and Systems)
- **Journal**: 2020 IEEE 4th International Conference on Image Processing,
  Applications and Systems (IPAS), Genova, Italy, 2020, pp. 77-82
- **Summary**: In the last five years, deep learning methods, in particular CNN, have attracted considerable attention in the field of face-based recognition, achieving impressive results. Despite this progress, it is not yet clear precisely to what extent deep features are able to follow all the intra-class variations that the face can present over time. In this paper we investigate the performance the performance improvement of face recognition systems by adopting self updating strategies of the face templates. For that purpose, we evaluate the performance of a well-known deep-learning face representation, namely, FaceNet, on a dataset that we generated explicitly conceived to embed intra-class variations of users on a large time span of captures: the APhotoEveryday (APE) dataset. Moreover, we compare these deep features with handcrafted features extracted using the BSIF algorithm. In both cases, we evaluate various template update strategies, in order to detect the most useful for such kind of features. Experimental results show the effectiveness of "optimized" self-update methods with respect to systems without update or random selection of templates.



### 3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2010.04075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04075v1)
- **Published**: 2020-10-08 15:57:06+00:00
- **Updated**: 2020-10-08 15:57:06+00:00
- **Authors**: Giorgia Pitteri, Aur√©lie Bugeau, Slobodan Ilic, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for detecting and estimating the 3D poses of objects in images that requires only an untextured CAD model and no training phase for new objects. Our approach combines Deep Learning and 3D geometry: It relies on an embedding of local 3D geometry to match the CAD models to the input images. For points at the surface of objects, this embedding can be computed directly from the CAD model; for image locations, we learn to predict it from the image itself. This establishes correspondences between 3D points on the CAD model and 2D locations of the input images. However, many of these correspondences are ambiguous as many points may have similar local geometries. We show that we can use Mask-RCNN in a class-agnostic way to detect the new objects without retraining and thus drastically limit the number of possible correspondences. We can then robustly estimate a 3D pose from these discriminative correspondences using a RANSAC- like algorithm. We demonstrate the performance of this approach on the T-LESS dataset, by using a small number of objects to learn the embedding and testing it on the other objects. Our experiments show that our method is on par or better than previous methods.



### Deep SVBRDF Estimation on Real Materials
- **Arxiv ID**: http://arxiv.org/abs/2010.04143v1
- **DOI**: 10.1109/3DV50981.2020.00126
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04143v1)
- **Published**: 2020-10-08 17:41:26+00:00
- **Updated**: 2020-10-08 17:41:26+00:00
- **Authors**: Louis-Philippe Asselin, Denis Laurendeau, Jean-Fran√ßois Lalonde
- **Comment**: Accepted submission to 3DV 2020. Project page
  https://lvsn.github.io/real-svbrdf
- **Journal**: None
- **Summary**: Recent work has demonstrated that deep learning approaches can successfully be used to recover accurate estimates of the spatially-varying BRDF (SVBRDF) of a surface from as little as a single image. Closer inspection reveals, however, that most approaches in the literature are trained purely on synthetic data, which, while diverse and realistic, is often not representative of the richness of the real world. In this paper, we show that training such networks exclusively on synthetic data is insufficient to achieve adequate results when tested on real data. Our analysis leverages a new dataset of real materials obtained with a novel portable multi-light capture apparatus. Through an extensive series of experiments and with the use of a novel deep learning architecture, we explore two strategies for improving results on real data: finetuning, and a per-material optimization procedure. We show that adapting network weights to real data is of critical importance, resulting in an approach which significantly outperforms previous methods for SVBRDF estimation on real materials. Dataset and code are available at https://lvsn.github.io/real-svbrdf



### A Survey On Anti-Spoofing Methods For Face Recognition with RGB Cameras of Generic Consumer Devices
- **Arxiv ID**: http://arxiv.org/abs/2010.04145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04145v1)
- **Published**: 2020-10-08 17:44:30+00:00
- **Updated**: 2020-10-08 17:44:30+00:00
- **Authors**: Zuheng Ming, Muriel Visani, Muhammad Muzzamil Luqman, Jean-Christophe Burie
- **Comment**: one-column, 53 pages,43 figures
- **Journal**: None
- **Summary**: The widespread deployment of face recognition-based biometric systems has made face Presentation Attack Detection (face anti-spoofing) an increasingly critical issue. This survey thoroughly investigates the face Presentation Attack Detection (PAD) methods, that only require RGB cameras of generic consumer devices, over the past two decades. We present an attack scenario-oriented typology of the existing face PAD methods and we provide a review of over 50 of the most recent face PAD methods and their related issues. We adopt a comprehensive presentation of the methods that have most influenced face PAD following the proposed typology, and in chronological order. By doing so, we depict the main challenges, evolutions and current trends in the field of face PAD, and provide insights on its future research. From an experimental point of view, this survey paper provides a summarized overview of the available public databases and extensive comparative experimental results of different PAD methods.



### Deformable DETR: Deformable Transformers for End-to-End Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.04159v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04159v4)
- **Published**: 2020-10-08 17:59:21+00:00
- **Updated**: 2021-03-18 03:14:26+00:00
- **Authors**: Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai
- **Comment**: ICLR 2021 Oral
- **Journal**: None
- **Summary**: DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.



### Efficient Real-Time Radial Distortion Correction for UAVs
- **Arxiv ID**: http://arxiv.org/abs/2010.04203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.04203v1)
- **Published**: 2020-10-08 18:34:56+00:00
- **Updated**: 2020-10-08 18:34:56+00:00
- **Authors**: Marcus Valtonen √ñrnhag, Patrik Persson, M√•rten Wadenb√§ck, Kalle √Östr√∂m, Anders Heyden
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2021, pp. 1751-1760
- **Summary**: In this paper we present a novel algorithm for onboard radial distortion correction for unmanned aerial vehicles (UAVs) equipped with an inertial measurement unit (IMU), that runs in real-time. This approach makes calibration procedures redundant, thus allowing for exchange of optics extemporaneously. By utilizing the IMU data, the cameras can be aligned with the gravity direction. This allows us to work with fewer degrees of freedom, and opens up for further intrinsic calibration. We propose a fast and robust minimal solver for simultaneously estimating the focal length, radial distortion profile and motion parameters from homographies. The proposed solver is tested on both synthetic and real data, and perform better or on par with state-of-the-art methods relying on pre-calibration procedures.



### Ensemble Hyperspectral Band Selection for Detecting Nitrogen Status in Grape Leaves
- **Arxiv ID**: http://arxiv.org/abs/2010.04225v2
- **DOI**: 10.1109/ICMLA51294.2020.00054
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04225v2)
- **Published**: 2020-10-08 19:09:10+00:00
- **Updated**: 2020-10-12 01:17:28+00:00
- **Authors**: Ryan Omidi, Ali Moghimi, Alireza Pourreza, Mohamed El-Hadedy, Anas Salah Eddin
- **Comment**: 8 PAGES, ACCEPTED IN 19TH IEEE INTERNATIONAL CONFERENCE ON MACHINE
  LEARNING AND APPLICATIONS (ICMLA 2020), MODIFIED ONE AUTHOR NAME
- **Journal**: None
- **Summary**: The large data size and dimensionality of hyperspectral data demands complex processing and data analysis. Multispectral data do not suffer the same limitations, but are normally restricted to blue, green, red, red edge, and near infrared bands. This study aimed to identify the optimal set of spectral bands for nitrogen detection in grape leaves using ensemble feature selection on hyperspectral data from over 3,000 leaves from 150 Flame Seedless table grapevines. Six machine learning base rankers were included in the ensemble: random forest, LASSO, SelectKBest, ReliefF, SVM-RFE, and chaotic crow search algorithm (CCSA). The pipeline identified less than 0.45% of the bands as most informative about grape nitrogen status. The selected violet, yellow-orange, and shortwave infrared bands lie outside of the typical blue, green, red, red edge, and near infrared bands of commercial multispectral cameras, so the potential improvement in remote sensing of nitrogen in grapevines brought forth by a customized multispectral sensor centered at the selected bands is promising and worth further investigation. The proposed pipeline may also be used for application-specific multispectral sensor design in domains other than agriculture.



### Fast Fourier Transformation for Optimizing Convolutional Neural Networks in Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.04257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04257v1)
- **Published**: 2020-10-08 21:07:55+00:00
- **Updated**: 2020-10-08 21:07:55+00:00
- **Authors**: Varsha Nair, Moitrayee Chatterjee, Neda Tavakoli, Akbar Siami Namin, Craig Snoeyink
- **Comment**: Pre-print of a paper to appear in the proceedings of the IEEE
  International Conference on Machine Learning Applications (ICMLA 2020), 10
  pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: This paper proposes to use Fast Fourier Transformation-based U-Net (a refined fully convolutional networks) and perform image convolution in neural networks. Leveraging the Fast Fourier Transformation, it reduces the image convolution costs involved in the Convolutional Neural Networks (CNNs) and thus reduces the overall computational costs. The proposed model identifies the object information from the images. We apply the Fast Fourier transform algorithm on an image data set to obtain more accessible information about the image data, before segmenting them through the U-Net architecture. More specifically, we implement the FFT-based convolutional neural network to improve the training time of the network. The proposed approach was applied to publicly available Broad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated improvement in training time during convolution from $600-700$ ms/step to $400-500$ ms/step. We evaluated the accuracy of our model using Intersection over Union (IoU) metric showing significant improvements.



### Refinement of Predicted Missing Parts Enhance Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2010.04278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.04278v1)
- **Published**: 2020-10-08 22:01:23+00:00
- **Updated**: 2020-10-08 22:01:23+00:00
- **Authors**: Alexis Mendoza, Alexander Apaza, Ivan Sipiran, Cristian Lopez
- **Comment**: 11 pages, 6 figures, conference
- **Journal**: None
- **Summary**: Point cloud completion is the task of predicting complete geometry from partial observations using a point set representation for a 3D shape. Previous approaches propose neural networks to directly estimate the whole point cloud through encoder-decoder models fed by the incomplete point set. By predicting the complete model, the current methods compute redundant information because the output also contains the known incomplete input geometry. This paper proposes an end-to-end neural network architecture that focuses on computing the missing geometry and merging the known input and the predicted point cloud. Our method is composed of two neural networks: the missing part prediction network and the merging-refinement network. The first module focuses on extracting information from the incomplete input to infer the missing geometry. The second module merges both point clouds and improves the distribution of the points. Our experiments on ShapeNet dataset show that our method outperforms the state-of-the-art methods in point cloud completion. The code of our methods and experiments is available in \url{https://github.com/ivansipiran/Refinement-Point-Cloud-Completion}.



### Large Scale Indexing of Generic Medical Image Data using Unbiased Shallow Keypoints and Deep CNN Features
- **Arxiv ID**: http://arxiv.org/abs/2010.04283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04283v2)
- **Published**: 2020-10-08 22:15:52+00:00
- **Updated**: 2020-10-20 19:31:49+00:00
- **Authors**: L. Chauvin, M. Ben Lazreg, J. B. Carluer, W. Wells, M. Toews
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: We propose a unified appearance model accounting for traditional shallow (i.e. 3D SIFT keypoints) and deep (i.e. CNN output layers) image feature representations, encoding respectively specific, localized neuroanatomical patterns and rich global information into a single indexing and classification framework. A novel Bayesian model combines shallow and deep features based on an assumption of conditional independence and validated by experiments indexing specific family members and general group categories in 3D MRI neuroimage data of 1010 subjects from the Human Connectome Project, including twins and non-twin siblings. A novel domain adaptation strategy is presented, transforming deep CNN vectors elements into binary class-informative descriptors. A GPU-based implementation of all processing is provided. State-of-the-art performance is achieved in large-scale neuroimage indexing, both in terms of computational complexity, accuracy in identifying family members and sex classification.



