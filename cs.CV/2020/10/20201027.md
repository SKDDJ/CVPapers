# Arxiv Papers in cs.CV on 2020-10-27
### Impact of Spherical Coordinates Transformation Pre-processing in Deep Convolution Neural Networks for Brain Tumor Segmentation and Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.13967v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13967v2)
- **Published**: 2020-10-27 00:33:03+00:00
- **Updated**: 2020-11-23 00:56:25+00:00
- **Authors**: Carlo Russo, Sidong Liu, Antonio Di Ieva
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-processing and Data Augmentation play an important role in Deep Convolutional Neural Networks (DCNN). Whereby several methods aim for standardization and augmentation of the dataset, we here propose a novel method aimed to feed DCNN with spherical space transformed input data that could better facilitate feature learning compared to standard Cartesian space images and volumes. In this work, the spherical coordinates transformation has been applied as a preprocessing method that, used in conjunction with normal MRI volumes, improves the accuracy of brain tumor segmentation and patient overall survival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020 dataset. The LesionEncoder framework has been then applied to automatically extract features from DCNN models, achieving 0.586 accuracy of OS prediction on the validation data set, which is one of the best results according to BraTS 2020 leaderboard.



### Decentralized Attribution of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2010.13974v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13974v4)
- **Published**: 2020-10-27 01:03:45+00:00
- **Updated**: 2021-04-28 13:04:51+00:00
- **Authors**: Changhoon Kim, Yi Ren, Yezhou Yang
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Growing applications of generative models have led to new threats such as malicious personation and digital copyright infringement. One solution to these threats is model attribution, i.e., the identification of user-end models where the contents under question are generated from. Existing studies showed empirical feasibility of attribution through a centralized classifier trained on all user-end models. However, this approach is not scalable in reality as the number of models ever grows. Neither does it provide an attributability guarantee. To this end, this paper studies decentralized attribution, which relies on binary classifiers associated with each user-end model. Each binary classifier is parameterized by a user-specific key and distinguishes its associated model distribution from the authentic data distribution. We develop sufficient conditions of the keys that guarantee an attributability lower bound. Our method is validated on MNIST, CelebA, and FFHQ datasets. We also examine the trade-off between generation quality and robustness of attribution against adversarial post-processes.



### Cross-directional Feature Fusion Network for Building Damage Assessment from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.14014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14014v2)
- **Published**: 2020-10-27 02:49:52+00:00
- **Updated**: 2020-11-18 19:07:27+00:00
- **Authors**: Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before an effective response is conducted. High-resolution satellite images provide rich information with pre- and post-disaster scenes for analysis. However, most existing works simply use pre- and post-disaster images as input without considering their correlations. In this paper, we propose a novel cross-directional fusion strategy to better explore the correlations between pre- and post-disaster images. Moreover, the data augmentation method CutMix is exploited to tackle the challenge of hard classes. The proposed method achieves state-of-the-art performance on a large-scale building damage assessment dataset -- xBD.



### Synthetic Training for Monocular Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/2010.14036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14036v1)
- **Published**: 2020-10-27 03:31:35+00:00
- **Updated**: 2020-10-27 03:31:35+00:00
- **Authors**: Yu Sun, Qian Bao, Wu Liu, Wenpeng Gao, Yili Fu, Chuang Gan, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering 3D human mesh from monocular images is a popular topic in computer vision and has a wide range of applications. This paper aims to estimate 3D mesh of multiple body parts (e.g., body, hands) with large-scale differences from a single RGB image. Existing methods are mostly based on iterative optimization, which is very time-consuming. We propose to train a single-shot model to achieve this goal. The main challenge is lacking training data that have complete 3D annotations of all body parts in 2D images. To solve this problem, we design a multi-branch framework to disentangle the regression of different body properties, enabling us to separate each component's training in a synthetic training manner using unpaired data available. Besides, to strengthen the generalization ability, most existing methods have used in-the-wild 2D pose datasets to supervise the estimated 3D pose via 3D-to-2D projection. However, we observe that the commonly used weak-perspective model performs poorly in dealing with the external foreshortening effect of camera projection. Therefore, we propose a depth-to-scale (D2S) projection to incorporate the depth difference into the projection function to derive per-joint scale variants for more proper supervision. The proposed method outperforms previous methods on the CMU Panoptic Studio dataset according to the evaluation results and achieves comparable results on the Human3.6M body and STB hand benchmarks. More impressively, the performance in close shot images gets significantly improved using the proposed D2S projection for weak supervision, while maintains obvious superiority in computational efficiency.



### Triple-view Convolutional Neural Networks for COVID-19 Diagnosis with Chest X-ray
- **Arxiv ID**: http://arxiv.org/abs/2010.14091v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14091v1)
- **Published**: 2020-10-27 06:15:32+00:00
- **Updated**: 2020-10-27 06:15:32+00:00
- **Authors**: Jianjia Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronavirus Disease 2019 (COVID-19) is affecting increasingly large number of people worldwide, posing significant stress to the health care systems. Early and accurate diagnosis of COVID-19 is critical in screening of infected patients and breaking the person-to-person transmission. Chest X-ray (CXR) based computer-aided diagnosis of COVID-19 using deep learning becomes a promising solution to this end. However, the diverse and various radiographic features of COVID-19 make it challenging, especially when considering each CXR scan typically only generates one single image. Data scarcity is another issue since collecting large-scale medical CXR data set could be difficult at present. Therefore, how to extract more informative and relevant features from the limited samples available becomes essential. To address these issues, unlike traditional methods processing each CXR image from a single view, this paper proposes triple-view convolutional neural networks for COVID-19 diagnosis with CXR images. Specifically, the proposed networks extract individual features from three views of each CXR image, i.e., the left lung view, the right lung view and the overall view, in three streams and then integrate them for joint diagnosis. The proposed network structure respects the anatomical structure of human lungs and is well aligned with clinical diagnosis of COVID-19 in practice. In addition, the labeling of the views does not require experts' domain knowledge, which is needed by many existing methods. The experimental results show that the proposed method achieves state-of-the-art performance, especially in the more challenging three class classification task, and admits wide generality and high flexibility.



### MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.14095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14095v1)
- **Published**: 2020-10-27 06:34:14+00:00
- **Updated**: 2020-10-27 06:34:14+00:00
- **Authors**: Aisha Urooj Khan, Amir Mazaheri, Niels da Vitoria Lobo, Mubarak Shah
- **Comment**: Accepted at Findings of EMNLP 2020
- **Journal**: None
- **Summary**: We present MMFT-BERT(MultiModal Fusion Transformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator's judgment. This set of questions helps us to study the model's behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.



### A Multi-task Two-stream Spatiotemporal Convolutional Neural Network for Convective Storm Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/2010.14100v2
- **DOI**: 10.1109/BigData50022.2020.9377890
- **Categories**: **cs.CV**, physics.ao-ph, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.14100v2)
- **Published**: 2020-10-27 07:11:53+00:00
- **Updated**: 2021-07-01 13:29:18+00:00
- **Authors**: W. Zhang, H. Liu, P. Li, L. Han
- **Comment**: 14 pages, 7 figures, 3 tables
- **Journal**: 2020 IEEE International Conference on Big Data (Big Data), 2020,
  pp. 3953-3960
- **Summary**: The goal of convective storm nowcasting is local prediction of severe and imminent convective storms. Here, we consider the convective storm nowcasting problem from the perspective of machine learning. First, we use a pixel-wise sampling method to construct spatiotemporal features for nowcasting, and flexibly adjust the proportions of positive and negative samples in the training set to mitigate class-imbalance issues. Second, we employ a concise two-stream convolutional neural network to extract spatial and temporal cues for nowcasting. This simplifies the network structure, reduces the training time requirement, and improves classification accuracy. The two-stream network used both radar and satellite data. In the resulting two-stream, fused convolutional neural network, some of the parameters are entered into a single-stream convolutional neural network, but it can learn the features of many data. Further, considering the relevance of classification and regression tasks, we develop a multi-task learning strategy that predicts the labels used in such tasks. We integrate two-stream multi-task learning into a single convolutional neural network. Given the compact architecture, this network is more efficient and easier to optimize than existing recurrent neural networks.



### Co-attentional Transformers for Story-Based Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2010.14104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.14104v1)
- **Published**: 2020-10-27 07:17:09+00:00
- **Updated**: 2020-10-27 07:17:09+00:00
- **Authors**: Björn Bebensee, Byoung-Tak Zhang
- **Comment**: 10 pages, 2 figures, submitted to ICASSP 2021
- **Journal**: None
- **Summary**: Inspired by recent trends in vision and language learning, we explore applications of attention mechanisms for visio-lingual fusion within an application to story-based video understanding. Like other video-based QA tasks, video story understanding requires agents to grasp complex temporal dependencies. However, as it focuses on the narrative aspect of video it also requires understanding of the interactions between different characters, as well as their actions and their motivations. We propose a novel co-attentional transformer model to better capture long-term dependencies seen in visual stories such as dramas and measure its performance on the video question answering task. We evaluate our approach on the recently introduced DramaQA dataset which features character-centered video story understanding questions. Our model outperforms the baseline model by 8 percentage points overall, at least 4.95 and up to 12.8 percentage points on all difficulty levels and manages to beat the winner of the DramaQA challenge.



### Micro-CT Synthesis and Inner Ear Super Resolution via Generative Adversarial Networks and Bayesian Inference
- **Arxiv ID**: http://arxiv.org/abs/2010.14105v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14105v2)
- **Published**: 2020-10-27 07:18:34+00:00
- **Updated**: 2021-02-04 23:25:19+00:00
- **Authors**: Hongwei Li, Rameshwara G. N. Prasad, Anjany Sekuboyina, Chen Niu, Siwei Bai, Werner Hemmert, Bjoern Menze
- **Comment**: final version in ISBI 2021
- **Journal**: None
- **Summary**: Existing medical image super-resolution methods rely on pairs of low- and high- resolution images to learn a mapping in a fully supervised manner. However, such image pairs are often not available in clinical practice. In this paper, we address super-resolution problem in a real-world scenario using unpaired data and synthesize linearly \textbf{eight times} higher resolved Micro-CT images of temporal bone structure, which is embedded in the inner ear. We explore cycle-consistency generative adversarial networks for super-resolution task and equip the translation approach with Bayesian inference. We further introduce \emph{Hu Moment distance} the evaluation metric to quantify the shape of the temporal bone. We evaluate our method on a public inner ear CT dataset and have seen both visual and quantitative improvement over state-of-the-art deep-learning-based methods. In addition, we perform a multi-rater visual evaluation experiment and find that trained experts consistently rate the proposed method the highest quality scores among all methods. Furthermore, we are able to quantify uncertainty in the unpaired translation task and the uncertainty map can provide structural information of the temporal bone.



### Hyperspectral Anomaly Change Detection Based on Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/2010.14119v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68U10, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2010.14119v1)
- **Published**: 2020-10-27 08:07:08+00:00
- **Updated**: 2020-10-27 08:07:08+00:00
- **Authors**: Meiqi Hu, Chen Wu, Liangpei Zhang, Bo Du
- **Comment**: 11 pages,9 figures,3 tables
- **Journal**: None
- **Summary**: With the hyperspectral imaging technology, hyperspectral data provides abundant spectral information and plays a more important role in geological survey, vegetation analysis and military reconnaissance. Different from normal change detection, hyperspectral anomaly change detection (HACD) helps to find those small but important anomaly changes between multi-temporal hyperspectral images (HSI). In previous works, most classical methods use linear regression to establish the mapping relationship between two HSIs and then detect the anomalies from the residual image. However, the real spectral differences between multi-temporal HSIs are likely to be quite complex and of nonlinearity, leading to the limited performance of these linear predictors. In this paper, we propose an original HACD algorithm based on auto-encoder (ACDA) to give a nonlinear solution. The proposed ACDA can construct an effective predictor model when facing complex imaging conditions. In the ACDA model, two systematic auto-encoder (AE) networks are deployed to construct two predictors from two directions. The predictor is used to model the spectral variation of the background to obtain the predicted image under another imaging condition. Then mean square error (MSE) between the predictive image and corresponding expected image is computed to obtain the loss map, where the spectral differences of the unchanged pixels are highly suppressed and anomaly changes are highlighted. Ultimately, we take the minimum of the two loss maps of two directions as the final anomaly change intensity map. The experiments results on public "Viareggio 2013" datasets demonstrate the efficiency and superiority over traditional methods.



### Mining Generalized Features for Detecting AI-Manipulated Fake Faces
- **Arxiv ID**: http://arxiv.org/abs/2010.14129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 62H30 (Primary) 14F05, 62H35 (Secondary), I.4
- **Links**: [PDF](http://arxiv.org/pdf/2010.14129v1)
- **Published**: 2020-10-27 08:41:16+00:00
- **Updated**: 2020-10-27 08:41:16+00:00
- **Authors**: Yang Yu, Rongrong Ni, Yao Zhao
- **Comment**: 14 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Recently, AI-manipulated face techniques have developed rapidly and constantly, which has raised new security issues in society. Although existing detection methods consider different categories of fake faces, the performance on detecting the fake faces with "unseen" manipulation techniques is still poor due to the distribution bias among cross-manipulation techniques. To solve this problem, we propose a novel framework that focuses on mining intrinsic features and further eliminating the distribution bias to improve the generalization ability. Firstly, we focus on mining the intrinsic clues in the channel difference image (CDI) and spectrum image (SI) from the camera imaging process and the indispensable step in AI manipulation process. Then, we introduce the Octave Convolution (OctConv) and an attention-based fusion module to effectively and adaptively mine intrinsic features from CDI and SI. Finally, we design an alignment module to eliminate the bias of manipulation techniques to obtain a more generalized detection framework. We evaluate the proposed framework on four categories of fake faces datasets with the most popular and state-of-the-art manipulation techniques, and achieve very competitive performances. To further verify the generalization ability of the proposed framework, we conduct experiments on cross-manipulation techniques, and the results show the advantages of our method.



### Reconstruction of Voxels with Position- and Angle-Dependent Weightings
- **Arxiv ID**: http://arxiv.org/abs/2010.14205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14205v1)
- **Published**: 2020-10-27 11:29:47+00:00
- **Updated**: 2020-10-27 11:29:47+00:00
- **Authors**: Lina Felsner, Tobias Würfl, Christopher Syben, Philipp Roser, Alexander Preuhs, Andreas Maier, Christian Riess
- **Comment**: This paper was originally published at the 6th International
  Conference on Image Formation in X-Ray Computed Tomography (CTmeeting 2020)
- **Journal**: None
- **Summary**: The reconstruction problem of voxels with individual weightings can be modeled a position- and angle- dependent function in the forward-projection. This changes the system matrix and prohibits to use standard filtered backprojection. In this work we first formulate this reconstruction problem in terms of a system matrix and weighting part. We compute the pseudoinverse and show that the solution is rank-deficient and hence very ill posed. This is a fundamental limitation for reconstruction. We then derive an iterative solution and experimentally show its uperiority to any closed-form solution.



### A Simple and Efficient Registration of 3D Point Cloud and Image Data for Indoor Mobile Mapping System
- **Arxiv ID**: http://arxiv.org/abs/2010.14261v1
- **DOI**: 10.1364/JOSAA.414042
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14261v1)
- **Published**: 2020-10-27 13:01:54+00:00
- **Updated**: 2020-10-27 13:01:54+00:00
- **Authors**: Hao Ma, Jingbin Liu, Keke Liu, Hongyu Qiu, Dong Xu, Zemin Wang, Xiaodong Gong, Sheng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Registration of 3D LiDAR point clouds with optical images is critical in the combination of multi-source data. Geometric misalignment originally exists in the pose data between LiDAR point clouds and optical images. To improve the accuracy of the initial pose and the applicability of the integration of 3D points and image data, we develop a simple but efficient registration method. We firstly extract point features from LiDAR point clouds and images: point features is extracted from single-frame LiDAR and point features from images using classical Canny method. Cost map is subsequently built based on Canny image edge detection. The optimization direction is guided by the cost map where low cost represents the the desired direction, and loss function is also considered to improve the robustness of the the purposed method. Experiments show pleasant results.



### End-to-end trainable network for degraded license plate detection via vehicle-plate relation mining
- **Arxiv ID**: http://arxiv.org/abs/2010.14266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14266v1)
- **Published**: 2020-10-27 13:05:31+00:00
- **Updated**: 2020-10-27 13:05:31+00:00
- **Authors**: Song-Lu Chen, Shu Tian, Jia-Wei Ma, Qi Liu, Chun Yang, Feng Chen, Xu-Cheng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: License plate detection is the first and essential step of the license plate recognition system and is still challenging in real applications, such as on-road scenarios. In particular, small-sized and oblique license plates, mainly caused by the distant and mobile camera, are difficult to detect. In this work, we propose a novel and applicable method for degraded license plate detection via vehicle-plate relation mining, which localizes the license plate in a coarse-to-fine scheme. First, we propose to estimate the local region around the license plate by using the relationships between the vehicle and the license plate, which can greatly reduce the search area and precisely detect very small-sized license plates. Second, we propose to predict the quadrilateral bounding box in the local region by regressing the four corners of the license plate to robustly detect oblique license plates. Moreover, the whole network can be trained in an end-to-end manner. Extensive experiments verify the effectiveness of our proposed method for small-sized and oblique license plates. Codes are available at https://github.com/chensonglu/LPD-end-to-end.



### A Method of Generating Measurable Panoramic Image for Indoor Mobile Measurement System
- **Arxiv ID**: http://arxiv.org/abs/2010.14270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14270v1)
- **Published**: 2020-10-27 13:12:02+00:00
- **Updated**: 2020-10-27 13:12:02+00:00
- **Authors**: Hao Ma, Jingbin Liu, Zhirong Hu, Hongyu Qiu, Dong Xu, Zemin Wang, Xiaodong Gong, Sheng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper designs a technique route to generate high-quality panoramic image with depth information, which involves two critical research hotspots: fusion of LiDAR and image data and image stitching. For the fusion of 3D points and image data, since a sparse depth map can be firstly generated by projecting LiDAR point onto the RGB image plane based on our reliable calibrated and synchronized sensors, we adopt a parameter self-adaptive framework to produce 2D dense depth map. For image stitching, optimal seamline for the overlapping area is searched using a graph-cuts-based method to alleviate the geometric influence and image blending based on the pyramid multi-band is utilized to eliminate the photometric effects near the stitching line. Since each pixel is associated with a depth value, we design this depth value as a radius in the spherical projection which can further project the panoramic image to the world coordinate and consequently produces a high-quality measurable panoramic image. The purposed method is tested on the data from our data collection platform and presents a satisfactory application prospects.



### Fast Local Attack: Generating Local Adversarial Examples for Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2010.14291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14291v1)
- **Published**: 2020-10-27 13:49:36+00:00
- **Updated**: 2020-10-27 13:49:36+00:00
- **Authors**: Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu
- **Comment**: Published in: 2020 International Joint Conference on Neural Networks
  (IJCNN)
- **Journal**: None
- **Summary**: The deep neural network is vulnerable to adversarial examples. Adding imperceptible adversarial perturbations to images is enough to make them fail. Most existing research focuses on attacking image classifiers or anchor-based object detectors, but they generate globally perturbation on the whole image, which is unnecessary. In our work, we leverage higher-level semantic information to generate high aggressive local perturbations for anchor-free object detectors. As a result, it is less computationally intensive and achieves a higher black-box attack as well as transferring attack performance. The adversarial examples generated by our method are not only capable of attacking anchor-free object detectors, but also able to be transferred to attack anchor-based object detector.



### Robust Odometry and Mapping for Multi-LiDAR Systems with Online Extrinsic Calibration
- **Arxiv ID**: http://arxiv.org/abs/2010.14294v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14294v2)
- **Published**: 2020-10-27 13:51:26+00:00
- **Updated**: 2021-05-05 15:32:33+00:00
- **Authors**: Jianhao Jiao, Haoyang Ye, Yilong Zhu, Ming Liu
- **Comment**: 20 pages, 22 figures, accepted by IEEE Transcation on Robotics
- **Journal**: None
- **Summary**: Combining multiple LiDARs enables a robot to maximize its perceptual awareness of environments and obtain sufficient measurements, which is promising for simultaneous localization and mapping (SLAM). This paper proposes a system to achieve robust and simultaneous extrinsic calibration, odometry, and mapping for multiple LiDARs. Our approach starts with measurement preprocessing to extract edge and planar features from raw measurements. After a motion and extrinsic initialization procedure, a sliding window-based multi-LiDAR odometry runs onboard to estimate poses with online calibration refinement and convergence identification. We further develop a mapping algorithm to construct a global map and optimize poses with sufficient features together with a method to model and reduce data uncertainty. We validate our approach's performance with extensive experiments on ten sequences (4.60km total length) for the calibration and SLAM and compare them against the state-of-the-art. We demonstrate that the proposed work is a complete, robust, and extensible system for various multi-LiDAR setups. The source code, datasets, and demonstrations are available at https://ram-lab.com/file/site/m-loam.



### Fit to Measure: Reasoning about Sizes for Robust Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.14296v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14296v1)
- **Published**: 2020-10-27 13:54:37+00:00
- **Updated**: 2020-10-27 13:54:37+00:00
- **Authors**: Agnese Chiatti, Enrico Motta, Enrico Daga, Gianluca Bardaro
- **Comment**: None
- **Journal**: None
- **Summary**: Service robots can help with many of our daily tasks, especially in those cases where it is inconvenient or unsafe for us to intervene: e.g., under extreme weather conditions or when social distance needs to be maintained. However, before we can successfully delegate complex tasks to robots, we need to enhance their ability to make sense of dynamic, real world environments. In this context, the first prerequisite to improving the Visual Intelligence of a robot is building robust and reliable object recognition systems. While object recognition solutions are traditionally based on Machine Learning methods, augmenting them with knowledge based reasoners has been shown to improve their performance. In particular, based on our prior work on identifying the epistemic requirements of Visual Intelligence, we hypothesise that knowledge of the typical size of objects could significantly improve the accuracy of an object recognition system. To verify this hypothesis, in this paper we present an approach to integrating knowledge about object sizes in a ML based architecture. Our experiments in a real world robotic scenario show that this combined approach ensures a significant performance increase over state of the art Machine Learning methods.



### Ice Monitoring in Swiss Lakes from Optical Satellites and Webcams using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.14300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.14300v1)
- **Published**: 2020-10-27 14:02:00+00:00
- **Updated**: 2020-10-27 14:02:00+00:00
- **Authors**: Manu Tom, Rajanie Prabha, Tianyu Wu, Emmanuel Baltsavias, Laura Leal-Taixe, Konrad Schindler
- **Comment**: Accepted for publication in MDPI Remote Sensing Journal
- **Journal**: None
- **Summary**: Continuous observation of climate indicators, such as trends in lake freezing, is important to understand the dynamics of the local and global climate system. Consequently, lake ice has been included among the Essential Climate Variables (ECVs) of the Global Climate Observing System (GCOS), and there is a need to set up operational monitoring capabilities. Multi-temporal satellite images and publicly available webcam streams are among the viable data sources to monitor lake ice. In this work we investigate machine learning-based image analysis as a tool to determine the spatio-temporal extent of ice on Swiss Alpine lakes as well as the ice-on and ice-off dates, from both multispectral optical satellite images (VIIRS and MODIS) and RGB webcam images. We model lake ice monitoring as a pixel-wise semantic segmentation problem, i.e., each pixel on the lake surface is classified to obtain a spatially explicit map of ice cover. We show experimentally that the proposed system produces consistently good results when tested on data from multiple winters and lakes. Our satellite-based method obtains mean Intersection-over-Union (mIoU) scores >93%, for both sensors. It also generalises well across lakes and winters with mIoU scores >78% and >80% respectively. On average, our webcam approach achieves mIoU values of 87% (approx.) and generalisation scores of 71% (approx.) and 69% (approx.) across different cameras and winters respectively. Additionally, we put forward a new benchmark dataset of webcam images (Photi-LakeIce) which includes data from two winters and three cameras.



### SIRI: Spatial Relation Induced Network For Spatial Description Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.14301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14301v1)
- **Published**: 2020-10-27 14:04:05+00:00
- **Updated**: 2020-10-27 14:04:05+00:00
- **Authors**: Peiyao Wang, Weixin Luo, Yanyu Xu, Haojie Li, Shugong Xu, Jianyu Yang, Shenghua Gao
- **Comment**: Accepted at NeurIPS 2020; Peiyao Wang and Weixin Luo made equal
  contribution
- **Journal**: None
- **Summary**: Spatial Description Resolution, as a language-guided localization task, is proposed for target location in a panoramic street view, given corresponding language descriptions. Explicitly characterizing an object-level relationship while distilling spatial relationships are currently absent but crucial to this task. Mimicking humans, who sequentially traverse spatial relationship words and objects with a first-person view to locate their target, we propose a novel spatial relationship induced (SIRI) network. Specifically, visual features are firstly correlated at an implicit object-level in a projected latent space; then they are distilled by each spatial relationship word, resulting in each differently activated feature representing each spatial relationship. Further, we introduce global position priors to fix the absence of positional information, which may result in global positional reasoning ambiguities. Both the linguistic and visual features are concatenated to finalize the target localization. Experimental results on the Touchdown show that our method is around 24\% better than the state-of-the-art method in terms of accuracy, measured by an 80-pixel radius. Our method also generalizes well on our proposed extended dataset collected using the same settings as Touchdown.



### Learning to Infer Unseen Attribute-Object Compositions
- **Arxiv ID**: http://arxiv.org/abs/2010.14343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14343v2)
- **Published**: 2020-10-27 14:57:35+00:00
- **Updated**: 2020-11-03 09:32:41+00:00
- **Authors**: Hui Chen, Zhixiong Nan, Jingjing Jiang, Nanning Zheng
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI), currently under review
- **Journal**: None
- **Summary**: The composition recognition of unseen attribute-object is critical to make machines learn to decompose and compose complex concepts like people. Most of the existing methods are limited to the composition recognition of single-attribute-object, and can hardly distinguish the compositions with similar appearances. In this paper, a graph-based model is proposed that can flexibly recognize both single- and multi-attribute-object compositions. The model maps the visual features of images and the attribute-object category labels represented by word embedding vectors into a latent space. Then, according to the constraints of the attribute-object semantic association, distances are calculated between visual features and the corresponding label semantic features in the latent space. During the inference, the composition that is closest to the given image feature among all compositions is used as the reasoning result. In addition, we build a large-scale Multi-Attribute Dataset (MAD) with 116,099 images and 8,030 composition categories. Experiments on MAD and two other single-attribute-object benchmark datasets demonstrate the effectiveness of our approach.



### CT Reconstruction with PDF: Parameter-Dependent Framework for Multiple Scanning Geometries and Dose Levels
- **Arxiv ID**: http://arxiv.org/abs/2010.14350v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14350v1)
- **Published**: 2020-10-27 15:05:32+00:00
- **Updated**: 2020-10-27 15:05:32+00:00
- **Authors**: Wenjun Xia, Zexin Lu, Yongqiang Huang, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Current mainstream of CT reconstruction methods based on deep learning usually needs to fix the scanning geometry and dose level, which will significantly aggravate the training cost and need more training data for clinical application. In this paper, we propose a parameter-dependent framework (PDF) which trains data with multiple scanning geometries and dose levels simultaneously. In the proposed PDF, the geometry and dose level are parameterized and fed into two multi-layer perceptrons (MLPs). The MLPs are leveraged to modulate the feature maps of CT reconstruction network, which condition the network outputs on different scanning geometries and dose levels. The experiments show that our proposed method can obtain competing performance similar to the original network trained with specific geometry and dose level, which can efficiently save the extra training cost for multiple scanning geometries and dose levels.



### Fourth-Order Nonlocal Tensor Decomposition Model for Spectral Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2010.14361v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14361v1)
- **Published**: 2020-10-27 15:14:36+00:00
- **Updated**: 2020-10-27 15:14:36+00:00
- **Authors**: Xiang Chen, Wenjun Xia, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang
- **Comment**: 4 pages, 3 figure
- **Journal**: None
- **Summary**: Spectral computed tomography (CT) can reconstruct spectral images from different energy bins using photon counting detectors (PCDs). However, due to the limited photons and counting rate in the corresponding spectral fraction, the reconstructed spectral images usually suffer from severe noise. In this paper, a fourth-order nonlocal tensor decomposition model for spectral CT image reconstruction (FONT-SIR) method is proposed. Similar patches are collected in both spatial and spectral dimensions simultaneously to form the basic tensor unit. Additionally, principal component analysis (PCA) is applied to extract latent features from the patches for a robust and efficient similarity measure. Then, low-rank and sparsity decomposition is performed on the produced fourth-order tensor unit, and the weighted nuclear norm and total variation (TV) norm are used to enforce the low-rank and sparsity constraints, respectively. The alternating direction method of multipliers (ADMM) is adopted to optimize the objective function. The experimental results with our proposed FONT-SIR demonstrates a superior qualitative and quantitative performance for both simulated and real data sets relative to several state-of-the-art methods, in terms of noise suppression and detail preservation.



### Pixel-based Facial Expression Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.14397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14397v1)
- **Published**: 2020-10-27 16:00:45+00:00
- **Updated**: 2020-10-27 16:00:45+00:00
- **Authors**: Arbish Akram, Nazar Khan
- **Comment**: ICPR 2020, 7 pages, 5 figures
- **Journal**: None
- **Summary**: Facial expression synthesis has achieved remarkable advances with the advent of Generative Adversarial Networks (GANs). However, GAN-based approaches mostly generate photo-realistic results as long as the testing data distribution is close to the training data distribution. The quality of GAN results significantly degrades when testing images are from a slightly different distribution. Moreover, recent work has shown that facial expressions can be synthesized by changing localized face regions. In this work, we propose a pixel-based facial expression synthesis method in which each output pixel observes only one input pixel. The proposed method achieves good generalization capability by leveraging only a few hundred training images. Experimental results demonstrate that the proposed method performs comparably well against state-of-the-art GANs on in-dataset images and significantly better on out-of-dataset images. In addition, the proposed model is two orders of magnitude smaller which makes it suitable for deployment on resource-constrained devices.



### On the Transfer of Disentangled Representations in Realistic Settings
- **Arxiv ID**: http://arxiv.org/abs/2010.14407v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.14407v2)
- **Published**: 2020-10-27 16:15:24+00:00
- **Updated**: 2021-03-11 11:43:10+00:00
- **Authors**: Andrea Dittadi, Frederik Träuble, Francesco Locatello, Manuel Wüthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, Bernhard Schölkopf
- **Comment**: Published at ICLR 2021
- **Journal**: None
- **Summary**: Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable. We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution. We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance.



### Improving Word Recognition using Multiple Hypotheses and Deep Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2010.14411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14411v1)
- **Published**: 2020-10-27 16:21:23+00:00
- **Updated**: 2020-10-27 16:21:23+00:00
- **Authors**: Siddhant Bansal, Praveen Krishnan, C. V. Jawahar
- **Comment**: 8 pages, 6 figures, Accepted in International Conference on Pattern
  Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: We propose a novel scheme for improving the word recognition accuracy using word image embeddings. We use a trained text recognizer, which can predict multiple text hypothesis for a given word image. Our fusion scheme improves the recognition process by utilizing the word image and text embeddings obtained from a trained word image embedding network. We propose EmbedNet, which is trained using a triplet loss for learning a suitable embedding space where the embedding of the word image lies closer to the embedding of the corresponding text transcription. The updated embedding space thus helps in choosing the correct prediction with higher confidence. To further improve the accuracy, we propose a plug-and-play module called Confidence based Accuracy Booster (CAB). The CAB module takes in the confidence scores obtained from the text recognizer and Euclidean distances between the embeddings to generate an updated distance vector. The updated distance vector has lower distance values for the correct words and higher distance values for the incorrect words. We rigorously evaluate our proposed method systematically on a collection of books in the Hindi language. Our method achieves an absolute improvement of around 10 percent in terms of word recognition accuracy.



### RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07949v2)
- **Published**: 2020-10-27 16:42:50+00:00
- **Updated**: 2021-03-15 10:52:53+00:00
- **Authors**: Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, Chuang Gan
- **Comment**: Accepted by AAAI-2021. Code and pre-trained models can be found at
  https://github.com/PeihaoChen/RSPNet
- **Journal**: None
- **Summary**: We study unsupervised video representation learning that seeks to learn both motion and appearance features from unlabeled video only, which can be reused for downstream tasks such as action recognition. This task, however, is extremely challenging due to 1) the highly complex spatial-temporal information in videos; and 2) the lack of labeled data for training. Unlike the representation learning for static images, it is difficult to construct a suitable self-supervised task to well model both motion and appearance features. More recently, several attempts have been made to learn video representation through video playback speed prediction. However, it is non-trivial to obtain precise speed labels for the videos. More critically, the learnt models may tend to focus on motion pattern and thus may not learn appearance features well. In this paper, we observe that the relative playback speed is more consistent with motion pattern, and thus provide more effective and stable supervision for representation learning. Therefore, we propose a new way to perceive the playback speed and exploit the relative speed between two video clips as labels. In this way, we are able to well perceive speed and learn better motion features. Moreover, to ensure the learning of appearance features, we further propose an appearance-focused task, where we enforce the model to perceive the appearance difference between two video clips. We show that optimizing the two tasks jointly consistently improves the performance on two downstream tasks, namely action recognition and video retrieval. Remarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy without the use of labeled data for pre-training, which outperforms the ImageNet supervised pre-trained model. Code and pre-trained models can be found at https://github.com/PeihaoChen/RSPNet.



### Structured Visual Search via Composition-aware Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.14438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14438v1)
- **Published**: 2020-10-27 16:52:03+00:00
- **Updated**: 2020-10-27 16:52:03+00:00
- **Authors**: Mert Kilickaya, Arnold W. M. Smeulders
- **Comment**: Accepted at WACV'21 (Algorithms Track)
- **Journal**: None
- **Summary**: This paper studies visual search using structured queries. The structure is in the form of a 2D composition that encodes the position and the category of the objects. The transformation of the position and the category of the objects leads to a continuous-valued relationship between visual compositions, which carries highly beneficial information, although not leveraged by previous techniques. To that end, in this work, our goal is to leverage these continuous relationships by using the notion of symmetry in equivariance. Our model output is trained to change symmetrically with respect to the input transformations, leading to a sensitive feature space. Doing so leads to a highly efficient search technique, as our approach learns from fewer data using a smaller feature space. Experiments on two large-scale benchmarks of MS-COCO and HICO-DET demonstrates that our approach leads to a considerable gain in the performance against competing techniques.



### Robust Skeletonization for Plant Root Structure Reconstruction from MRI
- **Arxiv ID**: http://arxiv.org/abs/2010.14440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14440v1)
- **Published**: 2020-10-27 16:54:40+00:00
- **Updated**: 2020-10-27 16:54:40+00:00
- **Authors**: Jannis Horn, Yi Zhao, Nils Wandel, Magdalena Landl, Andrea Schnepf, Sven Behnke
- **Comment**: Accepted final version. In 25th International Conference on Pattern
  Recognition (ICPR2020)
- **Journal**: None
- **Summary**: Structural reconstruction of plant roots from MRI is challenging, because of low resolution and low signal-to-noise ratio of the 3D measurements which may lead to disconnectivities and wrongly connected roots. We propose a two-stage approach for this task. The first stage is based on semantic root vs. soil segmentation and finds lowest-cost paths from any root voxel to the shoot. The second stage takes the largest fully connected component generated in the first stage and uses 3D skeletonization to extract a graph structure. We evaluate our method on 22 MRI scans and compare to human expert reconstructions.



### Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.14462v2
- **DOI**: None
- **Categories**: **cs.LG**, astro-ph.IM, cs.CV, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.14462v2)
- **Published**: 2020-10-27 17:23:09+00:00
- **Updated**: 2020-12-17 06:13:58+00:00
- **Authors**: He Sun, Katherine L. Bouman
- **Comment**: This paper has been accepted to AAAI 2021. Keywords: Computational
  Imaging, Normalizing Flow, Uncertainty Quantification, Interferometry, MRI
- **Journal**: None
- **Summary**: Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with underdetermined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging (MRI).



### Artificial intelligence based writer identification generates new evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the Great Isaiah Scroll (1QIsaa)
- **Arxiv ID**: http://arxiv.org/abs/2010.14476v1
- **DOI**: 10.1371/journal.pone.0249769
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14476v1)
- **Published**: 2020-10-27 17:36:18+00:00
- **Updated**: 2020-10-27 17:36:18+00:00
- **Authors**: Mladen Popović, Maruf A. Dhali, Lambert Schomaker
- **Comment**: 23 pages, 15 pages of supplementary materials, submitted to PLOS ONE
  on 19 October 2019
- **Journal**: PLoS ONE 2021
- **Summary**: The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal culture. Palaeography - the study of ancient handwriting - can provide access to this scribal culture. However, one of the problems of traditional palaeography is to determine writer identity when the writing style is near uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end, we used pattern recognition and artificial intelligence techniques to innovate the palaeography of the scrolls regarding writer identification and to pioneer the microlevel of individual scribes to open access to the Bible's ancient scribal culture. Although many scholars believe that 1QIsaa was written by one scribe, we report new evidence for a breaking point in the series of columns in this scroll. Without prior assumption of writer identity, based on point clouds of the reduced-dimensionality feature-space, we found that columns from the first and second halves of the manuscript ended up in two distinct zones of such scatter plots, notably for a range of digital palaeography tools, each addressing very different featural aspects of the script samples. In a secondary, independent, analysis, now assuming writer difference and using yet another independent feature method and several different types of statistical testing, a switching point was found in the column series. A clear phase transition is apparent around column 27. Given the statistically significant differences between the two halves, a tertiary, post-hoc analysis was performed. Demonstrating that two main scribes were responsible for the Great Isaiah Scroll, this study sheds new light on the Bible's ancient scribal culture by providing new, tangible evidence that ancient biblical texts were not copied by a single scribe only but that multiple scribes could closely collaborate on one particular manuscript.



### Memory Optimization for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.14501v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14501v3)
- **Published**: 2020-10-27 17:57:34+00:00
- **Updated**: 2021-04-03 00:32:47+00:00
- **Authors**: Aashaka Shah, Chao-Yuan Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krähenbühl
- **Comment**: 18 pages, ICLR'21
- **Journal**: None
- **Summary**: Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. Our code is available at https://github.com/utsaslab/MONeT.



### Neural Architecture Search of SPD Manifold Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.14535v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.14535v4)
- **Published**: 2020-10-27 18:08:57+00:00
- **Updated**: 2021-06-13 21:24:32+00:00
- **Authors**: Rhea Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Erik Goron Endsjo, Yan Wu, Luc Van Gool
- **Comment**: This paper is accepted for publication at IJCAI 2021
- **Journal**: None
- **Summary**: In this paper, we propose a new neural architecture search (NAS) problem of Symmetric Positive Definite (SPD) manifold networks, aiming to automate the design of SPD neural architectures. To address this problem, we first introduce a geometrically rich and diverse SPD neural architecture search space for an efficient SPD cell design. Further, we model our new NAS problem with a one-shot training process of a single supernet. Based on the supernet modeling, we exploit a differentiable NAS algorithm on our relaxed continuous search space for SPD neural architecture search. Statistical evaluation of our method on drone, action, and emotion recognition tasks mostly provides better results than the state-of-the-art SPD networks and traditional NAS algorithms. Empirical results show that our algorithm excels in discovering better performing SPD network design and provides models that are more than three times lighter than searched by the state-of-the-art NAS algorithms.



### Perception for Autonomous Systems (PAZ)
- **Arxiv ID**: http://arxiv.org/abs/2010.14541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.14541v1)
- **Published**: 2020-10-27 18:22:01+00:00
- **Updated**: 2020-10-27 18:22:01+00:00
- **Authors**: Octavio Arriaga, Matias Valdenegro-Toro, Mohandass Muthuraja, Sushma Devaramani, Frank Kirchner
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce the Perception for Autonomous Systems (PAZ) software library. PAZ is a hierarchical perception library that allow users to manipulate multiple levels of abstraction in accordance to their requirements or skill level. More specifically, PAZ is divided into three hierarchical levels which we refer to as pipelines, processors, and backends. These abstractions allows users to compose functions in a hierarchical modular scheme that can be applied for preprocessing, data-augmentation, prediction and postprocessing of inputs and outputs of machine learning (ML) models. PAZ uses these abstractions to build reusable training and prediction pipelines for multiple robot perception tasks such as: 2D keypoint estimation, 2D object detection, 3D keypoint discovery, 6D pose estimation, emotion classification, face recognition, instance segmentation, and attention mechanisms.



### Unsupervised Domain Adaptation for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2010.14543v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.14543v2)
- **Published**: 2020-10-27 18:22:43+00:00
- **Updated**: 2020-11-12 17:41:51+00:00
- **Authors**: Shangda Li, Devendra Singh Chaplot, Yao-Hung Hubert Tsai, Yue Wu, Louis-Philippe Morency, Ruslan Salakhutdinov
- **Comment**: Deep Reinforcement Learning Workshop at NeurIPS 2020. Camera Ready
  Version
- **Journal**: None
- **Summary**: Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world.



### Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.14551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14551v1)
- **Published**: 2020-10-27 18:41:49+00:00
- **Updated**: 2020-10-27 18:41:49+00:00
- **Authors**: Iro Laina, Ruth C. Fong, Andrea Vedaldi
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: The increasing impact of black box models, and particularly of unsupervised ones, comes with an increasing interest in tools to understand and interpret them. In this paper, we consider in particular how to characterise visual groupings discovered automatically by deep neural networks, starting with state-of-the-art clustering methods. In some cases, clusters readily correspond to an existing labelled dataset. However, often they do not, yet they still maintain an "intuitive interpretability". We introduce two concepts, visual learnability and describability, that can be used to quantify the interpretability of arbitrary image groupings, including unsupervised ones. The idea is to measure (1) how well humans can learn to reproduce a grouping by measuring their ability to generalise from a small set of visual examples (learnability) and (2) whether the set of visual examples can be replaced by a succinct, textual description (describability). By assessing human annotators as classifiers, we remove the subjective quality of existing evaluation metrics. For better scalability, we finally propose a class-level captioning system to generate descriptions for visual groupings automatically and compare it to human annotators using the describability metric.



### Contour Integration using Graph-Cut and Non-Classical Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/2010.14561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14561v2)
- **Published**: 2020-10-27 19:07:13+00:00
- **Updated**: 2021-05-10 21:07:53+00:00
- **Authors**: Zahra Mousavi Kouzehkanan, Reshad Hosseini, Babak Nadjar Araabi
- **Comment**: None
- **Journal**: None
- **Summary**: Many edge and contour detection algorithms give a soft-value as an output and the final binary map is commonly obtained by applying an optimal threshold. In this paper, we propose a novel method to detect image contours from the extracted edge segments of other algorithms. Our method is based on an undirected graphical model with the edge segments set as the vertices. The proposed energy functions are inspired by the surround modulation in the primary visual cortex that help suppressing texture noise. Our algorithm can improve extracting the binary map, because it considers other important factors such as connectivity, smoothness, and length of the contour beside the soft-values. Our quantitative and qualitative experimental results show the efficacy of the proposed method.



### Nested Grassmannians for Dimensionality Reduction with Applications
- **Arxiv ID**: http://arxiv.org/abs/2010.14589v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2010.14589v3)
- **Published**: 2020-10-27 20:09:12+00:00
- **Updated**: 2022-03-01 10:33:55+00:00
- **Authors**: Chun-Hao Yang, Baba C. Vemuri
- **Comment**: 21 pages, 9 figures. Accepted for publication at the Journal of
  Machine Learning for Biomedical Imaging (MELBA) https://www.melba-journal.org
- **Journal**: None
- **Summary**: In the recent past, nested structures in Riemannian manifolds has been studied in the context of dimensionality reduction as an alternative to the popular principal geodesic analysis (PGA) technique, for example, the principal nested spheres. In this paper, we propose a novel framework for constructing a nested sequence of homogeneous Riemannian manifolds. Common examples of homogeneous Riemannian manifolds include the $n$-sphere, the Stiefel manifold, the Grassmann manifold and many others. In particular, we focus on applying the proposed framework to the Grassmann manifold, giving rise to the nested Grassmannians (NG). An important application in which Grassmann manifolds are encountered is planar shape analysis. Specifically, each planar (2D) shape can be represented as a point in the complex projective space which is a complex Grass-mann manifold. Some salient features of our framework are: (i) it explicitly exploits the geometry of the homogeneous Riemannian manifolds and (ii) the nested lower-dimensional submanifolds need not be geodesic. With the proposed NG structure, we develop algorithms for the supervised and unsupervised dimensionality reduction problems respectively. The proposed algorithms are compared with PGA via simulation studies and real data experiments and are shown to achieve a higher ratio of expressed variance compared to PGA.



### Stereo Frustums: A Siamese Pipeline for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.14599v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14599v2)
- **Published**: 2020-10-27 20:46:17+00:00
- **Updated**: 2020-11-08 15:16:07+00:00
- **Authors**: Xi Mo, Usman Sajid, Guanghui Wang
- **Comment**: Accepted by Journal of Intelligent & Robotic Systems (JIRS)
- **Journal**: None
- **Summary**: The paper proposes a light-weighted stereo frustums matching module for 3D objection detection. The proposed framework takes advantage of a high-performance 2D detector and a point cloud segmentation network to regress 3D bounding boxes for autonomous driving vehicles. Instead of performing traditional stereo matching to compute disparities, the module directly takes the 2D proposals from both the left and the right views as input. Based on the epipolar constraints recovered from the well-calibrated stereo cameras, we propose four matching algorithms to search for the best match for each proposal between the stereo image pairs. Each matching pair proposes a segmentation of the scene which is then fed into a 3D bounding box regression network. Results of extensive experiments on KITTI dataset demonstrate that the proposed Siamese pipeline outperforms the state-of-the-art stereo-based 3D bounding box regression methods.



### Deformable Convolutional LSTM for Human Body Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.14607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14607v1)
- **Published**: 2020-10-27 21:01:09+00:00
- **Updated**: 2020-10-27 21:01:09+00:00
- **Authors**: Peyman Tahghighi, Abbas Koochari, Masoume Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: People represent their emotions in a myriad of ways. Among the most important ones is whole body expressions which have many applications in different fields such as human-computer interaction (HCI). One of the most important challenges in human emotion recognition is that people express the same feeling in various ways using their face and their body. Recently many methods have tried to overcome these challenges using Deep Neural Networks (DNNs). However, most of these methods were based on images or on facial expressions only and did not consider deformation that may happen in the images such as scaling and rotation which can adversely affect the recognition accuracy. In this work, motivated by recent researches on deformable convolutions, we incorporate the deformable behavior into the core of convolutional long short-term memory (ConvLSTM) to improve robustness to these deformations in the image and, consequently, improve its accuracy on the emotion recognition task from videos of arbitrary length. We did experiments on the GEMEP dataset and achieved state-of-the-art accuracy of 98.8% on the task of whole human body emotion recognition on the validation set.



