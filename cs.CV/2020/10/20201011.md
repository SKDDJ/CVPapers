# Arxiv Papers in cs.CV on 2020-10-11
### Towards Hardware-Agnostic Gaze-Trackers
- **Arxiv ID**: http://arxiv.org/abs/2010.05123v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05123v1)
- **Published**: 2020-10-11 00:53:57+00:00
- **Updated**: 2020-10-11 00:53:57+00:00
- **Authors**: Jatin Sharma, Jon Campbell, Pete Ansell, Jay Beavers, Christopher O'Dowd
- **Comment**: None
- **Journal**: None
- **Summary**: Gaze-tracking is a novel way of interacting with computers which allows new scenarios, such as enabling people with motor-neuron disabilities to control their computers or doctors to interact with patient information without touching screen or keyboard. Further, there are emerging applications of gaze-tracking in interactive gaming, user experience research, human attention analysis and behavioral studies. Accurate estimation of the gaze may involve accounting for head-pose, head-position, eye rotation, distance from the object as well as operating conditions such as illumination, occlusion, background noise and various biological aspects of the user. Commercially available gaze-trackers utilize specialized sensor assemblies that usually consist of an infrared light source and camera. There are several challenges in the universal proliferation of gaze-tracking as accessibility technologies, specifically its affordability, reliability, and ease-of-use. In this paper, we try to address these challenges through the development of a hardware-agnostic gaze-tracker. We present a deep neural network architecture as an appearance-based method for constrained gaze-tracking that utilizes facial imagery captured on an ordinary RGB camera ubiquitous in all modern computing devices. Our system achieved an error of 1.8073cm on GazeCapture dataset without any calibration or device specific fine-tuning. This research shows promise that one day soon any computer, tablet, or phone will be controllable using just your eyes due to the prediction capabilities of deep neutral networks.



### Segmenting Epipolar Line
- **Arxiv ID**: http://arxiv.org/abs/2010.05131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05131v1)
- **Published**: 2020-10-11 01:20:55+00:00
- **Updated**: 2020-10-11 01:20:55+00:00
- **Authors**: Shengjie Li, Qi Cai, Yuanxin Wu
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Identifying feature correspondence between two images is a fundamental procedure in three-dimensional computer vision. Usually the feature search space is confined by the epipolar line. Using the cheirality constraint, this paper finds that the feature search space can be restrained to one of two or three segments of the epipolar line that are defined by the epipole and a so-called virtual infinity point.



### SDMTL: Semi-Decoupled Multi-grained Trajectory Learning for 3D human motion prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.05133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05133v1)
- **Published**: 2020-10-11 01:29:21+00:00
- **Updated**: 2020-10-11 01:29:21+00:00
- **Authors**: Xiaoli Liu, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting future human motion is critical for intelligent robots to interact with humans in the real world, and human motion has the nature of multi-granularity. However, most of the existing work either implicitly modeled multi-granularity information via fixed modes or focused on modeling a single granularity, making it hard to well capture this nature for accurate predictions. In contrast, we propose a novel end-to-end network, Semi-Decoupled Multi-grained Trajectory Learning network (SDMTL), to predict future poses, which not only flexibly captures rich multi-grained trajectory information but also aggregates multi-granularity information for predictions. Specifically, we first introduce a Brain-inspired Semi-decoupled Motion-sensitive Encoding module (BSME), effectively capturing spatiotemporal features in a semi-decoupled manner. Then, we capture the temporal dynamics of motion trajectory at multi-granularity, including fine granularity and coarse granularity. We learn multi-grained trajectory information using BSMEs hierarchically and further capture the information of temporal evolutional directions at each granularity by gathering the outputs of BSMEs at each granularity and applying temporal convolutions along the motion trajectory. Next, the captured motion dynamics can be further enhanced by aggregating the information of multi-granularity with a weighted summation scheme. Finally, experimental results on two benchmarks, including Human3.6M and CMU-Mocap, show that our method achieves state-of-the-art performance, demonstrating the effectiveness of our proposed method. The code will be available if the paper is accepted.



### SDE-AWB: a Generic Solution for 2nd International Illumination Estimation Challenge
- **Arxiv ID**: http://arxiv.org/abs/2010.05149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05149v1)
- **Published**: 2020-10-11 03:31:49+00:00
- **Updated**: 2020-10-11 03:31:49+00:00
- **Authors**: Yanlin Qian, Sibo Feng, Kang Qian, Miaofeng Wang
- **Comment**: 6 pages; challenge paper for 2nd International Illumination
  Estimation Challenge, held in International Conference on Machine Vision 2020
- **Journal**: None
- **Summary**: We propose a neural network-based solution for three different tracks of 2nd International Illumination Estimation Challenge (chromaticity.iitp.ru). Our method is built on pre-trained Squeeze-Net backbone, differential 2D chroma histogram layer and a shallow MLP utilizing Exif information. By combining semantic feature, color feature and Exif metadata, the resulting method -- SDE-AWB -- obtains 1st place in both indoor and two-illuminant tracks and 2nd place in general track.



### MammoGANesis: Controlled Generation of High-Resolution Mammograms for Radiology Education
- **Arxiv ID**: http://arxiv.org/abs/2010.05177v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05177v1)
- **Published**: 2020-10-11 06:47:56+00:00
- **Updated**: 2020-10-11 06:47:56+00:00
- **Authors**: Cyril Zakka, Ghida Saheb, Elie Najem, Ghina Berjawi
- **Comment**: None
- **Journal**: None
- **Summary**: During their formative years, radiology trainees are required to interpret hundreds of mammograms per month, with the objective of becoming apt at discerning the subtle patterns differentiating benign from malignant lesions. Unfortunately, medico-legal and technical hurdles make it difficult to access and query medical images for training.   In this paper we train a generative adversarial network (GAN) to synthesize 512 x 512 high-resolution mammograms. The resulting model leads to the unsupervised separation of high-level features (e.g. the standard mammography views and the nature of the breast lesions), with stochastic variation in the generated images (e.g. breast adipose tissue, calcification), enabling user-controlled global and local attribute-editing of the synthesized images.   We demonstrate the model's ability to generate anatomically and medically relevant mammograms by achieving an average AUC of 0.54 in a double-blind study on four expert mammography radiologists to distinguish between generated and real images, ascribing to the high visual quality of the synthesized and edited mammograms, and to their potential use in advancing and facilitating medical education.



### Constructing a Visual Relationship Authenticity Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.05185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.05185v1)
- **Published**: 2020-10-11 07:38:33+00:00
- **Updated**: 2020-10-11 07:38:33+00:00
- **Authors**: Chenhui Chu, Yuto Takebayashi, Mishra Vipul, Yuta Nakashima
- **Comment**: None
- **Journal**: None
- **Summary**: A visual relationship denotes a relationship between two objects in an image, which can be represented as a triplet of (subject; predicate; object). Visual relationship detection is crucial for scene understanding in images. Existing visual relationship detection datasets only contain true relationships that correctly describe the content in an image. However, distinguishing false visual relationships from true ones is also crucial for image understanding and grounded natural language processing. In this paper, we construct a visual relationship authenticity dataset, where both true and false relationships among all objects appeared in the captions in the Flickr30k entities image caption dataset are annotated. The dataset is available at https://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this dataset can promote the study on both vision and language understanding.



### Tackling problems of marker-based augmented reality under water
- **Arxiv ID**: http://arxiv.org/abs/2010.11691v1
- **DOI**: 10.1007/978-3-030-37191-3_11
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.11691v1)
- **Published**: 2020-10-11 09:54:13+00:00
- **Updated**: 2020-10-11 09:54:13+00:00
- **Authors**: Jan ÄŒejka, Fotis Liarokapis
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater sites are a harsh environment for augmented reality applications. Obstacles that must be battled include poor visibility conditions, difficult navigation, and hard manipulation with devices under water. This chapter focuses on the problem of localizing a device under water using markers. It discusses various filters that enhance and improve images recorded under water, and their impact on marker-based tracking. It presents various combinations of 10 image improving algorithms and 4 marker detecting algorithms, and tests their performance in real situations. All solutions are designed to run real-time on mobile devices to provide a solid basis for augmented reality. Usability of this solution is evaluated on locations in Mediterranean Sea. It is shown that image improving algorithms with carefully chosen parameters can reduce the problems with visibility under water and improve the detection of markers. The best results are obtained with marker detecting algorithms that are specifically designed for underwater environments.



### Generalized Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.05210v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05210v4)
- **Published**: 2020-10-11 10:13:21+00:00
- **Updated**: 2022-05-31 07:01:12+00:00
- **Authors**: Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, Jiaya Jia
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Training semantic segmentation models requires a large amount of finely annotated data, making it hard to quickly adapt to novel classes not satisfying this condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many constraints. In this paper, we introduce a new benchmark, called Generalized Few-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability of simultaneously segmenting the novel categories with very few examples and the base categories with sufficient examples. It is the first study showing that previous representative state-of-the-art FS-Seg methods fall short in GFS-Seg and the performance discrepancy mainly comes from the constrained setting of FS-Seg. To make GFS-Seg tractable, we set up a GFS-Seg baseline that achieves decent performance without structural change on the original model. Then, since context is essential for semantic segmentation, we propose the Context-Aware Prototype Learning (CAPL) that significantly improves performance by 1) leveraging the co-occurrence prior knowledge from support samples, and 2) dynamically enriching contextual information to the classifier, conditioned on the content of each query image. Both two contributions are experimentally shown to have substantial practical merit. Extensive experiments on Pascal-VOC and COCO manifest the effectiveness of CAPL, and CAPL generalizes well to FS-Seg by achieving competitive performance. Code is available at https://github.com/dvlab-research/GFS-Seg.



### GuCNet: A Guided Clustering-based Network for Improved Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.05212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05212v1)
- **Published**: 2020-10-11 10:22:03+00:00
- **Updated**: 2020-10-11 10:22:03+00:00
- **Authors**: Ushasi Chaudhuri, Syomantak Chaudhuri, Subhasis Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: We deal with the problem of semantic classification of challenging and highly-cluttered dataset. We present a novel, and yet a very simple classification technique by leveraging the ease of classifiability of any existing well separable dataset for guidance. Since the guide dataset which may or may not have any semantic relationship with the experimental dataset, forms well separable clusters in the feature set, the proposed network tries to embed class-wise features of the challenging dataset to those distinct clusters of the guide set, making them more separable. Depending on the availability, we propose two types of guide sets: one using texture (image) guides and another using prototype vectors representing cluster centers. Experimental results obtained on the challenging benchmark RSSCN, LSUN, and TU-Berlin datasets establish the efficacy of the proposed method as we outperform the existing state-of-the-art techniques by a considerable margin.



### Partial FC: Training 10 Million Identities on a Single Machine
- **Arxiv ID**: http://arxiv.org/abs/2010.05222v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2010.05222v2)
- **Published**: 2020-10-11 11:15:26+00:00
- **Updated**: 2021-01-23 05:25:06+00:00
- **Authors**: Xiang An, Xuhan Zhu, Yang Xiao, Lan Wu, Ming Zhang, Yuan Gao, Bin Qin, Debing Zhang, Ying Fu
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.



### Domain Agnostic Learning for Unbiased Authentication
- **Arxiv ID**: http://arxiv.org/abs/2010.05250v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05250v2)
- **Published**: 2020-10-11 14:05:16+00:00
- **Updated**: 2020-11-23 09:13:33+00:00
- **Authors**: Jian Liang, Yuren Cao, Shuang Li, Bing Bai, Hao Li, Fei Wang, Kun Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Authentication is the task of confirming the matching relationship between a data instance and a given identity. Typical examples of authentication problems include face recognition and person re-identification. Data-driven authentication could be affected by undesired biases, i.e., the models are often trained in one domain (e.g., for people wearing spring outfits) while applied in other domains (e.g., they change the clothes to summer outfits). Previous works have made efforts to eliminate domain-difference. They typically assume domain annotations are provided, and all the domains share classes. However, for authentication, there could be a large number of domains shared by different identities/classes, and it is impossible to annotate these domains exhaustively. It could make domain-difference challenging to model and eliminate. In this paper, we propose a domain-agnostic method that eliminates domain-difference without domain labels. We alternately perform latent domain discovery and domain-difference elimination until our model no longer detects domain-difference. In our approach, the latent domains are discovered by learning the heterogeneous predictive relationships between inputs and outputs. Then domain-difference is eliminated in both class-dependent and class-independent spaces to improve robustness of elimination. We further extend our method to a meta-learning framework to pursue more thorough domain-difference elimination. Comprehensive empirical evaluation results are provided to demonstrate the effectiveness and superiority of our proposed method.



### Shape-aware Generative Adversarial Networks for Attribute Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.05259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05259v1)
- **Published**: 2020-10-11 14:52:32+00:00
- **Updated**: 2020-10-11 14:52:32+00:00
- **Authors**: Lei Luo, William Hsu, Shangxian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have been successfully applied to transfer visual attributes in many domains, including that of human face images. This success is partly attributable to the facts that human faces have similar shapes and the positions of eyes, noses, and mouths are fixed among different people. Attribute transfer is more challenging when the source and target domain share different shapes. In this paper, we introduce a shape-aware GAN model that is able to preserve shape when transferring attributes, and propose its application to some real-world domains. Compared to other state-of-art GANs-based image-to-image translation models, the model we propose is able to generate more visually appealing results while maintaining the quality of results from transfer learning.



### Infrared target tracking based on proximal robust principal component analysis method
- **Arxiv ID**: http://arxiv.org/abs/2010.05260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05260v1)
- **Published**: 2020-10-11 14:54:00+00:00
- **Updated**: 2020-10-11 14:54:00+00:00
- **Authors**: Chao Ma, Guohua Gu, Xin Miao, Minjie Wan, Weixian Qian, Kan Ren, Qian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared target tracking plays an important role in both civil and military fields. The main challenges in designing a robust and high-precision tracker for infrared sequences include overlap, occlusion and appearance change. To this end, this paper proposes an infrared target tracker based on proximal robust principal component analysis method. Firstly, the observation matrix is decomposed into a sparse occlusion matrix and a low-rank target matrix, and the constraint optimization is carried out with an approaching proximal norm which is better than L1-norm. To solve this convex optimization problem, Alternating Direction Method of Multipliers (ADMM) is employed to estimate the variables alternately. Finally, the framework of particle filter with model update strategy is exploited to locate the target. Through a series of experiments on real infrared target sequences, the effectiveness and robustness of our algorithm are proved.



### Boosting Continuous Sign Language Recognition via Cross Modality Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.05264v1
- **DOI**: 10.1145/3394171.3413931
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05264v1)
- **Published**: 2020-10-11 15:07:50+00:00
- **Updated**: 2020-10-11 15:07:50+00:00
- **Authors**: Junfu Pu, Wengang Zhou, Hezhen Hu, Houqiang Li
- **Comment**: Accepted to ACM Multimedia 2020
- **Journal**: None
- **Summary**: Continuous sign language recognition (SLR) deals with unaligned video-text pair and uses the word error rate (WER), i.e., edit distance, as the main evaluation metric. Since it is not differentiable, we usually instead optimize the learning model with the connectionist temporal classification (CTC) objective loss, which maximizes the posterior probability over the sequential alignment. Due to the optimization gap, the predicted sentence with the highest decoding probability may not be the best choice under the WER metric. To tackle this issue, we propose a novel architecture with cross modality augmentation. Specifically, we first augment cross-modal data by simulating the calculation procedure of WER, i.e., substitution, deletion and insertion on both text label and its corresponding video. With these real and generated pseudo video-text pairs, we propose multiple loss terms to minimize the cross modality distance between the video and ground truth label, and make the network distinguish the difference between real and pseudo modalities. The proposed framework can be easily extended to other existing CTC based continuous SLR architectures. Extensive experiments on two continuous SLR benchmarks, i.e., RWTH-PHOENIX-Weather and CSL, validate the effectiveness of our proposed method.



### IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration
- **Arxiv ID**: http://arxiv.org/abs/2010.05272v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05272v3)
- **Published**: 2020-10-11 15:36:40+00:00
- **Updated**: 2021-03-18 14:43:20+00:00
- **Authors**: Ziyi Wu, Yueqi Duan, He Wang, Qingnan Fan, Leonidas J. Guibas
- **Comment**: 17 pages, 8 figures. Update several experimental results compared
  with v2, e.g. cross dataset evaluation and baseline results of adversarial
  training
- **Journal**: None
- **Summary**: Point cloud is an important 3D data representation widely used in many essential applications. Leveraging deep neural networks, recent works have shown great success in processing 3D point clouds. However, those deep neural networks are vulnerable to various 3D adversarial attacks, which can be summarized as two primary types: point perturbation that affects local point distribution, and surface distortion that causes dramatic changes in geometry. In this paper, we simultaneously address both the aforementioned attacks by learning to restore the clean point clouds from the attacked ones. More specifically, we propose an IF-Defense framework to directly optimize the coordinates of input points with geometry-aware and distribution-aware constraints. The former aims to recover the surface of point cloud through implicit function, while the latter encourages evenly-distributed points. Our experimental results show that IF-Defense achieves the state-of-the-art defense performance against existing 3D adversarial attacks on PointNet, PointNet++, DGCNN, PointConv and RS-CNN. For example, compared with previous methods, IF-Defense presents 20.02% improvement in classification accuracy against salient point dropping attack and 16.29% against LG-GAN attack on PointNet. Our code is available at https://github.com/Wuziyi616/IF-Defense.



### Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.05300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05300v1)
- **Published**: 2020-10-11 17:55:06+00:00
- **Updated**: 2020-10-11 17:55:06+00:00
- **Authors**: Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, Gao Huang
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: The accuracy of deep convolutional neural networks (CNNs) generally improves when fueled with high resolution images. However, this often comes at a high computational cost and high memory footprint. Inspired by the fact that not all regions in an image are task-relevant, we propose a novel framework that performs efficient image classification by processing a sequence of relatively small inputs, which are strategically selected from the original image with reinforcement learning. Such a dynamic decision process naturally facilitates adaptive inference at test time, i.e., it can be terminated once the model is sufficiently confident about its prediction and thus avoids further redundant computation. Notably, our framework is general and flexible as it is compatible with most of the state-of-the-art light-weighted CNNs (such as MobileNets, EfficientNets and RegNets), which can be conveniently deployed as the backbone feature extractor. Experiments on ImageNet show that our method consistently improves the computational efficiency of a wide variety of deep models. For example, it further reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 20% without sacrificing accuracy. Code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch.



### PI-Net: Pose Interacting Network for Multi-Person Monocular 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.05302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05302v1)
- **Published**: 2020-10-11 17:58:24+00:00
- **Updated**: 2020-10-11 17:58:24+00:00
- **Authors**: Wen Guo, Enric Corona, Francesc Moreno-Noguer, Xavier Alameda-Pineda
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: Recent literature addressed the monocular 3D pose estimation task very satisfactorily. In these studies, different persons are usually treated as independent pose instances to estimate. However, in many every-day situations, people are interacting, and the pose of an individual depends on the pose of his/her interactees. In this paper, we investigate how to exploit this dependency to enhance current - and possibly future - deep networks for 3D monocular pose estimation. Our pose interacting network, or PI-Net, inputs the initial pose estimates of a variable number of interactees into a recurrent architecture used to refine the pose of the person-of-interest. Evaluating such a method is challenging due to the limited availability of public annotated multi-person 3D human pose datasets. We demonstrate the effectiveness of our method in the MuPoTS dataset, setting the new state-of-the-art on it. Qualitative results on other multi-person datasets (for which 3D pose ground-truth is not available) showcase the proposed PI-Net. PI-Net is implemented in PyTorch and the code will be made available upon acceptance of the paper.



### H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain Adaptation and Label Refinement
- **Arxiv ID**: http://arxiv.org/abs/2010.05309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.05309v1)
- **Published**: 2020-10-11 18:35:03+00:00
- **Updated**: 2020-10-11 18:35:03+00:00
- **Authors**: Peri Akiva, Matthew Purri, Kristin Dana, Beth Tellman, Tyler Anderson
- **Comment**: Submitted to WACV2021
- **Journal**: None
- **Summary**: Accurate flood detection in near real time via high resolution, high latency satellite imagery is essential to prevent loss of lives by providing quick and actionable information. Instruments and sensors useful for flood detection are only available in low resolution, low latency satellites with region re-visit periods of up to 16 days, making flood alerting systems that use such satellites unreliable. This work presents H2O-Network, a self supervised deep learning method to segment floods from satellites and aerial imagery by bridging domain gap between low and high latency satellite and coarse-to-fine label refinement. H2O-Net learns to synthesize signals highly correlative with water presence as a domain adaptation step for semantic segmentation in high resolution satellite imagery. Our work also proposes a self-supervision mechanism, which does not require any hand annotation, used during training to generate high quality ground truth data. We demonstrate that H2O-Net outperforms the state-of-the-art semantic segmentation methods on satellite imagery by 10% and 12% pixel accuracy and mIoU respectively for the task of flood segmentation. We emphasize the generalizability of our model by transferring model weights trained on satellite imagery to drone imagery, a highly different sensor and domain.



### Revising FUNSD dataset for key-value detection in document images
- **Arxiv ID**: http://arxiv.org/abs/2010.05322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05322v1)
- **Published**: 2020-10-11 19:06:56+00:00
- **Updated**: 2020-10-11 19:06:56+00:00
- **Authors**: Hieu M. Vu, Diep Thi-Ngoc Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: FUNSD is one of the limited publicly available datasets for information extraction from document im-ages. The information in the FUNSD dataset is defined by text areas of four categories ("key", "value", "header", "other", and "background") and connectivity between areas as key-value relations. In-specting FUNSD, we found several inconsistency in labeling, which impeded its applicability to thekey-value extraction problem. In this report, we described some labeling issues in FUNSD and therevision we made to the dataset. We also reported our implementation of for key-value detection onFUNSD using a UNet model as baseline results and an improved UNet model with Channel-InvariantDeformable Convolution.



### Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains
- **Arxiv ID**: http://arxiv.org/abs/2010.05334v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05334v3)
- **Published**: 2020-10-11 20:10:36+00:00
- **Updated**: 2020-11-21 14:31:00+00:00
- **Authors**: Justin N. M. Pinkney, Doron Adler
- **Comment**: 2 pages, 3 figures. Accepted to Machine Learning for Creativity and
  Design NeurIPS 2020 Workshop; Corrected typos
- **Journal**: None
- **Summary**: GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel domain, a task which GANs are inherently unable to do. It is also desirable to have a level of control so that there is a degree of artistic direction rather than purely curation of random results. Here we present a method for interpolating between generative models of the StyleGAN architecture in a resolution dependent manner. This allows us to generate images from an entirely novel domain and do this with a degree of control over the nature of the output.



### Self-attention aggregation network for video face representation and recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.05340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05340v1)
- **Published**: 2020-10-11 20:57:46+00:00
- **Updated**: 2020-10-11 20:57:46+00:00
- **Authors**: Ihor Protsenko, Taras Lehinevych, Dmytro Voitekh, Ihor Kroosh, Nick Hasty, Anthony Johnson
- **Comment**: None
- **Journal**: None
- **Summary**: Models based on self-attention mechanisms have been successful in analyzing temporal data and have been widely used in the natural language domain. We propose a new model architecture for video face representation and recognition based on a self-attention mechanism. Our approach could be used for video with single and multiple identities. To the best of our knowledge, no one has explored the aggregation approaches that consider the video with multiple identities. The proposed approach utilizes existing models to get the face representation for each video frame, e.g., ArcFace and MobileFaceNet, and the aggregation module produces the aggregated face representation vector for video by taking into consideration the order of frames and their quality scores. We demonstrate empirical results on a public dataset for video face recognition called IJB-C to indicate that the self-attention aggregation network (SAAN) outperforms naive average pooling. Moreover, we introduce a new multi-identity video dataset based on the publicly available UMDFaces dataset and collected GIFs from Giphy. We show that SAAN is capable of producing a compact face representation for both single and multiple identities in a video. The dataset and source code will be publicly available.



### Google Landmark Recognition 2020 Competition Third Place Solution
- **Arxiv ID**: http://arxiv.org/abs/2010.05350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05350v1)
- **Published**: 2020-10-11 21:30:43+00:00
- **Updated**: 2020-10-11 21:30:43+00:00
- **Authors**: Qishen Ha, Bo Liu, Fuxu Liu, Peiyuan Liao
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: We present our third place solution to the Google Landmark Recognition 2020 competition. It is an ensemble of global features only Sub-center ArcFace models. We introduce dynamic margins for ArcFace loss, a family of tune-able margin functions of class size, designed to deal with the extreme imbalance in GLDv2 dataset. Progressive finetuning and careful postprocessing are also key to the solution. Our two submissions scored 0.6344 and 0.6289 on private leaderboard, both ranking third place out of 736 teams.



### Simple Neighborhood Representative Pre-processing Boosts Outlier Detectors
- **Arxiv ID**: http://arxiv.org/abs/2010.12061v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12061v3)
- **Published**: 2020-10-11 21:31:14+00:00
- **Updated**: 2022-12-21 00:30:52+00:00
- **Authors**: Jiawei Yang, Yu Chen, Sylwan Rahardja
- **Comment**: None
- **Journal**: Information Sciences 2022
- **Summary**: Over the decades, traditional outlier detectors have ignored the group-level factor when calculating outlier scores for objects in data by evaluating only the object-level factor, failing to capture the collective outliers. To mitigate this issue, we present a method called neighborhood representative (NR), which empowers all the existing outlier detectors to efficiently detect outliers, including collective outliers, while maintaining their computational integrity. It achieves this by selecting representative objects, scoring these objects, then applies the score of the representative objects to its collective objects. Without altering existing detectors, NR is compatible with existing detectors, while improving performance on real world datasets with +8% (0.72 to 0.78 AUC) relative to state-of-the-art outlier detectors.



### Identifying Melanoma Images using EfficientNet Ensemble: Winning Solution to the SIIM-ISIC Melanoma Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2010.05351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05351v1)
- **Published**: 2020-10-11 21:38:04+00:00
- **Updated**: 2020-10-11 21:38:04+00:00
- **Authors**: Qishen Ha, Bo Liu, Fuxu Liu
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: We present our winning solution to the SIIM-ISIC Melanoma Classification Challenge. It is an ensemble of convolutions neural network (CNN) models with different backbones and input sizes, most of which are image-only models while a few of them used image-level and patient-level metadata. The keys to our winning are: (1) stable validation scheme (2) good choice of model target (3) carefully tuned pipeline and (4) ensembling with very diverse models. The winning submission scored 0.9600 AUC on cross validation and 0.9490 AUC on private leaderboard.



### MoCo-CXR: MoCo Pretraining Improves Representation and Transferability of Chest X-ray Models
- **Arxiv ID**: http://arxiv.org/abs/2010.05352v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05352v3)
- **Published**: 2020-10-11 21:42:10+00:00
- **Updated**: 2021-05-17 20:39:42+00:00
- **Authors**: Hari Sowrirajan, Jingbo Yang, Andrew Y. Ng, Pranav Rajpurkar
- **Comment**: Accepted at Medical Imaging with Deep Learning (MIDL) Conference 2021
- **Journal**: None
- **Summary**: Contrastive learning is a form of self-supervision that can leverage unlabeled data to produce pretrained models. While contrastive learning has demonstrated promising results on natural image classification tasks, its application to medical imaging tasks like chest X-ray interpretation has been limited. In this work, we propose MoCo-CXR, which is an adaptation of the contrastive learning method Momentum Contrast (MoCo), to produce models with better representations and initializations for the detection of pathologies in chest X-rays. In detecting pleural effusion, we find that linear models trained on MoCo-CXR-pretrained representations outperform those without MoCo-CXR-pretrained representations, indicating that MoCo-CXR-pretrained representations are of higher-quality. End-to-end fine-tuning experiments reveal that a model initialized via MoCo-CXR-pretraining outperforms its non-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides the most benefit with limited labeled training data. Finally, we demonstrate similar results on a target Tuberculosis dataset unseen during pretraining, indicating that MoCo-CXR-pretraining endows models with representations and transferability that can be applied across chest X-ray datasets and tasks.



### Medical Image Harmonization Using Deep Learning Based Canonical Mapping: Toward Robust and Generalizable Learning in Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.05355v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05355v1)
- **Published**: 2020-10-11 22:01:37+00:00
- **Updated**: 2020-10-11 22:01:37+00:00
- **Authors**: Vishnu M. Bashyam, Jimit Doshi, Guray Erus, Dhivya Srinivasan, Ahmed Abdulkadir, Mohamad Habes, Yong Fan, Colin L. Masters, Paul Maruff, Chuanjun Zhuo, Henry VÃ¶lzke, Sterling C. Johnson, Jurgen Fripp, Nikolaos Koutsouleris, Theodore D. Satterthwaite, Daniel H. Wolf, Raquel E. Gur, Ruben C. Gur, John C. Morris, Marilyn S. Albert, Hans J. Grabe, Susan M. Resnick, R. Nick Bryan, David A. Wolk, Haochang Shou, Ilya M. Nasrallah, Christos Davatzikos
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional and deep learning-based methods have shown great potential in the medical imaging domain, as means for deriving diagnostic, prognostic, and predictive biomarkers, and by contributing to precision medicine. However, these methods have yet to see widespread clinical adoption, in part due to limited generalization performance across various imaging devices, acquisition protocols, and patient populations. In this work, we propose a new paradigm in which data from a diverse range of acquisition conditions are "harmonized" to a common reference domain, where accurate model learning and prediction can take place. By learning an unsupervised image to image canonical mapping from diverse datasets to a reference domain using generative deep learning models, we aim to reduce confounding data variation while preserving semantic information, thereby rendering the learning task easier in the reference domain. We test this approach on two example problems, namely MRI-based brain age prediction and classification of schizophrenia, leveraging pooled cohorts of neuroimaging MRI data spanning 9 sites and 9701 subjects. Our results indicate a substantial improvement in these tasks in out-of-sample data, even when training is restricted to a single site.



### A range characterization of the single-quadrant ADRT
- **Arxiv ID**: http://arxiv.org/abs/2010.05360v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, 44A12, 65R10, 92C55, 68U05, 15A04
- **Links**: [PDF](http://arxiv.org/pdf/2010.05360v2)
- **Published**: 2020-10-11 22:14:22+00:00
- **Updated**: 2022-03-22 17:10:15+00:00
- **Authors**: Weilin Li, Kui Ren, Donsub Rim
- **Comment**: None
- **Journal**: None
- **Summary**: This work characterizes the range of the single-quadrant approximate discrete Radon transform (ADRT) of square images. The characterization follows from a set of linear constraints on the codomain. We show that for data satisfying these constraints, the exact and fast inversion formula [Rim, Appl. Math. Lett. 102 106159, 2020] yields a square image in a stable manner. The range characterization is obtained by first showing that the ADRT is a bijection between images supported on infinite half-strips, then identifying the linear subspaces that stay finitely supported under the inversion formula.



