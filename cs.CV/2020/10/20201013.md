# Arxiv Papers in cs.CV on 2020-10-13
### Contrast and Classify: Training Robust VQA Models
- **Arxiv ID**: http://arxiv.org/abs/2010.06087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06087v2)
- **Published**: 2020-10-13 00:23:59+00:00
- **Updated**: 2021-04-19 03:45:27+00:00
- **Authors**: Yash Kant, Abhinav Moudgil, Dhruv Batra, Devi Parikh, Harsh Agrawal
- **Comment**: None
- **Journal**: None
- **Summary**: Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of representations for answer prediction.   We find that optimizing both losses -- either alternately or jointly -- is key to effective training. On the VQA-Rephrasings benchmark, which measures the VQA model's answer consistency across human paraphrases of a question, ConClaT improves Consensus Score by 1 .63% over an improved baseline. In addition, on the standard VQA 2.0 benchmark, we improve the VQA accuracy by 0.78% overall. We also show that ConClaT is agnostic to the type of data-augmentation strategy used.



### Attn-HybridNet: Improving Discriminability of Hybrid Features with Attention Fusion
- **Arxiv ID**: http://arxiv.org/abs/2010.06096v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06096v2)
- **Published**: 2020-10-13 00:52:57+00:00
- **Updated**: 2020-10-14 12:44:41+00:00
- **Authors**: Sunny Verma, Chen Wang, Liming Zhu, Wei Liu
- **Comment**: Under minor review at IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: The principal component analysis network (PCANet) is an unsupervised parsimonious deep network, utilizing principal components as filters in its convolution layers. Albeit powerful, the PCANet consists of basic operations such as principal components and spatial pooling, which suffers from two fundamental problems. First, the principal components obtain information by transforming it to column vectors (which we call the amalgamated view), which incurs the loss of the spatial information in the data. Second, the generalized spatial pooling utilized in the PCANet induces feature redundancy and also fails to accommodate spatial statistics of natural images. In this research, we first propose a tensor-factorization based deep network called the Tensor Factorization Network (TFNet). The TFNet extracts features from the spatial structure of the data (which we call the minutiae view). We then show that the information obtained by the PCANet and the TFNet are distinctive and non-trivial but individually insufficient. This phenomenon necessitates the development of proposed HybridNet, which integrates the information discovery with the two views of the data. To enhance the discriminability of hybrid features, we propose Attn-HybridNet, which alleviates the feature redundancy by performing attention-based feature fusion. The significance of our proposed Attn-HybridNet is demonstrated on multiple real-world datasets where the features obtained with Attn-HybridNet achieves better classification performance over other popular baseline methods, demonstrating the effectiveness of the proposed technique.



### Gradient Descent Ascent for Minimax Problems on Riemannian Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2010.06097v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2010.06097v5)
- **Published**: 2020-10-13 00:54:00+00:00
- **Updated**: 2023-01-02 23:16:35+00:00
- **Authors**: Feihu Huang, Shangqian Gao
- **Comment**: This paper was accepted to IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of $O(\kappa^2\epsilon^{-2})$ for finding an $\epsilon$-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where $\kappa$ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of $O(\kappa^4\epsilon^{-4})$ for finding an $\epsilon$-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of $\tilde{O}(\kappa^{4}\epsilon^{-3})$ in searching for an $\epsilon$-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.



### Similarity Based Stratified Splitting: an approach to train better classifiers
- **Arxiv ID**: http://arxiv.org/abs/2010.06099v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.06099v1)
- **Published**: 2020-10-13 01:07:48+00:00
- **Updated**: 2020-10-13 01:07:48+00:00
- **Authors**: Felipe Farias, Teresa Ludermir, Carmelo Bastos-Filho
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Similarity-Based Stratified Splitting (SBSS) technique, which uses both the output and input space information to split the data. The splits are generated using similarity functions among samples to place similar samples in different splits. This approach allows for a better representation of the data in the training phase. This strategy leads to a more realistic performance estimation when used in real-world applications. We evaluate our proposal in twenty-two benchmark datasets with classifiers such as Multi-Layer Perceptron, Support Vector Machine, Random Forest and K-Nearest Neighbors, and five similarity functions Cityblock, Chebyshev, Cosine, Correlation, and Euclidean. According to the Wilcoxon Sign-Rank test, our approach consistently outperformed ordinary stratified 10-fold cross-validation in 75\% of the assessed scenarios.



### Invariant Representation Learning for Infant Pose Estimation with Small Data
- **Arxiv ID**: http://arxiv.org/abs/2010.06100v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06100v5)
- **Published**: 2020-10-13 01:10:14+00:00
- **Updated**: 2021-11-01 17:22:05+00:00
- **Authors**: Xiaofei Huang, Nihang Fu, Shuangjun Liu, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Infant motion analysis is a topic with critical importance in early childhood development studies. However, while the applications of human pose estimation have become more and more broad, models trained on large-scale adult pose datasets are barely successful in estimating infant poses due to the significant differences in their body ratio and the versatility of their poses. Moreover, the privacy and security considerations hinder the availability of adequate infant pose data required for training of a robust model from scratch. To address this problem, this paper presents (1) building and publicly releasing a hybrid synthetic and real infant pose (SyRIP) dataset with small yet diverse real infant images as well as generated synthetic infant poses and (2) a multi-stage invariant representation learning strategy that could transfer the knowledge from the adjacent domains of adult poses and synthetic infant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation model. In our ablation study, with identical network structure, models trained on SyRIP dataset show noticeable improvement over the ones trained on the only other public infant pose datasets. Integrated with pose estimation backbone networks with varying complexity, FiDIP performs consistently better than the fine-tuned versions of those models. One of our best infant pose estimation performers on the state-of-the-art DarkPose model shows mean average precision (mAP) of 93.6.



### SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.06107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06107v2)
- **Published**: 2020-10-13 01:23:17+00:00
- **Updated**: 2021-08-05 09:25:04+00:00
- **Authors**: Xiaoman Zhang, Shixiang Feng, Yuhang Zhou, Ya Zhang, Yanfeng Wang
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Automatic and accurate tumor segmentation on medical images is in high demand to assist physicians with diagnosis and treatment. However, it is difficult to obtain massive amounts of annotated training data required by the deep-learning models as the manual delineation process is often tedious and expertise required. Although self-supervised learning (SSL) scheme has been widely adopted to address this problem, most SSL methods focus only on global structure information, ignoring the key distinguishing features of tumor regions: local intensity variation and large size distribution. In this paper, we propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor segmentation. Specifically, a novel proxy task, i.e. scale discrimination, is formulated to pre-train the 3D neural network combined with the self-restoration task. Thus, the pre-trained model learns multi-level local representations through multi-scale inputs. Moreover, an adversarial learning module is further introduced to learn modality invariant representations from multiple unlabeled source datasets. We demonstrate the effectiveness of our methods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas tumor segmentation. Compared with the state-of-the-art 3D SSL methods, our proposed approach can significantly improve the segmentation accuracy. Besides, we analyze its advantages from multiple perspectives such as data efficiency, performance, and convergence speed.



### Checkerboard-Artifact-Free Image-Enhancement Network Considering Local and Global Features
- **Arxiv ID**: http://arxiv.org/abs/2010.12347v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12347v1)
- **Published**: 2020-10-13 01:28:23+00:00
- **Updated**: 2020-10-13 01:28:23+00:00
- **Authors**: Yuma Kinoshita, Hitoshi Kiya
- **Comment**: to appear in APSIPA ASC 2020. arXiv admin note: text overlap with
  arXiv:1905.02899
- **Journal**: None
- **Summary**: In this paper, we propose a novel convolutional neural network (CNN) that never causes checkerboard artifacts, for image enhancement. In research fields of image-to-image translation problems, it is well-known that images generated by usual CNNs are distorted by checkerboard artifacts which mainly caused in forward-propagation of upsampling layers. However, checkerboard artifacts in image enhancement have never been discussed. In this paper, we point out that applying U-Net based CNNs to image enhancement causes checkerboard artifacts. In contrast, the proposed network that contains fixed convolutional layers can perfectly prevent the artifacts. In addition, the proposed network architecture, which can handle both local and global features, enables us to improve the performance of image enhancement. Experimental results show that the use of fixed convolutional layers can prevent checkerboard artifacts and the proposed network outperforms state-of-the-art CNN-based image-enhancement methods in terms of various objective quality metrics: PSNR, SSIM, and NIQE.



### Map-Based Temporally Consistent Geolocalization through Learning Motion Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2010.06117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.06117v1)
- **Published**: 2020-10-13 02:08:45+00:00
- **Updated**: 2020-10-13 02:08:45+00:00
- **Authors**: Bing Zha, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel trajectory learning method that exploits motion trajectories on topological map using recurrent neural network for temporally consistent geolocalization of object. Inspired by human's ability to both be aware of distance and direction of self-motion in navigation, our trajectory learning method learns a pattern representation of trajectories encoded as a sequence of distances and turning angles to assist self-localization. We pose the learning process as a conditional sequence prediction problem in which each output locates the object on a traversable path in a map. Considering the prediction sequence ought to be topologically connected in the graph-structured map, we adopt two different hypotheses generation and elimination strategies to eliminate disconnected sequence prediction. We demonstrate our approach on the KITTI stereo visual odometry dataset which is a city-scale environment and can generate trajectory with metric information. The key benefits of our approach to geolocalization are that 1) we take advantage of powerful sequence modeling ability of recurrent neural network and its robustness to noisy input, 2) only require a map in the form of a graph and simply use an affordable sensor that generates motion trajectory and 3) do not need initial position. The experiments show that the motion trajectories can be learned by training an recurrent neural network, and temporally consistent geolocation can be predicted with both of the proposed strategies.



### Learning to Attack with Fewer Pixels: A Probabilistic Post-hoc Framework for Refining Arbitrary Dense Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2010.06131v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06131v2)
- **Published**: 2020-10-13 02:51:10+00:00
- **Updated**: 2022-02-21 05:46:54+00:00
- **Authors**: He Zhao, Thanh Nguyen, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, Dinh Phung
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Many adversarial attacks belong to the category of dense attacks, which generate adversarial examples by perturbing all the pixels of a natural image. To generate sparse perturbations, sparse attacks have been recently developed, which are usually independent attacks derived by modifying a dense attack's algorithm with sparsity regularisations, resulting in reduced attack efficiency. In this paper, we aim to tackle this task from a different perspective. We select the most effective perturbations from the ones generated from a dense attack, based on the fact we find that a considerable amount of the perturbations on an image generated by dense attacks may contribute little to attacking a classifier. Accordingly, we propose a probabilistic post-hoc framework that refines given dense attacks by significantly reducing the number of perturbed pixels but keeping their attack power, trained with mutual information maximisation. Given an arbitrary dense attack, the proposed model enjoys appealing compatibility for making its adversarial images more realistic and less detectable with fewer perturbations. Moreover, our framework performs adversarial attacks much faster than existing sparse attacks.



### Bridging 2D and 3D Segmentation Networks for Computation Efficient Volumetric Medical Image Segmentation: An Empirical Study of 2.5D Solutions
- **Arxiv ID**: http://arxiv.org/abs/2010.06163v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06163v2)
- **Published**: 2020-10-13 04:12:28+00:00
- **Updated**: 2022-02-07 12:45:54+00:00
- **Authors**: Yichi Zhang, Qingcheng Liao, Le Ding, Jicong Zhang
- **Comment**: Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Recently, deep convolutional neural networks have achieved great success for medical image segmentation. However, unlike segmentation of natural images, most medical images such as MRI and CT are volumetric data. In order to make full use of volumetric information, 3D CNNs are widely used. However, 3D CNNs suffer from higher inference time and computation cost, which hinders their further clinical applications. Additionally, with the increased number of parameters, the risk of overfitting is higher, especially for medical images where data and annotations are expensive to acquire. To issue this problem, many 2.5D segmentation methods have been proposed to make use of volumetric spatial information with less computation cost. Despite these works lead to improvements on a variety of segmentation tasks, to the best of our knowledge, there has not previously been a large-scale empirical comparison of these methods. In this paper, we aim to present a review of the latest developments of 2.5D methods for volumetric medical image segmentation. Additionally, to compare the performance and effectiveness of these methods, we provide an empirical study of these methods on three representative segmentation tasks involving different modalities and targets. Our experimental results highlight that 3D CNNs may not always be the best choice. Despite all these 2.5D methods can bring performance gains to 2D baseline, not all the methods hold the benefits on different datasets. We hope the results and conclusions of our study will prove useful for the community on exploring and developing efficient volumetric medical image segmentation methods.



### ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2010.06176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06176v1)
- **Published**: 2020-10-13 04:34:24+00:00
- **Updated**: 2020-10-13 04:34:24+00:00
- **Authors**: Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, Zhouchen Lin
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Neural architecture search (NAS) aims to produce the optimal sparse solution from a high-dimensional space spanned by all candidate connections. Current gradient-based NAS methods commonly ignore the constraint of sparsity in the search phase, but project the optimized solution onto a sparse one by post-processing. As a result, the dense super-net for search is inefficient to train and has a gap with the projected architecture for evaluation. In this paper, we formulate neural architecture search as a sparse coding problem. We perform the differentiable search on a compressed lower-dimensional space that has the same validation loss as the original sparse solution space, and recover an architecture by solving the sparse coding problem. The differentiable search and architecture recovery are optimized in an alternate manner. By doing so, our network for search at each update satisfies the sparsity constraint and is efficient to train. In order to also eliminate the depth and width gap between the network in search and the target-net in evaluation, we further propose a method to search and evaluate in one stage under the target-net settings. When training finishes, architecture variables are absorbed into network weights. Thus we get the searched architecture and optimized parameters in a single run. In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for search. Our one-stage method produces state-of-the-art performances on both CIFAR-10 and ImageNet at the cost of only evaluation time.



### COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework
- **Arxiv ID**: http://arxiv.org/abs/2010.06177v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06177v1)
- **Published**: 2020-10-13 04:34:30+00:00
- **Updated**: 2020-10-13 04:34:30+00:00
- **Authors**: Anwaar Ulhaq, Oliver Burmeister
- **Comment**: 2 images, 0 Table,8 pages
- **Journal**: None
- **Summary**: To address COVID-19 healthcare challenges, we need frequent sharing of health data, knowledge and resources at a global scale. However, in this digital age, data privacy is a big concern that requires the secure embedding of privacy assurance into the design of all technological solutions that use health data. In this paper, we introduce differential privacy by design (dPbD) framework and discuss its embedding into the federated machine learning system. To limit the scope of our paper, we focus on the problem scenario of COVID-19 imaging data privacy for disease diagnosis by computer vision and deep learning approaches. We discuss the evaluation of the proposed design of federated machine learning systems and discuss how differential privacy by design (dPbD) framework can enhance data privacy in federated learning systems with scalability and robustness. We argue that scalable differentially private federated learning design is a promising solution for building a secure, private and collaborative machine learning model such as required to combat COVID19 challenge.



### When Wireless Communications Meet Computer Vision in Beyond 5G
- **Arxiv ID**: http://arxiv.org/abs/2010.06188v1
- **DOI**: 10.1109/MCOMSTD.001.2000047
- **Categories**: **cs.CV**, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2010.06188v1)
- **Published**: 2020-10-13 05:25:35+00:00
- **Updated**: 2020-10-13 05:25:35+00:00
- **Authors**: Takayuki Nishio, Yusuke Koda, Jihong Park, Mehdi Bennis, Klaus Doppler
- **Comment**: 7 pages, 4 figures; This work has been submitted to IEEE
  Communications Standards Magazine for possible publication
- **Journal**: IEEE Communications Standards Magazine, Vol. 5, Issue 2, June 2021
- **Summary**: This article articulates the emerging paradigm, sitting at the confluence of computer vision and wireless communication, to enable beyond-5G/6G mission-critical applications (autonomous/remote-controlled vehicles, visuo-haptic VR, and other cyber-physical applications). First, drawing on recent advances in machine learning and the availability of non-RF data, vision-aided wireless networks are shown to significantly enhance the reliability of wireless communication without sacrificing spectral efficiency. In particular, we demonstrate how computer vision enables {look-ahead} prediction in a millimeter-wave channel blockage scenario, before the blockage actually happens. From a computer vision perspective, we highlight how radio frequency (RF) based sensing and imaging are instrumental in robustifying computer vision applications against occlusion and failure. This is corroborated via an RF-based image reconstruction use case, showcasing a receiver-side image failure correction resulting in reduced retransmission and latency. Taken together, this article sheds light on the much-needed convergence of RF and non-RF modalities to enable ultra-reliable communication and truly intelligent 6G networks.



### A Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts
- **Arxiv ID**: http://arxiv.org/abs/2010.07047v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07047v4)
- **Published**: 2020-10-13 06:34:45+00:00
- **Updated**: 2021-12-14 00:58:11+00:00
- **Authors**: Chaoqing Xu, Tyson Neuroth, Takanori Fujiwara, Ronghua Liang, Kwan-Liu Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce an intelligent visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system's AI-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space, and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using real data from the research database of Parkinson's Progression Markers Initiative.



### Experimental Quantum Generative Adversarial Networks for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.06201v3
- **DOI**: 10.1103/PhysRevApplied.16.024051
- **Categories**: **quant-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06201v3)
- **Published**: 2020-10-13 06:57:17+00:00
- **Updated**: 2021-09-07 11:26:38+00:00
- **Authors**: He-Liang Huang, Yuxuan Du, Ming Gong, Youwei Zhao, Yulin Wu, Chaoyue Wang, Shaowei Li, Futian Liang, Jin Lin, Yu Xu, Rui Yang, Tongliang Liu, Min-Hsiu Hsieh, Hui Deng, Hao Rong, Cheng-Zhi Peng, Chao-Yang Lu, Yu-Ao Chen, Dacheng Tao, Xiaobo Zhu, Jian-Wei Pan
- **Comment**: This work was completed in 2019, and the first version of manuscript
  was submitted to the journal in January 2020
- **Journal**: Phys. Rev. Applied 16, 024051 (2021)
- **Summary**: Quantum machine learning is expected to be one of the first practical applications of near-term quantum devices. Pioneer theoretical works suggest that quantum generative adversarial networks (GANs) may exhibit a potential exponential advantage over classical GANs, thus attracting widespread attention. However, it remains elusive whether quantum GANs implemented on near-term quantum devices can actually solve real-world learning tasks. Here, we devise a flexible quantum GAN scheme to narrow this knowledge gap, which could accomplish image generation with arbitrarily high-dimensional features, and could also take advantage of quantum superposition to train multiple examples in parallel. For the first time, we experimentally achieve the learning and generation of real-world hand-written digit images on a superconducting quantum processor. Moreover, we utilize a gray-scale bar dataset to exhibit the competitive performance between quantum GANs and the classical GANs based on multilayer perceptron and convolutional neural network architectures, respectively, benchmarked by the Fr\'echet Distance score. Our work provides guidance for developing advanced quantum generative models on near-term quantum devices and opens up an avenue for exploring quantum advantages in various GAN-related learning tasks.



### DoFE: Domain-oriented Feature Embedding for Generalizable Fundus Image Segmentation on Unseen Datasets
- **Arxiv ID**: http://arxiv.org/abs/2010.06208v1
- **DOI**: 10.1109/TMI.2020.3015224
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06208v1)
- **Published**: 2020-10-13 07:28:39+00:00
- **Updated**: 2020-10-13 07:28:39+00:00
- **Authors**: Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: Accepted at IEEE-TMI 2020
- **Journal**: None
- **Summary**: Deep convolutional neural networks have significantly boosted the performance of fundus image segmentation when test datasets have the same distribution as the training datasets. However, in clinical practice, medical images often exhibit variations in appearance for various reasons, e.g., different scanner vendors and image quality. These distribution discrepancies could lead the deep networks to over-fit on the training datasets and lack generalization ability on the unseen test datasets. To alleviate this issue, we present a novel Domain-oriented Feature Embedding (DoFE) framework to improve the generalization ability of CNNs on unseen target domains by exploring the knowledge from multiple source domains. Our DoFE framework dynamically enriches the image features with additional domain prior knowledge learned from multi-source domains to make the semantic features more discriminative. Specifically, we introduce a Domain Knowledge Pool to learn and memorize the prior information extracted from multi-source domains. Then the original image features are augmented with domain-oriented aggregated features, which are induced from the knowledge pool based on the similarity between the input image and multi-source domain images. We further design a novel domain code prediction branch to infer this similarity and employ an attention-guided mechanism to dynamically combine the aggregated features with the semantic features. We comprehensively evaluate our DoFE framework on two fundus image segmentation tasks, including the optic cup and disc segmentation and vessel segmentation. Our DoFE framework generates satisfying segmentation results on unseen datasets and surpasses other domain generalization and network regularization methods.



### Few-shot Action Recognition with Implicit Temporal Alignment and Pair Similarity Optimization
- **Arxiv ID**: http://arxiv.org/abs/2010.06215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06215v1)
- **Published**: 2020-10-13 07:56:06+00:00
- **Updated**: 2020-10-13 07:56:06+00:00
- **Authors**: Congqi Cao, Yajuan Li, Qinyi Lv, Peng Wang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to recognize instances from novel classes with few labeled samples, which has great value in research and application. Although there has been a lot of work in this area recently, most of the existing work is based on image classification tasks. Video-based few-shot action recognition has not been explored well and remains challenging: 1) the differences of implementation details among different papers make a fair comparison difficult; 2) the wide variations and misalignment of temporal sequences make the video-level similarity comparison difficult; 3) the scarcity of labeled data makes the optimization difficult. To solve these problems, this paper presents 1) a specific setting to evaluate the performance of few-shot action recognition algorithms; 2) an implicit sequence-alignment algorithm for better video-level similarity comparison; 3) an advanced loss for few-shot learning to optimize pair similarity with limited data. Specifically, we propose a novel few-shot action recognition framework that uses long short-term memory following 3D convolutional layers for sequence modeling and alignment. Circle loss is introduced to maximize the within-class similarity and minimize the between-class similarity flexibly towards a more definite convergence target. Instead of using random or ambiguous experimental settings, we set a concrete criterion analogous to the standard image-based few-shot learning setting for few-shot action recognition evaluation. Extensive experiments on two datasets demonstrate the effectiveness of our proposed method.



### Self-Supervised Multi-View Synchronization Learning for 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.06218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06218v1)
- **Published**: 2020-10-13 08:01:24+00:00
- **Updated**: 2020-10-13 08:01:24+00:00
- **Authors**: Simon Jenni, Paolo Favaro
- **Comment**: ACCV 2020 (oral)
- **Journal**: None
- **Summary**: Current state-of-the-art methods cast monocular 3D human pose estimation as a learning problem by training neural networks on large data sets of images and corresponding skeleton poses. In contrast, we propose an approach that can exploit small annotated data sets by fine-tuning networks pre-trained via self-supervised learning on (large) unlabeled data sets. To drive such networks towards supporting 3D pose estimation during the pre-training step, we introduce a novel self-supervised feature learning task designed to focus on the 3D structure in an image. We exploit images extracted from videos captured with a multi-view camera system. The task is to classify whether two images depict two views of the same scene up to a rigid transformation. In a multi-view data set, where objects deform in a non-rigid manner, a rigid transformation occurs only between two views taken at the exact same time, i.e., when they are synchronized. We demonstrate the effectiveness of the synchronization task on the Human3.6M data set and achieve state-of-the-art results in 3D human pose estimation.



### Two-Stream Compare and Contrast Network for Vertebral Compression Fracture Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2010.06224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06224v1)
- **Published**: 2020-10-13 08:12:19+00:00
- **Updated**: 2020-10-13 08:12:19+00:00
- **Authors**: Shixiang Feng, Beibei Liu, Ya Zhang, Xiaoyun Zhang, Yuehua Li
- **Comment**: submitted to TMI
- **Journal**: None
- **Summary**: Differentiating Vertebral Compression Fractures (VCFs) associated with trauma and osteoporosis (benign VCFs) or those caused by metastatic cancer (malignant VCFs) are critically important for treatment decisions. So far, automatic VCFs diagnosis is solved in a two-step manner, i.e. first identify VCFs and then classify it into benign or malignant. In this paper, we explore to model VCFs diagnosis as a three-class classification problem, i.e. normal vertebrae, benign VCFs, and malignant VCFs. However, VCFs recognition and classification require very different features, and both tasks are characterized by high intra-class variation and high inter-class similarity. Moreover, the dataset is extremely class-imbalanced. To address the above challenges, we propose a novel Two-Stream Compare and Contrast Network (TSCCN) for VCFs diagnosis. This network consists of two streams, a recognition stream which learns to identify VCFs through comparing and contrasting between adjacent vertebra, and a classification stream which compares and contrasts between intra-class and inter-class to learn features for fine-grained classification. The two streams are integrated via a learnable weight control module which adaptively sets their contribution. The TSCCN is evaluated on a dataset consisting of 239 VCFs patients and achieves the average sensitivity and specificity of 92.56\% and 96.29\%, respectively.



### Robust Two-Stream Multi-Feature Network for Driver Drowsiness Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.06235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06235v1)
- **Published**: 2020-10-13 08:49:35+00:00
- **Updated**: 2020-10-13 08:49:35+00:00
- **Authors**: Qi Shen, Shengjie Zhao, Rongqing Zhang, Bin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Drowsiness driving is a major cause of traffic accidents and thus numerous previous researches have focused on driver drowsiness detection. Many drive relevant factors have been taken into consideration for fatigue detection and can lead to high precision, but there are still several serious constraints, such as most existing models are environmentally susceptible. In this paper, fatigue detection is considered as temporal action detection problem instead of image classification. The proposed detection system can be divided into four parts: (1) Localize the key patches of the detected driver picture which are critical for fatigue detection and calculate the corresponding optical flow. (2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our system to reduce the impact of different light conditions. (3) Three individual two-stream networks combined with attention mechanism are designed for each feature to extract temporal information. (4) The outputs of the three sub-networks will be concatenated and sent to the fully-connected network, which judges the status of the driver. The drowsiness detection system is trained and evaluated on the famous Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%, which outperforms most existing fatigue detection models.



### Automation of Hemocompatibility Analysis Using Image Segmentation and a Random Forest
- **Arxiv ID**: http://arxiv.org/abs/2010.06245v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06245v1)
- **Published**: 2020-10-13 09:13:00+00:00
- **Updated**: 2020-10-13 09:13:00+00:00
- **Authors**: Johanna C. Clauser, Judith Maas, Jutta Arens, Thomas Schmitz-Rode, Ulrich Steinseifer, Benjamin Berkels
- **Comment**: None
- **Journal**: None
- **Summary**: The hemocompatibility of blood-contacting medical devices remains one of the major challenges in biomedical engineering and makes research in the field of new and improved materials inevitable. However, current in-vitro test and analysis methods are still lacking standardization and comparability, which impedes advances in material design. For example, the optical platelet analysis of material in-vitro hemocompatibility tests is carried out manually or semi-manually by each research group individually.   As a step towards standardization, this paper proposes an automation approach for the optical platelet count and analysis. To this end, fluorescence images are segmented using Zach's convexification of the multiphase-phase piecewise constant Mumford--Shah model. The resulting connected components of the non-background segments then need to be classified as platelet or no platelet. Therefore, a supervised random forest is applied to feature vectors derived from the components using features like area, perimeter and circularity. With an overall high accuracy and low error rates, the random forest achieves reliable results. This is supported by high areas under the receiver-operator and the prediction-recall curve, respectively.   We developed a new method for a fast, user-independent and reproducible analysis of material hemocompatibility tests, which is therefore a unique and powerful tool for advances in biomaterial research.



### Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2010.06255v6
- **DOI**: 10.1109/MGRS.2021.3072992
- **Categories**: **cs.CV**, cs.RO, eess.IV, 68-02, I.4.9; A.1
- **Links**: [PDF](http://arxiv.org/pdf/2010.06255v6)
- **Published**: 2020-10-13 09:35:40+00:00
- **Updated**: 2022-05-24 22:16:36+00:00
- **Authors**: Changhong Fu, Bowen Li, Fangqiang Ding, Fuling Lin, Geng Lu
- **Comment**: Accepted to IEEE Geoscience and Remote Sensing Magazine
- **Journal**: None
- **Summary**: Aerial tracking, which has exhibited its omnipresent dedication and splendid performance, is one of the most active applications in the remote sensing field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system, equipped with a visual tracking approach, has been widely used in aviation, navigation, agriculture,transportation, and public security, etc. As is mentioned above, the UAV-based aerial tracking platform has been gradually developed from research to practical application stage, reaching one of the main aerial remote sensing technologies in the future. However, due to the real-world onerous situations, e.g., harsh external challenges, the vibration of the UAV mechanical structure (especially under strong wind conditions), the maneuvering flight in complex environment, and the limited computation resources onboard, accuracy, robustness, and high efficiency are all crucial for the onboard tracking methods. Recently, the discriminative correlation filter (DCF)-based trackers have stood out for their high computational efficiency and appealing robustness on a single CPU, and have flourished in the UAV visual tracking community. In this work, the basic framework of the DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art DCF-based trackers are orderly summarized according to their innovations for solving various issues. Besides, exhaustive and quantitative experiments have been extended on various prevailing UAV tracking benchmarks, i.e., UAV123, UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903 frames in total. The experiments show the performance, verify the feasibility, and demonstrate the current challenges of DCF-based trackers onboard UAV tracking.



### DORi: Discovering Object Relationship for Moment Localization of a Natural-Language Query in Video
- **Arxiv ID**: http://arxiv.org/abs/2010.06260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06260v1)
- **Published**: 2020-10-13 09:50:29+00:00
- **Updated**: 2020-10-13 09:50:29+00:00
- **Authors**: Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Basura Fernando, Hongdong Li, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the task of temporal moment localization in a long untrimmed video using natural language query. Given a query sentence, the goal is to determine the start and end of the relevant segment within the video. Our key innovation is to learn a video feature embedding through a language-conditioned message-passing algorithm suitable for temporal moment localization which captures the relationships between humans, objects and activities in the video. These relationships are obtained by a spatial sub-graph that contextualizes the scene representation using detected objects and human features conditioned in the language query. Moreover, a temporal sub-graph captures the activities within the video through time. Our method is evaluated on three standard benchmark datasets, and we also introduce YouCookII as a new benchmark for this task. Experiments show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of our approach.



### A Scale and Rotational Invariant Key-point Detector based on Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2010.06264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06264v1)
- **Published**: 2020-10-13 10:08:12+00:00
- **Updated**: 2020-10-13 10:08:12+00:00
- **Authors**: Thanh Hong-Phuoc, Ling Guan
- **Comment**: A novel scale and rotational invariant key-point detector was
  proposed
- **Journal**: None
- **Summary**: Most popular hand-crafted key-point detectors such as Harris corner, SIFT, SURF aim to detect corners, blobs, junctions or other human defined structures in images. Though being robust with some geometric transformations, unintended scenarios or non-uniform lighting variations could significantly degrade their performance. Hence, a new detector that is flexible with context change and simultaneously robust with both geometric and non-uniform illumination variations is very desirable. In this paper, we propose a solution to this challenging problem by incorporating Scale and Rotation Invariant design (named SRI-SCK) into a recently developed Sparse Coding based Key-point detector (SCK). The SCK detector is flexible in different scenarios and fully invariant to affine intensity change, yet it is not designed to handle images with drastic scale and rotation changes. In SRI-SCK, the scale invariance is implemented with an image pyramid technique while the rotation invariance is realized by combining multiple rotated versions of the dictionary used in the sparse coding step of SCK. Techniques for calculation of key-points' characteristic scales and their sub-pixel accuracy positions are also proposed. Experimental results on three public datasets demonstrate that significantly high repeatability and matching score are achieved.



### Land Cover Semantic Segmentation Using ResUNet
- **Arxiv ID**: http://arxiv.org/abs/2010.06285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06285v1)
- **Published**: 2020-10-13 10:56:09+00:00
- **Updated**: 2020-10-13 10:56:09+00:00
- **Authors**: Vasilis Pollatos, Loukas Kouvaras, Eleni Charou
- **Comment**: 21 pages , 17 figures, presented in AI in Natural Science and
  Technology (https://ainst.scify.org/) Workshop of SETN 2020
- **Journal**: None
- **Summary**: In this paper we present our work on developing an automated system for land cover classification. This system takes a multiband satellite image of an area as input and outputs the land cover map of the area at the same resolution as the input. For this purpose convolutional machine learning models were trained in the task of predicting the land cover semantic segmentation of satellite images. This is a case of supervised learning. The land cover label data were taken from the CORINE Land Cover inventory and the satellite images were taken from the Copernicus hub. As for the model, U-Net architecture variations were applied. Our area of interest are the Ionian islands (Greece). We created a dataset from scratch covering this particular area. In addition, transfer learning from the BigEarthNet dataset [1] was performed. In [1] simple classification of satellite images into the classes of CLC is performed but not segmentation as we do. However, their models have been trained into a dataset much bigger than ours, so we applied transfer learning using their pretrained models as the first part of out network, utilizing the ability these networks have developed to extract useful features from the satellite images (we transferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer learning other techniques were applied in order to overcome the limitations set by the small size of our area of interest. We used data augmentation (cutting images into overlapping patches, applying random transformations such as rotations and flips) and cross validation. The results are tested on the 3 CLC class hierarchy levels and a comparative study is made on the results of different approaches.



### Impact of Thermal Throttling on Long-Term Visual Inference in a CPU-based Edge Device
- **Arxiv ID**: http://arxiv.org/abs/2010.06291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06291v2)
- **Published**: 2020-10-13 11:15:17+00:00
- **Updated**: 2020-12-03 10:37:01+00:00
- **Authors**: Théo Benoit-Cattin, Delia Velasco-Montero, Jorge Fernández-Berni
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Many application scenarios of edge visual inference, e.g., robotics or environmental monitoring, eventually require long periods of continuous operation. In such periods, the processor temperature plays a critical role to keep a prescribed frame rate. Particularly, the heavy computational load of convolutional neural networks (CNNs) may lead to thermal throttling and hence performance degradation in few seconds. In this paper, we report and analyze the long-term performance of 80 different cases resulting from running 5 CNN models on 4 software frameworks and 2 operating systems without and with active cooling. This comprehensive study was conducted on a low-cost edge platform, namely Raspberry Pi 4B (RPi4B), under stable indoor conditions. The results show that hysteresis-based active cooling prevented thermal throttling in all cases, thereby improving the throughput up to approximately 90% versus no cooling. Interestingly, the range of fan usage during active cooling varied from 33% to 65%. Given the impact of the fan on the power consumption of the system as a whole, these results stress the importance of a suitable selection of CNN model and software components. To assess the performance in outdoor applications, we integrated an external temperature sensor with the RPi4B and conducted a set of experiments with no active cooling in a wide interval of ambient temperature, ranging from 22 {\deg}C to 36 {\deg}C. Variations up to 27.7% were measured with respect to the maximum throughput achieved in that interval. This demonstrates that ambient temperature is a critical parameter in case active cooling cannot be applied.



### MixCo: Mix-up Contrastive Learning for Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/2010.06300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06300v2)
- **Published**: 2020-10-13 11:34:25+00:00
- **Updated**: 2020-11-15 08:29:07+00:00
- **Authors**: Sungnyun Kim, Gihun Lee, Sangmin Bae, Se-Young Yun
- **Comment**: accepted in NeurIPS 2020 Workshop on Self-Supervised Learning
- **Journal**: None
- **Summary**: Contrastive learning has shown remarkable results in recent self-supervised approaches for visual representation. By learning to contrast positive pairs' representation from the corresponding negatives pairs, one can train good visual representations without human annotations. This paper proposes Mix-up Contrast (MixCo), which extends the contrastive learning concept to semi-positives encoded from the mix-up of positive and negative images. MixCo aims to learn the relative similarity of representations, reflecting how much the mixed images have the original positives. We validate the efficacy of MixCo when applied to the recent self-supervised learning algorithms under the standard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In the experiments, MixCo consistently improves test accuracy. Remarkably, the improvement is more significant when the learning capacity (e.g., model size) is limited, suggesting that MixCo might be more useful in real-world scenarios. The code is available at: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast.



### How important are faces for person re-identification?
- **Arxiv ID**: http://arxiv.org/abs/2010.06307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06307v1)
- **Published**: 2020-10-13 11:47:16+00:00
- **Updated**: 2020-10-13 11:47:16+00:00
- **Authors**: Julia Dietlmeier, Joseph Antony, Kevin McGuinness, Noel E. O'Connor
- **Comment**: 25th International Conference on Pattern Recognition (ICPR2020),
  Milan, Italy, 10-15 January 2021
- **Journal**: None
- **Summary**: This paper investigates the dependence of existing state-of-the-art person re-identification models on the presence and visibility of human faces. We apply a face detection and blurring algorithm to create anonymized versions of several popular person re-identification datasets including Market1501, DukeMTMC-reID, CUHK03, Viper, and Airport. Using a cross-section of existing state-of-the-art models that range in accuracy and computational efficiency, we evaluate the effect of this anonymization on re-identification performance using standard metrics. Perhaps surprisingly, the effect on mAP is very small, and accuracy is recovered by simply training on the anonymized versions of the data rather than the original data. These findings are consistent across multiple models and datasets. These results indicate that datasets can be safely anonymized by blurring faces without significantly impacting the performance of person reidentification systems, and may allow for the release of new richer re-identification datasets where previously there were privacy or data protection concerns.



### Audio-Visual Self-Supervised Terrain Type Discovery for Mobile Platforms
- **Arxiv ID**: http://arxiv.org/abs/2010.06318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06318v1)
- **Published**: 2020-10-13 11:56:48+00:00
- **Updated**: 2020-10-13 11:56:48+00:00
- **Authors**: Akiyoshi Kurobe, Yoshikatsu Nakajima, Hideo Saito, Kris Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to both recognize and discover terrain characteristics is an important function required for many autonomous ground robots such as social robots, assistive robots, autonomous vehicles, and ground exploration robots. Recognizing and discovering terrain characteristics is challenging because similar terrains may have very different appearances (e.g., carpet comes in many colors), while terrains with very similar appearance may have very different physical properties (e.g. mulch versus dirt). In order to address the inherent ambiguity in vision-based terrain recognition and discovery, we propose a multi-modal self-supervised learning technique that switches between audio features extracted from a mic attached to the underside of a mobile platform and image features extracted by a camera on the platform to cluster terrain types. The terrain cluster labels are then used to train an image-based convolutional neural network to predict changes in terrain types. Through experiments, we demonstrate that the proposed self-supervised terrain type discovery method achieves over 80% accuracy, which greatly outperforms several baselines and suggests strong potential for assistive applications.



### LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization
- **Arxiv ID**: http://arxiv.org/abs/2010.06323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06323v1)
- **Published**: 2020-10-13 12:15:20+00:00
- **Updated**: 2020-10-13 12:15:20+00:00
- **Authors**: Lukas von Stumberg, Patrick Wenzel, Nan Yang, Daniel Cremers
- **Comment**: International Conference on 3D Vision (3DV), 2020
- **Journal**: None
- **Summary**: We present LM-Reloc -- a novel approach for visual relocalization based on direct image alignment. In contrast to prior works that tackle the problem with a feature-based formulation, the proposed method does not rely on feature matching and RANSAC. Hence, the method can utilize not only corners but any region of the image with gradients. In particular, we propose a loss formulation inspired by the classical Levenberg-Marquardt algorithm to train LM-Net. The learned features significantly improve the robustness of direct image alignment, especially for relocalization across different conditions. To further improve the robustness of LM-Net against large image baselines, we propose a pose estimation network, CorrPoseNet, which regresses the relative pose to bootstrap the direct image alignment. Evaluations on the CARLA and Oxford RobotCar relocalization tracking benchmark show that our approach delivers more accurate results than previous state-of-the-art methods while being comparable in terms of robustness.



### Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration
- **Arxiv ID**: http://arxiv.org/abs/2010.06349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06349v2)
- **Published**: 2020-10-13 13:06:10+00:00
- **Updated**: 2021-05-16 11:21:08+00:00
- **Authors**: Zongxin Yang, Yunchao Wei, Yi Yang
- **Comment**: Accepted by TPAMI; Journal extension of arXiv:2003.08333 (ECCV 2020,
  Spotlight)
- **Journal**: None
- **Summary**: This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Unlike previous practices that focus on exploring the embedding learning of foreground object (s), we consider background should be equally treated. Thus, we propose a Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. CFBI separates the feature embedding into the foreground object region and its corresponding background region, implicitly promoting them to be more contrastive and improving the segmentation results accordingly. Moreover, CFBI performs both pixel-level matching processes and instance-level attention mechanisms between the reference and the predicted sequence, making CFBI robust to various object scales. Based on CFBI, we introduce a multi-scale matching structure and propose an Atrous Matching strategy, resulting in a more robust and efficient framework, CFBI+. We conduct extensive experiments on two popular benchmarks, i.e., DAVIS and YouTube-VOS. Without applying any simulated data for pre-training, our CFBI+ achieves the performance (J&F) of 82.9% and 82.8%, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.



### A Generalized Zero-Shot Framework for Emotion Recognition from Body Gestures
- **Arxiv ID**: http://arxiv.org/abs/2010.06362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06362v2)
- **Published**: 2020-10-13 13:16:38+00:00
- **Updated**: 2020-10-20 08:15:45+00:00
- **Authors**: Jinting Wu, Yujia Zhang, Xiaoguang Zhao, Wenbin Gao
- **Comment**: The new version adds a co-author and revises the layout of Fig.3
- **Journal**: None
- **Summary**: Although automatic emotion recognition from facial expressions and speech has made remarkable progress, emotion recognition from body gestures has not been thoroughly explored. People often use a variety of body language to express emotions, and it is difficult to enumerate all emotional body gestures and collect enough samples for each category. Therefore, recognizing new emotional body gestures is critical for better understanding human emotions. However, the existing methods fail to accurately determine which emotional state a new body gesture belongs to. In order to solve this problem, we introduce a Generalized Zero-Shot Learning (GZSL) framework, which consists of three branches to infer the emotional state of the new body gestures with only their semantic descriptions. The first branch is a Prototype-Based Detector (PBD) which is used to determine whether an sample belongs to a seen body gesture category and obtain the prediction results of the samples from the seen categories. The second branch is a Stacked AutoEncoder (StAE) with manifold regularization, which utilizes semantic representations to predict samples from unseen categories. Note that both of the above branches are for body gesture recognition. We further add an emotion classifier with a softmax layer as the third branch in order to better learn the feature representations for this emotion classification task. The input features for these three branches are learned by a shared feature extraction network, i.e., a Bidirectional Long Short-Term Memory Networks (BLSTM) with a self-attention module. We treat these three branches as subtasks and use multi-task learning strategies for joint training. The performance of our framework on an emotion recognition dataset is significantly superior to the traditional method of emotion classification and state-of-the-art zero-shot learning methods.



### Three-Dimensional Lip Motion Network for Text-Independent Speaker Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.06363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06363v1)
- **Published**: 2020-10-13 13:18:33+00:00
- **Updated**: 2020-10-13 13:18:33+00:00
- **Authors**: Jianrong Wang, Tong Wu, Shanyu Wang, Mei Yu, Qiang Fang, Ju Zhang, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Lip motion reflects behavior characteristics of speakers, and thus can be used as a new kind of biometrics in speaker recognition. In the literature, lots of works used two-dimensional (2D) lip images to recognize speaker in a textdependent context. However, 2D lip easily suffers from various face orientations. To this end, in this work, we present a novel end-to-end 3D lip motion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM) to recognize speakers in both the text-independent and text-dependent contexts. A new regional feedback module (RFM) is proposed to obtain attentions in different lip regions. Besides, prior knowledge of lip motion is investigated to complement RFM, where landmark-level and frame-level features are merged to form a better feature representation. Moreover, we present two methods, i.e., coordinate transformation and face posture correction to pre-process the LSD-AV dataset, which contains 68 speakers and 146 sentences per speaker. The evaluation results on this dataset demonstrate that our proposed 3LMNet is superior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and outperforms the state-of-the-art using 2D lip image as well as the 3D face. The code of this work is released at https://github.com/wutong18/Three-Dimensional-Lip- Motion-Network-for-Text-Independent-Speaker-Recognition.



### Coarse and fine-grained automatic cropping deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2010.06379v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.06379v2)
- **Published**: 2020-10-13 13:36:33+00:00
- **Updated**: 2020-10-14 02:42:31+00:00
- **Authors**: Jingfei Chang
- **Comment**: 12 pages, 1 table
- **Journal**: None
- **Summary**: The existing convolutional neural network pruning algorithms can be divided into two categories: coarse-grained clipping and fine-grained clipping. This paper proposes a coarse and fine-grained automatic pruning algorithm, which can achieve more efficient and accurate compression acceleration for convolutional neural networks. First, cluster the intermediate feature maps of the convolutional neural network to obtain the network structure after coarse-grained clipping, and then use the particle swarm optimization algorithm to iteratively search and optimize the structure. Finally, the optimal network tailoring substructure is obtained.



### Which Model to Transfer? Finding the Needle in the Growing Haystack
- **Arxiv ID**: http://arxiv.org/abs/2010.06402v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06402v2)
- **Published**: 2020-10-13 14:00:22+00:00
- **Updated**: 2022-03-25 08:27:57+00:00
- **Authors**: Cedric Renggli, André Susano Pinto, Luka Rimanic, Joan Puigcerver, Carlos Riquelme, Ce Zhang, Mario Lucic
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning has been recently popularized as a data-efficient alternative to training models from scratch, in particular for computer vision tasks where it provides a remarkably solid baseline. The emergence of rich model repositories, such as TensorFlow Hub, enables the practitioners and researchers to unleash the potential of these models across a wide range of downstream tasks. As these repositories keep growing exponentially, efficiently selecting a good model for the task at hand becomes paramount. We provide a formalization of this problem through a familiar notion of regret and introduce the predominant strategies, namely task-agnostic (e.g. ranking models by their ImageNet performance) and task-aware search strategies (such as linear or kNN evaluation). We conduct a large-scale empirical study and show that both task-agnostic and task-aware methods can yield high regret. We then propose a simple and computationally efficient hybrid search strategy which outperforms the existing approaches. We highlight the practical benefits of the proposed solution on a set of 19 diverse vision tasks.



### Detecting Anomalies from Video-Sequences: a Novel Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2010.06407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06407v2)
- **Published**: 2020-10-13 14:07:43+00:00
- **Updated**: 2020-10-17 14:40:54+00:00
- **Authors**: Giulia Orrù, Davide Ghiani, Maura Pintor, Gian Luca Marcialis, Fabio Roli
- **Comment**: Accepted for the 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: We present a novel descriptor for crowd behavior analysis and anomaly detection. The goal is to measure by appropriate patterns the speed of formation and disintegration of groups in the crowd. This descriptor is inspired by the concept of one-dimensional local binary patterns: in our case, such patterns depend on the number of group observed in a time window. An appropriate measurement unit, named "trit" (trinary digit), represents three possible dynamic states of groups on a certain frame. Our hypothesis is that abrupt variations of the groups' number may be due to an anomalous event that can be accordingly detected, by translating these variations on temporal trit-based sequence of strings which are significantly different from the one describing the "no-anomaly" one. Due to the peculiarity of the rationale behind this work, relying on the number of groups, three different methods of people group's extraction are compared. Experiments are carried out on the Motion-Emotion benchmark data set. Reported results point out in which cases the trit-based measurement of group dynamics allows us to detect the anomaly. Besides the promising performance of our approach, we show how it is correlated with the anomaly typology and the camera's perspective to the crowd's flow (frontal, lateral).



### Electroencephalography signal processing based on textural features for monitoring the driver's state by a Brain-Computer Interface
- **Arxiv ID**: http://arxiv.org/abs/2010.06412v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06412v2)
- **Published**: 2020-10-13 14:16:00+00:00
- **Updated**: 2020-10-17 14:46:00+00:00
- **Authors**: Giulia Orrù, Marco Micheletto, Fabio Terranova, Gian Luca Marcialis
- **Comment**: Accepted for the 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: In this study we investigate a textural processing method of electroencephalography (EEG) signal as an indicator to estimate the driver's vigilance in a hypothetical Brain-Computer Interface (BCI) system. The novelty of the solution proposed relies on employing the one-dimensional Local Binary Pattern (1D-LBP) algorithm for feature extraction from pre-processed EEG data. From the resulting feature vector, the classification is done according to three vigilance classes: awake, tired and drowsy. The claim is that the class transitions can be detected by describing the variations of the micro-patterns' occurrences along the EEG signal. The 1D-LBP is able to describe them by detecting mutual variations of the signal temporarily "close" as a short bit-code. Our analysis allows to conclude that the 1D-LBP adoption has led to significant performance improvement. Moreover, capturing the class transitions from the EEG signal is effective, although the overall performance is not yet good enough to develop a BCI for assessing the driver's vigilance in real environments.



### RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification
- **Arxiv ID**: http://arxiv.org/abs/2010.06440v1
- **DOI**: 10.1016/j.media.2019.101549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06440v1)
- **Published**: 2020-10-13 14:55:47+00:00
- **Updated**: 2020-10-13 14:55:47+00:00
- **Authors**: Shujun Wang, Yaxi Zhu, Lequan Yu, Hao Chen, Huangjing Lin, Xiangbo Wan, Xinjuan Fan, Pheng-Ann Hen
- **Comment**: Accepted at Medical Image Analysis. Code:
  https://github.com/EmmaW8/RMDL
- **Journal**: None
- **Summary**: The whole slide histopathology images (WSIs) play a critical role in gastric cancer diagnosis. However, due to the large scale of WSIs and various sizes of the abnormal area, how to select informative regions and analyze them are quite challenging during the automatic diagnosis process. The multi-instance learning based on the most discriminative instances can be of great benefit for whole slide gastric image diagnosis. In this paper, we design a recalibrated multi-instance deep learning method (RMDL) to address this challenging problem. We first select the discriminative instances, and then utilize these instances to diagnose diseases based on the proposed RMDL approach. The designed RMDL network is capable of capturing instance-wise dependencies and recalibrating instance features according to the importance coefficient learned from the fused features. Furthermore, we build a large whole-slide gastric histopathology image dataset with detailed pixel-level annotations. Experimental results on the constructed gastric dataset demonstrate the significant improvement on the accuracy of our proposed framework compared with other state-of-the-art multi-instance learning methods. Moreover, our method is general and can be extended to other diagnosis tasks of different cancer types based on WSIs.



### A review of 3D human pose estimation algorithms for markerless motion capture
- **Arxiv ID**: http://arxiv.org/abs/2010.06449v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2010.06449v3)
- **Published**: 2020-10-13 15:07:01+00:00
- **Updated**: 2021-07-12 17:07:05+00:00
- **Authors**: Yann Desmarais, Denis Mottet, Pierre Slangen, Philippe Montesinos
- **Comment**: 25 pages, 3 figures
- **Journal**: None
- **Summary**: Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice. Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.



### Improving Road Signs Detection performance by Combining the Features of Hough Transform and Texture
- **Arxiv ID**: http://arxiv.org/abs/2010.06453v1
- **DOI**: 10.5120/12767-8795
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06453v1)
- **Published**: 2020-10-13 15:09:29+00:00
- **Updated**: 2020-10-13 15:09:29+00:00
- **Authors**: Tarik Ayaou, Mourad Boussaid, Karim Afdel, Abdellah Amghar
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: With the large uses of the intelligent systems in different domains, and in order to increase the drivers and pedestrians safety, the road and traffic sign recognition system has been a challenging issue and an important task for many years. But studies, done in this field of detection and recognition of traffic signs in an image, which are interested in the Arab context, are still insufficient. Detection of the road signs present in the scene is the one of the main stages of the traffic sign detection and recognition. In this paper, an efficient solution to enhance road signs detection, including Arabic context, performance based on color segmentation, Randomized Hough Transform and the combination of Zernike moments and Haralick features has been made. Segmentation stage is useful to determine the Region of Interest (ROI) in the image. The Randomized Hough Transform (RHT) is used to detect the circular and octagonal shapes. This stage is improved by the extraction of the Haralick features and Zernike moments. Furthermore, we use it as input of a classifier based on SVM. Experimental results show that the proposed approach allows us to perform the measurements precision.



### Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2010.06469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06469v1)
- **Published**: 2020-10-13 15:21:14+00:00
- **Updated**: 2020-10-13 15:21:14+00:00
- **Authors**: Clemens-Alexander Brust, Björn Barz, Joachim Denzler
- **Comment**: 9 pages pre-print. Accepted for publication at ICPR 2020
- **Journal**: None
- **Summary**: Noisy data, crawled from the web or supplied by volunteers such as Mechanical Turkers or citizen scientists, is considered an alternative to professionally labeled data. There has been research focused on mitigating the effects of label noise. It is typically modeled as inaccuracy, where the correct label is replaced by an incorrect label from the same set. We consider an additional dimension of label noise: imprecision. For example, a non-breeding snow bunting is labeled as a bird. This label is correct, but not as precise as the task requires.   Standard softmax classifiers cannot learn from such a weak label because they consider all classes mutually exclusive, which non-breeding snow bunting and bird are not. We propose CHILLAX (Class Hierarchies for Imprecise Label Learning and Annotation eXtrapolation), a method based on hierarchical classification, to fully utilize labels of any precision.   Experiments on noisy variants of NABirds and ILSVRC2012 show that our method outperforms strong baselines by as much as 16.4 percentage points, and the current state of the art by up to 3.9 percentage points.



### Multi-Scale Networks for 3D Human Pose Estimation with Inference Stage Optimization
- **Arxiv ID**: http://arxiv.org/abs/2010.06844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06844v2)
- **Published**: 2020-10-13 15:24:28+00:00
- **Updated**: 2020-10-16 19:42:53+00:00
- **Authors**: Cheng Yu, Bo Wang, Bo Yang, Robby T. Tan
- **Comment**: 14 pages, 13 figures. arXiv admin note: substantial text overlap with
  arXiv:2004.11822
- **Journal**: None
- **Summary**: Estimating 3D human poses from a monocular video is still a challenging task. Many existing methods' performance drops when the target person is occluded by other objects, or the motion is too fast/slow relative to the scale and speed of the training data. Moreover, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional networks (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground-truth data, we further utilize 2D video data to inject a semi-supervised learning capability to our network. Moreover, we observe that there is a discrepancy between 3D pose prediction and 2D pose estimation due to different pose variations between video and image training datasets. We, therefore propose a confidence-based inference stage optimization to adaptively enforce 3D pose projection to match 2D pose estimation to further improve final pose prediction accuracy. Experiments on public datasets validate the effectiveness of our method, and our ablation studies show the strengths of our network's individual submodules.



### Satellite Image Classification with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.06497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06497v1)
- **Published**: 2020-10-13 15:56:58+00:00
- **Updated**: 2020-10-13 15:56:58+00:00
- **Authors**: Mark Pritt, Gary Chern
- **Comment**: 7 pages, 18 figures, 2017 IEEE Applied Imagery Pattern Recognition
  Workshop (AIPR)
- **Journal**: 2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),
  Washington, DC, USA, 2017, pp. 1-7
- **Summary**: Satellite imagery is important for many applications including disaster response, law enforcement, and environmental monitoring. These applications require the manual identification of objects and facilities in the imagery. Because the geographic expanses to be covered are great and the analysts available to conduct the searches are few, automation is required. Yet traditional object detection and classification algorithms are too inaccurate and unreliable to solve the problem. Deep learning is a family of machine learning algorithms that have shown promise for the automation of such tasks. It has achieved success in image understanding by means of convolutional neural networks. In this paper we apply them to the problem of object and facility recognition in high-resolution, multi-spectral satellite imagery. We describe a deep learning system for classifying objects and facilities from the IARPA Functional Map of the World (fMoW) dataset into 63 different classes. The system consists of an ensemble of convolutional neural networks and additional neural networks that integrate satellite metadata with image features. It is implemented in Python using the Keras and TensorFlow deep learning libraries and runs on a Linux server with an NVIDIA Titan X graphics card. At the time of writing the system is in 2nd place in the fMoW TopCoder competition. Its total accuracy is 83%, the F1 score is 0.797, and it classifies 15 of the classes with accuracies of 95% or better.



### Deep Learning for Recognizing Mobile Targets in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.06520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06520v1)
- **Published**: 2020-10-13 16:26:42+00:00
- **Updated**: 2020-10-13 16:26:42+00:00
- **Authors**: Mark Pritt
- **Comment**: 7 pages, 15 figures, 2018 IEEE Applied Imagery Pattern Recognition
  Workshop (AIPR)
- **Journal**: 2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),
  Washington, DC, USA, 2018, pp. 1-7
- **Summary**: There is an increasing demand for software that automatically detects and classifies mobile targets such as airplanes, cars, and ships in satellite imagery. Applications of such automated target recognition (ATR) software include economic forecasting, traffic planning, maritime law enforcement, and disaster response. This paper describes the extension of a convolutional neural network (CNN) for classification to a sliding window algorithm for detection. It is evaluated on mobile targets of the xView dataset, on which it achieves detection and classification accuracies higher than 95%.



### Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!
- **Arxiv ID**: http://arxiv.org/abs/2010.06572v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06572v1)
- **Published**: 2020-10-13 17:45:28+00:00
- **Updated**: 2020-10-13 17:45:28+00:00
- **Authors**: Jack Hessel, Lillian Lee
- **Comment**: None
- **Journal**: Published in EMNLP 2020
- **Summary**: Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.



### Scenic: A Language for Scenario Specification and Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.06580v1
- **DOI**: None
- **Categories**: **cs.PL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06580v1)
- **Published**: 2020-10-13 17:58:31+00:00
- **Updated**: 2020-10-13 17:58:31+00:00
- **Authors**: Daniel J. Fremont, Edward Kim, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia
- **Comment**: Supercedes arXiv:1809.09310
- **Journal**: None
- **Summary**: We propose a new probabilistic programming language for the design and analysis of cyber-physical systems, especially those based on machine learning. Specifically, we consider the problems of training a system to be robust to rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs, then sampling these to generate specialized training and test data. More generally, such languages can be used to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment at any point in time is a 'scene', a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes and the behaviors of their agents over time. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.



### Training independent subnetworks for robust prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.06610v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.06610v2)
- **Published**: 2020-10-13 18:05:13+00:00
- **Updated**: 2021-08-04 23:30:03+00:00
- **Authors**: Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew M. Dai, Dustin Tran
- **Comment**: Updated to the ICLR camera ready version, added reference to Soflaei
  et al. 2020
- **Journal**: None
- **Summary**: Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.



### On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation
- **Arxiv ID**: http://arxiv.org/abs/2010.06626v2
- **DOI**: 10.1016/j.robot.2020.103701
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.06626v2)
- **Published**: 2020-10-13 18:37:38+00:00
- **Updated**: 2020-12-29 01:09:57+00:00
- **Authors**: Raul de Queiroz Mendes, Eduardo Godinho Ribeiro, Nicolas dos Santos Rosa, Valdir Grassi Jr
- **Comment**: 29 pages, 16 figures. Preprint published in the Elsevier's Robotics
  and Autonomous Systems journal on November 23, 2020
- **Journal**: Journal: Robotics and Autonomous Systems, publisher: Elsevier,
  volume number: 136, year: 2020, page number: 103701
- **Summary**: Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.



### Video Action Understanding
- **Arxiv ID**: http://arxiv.org/abs/2010.06647v2
- **DOI**: 10.1109/ACCESS.2021.3115476
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06647v2)
- **Published**: 2020-10-13 19:29:41+00:00
- **Updated**: 2021-10-03 21:02:42+00:00
- **Authors**: Matthew Hutchinson, Vijay Gadepally
- **Comment**: Accepted for publication in IEEE Access
- **Journal**: IEEE Access, Vol. 9, 2021
- **Summary**: Many believe that the successes of deep learning on image understanding problems can be replicated in the realm of video understanding. However, due to the scale and temporal nature of video, the span of video understanding problems and the set of proposed deep learning solutions is arguably wider and more diverse than those of their 2D image siblings. Finding, identifying, and predicting actions are a few of the most salient tasks in this emerging and rapidly evolving field. With a pedagogical emphasis, this tutorial introduces and systematizes fundamental topics, basic concepts, and notable examples in supervised video action understanding. Specifically, we clarify a taxonomy of action problems, catalog and highlight video datasets, describe common video data preparation methods, present the building blocks of state-of-the art deep learning model architectures, and formalize domain-specific metrics to baseline proposed solutions. This tutorial is intended to be accessible to a general computer science audience and assumes a conceptual understanding of supervised learning.



### Intrapersonal Parameter Optimization for Offline Handwritten Signature Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.06663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06663v1)
- **Published**: 2020-10-13 19:54:02+00:00
- **Updated**: 2020-10-13 19:54:02+00:00
- **Authors**: Teruo M. Maruyama, Luiz S. Oliveira, Alceu S. Britto Jr, Robert Sabourin
- **Comment**: 16 pages, 11 figures, To appear in the IEEE Transactions on
  Information Forensics & Security
- **Journal**: None
- **Summary**: Usually, in a real-world scenario, few signature samples are available to train an automatic signature verification system (ASVS). However, such systems do indeed need a lot of signatures to achieve an acceptable performance. Neuromotor signature duplication methods and feature space augmentation methods may be used to meet the need for an increase in the number of samples. Such techniques manually or empirically define a set of parameters to introduce a degree of writer variability. Therefore, in the present study, a method to automatically model the most common writer variability traits is proposed. The method is used to generate offline signatures in the image and the feature space and train an ASVS. We also introduce an alternative approach to evaluate the quality of samples considering their feature vectors. We evaluated the performance of an ASVS with the generated samples using three well-known offline signature datasets: GPDS, MCYT-75, and CEDAR. In GPDS-300, when the SVM classifier was trained using one genuine signature per writer and the duplicates generated in the image space, the Equal Error Rate (EER) decreased from 5.71% to 1.08%. Under the same conditions, the EER decreased to 1.04% using the feature space augmentation technique. We also verified that the model that generates duplicates in the image space reproduces the most common writer variability traits in the three different datasets.



### LiDAM: Semi-Supervised Learning with Localized Domain Adaptation and Iterative Matching
- **Arxiv ID**: http://arxiv.org/abs/2010.06668v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06668v2)
- **Published**: 2020-10-13 19:57:32+00:00
- **Updated**: 2020-11-23 22:42:54+00:00
- **Authors**: Qun Liu, Matthew Shreve, Raja Bala
- **Comment**: None
- **Journal**: None
- **Summary**: Although data is abundant, data labeling is expensive. Semi-supervised learning methods combine a few labeled samples with a large corpus of unlabeled data to effectively train models. This paper introduces our proposed method LiDAM, a semi-supervised learning approach rooted in both domain adaptation and self-paced learning. LiDAM first performs localized domain shifts to extract better domain-invariant features for the model that results in more accurate clusters and pseudo-labels. These pseudo-labels are then aligned with real class labels in a self-paced fashion using a novel iterative matching technique that is based on majority consistency over high-confidence predictions. Simultaneously, a final classifier is trained to predict ground-truth labels until convergence. LiDAM achieves state-of-the-art performance on the CIFAR-100 dataset, outperforming FixMatch (73.50% vs. 71.82%) when using 2500 labels.



### A Multi-Modal Method for Satire Detection using Textual and Visual Cues
- **Arxiv ID**: http://arxiv.org/abs/2010.06671v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06671v1)
- **Published**: 2020-10-13 20:08:29+00:00
- **Updated**: 2020-10-13 20:08:29+00:00
- **Authors**: Lily Li, Or Levi, Pedram Hosseini, David A. Broniatowski
- **Comment**: Accepted to the Third Workshop on NLP for Internet Freedom (NLP4IF):
  Censorship, Disinformation, and Propaganda. Co-located with COLING 2020
- **Journal**: None
- **Summary**: Satire is a form of humorous critique, but it is sometimes misinterpreted by readers as legitimate news, which can lead to harmful consequences. We observe that the images used in satirical news articles often contain absurd or ridiculous content and that image manipulation is used to create fictional scenarios. While previous work have studied text-based methods, in this work we propose a multi-modal approach based on state-of-the-art visiolinguistic model ViLBERT. To this end, we create a new dataset consisting of images and headlines of regular and satirical news for the task of satire detection. We fine-tune ViLBERT on the dataset and train a convolutional neural network that uses an image forensics technique. Evaluation on the dataset shows that our proposed multi-modal approach outperforms image-only, text-only, and simple fusion baselines.



### Are all negatives created equal in contrastive instance discrimination?
- **Arxiv ID**: http://arxiv.org/abs/2010.06682v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06682v2)
- **Published**: 2020-10-13 20:52:10+00:00
- **Updated**: 2020-10-25 16:55:39+00:00
- **Authors**: Tiffany Tianhui Cai, Jonathan Frankle, David J. Schwab, Ari S. Morcos
- **Comment**: Fixed author name error
- **Journal**: None
- **Summary**: Self-supervised learning has recently begun to rival supervised learning on computer vision tasks. Many of the recent approaches have been based on contrastive instance discrimination (CID), in which the network is trained to recognize two augmented versions of the same instance (a query and positive) while discriminating against a pool of other instances (negatives). The learned representation is then used on downstream tasks such as image classification. Using methodology from MoCo v2 (Chen et al., 2020), we divided negatives by their difficulty for a given query and studied which difficulty ranges were most important for learning useful representations. We found a minority of negatives -- the hardest 5% -- were both necessary and sufficient for the downstream task to reach nearly full accuracy. Conversely, the easiest 95% of negatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of negatives were unnecessary and sometimes detrimental. Finally, we studied the properties of negatives that affect their hardness, and found that hard negatives were more semantically similar to the query, and that some negatives were more consistently easy or hard than we would expect by chance. Together, our results indicate that negatives vary in importance and that CID may benefit from more intelligent negative treatment.



### Random Network Distillation as a Diversity Metric for Both Image and Text Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.06715v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06715v1)
- **Published**: 2020-10-13 22:03:52+00:00
- **Updated**: 2020-10-13 22:03:52+00:00
- **Authors**: Liam Fowl, Micah Goldblum, Arjun Gupta, Amr Sharaf, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.



### Measuring Visual Generalization in Continuous Control from Pixels
- **Arxiv ID**: http://arxiv.org/abs/2010.06740v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.06740v2)
- **Published**: 2020-10-13 23:42:40+00:00
- **Updated**: 2020-11-27 20:33:03+00:00
- **Authors**: Jake Grigsby, Yanjun Qi
- **Comment**: A total of 20 pages, 8 pages as the main text
- **Journal**: None
- **Summary**: Self-supervised learning and data augmentation have significantly reduced the performance gap between state and image-based reinforcement learning agents in continuous control tasks. However, it is still unclear whether current techniques can face a variety of visual conditions required by real-world environments. We propose a challenging benchmark that tests agents' visual generalization by adding graphical variety to existing continuous control domains. Our empirical analysis shows that current methods struggle to generalize across a diverse set of visual changes, and we examine the specific factors of variation that make these tasks difficult. We find that data augmentation techniques outperform self-supervised learning approaches and that more significant image transformations provide better visual generalization \footnote{The benchmark and our augmented actor-critic implementation are open-sourced @ https://github.com/QData/dmc_remastered)



### A Very Compact Embedded CNN Processor Design Based on Logarithmic Computing
- **Arxiv ID**: http://arxiv.org/abs/2010.11686v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11686v1)
- **Published**: 2020-10-13 23:48:36+00:00
- **Updated**: 2020-10-13 23:48:36+00:00
- **Authors**: Tsung-Ying Lu, Hsu-Hsun Chin, Hsin-I Wu, Ren-Song Tsay
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a very compact embedded CNN processor design based on a modified logarithmic computing method using very low bit-width representation. Our high-quality CNN processor can easily fit into edge devices. For Yolov2, our processing circuit takes only 0.15 mm2 using TSMC 40 nm cell library. The key idea is to constrain the activation and weight values of all layers uniformly to be within the range [-1, 1] and produce low bit-width logarithmic representation. With the uniform representations, we devise a unified, reusable CNN computing kernel and significantly reduce computing resources. The proposed approach has been extensively evaluated on many popular image classification CNN models (AlexNet, VGG16, and ResNet-18/34) and object detection models (Yolov2). The hardware-implemented results show that our design consumes only minimal computing and storage resources, yet attains very high accuracy. The design is thoroughly verified on FPGAs, and the SoC integration is underway with promising results. With extremely efficient resource and energy usage, our design is excellent for edge computing purposes.



