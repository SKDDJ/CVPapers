# Arxiv Papers in cs.CV on 2020-10-07
### Channel Recurrent Attention Networks for Video Pedestrian Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2010.03108v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03108v1)
- **Published**: 2020-10-07 02:01:13+00:00
- **Updated**: 2020-10-07 02:01:13+00:00
- **Authors**: Pengfei Fang, Pan Ji, Jieming Zhou, Lars Petersson, Mehrtash Harandi
- **Comment**: To appear in ACCV 2020
- **Journal**: None
- **Summary**: Full attention, which generates an attention value per element of the input feature maps, has been successfully demonstrated to be beneficial in visual tasks. In this work, we propose a fully attentional network, termed {\it channel recurrent attention network}, for the task of video pedestrian retrieval. The main attention unit, \textit{channel recurrent attention}, identifies attention maps at the frame level by jointly leveraging spatial and channel patterns via a recurrent neural network. This channel recurrent attention is designed to build a global receptive field by recurrently receiving and learning the spatial vectors. Then, a \textit{set aggregation} cell is employed to generate a compact video representation. Empirical experimental results demonstrate the superior performance of the proposed deep network, outperforming current state-of-the-art results across standard video person retrieval benchmarks, and a thorough ablation study shows the effectiveness of the proposed units.



### Kartta Labs: Collaborative Time Travel
- **Arxiv ID**: http://arxiv.org/abs/2010.06536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.06536v1)
- **Published**: 2020-10-07 02:19:32+00:00
- **Updated**: 2020-10-07 02:19:32+00:00
- **Authors**: Sasan Tavakkol, Feng Han, Brandon Mayer, Mark Phillips, Cyrus Shahabi, Yao-Yi Chiang, Raimondas Kiveris
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the modular and scalable design of Kartta Labs, an open source, open data, and scalable system for virtually reconstructing cities from historical maps and photos. Kartta Labs relies on crowdsourcing and artificial intelligence consisting of two major modules: Maps and 3D models. Each module, in turn, consists of sub-modules that enable the system to reconstruct a city from historical maps and photos. The result is a spatiotemporal reference that can be used to integrate various collected data (curated, sensed, or crowdsourced) for research, education, and entertainment purposes. The system empowers the users to experience collaborative time travel such that they work together to reconstruct the past and experience it on an open source and open data platform.



### SLCRF: Subspace Learning with Conditional Random Field for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.03115v1
- **DOI**: 10.1109/TGRS.2020.3011429
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2010.03115v1)
- **Published**: 2020-10-07 02:25:32+00:00
- **Updated**: 2020-10-07 02:25:32+00:00
- **Authors**: Yun Cao, Jie Mei, Yuebin Wang, Liqiang Zhang, Junhuan Peng, Bing Zhang, Lihua Li, Yibo Zheng
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Subspace learning (SL) plays an important role in hyperspectral image (HSI) classification, since it can provide an effective solution to reduce the redundant information in the image pixels of HSIs. Previous works about SL aim to improve the accuracy of HSI recognition. Using a large number of labeled samples, related methods can train the parameters of the proposed solutions to obtain better representations of HSI pixels. However, the data instances may not be sufficient enough to learn a precise model for HSI classification in real applications. Moreover, it is well-known that it takes much time, labor and human expertise to label HSI images. To avoid the aforementioned problems, a novel SL method that includes the probability assumption called subspace learning with conditional random field (SLCRF) is developed. In SLCRF, first, the 3D convolutional autoencoder (3DCAE) is introduced to remove the redundant information in HSI pixels. In addition, the relationships are also constructed using the spectral-spatial information among the adjacent pixels. Then, the conditional random field (CRF) framework can be constructed and further embedded into the HSI SL procedure with the semi-supervised approach. Through the linearized alternating direction method termed LADMAP, the objective function of SLCRF is optimized using a defined iterative algorithm. The proposed method is comprehensively evaluated using the challenging public HSI datasets. We can achieve stateof-the-art performance using these HSI sets.



### DML-GANR: Deep Metric Learning With Generative Adversarial Network Regularization for High Spatial Resolution Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2010.03116v1
- **DOI**: 10.1109/TGRS.2020.2991545
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2010.03116v1)
- **Published**: 2020-10-07 02:26:03+00:00
- **Updated**: 2020-10-07 02:26:03+00:00
- **Authors**: Yun Cao, Yuebin Wang, Junhuan Peng, Liqiang Zhang, Linlin Xu, Kai Yan, Lihua Li
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: With a small number of labeled samples for training, it can save considerable manpower and material resources, especially when the amount of high spatial resolution remote sensing images (HSR-RSIs) increases considerably. However, many deep models face the problem of overfitting when using a small number of labeled samples. This might degrade HSRRSI retrieval accuracy. Aiming at obtaining more accurate HSR-RSI retrieval performance with small training samples, we develop a deep metric learning approach with generative adversarial network regularization (DML-GANR) for HSR-RSI retrieval. The DML-GANR starts from a high-level feature extraction (HFE) to extract high-level features, which includes convolutional layers and fully connected (FC) layers. Each of the FC layers is constructed by deep metric learning (DML) to maximize the interclass variations and minimize the intraclass variations. The generative adversarial network (GAN) is adopted to mitigate the overfitting problem and validate the qualities of extracted high-level features. DML-GANR is optimized through a customized approach, and the optimal parameters are obtained. The experimental results on the three data sets demonstrate the superior performance of DML-GANR over state-of-the-art techniques in HSR-RSI retrieval.



### A Fast and Effective Method of Macula Automatic Detection for Retina Images
- **Arxiv ID**: http://arxiv.org/abs/2010.03122v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03122v1)
- **Published**: 2020-10-07 02:41:29+00:00
- **Updated**: 2020-10-07 02:41:29+00:00
- **Authors**: Yukang Jiang, Jianying Pan, Yanhe Shen, Jin Zhu, Jiamin Huang, Huirui Xie, Xueqin Wang, Yan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Retina image processing is one of the crucial and popular topics of medical image processing. The macula fovea is responsible for sharp central vision, which is necessary for human behaviors where visual detail is of primary importance, such as reading, writing, driving, etc. This paper proposes a novel method to locate the macula through a series of morphological processing. On the premise of maintaining high accuracy, our approach is simpler and faster than others. Furthermore, for the hospital's real images, our method is also able to detect the macula robustly.



### Conditional Generative Modeling via Learning the Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2010.03132v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03132v2)
- **Published**: 2020-10-07 03:11:34+00:00
- **Updated**: 2020-10-09 03:29:17+00:00
- **Authors**: Sameera Ramasinghe, Kanchana Ranasinghe, Salman Khan, Nick Barnes, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning has achieved appealing results on several machine learning tasks, most of the models are deterministic at inference, limiting their application to single-modal settings. We propose a novel general-purpose framework for conditional generation in multimodal spaces, that uses latent variables to model generalizable learning patterns while minimizing a family of regression cost functions. At inference, the latent variables are optimized to find optimal solutions corresponding to multiple output modes. Compared to existing generative solutions, in multimodal spaces, our approach demonstrates faster and stable convergence, and can learn better representations for downstream tasks. Importantly, it provides a simple generic model that can beat highly engineered pipelines tailored using domain expertise on a variety of tasks, while generating diverse outputs. Our codes will be released.



### VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks
- **Arxiv ID**: http://arxiv.org/abs/2010.03182v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03182v3)
- **Published**: 2020-10-07 05:25:30+00:00
- **Updated**: 2020-10-25 05:21:52+00:00
- **Authors**: Soyeon Caren Han, Siqu Long, Siwen Luo, Kunze Wang, Josiah Poon
- **Comment**: Accepted by COLING 2020
- **Journal**: None
- **Summary**: Text-to-image multimodal tasks, generating/retrieving an image from a given text description, are extremely challenging tasks since raw text descriptions cover quite limited information in order to fully describe visually realistic images. We propose a new visual contextual text representation for text-to-image multimodal tasks, VICTR, which captures rich visual semantic information of objects from the text input. First, we use the text description as initial input and conduct dependency parsing to extract the syntactic structure and analyse the semantic aspect, including object quantities, to extract the scene graph. Then, we train the extracted objects, attributes, and relations in the scene graph and the corresponding geometric relation information using Graph Convolutional Networks, and it generates text representation which integrates textual and visual semantic information. The text representation is aggregated with word-level and sentence-level embedding to generate both visual contextual word and sentence representation. For the evaluation, we attached VICTR to the state-of-the-art models in text-to-image generation.VICTR is easily added to existing models and improves across both quantitative and qualitative aspects.



### Visual Object Recognition in Indoor Environments Using Topologically Persistent Features
- **Arxiv ID**: http://arxiv.org/abs/2010.03196v5
- **DOI**: 10.1109/LRA.2021.3099460
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.03196v5)
- **Published**: 2020-10-07 06:04:17+00:00
- **Updated**: 2021-07-28 18:05:18+00:00
- **Authors**: Ekta U. Samani, Xingjian Yang, Ashis G. Banerjee
- **Comment**: This work has been accepted for publication in the IEEE Robotics And
  Automation Letters
- **Journal**: None
- **Summary**: Object recognition in unseen indoor environments remains a challenging problem for visual perception of mobile robots. In this letter, we propose the use of topologically persistent features, which rely on the objects' shape information, to address this challenge. In particular, we extract two kinds of features, namely, sparse persistence image (PI) and amplitude, by applying persistent homology to multi-directional height function-based filtrations of the cubical complexes representing the object segmentation maps. The features are then used to train a fully connected network for recognition. For performance evaluation, in addition to a widely used shape dataset and a benchmark indoor scenes dataset, we collect a new dataset, comprising scene images from two different environments, namely, a living room and a mock warehouse. The scenes are captured using varying camera poses under different illumination conditions and include up to five different objects from a given set of fourteen objects. On the benchmark indoor scenes dataset, sparse PI features show better recognition performance in unseen environments than the features learned using the widely used ResNetV2-56 and EfficientNet-B4 models. Further, they provide slightly higher recall and accuracy values than Faster R-CNN, an end-to-end object detection method, and its state-of-the-art variant, Domain Adaptive Faster R-CNN. The performance of our methods also remains relatively unchanged from the training environment (living room) to the unseen environment (mock warehouse) in the new dataset. In contrast, the performance of the object detection methods drops substantially. We also implement the proposed method on a real-world robot to demonstrate its usefulness.



### WDN: A Wide and Deep Network to Divide-and-Conquer Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.03199v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07 (Primary) 68T45, 68U10 (Secondary), I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.03199v1)
- **Published**: 2020-10-07 06:15:11+00:00
- **Updated**: 2020-10-07 06:15:11+00:00
- **Authors**: Vikram Singh, Anurag Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: Divide and conquer is an established algorithm design paradigm that has proven itself to solve a variety of problems efficiently. However, it is yet to be fully explored in solving problems with a neural network, particularly the problem of image super-resolution. In this work, we propose an approach to divide the problem of image super-resolution into multiple sub-problems and then solve/conquer them with the help of a neural network. Unlike a typical deep neural network, we design an alternate network architecture that is much wider (along with being deeper) than existing networks and is specially designed to implement the divide-and-conquer design paradigm with a neural network. Additionally, a technique to calibrate the intensities of feature map pixels is being introduced. Extensive experimentation on five datasets reveals that our approach towards the problem and the proposed architecture generate better and sharper results than current state-of-the-art methods.



### M3Lung-Sys: A Deep Learning System for Multi-Class Lung Pneumonia Screening from CT Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.03201v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03201v1)
- **Published**: 2020-10-07 06:22:24+00:00
- **Updated**: 2020-10-07 06:22:24+00:00
- **Authors**: Xuelin Qian, Huazhu Fu, Weiya Shi, Tao Chen, Yanwei Fu, Fei Shan, Xiangyang Xue
- **Comment**: IEEE Journal of Biomedical and Health Informatics (JBHI), 2020
- **Journal**: None
- **Summary**: To counter the outbreak of COVID-19, the accurate diagnosis of suspected cases plays a crucial role in timely quarantine, medical treatment, and preventing the spread of the pandemic. Considering the limited training cases and resources (e.g, time and budget), we propose a Multi-task Multi-slice Deep Learning System (M3Lung-Sys) for multi-class lung pneumonia screening from CT imaging, which only consists of two 2D CNN networks, i.e., slice- and patient-level classification networks. The former aims to seek the feature representations from abundant CT slices instead of limited CT volumes, and for the overall pneumonia screening, the latter one could recover the temporal information by feature refinement and aggregation between different slices. In addition to distinguish COVID-19 from Healthy, H1N1, and CAP cases, our M 3 Lung-Sys also be able to locate the areas of relevant lesions, without any pixel-level annotation. To further demonstrate the effectiveness of our model, we conduct extensive experiments on a chest CT imaging dataset with a total of 734 patients (251 healthy people, 245 COVID-19 patients, 105 H1N1 patients, and 133 CAP patients). The quantitative results with plenty of metrics indicate the superiority of our proposed model on both slice- and patient-level classification tasks. More importantly, the generated lesion location maps make our system interpretable and more valuable to clinicians.



### RealSmileNet: A Deep End-To-End Network for Spontaneous and Posed Smile Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.03203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03203v1)
- **Published**: 2020-10-07 06:23:38+00:00
- **Updated**: 2020-10-07 06:23:38+00:00
- **Authors**: Yan Yang, Md Zakir Hossain, Tom Gedeon, Shafin Rahman
- **Comment**: Accepted by ACCV
- **Journal**: None
- **Summary**: Smiles play a vital role in the understanding of social interactions within different communities, and reveal the physical state of mind of people in both real and deceptive ways. Several methods have been proposed to recognize spontaneous and posed smiles. All follow a feature-engineering based pipeline requiring costly pre-processing steps such as manual annotation of face landmarks, tracking, segmentation of smile phases, and hand-crafted features. The resulting computation is expensive, and strongly dependent on pre-processing steps. We investigate an end-to-end deep learning model to address these problems, the first end-to-end model for spontaneous and posed smile recognition. Our fully automated model is fast and learns the feature extraction processes by training a series of convolution and ConvLSTM layer from scratch. Our experiments on four datasets demonstrate the robustness and generalization of the proposed model by achieving state-of-the-art performances.



### Designing, Playing, and Performing with a Vision-based Mouth Interface
- **Arxiv ID**: http://arxiv.org/abs/2010.03213v1
- **DOI**: 10.5281/zenodo.1176529
- **Categories**: **cs.HC**, cs.CV, cs.SD, eess.AS, H.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.03213v1)
- **Published**: 2020-10-07 06:47:42+00:00
- **Updated**: 2020-10-07 06:47:42+00:00
- **Authors**: Michael J. Lyons, Michael Haehnel, Nobuji Tetsutani
- **Comment**: Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2003
- **Journal**: None
- **Summary**: The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.



### Sonification of Facial Actions for Musical Expression
- **Arxiv ID**: http://arxiv.org/abs/2010.03223v1
- **DOI**: 10.5281/zenodo.1176749
- **Categories**: **cs.HC**, cs.CV, cs.SD, eess.AS, H.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.03223v1)
- **Published**: 2020-10-07 07:04:07+00:00
- **Updated**: 2020-10-07 07:04:07+00:00
- **Authors**: Mathias Funk, Kazuhiro Kuwabara, Michael J. Lyons
- **Comment**: Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2005 (NIME-05)
- **Journal**: None
- **Summary**: The central role of the face in social interaction and non-verbal communication suggests we explore facial action as a means of musical expression. This paper presents the design, implementation, and preliminary studies of a novel system utilizing face detection and optic flow algorithms to associate facial movements with sound synthesis in a topographically specific fashion. We report on our experience with various gesture-to-sound mappings and applications, and describe our preliminary experiments at musical performance using the system.



### A Brief Review of Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.03978v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03978v1)
- **Published**: 2020-10-07 07:05:32+00:00
- **Updated**: 2020-10-07 07:05:32+00:00
- **Authors**: Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, Hamid R. Arabnia
- **Comment**: None
- **Journal**: None
- **Summary**: Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, This assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources, or having an out-dated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a sub-field within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, It presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.



### COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.05690v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05690v3)
- **Published**: 2020-10-07 07:43:57+00:00
- **Updated**: 2021-08-07 10:20:14+00:00
- **Authors**: Lalith Bharadwaj B, Rohit Boddeda, Sai Vardhan K, Madhu G
- **Comment**: This paper has serious technical concerns. The diagnostic model which
  was built is inaccurate and the results are flawed
- **Journal**: None
- **Summary**: The issue of COVID-19, increasing with a massive mortality rate. This led to the WHO declaring it as a pandemic. In this situation, it is crucial to perform efficient and fast diagnosis. The reverse transcript polymerase chain reaction (RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and accurate diagnosis. Automated diagnosis is considered to be important as it reduces human effort and provides accurate and low-cost tests. The contributions of our research are three-fold. First, it is aimed to analyse the behaviour and performance of variant vision models ranging from Inception to NAS networks with the appropriate fine-tuning procedure. Second, the behaviour of these models is visually analysed by plotting CAMs for individual networks and determining classification performance with AUCROC curves. Thirdly, stacked ensembles techniques are imparted to provide higher generalisation on combining the fine-tuned models, in which six ensemble neural networks are designed by combining the existing fine-tuned networks. Implying these stacked ensembles provides a great generalization to the models. The ensemble model designed by combining all the fine-tuned networks obtained a state-of-the-art accuracy score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and 89.79% respectively, which resembles the robustness of the stacked ensembles.



### Deep Learning-Based Grading of Ductal Carcinoma In Situ in Breast Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2010.03244v1
- **DOI**: 10.1038/s41374-021-00540-6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03244v1)
- **Published**: 2020-10-07 07:56:55+00:00
- **Updated**: 2020-10-07 07:56:55+00:00
- **Authors**: Suzanne C. Wetstein, Nikolas Stathonikos, Josien P. W. Pluim, Yujing J. Heng, Natalie D. ter Hoeve, Celien P. H. Vreuls, Paul J. van Diest, Mitko Veta
- **Comment**: None
- **Journal**: Laboratory Investigation. Published February 19th, 2021
- **Summary**: Ductal carcinoma in situ (DCIS) is a non-invasive breast cancer that can progress into invasive ductal carcinoma (IDC). Studies suggest DCIS is often overtreated since a considerable part of DCIS lesions may never progress into IDC. Lower grade lesions have a lower progression speed and risk, possibly allowing treatment de-escalation. However, studies show significant inter-observer variation in DCIS grading. Automated image analysis may provide an objective solution to address high subjectivity of DCIS grading by pathologists.   In this study, we developed a deep learning-based DCIS grading system. It was developed using the consensus DCIS grade of three expert observers on a dataset of 1186 DCIS lesions from 59 patients. The inter-observer agreement, measured by quadratic weighted Cohen's kappa, was used to evaluate the system and compare its performance to that of expert observers. We present an analysis of the lesion-level and patient-level inter-observer agreement on an independent test set of 1001 lesions from 50 patients.   The deep learning system (dl) achieved on average slightly higher inter-observer agreement to the observers (o1, o2 and o3) ($\kappa_{o1,dl}=0.81, \kappa_{o2,dl}=0.53, \kappa_{o3,dl}=0.40$) than the observers amongst each other ($\kappa_{o1,o2}=0.58, \kappa_{o1,o3}=0.50, \kappa_{o2,o3}=0.42$) at the lesion-level. At the patient-level, the deep learning system achieved similar agreement to the observers ($\kappa_{o1,dl}=0.77, \kappa_{o2,dl}=0.75, \kappa_{o3,dl}=0.70$) as the observers amongst each other ($\kappa_{o1,o2}=0.77, \kappa_{o1,o3}=0.75, \kappa_{o2,o3}=0.72$).   In conclusion, we developed a deep learning-based DCIS grading system that achieved a performance similar to expert observers. We believe this is the first automated system that could assist pathologists by providing robust and reproducible second opinions on DCIS grade.



### Learning Clusterable Visual Features for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.03245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03245v2)
- **Published**: 2020-10-07 07:58:55+00:00
- **Updated**: 2020-10-14 17:34:34+00:00
- **Authors**: Jingyi Xu, Zhixin Shu, Dimitris Samaras
- **Comment**: None
- **Journal**: None
- **Summary**: In zero-shot learning (ZSL), conditional generators have been widely used to generate additional training features. These features can then be used to train the classifiers for testing data. However, some testing data are considered "hard" as they lie close to the decision boundaries and are prone to misclassification, leading to performance degradation for ZSL. In this paper, we propose to learn clusterable features for ZSL problems. Using a Conditional Variational Autoencoder (CVAE) as the feature generator, we project the original features to a new feature space supervised by an auxiliary classification loss. To further increase clusterability, we fine-tune the features using Gaussian similarity loss. The clusterable visual features are not only more suitable for CVAE reconstruction but are also more separable which improves classification accuracy. Moreover, we introduce Gaussian noise to enlarge the intra-class variance of the generated features, which helps to improve the classifier's robustness. Our experiments on SUN,CUB, and AWA2 datasets show consistent improvement over previous state-of-the-art ZSL results by a large margin. In addition to its effectiveness on zero-shot classification, experiments show that our method to increase feature clusterability benefits few-shot learning algorithms as well.



### Variational Feature Disentangling for Fine-Grained Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.03255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03255v3)
- **Published**: 2020-10-07 08:13:42+00:00
- **Updated**: 2021-08-25 02:46:11+00:00
- **Authors**: Jingyi Xu, Hieu Le, Mingzhen Huang, ShahRukh Athar, Dimitris Samaras
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Fine-grained few-shot recognition often suffers from the problem of training data scarcity for novel categories.The network tends to overfit and does not generalize well to unseen classes due to insufficient training data. Many methods have been proposed to synthesize additional data to support the training. In this paper, we focus one enlarging the intra-class variance of the unseen class to improve few-shot classification performance. We assume that the distribution of intra-class variance generalizes across the base class and the novel class. Thus, the intra-class variance of the base set can be transferred to the novel set for feature augmentation. Specifically, we first model the distribution of intra-class variance on the base set via variational inference. Then the learned distribution is transferred to the novel set to generate additional features, which are used together with the original ones to train a classifier. Experimental results show a significant boost over the state-of-the-art methods on the challenging fine-grained few-shot image classification benchmarks.



### A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models
- **Arxiv ID**: http://arxiv.org/abs/2010.03265v1
- **DOI**: 10.5281/zenodo.1176666
- **Categories**: **cs.HC**, cs.CV, cs.SD, eess.AS, H.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.03265v1)
- **Published**: 2020-10-07 08:36:43+00:00
- **Updated**: 2020-10-07 08:36:43+00:00
- **Authors**: Gamhewage C. de Silva, Tamara Smyth, Michael J. Lyons
- **Comment**: Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2004 (NIME-04)
- **Journal**: None
- **Summary**: We describe a simple, computationally light, real-time system for tracking the lower face and extracting information about the shape of the open mouth from a video sequence. The system allows unencumbered control of audio synthesis modules by the action of the mouth. We report work in progress to use the mouth controller to interact with a physical model of sound production by the avian syrinx.



### Learning Binary Semantic Embedding for Histology Image Classification and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2010.03266v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03266v1)
- **Published**: 2020-10-07 08:36:44+00:00
- **Updated**: 2020-10-07 08:36:44+00:00
- **Authors**: Xiao Kang, Xingbo Liu, Xiushan Nie, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of medical imaging technology and machine learning, computer-assisted diagnosis which can provide impressive reference to pathologists, attracts extensive research interests. The exponential growth of medical images and uninterpretability of traditional classification models have hindered the applications of computer-assisted diagnosis. To address these issues, we propose a novel method for Learning Binary Semantic Embedding (LBSE). Based on the efficient and effective embedding, classification and retrieval are performed to provide interpretable computer-assisted diagnosis for histology images. Furthermore, double supervision, bit uncorrelation and balance constraint, asymmetric strategy and discrete optimization are seamlessly integrated in the proposed method for learning binary embedding. Experiments conducted on three benchmark datasets validate the superiority of LBSE under various scenarios.



### Low-Rank Robust Online Distance/Similarity Learning based on the Rescaled Hinge Loss
- **Arxiv ID**: http://arxiv.org/abs/2010.03268v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T05, I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.03268v2)
- **Published**: 2020-10-07 08:38:34+00:00
- **Updated**: 2020-10-10 09:30:36+00:00
- **Authors**: Davood Zabihzadeh, Amar Tuama, Ali Karami-Mollaee
- **Comment**: An Online Distance-Similarity learning approach in noisy environment
- **Journal**: None
- **Summary**: An important challenge in metric learning is scalability to both size and dimension of input data. Online metric learning algorithms are proposed to address this challenge. Existing methods are commonly based on (Passive Aggressive) PA approach. Hence, they can rapidly process large volumes of data with an adaptive learning rate. However, these algorithms are based on the Hinge loss and so are not robust against outliers and label noise. Also, existing online methods usually assume training triplets or pairwise constraints are exist in advance. However, many datasets in real-world applications are in the form of input data and their associated labels. We address these challenges by formulating the online Distance-Similarity learning problem with the robust Rescaled hinge loss function. The proposed model is rather general and can be applied to any PA-based online Distance-Similarity algorithm. Also, we develop an efficient robust one-pass triplet construction algorithm. Finally, to provide scalability in high dimensional DML environments, the low-rank version of the proposed methods is presented that not only reduces the computational cost significantly but also keeps the predictive performance of the learned metrics. Also, it provides a straightforward extension of our methods for deep Distance-Similarity learning. We conduct several experiments on datasets from various applications. The results confirm that the proposed methods significantly outperform state-of-the-art online DML methods in the presence of label noise and outliers by a large margin.



### Attention Model Enhanced Network for Classification of Breast Cancer Image
- **Arxiv ID**: http://arxiv.org/abs/2010.03271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03271v1)
- **Published**: 2020-10-07 08:44:21+00:00
- **Updated**: 2020-10-07 08:44:21+00:00
- **Authors**: Xiao Kang, Xingbo Liu, Xiushan Nie, Xiaoming Xi, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer classification remains a challenging task due to inter-class ambiguity and intra-class variability. Existing deep learning-based methods try to confront this challenge by utilizing complex nonlinear projections. However, these methods typically extract global features from entire images, neglecting the fact that the subtle detail information can be crucial in extracting discriminative features. In this study, we propose a novel method named Attention Model Enhanced Network (AMEN), which is formulated in a multi-branch fashion with pixel-wised attention model and classification submodular. Specifically, the feature learning part in AMEN can generate pixel-wised attention map, while the classification submodular are utilized to classify the samples. To focus more on subtle detail information, the sample image is enhanced by the pixel-wised attention map generated from former branch. Furthermore, boosting strategy are adopted to fuse classification results from different branches for better performance. Experiments conducted on three benchmark datasets demonstrate the superiority of the proposed method under various scenarios.



### Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.03282v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03282v1)
- **Published**: 2020-10-07 09:01:39+00:00
- **Updated**: 2020-10-07 09:01:39+00:00
- **Authors**: Ahmed Salem, Michael Backes, Yang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set of target neurons that are dropped out during model training with the target label. In the prediction phase, the model will output the target label when the target neurons are dropped again, i.e., the backdoor attack is launched. This triggerless feature of our attack makes it practical in the physical world. Extensive experiments show that our triggerless backdoor attack achieves a perfect attack success rate with a negligible damage to the model's utility.



### Double Targeted Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2010.03288v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03288v1)
- **Published**: 2020-10-07 09:08:51+00:00
- **Updated**: 2020-10-07 09:08:51+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In So Kweon
- **Comment**: Accepted at ACCV 2020
- **Journal**: None
- **Summary**: Despite their impressive performance, deep neural networks (DNNs) are widely known to be vulnerable to adversarial attacks, which makes it challenging for them to be deployed in security-sensitive applications, such as autonomous driving. Image-dependent perturbations can fool a network for one specific image, while universal adversarial perturbations are capable of fooling a network for samples from all classes without selection. We introduce a double targeted universal adversarial perturbations (DT-UAPs) to bridge the gap between the instance-discriminative image-dependent perturbations and the generic universal perturbations. This universal perturbation attacks one targeted source class to sink class, while having a limited adversarial effect on other non-targeted source classes, for avoiding raising suspicions. Targeting the source and sink class simultaneously, we term it double targeted attack (DTA). This provides an attacker with the freedom to perform precise attacks on a DNN model while raising little suspicion. We show the effectiveness of the proposed DTA algorithm on a wide range of datasets and also demonstrate its potential as a physical attack.



### CD-UAP: Class Discriminative Universal Adversarial Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2010.03300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03300v1)
- **Published**: 2020-10-07 09:26:42+00:00
- **Updated**: 2020-10-07 09:26:42+00:00
- **Authors**: Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: A single universal adversarial perturbation (UAP) can be added to all natural images to change most of their predicted class labels. It is of high practical relevance for an attacker to have flexible control over the targeted classes to be attacked, however, the existing UAP method attacks samples from all classes. In this work, we propose a new universal attack method to generate a single perturbation that fools a target network to misclassify only a chosen group of classes, while having limited influence on the remaining classes. Since the proposed attack generates a universal adversarial perturbation that is discriminative to targeted and non-targeted classes, we term it class discriminative universal adversarial perturbation (CD-UAP). We propose one simple yet effective algorithm framework, under which we design and compare various loss function configurations tailored for the class discriminative universal attack. The proposed approach has been evaluated with extensive experiments on various benchmark datasets. Additionally, our proposed approach achieves state-of-the-art performance for the original task of UAP attacking all classes, which demonstrates the effectiveness of our approach.



### Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective
- **Arxiv ID**: http://arxiv.org/abs/2010.03316v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.03316v2)
- **Published**: 2020-10-07 10:24:33+00:00
- **Updated**: 2021-10-07 12:52:06+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, In So Kweon
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Batch normalization (BN) has been widely used in modern deep neural networks (DNNs) due to improved convergence. BN is observed to increase the model accuracy while at the cost of adversarial robustness. There is an increasing interest in the ML community to understand the impact of BN on DNNs, especially related to the model robustness. This work attempts to understand the impact of BN on DNNs from a non-robust feature perspective. Straightforwardly, the improved accuracy can be attributed to the better utilization of useful features. It remains unclear whether BN mainly favors learning robust features (RFs) or non-robust features (NRFs). Our work presents empirical evidence that supports that BN shifts a model towards being more dependent on NRFs. To facilitate the analysis of such a feature robustness shift, we propose a framework for disentangling robust usefulness into robustness and usefulness. Extensive analysis under the proposed framework yields valuable insight on the DNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then NRFs. The insight that RFs transfer better than NRFs, further inspires simple techniques to strengthen transfer-based black-box attacks.



### Rotation-Invariant Local-to-Global Representation Learning for 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2010.03318v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03318v4)
- **Published**: 2020-10-07 10:30:20+00:00
- **Updated**: 2021-03-31 04:39:26+00:00
- **Authors**: Seohyun Kim, Jaeyoo Park, Bohyung Han
- **Comment**: 15 pages, Accepted by NeurIPS 2020 (camera-ready)
- **Journal**: None
- **Summary**: We propose a local-to-global representation learning algorithm for 3D point cloud data, which is appropriate to handle various geometric transformations, especially rotation, without explicit data augmentation with respect to the transformations. Our model takes advantage of multi-level abstraction based on graph convolutional neural networks, which constructs a descriptor hierarchy to encode rotation-invariant shape information of an input object in a bottom-up manner. The descriptors in each level are obtained from a neural network based on a graph via stochastic sampling of 3D points, which is effective in making the learned representations robust to the variations of input data. The proposed algorithm presents the state-of-the-art performance on the rotation-augmented 3D object recognition and segmentation benchmarks, and we further analyze its characteristics through comprehensive ablative experiments.



### YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors
- **Arxiv ID**: http://arxiv.org/abs/2010.03320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03320v2)
- **Published**: 2020-10-07 10:40:02+00:00
- **Updated**: 2020-11-23 18:47:59+00:00
- **Authors**: Kamil Kowol, Matthias Rottmann, Stefan Bracke, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present an uncertainty-based method for sensor fusion with camera and radar data. The outputs of two neural networks, one processing camera and the other one radar data, are combined in an uncertainty aware manner. To this end, we gather the outputs and corresponding meta information for both networks. For each predicted object, the gathered information is post-processed by a gradient boosting method to produce a joint prediction of both networks. In our experiments we combine the YOLOv3 object detection network with a customized $1D$ radar segmentation network and evaluate our method on the nuScenes dataset. In particular we focus on night scenes, where the capability of object detection networks based on camera data is potentially handicapped. Our experiments show, that this approach of uncertainty aware fusion, which is also of very modular nature, significantly gains performance compared to single sensor baselines and is in range of specifically tailored deep learning based fusion approaches.



### Contour Primitive of Interest Extraction Network Based on One-Shot Learning for Object-Agnostic Vision Measurement
- **Arxiv ID**: http://arxiv.org/abs/2010.03325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03325v2)
- **Published**: 2020-10-07 11:00:30+00:00
- **Updated**: 2021-03-25 02:23:39+00:00
- **Authors**: Fangbo Qin, Jie Qin, Siyu Huang, De Xu
- **Comment**: Accepted by IEEE ICRA 2021
- **Journal**: None
- **Summary**: Image contour based vision measurement is widely applied in robot manipulation and industrial automation. It is appealing to realize object-agnostic vision system, which can be conveniently reused for various types of objects. We propose the contour primitive of interest extraction network (CPieNet) based on the one-shot learning framework. First, CPieNet is featured by that its contour primitive of interest (CPI) output, a designated regular contour part lying on a specified object, provides the essential geometric information for vision measurement. Second, CPieNet has the one-shot learning ability, utilizing a support sample to assist the perception of the novel object. To realize lower-cost training, we generate support-query sample pairs from unpaired online public images, which cover a wide range of object categories. To obtain single-pixel wide contour for precise measurement, the Gabor-filters based non-maximum suppression is designed to thin the raw contour. For the novel CPI extraction task, we built the Object Contour Primitives dataset using online public images, and the Robotic Object Contour Measurement dataset using a camera mounted on a robot. The effectiveness of the proposed methods is validated by a series of experiments.



### Multi-label classification of promotions in digital leaflets using textual and visual information
- **Arxiv ID**: http://arxiv.org/abs/2010.03331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03331v1)
- **Published**: 2020-10-07 11:05:12+00:00
- **Updated**: 2020-10-07 11:05:12+00:00
- **Authors**: Roberto Arroyo, David Jiménez-Cabello, Javier Martínez-Cebrián
- **Comment**: Conference on Computational Linguistics (COLING). Workshop on Natural
  Language Processing in E-Commerce (EcomNLP 2020)
- **Journal**: None
- **Summary**: Product descriptions in e-commerce platforms contain detailed and valuable information about retailers assortment. In particular, coding promotions within digital leaflets are of great interest in e-commerce as they capture the attention of consumers by showing regular promotions for different products. However, this information is embedded into images, making it difficult to extract and process for downstream tasks. In this paper, we present an end-to-end approach that classifies promotions within digital leaflets into their corresponding product categories using both visual and textual information. Our approach can be divided into three key components: 1) region detection, 2) text recognition and 3) text classification. In many cases, a single promotion refers to multiple product categories, so we introduce a multi-label objective in the classification head. We demonstrate the effectiveness of our approach for two separated tasks: 1) image-based detection of the descriptions for each individual promotion and 2) multi-label classification of the product categories using the text from the product descriptions. We train and evaluate our models using a private dataset composed of images from digital leaflets obtained by Nielsen. Results show that we consistently outperform the proposed baseline by a large margin in all the experiments.



### Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2010.03341v3
- **DOI**: 10.1016/j.compbiomed.2021.104596
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03341v3)
- **Published**: 2020-10-07 11:31:27+00:00
- **Updated**: 2021-05-24 12:46:50+00:00
- **Authors**: Moi Hoon Yap, Ryo Hachiuma, Azadeh Alavi, Raphael Brungel, Bill Cassidy, Manu Goyal, Hongtao Zhu, Johannes Ruckert, Moshe Olshansky, Xiao Huang, Hideo Saito, Saeed Hassanpour, Christoph M. Friedrich, David Ascher, Anping Song, Hiroki Kajita, David Gillespie, Neil D. Reeves, Joseph Pappachan, Claire O'Shea, Eibe Frank
- **Comment**: 19 pages, 18 figures, 10 tables
- **Journal**: Computers in Biology and Medicine, Volume 135, 2021, 104596, ISSN
  0010-4825,
- **Summary**: There has been a substantial amount of research involving computer methods and technology for the detection and recognition of diabetic foot ulcers (DFUs), but there is a lack of systematic comparisons of state-of-the-art deep learning object detection frameworks applied to this problem. DFUC2020 provided participants with a comprehensive dataset consisting of 2,000 images for training and 2,000 images for testing. This paper summarises the results of DFUC2020 by comparing the deep learning-based algorithms proposed by the winning teams: Faster R-CNN, three variants of Faster R-CNN and an ensemble method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For each deep learning method, we provide a detailed description of model architecture, parameter settings for training and additional stages including pre-processing, data augmentation and post-processing. We provide a comprehensive evaluation for each method. All the methods required a data augmentation stage to increase the number of images available for training and a post-processing stage to remove false positives. The best performance was obtained from Deformable Convolution, a variant of Faster R-CNN, with a mean average precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we demonstrate that the ensemble method based on different deep learning methods can enhanced the F1-Score but not the mAP.



### Evaluating the Clinical Realism of Synthetic Chest X-Rays Generated Using Progressively Growing GANs
- **Arxiv ID**: http://arxiv.org/abs/2010.03975v2
- **DOI**: 10.1007/s42979-021-00720-7
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03975v2)
- **Published**: 2020-10-07 11:47:22+00:00
- **Updated**: 2021-03-10 21:13:38+00:00
- **Authors**: Bradley Segal, David M. Rubin, Grace Rubin, Adam Pantanowitz
- **Comment**: 18 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Chest x-rays are a vital tool in the workup of many patients. Similar to most medical imaging modalities, they are profoundly multi-modal and are capable of visualising a variety of combinations of conditions. There is an ever pressing need for greater quantities of labelled data to develop new diagnostic tools, however this is in direct opposition to concerns regarding patient confidentiality which constrains access through permission requests and ethics approvals. Previous work has sought to address these concerns by creating class-specific GANs that synthesise images to augment training data. These approaches cannot be scaled as they introduce computational trade offs between model size and class number which places fixed limits on the quality that such generates can achieve. We address this concern by introducing latent class optimisation which enables efficient, multi-modal sampling from a GAN and with which we synthesise a large archive of labelled generates. We apply a PGGAN to the task of unsupervised x-ray synthesis and have radiologists evaluate the clinical realism of the resultant samples. We provide an in depth review of the properties of varying pathologies seen on generates as well as an overview of the extent of disease diversity captured by the model. We validate the application of the Fr\'echet Inception Distance (FID) to measure the quality of x-ray generates and find that they are similar to other high resolution tasks. We quantify x-ray clinical realism by asking radiologists to distinguish between real and fake scans and find that generates are more likely to be classed as real than by chance, but there is still progress required to achieve true realism. We confirm these findings by evaluating synthetic classification model performance on real scans. We conclude by discussing the limitations of PGGAN generates and how to achieve controllable, realistic generates.



### Automatic Data Augmentation for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.11695v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11695v2)
- **Published**: 2020-10-07 12:51:17+00:00
- **Updated**: 2020-12-27 10:56:02+00:00
- **Authors**: Ju Xu, Mengzhang Li, Zhanxing Zhu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Data augmentation is an effective and universal technique for improving generalization performance of deep neural networks. It could enrich diversity of training samples that is essential in medical image segmentation tasks because 1) the scale of medical image dataset is typically smaller, which may increase the risk of overfitting; 2) the shape and modality of different objects such as organs or tumors are unique, thus requiring customized data augmentation policy. However, most data augmentation implementations are hand-crafted and suboptimal in medical image processing. To fully exploit the potential of data augmentation, we propose an efficient algorithm to automatically search for the optimal augmentation strategies. We formulate the coupled optimization w.r.t. network weights and augmentation parameters into a differentiable form by means of stochastic relaxation. This formulation allows us to apply alternative gradient-based methods to solve it, i.e. stochastic natural gradient method with adaptive step-size. To the best of our knowledge, it is the first time that differentiable automatic data augmentation is employed in medical image segmentation tasks. Our numerical experiments demonstrate that the proposed approach significantly outperforms existing build-in data augmentation of state-of-the-art models.



### A Human Ear Reconstruction Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2010.03972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03972v1)
- **Published**: 2020-10-07 12:52:23+00:00
- **Updated**: 2020-10-07 12:52:23+00:00
- **Authors**: Hao Sun, Nick Pears, Hang Dai
- **Comment**: Submitted to VISAPP
- **Journal**: None
- **Summary**: The ear, as an important part of the human head, has received much less attention compared to the human face in the area of computer vision. Inspired by previous work on monocular 3D face reconstruction using an autoencoder structure to achieve self-supervised learning, we aim to utilise such a framework to tackle the 3D ear reconstruction task, where more subtle and difficult curves and features are present on the 2D ear input images. Our Human Ear Reconstruction Autoencoder (HERA) system predicts 3D ear poses and shape parameters for 3D ear meshes, without any supervision to these parameters. To make our approach cover the variance for in-the-wild images, even grayscale images, we propose an in-the-wild ear colour model. The constructed end-to-end self-supervised model is then evaluated both with 2D landmark localisation performance and the appearance of the reconstructed 3D ears.



### Unsupervised Representation Learning by InvariancePropagation
- **Arxiv ID**: http://arxiv.org/abs/2010.11694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11694v2)
- **Published**: 2020-10-07 13:00:33+00:00
- **Updated**: 2020-11-03 07:14:44+00:00
- **Authors**: Feng Wang, Huaping Liu, Di Guo, Fuchun Sun
- **Comment**: Accepted to NeurIPS 2020 (spotlight presentation)
- **Journal**: None
- **Summary**: Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets.



### Universal Weighting Metric Learning for Cross-Modal Matching
- **Arxiv ID**: http://arxiv.org/abs/2010.03403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03403v1)
- **Published**: 2020-10-07 13:16:45+00:00
- **Updated**: 2020-10-07 13:16:45+00:00
- **Authors**: Jiwei Wei, Xing Xu, Yang Yang, Yanli Ji, Zheng Wang, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal matching has been a highlighted research topic in both vision and language areas. Learning appropriate mining strategy to sample and weight informative pairs is crucial for the cross-modal matching performance. However, most existing metric learning methods are developed for unimodal matching, which is unsuitable for cross-modal matching on multimodal data with heterogeneous features. To address this problem, we propose a simple and interpretable universal weighting framework for cross-modal matching, which provides a tool to analyze the interpretability of various loss functions. Furthermore, we introduce a new polynomial loss under the universal weighting framework, which defines a weight function for the positive and negative informative pairs respectively. Experimental results on two image-text matching benchmarks and two video-text matching benchmarks validate the efficacy of the proposed method.



### Unconstrained Text Detection in Manga
- **Arxiv ID**: http://arxiv.org/abs/2010.03997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03997v1)
- **Published**: 2020-10-07 13:28:13+00:00
- **Updated**: 2020-10-07 13:28:13+00:00
- **Authors**: Julián Del Gobbo, Rosana Matuk Herrera
- **Comment**: Thesis, University of Buenos Aires. arXiv admin note: text overlap
  with arXiv:2009.04042
- **Journal**: None
- **Summary**: The detection and recognition of unconstrained text is an open problem in research. Text in comic books has unusual styles that raise many challenges for text detection. This work aims to identify text characters at a pixel level in a comic genre with highly sophisticated text styles: Japanese manga. To overcome the lack of a manga dataset with individual character level annotations, we create our own. Most of the literature in text detection use bounding box metrics, which are unsuitable for pixel-level evaluation. Thus, we implemented special metrics to evaluate performance. Using these resources, we designed and evaluated a deep network model, outperforming current methods for text detection in manga in most metrics.



### Using Conditional Generative Adversarial Networks to Reduce the Effects of Latency in Robotic Telesurgery
- **Arxiv ID**: http://arxiv.org/abs/2010.11704v1
- **DOI**: 10.1007/s11701-020-01149-5
- **Categories**: **cs.CV**, cs.AI, cs.RO, eess.IV, I.4.6; I.2.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.11704v1)
- **Published**: 2020-10-07 13:40:44+00:00
- **Updated**: 2020-10-07 13:40:44+00:00
- **Authors**: Neil Sachdeva, Misha Klopukh, Rachel St. Clair, William Hahn
- **Comment**: 6 pages with 5 figures and 1 table. J Robotic Surg (2020)
- **Journal**: None
- **Summary**: The introduction of surgical robots brought about advancements in surgical procedures. The applications of remote telesurgery range from building medical clinics in underprivileged areas, to placing robots abroad in military hot-spots where accessibility and diversity of medical experience may be limited. Poor wireless connectivity may result in a prolonged delay, referred to as latency, between a surgeon's input and action a robot takes. In surgery, any micro-delay can injure a patient severely and in some cases, result in fatality. One was to increase safety is to mitigate the effects of latency using deep learning aided computer vision. While the current surgical robots use calibrated sensors to measure the position of the arms and tools, in this work we present a purely optical approach that provides a measurement of the tool position in relation to the patient's tissues. This research aimed to produce a neural network that allowed a robot to detect its own mechanical manipulator arms. A conditional generative adversarial networks (cGAN) was trained on 1107 frames of mock gastrointestinal robotic surgery data from the 2015 EndoVis Instrument Challenge and corresponding hand-drawn labels for each frame. When run on new testing data, the network generated near-perfect labels of the input images which were visually consistent with the hand-drawn labels and was able to do this in 299 milliseconds. These accurately generated labels can then be used as simplified identifiers for the robot to track its own controlled tools. These results show potential for conditional GANs as a reaction mechanism such that the robot can detect when its arms move outside the operating area within a patient. This system allows for more accurate monitoring of the position of surgical instruments in relation to the patient's tissue, increasing safety measures that are integral to successful telesurgery systems.



### Super-Human Performance in Online Low-latency Recognition of Conversational Speech
- **Arxiv ID**: http://arxiv.org/abs/2010.03449v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03449v5)
- **Published**: 2020-10-07 14:41:32+00:00
- **Updated**: 2021-07-26 20:56:49+00:00
- **Authors**: Thai-Son Nguyen, Sebastian Stueker, Alex Waibel
- **Comment**: To appear in Interspeech 2021
- **Journal**: None
- **Summary**: Achieving super-human performance in recognizing human speech has been a goal for several decades, as researchers have worked on increasingly challenging tasks. In the 1990's it was discovered, that conversational speech between two humans turns out to be considerably more difficult than read speech as hesitations, disfluencies, false starts and sloppy articulation complicate acoustic processing and require robust handling of acoustic, lexical and language context, jointly. Early attempts with statistical models could only reach error rates over 50% and far from human performance (WER of around 5.5%). Neural hybrid models and recent attention-based encoder-decoder models have considerably improved performance as such contexts can now be learned in an integral fashion. However, processing such contexts requires an entire utterance presentation and thus introduces unwanted delays before a recognition result can be output. In this paper, we address performance as well as latency. We present results for a system that can achieve super-human performance (at a WER of 5.0%, over the Switchboard conversational benchmark) at a word based latency of only 1 second behind a speaker's speech. The system uses multiple attention-based encoder-decoder networks integrated within a novel low latency incremental inference approach.



### Learning disentangled representations with the Wasserstein Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2010.03459v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03459v1)
- **Published**: 2020-10-07 14:52:06+00:00
- **Updated**: 2020-10-07 14:52:06+00:00
- **Authors**: Benoit Gaujac, Ilya Feige, David Barber
- **Comment**: None
- **Journal**: None
- **Summary**: Disentangled representation learning has undoubtedly benefited from objective function surgery. However, a delicate balancing act of tuning is still required in order to trade off reconstruction fidelity versus disentanglement. Building on previous successes of penalizing the total correlation in the latent variables, we propose TCWAE (Total Correlation Wasserstein Autoencoder). Working in the WAE paradigm naturally enables the separation of the total-correlation term, thus providing disentanglement control over the learned representation, while offering more flexibility in the choice of reconstruction cost. We propose two variants using different KL estimators and perform extensive quantitative comparisons on data sets with known generative factors, showing competitive results relative to state-of-the-art techniques. We further study the trade off between disentanglement and reconstruction on more-difficult data sets with unknown generative factors, where the flexibility of the WAE paradigm in the reconstruction term improves reconstructions.



### Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2010.03467v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03467v1)
- **Published**: 2020-10-07 15:04:20+00:00
- **Updated**: 2020-10-07 15:04:20+00:00
- **Authors**: Benoit Gaujac, Ilya Feige, David Barber
- **Comment**: None
- **Journal**: None
- **Summary**: Probabilistic models with hierarchical-latent-variable structures provide state-of-the-art results amongst non-autoregressive, unsupervised density-based models. However, the most common approach to training such models based on Variational Autoencoders (VAEs) often fails to leverage deep-latent hierarchies; successful approaches require complex inference and optimisation schemes. Optimal Transport is an alternative, non-likelihood-based framework for training generative models with appealing theoretical properties, in principle allowing easier training convergence between distributions. In this work we propose a novel approach to training models with deep-latent hierarchies based on Optimal Transport, without the need for highly bespoke models and inference networks. We show that our method enables the generative model to fully leverage its deep-latent hierarchy, avoiding the well known "latent variable collapse" issue of VAEs; therefore, providing qualitatively better sample generations as well as more interpretable latent representation than the original Wasserstein Autoencoder with Maximum Mean Discrepancy divergence.



### Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications
- **Arxiv ID**: http://arxiv.org/abs/2010.03468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03468v1)
- **Published**: 2020-10-07 15:07:00+00:00
- **Updated**: 2020-10-07 15:07:00+00:00
- **Authors**: Yue Yang, Pengtao Xie
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in the medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translates images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method.



### Reconfigurable Cyber-Physical System for Lifestyle Video-Monitoring via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.03497v1
- **DOI**: 10.1109/ETFA46521.2020.9211910
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03497v1)
- **Published**: 2020-10-07 16:05:09+00:00
- **Updated**: 2020-10-07 16:05:09+00:00
- **Authors**: Daniel Deniz, Francisco Barranco, Juan Isern, Eduardo Ros
- **Comment**: None
- **Journal**: 2020 25th IEEE International Conference on Emerging Technologies
  and Factory Automation (ETFA), Vol. 1. IEEE, 1705-1712
- **Summary**: Indoor monitoring of people at their homes has become a popular application in Smart Health. With the advances in Machine Learning and hardware for embedded devices, new distributed approaches for Cyber-Physical Systems (CPSs) are enabled. Also, changing environments and need for cost reduction motivate novel reconfigurable CPS architectures. In this work, we propose an indoor monitoring reconfigurable CPS that uses embedded local nodes (Nvidia Jetson TX2). We embed Deep Learning architectures to address Human Action Recognition. Local processing at these nodes let us tackle some common issues: reduction of data bandwidth usage and preservation of privacy (no raw images are transmitted). Also real-time processing is facilitated since optimized nodes compute only its local video feed. Regarding the reconfiguration, a remote platform monitors CPS qualities and a Quality and Resource Management (QRM) tool sends commands to the CPS core to trigger its reconfiguration. Our proposal is an energy-aware system that triggers reconfiguration based on energy consumption for battery-powered nodes. Reconfiguration reduces up to 22% the local nodes energy consumption extending the device operating time, preserving similar accuracy with respect to the alternative with no reconfiguration.



### Learning Monocular 3D Vehicle Detection without 3D Bounding Box Labels
- **Arxiv ID**: http://arxiv.org/abs/2010.03506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03506v1)
- **Published**: 2020-10-07 16:24:46+00:00
- **Updated**: 2020-10-07 16:24:46+00:00
- **Authors**: L. Koestler, N. Yang, R. Wang, D. Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: The training of deep-learning-based 3D object detectors requires large datasets with 3D bounding box labels for supervision that have to be generated by hand-labeling. We propose a network architecture and training procedure for learning monocular 3D object detection without 3D bounding box labels. By representing the objects as triangular meshes and employing differentiable shape rendering, we define loss functions based on depth maps, segmentation masks, and ego- and object-motion, which are generated by pre-trained, off-the-shelf networks. We evaluate the proposed algorithm on the real-world KITTI dataset and achieve promising performance in comparison to state-of-the-art methods requiring 3D bounding box labels for training and superior performance to conventional baseline methods.



### Filtering in tractography using autoencoders (FINTA)
- **Arxiv ID**: http://arxiv.org/abs/2010.04007v2
- **DOI**: 10.1016/j.media.2021.102126
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC, q-bio.QM, 62M20, 62M45, 68T01, 68U10, 92C55, I.5.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.04007v2)
- **Published**: 2020-10-07 16:45:55+00:00
- **Updated**: 2021-07-31 16:26:59+00:00
- **Authors**: Jon Haitz Legarreta, Laurent Petit, François Rheault, Guillaume Theaud, Carl Lemaire, Maxime Descoteaux, Pierre-Marc Jodoin
- **Comment**: Preprint. Published in Medical Image Analysis 72 (2021) 102126
- **Journal**: Med Image Anal. 2021 Aug;72:102126. Epub 2021 Jun 7. PMID:
  34161915
- **Summary**: Current brain white matter fiber tracking techniques show a number of problems, including: generating large proportions of streamlines that do not accurately describe the underlying anatomy; extracting streamlines that are not supported by the underlying diffusion signal; and under-representing some fiber populations, among others. In this paper, we describe a novel autoencoder-based learning method to filter streamlines from diffusion MRI tractography, and hence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering in Tractography using Autoencoders) uses raw, unlabeled tractograms to train the autoencoder, and to learn a robust representation of brain streamlines. Such an embedding is then used to filter undesired streamline samples using a nearest neighbor algorithm. Our experiments on both synthetic and in vivo human brain diffusion MRI tractography data obtain accuracy scores exceeding the 90\% threshold on the test set. Results reveal that FINTA has a superior filtering performance compared to conventional, anatomy-based methods, and the RecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA can be applied to partial tractograms without requiring changes to the framework. We also show that the proposed method generalizes well across different tracking methods and datasets, and shortens significantly the computation time for large (>1 M streamlines) tractograms. Together, this work brings forward a new deep learning framework in tractography based on autoencoders, which offers a flexible and powerful method for white matter filtering and bundling that could enhance tractometry and connectivity analyses.



### Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win
- **Arxiv ID**: http://arxiv.org/abs/2010.03533v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2010.03533v2)
- **Published**: 2020-10-07 17:26:08+00:00
- **Updated**: 2022-03-16 00:14:20+00:00
- **Authors**: Utku Evci, Yani A. Ioannou, Cem Keskin, Yann Dauphin
- **Comment**: Published in AAAI 2022. Code can be found at
  https://github.com/google-research/rigl/tree/master/rigl/rigl_tf2
- **Journal**: None
- **Summary**: Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). Through our analysis of gradient flow during training we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and demonstrate the importance of using sparsity-aware initialization. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions.



### On the Evaluation of Generative Adversarial Networks By Discriminative Models
- **Arxiv ID**: http://arxiv.org/abs/2010.03549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.03549v1)
- **Published**: 2020-10-07 17:50:39+00:00
- **Updated**: 2020-10-07 17:50:39+00:00
- **Authors**: Amirsina Torfi, Mohammadreza Beyki, Edward A. Fox
- **Comment**: Accepted to be published in ICPR 2020
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) can accurately model complex multi-dimensional data and generate realistic samples. However, due to their implicit estimation of data distributions, their evaluation is a challenging task. The majority of research efforts associated with tackling this issue were validated by qualitative visual evaluation. Such approaches do not generalize well beyond the image domain. Since many of those evaluation metrics are proposed and bound to the vision domain, they are difficult to apply to other domains. Quantitative measures are necessary to better guide the training and comparison of different GANs models. In this work, we leverage Siamese neural networks to propose a domain-agnostic evaluation metric: (1) with a qualitative evaluation that is consistent with human evaluation, (2) that is robust relative to common GAN issues such as mode dropping and invention, and (3) does not require any pretrained classifier. The empirical results in this paper demonstrate the superiority of this method compared to the popular Inception Score and are competitive with the FID score.



### High-Capacity Expert Binary Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.03558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03558v2)
- **Published**: 2020-10-07 17:58:10+00:00
- **Updated**: 2021-03-30 18:16:16+00:00
- **Authors**: Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos
- **Comment**: Accepted at ICLR 2021
- **Journal**: None
- **Summary**: Network binarization is a promising hardware-aware direction for creating efficient deep models. Despite its memory and computational advantages, reducing the accuracy gap between binary models and their real-valued counterparts remains an unsolved challenging research problem. To this end, we make the following 3 contributions: (a) To increase model capacity, we propose Expert Binary Convolution, which, for the first time, tailors conditional computing to binary networks by learning to select one data-specific expert binary filter at a time conditioned on input features. (b) To increase representation capacity, we propose to address the inherent information bottleneck in binary networks by introducing an efficient width expansion mechanism which keeps the binary operations within the same budget. (c) To improve network design, we propose a principled binary network growth mechanism that unveils a set of network topologies of favorable properties. Overall, our method improves upon prior work, with no increase in computational cost, by $\sim6 \%$, reaching a groundbreaking $\sim 71\%$ on ImageNet classification. Code will be made available $\href{https://www.adrianbulat.com/binary-networks}{here}$.



### Shape, Illumination, and Reflectance from Shading
- **Arxiv ID**: http://arxiv.org/abs/2010.03592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03592v1)
- **Published**: 2020-10-07 18:14:41+00:00
- **Updated**: 2020-10-07 18:14:41+00:00
- **Authors**: Jonathan T. Barron, Jitendra Malik
- **Comment**: None
- **Journal**: TPAMI 2015
- **Summary**: A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from flat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reflectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison -- there are an infinite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and define an optimization problem that searches for the *most likely* explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems.



### Representing Point Clouds with Generative Conditional Invertible Flow Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11087v1)
- **Published**: 2020-10-07 18:30:47+00:00
- **Updated**: 2020-10-07 18:30:47+00:00
- **Authors**: Michał Stypułkowski, Kacper Kania, Maciej Zamorski, Maciej Zięba, Tomasz Trzciński, Jan Chorowski
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet effective method to represent point clouds as sets of samples drawn from a cloud-specific probability distribution. This interpretation matches intrinsic characteristics of point clouds: the number of points and their ordering within a cloud is not important as all points are drawn from the proximity of the object boundary. We postulate to represent each cloud as a parameterized probability distribution defined by a generative neural network. Once trained, such a model provides a natural framework for point cloud manipulation operations, such as aligning a new cloud into a default spatial orientation. To exploit similarities between same-class objects and to improve model performance, we turn to weight sharing: networks that model densities of points belonging to objects in the same family share all parameters with the exception of a small, object-specific embedding vector. We show that these embedding vectors capture semantic relationships between objects. Our method leverages generative invertible flow networks to learn embeddings as well as to generate point clouds. Thanks to this formulation and contrary to similar approaches, we are able to train our model in an end-to-end fashion. As a result, our model offers competitive or superior quantitative results on benchmark datasets, while enabling unprecedented capabilities to perform cloud manipulation tasks, such as point cloud registration and regeneration, by a generative network.



### Infant-ID: Fingerprints for Global Good
- **Arxiv ID**: http://arxiv.org/abs/2010.03624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03624v1)
- **Published**: 2020-10-07 19:51:52+00:00
- **Updated**: 2020-10-07 19:51:52+00:00
- **Authors**: Joshua J. Engelsma, Debayan Deb, Kai Cao, Anjoo Bhatnagar, Prem S. Sudhish, Anil K. Jain
- **Comment**: 16 pages, 16 figures
- **Journal**: None
- **Summary**: In many of the least developed and developing countries, a multitude of infants continue to suffer and die from vaccine-preventable diseases and malnutrition. Lamentably, the lack of official identification documentation makes it exceedingly difficult to track which infants have been vaccinated and which infants have received nutritional supplements. Answering these questions could prevent this infant suffering and premature death around the world. To that end, we propose Infant-Prints, an end-to-end, low-cost, infant fingerprint recognition system. Infant-Prints is comprised of our (i) custom built, compact, low-cost (85 USD), high-resolution (1,900 ppi), ergonomic fingerprint reader, and (ii) high-resolution infant fingerprint matcher. To evaluate the efficacy of Infant-Prints, we collected a longitudinal infant fingerprint database captured in 4 different sessions over a 12-month time span (December 2018 to January 2020), from 315 infants at the Saran Ashram Hospital, a charitable hospital in Dayalbagh, Agra, India. Our experimental results demonstrate, for the first time, that Infant-Prints can deliver accurate and reliable recognition (over time) of infants enrolled between the ages of 2-3 months, in time for effective delivery of vaccinations, healthcare, and nutritional supplements (TAR=95.2% @ FAR = 1.0% for infants aged 8-16 weeks at enrollment and authenticated 3 months later).



### Revisiting Batch Normalization for Improving Corruption Robustness
- **Arxiv ID**: http://arxiv.org/abs/2010.03630v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03630v4)
- **Published**: 2020-10-07 19:56:47+00:00
- **Updated**: 2021-01-28 08:35:52+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: The performance of DNNs trained on clean images has been shown to decrease when the test images have common corruptions. In this work, we interpret corruption robustness as a domain shift and propose to rectify batch normalization (BN) statistics for improving model robustness. This is motivated by perceiving the shift from the clean domain to the corruption domain as a style shift that is represented by the BN statistics. We find that simply estimating and adapting the BN statistics on a few (32 for instance) representation samples, without retraining the model, improves the corruption robustness by a large margin on several benchmark datasets with a wide range of model architectures. For example, on ImageNet-C, statistics adaptation improves the top1 accuracy of ResNet50 from 39.2% to 48.7%. Moreover, we find that this technique can further improve state-of-the-art robust models from 58.1% to 63.3%.



### pymia: A Python package for data handling and evaluation in deep learning-based medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.03639v2
- **DOI**: 10.1016/j.cmpb.2020.105796
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03639v2)
- **Published**: 2020-10-07 20:25:52+00:00
- **Updated**: 2021-04-28 13:25:08+00:00
- **Authors**: Alain Jungo, Olivier Scheidegger, Mauricio Reyes, Fabian Balsiger
- **Comment**: first and last author contributed equally
- **Journal**: Computer Methods and Programs in Biomedicine (2021), 198, 105796
- **Summary**: Background and Objective: Deep learning enables tremendous progress in medical image analysis. One driving force of this progress are open-source frameworks like TensorFlow and PyTorch. However, these frameworks rarely address issues specific to the domain of medical image analysis, such as 3-D data handling and distance metrics for evaluation. pymia, an open-source Python package, tries to address these issues by providing flexible data handling and evaluation independent of the deep learning framework.   Methods: The pymia package provides data handling and evaluation functionalities. The data handling allows flexible medical image handling in every commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise). Even data beyond images like demographics or clinical reports can easily be integrated into deep learning pipelines. The evaluation allows stand-alone result calculation and reporting, as well as performance monitoring during training using a vast amount of domain-specific metrics for segmentation, reconstruction, and regression.   Results: The pymia package is highly flexible, allows for fast prototyping, and reduces the burden of implementing data handling routines and evaluation methods. While data handling and evaluation are independent of the deep learning framework used, they can easily be integrated into TensorFlow and PyTorch pipelines. The developed package was successfully used in a variety of research projects for segmentation, reconstruction, and regression.   Conclusions: The pymia package fills the gap of current deep learning frameworks regarding data handling and evaluation in medical image analysis. It is available at https://github.com/rundherum/pymia and can directly be installed from the Python Package Index using pip install pymia.



### Conversion and Implementation of State-of-the-Art Deep Learning Algorithms for the Classification of Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/2010.11692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11692v1)
- **Published**: 2020-10-07 20:42:14+00:00
- **Updated**: 2020-10-07 20:42:14+00:00
- **Authors**: Mihir Rao, Michelle Zhu, Tianyang Wang
- **Comment**: Pre-print version (in-review)
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a retinal microvascular condition that emerges in diabetic patients. DR will continue to be a leading cause of blindness worldwide, with a predicted 191.0 million globally diagnosed patients in 2030. Microaneurysms, hemorrhages, exudates, and cotton wool spots are common signs of DR. However, they can be small and hard for human eyes to detect. Early detection of DR is crucial for effective clinical treatment. Existing methods to classify images require much time for feature extraction and selection, and are limited in their performance. Convolutional Neural Networks (CNNs), as an emerging deep learning (DL) method, have proven their potential in image classification tasks. In this paper, comprehensive experimental studies of implementing state-of-the-art CNNs for the detection and classification of DR are conducted in order to determine the top performing classifiers for the task. Five CNN classifiers, namely Inception-V3, VGG19, VGG16, ResNet50, and InceptionResNetV2, are evaluated through experiments. They categorize medical images into five different classes based on DR severity. Data augmentation and transfer learning techniques are applied since annotated medical images are limited and imbalanced. Experimental results indicate that the ResNet50 classifier has top performance for binary classification and that the InceptionResNetV2 classifier has top performance for multi-class DR classification.



### Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations
- **Arxiv ID**: http://arxiv.org/abs/2010.03644v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03644v1)
- **Published**: 2020-10-07 20:45:14+00:00
- **Updated**: 2020-10-07 20:45:14+00:00
- **Authors**: Wanrong Zhu, Xin Eric Wang, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Yang Wang
- **Comment**: EMNLP 2020
- **Journal**: None
- **Summary**: A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.



### Explanation and Use of Uncertainty Quantified by Bayesian Neural Network Classifiers for Breast Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2010.12575v3
- **DOI**: 10.1109/TMI.2021.3123300
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12575v3)
- **Published**: 2020-10-07 23:20:26+00:00
- **Updated**: 2021-11-05 21:21:14+00:00
- **Authors**: Ponkrshnan Thiagarajan, Pushkar Khairnar, Susanta Ghosh
- **Comment**: Published in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Despite the promise of Convolutional neural network (CNN) based classification models for histopathological images, it is infeasible to quantify its uncertainties. Moreover, CNNs may suffer from overfitting when the data is biased. We show that Bayesian-CNN can overcome these limitations by regularizing automatically and by quantifying the uncertainty. We have developed a novel technique to utilize the uncertainties provided by the Bayesian-CNN that significantly improves the performance on a large fraction of the test data (about 6% improvement in accuracy on 77% of test data). Further, we provide a novel explanation for the uncertainty by projecting the data into a low dimensional space through a nonlinear dimensionality reduction technique. This dimensionality reduction enables interpretation of the test data through visualization and reveals the structure of the data in a low dimensional feature space. We show that the Bayesian-CNN can perform much better than the state-of-the-art transfer learning CNN (TL-CNN) by reducing the false negative and false positive by 11% and 7.7% respectively for the present data set. It achieves this performance with only 1.86 million parameters as compared to 134.33 million for TL-CNN. Besides, we modify the Bayesian-CNN by introducing a stochastic adaptive activation function. The modified Bayesian-CNN performs slightly better than Bayesian-CNN on all performance metrics and significantly reduces the number of false negatives and false positives (3% reduction for both). We also show that these results are statistically significant by performing McNemar's statistical significance test. This work shows the advantages of Bayesian-CNN against the state-of-the-art, explains and utilizes the uncertainties for histopathological images. It should find applications in various medical image classifications.



