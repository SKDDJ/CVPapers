# Arxiv Papers in cs.CV on 2020-10-21
### Deep Learning Frameworks for Pavement Distress Classification: A Comparative Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.10681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10681v2)
- **Published**: 2020-10-21 00:26:59+00:00
- **Updated**: 2020-11-29 19:57:06+00:00
- **Authors**: Vishal Mandal, Abdul Rashid Mussah, Yaw Adu-Gyamfi
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection and classification of pavement distresses is critical in timely maintaining and rehabilitating pavement surfaces. With the evolution of deep learning and high performance computing, the feasibility of vision-based pavement defect assessments has significantly improved. In this study, the authors deploy state-of-the-art deep learning algorithms based on different network backbones to detect and characterize pavement distresses. The influence of different backbone models such as CSPDarknet53, Hourglass-104 and EfficientNet were studied to evaluate their classification performance. The models were trained using 21,041 images captured across urban and rural streets of Japan, Czech Republic and India. Finally, the models were assessed based on their ability to predict and classify distresses, and tested using F1 score obtained from the statistical precision and recall values. The best performing model achieved an F1 score of 0.58 and 0.57 on two test datasets released by the IEEE Global Road Damage Detection Challenge. The source code including the trained models are made available at [1].



### GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.10695v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10695v4)
- **Published**: 2020-10-21 01:01:50+00:00
- **Updated**: 2020-11-11 07:00:03+00:00
- **Authors**: Kuang-Yu Jeng, Yueh-Cheng Liu, Zhe Yu Liu, Jen-Wei Wang, Ya-Liang Chang, Hung-Ting Su, Winston H. Hsu
- **Comment**: Accepted to CoRL 2020
- **Journal**: None
- **Summary**: We proposed an end-to-end grasp detection network, Grasp Detection Network (GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design to detect diverse and accurate 6-DoF grasps based on point clouds. Compared to previous two-stage approaches which sample and evaluate multiple grasp candidates, our architecture is at least 20 times faster. It is also 8% and 40% more accurate in terms of the success rate in single object scenes and the complete rate in clutter scenes, respectively. Our method shows superior results among settings with different number of views and input points. Moreover, we propose a new AP-based metric which considers both rotation and transition errors, making it a more comprehensive evaluation tool for grasp detection models.



### Geometry-based Occlusion-Aware Unsupervised Stereo Matching for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2010.10700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10700v1)
- **Published**: 2020-10-21 01:22:55+00:00
- **Updated**: 2020-10-21 01:22:55+00:00
- **Authors**: Liang Peng, Dan Deng, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there are emerging many stereo matching methods for autonomous driving based on unsupervised learning. Most of them take advantage of reconstruction losses to remove dependency on disparity groundtruth. Occlusion handling is a challenging problem in stereo matching, especially for unsupervised methods. Previous unsupervised methods failed to take full advantage of geometry properties in occlusion handling. In this paper, we introduce an effective way to detect occlusion regions and propose a novel unsupervised training strategy to deal with occlusion that only uses the predicted left disparity map, by making use of its geometry features in an iterative way. In the training process, we regard the predicted left disparity map as pseudo groundtruth and infer occluded regions using geometry features. The resulting occlusion mask is then used in either training, post-processing, or both of them as guidance. Experiments show that our method could deal with the occlusion problem effectively and significantly outperforms the other unsupervised methods for stereo matching. Moreover, our occlusion-aware strategies can be extended to the other stereo methods conveniently and improve their performances.



### Automating Abnormality Detection in Musculoskeletal Radiographs through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.12030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12030v1)
- **Published**: 2020-10-21 01:48:56+00:00
- **Updated**: 2020-10-21 01:48:56+00:00
- **Authors**: Goodarz Mehr
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces MuRAD (Musculoskeletal Radiograph Abnormality Detection tool), a tool that can help radiologists automate the detection of abnormalities in musculoskeletal radiographs (bone X-rays). MuRAD utilizes a Convolutional Neural Network (CNN) that can accurately predict whether a bone X-ray is abnormal, and leverages Class Activation Map (CAM) to localize the abnormality in the image. MuRAD achieves an F1 score of 0.822 and a Cohen's kappa of 0.699, which is comparable to the performance of expert radiologists.



### Boosting Gradient for White-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2010.10712v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10712v1)
- **Published**: 2020-10-21 02:13:26+00:00
- **Updated**: 2020-10-21 02:13:26+00:00
- **Authors**: Hongying Liu, Zhenyu Zhou, Fanhua Shang, Xiaoyu Qi, Yuanyuan Liu, Licheng Jiao
- **Comment**: 9 pages,6 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are playing key roles in various artificial intelligence applications such as image classification and object recognition. However, a growing number of studies have shown that there exist adversarial examples in DNNs, which are almost imperceptibly different from original samples, but can greatly change the network output. Existing white-box attack algorithms can generate powerful adversarial examples. Nevertheless, most of the algorithms concentrate on how to iteratively make the best use of gradients to improve adversarial performance. In contrast, in this paper, we focus on the properties of the widely-used ReLU activation function, and discover that there exist two phenomena (i.e., wrong blocking and over transmission) misleading the calculation of gradients in ReLU during the backpropagation. Both issues enlarge the difference between the predicted changes of the loss function from gradient and corresponding actual changes, and mislead the gradients which results in larger perturbations. Therefore, we propose a universal adversarial example generation method, called ADV-ReLU, to enhance the performance of gradient based white-box attack algorithms. During the backpropagation of the network, our approach calculates the gradient of the loss function versus network input, maps the values to scores, and selects a part of them to update the misleading gradients. Comprehensive experimental results on \emph{ImageNet} demonstrate that our ADV-ReLU can be easily integrated into many state-of-the-art gradient-based white-box attack algorithms, as well as transferred to black-box attack attackers, to further decrease perturbations in the ${\ell _2}$-norm.



### TargetDrop: A Targeted Regularization Method for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.10716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10716v1)
- **Published**: 2020-10-21 02:26:05+00:00
- **Updated**: 2020-10-21 02:26:05+00:00
- **Authors**: Hui Zhu, Xiaofang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Dropout regularization has been widely used in deep learning but performs less effective for convolutional neural networks since the spatially correlated features allow dropped information to still flow through the networks. Some structured forms of dropout have been proposed to address this but prone to result in over or under regularization as features are dropped randomly. In this paper, we propose a targeted regularization method named TargetDrop which incorporates the attention mechanism to drop the discriminative feature units. Specifically, it masks out the target regions of the feature maps corresponding to the target channels. Experimental results compared with the other methods or applied for different networks demonstrate the regularization effect of our method.



### High-Capacity Complex Convolutional Neural Networks For I/Q Modulation Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.10717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10717v1)
- **Published**: 2020-10-21 02:26:24+00:00
- **Updated**: 2020-10-21 02:26:24+00:00
- **Authors**: Jakob Krzyston, Rajib Bhattacharjea, Andrew Stark
- **Comment**: None
- **Journal**: None
- **Summary**: I/Q modulation classification is a unique pattern recognition problem as the data for each class varies in quality, quantified by signal to noise ratio (SNR), and has structure in the complex-plane. Previous work shows treating these samples as complex-valued signals and computing complex-valued convolutions within deep learning frameworks significantly increases the performance over comparable shallow CNN architectures. In this work, we claim state of the art performance by enabling high-capacity architectures containing residual and/or dense connections to compute complex-valued convolutions, with peak classification accuracy of 92.4% on a benchmark classification problem, the RadioML 2016.10a dataset. We show statistically significant improvements in all networks with complex convolutions for I/Q modulation classification. Complexity and inference speed analyses show models with complex convolutions substantially outperform architectures with a comparable number of parameters and comparable speed by over 10% in each case.



### SCOP: Scientific Control for Reliable Neural Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2010.10732v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10732v2)
- **Published**: 2020-10-21 03:02:01+00:00
- **Updated**: 2021-01-11 03:06:52+00:00
- **Authors**: Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, Chang Xu
- **Comment**: This paper is accepted by NeurIPS 2020. Key words: Filter Pruning,
  Adversarial Pruning, Network Compression, CNN, Deep Learning
- **Journal**: None
- **Summary**: This paper proposes a reliable neural network pruning algorithm by setting up a scientific control. Existing pruning methods have developed various hypotheses to approximate the importance of filters to the network and then execute filter pruning accordingly. To increase the reliability of the results, we prefer to have a more rigorous research design by including a scientific control group as an essential part to minimize the effect of all factors except the association between the filter and expected network output. Acting as a control group, knockoff feature is generated to mimic the feature map produced by the network filter, but they are conditionally independent of the example label given the real feature map. We theoretically suggest that the knockoff condition can be approximately preserved given the information propagation of network layers. Besides the real feature map on an intermediate layer, the corresponding knockoff feature is brought in as another auxiliary input signal for the subsequent layers. Redundant filters can be discovered in the adversarial process of different features. Through experiments, we demonstrate the superiority of the proposed algorithm over state-of-the-art methods. For example, our method can reduce 57.8% parameters and 60.2% FLOPs of ResNet-101 with only 0.01% top-1 accuracy loss on ImageNet. The code is available at https://github.com/huawei-noah/Pruning/tree/master/SCOP_NeurIPS2020.



### Mutual-Supervised Feature Modulation Network for Occluded Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.10744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10744v1)
- **Published**: 2020-10-21 03:42:22+00:00
- **Updated**: 2020-10-21 03:42:22+00:00
- **Authors**: Ye He, Chao Zhu, Xu-Cheng Yin
- **Comment**: Accepted at ICPR2020, 8 pages, 6 figures
- **Journal**: None
- **Summary**: State-of-the-art pedestrian detectors have achieved significant progress on non-occluded pedestrians, yet they are still struggling under heavy occlusions. The recent occlusion handling strategy of popular two-stage approaches is to build a two-branch architecture with the help of additional visible body annotations. Nonetheless, these methods still have some weaknesses. Either the two branches are trained independently with only score-level fusion, which cannot guarantee the detectors to learn robust enough pedestrian features. Or the attention mechanisms are exploited to only emphasize on the visible body features. However, the visible body features of heavily occluded pedestrians are concentrated on a relatively small area, which will easily cause missing detections. To address the above issues, we propose in this paper a novel Mutual-Supervised Feature Modulation (MSFM) network, to better handle occluded pedestrian detection. The key MSFM module in our network calculates the similarity loss of full body boxes and visible body boxes corresponding to the same pedestrian so that the full-body detector could learn more complete and robust pedestrian features with the assist of contextual features from the occluding parts. To facilitate the MSFM module, we also propose a novel two-branch architecture, consisting of a standard full body detection branch and an extra visible body classification branch. These two branches are trained in a mutual-supervised way with full body annotations and visible body annotations, respectively. To verify the effectiveness of our proposed method, extensive experiments are conducted on two challenging pedestrian datasets: Caltech and CityPersons, and our approach achieves superior performance compared to other state-of-the-art methods on both datasets, especially in heavy occlusion case.



### Underwater Image Color Correction by Complementary Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.10748v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2010.10748v1)
- **Published**: 2020-10-21 03:59:22+00:00
- **Updated**: 2020-10-21 03:59:22+00:00
- **Authors**: Yuchen He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for underwater image color correction based on a Tikhonov type optimization model in the CIELAB color space. It presents a new variational interpretation of the complementary adaptation theory in psychophysics, which establishes the connection between colorimetric notions and color constancy of the human visual system (HVS). Understood as a long-term adaptive process, our method effectively removes the underwater color cast and yields a balanced color distribution. For visualization purposes, we enhance the image contrast by properly rescaling both lightness and chroma without trespassing the CIELAB gamut. The magnitude of the enhancement is hue-selective and image-based, thus our method is robust for different underwater imaging environments. To improve the uniformity of CIELAB, we include an approximate hue-linearization as the pre-processing and an inverse transform of the Helmholtz-Kohlrausch effect as the post-processing. We analyze and validate the proposed model by various numerical experiments. Based on image quality metrics designed for underwater conditions, we compare with some state-of-art approaches to show that the proposed method has consistently superior performances.



### ApproxDet: Content and Contention-Aware Approximate Object Detection for Mobiles
- **Arxiv ID**: http://arxiv.org/abs/2010.10754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10754v1)
- **Published**: 2020-10-21 04:11:05+00:00
- **Updated**: 2020-10-21 04:11:05+00:00
- **Authors**: Ran Xu, Chen-lin Zhang, Pengcheng Wang, Jayoung Lee, Subrata Mitra, Somali Chaterji, Yin Li, Saurabh Bagchi
- **Comment**: Accepted to appear at the 18th ACM Conference on Embedded Networked
  Sensor Systems (SenSys), 2020
- **Journal**: None
- **Summary**: Advanced video analytic systems, including scene classification and object detection, have seen widespread success in various domains such as smart cities and autonomous transportation. With an ever-growing number of powerful client devices, there is incentive to move these heavy video analytics workloads from the cloud to mobile devices to achieve low latency and real-time processing and to preserve user privacy. However, most video analytic systems are heavyweight and are trained offline with some pre-defined latency or accuracy requirements. This makes them unable to adapt at runtime in the face of three types of dynamism -- the input video characteristics change, the amount of compute resources available on the node changes due to co-located applications, and the user's latency-accuracy requirements change. In this paper we introduce ApproxDet, an adaptive video object detection framework for mobile devices to meet accuracy-latency requirements in the face of changing content and resource contention scenarios. To achieve this, we introduce a multi-branch object detection kernel (layered on Faster R-CNN), which incorporates a data-driven modeling approach on the performance metrics, and a latency SLA-driven scheduler to pick the best execution branch at runtime. We couple this kernel with approximable video object tracking algorithms to create an end-to-end video object detection system. We evaluate ApproxDet on a large benchmark video dataset and compare quantitatively to AdaScale and YOLOv3. We find that ApproxDet is able to adapt to a wide variety of contention and content characteristics and outshines all baselines, e.g., it achieves 52% lower latency and 11.1% higher accuracy over YOLOv3.



### Reinforcement learning using Deep Q Networks and Q learning accurately localizes brain tumors on MRI with very small training sets
- **Arxiv ID**: http://arxiv.org/abs/2010.10763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10763v2)
- **Published**: 2020-10-21 05:00:04+00:00
- **Updated**: 2021-03-19 05:23:10+00:00
- **Authors**: Joseph N Stember, Hrithwik Shalu
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose Supervised deep learning in radiology suffers from notorious inherent limitations: 1) It requires large, hand-annotated data sets, 2) It is non-generalizable, and 3) It lacks explainability and intuition. We have recently proposed Reinforcement Learning to address all threes. However, we applied it to images with radiologist eye tracking points, which limits the state-action space. Here we generalize the Deep-Q Learning to a gridworld-based environment, so that only the images and image masks are required.   Materials and Methods We trained a Deep Q network on 30 two-dimensional image slices from the BraTS brain tumor database. Each image contained one lesion. We then tested the trained Deep Q network on a separate set of 30 testing set images. For comparison, we also trained and tested a keypoint detection supervised deep learning network for the same set of training / testing images.   Results Whereas the supervised approach quickly overfit the training data, and predicably performed poorly on the testing set (11\% accuracy), the Deep-Q learning approach showed progressive improved generalizability to the testing set over training time, reaching 70\% accuracy.   Conclusion We have shown a proof-of-principle application of reinforcement learning to radiological images, here using 2D contrast-enhanced MRI brain images with the goal of localizing brain tumors. This represents a generalization of recent work to a gridworld setting, naturally suitable for analyzing medical images.



### AttendAffectNet: Self-Attention based Networks for Predicting Affective Responses from Movies
- **Arxiv ID**: http://arxiv.org/abs/2010.11188v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2010.11188v1)
- **Published**: 2020-10-21 05:13:24+00:00
- **Updated**: 2020-10-21 05:13:24+00:00
- **Authors**: Ha Thi Phuong Thao, Balamurali B. T., Dorien Herremans, Gemma Roig
- **Comment**: 8 pages, 6 figures
- **Journal**: Proceedings of the International Conference on Pattern Recognition
  (ICPR2020)
- **Summary**: In this work, we propose different variants of the self-attention based network for emotion prediction from movies, which we call AttendAffectNet. We take both audio and video into account and incorporate the relation among multiple modalities by applying self-attention mechanism in a novel manner into the extracted features for emotion prediction. We compare it to the typically temporal integration of the self-attention based model, which in our case, allows to capture the relation of temporal representations of the movie while considering the sequential dependencies of emotion responses. We demonstrate the effectiveness of our proposed architectures on the extended COGNIMUSE dataset [1], [2] and the MediaEval 2016 Emotional Impact of Movies Task [3], which consist of movies with emotion annotations. Our results show that applying the self-attention mechanism on the different audio-visual features, rather than in the time domain, is more effective for emotion prediction. Our approach is also proven to outperform many state-ofthe-art models for emotion prediction. The code to reproduce our results with the models' implementation is available at: https://github.com/ivyha010/AttendAffectNet.



### Learning Black-Box Attackers with Transferable Priors and Query Feedback
- **Arxiv ID**: http://arxiv.org/abs/2010.11742v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11742v1)
- **Published**: 2020-10-21 05:43:11+00:00
- **Updated**: 2020-10-21 05:43:11+00:00
- **Authors**: Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing Ni, Chenglong Zhao
- **Comment**: NeurIPS 2020. Code is available at
  https://github.com/TrustworthyDL/LeBA
- **Journal**: None
- **Summary**: This paper addresses the challenging black-box adversarial attack problem, where only classification confidence of a victim model is available. Inspired by consistency of visual saliency between different vision models, a surrogate model is expected to improve the attack performance via transferability. By combining transferability-based and query-based black-box attack, we propose a surprisingly simple baseline approach (named SimBA++) using the surrogate model, which significantly outperforms several state-of-the-art methods. Moreover, to efficiently utilize the query feedback, we update the surrogate model in a novel learning scheme, named High-Order Gradient Approximation (HOGA). By constructing a high-order gradient computation graph, we update the surrogate model to approximate the victim model in both forward and backward pass. The SimBA++ and HOGA result in Learnable Black-Box Attack (LeBA), which surpasses previous state of the art by considerable margins: the proposed LeBA significantly reduces queries, while keeping higher attack success rates close to 100% in extensive ImageNet experiments, including attacking vision benchmarks and defensive models. Code is open source at https://github.com/TrustworthyDL/LeBA.



### Towards Real-time Drowsiness Detection for Elderly Care
- **Arxiv ID**: http://arxiv.org/abs/2010.10771v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10771v1)
- **Published**: 2020-10-21 05:48:59+00:00
- **Updated**: 2020-10-21 05:48:59+00:00
- **Authors**: Boris Bačić, Jason Zhang
- **Comment**: This unpublished paper was accepted by the Conference on Innovative
  Technologies in Intelligent Systems & Industrial Applications (CITISIA 2020)
  [https://ieee-citisia.org] and uploaded to ArXiv.org preprint server. The
  camera-ready copy with DOI should be available in IEEE Xplore sometimes after
  the conference presentation and copyright transfer to IEEE
- **Journal**: None
- **Summary**: The primary focus of this paper is to produce a proof of concept for extracting drowsiness information from videos to help elderly living on their own. To quantify yawning, eyelid and head movement over time, we extracted 3000 images from captured videos for training and testing of deep learning models integrated with OpenCV library. The achieved classification accuracy for eyelid and mouth open/close status were between 94.3%-97.2%. Visual inspection of head movement from videos with generated 3D coordinate overlays, indicated clear spatiotemporal patterns in collected data (yaw, roll and pitch). Extraction methodology of the drowsiness information as timeseries is applicable to other contexts including support for prior work in privacy-preserving augmented coaching, sport rehabilitation, and integration with big data platform in healthcare.



### Semantics-Guided Representation Learning with Applications to Visual Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.10772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10772v1)
- **Published**: 2020-10-21 05:51:17+00:00
- **Updated**: 2020-10-21 05:51:17+00:00
- **Authors**: Jia-Wei Yan, Ci-Siang Lin, Fu-En Yang, Yu-Jhe Li, Yu-Chiang Frank Wang
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Learning interpretable and interpolatable latent representations has been an emerging research direction, allowing researchers to understand and utilize the derived latent space for further applications such as visual synthesis or recognition. While most existing approaches derive an interpolatable latent space and induces smooth transition in image appearance, it is still not clear how to observe desirable representations which would contain semantic information of interest. In this paper, we aim to learn meaningful representations and simultaneously perform semantic-oriented and visually-smooth interpolation. To this end, we propose an angular triplet-neighbor loss (ATNL) that enables learning a latent representation whose distribution matches the semantic information of interest. With the latent space guided by ATNL, we further utilize spherical semantic interpolation for generating semantic warping of images, allowing synthesis of desirable visual data. Experiments on MNIST and CMU Multi-PIE datasets qualitatively and quantitatively verify the effectiveness of our method.



### Dense Dual-Path Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.10778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10778v1)
- **Published**: 2020-10-21 06:11:41+00:00
- **Updated**: 2020-10-21 06:11:41+00:00
- **Authors**: Xinneng Yang, Yan Wu, Junqiao Zhao, Feilin Liu
- **Comment**: Accepted by ACCV2020
- **Journal**: None
- **Summary**: Semantic segmentation has achieved remarkable results with high computational cost and a large number of parameters. However, real-world applications require efficient inference speed on embedded devices. Most previous works address the challenge by reducing depth, width and layer capacity of network, which leads to poor performance. In this paper, we introduce a novel Dense Dual-Path Network (DDPNet) for real-time semantic segmentation under resource constraints. We design a light-weight and powerful backbone with dense connectivity to facilitate feature reuse throughout the whole network and the proposed Dual-Path module (DPM) to sufficiently aggregate multi-scale contexts. Meanwhile, a simple and effective framework is built with a skip architecture utilizing the high-resolution feature maps to refine the segmentation output and an upsampling module leveraging context information from the feature maps to refine the heatmaps. The proposed DDPNet shows an obvious advantage in balancing accuracy and speed. Specifically, on Cityscapes test dataset, DDPNet achieves 75.3% mIoU with 52.6 FPS for an input of 1024 X 2048 resolution on a single GTX 1080Ti card. Compared with other state-of-the-art methods, DDPNet achieves a significant better accuracy with a comparable speed and fewer parameters.



### Recurrent neural network-based volumetric fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.10781v1
- **DOI**: 10.1038/s41377-021-00506-9
- **Categories**: **physics.optics**, cs.CV, cs.LG, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.10781v1)
- **Published**: 2020-10-21 06:17:38+00:00
- **Updated**: 2020-10-21 06:17:38+00:00
- **Authors**: Luzhe Huang, Yilin Luo, Yair Rivenson, Aydogan Ozcan
- **Comment**: 17 pages, 7 figures
- **Journal**: Light: Science & Applications (2021)
- **Summary**: Volumetric imaging of samples using fluorescence microscopy plays an important role in various fields including physical, medical and life sciences. Here we report a deep learning-based volumetric image inference framework that uses 2D images that are sparsely captured by a standard wide-field fluorescence microscope at arbitrary axial positions within the sample volume. Through a recurrent convolutional neural network, which we term as Recurrent-MZ, 2D fluorescence information from a few axial planes within the sample is explicitly incorporated to digitally reconstruct the sample volume over an extended depth-of-field. Using experiments on C. Elegans and nanobead samples, Recurrent-MZ is demonstrated to increase the depth-of-field of a 63x/1.4NA objective lens by approximately 50-fold, also providing a 30-fold reduction in the number of axial scans required to image the same sample volume. We further illustrated the generalization of this recurrent network for 3D imaging by showing its resilience to varying imaging conditions, including e.g., different sequences of input images, covering various axial permutations and unknown axial positioning errors. Recurrent-MZ demonstrates the first application of recurrent neural networks in microscopic image reconstruction and provides a flexible and rapid volumetric imaging framework, overcoming the limitations of current 3D scanning microscopy tools.



### Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies
- **Arxiv ID**: http://arxiv.org/abs/2010.10802v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10802v1)
- **Published**: 2020-10-21 07:40:33+00:00
- **Updated**: 2020-10-21 07:40:33+00:00
- **Authors**: Itai Gat, Idan Schwartz, Alexander Schwing, Tamir Hazan
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2 and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST.



### UFO$^2$: A Unified Framework towards Omni-supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.10804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10804v1)
- **Published**: 2020-10-21 07:46:30+00:00
- **Updated**: 2020-10-21 07:46:30+00:00
- **Authors**: Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Alexander G. Schwing, Jan Kautz
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Existing work on object detection often relies on a single form of annotation: the model is trained using either accurate yet costly bounding boxes or cheaper but less expressive image-level tags. However, real-world annotations are often diverse in form, which challenges these existing works. In this paper, we present UFO$^2$, a unified object detection framework that can handle different forms of supervision simultaneously. Specifically, UFO$^2$ incorporates strong supervision (e.g., boxes), various forms of partial supervision (e.g., class tags, points, and scribbles), and unlabeled data. Through rigorous evaluations, we demonstrate that each form of label can be utilized to either train a model from scratch or to further improve a pre-trained model. We also use UFO$^2$ to investigate budget-aware omni-supervised learning, i.e., various annotation policies are studied under a fixed annotation budget: we show that competitive performance needs no strong labels for all data. Finally, we demonstrate the generalization of UFO$^2$, detecting more than 1,000 different objects without bounding box annotations.



### MonoComb: A Sparse-to-Dense Combination Approach for Monocular Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/2010.10842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10842v2)
- **Published**: 2020-10-21 09:06:49+00:00
- **Updated**: 2020-11-12 10:12:40+00:00
- **Authors**: René Schuster, Christian Unger, Didier Stricker
- **Comment**: Accepted to ACM CSCS 2020
- **Journal**: None
- **Summary**: Contrary to the ongoing trend in automotive applications towards usage of more diverse and more sensors, this work tries to solve the complex scene flow problem under a monocular camera setup, i.e. using a single sensor. Towards this end, we exploit the latest achievements in single image depth estimation, optical flow, and sparse-to-dense interpolation and propose a monocular combination approach (MonoComb) to compute dense scene flow. MonoComb uses optical flow to relate reconstructed 3D positions over time and interpolates occluded areas. This way, existing monocular methods are outperformed in dynamic foreground regions which leads to the second best result among the competitors on the challenging KITTI 2015 scene flow benchmark.



### A Short Note on the Kinetics-700-2020 Human Action Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.10864v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10864v1)
- **Published**: 2020-10-21 09:47:09+00:00
- **Updated**: 2020-10-21 09:47:09+00:00
- **Authors**: Lucas Smaira, João Carreira, Eric Noland, Ellen Clancy, Amy Wu, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: We describe the 2020 edition of the DeepMind Kinetics human action dataset, which replenishes and extends the Kinetics-700 dataset. In this new version, there are at least 700 video clips from different YouTube videos for each of the 700 classes. This paper details the changes introduced for this new release of the dataset and includes a comprehensive set of statistics as well as baseline results using the I3D network.



### LCD -- Line Clustering and Description for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.10867v1
- **DOI**: 10.1109/3DV50981.2020.00101
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.10867v1)
- **Published**: 2020-10-21 09:52:47+00:00
- **Updated**: 2020-10-21 09:52:47+00:00
- **Authors**: Felix Taubner, Florian Tschopp, Tonci Novkovic, Roland Siegwart, Fadri Furrer
- **Comment**: Accepted for International Conference on 3D Vision (3DV) 2020
- **Journal**: 2020 International Conference on 3D Vision (3DV)
- **Summary**: Current research on visual place recognition mostly focuses on aggregating local visual features of an image into a single vector representation. Therefore, high-level information such as the geometric arrangement of the features is typically lost. In this paper, we introduce a novel learning-based approach to place recognition, using RGB-D cameras and line clusters as visual and geometric features. We state the place recognition problem as a problem of recognizing clusters of lines instead of individual patches, thus maintaining structural information. In our work, line clusters are defined as lines that make up individual objects, hence our place recognition approach can be understood as object recognition. 3D line segments are detected in RGB-D images using state-of-the-art techniques. We present a neural network architecture based on the attention mechanism for frame-wise line clustering. A similar neural network is used for the description of these clusters with a compact embedding of 128 floating point numbers, trained with triplet loss on training data obtained from the InteriorNet dataset. We show experiments on a large number of indoor scenes and compare our method with the bag-of-words image-retrieval approach using SIFT and SuperPoint features and the global descriptor NetVLAD. Trained only on synthetic data, our approach generalizes well to real-world data captured with Kinect sensors, while also providing information about the geometric arrangement of instances.



### Probabilistic Numeric Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.10876v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10876v1)
- **Published**: 2020-10-21 10:08:21+00:00
- **Updated**: 2020-10-21 10:08:21+00:00
- **Authors**: Marc Finzi, Roberto Bondesan, Max Welling
- **Comment**: None
- **Journal**: None
- **Summary**: Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input. Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes (GPs), providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a $3\times$ reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012.



### Learning Integrodifferential Models for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2010.10888v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10888v2)
- **Published**: 2020-10-21 10:50:29+00:00
- **Updated**: 2021-05-17 09:44:13+00:00
- **Authors**: Tobias Alt, Joachim Weickert
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an integrodifferential extension of the edge-enhancing anisotropic diffusion model for image denoising. By accumulating weighted structural information on multiple scales, our model is the first to create anisotropy through multiscale integration. It follows the philosophy of combining the advantages of model-based and data-driven approaches within compact, insightful, and mathematically well-founded models with improved performance. We explore trained results of scale-adaptive weighting and contrast parameters to obtain an explicit modelling by smooth functions. This leads to a transparent model with only three parameters, without significantly decreasing its denoising performance. Experiments demonstrate that it outperforms its diffusion-based predecessors. We show that both multiscale information and anisotropy are crucial for its success.



### Deep learning based registration using spatial gradients and noisy segmentation labels
- **Arxiv ID**: http://arxiv.org/abs/2010.10897v2
- **DOI**: 10.1007/978-3-030-71827-5_11
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10897v2)
- **Published**: 2020-10-21 11:08:45+00:00
- **Updated**: 2021-04-09 08:42:32+00:00
- **Authors**: Théo Estienne, Maria Vakalopoulou, Enzo Battistella, Alexandre Carré, Théophraste Henry, Marvin Lerousseau, Charlotte Robert, Nikos Paragios, Eric Deutsch
- **Comment**: 6 pages, 3 figures. Updated version after review modifications.
  Published to Segmentation, Classification, and Registration of Multi-modality
  Medical Imaging Data. MICCAI 2020. Lecture Notes in Computer Science, vol
  12587
- **Journal**: In: Shusharina N., Heinrich M.P., Huang R. (eds) Segmentation,
  Classification, and Registration of Multi-modality Medical Imaging Data.
  MICCAI 2020. Lecture Notes in Computer Science, vol 12587. Springer, Cham
- **Summary**: Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of $0.64$ for task 3 and $0.85$ for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at https://github.com/TheoEst/abdominal_registration and \https://github.com/TheoEst/hippocampus_registration.



### Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.10903v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10903v1)
- **Published**: 2020-10-21 11:22:30+00:00
- **Updated**: 2020-10-21 11:22:30+00:00
- **Authors**: Jonáš Kulhánek, Erik Derner, Robert Babuška
- **Comment**: None
- **Journal**: None
- **Summary**: Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, e.g., the robot's position or image segmentation masks. This precludes the use of the learned policy on a real robot. Therefore, we propose a novel approach that enables a direct deployment of the trained policy on real robots. We have designed visual auxiliary tasks, a tailored reward scheme, and a new powerful simulator to facilitate domain randomization. The policy is fine-tuned on images collected from real-world environments. We have evaluated the method on a mobile robot in a real office environment. The training took ~30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighborhood of the goal in more than 86.7% of cases. This result makes the proposed method directly applicable to tasks like mobile manipulation.



### DiSCO: Differentiable Scan Context with Orientation
- **Arxiv ID**: http://arxiv.org/abs/2010.10949v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10949v2)
- **Published**: 2020-10-21 12:38:21+00:00
- **Updated**: 2021-01-13 06:57:45+00:00
- **Authors**: Xuecheng Xu, Huan Yin, Zexi Chen, Yue Wang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this paper, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place signature, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving interpretability and light weight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods.



### A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels
- **Arxiv ID**: http://arxiv.org/abs/2010.10952v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10952v4)
- **Published**: 2020-10-21 12:42:23+00:00
- **Updated**: 2021-01-21 10:00:28+00:00
- **Authors**: Leon Lang, Maurice Weiler
- **Comment**: 100 pages
- **Journal**: None
- **Summary**: Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with G-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the G-steerability constraint has been derived, it has to date only been solved for specific use cases - a general characterization of G-steerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of G being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, and 3) harmonic basis functions on homogeneous spaces.



### 2nd Place Solution to Instance Segmentation of IJCAI 3D AI Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2010.10957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10957v1)
- **Published**: 2020-10-21 12:53:01+00:00
- **Updated**: 2020-10-21 12:53:01+00:00
- **Authors**: Kai Jiang, Xiangyue Liu, Zheng Ju, Xiang Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with MS-COCO, the dataset for the competition has a larger proportion of large objects which area is greater than 96x96 pixels. As getting fine boundaries is vitally important for large object segmentation, Mask R-CNN with PointRend is selected as the base segmentation framework to output high-quality object boundaries. Besides, a better engine that integrates ResNeSt, FPN and DCNv2, and a range of effective tricks that including multi-scale training and test time augmentation are applied to improve segmentation performance. Our best performance is an ensemble of four models (three PointRend-based models and SOLOv2), which won the 2nd place in IJCAI-PRICAI 3D AI Challenge 2020: Instance Segmentation.



### Learning to Guide Local Feature Matches
- **Arxiv ID**: http://arxiv.org/abs/2010.10959v1
- **DOI**: 10.1109/3DV50981.2020.00123
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10959v1)
- **Published**: 2020-10-21 12:53:36+00:00
- **Updated**: 2020-10-21 12:53:36+00:00
- **Authors**: François Darmon, Mathieu Aubry, Pascal Monasse
- **Comment**: Accepted to 3DV 2020
- **Journal**: None
- **Summary**: We tackle the problem of finding accurate and robust keypoint correspondences between images. We propose a learning-based approach to guide local feature matches via a learned approximate image matching. Our approach can boost the results of SIFT to a level similar to state-of-the-art deep descriptors, such as Superpoint, ContextDesc, or D2-Net and can improve performance for these descriptors. We introduce and study different levels of supervision to learn coarse correspondences. In particular, we show that weak supervision from epipolar geometry leads to performances higher than the stronger but more biased point level supervision and is a clear improvement over weak image level supervision. We demonstrate the benefits of our approach in a variety of conditions by evaluating our guided keypoint correspondences for localization of internet images on the YFCC100M dataset and indoor images on theSUN3D dataset, for robust localization on the Aachen day-night benchmark and for 3D reconstruction in challenging conditions using the LTLL historical image data.



### Progressive Batching for Efficient Non-linear Least Squares
- **Arxiv ID**: http://arxiv.org/abs/2010.10968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10968v1)
- **Published**: 2020-10-21 13:00:04+00:00
- **Updated**: 2020-10-21 13:00:04+00:00
- **Authors**: Huu Le, Christopher Zach, Edward Rosten, Oliver J. Woodford
- **Comment**: Accepted to ACCV 2020
- **Journal**: None
- **Summary**: Non-linear least squares solvers are used across a broad range of offline and real-time model fitting problems. Most improvements of the basic Gauss-Newton algorithm tackle convergence guarantees or leverage the sparsity of the underlying problem structure for computational speedup. With the success of deep learning methods leveraging large datasets, stochastic optimization methods received recently a lot of attention. Our work borrows ideas from both stochastic machine learning and statistics, and we present an approach for non-linear least-squares that guarantees convergence while at the same time significantly reduces the required amount of computation. Empirical results show that our proposed method achieves competitive convergence rates compared to traditional second-order approaches on common computer vision problems, such as image alignment and essential matrix estimation, with very large numbers of residuals.



### Synthetic Expressions are Better Than Real for Learning to Detect Facial Actions
- **Arxiv ID**: http://arxiv.org/abs/2010.10979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10979v1)
- **Published**: 2020-10-21 13:11:45+00:00
- **Updated**: 2020-10-21 13:11:45+00:00
- **Authors**: Koichiro Niinuma, Itir Onal Ertugrul, Jeffrey F Cohn, László A Jeni
- **Comment**: None
- **Journal**: None
- **Summary**: Critical obstacles in training classifiers to detect facial actions are the limited sizes of annotated video databases and the relatively low frequencies of occurrence of many actions. To address these problems, we propose an approach that makes use of facial expression generation. Our approach reconstructs the 3D shape of the face from each video frame, aligns the 3D mesh to a canonical view, and then trains a GAN-based network to synthesize novel images with facial action units of interest. To evaluate this approach, a deep neural network was trained on two separate datasets: One network was trained on video of synthesized facial expressions generated from FERA17; the other network was trained on unaltered video from the same database. Both networks used the same train and validation partitions and were tested on the test partition of actual video from FERA17. The network trained on synthesized facial expressions outperformed the one trained on actual facial expressions and surpassed current state-of-the-art approaches.



### What is Wrong with Continual Learning in Medical Image Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2010.11008v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11008v2)
- **Published**: 2020-10-21 13:48:37+00:00
- **Updated**: 2023-02-10 16:48:21+00:00
- **Authors**: Camila Gonzalez, Nick Lemke, Georgios Sakas, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning protocols are attracting increasing attention from the medical imaging community. In continual environments, datasets acquired under different conditions arrive sequentially; and each is only available for a limited period of time. Given the inherent privacy risks associated with medical data, this setup reflects the reality of deployment for deep learning diagnostic radiology systems. Many techniques exist to learn continuously for image classification, and several have been adapted to semantic segmentation. Yet most struggle to accumulate knowledge in a meaningful manner. Instead, they focus on preventing the problem of catastrophic forgetting, even when this reduces model plasticity and thereon burdens the training process. This puts into question whether the additional overhead of knowledge preservation is worth it - particularly for medical image segmentation, where computation requirements are already high - or if maintaining separate models would be a better solution. We propose UNEG, a simple and widely applicable multi-model benchmark that maintains separate segmentation and autoencoder networks for each training stage. The autoencoder is built from the same architecture as the segmentation network, which in our case is a full-resolution nnU-Net, to bypass any additional design decisions. During inference, the reconstruction error is used to select the most appropriate segmenter for each test image. Open this concept, we develop a fair evaluation scheme for different continual learning settings that moves beyond the prevention of catastrophic forgetting. Our results across three regions of interest (prostate, hippocampus, and right ventricle) show that UNEG outperforms several continual learning methods, reinforcing the need for strong baselines in continual learning research.



### Learning Curves for Analysis of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11029v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.11029v2)
- **Published**: 2020-10-21 14:20:05+00:00
- **Updated**: 2021-04-05 17:01:02+00:00
- **Authors**: Derek Hoiem, Tanmay Gupta, Zhizhong Li, Michal M. Shlapentokh-Rothman
- **Comment**: Improved text and figure organization, additional experiments on
  optimization
- **Journal**: None
- **Summary**: Learning curves model a classifier's test error as a function of the number of training samples. Prior works show that learning curves can be used to select model parameters and extrapolate performance. We investigate how to use learning curves to evaluate design choices, such as pretraining, architecture, and data augmentation. We propose a method to robustly estimate learning curves, abstract their parameters into error and data-reliance, and evaluate the effectiveness of different parameterizations. Our experiments exemplify use of learning curves for analysis and yield several interesting observations.



### Anatomically-Informed Deep Learning on Contrast-Enhanced Cardiac MRI for Scar Segmentation and Clinical Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2010.11081v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11081v2)
- **Published**: 2020-10-21 15:43:08+00:00
- **Updated**: 2021-01-08 21:04:30+00:00
- **Authors**: Haley G. Abramson, Dan M. Popescu, Rebecca Yu, Changxin Lai, Julie K. Shade, Katherine C. Wu, Mauro Maggioni, Natalia A. Trayanova
- **Comment**: Haley G. Abramson and Dan M. Popescu contributed equally to this work
- **Journal**: None
- **Summary**: Visualizing disease-induced scarring and fibrosis in the heart on cardiac magnetic resonance (CMR) imaging with contrast enhancement (LGE) is paramount in characterizing disease progression and quantifying pathophysiological substrates of arrhythmias. However, segmentation and scar/fibrosis identification from LGE-CMR is an intensive manual process prone to large inter-observer variability. Here, we present a novel fully-automated anatomically-informed deep learning solution for left ventricle (LV) and scar/fibrosis segmentation and clinical feature extraction from LGE-CMR. The technology involves three cascading convolutional neural networks that segment myocardium and scar/fibrosis from raw LGE-CMR images and constrain these segmentations within anatomical guidelines, thus facilitating seamless derivation of clinically-significant parameters. In addition to available LGE-CMR images, training used "LGE-like" synthetically enhanced cine scans. Results show excellent agreement with those of trained experts in terms of segmentation (balanced accuracy of $96\%$ and $75\%$ for LV and scar segmentation), clinical features ($2\%$ difference in mean scar-to-LV wall volume fraction), and anatomical fidelity. Our segmentation technology is extendable to other computer vision medical applications and to problems requiring guidelines adherence of predicted outputs.



### Adaptive Pixel-wise Structured Sparse Network for Efficient CNNs
- **Arxiv ID**: http://arxiv.org/abs/2010.11083v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11083v3)
- **Published**: 2020-10-21 15:47:13+00:00
- **Updated**: 2021-03-20 08:46:42+00:00
- **Authors**: Chen Tang, Wenyu Sun, Zhuqing Yuan, Yongpan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: To accelerate deep CNN models, this paper proposes a novel spatially adaptive framework that can dynamically generate pixel-wise sparsity according to the input image. The sparse scheme is pixel-wise refined, regional adaptive under a unified importance map, which makes it friendly to hardware implementation. A sparse controlling method is further presented to enable online adjustment for applications with different precision/latency requirements. The sparse model is applicable to a wide range of vision tasks. Experimental results show that this method efficiently improve the computing efficiency for both image classification using ResNet-18 and super resolution using SRResNet. On image classification task, our method can save 30%-70% MACs with a slightly drop in top-1 and top-5 accuracy. On super resolution task, our method can reduce more than 90% MACs while only causing around 0.1 dB and 0.01 decreasing in PSNR and SSIM. Hardware validation is also included.



### UAV LiDAR Point Cloud Segmentation of A Stack Interchange with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11106v1
- **DOI**: 10.1109/IGARSS47720.2021.9554610
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11106v1)
- **Published**: 2020-10-21 16:15:41+00:00
- **Updated**: 2020-10-21 16:15:41+00:00
- **Authors**: Weikai Tan, Dedong Zhang, Lingfei Ma, Ying Li, Lanying Wang, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Stack interchanges are essential components of transportation systems. Mobile laser scanning (MLS) systems have been widely used in road infrastructure mapping, but accurate mapping of complicated multi-layer stack interchanges are still challenging. This study examined the point clouds collected by a new Unmanned Aerial Vehicle (UAV) Light Detection and Ranging (LiDAR) system to perform the semantic segmentation task of a stack interchange. An end-to-end supervised 3D deep learning framework was proposed to classify the point clouds. The proposed method has proven to capture 3D features in complicated interchange scenarios with stacked convolution and the result achieved over 93% classification accuracy. In addition, the new low-cost semi-solid-state LiDAR sensor Livox Mid-40 featuring a incommensurable rosette scanning pattern has demonstrated its potential in high-definition urban mapping.



### One Model to Reconstruct Them All: A Novel Way to Use the Stochastic Noise in StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2010.11113v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11113v1)
- **Published**: 2020-10-21 16:24:07+00:00
- **Updated**: 2020-10-21 16:24:07+00:00
- **Authors**: Christian Bartz, Joseph Bethge, Haojin Yang, Christoph Meinel
- **Comment**: Code and Models are available at
  https://github.com/Bartzi/one-model-to-reconstruct-them-all
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have achieved state-of-the-art performance for several image generation and manipulation tasks. Different works have improved the limited understanding of the latent space of GANs by embedding images into specific GAN architectures to reconstruct the original images. We present a novel StyleGAN-based autoencoder architecture, which can reconstruct images with very high quality across several data domains. We demonstrate a previously unknown grade of generalizablility by training the encoder and decoder independently and on different datasets. Furthermore, we provide new insights about the significance and capabilities of noise inputs of the well-known StyleGAN architecture. Our proposed architecture can handle up to 40 images per second on a single GPU, which is approximately 28x faster than previous approaches. Finally, our model also shows promising results, when compared to the state-of-the-art on the image denoising task, although it was not explicitly designed for this task.



### Study of star clusters in the M83 galaxy with a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2010.11126v1
- **DOI**: 10.3847/1538-3881/abbf53
- **Categories**: **astro-ph.GA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11126v1)
- **Published**: 2020-10-21 16:35:09+00:00
- **Updated**: 2020-10-21 16:35:09+00:00
- **Authors**: J. Bialopetravičius, D. Narbutis
- **Comment**: 14 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: We present a study of evolutionary and structural parameters of star cluster candidates in the spiral galaxy M83. For this we use a convolutional neural network trained on mock clusters and capable of fast identification and localization of star clusters, as well as inference of their parameters from multi-band images. We use this pipeline to detect 3,380 cluster candidates in Hubble Space Telescope observations. The sample of cluster candidates shows an age gradient across the galaxy's spiral arms, which is in good agreement with predictions of the density wave theory and other studies. As measured from the dust lanes of the spiral arms, the younger population of cluster candidates peaks at the distance of $\sim$0.4 kpc while the older candidates are more dispersed, but shifted towards $\gtrsim$0.7 kpc in the leading part of the spiral arms. We find high extinction cluster candidates positioned in the trailing part of the spiral arms, close to the dust lanes. We also find a large number of dense older clusters near the center of the galaxy and a slight increase of the typical cluster size further from the center.



### Black-Box Ripper: Copying black-box models using generative evolutionary algorithms
- **Arxiv ID**: http://arxiv.org/abs/2010.11158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.11158v1)
- **Published**: 2020-10-21 17:25:23+00:00
- **Updated**: 2020-10-21 17:25:23+00:00
- **Authors**: Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, Marius Popescu
- **Comment**: Accepted as Oral at NeurIPS 2020
- **Journal**: None
- **Summary**: We study the task of replicating the functionality of black-box neural models, for which we only know the output class probabilities provided for a set of input images. We assume back-propagation through the black-box model is not possible and its training images are not available, e.g. the model could be exposed only through an API. In this context, we present a teacher-student framework that can distill the black-box (teacher) model into a student model with minimal accuracy loss. To generate useful data samples for training the student, our framework (i) learns to generate images on a proxy data set (with images and classes different from those used to train the black-box) and (ii) applies an evolutionary strategy to make sure that each generated data sample exhibits a high response for a specific class when given as input to the black box. Our framework is compared with several baseline and state-of-the-art methods on three benchmark data sets. The empirical evidence indicates that our model is superior to the considered baselines. Although our method does not back-propagate through the black-box network, it generally surpasses state-of-the-art methods that regard the teacher as a glass-box model. Our code is available at: https://github.com/antoniobarbalau/black-box-ripper.



### 3D Meta Point Signature: Learning to Learn 3D Point Signature for 3D Dense Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2010.11159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11159v1)
- **Published**: 2020-10-21 17:27:39+00:00
- **Updated**: 2020-10-21 17:27:39+00:00
- **Authors**: Hao Huang, Lingjing Wang, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Point signature, a representation describing the structural neighborhood of a point in 3D shapes, can be applied to establish correspondences between points in 3D shapes. Conventional methods apply a weight-sharing network, e.g., any kind of graph neural networks, across all neighborhoods to directly generate point signatures and gain the generalization ability by extensive training over a large amount of training samples from scratch. However, these methods lack the flexibility in rapidly adapting to unseen neighborhood structures and thus generalizes poorly on new point sets. In this paper, we propose a novel meta-learning based 3D point signature model, named 3Dmetapointsignature (MEPS) network, that is capable of learning robust point signatures in 3D shapes. By regarding each point signature learning process as a task, our method obtains an optimized model over the best performance on the distribution of all tasks, generating reliable signatures for new tasks, i.e., signatures of unseen point neighborhoods. Specifically, the MEPS consists of two modules: a base signature learner and a meta signature learner. During training, the base-learner is trained to perform specific signature learning tasks. In the meantime, the meta-learner is trained to update the base-learner with optimal parameters. During testing, the meta-learner that is learned with the distribution of all tasks can adaptively change parameters of the base-learner, accommodating to unseen local neighborhoods. We evaluate the MEPS model on two datasets, e.g., FAUST and TOSCA, for dense 3Dshape correspondence. Experimental results demonstrate that our method not only gains significant improvements over the baseline model and achieves state-of-the-art results, but also is capable of handling unseen 3D shapes.



### In-the-wild Drowsiness Detection from Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2010.11162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11162v1)
- **Published**: 2020-10-21 17:28:56+00:00
- **Updated**: 2020-10-21 17:28:56+00:00
- **Authors**: Ajjen Joshi, Survi Kyal, Sandipan Banerjee, Taniya Mishra
- **Comment**: Paper from HSIM Workshop at IEEE Intelligent Vehicles Symposium 2020
  (IV2020)
- **Journal**: None
- **Summary**: Driving in a state of drowsiness is a major cause of road accidents, resulting in tremendous damage to life and property. Developing robust, automatic, real-time systems that can infer drowsiness states of drivers has the potential of making life-saving impact. However, developing drowsiness detection systems that work well in real-world scenarios is challenging because of the difficulties associated with collecting high-volume realistic drowsy data and modeling the complex temporal dynamics of evolving drowsy states. In this paper, we propose a data collection protocol that involves outfitting vehicles of overnight shift workers with camera kits that record their faces while driving. We develop a drowsiness annotation guideline to enable humans to label the collected videos into 4 levels of drowsiness: `alert', `slightly drowsy', `moderately drowsy' and `extremely drowsy'. We experiment with different convolutional and temporal neural network architectures to predict drowsiness states from pose, expression and emotion-based representation of the input video of the driver's face. Our best performing model achieves a macro ROC-AUC of 0.78, compared to 0.72 for a baseline model.



### Neural Star Domain as Primitive Representation
- **Arxiv ID**: http://arxiv.org/abs/2010.11248v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11248v2)
- **Published**: 2020-10-21 19:05:16+00:00
- **Updated**: 2020-11-12 14:22:27+00:00
- **Authors**: Yuki Kawana, Yusuke Mukuta, Tatsuya Harada
- **Comment**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020), Vancouver, Canada
- **Journal**: None
- **Summary**: Reconstructing 3D objects from 2D images is a fundamental task in computer vision. Accurate structured reconstruction by parsimonious and semantic primitive representation further broadens its application. When reconstructing a target shape with multiple primitives, it is preferable that one can instantly access the union of basic properties of the shape such as collective volume and surface, treating the primitives as if they are one single shape. This becomes possible by primitive representation with unified implicit and explicit representations. However, primitive representations in current approaches do not satisfy all of the above requirements at the same time. To solve this problem, we propose a novel primitive representation named neural star domain (NSD) that learns primitive shapes in the star domain. We show that NSD is a universal approximator of the star domain and is not only parsimonious and semantic but also an implicit and explicit shape representation. We demonstrate that our approach outperforms existing methods in image reconstruction tasks, semantic capabilities, and speed and quality of sampling high-resolution meshes.



### Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining
- **Arxiv ID**: http://arxiv.org/abs/2010.11289v3
- **DOI**: 10.1016/j.dss.2022.113794
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11289v3)
- **Published**: 2020-10-21 20:01:52+00:00
- **Updated**: 2022-05-02 16:13:28+00:00
- **Authors**: Wolfgang Kratsch, Fabian Koenig, Maximilian Roeglinger
- **Comment**: None
- **Journal**: Decision Support Systems, 2022
- **Summary**: Process mining is one of the most active research streams in business process management. In recent years, numerous methods have been proposed for analyzing structured process data. Yet, in many cases, it is only the digitized parts of processes that are directly captured from process-aware information systems, and manual activities often result in blind spots. While the use of video cameras to observe these activities could help to fill this gap, a standardized approach to extracting event logs from unstructured video data remains lacking. Here, we propose a reference architecture to bridge the gap between computer vision and process mining. Various evaluation activities (i.e., competing artifact analysis, prototyping, and real-world application) ensured that the proposed reference architecture allows flexible, use-case-driven, and context-specific instantiations. Our results also show that an exemplary software prototype instantiation of the proposed reference architecture is capable of automatically extracting most of the process-relevant events from unstructured video data.



### Unrolling of Deep Graph Total Variation for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2010.11290v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11290v2)
- **Published**: 2020-10-21 20:04:22+00:00
- **Updated**: 2021-03-24 02:04:56+00:00
- **Authors**: Huy Vu, Gene Cheung, Yonina C. Eldar
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning (DL) architectures like convolutional neural networks (CNNs) have enabled effective solutions in image denoising, in general their implementations overly rely on training data, lack interpretability, and require tuning of a large parameter set. In this paper, we combine classical graph signal filtering with deep feature learning into a competitive hybrid design -- one that utilizes interpretable analytical low-pass graph filters and employs 80% fewer network parameters than state-of-the-art DL denoising scheme DnCNN. Specifically, to construct a suitable similarity graph for graph spectral filtering, we first adopt a CNN to learn feature representations per pixel, and then compute feature distances to establish edge weights. Given a constructed graph, we next formulate a convex optimization problem for denoising using a graph total variation (GTV) prior. Via a $l_1$ graph Laplacian reformulation, we interpret its solution in an iterative procedure as a graph low-pass filter and derive its frequency response. For fast filter implementation, we realize this response using a Lanczos approximation. Experimental results show that in the case of statistical mistmatch, our algorithm outperformed DnCNN by up to 3dB in PSNR.



### Performance Prediction for Convolutional Neural Networks in Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2010.11297v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11297v1)
- **Published**: 2020-10-21 20:21:25+00:00
- **Updated**: 2020-10-21 20:21:25+00:00
- **Authors**: Halima Bouzidi, Hamza Ouarnoughi, Smail Niar, Abdessamad Ait El Cadi
- **Comment**: None
- **Journal**: None
- **Summary**: Running Convolutional Neural Network (CNN) based applications on edge devices near the source of data can meet the latency and privacy challenges. However due to their reduced computing resources and their energy constraints, these edge devices can hardly satisfy CNN needs in processing and data storage. For these platforms, choosing the CNN with the best trade-off between accuracy and execution time while respecting Hardware constraints is crucial. In this paper, we present and compare five (5) of the widely used Machine Learning based methods for execution time prediction of CNNs on two (2) edge GPU platforms. For these 5 methods, we also explore the time needed for their training and tuning their corresponding hyperparameters. Finally, we compare times to run the prediction models on different platforms. The utilization of these methods will highly facilitate design space exploration by providing quickly the best CNN on a target edge GPU. Experimental results show that eXtreme Gradient Boosting (XGBoost) provides a less than 14.73% average prediction error even for unexplored and unseen CNN models' architectures. Random Forest (RF) depicts comparable accuracy but needs more effort and time to be trained. The other 3 approaches (OLS, MLP and SVR) are less accurate for CNN performances estimation.



### Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training
- **Arxiv ID**: http://arxiv.org/abs/2010.12440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.12440v1)
- **Published**: 2020-10-21 20:43:47+00:00
- **Updated**: 2020-10-21 20:43:47+00:00
- **Authors**: Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site Li, Jane You, Ju Lu
- **Comment**: Published in Thirty-Fourth AAAI Conference on Artificial Intelligence
  (AAAI) 2020. arXiv admin note: text overlap with arXiv:2008.04751
- **Journal**: None
- **Summary**: Semantic segmentation (SS) is an important perception manner for self-driving cars and robotics, which classifies each pixel into a pre-determined class. The widely-used cross entropy (CE) loss-based deep networks has achieved significant progress w.r.t. the mean Intersection-over Union (mIoU). However, the cross entropy loss can not take the different importance of each class in an self-driving system into account. For example, pedestrians in the image should be much more important than the surrounding buildings when make a decisions in the driving, so their segmentation results are expected to be as accurate as possible. In this paper, we propose to incorporate the importance-aware inter-class correlation in a Wasserstein training framework by configuring its ground distance matrix. The ground distance matrix can be pre-defined following a priori in a specific task, and the previous importance-ignored methods can be the particular cases. From an optimization perspective, we also extend our ground metric to a linear, convex or concave increasing function $w.r.t.$ pre-defined ground distance. We evaluate our method on CamVid and Cityscapes datasets with different backbones (SegNet, ENet, FCN and Deeplab) in a plug and play fashion. In our extenssive experiments, Wasserstein loss demonstrates superior segmentation performance on the predefined critical classes for safe-driving.



### Voronoi Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11339v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.11339v1)
- **Published**: 2020-10-21 22:42:19+00:00
- **Updated**: 2020-10-21 22:42:19+00:00
- **Authors**: Soroosh Yazdani, Andrea Tagliasacchi
- **Comment**: Technical report
- **Journal**: None
- **Summary**: In this technical report, we investigate extending convolutional neural networks to the setting where functions are not sampled in a grid pattern. We show that by treating the samples as the average of a function within a cell, we can find a natural equivalent of most layers used in CNN. We also present an algorithm for running inference for these models exactly using standard convex geometry algorithms.



