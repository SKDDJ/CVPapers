# Arxiv Papers in cs.CV on 2020-10-28
### Scaling Laws for Autoregressive Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/2010.14701v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14701v2)
- **Published**: 2020-10-28 02:17:24+00:00
- **Updated**: 2020-11-06 04:16:36+00:00
- **Authors**: Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish
- **Comment**: 20+17 pages, 33 figures; added appendix with additional language
  results
- **Journal**: None
- **Summary**: We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains.   The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions.   We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question "Is a picture worth a thousand words?"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.



### Quantified Facial Temporal-Expressiveness Dynamics for Affect Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.14705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14705v1)
- **Published**: 2020-10-28 02:22:22+00:00
- **Updated**: 2020-10-28 02:22:22+00:00
- **Authors**: Md Taufeeq Uddin, Shaun Canavan
- **Comment**: 25th International Conference on Pattern Recognition (ICPR2020)
- **Journal**: None
- **Summary**: The quantification of visual affect data (e.g. face images) is essential to build and monitor automated affect modeling systems efficiently. Considering this, this work proposes quantified facial Temporal-expressiveness Dynamics (TED) to quantify the expressiveness of human faces. The proposed algorithm leverages multimodal facial features by incorporating static and dynamic information to enable accurate measurements of facial expressiveness. We show that TED can be used for high-level tasks such as summarization of unstructured visual data, and expectation from and interpretation of automated affect recognition models. To evaluate the positive impact of using TED, a case study was conducted on spontaneous pain using the UNBC-McMaster spontaneous shoulder pain dataset. Experimental results show the efficacy of using TED for quantified affect analysis.



### CompRess: Self-Supervised Learning by Compressing Representations
- **Arxiv ID**: http://arxiv.org/abs/2010.14713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14713v1)
- **Published**: 2020-10-28 02:49:18+00:00
- **Updated**: 2020-10-28 02:49:18+00:00
- **Authors**: Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning aims to learn good representations with unlabeled data. Recent works have shown that larger models benefit more from self-supervised learning than smaller models. As a result, the gap between supervised and self-supervised learning has been greatly reduced for larger models. In this work, instead of designing a new pseudo task for self-supervised learning, we develop a model compression method to compress an already learned, deep self-supervised model (teacher) to a smaller one (student). We train the student model so that it mimics the relative similarity between the data points in the teacher's embedding space. For AlexNet, our method outperforms all previous methods including the fully supervised model on ImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor evaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the first time a self-supervised AlexNet has outperformed supervised one on ImageNet classification. Our code is available here: https://github.com/UMBCvision/CompRess



### Differentiable Channel Sparsity Search via Weight Sharing within Filters
- **Arxiv ID**: http://arxiv.org/abs/2010.14714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14714v2)
- **Published**: 2020-10-28 02:49:32+00:00
- **Updated**: 2022-01-05 13:39:06+00:00
- **Authors**: Yu Zhao, Chung-Kuei Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the differentiable channel sparsity search (DCSS) for convolutional neural networks. Unlike traditional channel pruning algorithms which require users to manually set prune ratios for each convolutional layer, DCSS automatically searches the optimal combination of sparsities. Inspired by the differentiable architecture search (DARTS), we draw lessons from the continuous relaxation and leverage the gradient information to balance the computational cost and metrics. Since directly applying the scheme of DARTS causes shape mismatching and excessive memory consumption, we introduce a novel technique called weight sharing within filters. This technique elegantly eliminates the problem of shape mismatching with negligible additional resources. We conduct comprehensive experiments on not only image classification but also find-grained tasks including semantic segmentation and image super resolution to verify the effectiveness of DCSS. Compared with previous network pruning approaches, DCSS achieves state-of-the-art results for image classification. Experimental results of semantic segmentation and image super resolution indicate that task-specific search achieves better performance than transferring slim models, demonstrating the wide applicability and high efficiency of DCSS.



### Deep DA for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2010.15675v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15675v3)
- **Published**: 2020-10-28 03:20:34+00:00
- **Updated**: 2021-11-09 16:04:43+00:00
- **Authors**: Gnana Praveen R, Eric Granger, Patrick Cardinal
- **Comment**: This is a multiple copy of the same version
- **Journal**: None
- **Summary**: Automatic estimation of pain intensity from facial expressions in videos has an immense potential in health care applications. However, domain adaptation (DA) is needed to alleviate the problem of domain shifts that typically occurs between video data captured in source and target do-mains. Given the laborious task of collecting and annotating videos, and the subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is gaining attention in such applications. Yet, most state-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relation between intensity levels, nor the temporal coherence of multiple consecutive frames. This paper introduces a new deep learn-ing model for weakly-supervised DA with ordinal regression(WSDA-OR), where videos in target domain have coarse la-bels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among the intensity levels as-signed to the target sequences, and associates multiple relevant frames to sequence-level labels (instead of a single frame). In particular, it learns discriminant and domain-invariant feature representations by integrating multiple in-stance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from the target domain. The proposed approach was validated on the RECOLA video dataset as fully-labeled source domain, and UNBC-McMaster video data as weakly-labeled target domain. We have also validated WSDA-OR on BIOVID and Fatigue (private) datasets for sequence level estimation. Experimental results indicate that our approach can provide a significant improvement over the state-of-the-art models, allowing to achieve a greater localization accuracy.



### MultiMix: Sparingly Supervised, Extreme Multitask Learning From Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2010.14731v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14731v2)
- **Published**: 2020-10-28 03:47:29+00:00
- **Updated**: 2021-04-01 06:21:28+00:00
- **Authors**: Ayaan Haque, Abdullah-Al-Zubaer Imran, Adam Wang, Demetri Terzopoulos
- **Comment**: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)
  2021
- **Journal**: None
- **Summary**: Semi-supervised learning via learning from limited quantities of labeled data has been investigated as an alternative to supervised counterparts. Maximizing knowledge gains from copious unlabeled data benefit semi-supervised learning settings. Moreover, learning multiple tasks within the same model further improves model generalizability. We propose a novel multitask learning model, namely MultiMix, which jointly learns disease classification and anatomical segmentation in a sparingly supervised manner, while preserving explainability through bridge saliency between the two tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justify the effectiveness of our multitasking model for the classification of pneumonia and segmentation of lungs from chest X-ray images. Moreover, both in-domain and cross-domain evaluations across the tasks further showcase the potential of our model to adapt to challenging generalization scenarios.



### ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications
- **Arxiv ID**: http://arxiv.org/abs/2010.14742v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14742v1)
- **Published**: 2020-10-28 04:27:28+00:00
- **Updated**: 2020-10-28 04:27:28+00:00
- **Authors**: Hochul Hwang, Cheongjae Jang, Geonwoo Park, Junghyun Cho, Ig-Jae Kim
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: To train deep learning models for vision-based action recognition of elders' daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders' activities in many aspects, making it challenging to recognize elders' daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders' daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options, and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders' activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders' daily activities.



### Classification Beats Regression: Counting of Cells from Greyscale Microscopic Images based on Annotation-free Training Samples
- **Arxiv ID**: http://arxiv.org/abs/2010.14782v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14782v2)
- **Published**: 2020-10-28 06:19:30+00:00
- **Updated**: 2020-10-29 20:35:45+00:00
- **Authors**: Xin Ding, Qiong Zhang, William J. Welch
- **Comment**: None
- **Journal**: The CAAI International Conference on Artificial Intelligence
  (CICAI 2021)
- **Summary**: Modern methods often formulate the counting of cells from microscopic images as a regression problem and more or less rely on expensive, manually annotated training images (e.g., dot annotations indicating the centroids of cells or segmentation masks identifying the contours of cells). This work proposes a supervised learning framework based on classification-oriented convolutional neural networks (CNNs) to count cells from greyscale microscopic images without using annotated training images. In this framework, we formulate the cell counting task as an image classification problem, where the cell counts are taken as class labels. This formulation has its limitation when some cell counts in the test stage do not appear in the training data. Moreover, the ordinal relation among cell counts is not utilized. To deal with these limitations, we propose a simple but effective data augmentation (DA) method to synthesize images for the unseen cell counts. We also introduce an ensemble method, which can not only moderate the influence of unseen cell counts but also utilize the ordinal information to improve the prediction accuracy. This framework outperforms many modern cell counting methods and won the data analysis competition (Case Study 1: Counting Cells From Microscopic Images https://ssc.ca/en/case-study/case-study-1-counting-cells-microscopic-images) of the 47th Annual Meeting of the Statistical Society of Canada (SSC). Our code is available at https://github.com/anno2020/CellCount_TinyBBBC005.



### Point of Care Image Analysis for COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2011.01789v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01789v2)
- **Published**: 2020-10-28 06:43:52+00:00
- **Updated**: 2020-11-10 06:12:19+00:00
- **Authors**: Daniel Yaron, Daphna Keidar, Elisha Goldstein, Yair Shachar, Ayelet Blass, Oz Frank, Nir Schipper, Nogah Shabshin, Ahuva Grubstein, Dror Suhami, Naama R. Bogot, Eyal Sela, Amiel A. Dror, Mordehay Vaturi, Federico Mento, Elena Torri, Riccardo Inchingolo, Andrea Smargiassi, Gino Soldati, Tiziano Perrone, Libertario Demi, Meirav Galun, Shai Bagon, Yishai M. Elyada, Yonina C. Eldar
- **Comment**: Not approved for arXiv
- **Journal**: None
- **Summary**: Early detection of COVID-19 is key in containing the pandemic. Disease detection and evaluation based on imaging is fast and cheap and therefore plays an important role in COVID-19 handling. COVID-19 is easier to detect in chest CT, however, it is expensive, non-portable, and difficult to disinfect, making it unfit as a point-of-care (POC) modality. On the other hand, chest X-ray (CXR) and lung ultrasound (LUS) are widely used, yet, COVID-19 findings in these modalities are not always very clear. Here we train deep neural networks to significantly enhance the capability to detect, grade and monitor COVID-19 patients using CXRs and LUS. Collaborating with several hospitals in Israel we collect a large dataset of CXRs and use this dataset to train a neural network obtaining above 90% detection rate for COVID-19. In addition, in collaboration with ULTRa (Ultrasound Laboratory Trento, Italy) and hospitals in Italy we obtained POC ultrasound data with annotations of the severity of disease and trained a deep network for automatic severity grading.



### Class-Agnostic Segmentation Loss and Its Application to Salient Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.14793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14793v1)
- **Published**: 2020-10-28 07:11:15+00:00
- **Updated**: 2020-10-28 07:11:15+00:00
- **Authors**: Angira Sharma, Naeemullah Khan, Ganesh Sundaramoorthi, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel loss function, called class-agnostic segmentation (CAS) loss. With CAS loss the class descriptors are learned during training of the network. We don't require to define the label of a class a-priori, rather the CAS loss clusters regions with similar appearance together in a weakly-supervised manner. Furthermore, we show that the CAS loss function is sparse, bounded, and robust to class-imbalance. We apply our CAS loss function with fully-convolutional ResNet101 and DeepLab-v3 architectures to the binary segmentation problem of salient object detection. We investigate the performance against the state-of-the-art methods in two settings of low and high-fidelity training data on seven salient object detection datasets. For low-fidelity training data (incorrect class label) class-agnostic segmentation loss outperforms the state-of-the-art methods on salient object detection datasets by staggering margins of around 50%. For high-fidelity training data (correct class labels) class-agnostic segmentation models perform as good as the state-of-the-art approaches while beating the state-of-the-art methods on most datasets. In order to show the utility of the loss function across different domains we also test on general segmentation dataset, where class-agnostic segmentation loss outperforms cross-entropy based loss by huge margins on both region and edge metrics.



### SFU-Store-Nav: A Multimodal Dataset for Indoor Human Navigation
- **Arxiv ID**: http://arxiv.org/abs/2010.14802v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.14802v1)
- **Published**: 2020-10-28 08:00:35+00:00
- **Updated**: 2020-10-28 08:00:35+00:00
- **Authors**: Zhitian Zhang, Jimin Rhim, Taher Ahmadi, Kefan Yang, Angelica Lim, Mo Chen
- **Comment**: 5 pages, paper submitted to Data In Brief Journal
- **Journal**: None
- **Summary**: This article describes a dataset collected in a set of experiments that involves human participants and a robot. The set of experiments was conducted in the computing science robotics lab in Simon Fraser University, Burnaby, BC, Canada, and its aim is to gather data containing common gestures, movements, and other behaviours that may indicate humans' navigational intent relevant for autonomous robot navigation. The experiment simulates a shopping scenario where human participants come in to pick up items from his/her shopping list and interact with a Pepper robot that is programmed to help the human participant. We collected visual data and motion capture data from 108 human participants. The visual data contains live recordings of the experiments and the motion capture data contains the position and orientation of the human participants in world coordinates. This dataset could be valuable for researchers in the robotics, machine learning and computer vision community.



### Large-Scale MIDI-based Composer Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.14805v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2010.14805v1)
- **Published**: 2020-10-28 08:07:55+00:00
- **Updated**: 2020-10-28 08:07:55+00:00
- **Authors**: Qiuqiang Kong, Keunwoo Choi, Yuxuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Music classification is a task to classify a music piece into labels such as genres or composers. We propose large-scale MIDI based composer classification systems using GiantMIDI-Piano, a transcription-based dataset. We propose to use piano rolls, onset rolls, and velocity rolls as input representations and use deep neural networks as classifiers. To our knowledge, we are the first to investigate the composer classification problem with up to 100 composers. By using convolutional recurrent neural networks as models, our MIDI based composer classification system achieves a 10-composer and a 100-composer classification accuracies of 0.648 and 0.385 (evaluated on 30-second clips) and 0.739 and 0.489 (evaluated on music pieces), respectively. Our MIDI based composer system outperforms several audio-based baseline classification systems, indicating the effectiveness of using compact MIDI representations for composer classification.



### AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?
- **Arxiv ID**: http://arxiv.org/abs/2010.14808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14808v2)
- **Published**: 2020-10-28 08:15:27+00:00
- **Updated**: 2021-07-21 03:36:11+00:00
- **Authors**: Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, Shucheng Cao, Qi Zhang, Shangqing Liu, Yunpeng Wang, Yuhui Li, Jian He, Xiaoping Yang
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: With the unprecedented developments in deep learning, automatic segmentation of main abdominal organs seems to be a solved problem as state-of-the-art (SOTA) methods have achieved comparable results with inter-rater variability on many benchmark datasets. However, most of the existing abdominal datasets only contain single-center, single-phase, single-vendor, or single-disease cases, and it is unclear whether the excellent performance can generalize on diverse datasets. This paper presents a large and diverse abdominal CT organ segmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans from 12 medical centers, including multi-phase, multi-vendor, and multi-disease cases. Furthermore, we conduct a large-scale study for liver, kidney, spleen, and pancreas segmentation and reveal the unsolved segmentation problems of the SOTA methods, such as the limited generalization ability on distinct medical centers, phases, and unseen diseases. To advance the unsolved problems, we further build four organ segmentation benchmarks for fully supervised, semi-supervised, weakly supervised, and continual learning, which are currently challenging and active research topics. Accordingly, we develop a simple and effective method for each benchmark, which can be used as out-of-the-box methods and strong baselines. We believe the AbdomenCT-1K dataset will promote future in-depth research towards clinical applicable abdominal organ segmentation methods. The datasets, codes, and trained models are publicly available at https://github.com/JunMa11/AbdomenCT-1K.



### Cycle-Contrast for Self-Supervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.14810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14810v1)
- **Published**: 2020-10-28 08:27:58+00:00
- **Updated**: 2020-10-28 08:27:58+00:00
- **Authors**: Quan Kong, Wenpeng Wei, Ziwei Deng, Tomoaki Yoshinaga, Tomokazu Murakami
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We present Cycle-Contrastive Learning (CCL), a novel self-supervised method for learning video representation. Following a nature that there is a belong and inclusion relation of video and its frames, CCL is designed to find correspondences across frames and videos considering the contrastive representation in their domains respectively. It is different from recent approaches that merely learn correspondences across frames or clips. In our method, the frame and video representations are learned from a single network based on an R3D architecture, with a shared non-linear transformation for embedding both frame and video features before the cycle-contrastive loss. We demonstrate that the video representation learned by CCL can be transferred well to downstream tasks of video understanding, outperforming previous methods in nearest neighbour retrieval and action recognition tasks on UCF101, HMDB51 and MMAct.



### Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets
- **Arxiv ID**: http://arxiv.org/abs/2010.14819v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14819v2)
- **Published**: 2020-10-28 08:49:45+00:00
- **Updated**: 2020-12-24 09:21:04+00:00
- **Authors**: Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, Tong Zhang
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: To obtain excellent deep neural architectures, a series of techniques are carefully designed in EfficientNets. The giant formula for simultaneously enlarging the resolution, depth and width provides us a Rubik's cube for neural networks. So that we can find networks with high efficiency and excellent performance by twisting the three dimensions. This paper aims to explore the twisting rules for obtaining deep neural networks with minimum model sizes and computational costs. Different from the network enlarging, we observe that resolution and depth are more important than width for tiny networks. Therefore, the original method, i.e., the compound scaling in EfficientNet is no longer suitable. To this end, we summarize a tiny formula for downsizing neural architectures through a series of smaller models derived from the EfficientNet-B0 with the FLOPs constraint. Experimental results on the ImageNet benchmark illustrate that our TinyNet performs much better than the smaller version of EfficientNets using the inversed giant formula. For instance, our TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is about 1.9% higher than that of the previous best MobileNetV3 with similar computational cost. Code will be available at https://github.com/huawei-noah/ghostnet/tree/master/tinynet_pytorch, and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/tinynet.



### Deep Manifold Transformation for Nonlinear Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/2010.14831v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.14831v3)
- **Published**: 2020-10-28 09:09:41+00:00
- **Updated**: 2021-05-03 15:24:11+00:00
- **Authors**: Stan Z. Li, Zelin Zang, Lirong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Manifold learning-based encoders have been playing important roles in nonlinear dimensionality reduction (NLDR) for data exploration. However, existing methods can often fail to preserve geometric, topological and/or distributional structures of data. In this paper, we propose a deep manifold learning framework, called deep manifold transformation (DMT) for unsupervised NLDR and embedding learning. DMT enhances deep neural networks by using cross-layer local geometry-preserving (LGP) constraints. The LGP constraints constitute the loss for deep manifold learning and serve as geometric regularizers for NLDR network training. Extensive experiments on synthetic and real-world data demonstrate that DMT networks outperform existing leading manifold-based NLDR methods in terms of preserving the structures of data.



### Micro Stripes Analyses for Iris Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.14850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14850v2)
- **Published**: 2020-10-28 09:55:35+00:00
- **Updated**: 2020-11-03 11:56:53+00:00
- **Authors**: Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at International Join Conference on Biometrics (IJCB 2020)
- **Journal**: None
- **Summary**: Iris recognition systems are vulnerable to the presentation attacks, such as textured contact lenses or printed images. In this paper, we propose a lightweight framework to detect iris presentation attacks by extracting multiple micro-stripes of expanded normalized iris textures. In this procedure, a standard iris segmentation is modified. For our presentation attack detection network to better model the classification problem, the segmented area is processed to provide lower dimensional input segments and a higher number of learning samples. Our proposed Micro Stripes Analyses (MSA) solution samples the segmented areas as individual stripes. Then, the majority vote makes the final classification decision of those micro-stripes. Experiments are demonstrated on five databases, where two databases (IIITD-WVU and Notre Dame) are from the LivDet-2017 Iris competition. An in-depth experimental evaluation of this framework reveals a superior performance compared with state-of-the-art algorithms. Moreover, our solution minimizes the confusion between textured (attack) and soft (bona fide) contact lens presentations.



### Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.14851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14851v1)
- **Published**: 2020-10-28 09:57:00+00:00
- **Updated**: 2020-10-28 09:57:00+00:00
- **Authors**: Jianyuan Wang, Yiran Zhong, Yuchao Dai, Kaihao Zhang, Pan Ji, Hongdong Li
- **Comment**: NeurIPS 2020, 9 pages
- **Journal**: None
- **Summary**: Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark.



### Medical Deep Learning -- A systematic Meta-Review
- **Arxiv ID**: http://arxiv.org/abs/2010.14881v5
- **DOI**: 10.1016/j.cmpb.2022.106874
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14881v5)
- **Published**: 2020-10-28 11:01:40+00:00
- **Updated**: 2022-05-18 10:18:17+00:00
- **Authors**: Jan Egger, Christina Gsaxner, Antonio Pepe, Kelsey L. Pomykala, Frederic Jonske, Manuel Kurz, Jianning Li, Jens Kleesiek
- **Comment**: 22 pages, 7 figures, 7 tables, 159 references. Computer Methods and
  Programs in Biomedicine (CMPB), Elsevier, May 2022
- **Journal**: None
- **Summary**: Deep learning (DL) has remarkably impacted several different scientific disciplines over the last few years. E.g., in image processing and analysis, DL algorithms were able to outperform other cutting-edge methods. Additionally, DL has delivered state-of-the-art results in tasks like autonomous driving, outclassing previous attempts. There are even instances where DL outperformed humans, for example with object recognition and gaming. DL is also showing vast potential in the medical domain. With the collection of large quantities of patient records and data, and a trend towards personalized treatments, there is a great need for automated and reliable processing and analysis of health information. Patient data is not only collected in clinical centers, like hospitals and private practices, but also by mobile healthcare apps or online websites. The abundance of collected patient data and the recent growth in the DL field has resulted in a large increase in research efforts. In Q2/2020, the search engine PubMed returned already over 11,000 results for the search term 'deep learning', and around 90% of these publications are from the last three years. However, even though PubMed represents the largest search engine in the medical field, it does not cover all medical-related publications. Hence, a complete overview of the field of 'medical deep learning' is almost impossible to obtain and acquiring a full overview of medical sub-fields is becoming increasingly more difficult. Nevertheless, several review and survey articles about medical DL have been published within the last few years. They focus, in general, on specific medical scenarios, like the analysis of medical images containing specific pathologies. With these surveys as a foundation, the aim of this article is to provide the first high-level, systematic meta-review of medical DL surveys.



### Transferable Universal Adversarial Perturbations Using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2010.14919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14919v2)
- **Published**: 2020-10-28 12:31:59+00:00
- **Updated**: 2020-10-29 15:19:41+00:00
- **Authors**: Atiye Sadat Hashemi, Andreas Bär, Saeed Mozaffari, Tim Fingscheidt
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks tend to be vulnerable to adversarial perturbations, which by adding to a natural image can fool a respective model with high confidence. Recently, the existence of image-agnostic perturbations, also known as universal adversarial perturbations (UAPs), were discovered. However, existing UAPs still lack a sufficiently high fooling rate, when being applied to an unknown target model. In this paper, we propose a novel deep learning technique for generating more transferable UAPs. We utilize a perturbation generator and some given pretrained networks so-called source models to generate UAPs using the ImageNet dataset. Due to the similar feature representation of various model architectures in the first layer, we propose a loss formulation that focuses on the adversarial energy only in the respective first layer of the source models. This supports the transferability of our generated UAPs to any other target model. We further empirically analyze our generated UAPs and demonstrate that these perturbations generalize very well towards different target models. Surpassing the current state of the art in both, fooling rate and model-transferability, we can show the superiority of our proposed approach. Using our generated non-targeted UAPs, we obtain an average fooling rate of 93.36% on the source models (state of the art: 82.16%). Generating our UAPs on the deep ResNet-152, we obtain about a 12% absolute fooling rate advantage vs. cutting-edge methods on VGG-16 and VGG-19 target models.



### Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2010.14924v2
- **DOI**: 10.1109/ICPR48806.2021.9413109
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14924v2)
- **Published**: 2020-10-28 12:38:41+00:00
- **Updated**: 2021-06-29 11:45:19+00:00
- **Authors**: Jyri Maanpää, Josef Taher, Petri Manninen, Leo Pakola, Iaroslav Melekhov, Juha Hyyppä
- **Comment**: 8 pages, 8 figures, included in the conference proceedings of the
  2020 25th International Conference on Pattern Recognition (ICPR), 2021
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR),
  2021, pp. 699-706
- **Summary**: Autonomous driving is challenging in adverse road and weather conditions in which there might not be lane lines, the road might be covered in snow and the visibility might be poor. We extend the previous work on end-to-end learning for autonomous steering to operate in these adverse real-life conditions with multimodal data. We collected 28 hours of driving data in several road and weather conditions and trained convolutional neural networks to predict the car steering wheel angle from front-facing color camera images and lidar range and reflectance data. We compared the CNN model performances based on the different modalities and our results show that the lidar modality improves the performances of different multimodal sensor-fusion models. We also performed on-road tests with different models and they support this observation.



### MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.14925v4
- **DOI**: 10.1109/ISBI48211.2021.9434062
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.14925v4)
- **Published**: 2020-10-28 12:41:18+00:00
- **Updated**: 2021-05-20 22:47:30+00:00
- **Authors**: Jiancheng Yang, Rui Shi, Bingbing Ni
- **Comment**: ISBI 2021. Code and dataset are available at
  https://medmnist.github.io/
- **Journal**: None
- **Summary**: We present MedMNIST, a collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge. Covering the primary data modalities in medical image analysis, it is diverse on data scale (from 100 to 100,000) and tasks (binary/multi-class, ordinal regression and multi-label). MedMNIST could be used for educational purpose, rapid prototyping, multi-modal machine learning or AutoML in medical image analysis. Moreover, MedMNIST Classification Decathlon is designed to benchmark AutoML algorithms on all 10 datasets; We have compared several baseline methods, including open-source or commercial AutoML tools. The datasets, evaluation code and baseline methods for MedMNIST are publicly available at https://medmnist.github.io/.



### Leveraging Visual Question Answering to Improve Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.14953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14953v1)
- **Published**: 2020-10-28 13:11:34+00:00
- **Updated**: 2020-10-28 13:11:34+00:00
- **Authors**: Stanislav Frolov, Shailza Jolly, Jörn Hees, Andreas Dengel
- **Comment**: Accepted to the LANTERN workshop at COLING 2020
- **Journal**: None
- **Summary**: Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82% to 84.79% when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.



### Object Hider: Adversarial Patch Attack Against Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2010.14974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14974v1)
- **Published**: 2020-10-28 13:34:16+00:00
- **Updated**: 2020-10-28 13:34:16+00:00
- **Authors**: Yusheng Zhao, Huanqian Yan, Xingxing Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been widely used in many computer vision tasks. However, it is proved that they are susceptible to small, imperceptible perturbations added to the input. Inputs with elaborately designed perturbations that can fool deep learning models are called adversarial examples, and they have drawn great concerns about the safety of deep neural networks. Object detection algorithms are designed to locate and classify objects in images or videos and they are the core of many computer vision tasks, which have great research value and wide applications. In this paper, we focus on adversarial attack on some state-of-the-art object detection models. As a practical alternative, we use adversarial patches for the attack. Two adversarial patch generation algorithms have been proposed: the heatmap-based algorithm and the consensus-based algorithm. The experiment results have shown that the proposed methods are highly effective, transferable and generic. Additionally, we have applied the proposed methods to competition "Adversarial Challenge on Object Detection" that is organized by Alibaba on the Tianchi platform and won top 7 in 1701 teams. Code is available at: https://github.com/FenHua/DetDak



### Real-time Tropical Cyclone Intensity Estimation by Handling Temporally Heterogeneous Satellite Data
- **Arxiv ID**: http://arxiv.org/abs/2010.14977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.14977v1)
- **Published**: 2020-10-28 13:40:07+00:00
- **Updated**: 2020-10-28 13:40:07+00:00
- **Authors**: Boyo Chen, Buo-Fu Chen, Yun-Nung Chen
- **Comment**: under review of AAAI 2021
- **Journal**: None
- **Summary**: Analyzing big geophysical observational data collected by multiple advanced sensors on various satellite platforms promotes our understanding of the geophysical system. For instance, convolutional neural networks (CNN) have achieved great success in estimating tropical cyclone (TC) intensity based on satellite data with fixed temporal frequency (e.g., 3 h). However, to achieve more timely (under 30 min) and accurate TC intensity estimates, a deep learning model is demanded to handle temporally-heterogeneous satellite observations. Specifically, infrared (IR1) and water vapor (WV) images are available under every 15 minutes, while passive microwave rain rate (PMW) is available for about every 3 hours. Meanwhile, the visible (VIS) channel is severely affected by noise and sunlight intensity, making it difficult to be utilized. Therefore, we propose a novel framework that combines generative adversarial network (GAN) with CNN. The model utilizes all data, including VIS and PMW information, during the training phase and eventually uses only the high-frequent IR1 and WV data for providing intensity estimates during the predicting phase. Experimental results demonstrate that the hybrid GAN-CNN framework achieves comparable precision to the state-of-the-art models, while possessing the capability of increasing the maximum estimation frequency from 3 hours to less than 15 minutes.



### Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.14982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14982v2)
- **Published**: 2020-10-28 13:47:16+00:00
- **Updated**: 2022-06-10 10:50:48+00:00
- **Authors**: Rui Dai, Srijan Das, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, Gianpiero Francesca
- **Comment**: Toyota Smarthome Untrimmed dataset, project page:
  https://project.inria.fr/toyotasmarthome
- **Journal**: None
- **Summary**: Designing activity detection systems that can be successfully deployed in daily-living environments requires datasets that pose the challenges typical of real-world scenarios. In this paper, we introduce a new untrimmed daily-living dataset that features several real-world challenges: Toyota Smarthome Untrimmed (TSU). TSU contains a wide variety of activities performed in a spontaneous manner. The dataset contains dense annotations including elementary, composite activities and activities involving interactions with objects. We provide an analysis of the real-world challenges featured by our dataset, highlighting the open issues for detection algorithms. We show that current state-of-the-art methods fail to achieve satisfactory performance on the TSU dataset. Therefore, we propose a new baseline method for activity detection to tackle the novel challenges provided by our dataset. This method leverages one modality (i.e. optic flow) to generate the attention weights to guide another modality (i.e RGB) to better detect the activity boundaries. This is particularly beneficial to detect activities characterized by high temporal variance. We show that the method we propose outperforms state-of-the-art methods on TSU and on another popular challenging dataset, Charades.



### Road Damage Detection and Classification with Detectron2 and Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2010.15021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15021v1)
- **Published**: 2020-10-28 14:53:17+00:00
- **Updated**: 2020-10-28 14:53:17+00:00
- **Authors**: Vung Pham, Chau Pham, Tommy Dang
- **Comment**: Under review for Global Road Damage Detection Challenge 2020, A Track
  in the IEEE Big Data 2020 Big Data Cup Challenge
- **Journal**: None
- **Summary**: The road is vital for many aspects of life, and road maintenance is crucial for human safety. One of the critical tasks to allow timely repair of road damages is to quickly and efficiently detect and classify them. This work details the strategies and experiments evaluated for these tasks. Specifically, we evaluate Detectron2's implementation of Faster R-CNN using different base models and configurations. We also experiment with these approaches using the Global Road Damage Detection Challenge 2020, A Track in the IEEE Big Data 2020 Big Data Cup Challenge dataset. The results show that the X101-FPN base model for Faster R-CNN with Detectron2's default configurations are efficient and general enough to be transferable to different countries in this challenge. This approach results in F1 scores of 51.0% and 51.4% for the test1 and test2 sets of the challenge, respectively. Though the visualizations show good prediction results, the F1 scores are low. Therefore, we also evaluate the prediction results against the existing annotations and discover some discrepancies. Thus, we also suggest strategies to improve the labeling process for this dataset.



### Data Agnostic Filter Gating for Efficient Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.15041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15041v1)
- **Published**: 2020-10-28 15:26:40+00:00
- **Updated**: 2020-10-28 15:26:40+00:00
- **Authors**: Xiu Su, Shan You, Tao Huang, Hongyan Xu, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: To deploy a well-trained CNN model on low-end computation edge devices, it is usually supposed to compress or prune the model under certain computation budget (e.g., FLOPs). Current filter pruning methods mainly leverage feature maps to generate important scores for filters and prune those with smaller scores, which ignores the variance of input batches to the difference in sparse structure over filters. In this paper, we propose a data agnostic filter pruning method that uses an auxiliary network named Dagger module to induce pruning and takes pretrained weights as input to learn the importance of each filter. In addition, to help prune filters with certain FLOPs constraints, we leverage an explicit FLOPs-aware regularization to directly promote pruning filters toward target FLOPs. Extensive experimental results on CIFAR-10 and ImageNet datasets indicate our superiority to other state-of-the-art filter pruning methods. For example, our 50\% FLOPs ResNet-50 can achieve 76.1\% Top-1 accuracy on ImageNet dataset, surpassing many other filter pruning methods.



### Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases
- **Arxiv ID**: http://arxiv.org/abs/2010.15052v3
- **DOI**: 10.1145/3442188.3445932
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15052v3)
- **Published**: 2020-10-28 15:55:49+00:00
- **Updated**: 2021-01-27 18:48:10+00:00
- **Authors**: Ryan Steed, Aylin Caliskan
- **Comment**: 10 pages, 3 figures. Replaced example image completions of real
  people with completions of artificial people
- **Journal**: None
- **Summary**: Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.



### Attribution Preservation in Network Compression for Reliable Network Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2010.15054v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15054v1)
- **Published**: 2020-10-28 16:02:31+00:00
- **Updated**: 2020-10-28 16:02:31+00:00
- **Authors**: Geondo Park, June Yong Yang, Sung Ju Hwang, Eunho Yang
- **Comment**: NeurIPS 2020. Code: https://github.com/GeondoPark/attribute-preserve
- **Journal**: None
- **Summary**: Neural networks embedded in safety-sensitive applications such as self-driving cars and wearable health monitors rely on two important techniques: input attribution for hindsight analysis and network compression to reduce its size for edge-computing. In this paper, we show that these seemingly unrelated techniques conflict with each other as network compression deforms the produced attributions, which could lead to dire consequences for mission-critical applications. This phenomenon arises due to the fact that conventional network compression methods only preserve the predictions of the network while ignoring the quality of the attributions. To combat the attribution inconsistency problem, we present a framework that can preserve the attributions while compressing a network. By employing the Weighted Collapsed Attribution Matching regularizer, we match the attribution maps of the network being compressed to its pre-compression former self. We demonstrate the effectiveness of our algorithm both quantitatively and qualitatively on diverse compression methods.



### Generative Adversarial Networks in Human Emotion Synthesis:A Review
- **Arxiv ID**: http://arxiv.org/abs/2010.15075v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15075v2)
- **Published**: 2020-10-28 16:45:36+00:00
- **Updated**: 2020-11-07 11:05:36+00:00
- **Authors**: Noushin Hajarolasvadi, Miguel Arjona Ramírez, Hasan Demirel
- **Comment**: 46 pages, 28 figures
- **Journal**: None
- **Summary**: Synthesizing realistic data samples is of great value for both academic and industrial communities. Deep generative models have become an emerging topic in various research areas like computer vision and signal processing. Affective computing, a topic of a broad interest in computer vision society, has been no exception and has benefited from generative models. In fact, affective computing observed a rapid derivation of generative models during the last two decades. Applications of such models include but are not limited to emotion recognition and classification, unimodal emotion synthesis, and cross-modal emotion synthesis. As a result, we conducted a review of recent advances in human emotion synthesis by studying available databases, advantages, and disadvantages of the generative models along with the related training strategies considering two principal human communication modalities, namely audio and video. In this context, facial expression synthesis, speech emotion synthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed extensively under different application scenarios. Gradually, we discuss open research problems to push the boundaries of this research area for future works.



### Forgery Blind Inspection for Detecting Manipulations of Gel Electrophoresis Images
- **Arxiv ID**: http://arxiv.org/abs/2010.15086v1
- **DOI**: 10.1109/GlobalSIP.2018.8646594
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15086v1)
- **Published**: 2020-10-28 16:59:51+00:00
- **Updated**: 2020-10-28 16:59:51+00:00
- **Authors**: Hao-Chiang Shao, Ya-Jen Cheng, Meng-Yun Duh, Chia-Wen Lin
- **Comment**: This version is an extension of Prof. Shao's previous conference
  paper (IEEE GlobalSIP 2018): "Unveiling Vestiges of Man-Made Modifications on
  Molecular-Biological Experiment Images."
  (https://doi.org/10.1109/GlobalSIP.2018.8646594)
- **Journal**: None
- **Summary**: Recently, falsified images have been found in papers involved in research misconducts. However, although there have been many image forgery detection methods, none of them was designed for molecular-biological experiment images. In this paper, we proposed a fast blind inquiry method, named FBI$_{GEL}$, for integrity of images obtained from two common sorts of molecular experiments, i.e., western blot (WB) and polymerase chain reaction (PCR). Based on an optimized pseudo-background capable of highlighting local residues, FBI$_{GEL}$ can reveal traceable vestiges suggesting inappropriate local modifications on WB/PCR images. Additionally, because the optimized pseudo-background is derived according to a closed-form solution, FBI$_{GEL}$ is computationally efficient and thus suitable for large scale inquiry tasks for WB/PCR image integrity. We applied FBI$_{GEL}$ on several papers questioned by the public on \textbf{PUBPEER}, and our results show that figures of those papers indeed contain doubtful unnatural patterns.



### Panoster: End-to-end Panoptic Segmentation of LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.15157v2
- **DOI**: 10.1109/LRA.2021.3060405
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.15157v2)
- **Published**: 2020-10-28 18:10:20+00:00
- **Updated**: 2021-02-12 17:46:31+00:00
- **Authors**: Stefano Gasperini, Mohammad-Ali Nikouei Mahani, Alvaro Marcos-Ramiro, Nassir Navab, Federico Tombari
- **Comment**: Preprint of IEEE RA-L article
- **Journal**: None
- **Summary**: Panoptic segmentation has recently unified semantic and instance segmentation, previously addressed separately, thus taking a step further towards creating more comprehensive and efficient perception systems. In this paper, we present Panoster, a novel proposal-free panoptic segmentation method for LiDAR point clouds. Unlike previous approaches relying on several steps to group pixels or points into objects, Panoster proposes a simplified framework incorporating a learning-based clustering solution to identify instances. At inference time, this acts as a class-agnostic segmentation, allowing Panoster to be fast, while outperforming prior methods in terms of accuracy. Without any post-processing, Panoster reached state-of-the-art results among published approaches on the challenging SemanticKITTI benchmark, and further increased its lead by exploiting heuristic techniques. Additionally, we showcase how our method can be flexibly and effectively applied on diverse existing semantic architectures to deliver panoptic predictions.



### CNN Profiler on Polar Coordinate Images for Tropical Cyclone Structure Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.15158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.15158v1)
- **Published**: 2020-10-28 18:13:19+00:00
- **Updated**: 2020-10-28 18:13:19+00:00
- **Authors**: Boyo Chen, Buo-Fu Chen, Chun-Min Hsiao
- **Comment**: Submitted to AAAI2021
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have achieved great success in analyzing tropical cyclones (TC) with satellite images in several tasks, such as TC intensity estimation. In contrast, TC structure, which is conventionally described by a few parameters estimated subjectively by meteorology specialists, is still hard to be profiled objectively and routinely. This study applies CNN on satellite images to create the entire TC structure profiles, covering all the structural parameters. By utilizing the meteorological domain knowledge to construct TC wind profiles based on historical structure parameters, we provide valuable labels for training in our newly released benchmark dataset. With such a dataset, we hope to attract more attention to this crucial issue among data scientists. Meanwhile, a baseline is established with a specialized convolutional model operating on polar-coordinates. We discovered that it is more feasible and physically reasonable to extract structural information on polar-coordinates, instead of Cartesian coordinates, according to a TC's rotational and spiral natures. Experimental results on the released benchmark dataset verified the robustness of the proposed model and demonstrated the potential for applying deep learning techniques for this barely developed yet important topic.



### Predicting intubation support requirement of patients using Chest X-ray with Deep Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.01787v1
- **DOI**: 10.13140/RG.2.2.18271.69282
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01787v1)
- **Published**: 2020-10-28 19:12:50+00:00
- **Updated**: 2020-10-28 19:12:50+00:00
- **Authors**: Aniket Maurya
- **Comment**: This work is in active progress
- **Journal**: None
- **Summary**: Recent developments in medical imaging with Deep Learning presents evidence of automated diagnosis and prognosis. It can also be a complement to currently available diagnosis methods. Deep Learning can be leveraged for diagnosis, severity prediction, intubation support prediction and many similar tasks. We present prediction of intubation support requirement for patients from the Chest X-ray using Deep representation learning. We release our source code publicly at https://github.com/aniketmaurya/covid-research.



### Ground Roll Suppression using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.15209v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15209v1)
- **Published**: 2020-10-28 20:21:21+00:00
- **Updated**: 2020-10-28 20:21:21+00:00
- **Authors**: Dario Augusto Borges Oliveira, Daniil Semin, Semen Zaytsev
- **Comment**: EAGE 2020
- **Journal**: None
- **Summary**: Seismic data processing plays a major role in seismic exploration as it conditions much of the seismic interpretation performance. In this context, generating reliable post-stack seismic data depends also on disposing of an efficient pre-stack noise attenuation tool. Here we tackle ground roll noise, one of the most challenging and common noises observed in pre-stack seismic data. Since ground roll is characterized by relative low frequencies and high amplitudes, most commonly used approaches for its suppression are based on frequency-amplitude filters for ground roll characteristic bands. However, when signal and noise share the same frequency ranges, these methods usually deliver also signal suppression or residual noise. In this paper we take advantage of the highly non-linear features of convolutional neural networks, and propose to use different architectures to detect ground roll in shot gathers and ultimately to suppress them using conditional generative adversarial networks. Additionally, we propose metrics to evaluate ground roll suppression, and report strong results compared to expert filtering. Finally, we discuss generalization of trained models for similar and different geologies to better understand the feasibility of our proposal in real applications.



### Accurate Prostate Cancer Detection and Segmentation on Biparametric MRI using Non-local Mask R-CNN with Histopathological Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/2010.15233v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15233v1)
- **Published**: 2020-10-28 21:07:09+00:00
- **Updated**: 2020-10-28 21:07:09+00:00
- **Authors**: Zhenzhen Dai, Ivan Jambor, Pekka Taimen, Milan Pantelic, Mohamed Elshaikh, Craig Rogers, Otto Ettala, Peter Boström, Hannu Aronen, Harri Merisaari, Ning Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: We aimed to develop deep machine learning (DL) models to improve the detection and segmentation of intraprostatic lesions (IL) on bp-MRI by using whole amount prostatectomy specimen-based delineations. We also aimed to investigate whether transfer learning and self-training would improve results with small amount labelled data.   Methods: 158 patients had suspicious lesions delineated on MRI based on bp-MRI, 64 patients had ILs delineated on MRI based on whole mount prostatectomy specimen sections, 40 patients were unlabelled. A non-local Mask R-CNN was proposed to improve the segmentation accuracy. Transfer learning was investigated by fine-tuning a model trained using MRI-based delineations with prostatectomy-based delineations. Two label selection strategies were investigated in self-training. The performance of models was evaluated by 3D detection rate, dice similarity coefficient (DSC), 95 percentile Hausdrauff (95 HD, mm) and true positive ratio (TPR).   Results: With prostatectomy-based delineations, the non-local Mask R-CNN with fine-tuning and self-training significantly improved all evaluation metrics. For the model with the highest detection rate and DSC, 80.5% (33/41) of lesions in all Gleason Grade Groups (GGG) were detected with DSC of 0.548[0.165], 95 HD of 5.72[3.17] and TPR of 0.613[0.193]. Among them, 94.7% (18/19) of lesions with GGG > 2 were detected with DSC of 0.604[0.135], 95 HD of 6.26[3.44] and TPR of 0.580[0.190].   Conclusion: DL models can achieve high prostate cancer detection and segmentation accuracy on bp-MRI based on annotations from histologic images. To further improve the performance, more data with annotations of both MRI and whole amount prostatectomy specimens are required.



### Semantic video segmentation for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2010.15250v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15250v1)
- **Published**: 2020-10-28 21:42:16+00:00
- **Updated**: 2020-10-28 21:42:16+00:00
- **Authors**: Minh Triet Chau
- **Comment**: This work was done around 2017. Some minor changes were added
- **Journal**: None
- **Summary**: We aim to solve semantic video segmentation in autonomous driving, namely road detection in real time video, using techniques discussed in (Shelhamer et al., 2016a). While fully convolutional network gives good result, we show that the speed can be halved while preserving the accuracy. The test dataset being used is KITTI, which consists of real footage from Germany's streets.



### Fusion Models for Improved Visual Captioning
- **Arxiv ID**: http://arxiv.org/abs/2010.15251v2
- **DOI**: 10.1007/978-3-030-68780-9_32
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15251v2)
- **Published**: 2020-10-28 21:55:25+00:00
- **Updated**: 2020-12-05 04:01:02+00:00
- **Authors**: Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow
- **Comment**: Accepted at "Multi-Modal Deep Learning: Challenges and Applications"
  (MMDLCA), International Conference on Pattern Recognition (ICPR)-2020,
  Milano, Italia
- **Journal**: Springer LNCS, volume 12666, 2021
- **Summary**: Visual captioning aims to generate textual descriptions given images or videos. Traditionally, image captioning models are trained on human annotated datasets such as Flickr30k and MS-COCO, which are limited in size and diversity. This limitation hinders the generalization capabilities of these models while also rendering them liable to making mistakes. Language models can, however, be trained on vast amounts of freely available unlabelled data and have recently emerged as successful language encoders and coherent text generators. Meanwhile, several unimodal and multimodal fusion techniques have been proven to work well for natural language generation and automatic speech recognition. Building on these recent developments, and with the aim of improving the quality of generated captions, the contribution of our work in this paper is two-fold: First, we propose a generic multimodal model fusion framework for caption generation as well as emendation where we utilize different fusion strategies to integrate a pretrained Auxiliary Language Model (AuxLM) within the traditional encoder-decoder visual captioning frameworks. Next, we employ the same fusion strategies to integrate a pretrained Masked Language Model (MLM), namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for emending both syntactic and semantic errors in captions. Our caption emendation experiments on three benchmark image captioning datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the baseline, indicating the usefulness of our proposed multimodal fusion strategies. Further, we perform a preliminary qualitative analysis on the emended captions and identify error categories based on the type of corrections.



### Object sieving and morphological closing to reduce false detections in wide-area aerial imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.15260v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15260v1)
- **Published**: 2020-10-28 22:20:17+00:00
- **Updated**: 2020-10-28 22:20:17+00:00
- **Authors**: Xin Gao, Sundaresh Ram, Jeffrey J. Rodriguez
- **Comment**: 5 Pages, Submitted to 2016 23rd International Conference of Image
  Processing (ICIP), September 23-28, Phoenix, AZ, USA (Paper ID: 3218)
- **Journal**: None
- **Summary**: For object detection in wide-area aerial imagery, post-processing is usually needed to reduce false detections. We propose a two-stage post-processing scheme which comprises an area-thresholding sieving process and a morphological closing operation. We use two wide-area aerial videos to compare the performance of five object detection algorithms in the absence and in the presence of our post-processing scheme. The automatic detection results are compared with the ground-truth objects. Several metrics are used for performance comparison.



### Deep Shells: Unsupervised Shape Correspondence with Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2010.15261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15261v1)
- **Published**: 2020-10-28 22:24:07+00:00
- **Updated**: 2020-10-28 22:24:07+00:00
- **Authors**: Marvin Eisenberger, Aysim Toker, Laura Leal-Taixé, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel unsupervised learning approach to 3D shape correspondence that builds a multiscale matching pipeline into a deep neural network. This approach is based on smooth shells, the current state-of-the-art axiomatic correspondence method, which requires an a priori stochastic search over the space of initial poses. Our goal is to replace this costly preprocessing step by directly learning good initializations from the input surfaces. To that end, we systematically derive a fully differentiable, hierarchical matching pipeline from entropy regularized optimal transport. This allows us to combine it with a local feature extractor based on smooth, truncated spectral convolution filters. Finally, we show that the proposed unsupervised method significantly improves over the state-of-the-art on multiple datasets, even in comparison to the most recent supervised methods. Moreover, we demonstrate compelling generalization results by applying our learned filters to examples that significantly deviate from the training set.



### GloFlow: Global Image Alignment for Creation of Whole Slide Images for Pathology from Video
- **Arxiv ID**: http://arxiv.org/abs/2010.15269v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15269v2)
- **Published**: 2020-10-28 23:01:31+00:00
- **Updated**: 2020-11-12 08:53:58+00:00
- **Authors**: Viswesh Krishna, Anirudh Joshi, Philip L. Bulterys, Eric Yang, Andrew Y. Ng, Pranav Rajpurkar
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: The application of deep learning to pathology assumes the existence of digital whole slide images of pathology slides. However, slide digitization is bottlenecked by the high cost of precise motor stages in slide scanners that are needed for position information used for slide stitching. We propose GloFlow, a two-stage method for creating a whole slide image using optical flow-based image registration with global alignment using a computationally tractable graph-pruning approach. In the first stage, we train an optical flow predictor to predict pairwise translations between successive video frames to approximate a stitch. In the second stage, this approximate stitch is used to create a neighborhood graph to produce a corrected stitch. On a simulated dataset of video scans of WSIs, we find that our method outperforms known approaches to slide-stitching, and stitches WSIs resembling those produced by slide scanners.



### Class-incremental learning: survey and performance evaluation on image classification
- **Arxiv ID**: http://arxiv.org/abs/2010.15277v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15277v3)
- **Published**: 2020-10-28 23:28:15+00:00
- **Updated**: 2022-10-11 14:57:02+00:00
- **Authors**: Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D. Bagdanov, Joost van de Weijer
- **Comment**: Paper accepted for publication at TPAMI 2022. Code publicly available
  at https://github.com/mmasana/FACIL
- **Journal**: None
- **Summary**: For future learning systems, incremental learning is desirable because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored -- also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular, we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, an investigation into small and large domain shifts, and a comparison of various network architectures.



