# Arxiv Papers in cs.CV on 2020-10-29
### Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2010.15288v1
- **DOI**: 10.21437/Interspeech.2020
- **Categories**: **cs.LG**, cs.CV, cs.IT, cs.MM, math.IT, 68T01, 68T05, 68T07, 68T10, 62P15, I.2; I.2.0; I.2.6; I.2.7; I.2.11; I.5; I.5.1; I.5.2; I.5.4; I.4.10;
  H.5.1; H.5.2; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.15288v1)
- **Published**: 2020-10-29 00:14:33+00:00
- **Updated**: 2020-10-29 00:14:33+00:00
- **Authors**: Masood S. Mortazavi
- **Comment**: None
- **Journal**: Proceedings of INTERSPEECH 2020
- **Summary**: Semantically-aligned $(speech, image)$ datasets can be used to explore "visually-grounded speech". In a majority of existing investigations, features of an image signal are extracted using neural networks "pre-trained" on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without "transfer learning" through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \rightarrow image$ and $image \rightarrow speech$ queries.   Choosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \rightarrow image$ and $image \rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \rightarrow image$ and $image \rightarrow speech$ queries.



### Point Cloud Attribute Compression via Successive Subspace Graph Transform
- **Arxiv ID**: http://arxiv.org/abs/2010.15302v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.15302v1)
- **Published**: 2020-10-29 01:40:54+00:00
- **Updated**: 2020-10-29 01:40:54+00:00
- **Authors**: Yueru Chen, Yiting Shao, Jing Wang, Ge Li, C. -C. Jay Kuo
- **Comment**: Accepted by VCIP 2020
- **Journal**: None
- **Summary**: Inspired by the recently proposed successive subspace learning (SSL) principles, we develop a successive subspace graph transform (SSGT) to address point cloud attribute compression in this work. The octree geometry structure is utilized to partition the point cloud, where every node of the octree represents a point cloud subspace with a certain spatial size. We design a weighted graph with self-loop to describe the subspace and define a graph Fourier transform based on the normalized graph Laplacian. The transforms are applied to large point clouds from the leaf nodes to the root node of the octree recursively, while the represented subspace is expanded from the smallest one to the whole point cloud successively. It is shown by experimental results that the proposed SSGT method offers better R-D performances than the previous Region Adaptive Haar Transform (RAHT) method.



### Automatic joint damage quantification using computer vision and deep learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15303v1)
- **Published**: 2020-10-29 01:41:20+00:00
- **Updated**: 2020-10-29 01:41:20+00:00
- **Authors**: Quang Tran, Jeffery R. Roesler
- **Comment**: None
- **Journal**: None
- **Summary**: Joint raveled or spalled damage (henceforth called joint damage) can affect the safety and long-term performance of concrete pavements. It is important to assess and quantify the joint damage over time to assist in building action plans for maintenance, predicting maintenance costs, and maximize the concrete pavement service life. A framework for the accurate, autonomous, and rapid quantification of joint damage with a low-cost camera is proposed using a computer vision technique with a deep learning (DL) algorithm. The DL model is employed to train 263 images of sawcuts with joint damage. The trained DL model is used for pixel-wise color-masking joint damage in a series of query 2D images, which are used to reconstruct a 3D image using open-source structure from motion algorithm. Another damage quantification algorithm using a color threshold is applied to detect and compute the surface area of the damage in the 3D reconstructed image. The effectiveness of the framework was validated through inspecting joint damage at four transverse contraction joints in Illinois, USA, including three acceptable joints and one unacceptable joint by visual inspection. The results show the framework achieves 76% recall and 10% error.



### Recurrent neural circuits for contour detection
- **Arxiv ID**: http://arxiv.org/abs/2010.15314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.15314v1)
- **Published**: 2020-10-29 02:09:40+00:00
- **Updated**: 2020-10-29 02:09:40+00:00
- **Authors**: Drew Linsley, Junkyung Kim, Alekh Ashok, Thomas Serre
- **Comment**: Published in ICLR 2020
- **Journal**: None
- **Summary**: We introduce a deep recurrent neural network architecture that approximates visual cortical circuits. We show that this architecture, which we refer to as the gamma-net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces gamma-net contour detection accuracy by driving it to prefer low-level edges over high-level object boundary contours. Overall, our study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and that incorporating these circuits in artificial neural networks can improve computer vision.



### Exploring Generative Adversarial Networks for Image-to-Image Translation in STEM Simulation
- **Arxiv ID**: http://arxiv.org/abs/2010.15315v2
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15315v2)
- **Published**: 2020-10-29 02:14:57+00:00
- **Updated**: 2022-11-16 23:51:44+00:00
- **Authors**: Nick Lawrence, Mingren Shen, Ruiqi Yin, Cloris Feng, Dane Morgan
- **Comment**: None
- **Journal**: None
- **Summary**: The use of accurate scanning transmission electron microscopy (STEM) image simulation methods require large computation times that can make their use infeasible for the simulation of many images. Other simulation methods based on linear imaging models, such as the convolution method, are much faster but are too inaccurate to be used in application. In this paper, we explore deep learning models that attempt to translate a STEM image produced by the convolution method to a prediction of the high accuracy multislice image. We then compare our results to those of regression methods. We find that using the deep learning model Generative Adversarial Network (GAN) provides us with the best results and performs at a similar accuracy level to previous regression models on the same dataset. Codes and data for this project can be found in this GitHub repository, https://github.com/uw-cmg/GAN-STEM-Conv2MultiSlice.



### SAR-NAS: Skeleton-based Action Recognition via Neural Architecture Searching
- **Arxiv ID**: http://arxiv.org/abs/2010.15336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15336v1)
- **Published**: 2020-10-29 03:24:15+00:00
- **Updated**: 2020-10-29 03:24:15+00:00
- **Authors**: Haoyuan Zhang, Yonghong Hou, Pichao Wang, Zihui Guo, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a study of automatic design of neural network architectures for skeleton-based action recognition. Specifically, we encode a skeleton-based action instance into a tensor and carefully define a set of operations to build two types of network cells: normal cells and reduction cells. The recently developed DARTS (Differentiable Architecture Search) is adopted to search for an effective network architecture that is built upon the two types of cells. All operations are 2D based in order to reduce the overall computation and search space. Experiments on the challenging NTU RGB+D and Kinectics datasets have verified that most of the networks developed to date for skeleton-based action recognition are likely not compact and efficient. The proposed method provides an approach to search for such a compact network that is able to achieve comparative or even better performance than the state-of-the-art methods.



### Identifying safe intersection design through unsupervised feature extraction from satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.15343v1
- **DOI**: 10.1111/mice.12623
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV, 68T45 (Primary) 68U07, 68U10 (Secondary), I.2.10; I.4.6; I.4.8; I.4.9; I.5.3; J.2; J.6
- **Links**: [PDF](http://arxiv.org/pdf/2010.15343v1)
- **Published**: 2020-10-29 03:42:09+00:00
- **Updated**: 2020-10-29 03:42:09+00:00
- **Authors**: Jasper S. Wijnands, Haifeng Zhao, Kerry A. Nice, Jason Thompson, Katherine Scully, Jingqiu Guo, Mark Stevenson
- **Comment**: 16 pages, 10 figures. Computer-Aided Civil and Infrastructure
  Engineering (2020)
- **Journal**: None
- **Summary**: The World Health Organization has listed the design of safer intersections as a key intervention to reduce global road trauma. This article presents the first study to systematically analyze the design of all intersections in a large country, based on aerial imagery and deep learning. Approximately 900,000 satellite images were downloaded for all intersections in Australia and customized computer vision techniques emphasized the road infrastructure. A deep autoencoder extracted high-level features, including the intersection's type, size, shape, lane markings, and complexity, which were used to cluster similar designs. An Australian telematics data set linked infrastructure design to driving behaviors captured during 66 million kilometers of driving. This showed more frequent hard acceleration events (per vehicle) at four- than three-way intersections, relatively low hard deceleration frequencies at T-intersections, and consistently low average speeds on roundabouts. Overall, domain-specific feature extraction enabled the identification of infrastructure improvements that could result in safer driving behaviors, potentially reducing road trauma.



### Sea-Net: Squeeze-And-Excitation Attention Net For Diabetic Retinopathy Grading
- **Arxiv ID**: http://arxiv.org/abs/2010.15344v1
- **DOI**: 10.1109/ICIP40778.2020.9191345
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15344v1)
- **Published**: 2020-10-29 03:48:01+00:00
- **Updated**: 2020-10-29 03:48:01+00:00
- **Authors**: Ziyuan Zhao, Kartik Chopra, Zeng Zeng, Xiaoli Li
- **Comment**: Accepted to ICIP 2020
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP), pp.
  2496-2500
- **Summary**: Diabetes is one of the most common disease in individuals. \textit{Diabetic retinopathy} (DR) is a complication of diabetes, which could lead to blindness. Automatic DR grading based on retinal images provides a great diagnostic and prognostic value for treatment planning. However, the subtle differences among severity levels make it difficult to capture important features using conventional methods. To alleviate the problems, a new deep learning architecture for robust DR grading is proposed, referred to as SEA-Net, in which, spatial attention and channel attention are alternatively carried out and boosted with each other, improving the classification performance. In addition, a hybrid loss function is proposed to further maximize the inter-class distance and reduce the intra-class variability. Experimental results have shown the effectiveness of the proposed architecture.



### An automated and multi-parametric algorithm for objective analysis of meibography images
- **Arxiv ID**: http://arxiv.org/abs/2010.15352v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15352v1)
- **Published**: 2020-10-29 04:26:51+00:00
- **Updated**: 2020-10-29 04:26:51+00:00
- **Authors**: Peng Xiao, Zhongzhou Luo, Yuqing Deng, Gengyuan Wang, Jin Yuan
- **Comment**: 20 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Meibography is a non-contact imaging technique used by ophthalmologists to assist in the evaluation and diagnosis of meibomian gland dysfunction (MGD). While artificial qualitative analysis of meibography images could lead to low repeatability and efficiency and multi-parametric analysis is demanding to offer more comprehensive information in discovering subtle changes of meibomian glands during MGD progression, we developed an automated and multi-parametric algorithm for objective and quantitative analysis of meibography images. The full architecture of the algorithm can be divided into three steps: (1) segmentation of the tarsal conjunctiva area as the region of interest (ROI); (2) segmentation and identification of glands within the ROI; and (3) quantitative multi-parametric analysis including newly defined gland diameter deformation index (DI), gland tortuosity index (TI), and glands signal index (SI). To evaluate the performance of the automated algorithm, the similarity index (k) and the segmentation error including the false positive rate (r_P) and the false negative rate (r_N) are calculated between the manually defined ground truth and the automatic segmentations of both the ROI and meibomian glands of 15 typical meibography images. The feasibility of the algorithm is demonstrated in analyzing typical meibograhy images.



### Financial ticket intelligent recognition system based on deep learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15356v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15356v1)
- **Published**: 2020-10-29 05:07:40+00:00
- **Updated**: 2020-10-29 05:07:40+00:00
- **Authors**: Fukang Tian, Haiyu Wu, Bo Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Facing the rapid growth in the issuance of financial tickets (or bills, invoices etc.), traditional manual invoice reimbursement and financial accounting system are imposing an increasing burden on financial accountants and consuming excessive manpower. To solve this problem, we proposes an iterative self-learning Framework of Financial Ticket intelligent Recognition System (FFTRS), which can support the fast iterative updating and extensibility of the algorithm model, which are the fundamental requirements for a practical financial accounting system. In addition, we designed a simple yet efficient Financial Ticket Faster Detection network (FTFDNet) and an intelligent data warehouse of financial ticket are designed to strengthen its efficiency and performance. At present, the system can recognize 194 kinds of financial tickets and has an automatic iterative optimization mechanism, which means, with the increase of application time, the types of tickets supported by the system will continue to increase, and the accuracy of recognition will continue to improve. Experimental results show that the average recognition accuracy of the system is 97.07%, and the average running time for a single ticket is 175.67ms. The practical value of the system has been tested in a commercial application, which makes a beneficial attempt for the deep learning technology in financial accounting work.



### Collaborative Method for Incremental Learning on Classification and Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.15378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15378v1)
- **Published**: 2020-10-29 06:34:53+00:00
- **Updated**: 2020-10-29 06:34:53+00:00
- **Authors**: Byungju Kim, Jaeyoung Lee, Kyungsu Kim, Sungjin Kim, Junmo Kim
- **Comment**: published on ICIP
- **Journal**: None
- **Summary**: Although well-trained deep neural networks have shown remarkable performance on numerous tasks, they rapidly forget what they have learned as soon as they begin to learn with additional data with the previous data stop being provided. In this paper, we introduce a novel algorithm, Incremental Class Learning with Attribute Sharing (ICLAS), for incremental class learning with deep neural networks. As one of its component, we also introduce a generative model, incGAN, which can generate images with increased variety compared with the training data. Under challenging environment of data deficiency, ICLAS incrementally trains classification and the generation networks. Since ICLAS trains both networks, our algorithm can perform multiple times of incremental class learning. The experiments on MNIST dataset demonstrate the advantages of our algorithm.



### Measuring and Harnessing Transference in Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15413v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.15413v3)
- **Published**: 2020-10-29 08:25:43+00:00
- **Updated**: 2021-09-10 06:55:37+00:00
- **Authors**: Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, Chelsea Finn
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning can leverage information learned by one task to benefit the training of other tasks. Despite this capacity, naive formulations often degrade performance and in particular, identifying the tasks that would benefit from co-training remains a challenging design question. In this paper, we analyze the dynamics of information transfer, or transference, across tasks throughout training. Specifically, we develop a similarity measure that can quantify transference among tasks and use this quantity to both better understand the optimization dynamics of multi-task learning as well as improve overall learning performance. In the latter case, we propose two methods to leverage our transference metric. The first operates at a macro-level by selecting which tasks should train together while the second functions at a micro-level by determining how to combine task gradients at each training step. We find these methods can lead to significant improvement over prior work on three supervised multi-task learning benchmarks and one multi-task reinforcement learning paradigm.



### ProCAN: Progressive Growing Channel Attentive Non-Local Network for Lung Nodule Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.15417v3
- **DOI**: 10.1016/j.patcog.2021.108309
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15417v3)
- **Published**: 2020-10-29 08:42:11+00:00
- **Updated**: 2021-09-17 12:09:56+00:00
- **Authors**: Mundher Al-Shabi, Kelvin Shak, Maxine Tan
- **Comment**: None
- **Journal**: Published in 2022, Pattern Recognition
- **Summary**: Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method.



### FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements
- **Arxiv ID**: http://arxiv.org/abs/2010.15440v1
- **DOI**: 10.1109/TPAMI.2020.3033882
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15440v1)
- **Published**: 2020-10-29 09:20:22+00:00
- **Updated**: 2020-10-29 09:20:22+00:00
- **Authors**: Salman S. Khan, Varun Sundar, Vivek Boominathan, Ashok Veeraraghavan, Kaushik Mitra
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2020. Supplementary material attached. For project
  website, see https://siddiquesalman.github.io/flatnet/
- **Journal**: None
- **Summary**: Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, the current iterative-optimization-based reconstruction algorithms produce noisier and perceptually poorer images. In this work, we propose a non-iterative deep learning based reconstruction approach that results in orders of magnitude improvement in image quality for lensless reconstructions. Our approach, called $\textit{FlatNet}$, lays down a framework for reconstructing high-quality photorealistic images from mask-based lensless cameras, where the camera's forward model formulation is known. FlatNet consists of two stages: (1) an inversion stage that maps the measurement into a space of intermediate reconstruction by learning parameters within the forward model formulation, and (2) a perceptual enhancement stage that improves the perceptual quality of this intermediate reconstruction. These stages are trained together in an end-to-end manner. We show high-quality reconstructions by performing extensive experiments on real and challenging scenes using two different types of lensless prototypes: one which uses a separable forward model and another, which uses a more general non-separable cropped-convolution model. Our end-to-end approach is fast, produces photorealistic reconstructions, and is easy to adopt for other mask-based lensless cameras.



### Pretext-Contrastive Learning: Toward Good Practices in Self-supervised Video Representation Leaning
- **Arxiv ID**: http://arxiv.org/abs/2010.15464v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15464v2)
- **Published**: 2020-10-29 10:20:35+00:00
- **Updated**: 2021-04-04 14:42:00+00:00
- **Authors**: Li Tao, Xueting Wang, Toshihiko Yamasaki
- **Comment**: Under review
- **Journal**: None
- **Summary**: Recently, pretext-task based methods are proposed one after another in self-supervised video feature learning. Meanwhile, contrastive learning methods also yield good performance. Usually, new methods can beat previous ones as claimed that they could capture "better" temporal information. However, there exist setting differences among them and it is hard to conclude which is better. It would be much more convincing in comparison if these methods have reached as closer to their performance limits as possible. In this paper, we start from one pretext-task baseline, exploring how far it can go by combining it with contrastive learning, data pre-processing, and data augmentation. A proper setting has been found from extensive experiments, with which huge improvements over the baselines can be achieved, indicating a joint optimization framework can boost both pretext task and contrastive learning. We denote the joint optimization framework as Pretext-Contrastive Learning (PCL). The other two pretext task baselines are used to validate the effectiveness of PCL. And we can easily outperform current state-of-the-art methods in the same training manner, showing the effectiveness and the generality of our proposal. It is convenient to treat PCL as a standard training strategy and apply it to many other works in self-supervised video feature learning.



### Beyond cross-entropy: learning highly separable feature distributions for robust and accurate classification
- **Arxiv ID**: http://arxiv.org/abs/2010.15487v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15487v1)
- **Published**: 2020-10-29 11:15:17+00:00
- **Updated**: 2020-10-29 11:15:17+00:00
- **Authors**: Arslan Ali, Andrea Migliorati, Tiziano Bianchi, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown outstanding performance in several applications including image classification. However, deep classifiers are known to be highly vulnerable to adversarial attacks, in that a minor perturbation of the input can easily lead to an error. Providing robustness to adversarial attacks is a very challenging task especially in problems involving a large number of classes, as it typically comes at the expense of an accuracy decrease. In this work, we propose the Gaussian class-conditional simplex (GCCS) loss: a novel approach for training deep robust multiclass classifiers that provides adversarial robustness while at the same time achieving or even surpassing the classification accuracy of state-of-the-art methods. Differently from other frameworks, the proposed method learns a mapping of the input classes onto target distributions in a latent space such that the classes are linearly separable. Instead of maximizing the likelihood of target labels for individual samples, our objective function pushes the network to produce feature distributions yielding high inter-class separation. The mean values of the distributions are centered on the vertices of a simplex such that each class is at the same distance from every other class. We show that the regularization of the latent space based on our approach yields excellent classification accuracy and inherently provides robustness to multiple adversarial attacks, both targeted and untargeted, outperforming state-of-the-art approaches over challenging datasets.



### A Novel Fast 3D Single Image Super-Resolution Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2010.15491v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15491v1)
- **Published**: 2020-10-29 11:23:28+00:00
- **Updated**: 2020-10-29 11:23:28+00:00
- **Authors**: Nwigbo Kenule Tuador, Duong Hung Pham, Jérôme Michetti, Adrian Basarab, Denis Kouamé
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: This paper introduces a novel computationally efficient method of solving the 3D single image super-resolution (SR) problem, i.e., reconstruction of a high-resolution volume from its low-resolution counterpart. The main contribution lies in the original way of handling simultaneously the associated decimation and blurring operators, based on their underlying properties in the frequency domain. In particular, the proposed decomposition technique of the 3D decimation operator allows a straightforward implementation for Tikhonov regularization, and can be further used to take into consideration other regularization functions such as the total variation, enabling the computational cost of state-of-the-art algorithms to be considerably decreased. Numerical experiments carried out showed that the proposed approach outperforms existing 3D SR methods.



### CNN based Multistage Gated Average Fusion (MGAF) for Human Action Recognition Using Depth and Inertial Sensors
- **Arxiv ID**: http://arxiv.org/abs/2010.16073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16073v1)
- **Published**: 2020-10-29 11:49:13+00:00
- **Updated**: 2020-10-29 11:49:13+00:00
- **Authors**: Zeeshan Ahmad, Naimul khan
- **Comment**: arXiv admin note: text overlap with arXiv:1910.11482
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) provides leverage to extract and fuse features from all layers of its architecture. However, extracting and fusing intermediate features from different layers of CNN structure is still uninvestigated for Human Action Recognition (HAR) using depth and inertial sensors. To get maximum benefit of accessing all the CNN's layers, in this paper, we propose novel Multistage Gated Average Fusion (MGAF) network which extracts and fuses features from all layers of CNN using our novel and computationally efficient Gated Average Fusion (GAF) network, a decisive integral element of MGAF. At the input of the proposed MGAF, we transform the depth and inertial sensor data into depth images called sequential front view images (SFI) and signal images (SI) respectively. These SFI are formed from the front view information generated by depth data. CNN is employed to extract feature maps from both input modalities. GAF network fuses the extracted features effectively while preserving the dimensionality of fused feature as well. The proposed MGAF network has structural extensibility and can be unfolded to more than two modalities. Experiments on three publicly available multimodal HAR datasets demonstrate that the proposed MGAF outperforms the previous state of the art fusion methods for depth-inertial HAR in terms of recognition accuracy while being computationally much more efficient. We increase the accuracy by an average of 1.5 percent while reducing the computational cost by approximately 50 percent over the previous state of the art.



### Dynamic Resource-aware Corner Detection for Bio-inspired Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2010.15507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15507v1)
- **Published**: 2020-10-29 12:01:33+00:00
- **Updated**: 2020-10-29 12:01:33+00:00
- **Authors**: Sherif A. S. Mohamed, Jawad N. Yasin, Mohammad-hashem Haghbayan, Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, Juha Plosila
- **Comment**: Accepted to 25th international conference on pattern recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: Event-based cameras are vision devices that transmit only brightness changes with low latency and ultra-low power consumption. Such characteristics make event-based cameras attractive in the field of localization and object tracking in resource-constrained systems. Since the number of generated events in such cameras is huge, the selection and filtering of the incoming events are beneficial from both increasing the accuracy of the features and reducing the computational load. In this paper, we present an algorithm to detect asynchronous corners from a stream of events in real-time on embedded systems. The algorithm is called the Three Layer Filtering-Harris or TLF-Harris algorithm. The algorithm is based on an events' filtering strategy whose purpose is 1) to increase the accuracy by deliberately eliminating some incoming events, i.e., noise, and 2) to improve the real-time performance of the system, i.e., preserving a constant throughput in terms of input events per second, by discarding unnecessary events with a limited accuracy loss. An approximation of the Harris algorithm, in turn, is used to exploit its high-quality detection capability with a low-complexity implementation to enable seamless real-time performance on embedded computing platforms. The proposed algorithm is capable of selecting the best corner candidate among neighbors and achieves an average execution time savings of 59 % compared with the conventional Harris score. Moreover, our approach outperforms the competing methods, such as eFAST, eHarris, and FA-Harris, in terms of real-time performance, and surpasses Arc* in terms of accuracy.



### Night vision obstacle detection and avoidance based on Bio-Inspired Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2010.15509v1
- **DOI**: 10.1109/SENSORS47125.2020.9278914
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.15509v1)
- **Published**: 2020-10-29 12:02:02+00:00
- **Updated**: 2020-10-29 12:02:02+00:00
- **Authors**: Jawad N. Yasin, Sherif A. S. Mohamed, Mohammad-hashem Haghbayan, Jukka Heikkonen, Hannu Tenhunen, Muhammad Mehboob Yasin, Juha Plosila
- **Comment**: Accepted to IEEE SENSORS 2020
- **Journal**: None
- **Summary**: Moving towards autonomy, unmanned vehicles rely heavily on state-of-the-art collision avoidance systems (CAS). However, the detection of obstacles especially during night-time is still a challenging task since the lighting conditions are not sufficient for traditional cameras to function properly. Therefore, we exploit the powerful attributes of event-based cameras to perform obstacle detection in low lighting conditions. Event cameras trigger events asynchronously at high output temporal rate with high dynamic range of up to 120 $dB$. The algorithm filters background activity noise and extracts objects using robust Hough transform technique. The depth of each detected object is computed by triangulating 2D features extracted utilising LC-Harris. Finally, asynchronous adaptive collision avoidance (AACA) algorithm is applied for effective avoidance. Qualitative evaluation is compared using event-camera and traditional camera.



### Asynchronous Corner Tracking Algorithm based on Lifetime of Events for DAVIS Cameras
- **Arxiv ID**: http://arxiv.org/abs/2010.15510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15510v1)
- **Published**: 2020-10-29 12:02:40+00:00
- **Updated**: 2020-10-29 12:02:40+00:00
- **Authors**: Sherif A. S. Mohamed, Jawad N. Yasin, Mohammad-Hashem Haghbayan, Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, Juha Plosila
- **Comment**: Accepted to 15th International Symposium on Visual Computing
  (ISVC2020)
- **Journal**: None
- **Summary**: Event cameras, i.e., the Dynamic and Active-pixel Vision Sensor (DAVIS) ones, capture the intensity changes in the scene and generates a stream of events in an asynchronous fashion. The output rate of such cameras can reach up to 10 million events per second in high dynamic environments. DAVIS cameras use novel vision sensors that mimic human eyes. Their attractive attributes, such as high output rate, High Dynamic Range (HDR), and high pixel bandwidth, make them an ideal solution for applications that require high-frequency tracking. Moreover, applications that operate in challenging lighting scenarios can exploit the high HDR of event cameras, i.e., 140 dB compared to 60 dB of traditional cameras. In this paper, a novel asynchronous corner tracking method is proposed that uses both events and intensity images captured by a DAVIS camera. The Harris algorithm is used to extract features, i.e., frame-corners from keyframes, i.e., intensity images. Afterward, a matching algorithm is used to extract event-corners from the stream of events. Events are solely used to perform asynchronous tracking until the next keyframe is captured. Neighboring events, within a window size of 5x5 pixels around the event-corner, are used to calculate the velocity and direction of extracted event-corners by fitting the 2D planar using a randomized Hough transform algorithm. Experimental evaluation showed that our approach is able to update the location of the extracted corners up to 100 times during the blind time of traditional cameras, i.e., between two consecutive intensity images.



### An automatic multi-tissue human fetal brain segmentation benchmark using the Fetal Tissue Annotation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.15526v4
- **DOI**: 10.1038/s41597-021-00946-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15526v4)
- **Published**: 2020-10-29 12:46:05+00:00
- **Updated**: 2021-07-07 12:17:26+00:00
- **Authors**: Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes C. Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke, Patrice Grehten, Hui Ji, Levente Lanczi, Marianna Nagy, Monika Beresova, Thi Dao Nguyen, Giancarlo Natalucci, Theofanis Karayannis, Bjoern Menze, Meritxell Bach Cuadra, Andras Jakab
- **Comment**: This is a preprint of an article published in Nature Scientific Data.
  The final authenticated version is available online at:
  https://doi.org/10.1038/s41597-021-00946-3
- **Journal**: Sci Data 8, 167 (2021)
- **Summary**: It is critical to quantitatively analyse the developing human fetal brain in order to fully understand neurodevelopment in both normal fetuses and those with congenital disorders. To facilitate this analysis, automatic multi-tissue fetal brain segmentation algorithms are needed, which in turn requires open databases of segmented fetal brains. Here we introduce a publicly available database of 50 manually segmented pathological and non-pathological fetal magnetic resonance brain volume reconstructions across a range of gestational ages (20 to 33 weeks) into 7 different tissue categories (external cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep grey matter, brainstem/spinal cord). In addition, we quantitatively evaluate the accuracy of several automatic multi-tissue segmentation algorithms of the developing human fetal brain. Four research groups participated, submitting a total of 10 algorithms, demonstrating the benefits the database for the development of automatic algorithms.



### An End to End Network Architecture for Fundamental Matrix Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.15528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15528v1)
- **Published**: 2020-10-29 12:48:43+00:00
- **Updated**: 2020-10-29 12:48:43+00:00
- **Authors**: Yesheng Zhang, Xu Zhao, Dahong Qian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel end-to-end network architecture to estimate fundamental matrix directly from stereo images. To establish a complete working pipeline, different deep neural networks in charge of finding correspondences in images, performing outlier rejection and calculating fundamental matrix, are integrated into an end-to-end network architecture.   To well train the network and preserve geometry properties of fundamental matrix, a new loss function is introduced. To evaluate the accuracy of estimated fundamental matrix more reasonably, we design a new evaluation metric which is highly consistent with visualization result. Experiments conducted on both outdoor and indoor data-sets show that this network outperforms traditional methods as well as previous deep learning based methods on various metrics and achieves significant performance improvements.



### Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel Segmentation Using a Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2010.15560v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15560v4)
- **Published**: 2020-10-29 13:31:36+00:00
- **Updated**: 2021-06-11 09:58:57+00:00
- **Authors**: Jiahong Wei, Zhun Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many methods based on hand-designed convolutional neural networks (CNNs) have achieved promising results in automatic retinal vessel segmentation. However, these CNNs remain constrained in capturing retinal vessels in complex fundus images. To improve their segmentation performance, these CNNs tend to have many parameters, which may lead to overfitting and high computational complexity. Moreover, the manual design of competitive CNNs is time-consuming and requires extensive empirical knowledge. Herein, a novel automated design method, called Genetic U-Net, is proposed to generate a U-shaped CNN that can achieve better retinal vessel segmentation but with fewer architecture-based parameters, thereby addressing the above issues. First, we devised a condensed but flexible search space based on a U-shaped encoder-decoder. Then, we used an improved genetic algorithm to identify better-performing architectures in the search space and investigated the possibility of finding a superior network architecture with fewer parameters. The experimental results show that the architecture obtained using the proposed method offered a superior performance with less than 1% of the number of the original U-Net parameters in particular and with significantly fewer parameters than other state-of-the-art models. Furthermore, through in-depth investigation of the experimental results, several effective operations and patterns of networks to generate superior retinal vessel segmentations were identified.



### Suppressing Mislabeled Data via Grouping and Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.15603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15603v1)
- **Published**: 2020-10-29 13:54:16+00:00
- **Updated**: 2020-10-29 13:54:16+00:00
- **Authors**: Xiaojiang Peng, Kai Wang, Zhaoyang Zeng, Qing Li, Jianfei Yang, Yu Qiao
- **Comment**: This is a novel noisy-robust learning method
- **Journal**: None
- **Summary**: Deep networks achieve excellent results on large-scale clean data but degrade significantly when learning from noisy labels. To suppressing the impact of mislabeled data, this paper proposes a conceptually simple yet efficient training block, termed as Attentive Feature Mixup (AFM), which allows paying more attention to clean samples and less to mislabeled ones via sample interactions in small groups. Specifically, this plug-and-play AFM first leverages a \textit{group-to-attend} module to construct groups and assign attention weights for group-wise samples, and then uses a \textit{mixup} module with the attention weights to interpolate massive noisy-suppressed samples. The AFM has several appealing benefits for noise-robust deep learning. (i) It does not rely on any assumptions and extra clean subset. (ii) With massive interpolations, the ratio of useless samples is reduced dramatically compared to the original noisy ratio. (iii) \pxj{It jointly optimizes the interpolation weights with classifiers, suppressing the influence of mislabeled data via low attention weights. (iv) It partially inherits the vicinal risk minimization of mixup to alleviate over-fitting while improves it by sampling fewer feature-target vectors around mislabeled data from the mixup vicinal distribution.} Extensive experiments demonstrate that AFM yields state-of-the-art results on two challenging real-world noisy datasets: Food101N and Clothing1M. The code will be available at https://github.com/kaiwang960112/AFM.



### An Overview Of 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.15614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15614v1)
- **Published**: 2020-10-29 14:04:50+00:00
- **Updated**: 2020-10-29 14:04:50+00:00
- **Authors**: Yilin Wang, Jiayi Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud 3D object detection has recently received major attention and becomes an active research topic in 3D computer vision community. However, recognizing 3D objects in LiDAR (Light Detection and Ranging) is still a challenge due to the complexity of point clouds. Objects such as pedestrians, cyclists, or traffic cones are usually represented by quite sparse points, which makes the detection quite complex using only point cloud. In this project, we propose a framework that uses both RGB and point cloud data to perform multiclass object recognition. We use existing 2D detection models to localize the region of interest (ROI) on the RGB image, followed by a pixel mapping strategy in the point cloud, and finally, lift the initial 2D bounding box to 3D space. We use the recently released nuScenes dataset---a large-scale dataset contains many data formats---to training and evaluate our proposed architecture.



### Free-Form Image Inpainting via Contrastive Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2010.15643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15643v1)
- **Published**: 2020-10-29 14:46:05+00:00
- **Updated**: 2020-10-29 14:46:05+00:00
- **Authors**: Xin Ma, Xiaoqiang Zhou, Huaibo Huang, Zhenhua Chai, Xiaolin Wei, Ran He
- **Comment**: Accepted by ICPR 2020
- **Journal**: None
- **Summary**: Most deep learning based image inpainting approaches adopt autoencoder or its variants to fill missing regions in images. Encoders are usually utilized to learn powerful representational spaces, which are important for dealing with sophisticated learning tasks. Specifically, in image inpainting tasks, masks with any shapes can appear anywhere in images (i.e., free-form masks) which form complex patterns. It is difficult for encoders to capture such powerful representations under this complex situation. To tackle this problem, we propose a self-supervised Siamese inference network to improve the robustness and generalization. It can encode contextual semantics from full resolution images and obtain more discriminative representations. we further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine both the restored and known regions in a smooth way. This multi-scale architecture is beneficial for decoding discriminative representations learned by encoders into images layer by layer. In this way, unknown regions will be filled naturally from outside to inside. Qualitative and quantitative experiments on multiple datasets, including facial and natural datasets (i.e., Celeb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our proposed method outperforms state-of-the-art methods in generating high-quality inpainting results.



### Brain Tumor Segmentation Network Using Attention-based Fusion and Spatial Relationship Constraint
- **Arxiv ID**: http://arxiv.org/abs/2010.15647v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15647v2)
- **Published**: 2020-10-29 14:51:10+00:00
- **Updated**: 2020-10-31 07:34:53+00:00
- **Authors**: Chenyu Liu, Wangbin Ding, Lei Li, Zhen Zhang, Chenhao Pei, Liqin Huang, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Delineating the brain tumor from magnetic resonance (MR) images is critical for the treatment of gliomas. However, automatic delineation is challenging due to the complex appearance and ambiguous outlines of tumors. Considering that multi-modal MR images can reflect different tumor biological properties, we develop a novel multi-modal tumor segmentation network (MMTSN) to robustly segment brain tumors based on multi-modal MR images. The MMTSN is composed of three sub-branches and a main branch. Specifically, the sub-branches are used to capture different tumor features from multi-modal images, while in the main branch, we design a spatial-channel fusion block (SCFB) to effectively aggregate multi-modal features. Additionally, inspired by the fact that the spatial relationship between sub-regions of tumor is relatively fixed, e.g., the enhancing tumor is always in the tumor core, we propose a spatial loss to constrain the relationship between different sub-regions of tumor. We evaluate our method on the test set of multi-modal brain tumor segmentation challenge 2020 (BraTs2020). The method achieves 0.8764, 0.8243 and 0.773 dice score for whole tumor, tumor core and enhancing tumor, respectively.



### Maximum a posteriori signal recovery for optical coherence tomography angiography image generation and denoising
- **Arxiv ID**: http://arxiv.org/abs/2010.15682v1
- **DOI**: 10.1364/BOE.408903
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15682v1)
- **Published**: 2020-10-29 15:28:22+00:00
- **Updated**: 2020-10-29 15:28:22+00:00
- **Authors**: Lennart Husvogt, Stefan B. Ploner, Siyu Chen, Daniel Stromer, Julia Schottenhamml, A. Yasin Alibhai, Eric Moult, Nadia K. Waheed, James G. Fujimoto, Andreas Maier
- **Comment**: 14 pages, 4 figures, to be published in Biomedical Optics Express
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is a novel and clinically promising imaging modality to image retinal and sub-retinal vasculature. Based on repeated optical coherence tomography (OCT) scans, intensity changes are observed over time and used to compute OCTA image data. OCTA data are prone to noise and artifacts caused by variations in flow speed and patient movement. We propose a novel iterative maximum a posteriori signal recovery algorithm in order to generate OCTA volumes with reduced noise and increased image quality. This algorithm is based on previous work on probabilistic OCTA signal models and maximum likelihood estimates. Reconstruction results using total variation minimization and wavelet shrinkage for regularization were compared against an OCTA ground truth volume, merged from six co-registered single OCTA volumes. The results show a significant improvement in peak signal-to-noise ratio and structural similarity. The presented algorithm brings together OCTA image generation and Bayesian statistics and can be developed into new OCTA image generation and denoising algorithms.



### Deep Autofocus for Synthetic Aperture Sonar
- **Arxiv ID**: http://arxiv.org/abs/2010.15687v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15687v2)
- **Published**: 2020-10-29 15:31:15+00:00
- **Updated**: 2021-07-30 11:38:07+00:00
- **Authors**: Isaac Gerg, Vishal Monga
- **Comment**: superseded by another work
- **Journal**: None
- **Summary**: Synthetic aperture sonar (SAS) requires precise positional and environmental information to produce well-focused output during the image reconstruction step. However, errors in these measurements are commonly present resulting in defocused imagery. To overcome these issues, an \emph{autofocus} algorithm is employed as a post-processing step after image reconstruction for the purpose of improving image quality using the image content itself. These algorithms are usually iterative and metric-based in that they seek to optimize an image sharpness metric. In this letter, we demonstrate the potential of machine learning, specifically deep learning, to address the autofocus problem. We formulate the problem as a self-supervised, phase error estimation task using a deep network we call Deep Autofocus. Our formulation has the advantages of being non-iterative (and thus fast) and not requiring ground truth focused-defocused images pairs as often required by other deblurring deep learning methods. We compare our technique against a set of common sharpness metrics optimized using gradient descent over a real-world dataset. Our results demonstrate Deep Autofocus can produce imagery that is perceptually as good as benchmark iterative techniques but at a substantially lower computational cost. We conclude that our proposed Deep Autofocus can provide a more favorable cost-quality trade-off than state-of-the-art alternatives with significant potential of future research.



### Learning Deep Interleaved Networks with Asymmetric Co-Attention for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2010.15689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15689v1)
- **Published**: 2020-10-29 15:32:00+00:00
- **Updated**: 2020-10-29 15:32:00+00:00
- **Authors**: Feng Li, Runmin Cong, Huihui Bai, Yifan He, Yao Zhao, Ce Zhu
- **Comment**: 15pages, 15figures
- **Journal**: None
- **Summary**: Recently, convolutional neural network (CNN) has demonstrated significant success for image restoration (IR) tasks (e.g., image super-resolution, image deblurring, rain streak removal, and dehazing). However, existing CNN based models are commonly implemented as a single-path stream to enrich feature representations from low-quality (LQ) input space for final predictions, which fail to fully incorporate preceding low-level contexts into later high-level features within networks, thereby producing inferior results. In this paper, we present a deep interleaved network (DIN) that learns how information at different states should be combined for high-quality (HQ) images reconstruction. The proposed DIN follows a multi-path and multi-branch pattern allowing multiple interconnected branches to interleave and fuse at different states. In this way, the shallow information can guide deep representative features prediction to enhance the feature expression ability. Furthermore, we propose asymmetric co-attention (AsyCA) which is attached at each interleaved node to model the feature dependencies. Such AsyCA can not only adaptively emphasize the informative features from different states, but also improves the discriminative ability of networks. Our presented DIN can be trained end-to-end and applied to various IR tasks. Comprehensive evaluations on public benchmarks and real-world datasets demonstrate that the proposed DIN perform favorably against the state-of-the-art methods quantitatively and qualitatively.



### Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-fine Framework and Its Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2010.16074v1
- **DOI**: 10.1007/978-3-030-13969-8_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16074v1)
- **Published**: 2020-10-29 15:39:19+00:00
- **Updated**: 2020-10-29 15:39:19+00:00
- **Authors**: Yingwei Li, Zhuotun Zhu, Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1712.00201
- **Journal**: None
- **Summary**: Although deep neural networks have been a dominant method for many 2D vision tasks, it is still challenging to apply them to 3D tasks, such as medical image segmentation, due to the limited amount of annotated 3D data and limited computational resources. In this chapter, by rethinking the strategy to apply 3D Convolutional Neural Networks to segment medical images, we propose a novel 3D-based coarse-to-fine framework to efficiently tackle these challenges. The proposed 3D-based framework outperforms their 2D counterparts by a large margin since it can leverage the rich spatial information along all three axes. We further analyze the threat of adversarial attacks on the proposed framework and show how to defense against the attack. We conduct experiments on three datasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMI pathological cyst dataset, where the first two and the last one contain healthy and pathological pancreases respectively, and achieve the current state-of-the-art in terms of Dice-Sorensen Coefficient (DSC) on all of them. Especially, on the NIH pancreas segmentation dataset, we outperform the previous best by an average of over $2\%$, and the worst case is improved by $7\%$ to reach almost $70\%$, which indicates the reliability of our framework in clinical applications.



### Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.15703v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.15703v3)
- **Published**: 2020-10-29 15:47:26+00:00
- **Updated**: 2021-04-10 22:27:02+00:00
- **Authors**: Julieta Martinez, Jashan Shewakramani, Ting Wei Liu, Ioan Andrei Bârsan, Wenyuan Zeng, Raquel Urtasun
- **Comment**: CVPR 21 Oral
- **Journal**: None
- **Summary**: Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appealing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art network compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional filters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate modern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one filter is compressed to the same codeword). In this paper we make the observation that the weights of two adjacent layers can be permuted while expressing the same function. We then establish a connection to rate-distortion theory and search for permutations that result in networks that are easier to compress. Finally, we rely on an annealed quantization algorithm to better compress the network and achieve higher final accuracy. We show results on image classification, object detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% with respect to the current state of the art.



### Recurrent Neural Networks for video object detection
- **Arxiv ID**: http://arxiv.org/abs/2010.15740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15740v1)
- **Published**: 2020-10-29 16:40:10+00:00
- **Updated**: 2020-10-29 16:40:10+00:00
- **Authors**: Ahmad B Qasim, Arnd Pettirsch
- **Comment**: None
- **Journal**: None
- **Summary**: There is lots of scientific work about object detection in images. For many applications like for example autonomous driving the actual data on which classification has to be done are videos. This work compares different methods, especially those which use Recurrent Neural Networks to detect objects in videos. We differ between feature-based methods, which feed feature maps of different frames into the recurrent units, box-level methods, which feed bounding boxes with class probabilities into the recurrent units and methods which use flow networks. This study indicates common outcomes of the compared methods like the benefit of including the temporal context into object detection and states conclusions and guidelines for video object detection networks.



### WaveTransform: Crafting Adversarial Examples via Input Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2010.15773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2010.15773v1)
- **Published**: 2020-10-29 17:16:59+00:00
- **Updated**: 2020-10-29 17:16:59+00:00
- **Authors**: Divyam Anshumaan, Akshay Agarwal, Mayank Vatsa, Richa Singh
- **Comment**: ECCV Workshop Adversarial Robustness in the Real World 2020, 17
  pages, 3 Tables, 6 Figures
- **Journal**: None
- **Summary**: Frequency spectrum has played a significant role in learning unique and discriminating features for object recognition. Both low and high frequency information present in images have been extracted and learnt by a host of representation learning techniques, including deep learning. Inspired by this observation, we introduce a novel class of adversarial attacks, namely `WaveTransform', that creates adversarial noise corresponding to low-frequency and high-frequency subbands, separately (or in combination). The frequency subbands are analyzed using wavelet decomposition; the subbands are corrupted and then used to construct an adversarial example. Experiments are performed using multiple databases and CNN models to establish the effectiveness of the proposed WaveTransform attack and analyze the importance of a particular frequency component. The robustness of the proposed attack is also evaluated through its transferability and resiliency against a recent adversarial defense algorithm. Experiments show that the proposed attack is effective against the defense algorithm and is also transferable across CNNs.



### Understanding the Failure Modes of Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2010.15775v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.15775v2)
- **Published**: 2020-10-29 17:19:03+00:00
- **Updated**: 2021-04-29 04:08:02+00:00
- **Authors**: Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur
- **Comment**: None
- **Journal**: None
- **Summary**: Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.



### Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2010.15821v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15821v3)
- **Published**: 2020-10-29 17:55:05+00:00
- **Updated**: 2021-04-12 06:30:36+00:00
- **Authors**: Houwen Peng, Hao Du, Hongyuan Yu, Qi Li, Jing Liao, Jianlong Fu
- **Comment**: NeurIPS 2020, code url: https://github.com/microsoft/Cream
- **Journal**: None
- **Summary**: One-shot weight sharing methods have recently drawn great attention in neural architecture search due to high efficiency and competitive performance. However, weight sharing across models has an inherent deficiency, i.e., insufficient training of subnetworks in hypernetworks. To alleviate this problem, we present a simple yet effective architecture distillation method. The central idea is that subnetworks can learn collaboratively and teach each other throughout the training process, aiming to boost the convergence of individual models. We introduce the concept of prioritized path, which refers to the architecture candidates exhibiting superior performance during training. Distilling knowledge from the prioritized paths is able to boost the training of subnetworks. Since the prioritized paths are changed on the fly depending on their performance and complexity, the final obtained paths are the cream of the crop. We directly select the most promising one from the prioritized paths as the final architecture, without using other complex search methods, such as reinforcement learning or evolution algorithms. The experiments on ImageNet verify such path distillation method can improve the convergence ratio and performance of the hypernetwork, as well as boosting the training of subnetworks. The discovered architectures achieve superior performance compared to the recent MobileNetV3 and EfficientNet families under aligned settings. Moreover, the experiments on object detection and more challenging search space show the generality and robustness of the proposed method. Code and models are available at https://github.com/microsoft/cream.git.



### Black-Box Optimization of Object Detector Scales
- **Arxiv ID**: http://arxiv.org/abs/2010.15823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15823v1)
- **Published**: 2020-10-29 17:55:54+00:00
- **Updated**: 2020-10-29 17:55:54+00:00
- **Authors**: Mohandass Muthuraja, Octavio Arriaga, Paul Plöger, Frank Kirchner, Matias Valdenegro-Toro
- **Comment**: 17 pages, 7 figures, with appendix
- **Journal**: None
- **Summary**: Object detectors have improved considerably in the last years by using advanced CNN architectures. However, many detector hyper-parameters are generally manually tuned, or they are used with values set by the detector authors. Automatic Hyper-parameter optimization has not been explored in improving CNN-based object detectors hyper-parameters. In this work, we propose the use of Black-box optimization methods to tune the prior/default box scales in Faster R-CNN and SSD, using Bayesian Optimization, SMAC, and CMA-ES. We show that by tuning the input image size and prior box anchor scale on Faster R-CNN mAP increases by 2% on PASCAL VOC 2007, and by 3% with SSD. On the COCO dataset with SSD there are mAP improvement in the medium and large objects, but mAP decreases by 1% in small objects. We also perform a regression analysis to find the significant hyper-parameters to tune.



### Passport-aware Normalization for Deep Model Protection
- **Arxiv ID**: http://arxiv.org/abs/2010.15824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15824v2)
- **Published**: 2020-10-29 17:57:12+00:00
- **Updated**: 2020-11-03 16:19:59+00:00
- **Authors**: Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu
- **Comment**: Fix typo error. To Appear in NeurIPs 2020, code at
  "https://github.com/ZJZAC/Passport-aware-Normalization"
- **Journal**: None
- **Summary**: Despite tremendous success in many application scenarios, deep learning faces serious intellectual property (IP) infringement threats. Considering the cost of designing and training a good model, infringements will significantly infringe the interests of the original model owner. Recently, many impressive works have emerged for deep model IP protection. However, they either are vulnerable to ambiguity attacks, or require changes in the target network structure by replacing its original normalization layers and hence cause significant performance drops. To this end, we propose a new passport-aware normalization formulation, which is generally applicable to most existing normalization layers and only needs to add another passport-aware branch for IP protection. This new branch is jointly trained with the target model but discarded in the inference stage. Therefore it causes no structure change in the target model. Only when the model IP is suspected to be stolen by someone, the private passport-aware branch is added back for ownership verification. Through extensive experiments, we verify its effectiveness in both image and 3D point recognition models. It is demonstrated to be robust not only to common attack techniques like fine-tuning and model compression, but also to ambiguity attacks. By further combining it with trigger-set based methods, both black-box and white-box verification can be achieved for enhanced security of deep learning models deployed in real systems. Code can be found at https://github.com/ZJZAC/Passport-aware-Normalization.



### RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder
- **Arxiv ID**: http://arxiv.org/abs/2010.15831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15831v1)
- **Published**: 2020-10-29 17:59:45+00:00
- **Updated**: 2020-10-29 17:59:45+00:00
- **Authors**: Cheng Chi, Fangyun Wei, Han Hu
- **Comment**: NeurIPS2020 Spotlight
- **Journal**: None
- **Summary**: Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer~\cite{vaswani2017attention} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of \emph{key} instances to strengthen the main \emph{query} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a \emph{key sampling} approach and a \emph{shared location embedding} approach. The proposed module is named \emph{bridging visual representations} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about $1.5\sim3.0$ AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about $2.0$ AP, reaching $52.7$ AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at https://github.com/microsoft/RelationNet2.



### Ink Marker Segmentation in Histopathology Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15865v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15865v1)
- **Published**: 2020-10-29 18:09:59+00:00
- **Updated**: 2020-10-29 18:09:59+00:00
- **Authors**: Danial Maleki, Mehdi Afshari, Morteza Babaie, H. R. Tizhoosh
- **Comment**: Accepted for publication in the 15th International Symposium on
  Visual Computing (ISVC 2020)
- **Journal**: None
- **Summary**: Due to the recent advancements in machine vision, digital pathology has gained significant attention. Histopathology images are distinctly rich in visual information. The tissue glass slide images are utilized for disease diagnosis. Researchers study many methods to process histopathology images and facilitate fast and reliable diagnosis; therefore, the availability of high-quality slides becomes paramount. The quality of the images can be negatively affected when the glass slides are ink-marked by pathologists to delineate regions of interest. As an example, in one of the largest public histopathology datasets, The Cancer Genome Atlas (TCGA), approximately $12\%$ of the digitized slides are affected by manual delineations through ink markings. To process these open-access slide images and other repositories for the design and validation of new methods, an algorithm to detect the marked regions of the images is essential to avoid confusing tissue pixels with ink-colored pixels for computer methods. In this study, we propose to segment the ink-marked areas of pathology patches through a deep network. A dataset from $79$ whole slide images with $4,305$ patches was created and different networks were trained. Finally, the results showed an FPN model with the EffiecentNet-B3 as the backbone was found to be the superior configuration with an F1 score of $94.53\%$.



### Perception Matters: Exploring Imperceptible and Transferable Anti-forensics for GAN-generated Fake Face Imagery Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.15886v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15886v1)
- **Published**: 2020-10-29 18:54:06+00:00
- **Updated**: 2020-10-29 18:54:06+00:00
- **Authors**: Yongwei Wang, Xin Ding, Li Ding, Rabab Ward, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, generative adversarial networks (GANs) can generate photo-realistic fake facial images which are perceptually indistinguishable from real face photos, promoting research on fake face detection. Though fake face forensics can achieve high detection accuracy, their anti-forensic counterparts are less investigated. Here we explore more \textit{imperceptible} and \textit{transferable} anti-forensics for fake face imagery detection based on adversarial attacks. Since facial and background regions are often smooth, even small perturbation could cause noticeable perceptual impairment in fake face images. Therefore it makes existing adversarial attacks ineffective as an anti-forensic method. Our perturbation analysis reveals the intuitive reason of the perceptual degradation issue when directly applying existing attacks. We then propose a novel adversarial attack method, better suitable for image anti-forensics, in the transformed color domain by considering visual perception. Simple yet effective, the proposed method can fool both deep learning and non-deep learning based forensic detectors, achieving higher attack success rate and significantly improved visual quality. Specially, when adversaries consider imperceptibility as a constraint, the proposed anti-forensic method can improve the average attack success rate by around 30\% on fake face images over two baseline attacks. \textit{More imperceptible} and \textit{more transferable}, the proposed method raises new security concerns to fake face imagery detection. We have released our code for public use, and hopefully the proposed method can be further explored in related forensic applications as an anti-forensic benchmark.



### Multi-agent Trajectory Prediction with Fuzzy Query Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.15891v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15891v1)
- **Published**: 2020-10-29 19:12:12+00:00
- **Updated**: 2020-10-29 19:12:12+00:00
- **Authors**: Nitin Kamra, Hao Zhu, Dweep Trivedi, Ming Zhang, Yan Liu
- **Comment**: NeurIPS 2020 Camera-ready version. Code:
  https://github.com/nitinkamra1992/FQA
- **Journal**: None
- **Summary**: Trajectory prediction for scenes with multiple agents and entities is a challenging problem in numerous domains such as traffic prediction, pedestrian tracking and path planning. We present a general architecture to address this challenge which models the crucial inductive biases of motion, namely, inertia, relative motion, intents and interactions. Specifically, we propose a relational model to flexibly model interactions between agents in diverse environments. Since it is well-known that human decision making is fuzzy by nature, at the core of our model lies a novel attention mechanism which models interactions by making continuous-valued (fuzzy) decisions and learning the corresponding responses. Our architecture demonstrates significant performance gains over existing state-of-the-art predictive models in diverse domains such as human crowd trajectories, US freeway traffic, NBA sports data and physics datasets. We also present ablations and augmentations to understand the decision-making process and the source of gains in our model.



### A Comprehensive Comparison of End-to-End Approaches for Handwritten Digit String Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.15904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.15904v1)
- **Published**: 2020-10-29 19:38:08+00:00
- **Updated**: 2020-10-29 19:38:08+00:00
- **Authors**: Andre G. Hochuli, Alceu S. Britto Jr, David A. Saji, Jose M. Saavedra, Robert Sabourin, Luiz S. Oliveira
- **Comment**: None
- **Journal**: Expert System with Applications (2020)
- **Summary**: Over the last decades, most approaches proposed for handwritten digit string recognition (HDSR) have resorted to digit segmentation, which is dominated by heuristics, thereby imposing substantial constraints on the final performance. Few of them have been based on segmentation-free strategies where each pixel column has a potential cut location. Recently, segmentation-free strategies has added another perspective to the problem, leading to promising results. However, these strategies still show some limitations when dealing with a large number of touching digits. To bridge the resulting gap, in this paper, we hypothesize that a string of digits can be approached as a sequence of objects. We thus evaluate different end-to-end approaches to solve the HDSR problem, particularly in two verticals: those based on object-detection (e.g., Yolo and RetinaNet) and those based on sequence-to-sequence representation (CRNN). The main contribution of this work lies in its provision of a comprehensive comparison with a critical analysis of the above mentioned strategies on five benchmarks commonly used to assess HDSR, including the challenging Touching Pair dataset, NIST SD19, and two real-world datasets (CAR and CVL) proposed for the ICFHR 2014 competition on HDSR. Our results show that the Yolo model compares favorably against segmentation-free models with the advantage of having a shorter pipeline that minimizes the presence of heuristics-based models. It achieved a 97%, 96%, and 84% recognition rate on the NIST-SD19, CAR, and CVL datasets, respectively.



### Detecting small polyps using a Dynamic SSD-GAN
- **Arxiv ID**: http://arxiv.org/abs/2010.15937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15937v1)
- **Published**: 2020-10-29 20:54:34+00:00
- **Updated**: 2020-10-29 20:54:34+00:00
- **Authors**: Daniel C. Ohrenstein, Patrick Brandao, Daniel Toth, Laurence Lovat, Danail Stoyanov, Peter Mountney
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: Endoscopic examinations are used to inspect the throat, stomach and bowel for polyps which could develop into cancer. Machine learning systems can be trained to process colonoscopy images and detect polyps. However, these systems tend to perform poorly on objects which appear visually small in the images. It is shown here that combining the single-shot detector as a region proposal network with an adversarially-trained generator to upsample small region proposals can significantly improve the detection of visually-small polyps. The Dynamic SSD-GAN pipeline introduced in this paper achieved a 12% increase in sensitivity on visually-small polyps compared to a conventional FCN baseline.



### Machine versus Human Attention in Deep Reinforcement Learning Tasks
- **Arxiv ID**: http://arxiv.org/abs/2010.15942v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.15942v3)
- **Published**: 2020-10-29 20:58:45+00:00
- **Updated**: 2021-11-02 23:01:39+00:00
- **Authors**: Sihang Guo, Ruohan Zhang, Bo Liu, Yifeng Zhu, Mary Hayhoe, Dana Ballard, Peter Stone
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning (RL) algorithms are powerful tools for solving visuomotor decision tasks. However, the trained models are often difficult to interpret, because they are represented as end-to-end deep neural networks. In this paper, we shed light on the inner workings of such trained models by analyzing the pixels that they attend to during task execution, and comparing them with the pixels attended to by humans executing the same tasks. To this end, we investigate the following two questions that, to the best of our knowledge, have not been previously studied. 1) How similar are the visual representations learned by RL agents and humans when performing the same task? and, 2) How do similarities and differences in these learned representations explain RL agents' performance on these tasks? Specifically, we compare the saliency maps of RL agents against visual attention models of human experts when learning to play Atari games. Further, we analyze how hyperparameters of the deep RL algorithm affect the learned representations and saliency maps of the trained agents. The insights provided have the potential to inform novel algorithms for closing the performance gap between human experts and RL agents.



### PAL : Pretext-based Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15947v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15947v3)
- **Published**: 2020-10-29 21:16:37+00:00
- **Updated**: 2021-03-28 21:04:37+00:00
- **Authors**: Shubhang Bhatnagar, Sachin Goyal, Darshan Tank, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of pool-based active learning is to judiciously select a fixed-sized subset of unlabeled samples from a pool to query an oracle for their labels, in order to maximize the accuracy of a supervised learner. However, the unsaid requirement that the oracle should always assign correct labels is unreasonable for most situations. We propose an active learning technique for deep neural networks that is more robust to mislabeling than the previously proposed techniques. Previous techniques rely on the task network itself to estimate the novelty of the unlabeled samples, but learning the task (generalization) and selecting samples (out-of-distribution detection) can be conflicting goals. We use a separate network to score the unlabeled samples for selection. The scoring network relies on self-supervision for modeling the distribution of the labeled samples to reduce the dependency on potentially noisy labels. To counter the paucity of data, we also deploy another head on the scoring network for regularization via multi-task learning and use an unusual self-balancing hybrid scoring function. Furthermore, we divide each query into sub-queries before labeling to ensure that the query has diverse samples. In addition to having a higher tolerance to mislabeling of samples by the oracle, the resultant technique also produces competitive accuracy in the absence of label noise. The technique also handles the introduction of new classes on-the-fly well by temporarily increasing the sampling rate of these classes.



### Can the state of relevant neurons in a deep neural networks serve as indicators for detecting adversarial attacks?
- **Arxiv ID**: http://arxiv.org/abs/2010.15974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15974v1)
- **Published**: 2020-10-29 22:31:42+00:00
- **Updated**: 2020-10-29 22:31:42+00:00
- **Authors**: Roger Granda, Tinne Tuytelaars, Jose Oramas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for adversarial attack detection based on the inspection of a sparse set of neurons. We follow the hypothesis that adversarial attacks introduce imperceptible perturbations in the input and that these perturbations change the state of neurons relevant for the concepts modelled by the attacked model. Therefore, monitoring the status of these neurons would enable the detection of adversarial attacks. Focusing on the image classification task, our method identifies neurons that are relevant for the classes predicted by the model. A deeper qualitative inspection of these sparse set of neurons indicates that their state changes in the presence of adversarial samples. Moreover, quantitative results from our empirical evaluation indicate that our method is capable of recognizing adversarial samples, produced by state-of-the-art attack methods, with comparable accuracy to that of state-of-the-art detectors.



### AutoAtlas: Neural Network for 3D Unsupervised Partitioning and Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.15987v3
- **DOI**: 10.1109/JBHI.2021.3124733
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.15987v3)
- **Published**: 2020-10-29 23:07:31+00:00
- **Updated**: 2021-11-11 14:38:55+00:00
- **Authors**: K. Aditya Mohan, Alan D. Kaplan
- **Comment**: IEEE Journal of Biomedical and Health Informatics
- **Journal**: None
- **Summary**: We present a novel neural network architecture called AutoAtlas for fully unsupervised partitioning and representation learning of 3D brain Magnetic Resonance Imaging (MRI) volumes. AutoAtlas consists of two neural network components: one neural network to perform multi-label partitioning based on local texture in the volume, and a second neural network to compress the information contained within each partition. We train both of these components simultaneously by optimizing a loss function that is designed to promote accurate reconstruction of each partition, while encouraging spatially smooth and contiguous partitioning, and discouraging relatively small partitions. We show that the partitions adapt to the subject specific structural variations of brain tissue while consistently appearing at similar spatial locations across subjects. AutoAtlas also produces very low dimensional features that represent local texture of each partition. We demonstrate prediction of metadata associated with each subject using the derived feature representations and compare the results to prediction using features derived from FreeSurfer anatomical parcellation. Since our features are intrinsically linked to distinct partitions, we can then map values of interest, such as partition-specific feature importance scores onto the brain for visualization.



