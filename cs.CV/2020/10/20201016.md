# Arxiv Papers in cs.CV on 2020-10-16
### Performance evaluation and application of computation based low-cost homogeneous machine learning model algorithm for image classification
- **Arxiv ID**: http://arxiv.org/abs/2010.08087v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08087v1)
- **Published**: 2020-10-16 01:05:49+00:00
- **Updated**: 2020-10-16 01:05:49+00:00
- **Authors**: W. H. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The image classification machine learning model was trained with the intention to predict the category of the input image. While multiple state-of-the-art ensemble model methodologies are openly available, this paper evaluates the performance of a low-cost, simple algorithm that would integrate seamlessly into modern production-grade cloud-based applications. The homogeneous models, trained with the full instead of subsets of data, contains varying hyper-parameters and neural layers from one another. These models' inferences will be processed by the new algorithm, which is loosely based on conditional probability theories. The final output will be evaluated.



### Human Segmentation with Dynamic LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2010.08092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08092v1)
- **Published**: 2020-10-16 01:26:35+00:00
- **Updated**: 2020-10-16 01:26:35+00:00
- **Authors**: Tao Zhong, Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi
- **Comment**: None
- **Journal**: None
- **Summary**: Consecutive LiDAR scans compose dynamic 3D sequences, which contain more abundant information than a single frame. Similar to the development history of image and video perception, dynamic 3D sequence perception starts to come into sight after inspiring research on static 3D data perception. This work proposes a spatio-temporal neural network for human segmentation with the dynamic LiDAR point clouds. It takes a sequence of depth images as input. It has a two-branch structure, i.e., the spatial segmentation branch and the temporal velocity estimation branch. The velocity estimation branch is designed to capture motion cues from the input sequence and then propagates them to the other branch. So that the segmentation branch segments humans according to both spatial and temporal features. These two branches are jointly learned on a generated dynamic point cloud dataset for human recognition. Our works fill in the blank of dynamic point cloud perception with the spherical representation of point cloud and achieves high accuracy. The experiments indicate that the introduction of temporal feature benefits the segmentation of dynamic point cloud.



### Physics-informed GANs for Coastal Flood Visualization
- **Arxiv ID**: http://arxiv.org/abs/2010.08103v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08103v2)
- **Published**: 2020-10-16 02:15:34+00:00
- **Updated**: 2021-02-12 06:26:46+00:00
- **Authors**: Björn Lütjens, Brandon Leshchinskiy, Christian Requena-Mesa, Farrukh Chishtie, Natalia Díaz-Rodriguez, Océane Boulais, Aaron Piña, Dava Newman, Alexander Lavin, Yarin Gal, Chedy Raïssi
- **Comment**: Under Review
- **Journal**: None
- **Summary**: As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, but during hurricanes the area is largely covered by clouds and emergency managers must rely on nonintuitive flood visualizations for mission planning. To assist these emergency managers, we have created a deep learning pipeline that generates visual satellite images of current and future coastal flooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. While this work focused on the visualization of coastal floods, we envision the creation of a global visualization of how climate change will shape our earth.



### Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2010.08115v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08115v2)
- **Published**: 2020-10-16 02:34:15+00:00
- **Updated**: 2021-06-05 06:32:07+00:00
- **Authors**: Sanjay Kumar Sonbhadra, Sonali Agarwal, P. Nagabhushan
- **Comment**: None
- **Journal**: None
- **Summary**: The infection of respiratory coronavirus disease 2019 (COVID-19) starts with the upper respiratory tract and as the virus grows, the infection can progress to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is reverse transcription polymerase chain reaction (RT-PCR), which is less sensitive during early stages; especially if the patient is asymptomatic, which may further cause more severe pneumonia. In this context, several deep learning models have been proposed to identify pulmonary infections using publicly available chest X-ray (CXR) image datasets for early diagnosis, better treatment and quick cure. In these datasets, presence of less number of COVID-19 positive samples compared to other classes (normal, pneumonia and Tuberculosis) raises the challenge for unbiased learning of deep learning models. All deep learning models opted class balancing techniques to solve this issue; which however should be avoided in any medical diagnosis process. Moreover, the deep learning models are also data hungry and need massive computation resources. Therefore for quicker diagnosis, this research proposes a novel pinball loss function based one-class support vector machine (PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples with objectives to maximize the learning efficiency and to minimize the false predictions. The performance of the proposed model is compared with conventional OCSVM and existing deep learning models, and the experimental results prove that the proposed model outperformed over state-of-the-art methods. To validate the robustness of the proposed model, experiments are also performed with noisy CXR images and UCI benchmark datasets.



### Learning Panoptic Segmentation from Instance Contours
- **Arxiv ID**: http://arxiv.org/abs/2010.11681v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.11681v2)
- **Published**: 2020-10-16 03:05:48+00:00
- **Updated**: 2021-04-06 01:09:26+00:00
- **Authors**: Sumanth Chennupati, Venkatraman Narayanan, Ganesh Sistu, Senthil Yogamani, Samir A Rawashdeh
- **Comment**: Accepted at ICRA 2021. Overview Video: https://youtu.be/wBtcxRhG3e0
- **Journal**: None
- **Summary**: Panoptic Segmentation aims to provide an understanding of background (stuff) and instances of objects (things) at a pixel level. It combines the separate tasks of semantic segmentation (pixel level classification) and instance segmentation to build a single unified scene understanding task. Typically, panoptic segmentation is derived by combining semantic and instance segmentation tasks that are learned separately or jointly (multi-task networks). In general, instance segmentation networks are built by adding a foreground mask estimation layer on top of object detectors or using instance clustering methods that assign a pixel to an instance center. In this work, we present a fully convolution neural network that learns instance segmentation from semantic segmentation and instance contours (boundaries of things). Instance contours along with semantic segmentation yield a boundary aware semantic segmentation of things. Connected component labeling on these results produces instance segmentation. We merge semantic and instance segmentation results to output panoptic segmentation. We evaluate our proposed method on the CityScapes dataset to demonstrate qualitative and quantitative performances along with several ablation studies. Our overview video can be accessed from url:https://youtu.be/wBtcxRhG3e0.



### The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers
- **Arxiv ID**: http://arxiv.org/abs/2010.08127v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2010.08127v2)
- **Published**: 2020-10-16 03:07:49+00:00
- **Updated**: 2021-02-19 03:24:24+00:00
- **Authors**: Preetum Nakkiran, Behnam Neyshabur, Hanie Sedghi
- **Comment**: Accepted to ICLR 2021
- **Journal**: None
- **Summary**: We propose a new framework for reasoning about generalization in deep learning. The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in offline learning to the problem of optimization in online learning. We then give empirical evidence that this gap between worlds can be small in realistic deep learning settings, in particular supervised image classification. For example, CNNs generalize better than MLPs on image distributions in the Real World, but this is "because" they optimize faster on the population loss in the Ideal World. This suggests our framework is a useful tool for understanding generalization in deep learning, and lays a foundation for future research in the area.



### Semantic Editing On Segmentation Map Via Multi-Expansion Loss
- **Arxiv ID**: http://arxiv.org/abs/2010.08128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.08128v1)
- **Published**: 2020-10-16 03:12:26+00:00
- **Updated**: 2020-10-16 03:12:26+00:00
- **Authors**: Jianfeng He, Xuchao Zhang, Shuo Lei, Shuhui Wang, Qingming Huang, Chang-Tien Lu, Bei Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic editing on segmentation map has been proposed as an intermediate interface for image generation, because it provides flexible and strong assistance in various image generation tasks. This paper aims to improve quality of edited segmentation map conditioned on semantic inputs. Even though recent studies apply global and local adversarial losses extensively to generate images for higher image quality, we find that they suffer from the misalignment of the boundary area in the mask area. To address this, we propose MExGAN for semantic editing on segmentation map, which uses a novel Multi-Expansion (MEx) loss implemented by adversarial losses on MEx areas. Each MEx area has the mask area of the generation as the majority and the boundary of original context as the minority. To boost convenience and stability of MEx loss, we further propose an Approximated MEx (A-MEx) loss. Besides, in contrast to previous model that builds training data for semantic editing on segmentation map with part of the whole image, which leads to model performance degradation, MExGAN applies the whole image to build the training data. Extensive experiments on semantic editing on segmentation map and natural image inpainting show competitive results on four datasets.



### Input-Aware Dynamic Backdoor Attack
- **Arxiv ID**: http://arxiv.org/abs/2010.08138v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08138v1)
- **Published**: 2020-10-16 03:57:12+00:00
- **Updated**: 2020-10-16 03:57:12+00:00
- **Authors**: Anh Nguyen, Anh Tran
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: In recent years, neural backdoor attack has been considered to be a potential security threat to deep learning systems. Such systems, while achieving the state-of-the-art performance on clean data, perform abnormally on inputs with predefined triggers. Current backdoor techniques, however, rely on uniform trigger patterns, which are easily detected and mitigated by current defense methods. In this work, we propose a novel backdoor attack technique in which the triggers vary from input to input. To achieve this goal, we implement an input-aware trigger generator driven by diversity loss. A novel cross-trigger test is applied to enforce trigger nonreusablity, making backdoor verification impossible. Experiments show that our method is efficient in various attack scenarios as well as multiple datasets. We further demonstrate that our backdoor can bypass the state of the art defense methods. An analysis with a famous neural network inspector again proves the stealthiness of the proposed attack. Our code is publicly available at https://github.com/VinAIResearch/input-aware-backdoor-attack-release.



### Pose And Joint-Aware Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.08164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08164v2)
- **Published**: 2020-10-16 04:43:34+00:00
- **Updated**: 2021-10-29 21:12:40+00:00
- **Authors**: Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng Chen, Rama Chellappa, Abhinav Shrivastava
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Recent progress on action recognition has mainly focused on RGB and optical flow features. In this paper, we approach the problem of joint-based action recognition. Unlike other modalities, constellation of joints and their motion generate models with succinct human motion information for activity recognition. We present a new model for joint-based action recognition, which first extracts motion features from each joint separately through a shared motion encoder before performing collective reasoning. Our joint selector module re-weights the joint information to select the most discriminative joints for the task. We also propose a novel joint-contrastive loss that pulls together groups of joint features which convey the same action. We strengthen the joint-based representations by using a geometry-aware data augmentation technique which jitters pose heatmaps while retaining the dynamics of the action. We show large improvements over the current state-of-the-art joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition datasets. A late fusion with RGB and Flow-based approaches yields additional improvements. Our model also outperforms the existing baseline on Mimetics, a dataset with out-of-context actions.



### DPAttack: Diffused Patch Attacks against Universal Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.11679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11679v1)
- **Published**: 2020-10-16 04:48:24+00:00
- **Updated**: 2020-10-16 04:48:24+00:00
- **Authors**: Shudeng Wu, Tao Dai, Shu-Tao Xia
- **Comment**: 4 pages, 2 figures, CIKM Workshop
- **Journal**: None
- **Summary**: Recently, deep neural networks (DNNs) have been widely and successfully used in Object Detection, e.g. Faster RCNN, YOLO, CenterNet. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. Adversarial attacks against object detection can be divided into two categories, whole-pixel attacks and patch attacks. While these attacks add perturbations to a large number of pixels in images, we proposed a diffused patch attack (\textbf{DPAttack}) to successfully fool object detectors by diffused patches of asteroid-shaped or grid-shape, which only change a small number of pixels. Experiments show that our DPAttack can successfully fool most object detectors with diffused patches and we get the second place in the Alibaba Tianchi competition: Alibaba-Tsinghua Adversarial Challenge on Object Detection. Our code can be obtained from https://github.com/Wu-Shudeng/DPAttack.



### A Generalizable and Accessible Approach to Machine Learning with Global Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.08168v1
- **DOI**: 10.1038/s41467-021-24638-z
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08168v1)
- **Published**: 2020-10-16 05:00:39+00:00
- **Updated**: 2020-10-16 05:00:39+00:00
- **Authors**: Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, Solomon Hsiang
- **Comment**: None
- **Journal**: None
- **Summary**: Combining satellite imagery with machine learning (SIML) has the potential to address global challenges by remotely estimating socioeconomic and environmental conditions in data-poor regions, yet the resource requirements of SIML limit its accessibility and use. We show that a single encoding of satellite imagery can generalize across diverse prediction tasks (e.g. forest cover, house price, road length). Our method achieves accuracy competitive with deep neural networks at orders of magnitude lower computational cost, scales globally, delivers label super-resolution predictions, and facilitates characterizations of uncertainty. Since image encodings are shared across tasks, they can be centrally computed and distributed to unlimited researchers, who need only fit a linear regression to their own ground truth data in order to achieve state-of-the-art SIML performance.



### Anisotropic Stroke Control for Multiple Artists Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.08175v2
- **DOI**: 10.1145/3394171.3413770
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.08175v2)
- **Published**: 2020-10-16 05:32:26+00:00
- **Updated**: 2021-06-14 14:25:27+00:00
- **Authors**: Xuanhong Chen, Xirui Yan, Naiyuan Liu, Ting Qiu, Bingbing Ni
- **Comment**: ACMMM2020
- **Journal**: ACM Multimedia 2020
- **Summary**: Though significant progress has been made in artistic style transfer, semantic information is usually difficult to be preserved in a fine-grained locally consistent manner by most existing methods, especially when multiple artists styles are required to transfer within one single model. To circumvent this issue, we propose a Stroke Control Multi-Artist Style Transfer framework. On the one hand, we develop a multi-condition single-generator structure which first performs multi-artist style transfer. On the one hand, we design an Anisotropic Stroke Module (ASM) which realizes the dynamic adjustment of style-stroke between the non-trivial and the trivial regions. ASM endows the network with the ability of adaptive semantic-consistency among various styles. On the other hand, we present an novel Multi-Scale Projection Discriminator} to realize the texture-level conditional generation. In contrast to the single-scale conditional discriminator, our discriminator is able to capture multi-scale texture clue to effectively distinguish a wide range of artistic styles. Extensive experimental results well demonstrate the feasibility and effectiveness of our approach. Our framework can transform a photograph into different artistic style oil painting via only ONE single model. Furthermore, the results are with distinctive artistic style and retain the anisotropic semantic information. The code is already available on github: https://github.com/neuralchen/ASMAGAN.



### How many images do I need? Understanding how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring
- **Arxiv ID**: http://arxiv.org/abs/2010.08186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2010.08186v1)
- **Published**: 2020-10-16 06:28:35+00:00
- **Updated**: 2020-10-16 06:28:35+00:00
- **Authors**: Saleh Shahinfar, Paul Meek, Greg Falzon
- **Comment**: None
- **Journal**: Ecological Informatics, 2020, 57:101085
- **Summary**: Deep learning (DL) algorithms are the state of the art in automated classification of wildlife camera trap images. The challenge is that the ecologist cannot know in advance how many images per species they need to collect for model training in order to achieve their desired classification accuracy. In fact there is limited empirical evidence in the context of camera trapping to demonstrate that increasing sample size will lead to improved accuracy. In this study we explore in depth the issues of deep learning model performance for progressively increasing per class (species) sample sizes. We also provide ecologists with an approximation formula to estimate how many images per animal species they need for certain accuracy level a priori. This will help ecologists for optimal allocation of resources, work and efficient study design. In order to investigate the effect of number of training images; seven training sets with 10, 20, 50, 150, 500, 1000 images per class were designed. Six deep learning architectures namely ResNet-18, ResNet-50, ResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a common exclusive testing set of 250 images per class. The whole experiment was repeated on three similar datasets from Australia, Africa and North America and the results were compared. Simple regression equations for use by practitioners to approximate model performance metrics are provided. Generalized additive models (GAM) are shown to be effective in modelling DL performance metrics based on the number of training images per class, tuning scheme and dataset.   Key-words: Camera Traps, Deep Learning, Ecological Informatics, Generalised Additive Models, Learning Curves, Predictive Modelling, Wildlife.



### Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation
- **Arxiv ID**: http://arxiv.org/abs/2010.08188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08188v2)
- **Published**: 2020-10-16 06:50:47+00:00
- **Updated**: 2021-03-30 13:17:23+00:00
- **Authors**: Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee, Sookyung Kim, Edward Choi
- **Comment**: Accepted to AAAI 2021, 22 pages
- **Journal**: None
- **Summary**: Video generation models often operate under the assumption of fixed frame rates, which leads to suboptimal performance when it comes to handling flexible frame rates (e.g., increasing the frame rate of the more dynamic portion of the video as well as handling missing video frames). To resolve the restricted nature of existing video generation models' ability to handle arbitrary timesteps, we propose continuous-time video generation by combining neural ODE (Vid-ODE) with pixel-level video processing techniques. Using ODE-ConvGRU as an encoder, a convolutional version of the recently proposed neural ODE, which enables us to learn continuous-time dynamics, Vid-ODE can learn the spatio-temporal dynamics of input videos of flexible frame rates. The decoder integrates the learned dynamics function to synthesize video frames at any given timesteps, where the pixel-level composition technique is used to maintain the sharpness of individual frames. With extensive experiments on four real-world video datasets, we verify that the proposed Vid-ODE outperforms state-of-the-art approaches under various video generation settings, both within the trained time range (interpolation) and beyond the range (extrapolation). To the best of our knowledge, Vid-ODE is the first work successfully performing continuous-time video generation using real-world videos.



### New Ideas and Trends in Deep Multimodal Content Understanding: A Review
- **Arxiv ID**: http://arxiv.org/abs/2010.08189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08189v1)
- **Published**: 2020-10-16 06:50:54+00:00
- **Updated**: 2020-10-16 06:50:54+00:00
- **Authors**: Wei Chen, Weiping Wang, Li Liu, Michael S. Lew
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: The focus of this survey is on the analysis of two modalities of multimodal deep learning: image and text. Unlike classic reviews of deep learning where monomodal image classifiers such as VGG, ResNet and Inception module are central topics, this paper will examine recent multimodal deep models and structures, including auto-encoders, generative adversarial nets and their variants. These models go beyond the simple image classifiers in which they can do uni-directional (e.g. image captioning, image generation) and bi-directional (e.g. cross-modal retrieval, visual question answering) multimodal tasks. Besides, we analyze two aspects of the challenge in terms of better content understanding in deep multimodal applications. We then introduce current ideas and trends in deep multimodal feature learning, such as feature embedding approaches and objective function design, which are crucial in overcoming the aforementioned challenges. Finally, we include several promising directions for future research.



### ASMFS: Adaptive-Similarity-based Multi-modality Feature Selection for Classification of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2010.08190v1
- **DOI**: 10.1016/j.patcog.2022.108566
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08190v1)
- **Published**: 2020-10-16 06:53:27+00:00
- **Updated**: 2020-10-16 06:53:27+00:00
- **Authors**: Yuang Shi, Chen Zu, Mei Hong, Luping Zhou, Lei Wang, Xi Wu, Jiliu Zhou, Daoqiang Zhang, Yan Wang
- **Comment**: 27 pages, 10 figures
- **Journal**: None
- **Summary**: With the increasing amounts of high-dimensional heterogeneous data to be processed, multi-modality feature selection has become an important research direction in medical image analysis. Traditional methods usually depict the data structure using fixed and predefined similarity matrix for each modality separately, without considering the potential relationship structure across different modalities. In this paper, we propose a novel multi-modality feature selection method, which performs feature selection and local similarity learning simultaniously. Specially, a similarity matrix is learned by jointly considering different imaging modalities. And at the same time, feature selection is conducted by imposing sparse l_{2, 1} norm constraint. The effectiveness of our proposed joint learning method can be well demonstrated by the experimental results on Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, which outperforms existing the state-of-the-art multi-modality approaches.



### Extracting Signals of Higgs Boson From Background Noise Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.08201v1
- **DOI**: None
- **Categories**: **hep-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08201v1)
- **Published**: 2020-10-16 07:19:39+00:00
- **Updated**: 2020-10-16 07:19:39+00:00
- **Authors**: Muhammad Abbas, Asifullah Khan, Aqsa Saeed Qureshi, Muhammad Waleed Khan
- **Comment**: Figures: 2, Table: 1
- **Journal**: None
- **Summary**: Higgs boson is a fundamental particle, and the classification of Higgs signals is a well-known problem in high energy physics. The identification of the Higgs signal is a challenging task because its signal has a resemblance to the background signals. This study proposes a Higgs signal classification using a novel combination of random forest, auto encoder and deep auto encoder to build a robust and generalized Higgs boson prediction system to discriminate the Higgs signal from the background noise. The proposed ensemble technique is based on achieving diversity in the decision space, and the results show good discrimination power on the private leaderboard; achieving an area under the Receiver Operating Characteristic curve of 0.9 and an Approximate Median Significance score of 3.429.



### Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames
- **Arxiv ID**: http://arxiv.org/abs/2010.08202v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08202v4)
- **Published**: 2020-10-16 07:24:32+00:00
- **Updated**: 2022-10-11 03:17:11+00:00
- **Authors**: Xiaotong Chen, Kaizhi Zheng, Zhen Zeng, Cameron Kisailus, Shreshtha Basu, James Cooney, Jana Pavlasek, Odest Chadwicke Jenkins
- **Comment**: Humanoids 2022 paper. video link:
  https://www.youtube.com/watch?v=VkLI8m_3erU;
  https://www.youtube.com/watch?v=7P9_O9wveYk, github link:
  https://github.com/cxt98/ACF_perception
- **Journal**: None
- **Summary**: In order to enable robust operation in unstructured environments, robots should be able to generalize manipulation actions to novel object instances. For example, to pour and serve a drink, a robot should be able to recognize novel containers which afford the task. Most importantly, robots should be able to manipulate these novel containers to fulfill the task. To achieve this, we aim to provide robust and generalized perception of object affordances and their associated manipulation poses for reliable manipulation. In this work, we combine the notions of affordance and category-level pose, and introduce the Affordance Coordinate Frame (ACF). With ACF, we represent each object class in terms of individual affordance parts and the compatibility between them, where each part is associated with a part category-level pose for robot manipulation. In our experiments, we demonstrate that ACF outperforms state-of-the-art methods for object detection, as well as category-level pose estimation for object parts. We further demonstrate the applicability of ACF to robot manipulation tasks through experiments in a simulated environment.



### Human Perception-based Evaluation Criterion for Ultra-high Resolution Cell Membrane Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.08209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08209v1)
- **Published**: 2020-10-16 07:39:17+00:00
- **Updated**: 2020-10-16 07:39:17+00:00
- **Authors**: Ruohua Shi, Wenyao Wang, Zhixuan Li, Liuyuan He, Kaiwen Sheng, Lei Ma, Kai Du, Tingting Jiang, Tiejun Huang
- **Comment**: submitted to ICLR 2021
- **Journal**: None
- **Summary**: Computer vision technology is widely used in biological and medical data analysis and understanding. However, there are still two major bottlenecks in the field of cell membrane segmentation, which seriously hinder further research: lack of sufficient high-quality data and lack of suitable evaluation criteria. In order to solve these two problems, this paper first proposes an Ultra-high Resolution Image Segmentation dataset for the Cell membrane, called U-RISC, the largest annotated Electron Microscopy (EM) dataset for the Cell membrane with multiple iterative annotations and uncompressed high-resolution raw data. During the analysis process of the U-RISC, we found that the current popular segmentation evaluation criteria are inconsistent with human perception. This interesting phenomenon is confirmed by a subjective experiment involving twenty people. Furthermore, to resolve this inconsistency, we propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of cell membrane segmentation results. Detailed performance comparison and discussion of classic segmentation methods along with two iterative manual annotation results under existing evaluation criteria and PHD is given.



### How Does Supernet Help in Neural Architecture Search?
- **Arxiv ID**: http://arxiv.org/abs/2010.08219v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.08219v2)
- **Published**: 2020-10-16 08:07:03+00:00
- **Updated**: 2021-05-05 07:26:48+00:00
- **Authors**: Yuge Zhang, Quanlu Zhang, Yaming Yang
- **Comment**: Accepted by 2nd Workshop on Neural Architecture Search at ICLR 2021
- **Journal**: None
- **Summary**: Weight sharing, as an approach to speed up architecture performance estimation has received wide attention. Instead of training each architecture separately, weight sharing builds a supernet that assembles all the architectures as its submodels. However, there has been debate over whether the NAS process actually benefits from weight sharing, due to the gap between supernet optimization and the objective of NAS. To further understand the effect of weight sharing on NAS, we conduct a comprehensive analysis on five search spaces, including NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10, DARTS-PTB, and ProxylessNAS. We find that weight sharing works well on some search spaces but fails on others. Taking a step forward, we further identified biases accounting for such phenomenon and the capacity of weight sharing. Our work is expected to inspire future NAS researchers to better leverage the power of weight sharing.



### HPERL: 3D Human Pose Estimation from RGB and LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2010.08221v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.1.4
- **Links**: [PDF](http://arxiv.org/pdf/2010.08221v1)
- **Published**: 2020-10-16 08:09:49+00:00
- **Updated**: 2020-10-16 08:09:49+00:00
- **Authors**: Michael Fürst, Shriya T. P. Gupta, René Schuster, Oliver Wasenmüller, Didier Stricker
- **Comment**: 7 pages, 6 figures, 4 tables, LiDAR and RGB Fusion
- **Journal**: None
- **Summary**: In-the-wild human pose estimation has a huge potential for various fields, ranging from animation and action recognition to intention recognition and prediction for autonomous driving. The current state-of-the-art is focused only on RGB and RGB-D approaches for predicting the 3D human pose. However, not using precise LiDAR depth information limits the performance and leads to very inaccurate absolute pose estimation. With LiDAR sensors becoming more affordable and common on robots and autonomous vehicle setups, we propose an end-to-end architecture using RGB and LiDAR to predict the absolute 3D human pose with unprecedented precision. Additionally, we introduce a weakly-supervised approach to generate 3D predictions using 2D pose annotations from PedX [1]. This allows for many new opportunities in the field of 3D human pose estimation.



### SF-UDA$^{3D}$: Source-Free Unsupervised Domain Adaptation for LiDAR-Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.08243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08243v2)
- **Published**: 2020-10-16 08:44:49+00:00
- **Updated**: 2020-10-19 14:19:30+00:00
- **Authors**: Cristiano Saltori, Stéphane Lathuiliére, Nicu Sebe, Elisa Ricci, Fabio Galasso
- **Comment**: Accepted paper at 3DV 2020
- **Journal**: None
- **Summary**: 3D object detectors based only on LiDAR point clouds hold the state-of-the-art on modern street-view benchmarks. However, LiDAR-based detectors poorly generalize across domains due to domain shift. In the case of LiDAR, in fact, domain shift is not only due to changes in the environment and in the object appearances, as for visual data from RGB cameras, but is also related to the geometry of the point clouds (e.g., point density variations). This paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain Adaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D detector to target domains for which we have no annotations (unsupervised), neither we hold images nor annotations of the source domain (source-free). SF-UDA$^{3D}$ is novel on both aspects. Our approach is based on pseudo-annotations, reversible scale-transformations and motion coherency. SF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on features alignment and state-of-the-art 3D object detection methods which additionally use few-shot target annotations or target annotation statistics. This is demonstrated by extensive experiments on two large-scale datasets, i.e., KITTI and nuScenes.



### Auxiliary Task Reweighting for Minimum-data Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.08244v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.08244v1)
- **Published**: 2020-10-16 08:45:37+00:00
- **Updated**: 2020-10-16 08:45:37+00:00
- **Authors**: Baifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu
- **Comment**: NeurIPS 2020. Project page:
  https://sites.google.com/view/auxiliary-task-reweighting/home
- **Journal**: None
- **Summary**: Supervised learning requires a large amount of training data, limiting its application where labeled data is scarce. To compensate for data scarcity, one possible method is to utilize auxiliary tasks to provide additional supervision for the main task. Assigning and optimizing the importance weights for different auxiliary tasks remains an crucial and largely understudied research question. In this work, we propose a method to automatically reweight auxiliary tasks in order to reduce the data requirement on the main task. Specifically, we formulate the weighted likelihood function of auxiliary tasks as a surrogate prior for the main task. By adjusting the auxiliary task weights to minimize the divergence between the surrogate prior and the true prior of the main task, we obtain a more accurate prior estimation, achieving the goal of minimizing the required amount of training data for the main task and avoiding a costly grid search. In multiple experimental settings (e.g. semi-supervised learning, multi-label classification), we demonstrate that our algorithm can effectively utilize limited labeled data of the main task with the benefit of auxiliary tasks compared with previous task reweighting methods. We also show that under extreme cases with only a few extra examples (e.g. few-shot domain adaptation), our algorithm results in significant improvement over the baseline.



### Local plasticity rules can learn deep representations using self-supervised contrastive predictions
- **Arxiv ID**: http://arxiv.org/abs/2010.08262v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.AR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08262v5)
- **Published**: 2020-10-16 09:32:35+00:00
- **Updated**: 2021-10-25 10:23:56+00:00
- **Authors**: Bernd Illing, Jean Ventura, Guillaume Bellec, Wulfram Gerstner
- **Comment**: None
- **Journal**: None
- **Summary**: Learning in the brain is poorly understood and learning rules that respect biological constraints, yet yield deep hierarchical representations, are still unknown. Here, we propose a learning rule that takes inspiration from neuroscience and recent advances in self-supervised deep learning. Learning minimizes a simple layer-specific loss function and does not need to back-propagate error signals within or between layers. Instead, weight updates follow a local, Hebbian, learning rule that only depends on pre- and post-synaptic neuronal activity, predictive dendritic input and widely broadcasted modulation factors which are identical for large groups of neurons. The learning rule applies contrastive predictive learning to a causal, biological setting using saccades (i.e. rapid shifts in gaze direction). We find that networks trained with this self-supervised and local rule build deep hierarchical representations of images, speech and video.



### Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization
- **Arxiv ID**: http://arxiv.org/abs/2010.08276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.08276v2)
- **Published**: 2020-10-16 09:52:13+00:00
- **Updated**: 2022-04-29 05:37:30+00:00
- **Authors**: Biao Zhang, Peter Wonka
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: We propose a novel 3d shape representation for 3d shape reconstruction from a single image. Rather than predicting a shape directly, we train a network to generate a training set which will be fed into another learning algorithm to define the shape. The nested optimization problem can be modeled by bi-level optimization. Specifically, the algorithms for bi-level optimization are also being used in meta learning approaches for few-shot learning. Our framework establishes a link between 3D shape analysis and few-shot learning. We combine training data generating networks with bi-level optimization algorithms to obtain a complete framework for which all components can be jointly trained. We improve upon recent work on standard benchmarks for 3d shape reconstruction.



### Real-Time Face & Eye Tracking and Blink Detection using Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2010.08278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08278v1)
- **Published**: 2020-10-16 10:02:41+00:00
- **Updated**: 2020-10-16 10:02:41+00:00
- **Authors**: Cian Ryan, Brian O Sullivan, Amr Elrasad, Joe Lemley, Paul Kielty, Christoph Posch, Etienne Perot
- **Comment**: 20 Pages, 8 Figures
- **Journal**: None
- **Summary**: Event cameras contain emerging, neuromorphic vision sensors that capture local light intensity changes at each pixel, generating a stream of asynchronous events. This way of acquiring visual information constitutes a departure from traditional frame based cameras and offers several significant advantages: low energy consumption, high temporal resolution, high dynamic range and low latency. Driver monitoring systems (DMS) are in-cabin safety systems designed to sense and understand a drivers physical and cognitive state. Event cameras are particularly suited to DMS due to their inherent advantages. This paper proposes a novel method to simultaneously detect and track faces and eyes for driver monitoring. A unique, fully convolutional recurrent neural network architecture is presented. To train this network, a synthetic event-based dataset is simulated with accurate bounding box annotations, called Neuromorphic HELEN. Additionally, a method to detect and analyse drivers eye blinks is proposed, exploiting the high temporal resolution of event cameras. Behaviour of blinking provides greater insights into a driver level of fatigue or drowsiness. We show that blinks have a unique temporal signature that can be better captured by event cameras.



### Minimizing Labeling Effort for Tree Skeleton Segmentation using an Automated Iterative Training Methodology
- **Arxiv ID**: http://arxiv.org/abs/2010.08296v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08296v3)
- **Published**: 2020-10-16 10:38:43+00:00
- **Updated**: 2021-08-09 07:38:49+00:00
- **Authors**: Keenan Granland, Rhys Newbury, David Ting, Chao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Training of convolutional neural networks for semantic segmentation requires accurate pixel-wise labeling which requires large amounts of human effort. The human-in-the-loop method reduces labeling effort; however, it requires human intervention for each image. This paper describes a general iterative training methodology for semantic segmentation, Automating-the-Loop. This aims to replicate the manual adjustments of the human-in-the-loop method with an automated process, hence, drastically reducing labeling effort. Using the application of detecting partially occluded apple tree segmentation, we compare manually labeled annotations, self-training, human-in-the-loop, and Automating-the-Loop methods in both the quality of the trained convolutional neural networks, and the effort needed to create them. The convolutional neural network (U-Net) performance is analyzed using traditional metrics and a new metric, Complete Grid Scan, which promotes connectivity and low noise. It is shown that in our application, the new Automating-the-Loop method greatly reduces the labeling effort while producing comparable performance to both human-in-the-loop and complete manual labeling methods.



### In Depth Bayesian Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2010.08310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08310v1)
- **Published**: 2020-10-16 11:05:31+00:00
- **Updated**: 2020-10-16 11:05:31+00:00
- **Authors**: David Gillsjö, Kalle Åström
- **Comment**: Accepted at ICPR2020, 10 pages, 12 figures
- **Journal**: None
- **Summary**: This work studies Semantic Scene Completion which aims to predict a 3D semantic segmentation of our surroundings, even though some areas are occluded. For this we construct a Bayesian Convolutional Neural Network (BCNN), which is not only able to perform the segmentation, but also predict model uncertainty. This is an important feature not present in standard CNNs.   We show on the MNIST dataset that the Bayesian approach performs equal or better to the standard CNN when processing digits unseen in the training phase when looking at accuracy, precision and recall. With the added benefit of having better calibrated scores and the ability to express model uncertainty.   We then show results for the Semantic Scene Completion task where a category is introduced at test time on the SUNCG dataset. In this more complex task the Bayesian approach outperforms the standard CNN. Showing better Intersection over Union score and excels in Average Precision and separation scores.



### Learning Accurate Entropy Model with Global Reference for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2010.08321v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08321v3)
- **Published**: 2020-10-16 11:27:46+00:00
- **Updated**: 2022-01-05 02:35:41+00:00
- **Authors**: Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong Sun, Hao Li, Rong Jin
- **Comment**: None
- **Journal**: International Conference on Learning Representations (2021)
- **Summary**: In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry.



### Learning Monocular Dense Depth from Events
- **Arxiv ID**: http://arxiv.org/abs/2010.08350v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08350v2)
- **Published**: 2020-10-16 12:36:23+00:00
- **Updated**: 2020-10-22 08:33:43+00:00
- **Authors**: Javier Hidalgo-Carrió, Daniel Gehrig, Davide Scaramuzza
- **Comment**: IEEE International Conference on 3D Vision (3DV), 2020
- **Journal**: None
- **Summary**: Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous events instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50% improvement in average depth error with respect to previous event-based methods.



### VolumeNet: A Lightweight Parallel Network for Super-Resolution of Medical Volumetric Data
- **Arxiv ID**: http://arxiv.org/abs/2010.08357v2
- **DOI**: 10.1109/TIP.2021.3076285
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08357v2)
- **Published**: 2020-10-16 12:53:15+00:00
- **Updated**: 2020-10-24 06:00:55+00:00
- **Authors**: Yinhao Li, Yutaro Iwamoto, Lanfen Lin, Rui Xu, Yen-Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based super-resolution (SR) techniques have generally achieved excellent performance in the computer vision field. Recently, it has been proven that three-dimensional (3D) SR for medical volumetric data delivers better visual results than conventional two-dimensional (2D) processing. However, deepening and widening 3D networks increases training difficulty significantly due to the large number of parameters and small number of training samples. Thus, we propose a 3D convolutional neural network (CNN) for SR of medical volumetric data called ParallelNet using parallel connections. We construct a parallel connection structure based on the group convolution and feature aggregation to build a 3D CNN that is as wide as possible with few parameters. As a result, the model thoroughly learns more feature maps with larger receptive fields. In addition, to further improve accuracy, we present an efficient version of ParallelNet (called VolumeNet), which reduces the number of parameters and deepens ParallelNet using a proposed lightweight building block module called the Queue module. Unlike most lightweight CNNs based on depthwise convolutions, the Queue module is primarily constructed using separable 2D cross-channel convolutions. As a result, the number of network parameters and computational complexity can be reduced significantly while maintaining accuracy due to full channel fusion. Experimental results demonstrate that the proposed VolumeNet significantly reduces the number of model parameters and achieves high precision results compared to state-of-the-art methods.



### G-DARTS-A: Groups of Channel Parallel Sampling with Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.08360v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08360v1)
- **Published**: 2020-10-16 12:58:08+00:00
- **Updated**: 2020-10-16 12:58:08+00:00
- **Authors**: Zhaowen Wang, Wei Zhang, Zhiming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable Architecture Search (DARTS) provides a baseline for searching effective network architectures based gradient, but it is accompanied by huge computational overhead in searching and training network architecture. Recently, many novel works have improved DARTS. Particularly, Partially-Connected DARTS(PC-DARTS) proposed the partial channel sampling technique which achieved good results. In this work, we found that the backbone provided by DARTS is prone to overfitting. To mitigate this problem, we propose an approach named Group-DARTS with Attention (G-DARTS-A), using multiple groups of channels for searching. Inspired by the partially sampling strategy of PC-DARTS, we use groups channels to sample the super-network to perform a more efficient search while maintaining the relative integrity of the network information. In order to relieve the competition between channel groups and keep channel balance, we follow the attention mechanism in Squeeze-and-Excitation Network. Each group of channels shares defined weights thence they can provide different suggestion for searching. The searched architecture is more powerful and better adapted to different deployments. Specifically, by only using the attention module on DARTS we achieved an error rate of 2.82%/16.36% on CIFAR10/100 with 0.3GPU-days for search process on CIFAR10. Apply our G-DARTS-A to DARTS/PC-DARTS, an error rate of 2.57%/2.61% on CIFAR10 with 0.5/0.4 GPU-days is achieved.



### A Simple Baseline for Pose Tracking in Videos of Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.10007v2
- **DOI**: 10.1145/3394171.3416300
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10007v2)
- **Published**: 2020-10-16 13:06:21+00:00
- **Updated**: 2020-10-21 03:37:18+00:00
- **Authors**: Li Yuan, Shuning Chang, Ziyuan Huang, Yichen Zhou, Yunpeng Chen, Xuecheng Nie, Francis E. H. Tay, Jiashi Feng, Shuicheng Yan
- **Comment**: 2nd Place in ACM Multimedia Grand Challenge: Human in Events, Track3:
  Crowd Pose Tracking in Complex Events. ACM Multimedia 2020. arXiv admin note:
  substantial text overlap with arXiv:2010.08365, arXiv:2010.10008
- **Journal**: None
- **Summary**: This paper presents our solution to ACM MM challenge: Large-scale Human-centric Video Analysis in Complex Events\cite{lin2020human}; specifically, here we focus on Track3: Crowd Pose Tracking in Complex Events. Remarkable progress has been made in multi-pose training in recent years. However, how to track the human pose in crowded and complex environments has not been well addressed. We formulate the problem as several subproblems to be solved. First, we use a multi-object tracking method to assign human ID to each bounding box generated by the detection model. After that, a pose is generated to each bounding box with ID. At last, optical flow is used to take advantage of the temporal information in the videos and generate the final pose tracking result.



### Toward Accurate Person-level Action Recognition in Videos of Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.08365v1
- **DOI**: 10.1145/3394171.3416301
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08365v1)
- **Published**: 2020-10-16 13:08:50+00:00
- **Updated**: 2020-10-16 13:08:50+00:00
- **Authors**: Li Yuan, Yichen Zhou, Shuning Chang, Ziyuan Huang, Yunpeng Chen, Xuecheng Nie, Tao Wang, Jiashi Feng, Shuicheng Yan
- **Comment**: 1'st Place in ACM Multimedia Grand Challenge: Human in Events,
  Track4: Person-level Action Recognition in Complex Events
- **Journal**: ACM Multimedia 2020
- **Summary**: Detecting and recognizing human action in videos with crowded scenes is a challenging problem due to the complex environment and diversity events. Prior works always fail to deal with this problem in two aspects: (1) lacking utilizing information of the scenes; (2) lacking training data in the crowd and complex scenes. In this paper, we focus on improving spatio-temporal action recognition by fully-utilizing the information of scenes and collecting new data. A top-down strategy is used to overcome the limitations. Specifically, we adopt a strong human detector to detect the spatial location of each frame. We then apply action recognition models to learn the spatio-temporal information from video frames on both the HIE dataset and new data with diverse scenes from the internet, which can improve the generalization ability of our model. Besides, the scenes information is extracted by the semantic segmentation model to assistant the process. As a result, our method achieved an average 26.05 wf\_mAP (ranking 1st place in the ACM MM grand challenge 2020: Human in Events).



### Towards Accurate Human Pose Estimation in Videos of Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.10008v2
- **DOI**: 10.1145/3394171.3416299
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10008v2)
- **Published**: 2020-10-16 13:19:11+00:00
- **Updated**: 2020-10-21 03:37:40+00:00
- **Authors**: Li Yuan, Shuning Chang, Xuecheng Nie, Ziyuan Huang, Yichen Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan
- **Comment**: 2nd Place in ACM Multimedia Grand Challenge: Human in Events, Track2:
  Crowd Pose Estimation in Complex Events. ACM Multimedia 2020. arXiv admin
  note: substantial text overlap with arXiv:2010.08365, arXiv:2010.10007
- **Journal**: None
- **Summary**: Video-based human pose estimation in crowded scenes is a challenging problem due to occlusion, motion blur, scale variation and viewpoint change, etc. Prior approaches always fail to deal with this problem because of (1) lacking of usage of temporal information; (2) lacking of training data in crowded scenes. In this paper, we focus on improving human pose estimation in videos of crowded scenes from the perspectives of exploiting temporal context and collecting new data. In particular, we first follow the top-down strategy to detect persons and perform single-person pose estimation for each frame. Then, we refine the frame-based pose estimation with temporal contexts deriving from the optical-flow. Specifically, for one frame, we forward the historical poses from the previous frames and backward the future poses from the subsequent frames to current frame, leading to stable and accurate human pose estimation in videos. In addition, we mine new data of similar scenes to HIE dataset from the Internet for improving the diversity of training set. In this way, our model achieves best performance on 7 out of 13 videos and 56.33 average w\_AP on test dataset of HIE challenge.



### On the surprising similarities between supervised and self-supervised models
- **Arxiv ID**: http://arxiv.org/abs/2010.08377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2010.08377v1)
- **Published**: 2020-10-16 13:28:13+00:00
- **Updated**: 2020-10-16 13:28:13+00:00
- **Authors**: Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Matthias Bethge, Felix A. Wichmann, Wieland Brendel
- **Comment**: None
- **Journal**: None
- **Summary**: How do humans learn to acquire a powerful, flexible and robust representation of objects? While much of this process remains unknown, it is clear that humans do not require millions of object labels. Excitingly, recent algorithmic advancements in self-supervised learning now enable convolutional neural networks (CNNs) to learn useful visual object representations without supervised labels, too. In the light of this recent breakthrough, we here compare self-supervised networks to supervised models and human behaviour. We tested models on 15 generalisation datasets for which large-scale human behavioural data is available (130K highly controlled psychophysical trials). Surprisingly, current self-supervised CNNs share four key characteristics of their supervised counterparts: (1.) relatively poor noise robustness (with the notable exception of SimCLR), (2.) non-human category-level error patterns, (3.) non-human image-level error patterns (yet high similarity to supervised model errors) and (4.) a bias towards texture. Taken together, these results suggest that the strategies learned through today's supervised and self-supervised training objectives end up being surprisingly similar, but distant from human-like behaviour. That being said, we are clearly just at the beginning of what could be called a self-supervised revolution of machine vision, and we are hopeful that future self-supervised models behave differently from supervised ones, and---perhaps---more similar to robust human object recognition.



### Volumetric Calculation of Quantization Error in 3-D Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/2010.08390v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08390v1)
- **Published**: 2020-10-16 13:48:30+00:00
- **Updated**: 2020-10-16 13:48:30+00:00
- **Authors**: Eleni Bohacek, Andrew J. Coates, David R. Selviah
- **Comment**: As submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence on 4th September 2020
- **Journal**: None
- **Summary**: This paper investigates how the inherent quantization of camera sensors introduces uncertainty in the calculated position of an observed feature during 3-D mapping. It is typically assumed that pixels and scene features are points, however, a pixel is a two-dimensional area that maps onto multiple points in the scene. This uncertainty region is a bound for quantization error in the calculated point positions. Earlier studies calculated the volume of two intersecting pixel views, approximated as a cuboid, by projecting pyramids and cones from the pixels into the scene. In this paper, we reverse this approach by generating an array of scene points and calculating which scene points are detected by which pixel in each camera. This enables us to map the uncertainty regions for every pixel correspondence for a given camera system in one calculation, without approximating the complex shapes. The dependence of the volumes of the uncertainty regions on camera baseline length, focal length, pixel size, and distance to object, shows that earlier studies overestimated the quantization error by at least a factor of two. For static camera systems the method can also be used to determine volumetric scene geometry without the need to calculate disparity maps.



### Reconstructing A Large Scale 3D Face Dataset for Deep 3D Face Identification
- **Arxiv ID**: http://arxiv.org/abs/2010.08391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.08391v2)
- **Published**: 2020-10-16 13:48:38+00:00
- **Updated**: 2022-06-12 10:01:39+00:00
- **Authors**: Cuican Yu, Zihui Zhang, Huibin Li
- **Comment**: we want to re-organize this paper
- **Journal**: None
- **Summary**: Deep learning methods have brought many breakthroughs to computer vision, especially in 2D face recognition. However, the bottleneck of deep learning based 3D face recognition is that it is difficult to collect millions of 3D faces, whether for industry or academia. In view of this situation, there are many methods to generate more 3D faces from existing 3D faces through 3D face data augmentation, which are used to train deep 3D face recognition models. However, to the best of our knowledge, there is no method to generate 3D faces from 2D face images for training deep 3D face recognition models. This letter focuses on the role of reconstructed 3D facial surfaces in 3D face identification and proposes a framework of 2D-aided deep 3D face identification. In particular, we propose to reconstruct millions of 3D face scans from a large scale 2D face database (i.e.VGGFace2), using a deep learning based 3D face reconstruction method (i.e.ExpNet). Then, we adopt a two-phase training approach: In the first phase, we use millions of face images to pre-train the deep convolutional neural network (DCNN), and in the second phase, we use normal component images (NCI) of reconstructed 3D face scans to train the DCNN. Extensive experimental results illustrate that the proposed approach can greatly improve the rank-1 score of 3D face identification on the FRGC v2.0, the Bosphorus, and the BU-3DFE 3D face databases, compared to the model trained by 2D face images. Finally, our proposed approach achieves state-of-the-art rank-1 scores on the FRGC v2.0 (97.6%), Bosphorus (98.4%), and BU-3DFE (98.8%) databases. The experimental results show that the reconstructed 3D facial surfaces are useful and our 2D-aided deep 3D face identification framework is meaningful, facing the scarcity of 3D faces.



### Difference-in-Differences: Bridging Normalization and Disentanglement in PG-GAN
- **Arxiv ID**: http://arxiv.org/abs/2010.08402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08402v1)
- **Published**: 2020-10-16 14:02:53+00:00
- **Updated**: 2020-10-16 14:02:53+00:00
- **Authors**: Xiao Liu, Jiajie Zhang, Siting Li, Zuotong Wu, Yang Yu
- **Comment**: None
- **Journal**: None
- **Summary**: What mechanisms causes GAN's entanglement? Although developing disentangled GAN has attracted sufficient attention, it is unclear how entanglement is originated by GAN transformation. We in this research propose a difference-in-difference (DID) counterfactual framework to design experiments for analyzing the entanglement mechanism in on of the Progressive-growing GAN (PG-GAN). Our experiment clarify the mechanisms how pixel normalization causes PG-GAN entanglement during a input-unit-ablation transformation. We discover that pixel normalization causes object entanglement by in-painting the area occupied by ablated objects. We also discover the unit-object relation determines whether and how pixel normalization causes objects entanglement. Our DID framework theoretically guarantees that the mechanisms that we discover is solid, explainable and comprehensively.



### Deep Learning based Automated Forest Health Diagnosis from Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2010.08437v1
- **DOI**: 10.1109/ACCESS.2020.3012417.
- **Categories**: **cs.CV**, I.4.6; I.4.9; I.2.6; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2010.08437v1)
- **Published**: 2020-10-16 15:07:56+00:00
- **Updated**: 2020-10-16 15:07:56+00:00
- **Authors**: Chia-Yen Chiang, Chloe Barnes, Plamen Angelov, Richard Jiang
- **Comment**: 16 pages
- **Journal**: IEEE Access, vol. 8, pp. 144064-144076, 2020
- **Summary**: Global climate change has had a drastic impact on our environment. Previous study showed that pest disaster occured from global climate change may cause a tremendous number of trees died and they inevitably became a factor of forest fire. An important portent of the forest fire is the condition of forests. Aerial image-based forest analysis can give an early detection of dead trees and living trees. In this paper, we applied a synthetic method to enlarge imagery dataset and present a new framework for automated dead tree detection from aerial images using a re-trained Mask RCNN (Mask Region-based Convolutional Neural Network) approach, with a transfer learning scheme. We apply our framework to our aerial imagery datasets,and compare eight fine-tuned models. The mean average precision score (mAP) for the best of these models reaches 54%. Following the automated detection, we are able to automatically produce and calculate number of dead tree masks to label the dead trees in an image, as an indicator of forest health that could be linked to the causal analysis of environmental changes and the predictive likelihood of forest fire.



### Towards Online Steering of Flame Spray Pyrolysis Nanoparticle Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.08486v1
- **DOI**: 10.1109/XLOOP51963.2020.00011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08486v1)
- **Published**: 2020-10-16 16:38:16+00:00
- **Updated**: 2020-10-16 16:38:16+00:00
- **Authors**: Maksim Levental, Ryan Chard, Joseph A. Libera, Kyle Chard, Aarthi Koripelly, Jakob R. Elias, Marcus Schwarting, Ben Blaiszik, Marius Stan, Santanu Chaudhuri, Ian Foster
- **Comment**: None
- **Journal**: None
- **Summary**: Flame Spray Pyrolysis (FSP) is a manufacturing technique to mass produce engineered nanoparticles for applications in catalysis, energy materials, composites, and more. FSP instruments are highly dependent on a number of adjustable parameters, including fuel injection rate, fuel-oxygen mixtures, and temperature, which can greatly affect the quality, quantity, and properties of the yielded nanoparticles. Optimizing FSP synthesis requires monitoring, analyzing, characterizing, and modifying experimental conditions.Here, we propose a hybrid CPU-GPU Difference of Gaussians (DoG)method for characterizing the volume distribution of unburnt solution, so as to enable near-real-time optimization and steering of FSP experiments. Comparisons against standard implementations show our method to be an order of magnitude more efficient. This surrogate signal can be deployed as a component of an online end-to-end pipeline that maximizes the synthesis yield.



### Latent Vector Recovery of Audio GANs
- **Arxiv ID**: http://arxiv.org/abs/2010.08534v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08534v1)
- **Published**: 2020-10-16 17:45:35+00:00
- **Updated**: 2020-10-16 17:45:35+00:00
- **Authors**: Andrew Keyes, Nicky Bayat, Vahid Reza Khazaie, Yalda Mohsenzadeh
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Advanced Generative Adversarial Networks (GANs) are remarkable in generating intelligible audio from a random latent vector. In this paper, we examine the task of recovering the latent vector of both synthesized and real audio. Previous works recovered latent vectors of given audio through an auto-encoder inspired technique that trains an encoder network either in parallel with the GAN or after the generator is trained. With our approach, we train a deep residual neural network architecture to project audio synthesized by WaveGAN into the corresponding latent space with near identical reconstruction performance. To accommodate for the lack of an original latent vector for real audio, we optimize the residual network on the perceptual loss between the real audio samples and the reconstructed audio of the predicted latent vectors. In the case of synthesized audio, the Mean Squared Error (MSE) between the ground truth and recovered latent vector is minimized as well. We further investigated the audio reconstruction performance when several gradient optimization steps are applied to the predicted latent vector. Through our deep neural network based method of training on real and synthesized audio, we are able to predict a latent vector that corresponds to a reasonable reconstruction of real audio. Even though we evaluated our method on WaveGAN, our proposed method is universal and can be applied to any other GANs.



### What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions
- **Arxiv ID**: http://arxiv.org/abs/2010.08539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08539v2)
- **Published**: 2020-10-16 17:46:53+00:00
- **Updated**: 2021-03-06 19:28:58+00:00
- **Authors**: Kiana Ehsani, Daniel Gordon, Thomas Nguyen, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: Published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our "muscly-supervised" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.



### CT Image Segmentation for Inflamed and Fibrotic Lungs Using a Multi-Resolution Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2010.08582v2
- **DOI**: 10.1038/s41598-020-80936-4
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08582v2)
- **Published**: 2020-10-16 18:25:59+00:00
- **Updated**: 2021-01-14 21:09:48+00:00
- **Authors**: Sarah E. Gerard, Jacob Herrmann, Yi Xin, Kevin T. Martin, Emanuele Rezoagli, Davide Ippolito, Giacomo Bellani, Maurizio Cereda, Junfeng Guo, Eric A. Hoffman, David W. Kaczka, Joseph M. Reinhardt
- **Comment**: None
- **Journal**: Sci Rep 11, 1455 (2021)
- **Summary**: The purpose of this study was to develop a fully-automated segmentation algorithm, robust to various density enhancing lung abnormalities, to facilitate rapid quantitative analysis of computed tomography images. A polymorphic training approach is proposed, in which both specifically labeled left and right lungs of humans with COPD, and nonspecifically labeled lungs of animals with acute lung injury, were incorporated into training a single neural network. The resulting network is intended for predicting left and right lung regions in humans with or without diffuse opacification and consolidation. Performance of the proposed lung segmentation algorithm was extensively evaluated on CT scans of subjects with COPD, confirmed COVID-19, lung cancer, and IPF, despite no labeled training data of the latter three diseases. Lobar segmentations were obtained using the left and right lung segmentation as input to the LobeNet algorithm. Regional lobar analysis was performed using hierarchical clustering to identify radiographic subtypes of COVID-19. The proposed lung segmentation algorithm was quantitatively evaluated using semi-automated and manually-corrected segmentations in 87 COVID-19 CT images, achieving an average symmetric surface distance of $0.495 \pm 0.309$ mm and Dice coefficient of $0.985 \pm 0.011$. Hierarchical clustering identified four radiographical phenotypes of COVID-19 based on lobar fractions of consolidated and poorly aerated tissue. Lower left and lower right lobes were consistently more afflicted with poor aeration and consolidation. However, the most severe cases demonstrated involvement of all lobes. The polymorphic training approach was able to accurately segment COVID-19 cases with diffuse consolidation without requiring COVID-19 cases for training.



### A general approach to compute the relevance of middle-level input features
- **Arxiv ID**: http://arxiv.org/abs/2010.08639v2
- **DOI**: 10.1007/978-3-030-68796-0_14
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08639v2)
- **Published**: 2020-10-16 21:46:50+00:00
- **Updated**: 2021-01-28 00:05:16+00:00
- **Authors**: Andrea Apicella, Salvatore Giugliano, Francesco Isgrò, Roberto Prevete
- **Comment**: Presented on the Explainable Deep Learning/AI (EDL/AI) Workshop
  during the 25th International Conference on Pattern Recognition (ICPR2020)
- **Journal**: Pattern Recognition. ICPR International Workshops and Challenges,
  2021, Springer International Publishing, pag. 189-203, vol III
- **Summary**: This work proposes a novel general framework, in the context of eXplainable Artificial Intelligence (XAI), to construct explanations for the behaviour of Machine Learning (ML) models in terms of middle-level features. One can isolate two different ways to provide explanations in the context of XAI: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ML model responses has never been proposed in the literature.



### Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels
- **Arxiv ID**: http://arxiv.org/abs/2010.08644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08644v1)
- **Published**: 2020-10-16 22:06:43+00:00
- **Updated**: 2020-10-16 22:06:43+00:00
- **Authors**: Xiangwei Shi, Seyran Khademi, Yunqiang Li, Jan van Gemert
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Current weakly supervised object localization and segmentation rely on class-discriminative visualization techniques to generate pseudo-labels for pixel-level training. Such visualization methods, including class activation mapping (CAM) and Grad-CAM, use only the deepest, lowest resolution convolutional layer, missing all information in intermediate layers. We propose Zoom-CAM: going beyond the last lowest resolution layer by integrating the importance maps over all activations in intermediate layers. Zoom-CAM captures fine-grained small-scale objects for various discriminative class instances, which are commonly missed by the baseline visualization methods. We focus on generating pixel-level pseudo-labels from class labels. The quality of our pseudo-labels evaluated on the ImageNet localization task exhibits more than 2.8% improvement on top-1 error. For weakly supervised semantic segmentation our generated pseudo-labels improve a state of the art model by 1.1%.



### Ensembling Low Precision Models for Binary Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.08648v1
- **DOI**: 10.1109/WACV48630.2021.00037
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08648v1)
- **Published**: 2020-10-16 22:12:20+00:00
- **Updated**: 2020-10-16 22:12:20+00:00
- **Authors**: Tianyu Ma, Hang Zhang, Hanley Ong, Amar Vora, Thanh D. Nguyen, Ajay Gupta, Yi Wang, Mert Sabuncu
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Segmentation of anatomical regions of interest such as vessels or small lesions in medical images is still a difficult problem that is often tackled with manual input by an expert. One of the major challenges for this task is that the appearance of foreground (positive) regions can be similar to background (negative) regions. As a result, many automatic segmentation algorithms tend to exhibit asymmetric errors, typically producing more false positives than false negatives. In this paper, we aim to leverage this asymmetry and train a diverse ensemble of models with very high recall, while sacrificing their precision. Our core idea is straightforward: A diverse ensemble of low precision and high recall models are likely to make different false positive errors (classifying background as foreground in different parts of the image), but the true positives will tend to be consistent. Thus, in aggregate the false positive errors will cancel out, yielding high performance for the ensemble. Our strategy is general and can be applied with any segmentation model. In three different applications (carotid artery segmentation in a neck CT angiography, myocardium segmentation in a cardiovascular MRI and multiple sclerosis lesion segmentation in a brain MRI), we show how the proposed approach can significantly boost the performance of a baseline segmentation method.



### Class-incremental Learning with Pre-allocated Fixed Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2010.08657v3
- **DOI**: 10.1109/ICPR48806.2021.9413299
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08657v3)
- **Published**: 2020-10-16 22:40:28+00:00
- **Updated**: 2023-08-05 11:56:03+00:00
- **Authors**: Federico Pernici, Matteo Bruni, Claudio Baecchi, Francesco Turchini, Alberto Del Bimbo
- **Comment**: ICPR 2021 (figure and typos fixed)
- **Journal**: None
- **Summary**: In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.   In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.   Experiments with public datasets show that the proposed approach is as effective as the expanding classifier while exhibiting novel intriguing properties of the internal feature representation that are otherwise not-existent. Our ablation study on pre-allocating a large number of classes further validates the approach.



### Generalized Intersection Algorithms with Fixpoints for Image Decomposition Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.08661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 65D18 (Primary) 68U10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2010.08661v1)
- **Published**: 2020-10-16 22:55:34+00:00
- **Updated**: 2020-10-16 22:55:34+00:00
- **Authors**: Robin Richter, Duy H. Thai, Stephan F. Huckemann
- **Comment**: 30 pages, 4 figures
- **Journal**: None
- **Summary**: In image processing, classical methods minimize a suitable functional that balances between computational feasibility (convexity of the functional is ideal) and suitable penalties reflecting the desired image decomposition. The fact that algorithms derived from such minimization problems can be used to construct (deep) learning architectures has spurred the development of algorithms that can be trained for a specifically desired image decomposition, e.g. into cartoon and texture. While many such methods are very successful, theoretical guarantees are only scarcely available. To this end, in this contribution, we formalize a general class of intersection point problems encompassing a wide range of (learned) image decomposition models, and we give an existence result for a large subclass of such problems, i.e. giving the existence of a fixpoint of the corresponding algorithm. This class generalizes classical model-based variational problems, such as the TV-l2 -model or the more general TV-Hilbert model. To illustrate the potential for learned algorithms, novel (non learned) choices within our class show comparable results in denoising and texture removal.



### Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2010.08666v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08666v3)
- **Published**: 2020-10-16 23:37:44+00:00
- **Updated**: 2021-10-10 02:26:11+00:00
- **Authors**: Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, Judy Hoffman
- **Comment**: Published at ICCV 2021. Our code is available at
  https://github.com/virajprabhu/CLUE
- **Journal**: None
- **Summary**: Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a maximally-informative subset via active learning (AL). We study the problem of AL under a domain shift, called Active Domain Adaptation (Active DA). We demonstrate how existing AL approaches based solely on model uncertainty or diversity sampling are less effective for Active DA. We propose Clustering Uncertainty-weighted Embeddings (CLUE), a novel label acquisition strategy for Active DA that performs uncertainty-weighted clustering to identify target instances for labeling that are both uncertain under the model and diverse in feature space. CLUE consistently outperforms competing label acquisition strategies for Active DA and AL across learning settings on 6 diverse domain shifts for image classification.



