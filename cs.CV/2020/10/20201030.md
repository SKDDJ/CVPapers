# Arxiv Papers in cs.CV on 2020-10-30
### PIINET: A 360-degree Panoramic Image Inpainting Network Using a Cube Map
- **Arxiv ID**: http://arxiv.org/abs/2010.16003v2
- **DOI**: 10.32604/cmc.2020.012223
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16003v2)
- **Published**: 2020-10-30 00:42:45+00:00
- **Updated**: 2021-01-26 05:20:20+00:00
- **Authors**: Seo Woo Han, Doug Young Suh
- **Comment**: None
- **Journal**: Vol.66, No.1, 2021, pp.213-228
- **Summary**: Inpainting has been continuously studied in the field of computer vision. As artificial intelligence technology developed, deep learning technology was introduced in inpainting research, helping to improve performance. Currently, the input target of an inpainting algorithm using deep learning has been studied from a single image to a video. However, deep learning-based inpainting technology for panoramic images has not been actively studied. We propose a 360-degree panoramic image inpainting method using generative adversarial networks (GANs). The proposed network inputs a 360-degree equirectangular format panoramic image converts it into a cube map format, which has relatively little distortion and uses it as a training network. Since the cube map format is used, the correlation of the six sides of the cube map should be considered. Therefore, all faces of the cube map are used as input for the whole discriminative network, and each face of the cube map is used as input for the slice discriminative network to determine the authenticity of the generated image. The proposed network performed qualitatively better than existing single-image inpainting algorithms and baseline algorithms.



### Loss re-scaling VQA: Revisiting the LanguagePrior Problem from a Class-imbalance View
- **Arxiv ID**: http://arxiv.org/abs/2010.16010v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16010v4)
- **Published**: 2020-10-30 00:57:17+00:00
- **Updated**: 2021-12-14 16:18:33+00:00
- **Authors**: Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Qi Tian, Min Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have pointed out that many well-developed Visual Question Answering (VQA) models are heavily affected by the language prior problem, which refers to making predictions based on the co-occurrence pattern between textual questions and answers instead of reasoning visual contents. To tackle it, most existing methods focus on enhancing visual feature learning to reduce this superficial textual shortcut influence on VQA model decisions. However, limited effort has been devoted to providing an explicit interpretation for its inherent cause. It thus lacks a good guidance for the research community to move forward in a purposeful way, resulting in model construction perplexity in overcoming this non-trivial problem. In this paper, we propose to interpret the language prior problem in VQA from a class-imbalance view. Concretely, we design a novel interpretation scheme whereby the loss of mis-predicted frequent and sparse answers of the same question type is distinctly exhibited during the late training phase. It explicitly reveals why the VQA model tends to produce a frequent yet obviously wrong answer, to a given question whose right answer is sparse in the training set. Based upon this observation, we further develop a novel loss re-scaling approach to assign different weights to each answer based on the training data statistics for computing the final loss. We apply our approach into three baselines and the experimental results on two VQA-CP benchmark datasets evidently demonstrate its effectiveness. In addition, we also justify the validity of the class imbalance interpretation scheme on other computer vision tasks, such as face recognition and image classification.



### SMOT: Single-Shot Multi Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.16031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16031v1)
- **Published**: 2020-10-30 02:46:54+00:00
- **Updated**: 2020-10-30 02:46:54+00:00
- **Authors**: Wei Li, Yuanjun Xiong, Shuo Yang, Siqi Deng, Wei Xia
- **Comment**: None
- **Journal**: None
- **Summary**: We present single-shot multi-object tracker (SMOT), a new tracking framework that converts any single-shot detector (SSD) model into an online multiple object tracker, which emphasizes simultaneously detecting and tracking of the object paths. Contrary to the existing tracking by detection approaches which suffer from errors made by the object detectors, SMOT adopts the recently proposed scheme of tracking by re-detection. We combine this scheme with SSD detectors by proposing a novel tracking anchor assignment module. With this design SMOT is able to generate tracklets with a constant per-frame runtime. A light-weighted linkage algorithm is then used for online tracklet linking. On three benchmarks of object tracking: Hannah, Music Videos, and MOT17, the proposed SMOT achieves state-of-the-art performance.



### FLANNEL: Focal Loss Based Neural Network Ensemble for COVID-19 Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.16039v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.16039v1)
- **Published**: 2020-10-30 03:17:31+00:00
- **Updated**: 2020-10-30 03:17:31+00:00
- **Authors**: Zhi Qiao, Austin Bae, Lucas M. Glass, Cao Xiao, Jimeng Sun
- **Comment**: None
- **Journal**: None
- **Summary**: To test the possibility of differentiating chest x-ray images of COVID-19 against other pneumonia and healthy patients using deep neural networks. We construct the X-ray imaging data from two publicly available sources, which include 5508 chest x-ray images across 2874 patients with four classes: normal, bacterial pneumonia, non-COVID-19 viral pneumonia, and COVID-19. To identify COVID-19, we propose a Focal Loss Based Neural Ensemble Network (FLANNEL), a flexible module to ensemble several convolutional neural network (CNN) models and fuse with a focal loss for accurate COVID-19 detection on class imbalance data. FLANNEL consistently outperforms baseline models on COVID-19 identification task in all metrics. Compared with the best baseline, FLANNEL shows a higher macro-F1 score with 6% relative increase on Covid-19 identification task where it achieves 0.7833(0.07) in Precision, 0.8609(0.03) in Recall, and 0.8168(0.03) F1 score.



### Road Damage Detection using Deep Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.00728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.00728v1)
- **Published**: 2020-10-30 03:18:14+00:00
- **Updated**: 2020-10-30 03:18:14+00:00
- **Authors**: Keval Doshi, Yasin Yilmaz
- **Comment**: Submitted to IEEE BigData 2020. arXiv admin note: text overlap with
  arXiv:2008.13101, arXiv:1811.04535 by other authors
- **Journal**: None
- **Summary**: Road damage detection is critical for the maintenance of a road, which traditionally has been performed using expensive high-performance sensors. With the recent advances in technology, especially in computer vision, it is now possible to detect and categorize different types of road damages, which can facilitate efficient maintenance and resource management. In this work, we present an ensemble model for efficient detection and classification of road damages, which we have submitted to the IEEE BigData Cup Challenge 2020. Our solution utilizes a state-of-the-art object detector known as You Only Look Once (YOLO-v4), which is trained on images of various types of road damages from Czech, Japan and India. Our ensemble approach was extensively tested with several different model versions and it was able to achieve an F1 score of 0.628 on the test 1 dataset and 0.6358 on the test 2 dataset.



### COVID-FACT: A Fully-Automated Capsule Network-based Framework for Identification of COVID-19 Cases from Chest CT scans
- **Arxiv ID**: http://arxiv.org/abs/2010.16041v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.16041v1)
- **Published**: 2020-10-30 03:30:22+00:00
- **Updated**: 2020-10-30 03:30:22+00:00
- **Authors**: Shahin Heidarian, Parnian Afshar, Nastaran Enshaei, Farnoosh Naderkhani, Anastasia Oikonomou, S. Farokh Atashzar, Faranak Babaki Fard, Kaveh Samimi, Konstantinos N. Plataniotis, Arash Mohammadi, Moezedin Javad Rafiee
- **Comment**: None
- **Journal**: None
- **Summary**: The newly discovered Corona virus Disease 2019 (COVID-19) has been globally spreading and causing hundreds of thousands of deaths around the world as of its first emergence in late 2019. Computed tomography (CT) scans have shown distinctive features and higher sensitivity compared to other diagnostic tests, in particular the current gold standard, i.e., the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Current deep learning-based algorithms are mainly developed based on Convolutional Neural Networks (CNNs) to identify COVID-19 pneumonia cases. CNNs, however, require extensive data augmentation and large datasets to identify detailed spatial relations between image instances. Furthermore, existing algorithms utilizing CT scans, either extend slice-level predictions to patient-level ones using a simple thresholding mechanism or rely on a sophisticated infection segmentation to identify the disease. In this paper, we propose a two-stage fully-automated CT-based framework for identification of COVID-19 positive cases referred to as the "COVID-FACT". COVID-FACT utilizes Capsule Networks, as its main building blocks and is, therefore, capable of capturing spatial information. In particular, to make the proposed COVID-FACT independent from sophisticated segmentation of the area of infection, slices demonstrating infection are detected at the first stage and the second stage is responsible for classifying patients into COVID and non-COVID cases. COVID-FACT detects slices with infection, and identifies positive COVID-19 cases using an in-house CT scan dataset, containing COVID-19, community acquired pneumonia, and normal cases. Based on our experiments, COVID-FACT achieves an accuracy of 90.82%, a sensitivity of 94.55%, a specificity of 86.04%, and an Area Under the Curve (AUC) of 0.98, while depending on far less supervision and annotation, in comparison to its counterparts.



### CT-CAPS: Feature Extraction-based Automated Framework for COVID-19 Disease Identification from Chest CT Scans using Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.16043v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.16043v1)
- **Published**: 2020-10-30 03:35:29+00:00
- **Updated**: 2020-10-30 03:35:29+00:00
- **Authors**: Shahin Heidarian, Parnian Afshar, Arash Mohammadi, Moezedin Javad Rafiee, Anastasia Oikonomou, Konstantinos N. Plataniotis, Farnoosh Naderkhani
- **Comment**: None
- **Journal**: None
- **Summary**: The global outbreak of the novel corona virus (COVID-19) disease has drastically impacted the world and led to one of the most challenging crisis across the globe since World War II. The early diagnosis and isolation of COVID-19 positive cases are considered as crucial steps towards preventing the spread of the disease and flattening the epidemic curve. Chest Computed Tomography (CT) scan is a highly sensitive, rapid, and accurate diagnostic technique that can complement Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Recently, deep learning-based models, mostly based on Convolutional Neural Networks (CNN), have shown promising diagnostic results. CNNs, however, are incapable of capturing spatial relations between image instances and require large datasets. Capsule Networks, on the other hand, can capture spatial relations, require smaller datasets, and have considerably fewer parameters. In this paper, a Capsule network framework, referred to as the "CT-CAPS", is presented to automatically extract distinctive features of chest CT scans. These features, which are extracted from the layer before the final capsule layer, are then leveraged to differentiate COVID-19 from Non-COVID cases. The experiments on our in-house dataset of 307 patients show the state-of-the-art performance with the accuracy of 90.8%, sensitivity of 94.5%, and specificity of 86.0%.



### LIFI: Towards Linguistically Informed Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2010.16078v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16078v5)
- **Published**: 2020-10-30 05:02:23+00:00
- **Updated**: 2020-12-02 16:47:06+00:00
- **Authors**: Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah, Roger Zimmermann
- **Comment**: 9 pages, 7 tables, 4 figures
- **Journal**: None
- **Summary**: In this work, we explore a new problem of frame interpolation for speech videos. Such content today forms the major form of online communication. We try to solve this problem by using several deep learning video generation algorithms to generate the missing frames. We also provide examples where computer vision models despite showing high performance on conventional non-linguistic metrics fail to accurately produce faithful interpolation of speech. With this motivation, we provide a new set of linguistically-informed metrics specifically targeted to the problem of speech videos interpolation. We also release several datasets to test computer vision video generation models of their speech understanding.



### Correspondence Matrices are Underrated
- **Arxiv ID**: http://arxiv.org/abs/2010.16085v1
- **DOI**: 10.1109/3DV50981.2020.00070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16085v1)
- **Published**: 2020-10-30 05:31:50+00:00
- **Updated**: 2020-10-30 05:31:50+00:00
- **Authors**: Tejas Zodage, Rahul Chakwate, Vinit Sarode, Rangaprasad Arun Srivatsan, Howie Choset
- **Comment**: Accepted in the conference, 3DV 2020. 8 pages + 2 supplementary pages
  + 2 reference papers
- **Journal**: International Conference on 3D Vision (2020) 603-612
- **Summary**: Point-cloud registration (PCR) is an important task in various applications such as robotic manipulation, augmented and virtual reality, SLAM, etc. PCR is an optimization problem involving minimization over two different types of interdependent variables: transformation parameters and point-to-point correspondences. Recent developments in deep-learning have produced computationally fast approaches for PCR. The loss functions that are optimized in these networks are based on the error in the transformation parameters. We hypothesize that these methods would perform significantly better if they calculated their loss function using correspondence error instead of only using error in transformation parameters. We define correspondence error as a metric based on incorrectly matched point pairs. We provide a fundamental explanation for why this is the case and test our hypothesis by modifying existing methods to use correspondence-based loss instead of transformation-based loss. These experiments show that the modified networks converge faster and register more accurately even at larger misalignment when compared to the original networks.



### An Unsupervised Approach towards Varying Human Skin Tone Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.16092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16092v1)
- **Published**: 2020-10-30 06:27:03+00:00
- **Updated**: 2020-10-30 06:27:03+00:00
- **Authors**: Debapriya Roy, Diganta Mukherjee, Bhabatosh Chanda
- **Comment**: Accepted in International Conference on Pattern Recognition 2020
- **Journal**: None
- **Summary**: With the increasing popularity of augmented and virtual reality, retailers are now focusing more towards customer satisfaction to increase the amount of sales. Although augmented reality is not a new concept but it has gained much needed attention over the past few years. Our present work is targeted towards this direction which may be used to enhance user experience in various virtual and augmented reality based applications. We propose a model to change skin tone of a person. Given any input image of a person or a group of persons with some value indicating the desired change of skin color towards fairness or darkness, this method can change the skin tone of the persons in the image. This is an unsupervised method and also unconstrained in terms of pose, illumination, number of persons in the image etc. The goal of this work is to reduce the time and effort which is generally required for changing the skin tone using existing applications (e.g., Photoshop) by professionals or novice. To establish the efficacy of this method we have compared our result with that of some popular photo editor and also with the result of some existing benchmark method related to human attribute manipulation. Rigorous experiments on different datasets show the effectiveness of this method in terms of synthesizing perceptually convincing outputs.



### Perception Improvement for Free: Exploring Imperceptible Black-box Adversarial Attacks on Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.05254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05254v1)
- **Published**: 2020-10-30 07:17:12+00:00
- **Updated**: 2020-10-30 07:17:12+00:00
- **Authors**: Yongwei Wang, Mingquan Feng, Rabab Ward, Z. Jane Wang, Lanjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial attacks. White-box adversarial attacks can fool neural networks with small adversarial perturbations, especially for large size images. However, keeping successful adversarial perturbations imperceptible is especially challenging for transfer-based black-box adversarial attacks. Often such adversarial examples can be easily spotted due to their unpleasantly poor visual qualities, which compromises the threat of adversarial attacks in practice. In this study, to improve the image quality of black-box adversarial examples perceptually, we propose structure-aware adversarial attacks by generating adversarial images based on psychological perceptual models. Specifically, we allow higher perturbations on perceptually insignificant regions, while assigning lower or no perturbation on visually sensitive regions. In addition to the proposed spatial-constrained adversarial perturbations, we also propose a novel structure-aware frequency adversarial attack method in the discrete cosine transform (DCT) domain. Since the proposed attacks are independent of the gradient estimation, they can be directly incorporated with existing gradient-based attacks. Experimental results show that, with the comparable attack success rate (ASR), the proposed methods can produce adversarial examples with considerably improved visual quality for free. With the comparable perceptual quality, the proposed approaches achieve higher attack success rates: particularly for the frequency structure-aware attacks, the average ASR improves more than 10% over the baseline attacks.



### Classifying Malware Images with Convolutional Neural Network Models
- **Arxiv ID**: http://arxiv.org/abs/2010.16108v1
- **DOI**: 10.6633/IJNS.202011_22(6).17
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16108v1)
- **Published**: 2020-10-30 07:39:30+00:00
- **Updated**: 2020-10-30 07:39:30+00:00
- **Authors**: Ahmed Bensaoud, Nawaf Abudawaood, Jugal Kalita
- **Comment**: 12 pages, 11 Figures
- **Journal**: Vol.22, No.6, PP.1022-1031, Nov. 2020
- **Summary**: Due to increasing threats from malicious software (malware) in both number and complexity, researchers have developed approaches to automatic detection and classification of malware, instead of analyzing methods for malware files manually in a time-consuming effort. At the same time, malware authors have developed techniques to evade signature-based detection techniques used by antivirus companies. Most recently, deep learning is being used in malware classification to solve this issue. In this paper, we use several convolutional neural network (CNN) models for static malware classification. In particular, we use six deep learning models, three of which are past winners of the ImageNet Large-Scale Visual Recognition Challenge. The other three models are CNN-SVM, GRU-SVM and MLP-SVM, which enhance neural models with support vector machines (SVM). We perform experiments using the Malimg dataset, which has malware images that were converted from Portable Executable malware binaries. The dataset is divided into 25 malware families. Comparisons show that the Inception V3 model achieves a test accuracy of 99.24%, which is better than the accuracy of 98.52% achieved by the current state-of-the-art system called the M-CNN model.



### PyraPose: Feature Pyramids for Fast and Accurate Object Pose Estimation under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2010.16117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16117v1)
- **Published**: 2020-10-30 08:26:22+00:00
- **Updated**: 2020-10-30 08:26:22+00:00
- **Authors**: Stefan Thalhammer, Markus Leitner, Timothy Patten, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: Object pose estimation enables robots to understand and interact with their environments. Training with synthetic data is necessary in order to adapt to novel situations. Unfortunately, pose estimation under domain shift, i.e., training on synthetic data and testing in the real world, is challenging. Deep learning-based approaches currently perform best when using encoder-decoder networks but typically do not generalize to new scenarios with different scene characteristics. We argue that patch-based approaches, instead of encoder-decoder networks, are more suited for synthetic-to-real transfer because local to global object information is better represented. To that end, we present a novel approach based on a specialized feature pyramid network to compute multi-scale features for creating pose hypotheses on different feature map resolutions in parallel. Our single-shot pose estimation approach is evaluated on multiple standard datasets and outperforms the state of the art by up to 35%. We also perform grasping experiments in the real world to demonstrate the advantage of using synthetic data to generalize to novel environments.



### Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.16119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16119v1)
- **Published**: 2020-10-30 08:34:35+00:00
- **Updated**: 2020-10-30 08:34:35+00:00
- **Authors**: Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, Liang Lin
- **Comment**: NeurIPS2020
- **Journal**: None
- **Summary**: Panoptic segmentation is posed as a new popular test-bed for the state-of-the-art holistic scene understanding methods with the requirement of simultaneously segmenting both foreground things and background stuff. The state-of-the-art panoptic segmentation network exhibits high structural complexity in different network components, i.e. backbone, proposal-based foreground branch, segmentation-based background branch, and feature fusion module across branches, which heavily relies on expert knowledge and tedious trials. In this work, we propose an efficient, cooperative and highly automated framework to simultaneously search for all main components including backbone, segmentation branches, and feature fusion module in a unified panoptic segmentation pipeline based on the prevailing one-shot Network Architecture Search (NAS) paradigm. Notably, we extend the common single-task NAS into the multi-component scenario by taking the advantage of the newly proposed intra-modular search space and problem-oriented inter-modular search space, which helps us to obtain an optimal network architecture that not only performs well in both instance segmentation and semantic segmentation tasks but also be aware of the reciprocal relations between foreground things and background stuff classes. To relieve the vast computation burden incurred by applying NAS to complicated network architectures, we present a novel path-priority greedy search policy to find a robust, transferrable architecture with significantly reduced searching overhead. Our searched architecture, namely Auto-Panoptic, achieves the new state-of-the-art on the challenging COCO and ADE20K benchmarks. Moreover, extensive experiments are conducted to demonstrate the effectiveness of path-priority policy and transferability of Auto-Panoptic across different datasets. Codes and models are available at: https://github.com/Jacobew/AutoPanoptic.



### Bayesian Optimization Meets Laplace Approximation for Robotic Introspection
- **Arxiv ID**: http://arxiv.org/abs/2010.16141v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.16141v1)
- **Published**: 2020-10-30 09:28:10+00:00
- **Updated**: 2020-10-30 09:28:10+00:00
- **Authors**: Matthias Humt, Jongseok Lee, Rudolph Triebel
- **Comment**: Accepted to the IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2020 Long-Term Autonomy Workshop
- **Journal**: None
- **Summary**: In robotics, deep learning (DL) methods are used more and more widely, but their general inability to provide reliable confidence estimates will ultimately lead to fragile and unreliable systems. This impedes the potential deployments of DL methods for long-term autonomy. Therefore, in this paper we introduce a scalable Laplace Approximation (LA) technique to make Deep Neural Networks (DNNs) more introspective, i.e. to enable them to provide accurate assessments of their failure probability for unseen test data. In particular, we propose a novel Bayesian Optimization (BO) algorithm to mitigate their tendency of under-fitting the true weight posterior, so that both the calibration and the accuracy of the predictions can be simultaneously optimized. We demonstrate empirically that the proposed BO approach requires fewer iterations for this when compared to random search, and we show that the proposed framework can be scaled up to large datasets and architectures.



### Small Noisy and Perspective Face Detection using Deformable Symmetric Gabor Wavelet Network
- **Arxiv ID**: http://arxiv.org/abs/2010.16164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16164v1)
- **Published**: 2020-10-30 10:07:34+00:00
- **Updated**: 2020-10-30 10:07:34+00:00
- **Authors**: Sherzod Salokhiddinov, Seungkyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection and tracking in low resolution image is not a trivial task due to the limitation in the appearance features for face characterization. Moreover, facial expression gives additional distortion on this small and noisy face. In this paper, we propose deformable symmetric Gabor wavelet network face model for face detection in low resolution image. Our model optimizes the rotation, translation, dilation, perspective and partial deformation amount of the face model with symmetry constraints. Symmetry constraints help our model to be more robust to noise and distortion. Experimental results on our low resolution face image dataset and videos show promising face detection and tracking results under various challenging conditions.



### Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2010.16165v2
- **DOI**: 10.1109/TCAD.2020.3013050
- **Categories**: **cs.NE**, cs.CV, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2010.16165v2)
- **Published**: 2020-10-30 10:10:08+00:00
- **Updated**: 2020-11-29 12:50:10+00:00
- **Authors**: Guangli Li, Xiu Ma, Xueying Wang, Lei Liu, Jingling Xue, Xiaobing Feng
- **Comment**: Published in IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems (TCAD)
- **Journal**: None
- **Summary**: The increasing computational cost of deep neural network models limits the applicability of intelligent applications on resource-constrained edge devices. While a number of neural network pruning methods have been proposed to compress the models, prevailing approaches focus only on parametric operators (e.g., convolution), which may miss optimization opportunities. In this paper, we present a novel fusion-catalyzed pruning approach, called FuPruner, which simultaneously optimizes the parametric and non-parametric operators for accelerating neural networks. We introduce an aggressive fusion method to equivalently transform a model, which extends the optimization space of pruning and enables non-parametric operators to be pruned in a similar manner as parametric operators, and a dynamic filter pruning method is applied to decrease the computational cost of models while retaining the accuracy requirement. Moreover, FuPruner provides configurable optimization options for controlling fusion and pruning, allowing much more flexible performance-accuracy trade-offs to be made. Evaluation with state-of-the-art residual neural networks on five representative intelligent edge platforms, Jetson TX2, Jetson Nano, Edge TPU, NCS, and NCS2, demonstrates the effectiveness of our approach, which can accelerate the inference of models on CIFAR-10 and ImageNet datasets.



### Bridging Composite and Real: Towards End-to-end Deep Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2010.16188v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16188v3)
- **Published**: 2020-10-30 10:57:13+00:00
- **Updated**: 2021-10-27 03:31:34+00:00
- **Authors**: Jizhizi Li, Jing Zhang, Stephen J. Maybank, Dacheng Tao
- **Comment**: Accepted by the International Journal of Computer Vision (IJCV). Both
  the datasets and source code are available at
  https://github.com/JizhiziLi/GFM
- **Journal**: None
- **Summary**: Extracting accurate foregrounds from natural images benefits many downstream applications such as film production and augmented reality. However, the furry characteristics and various appearance of the foregrounds, e.g., animal and portrait, challenge existing matting methods, which usually require extra user inputs such as trimap or scribbles. To resolve these problems, we study the distinct roles of semantics and details for image matting and decompose the task into two parallel sub-tasks: high-level semantic segmentation and low-level details matting. Specifically, we propose a novel Glance and Focus Matting network (GFM), which employs a shared encoder and two separate decoders to learn both tasks in a collaborative manner for end-to-end natural image matting. Besides, due to the limitation of available natural images in the matting task, previous methods typically adopt composite images for training and evaluation, which result in limited generalization ability on real-world images. In this paper, we investigate the domain gap issue between composite images and real-world images systematically by conducting comprehensive analyses of various discrepancies between the foreground and background images. We find that a carefully designed composition route RSSN that aims to reduce the discrepancies can lead to a better model with remarkable generalization ability. Furthermore, we provide a benchmark containing 2,000 high-resolution real-world animal images and 10,000 portrait images along with their manually labeled alpha mattes to serve as a test bed for evaluating matting model's generalization ability on real-world images. Comprehensive empirical studies have demonstrated that GFM outperforms state-of-the-art methods and effectively reduces the generalization error. The code and the datasets will be released at https://github.com/JizhiziLi/GFM.



### Automatic Myocardial Infarction Evaluation from Delayed-Enhancement Cardiac MRI using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.16198v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16198v1)
- **Published**: 2020-10-30 11:18:25+00:00
- **Updated**: 2020-10-30 11:18:25+00:00
- **Authors**: Kibrom Berihu Girum, Youssef Skandarani, Raabid Hussain, Alexis Bozorg Grayeli, Gilles Créhange, Alain Lalande
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new deep learning framework for an automatic myocardial infarction evaluation from clinical information and delayed enhancement-MRI (DE-MRI). The proposed framework addresses two tasks. The first task is automatic detection of myocardial contours, the infarcted area, the no-reflow area, and the left ventricular cavity from a short-axis DE-MRI series. It employs two segmentation neural networks. The first network is used to segment the anatomical structures such as the myocardium and left ventricular cavity. The second network is used to segment the pathological areas such as myocardial infarction, myocardial no-reflow, and normal myocardial region. The segmented myocardium region from the first network is further used to refine the second network's pathological segmentation results. The second task is to automatically classify a given case into normal or pathological from clinical information with or without DE-MRI. A cascaded support vector machine (SVM) is employed to classify a given case from its associated clinical information. The segmented pathological areas from DE-MRI are also used for the classification task. We evaluated our method on the 2020 EMIDEC MICCAI challenge dataset. It yielded an average Dice index of 0.93 and 0.84, respectively, for the left ventricular cavity and the myocardium. The classification from using only clinical information yielded 80% accuracy over five-fold cross-validation. Using the DE-MRI, our method can classify the cases with 93.3% accuracy. These experimental results reveal that the proposed method can automatically evaluate the myocardial infarction.



### Statistical Analysis of Signal-Dependent Noise: Application in Blind Localization of Image Splicing Forgery
- **Arxiv ID**: http://arxiv.org/abs/2010.16211v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16211v2)
- **Published**: 2020-10-30 11:53:53+00:00
- **Updated**: 2020-11-02 09:34:16+00:00
- **Authors**: Mian Zou, Heng Yao, Chuan Qin, Xinpeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual noise is often regarded as a disturbance in image quality, whereas it can also provide a crucial clue for image-based forensic tasks. Conventionally, noise is assumed to comprise an additive Gaussian model to be estimated and then used to reveal anomalies. However, for real sensor noise, it should be modeled as signal-dependent noise (SDN). In this work, we apply SDN to splicing forgery localization tasks. Through statistical analysis of the SDN model, we assume that noise can be modeled as a Gaussian approximation for a certain brightness and propose a likelihood model for a noise level function. By building a maximum a posterior Markov random field (MAP-MRF) framework, we exploit the likelihood of noise to reveal the alien region of spliced objects, with a probability combination refinement strategy. To ensure a completely blind detection, an iterative alternating method is adopted to estimate the MRF parameters. Experimental results demonstrate that our method is effective and provides a comparative localization performance.



### HOI Analysis: Integrating and Decomposing Human-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2010.16219v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16219v2)
- **Published**: 2020-10-30 12:22:38+00:00
- **Updated**: 2020-11-07 11:39:56+00:00
- **Authors**: Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Cewu Lu
- **Comment**: Accepted in NeurIPS 2020. Code:
  github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network),
  Project: https://github.com/DirtyHarryLYL/HAKE-Action-Torch
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) consists of human, object and implicit interaction/verb. Different from previous methods that directly map pixels to HOI semantics, we propose a novel perspective for HOI learning in an analytical manner. In analogy to Harmonic Analysis, whose goal is to study how to represent the signals with the superposition of basic waves, we propose the HOI Analysis. We argue that coherent HOI can be decomposed into isolated human and object. Meanwhile, isolated human and object can also be integrated into coherent HOI again. Moreover, transformations between human-object pairs with the same HOI can also be easier approached with integration and decomposition. As a result, the implicit verb will be represented in the transformation function space. In light of this, we propose an Integration-Decomposition Network (IDN) to implement the above transformations and achieve state-of-the-art performance on widely-used HOI detection benchmarks. Code is available at https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network).



### Experimental design for MRI by greedy policy search
- **Arxiv ID**: http://arxiv.org/abs/2010.16262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.16262v2)
- **Published**: 2020-10-30 13:38:09+00:00
- **Updated**: 2020-12-15 11:12:46+00:00
- **Authors**: Tim Bakker, Herke van Hoof, Max Welling
- **Comment**: Accepted to NeurIPS 2020 (spotlight), 15-12-2020: Fixed typos, Figure
  9, and pseudocode
- **Journal**: None
- **Summary**: In today's clinical practice, magnetic resonance imaging (MRI) is routinely accelerated through subsampling of the associated Fourier domain. Currently, the construction of these subsampling strategies - known as experimental design - relies primarily on heuristics. We propose to learn experimental design strategies for accelerated MRI with policy gradient methods. Unexpectedly, our experiments show that a simple greedy approximation of the objective leads to solutions nearly on-par with the more general non-greedy approach. We offer a partial explanation for this phenomenon rooted in greater variance in the non-greedy objective's gradient estimates, and experimentally verify that this variance hampers non-greedy models in adapting their policies to individual MR images. We empirically show that this adaptivity is key to improving subsampling designs.



### Exploring Dynamic Context for Multi-path Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.16267v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2010.16267v3)
- **Published**: 2020-10-30 13:39:20+00:00
- **Updated**: 2021-03-24 10:28:47+00:00
- **Authors**: Hao Cheng, Wentong Liao, Xuejiao Tang, Michael Ying Yang, Monika Sester, Bodo Rosenhahn
- **Comment**: accpeted by ICRA 2021, code available
- **Journal**: None
- **Summary**: To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https://github.com/wtliao/DCENet.



### 3D Object Recognition By Corresponding and Quantizing Neural 3D Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/2010.16279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16279v1)
- **Published**: 2020-10-30 13:56:09+00:00
- **Updated**: 2020-10-30 13:56:09+00:00
- **Authors**: Mihir Prabhudesai, Shamit Lal, Hsiao-Yu Fish Tung, Adam W. Harley, Shubhankar Potdar, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a system that learns to detect objects and infer their 3D poses in RGB-D images. Many existing systems can identify objects and infer 3D poses, but they heavily rely on human labels and 3D annotations. The challenge here is to achieve this without relying on strong supervision signals. To address this challenge, we propose a model that maps RGB-D images to a set of 3D visual feature maps in a differentiable fully-convolutional manner, supervised by predicting views. The 3D feature maps correspond to a featurization of the 3D world scene depicted in the images. The object 3D feature representations are invariant to camera viewpoint changes or zooms, which means feature matching can identify similar objects under different camera viewpoints. We can compare the 3D feature maps of two objects by searching alignment across scales and 3D rotations, and, as a result of the operation, we can estimate pose and scale changes without the need for 3D pose annotations. We cluster object feature maps into a set of 3D prototypes that represent familiar objects in canonical scales and orientations. We then parse images by inferring the prototype identity and 3D pose for each detected object. We compare our method to numerous baselines that do not learn 3D feature visual representations or do not attempt to correspond features across scenes, and outperform them by a large margin in the tasks of object retrieval and object pose estimation. Thanks to the 3D nature of the object-centric feature maps, the visual similarity cues are invariant to 3D pose changes or small scale changes, which gives our method an advantage over 2D and 1D methods.



### Quasiconformal model with CNN features for large deformation image registration
- **Arxiv ID**: http://arxiv.org/abs/2011.00731v3
- **DOI**: 10.3934/ipi.2022010
- **Categories**: **cs.CV**, cs.CG, cs.LG, math.CV, 65D18, 68U05, 68U10, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2011.00731v3)
- **Published**: 2020-10-30 14:15:31+00:00
- **Updated**: 2021-06-28 10:59:26+00:00
- **Authors**: Ho Law, Gary P. T. Choi, Ka Chun Lam, Lok Ming Lui
- **Comment**: None
- **Journal**: Inverse Problems and Imaging, 16(4), 1019-1046 (2022)
- **Summary**: Image registration has been widely studied over the past several decades, with numerous applications in science, engineering and medicine. Most of the conventional mathematical models for large deformation image registration rely on prescribed landmarks, which usually require tedious manual labeling and are prone to error. In recent years, there has been a surge of interest in the use of machine learning for image registration. In this paper, we develop a novel method for large deformation image registration by a fusion of quasiconformal theory and convolutional neural network (CNN). More specifically, we propose a quasiconformal energy model with a novel fidelity term that incorporates the features extracted using a pre-trained CNN, thereby allowing us to obtain meaningful registration results without any guidance of prescribed landmarks. Moreover, unlike many prior image registration methods, the bijectivity of our method is guaranteed by quasiconformal theory. Experimental results are presented to demonstrate the effectiveness of the proposed method. More broadly, our work sheds light on how rigorous mathematical theories and practical machine learning approaches can be integrated for developing computational methods with improved performance.



### All-Weather Object Recognition Using Radar and Infrared Sensing
- **Arxiv ID**: http://arxiv.org/abs/2010.16285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.16285v1)
- **Published**: 2020-10-30 14:16:39+00:00
- **Updated**: 2020-10-30 14:16:39+00:00
- **Authors**: Marcel Sheeny
- **Comment**: 201 pages, PhD Thesis, Heriot-Watt University, October 2020. Includes
  results from arXiv:1804.02576, arXiv:1912.03157,
  https://www.mdpi.com/2076-3417/10/11/3861 and arXiv:2010.09076
- **Journal**: None
- **Summary**: Autonomous cars are an emergent technology which has the capacity to change human lives. The current sensor systems which are most capable of perception are based on optical sensors. For example, deep neural networks show outstanding results in recognising objects when used to process data from cameras and Light Detection And Ranging (LiDAR) sensors. However these sensors perform poorly under adverse weather conditions such as rain, fog, and snow due to the sensor wavelengths. This thesis explores new sensing developments based on long wave polarised infrared (IR) imagery and imaging radar to recognise objects. First, we developed a methodology based on Stokes parameters using polarised infrared data to recognise vehicles using deep neural networks. Second, we explored the potential of using only the power spectrum captured by low-THz radar sensors to perform object recognition in a controlled scenario. This latter work is based on a data-driven approach together with the development of a data augmentation method based on attenuation, range and speckle noise. Last, we created a new large-scale dataset in the "wild" with many different weather scenarios (sunny, overcast, night, fog, rain and snow) showing radar robustness to detect vehicles in adverse weather. High resolution radar and polarised IR imagery, combined with a deep learning approach, are shown as a potential alternative to current automotive sensing systems based on visible spectrum optical technology as they are more robust in severe weather and adverse light conditions.



### Surgical Data Science -- from Concepts toward Clinical Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.02284v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02284v2)
- **Published**: 2020-10-30 14:20:16+00:00
- **Updated**: 2021-07-30 20:48:03+00:00
- **Authors**: Lena Maier-Hein, Matthias Eisenmann, Duygu Sarikaya, Keno März, Toby Collins, Anand Malpani, Johannes Fallert, Hubertus Feussner, Stamatia Giannarou, Pietro Mascagni, Hirenkumar Nakawala, Adrian Park, Carla Pugh, Danail Stoyanov, Swaroop S. Vedula, Kevin Cleary, Gabor Fichtinger, Germain Forestier, Bernard Gibaud, Teodor Grantcharov, Makoto Hashizume, Doreen Heckmann-Nötzel, Hannes G. Kenngott, Ron Kikinis, Lars Mündermann, Nassir Navab, Sinan Onogur, Raphael Sznitman, Russell H. Taylor, Minu D. Tizabi, Martin Wagner, Gregory D. Hager, Thomas Neumuth, Nicolas Padoy, Justin Collins, Ines Gockel, Jan Goedeke, Daniel A. Hashimoto, Luc Joyeux, Kyle Lam, Daniel R. Leff, Amin Madani, Hani J. Marcus, Ozanan Meireles, Alexander Seitel, Dogu Teber, Frank Ückert, Beat P. Müller-Stich, Pierre Jannin, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.



### OpenKinoAI: An Open Source Framework for Intelligent Cinematography and Editing of Live Performances
- **Arxiv ID**: http://arxiv.org/abs/2011.05203v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.05203v1)
- **Published**: 2020-10-30 14:21:53+00:00
- **Updated**: 2020-10-30 14:21:53+00:00
- **Authors**: Rémi Ronfard, Rémi Colin de Verdière
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: OpenKinoAI is an open source framework for post-production of ultra high definition video which makes it possible to emulate professional multiclip editing techniques for the case of single camera recordings. OpenKinoAI includes tools for uploading raw video footage of live performances on a remote web server, detecting, tracking and recognizing the performers in the original material, reframing the raw video into a large choice of cinematographic rushes, editing the rushes into movies, and annotating rushes and movies for documentation purposes. OpenKinoAI is made available to promote research in multiclip video editing of ultra high definition video, and to allow performing artists and companies to use this research for archiving, documenting and sharing their work online in an innovative fashion.



### Brain tumor segmentation with self-ensembled, deeply-supervised 3D U-net neural networks: a BraTS 2020 challenge solution
- **Arxiv ID**: http://arxiv.org/abs/2011.01045v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01045v2)
- **Published**: 2020-10-30 14:36:10+00:00
- **Updated**: 2020-11-27 15:58:32+00:00
- **Authors**: Theophraste Henry, Alexandre Carre, Marvin Lerousseau, Theo Estienne, Charlotte Robert, Nikos Paragios, Eric Deutsch
- **Comment**: BraTS 2020 proceedings (LNCS) paper
- **Journal**: None
- **Summary**: Brain tumor segmentation is a critical task for patient's disease management. In order to automate and standardize this task, we trained multiple U-net like neural networks, mainly with deep supervision and stochastic weight averaging, on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 training dataset. Two independent ensembles of models from two different training pipelines were trained, and each produced a brain tumor segmentation map. These two labelmaps per patient were then merged, taking into account the performance of each ensemble for specific tumor subregions. Our performance on the online validation dataset with test time augmentation were as follows: Dice of 0.81, 0.91 and 0.85; Hausdorff (95%) of 20.6, 4,3, 5.7 mm for the enhancing tumor, whole tumor and tumor core, respectively. Similarly, our solution achieved a Dice of 0.79, 0.89 and 0.84, as well as Hausdorff (95%) of 20.4, 6.7 and 19.5mm on the final test dataset, ranking us among the top ten teams. More complicated training schemes and neural network architectures were investigated without significant performance gain at the cost of greatly increased training time. Overall, our approach yielded good and balanced performance for each tumor subregion. Our solution is open sourced at https://github.com/lescientifik/open_brats2020.



### Learning Vision-based Reactive Policies for Obstacle Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2010.16298v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, I.2.9; I.2.6; I.2.10; I.5.0; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2010.16298v1)
- **Published**: 2020-10-30 14:50:32+00:00
- **Updated**: 2020-10-30 14:50:32+00:00
- **Authors**: Elie Aljalbout, Ji Chen, Konstantin Ritt, Maximilian Ulmer, Sami Haddadin
- **Comment**: Accepted for publication at CoRL 2020
- **Journal**: None
- **Summary**: In this paper, we address the problem of vision-based obstacle avoidance for robotic manipulators. This topic poses challenges for both perception and motion generation. While most work in the field aims at improving one of those aspects, we provide a unified framework for approaching this problem. The main goal of this framework is to connect perception and motion by identifying the relationship between the visual input and the corresponding motion representation. To this end, we propose a method for learning reactive obstacle avoidance policies. We evaluate our method on goal-reaching tasks for single and multiple obstacles scenarios. We show the ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate, while maintaining closed-loop responsiveness required for critical applications like human-robot interaction.



### Automatic Counting and Identification of Train Wagons Based on Computer Vision and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.16307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16307v1)
- **Published**: 2020-10-30 14:56:54+00:00
- **Updated**: 2020-10-30 14:56:54+00:00
- **Authors**: Rayson Laroca, Alessander Cidral Boslooper, David Menotti
- **Comment**: An article about the proposed system has been published in the
  October 2020 issue of Railway Gazette International, the leading business
  journal for the worldwide rail industry
- **Journal**: None
- **Summary**: In this work, we present a robust and efficient solution for counting and identifying train wagons using computer vision and deep learning. The proposed solution is cost-effective and can easily replace solutions based on radiofrequency identification (RFID), which are known to have high installation and maintenance costs. According to our experiments, our two-stage methodology achieves impressive results on real-world scenarios, i.e., 100% accuracy in the counting stage and 99.7% recognition rate in the identification one. Moreover, the system is able to automatically reject some of the train wagons successfully counted, as they have damaged identification codes. The results achieved were surprising considering that the proposed system requires low processing power (i.e., it can run in low-end setups) and that we used a relatively small number of images to train our Convolutional Neural Network (CNN) for character recognition. The proposed method is registered, under number BR512020000808-9, with the National Institute of Industrial Property (Brazil).



### DeepWay: a Deep Learning Waypoint Estimator for Global Path Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.16322v2
- **DOI**: 10.1016/j.compag.2021.106091
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16322v2)
- **Published**: 2020-10-30 15:27:42+00:00
- **Updated**: 2021-01-21 17:01:14+00:00
- **Authors**: Vittorio Mazzia, Francesco Salvetti, Diego Aghi, Marcello Chiaberge
- **Comment**: Submitted to Computers and Electronics in Agriculture
- **Journal**: Volume 184, May 2021, 106091
- **Summary**: Agriculture 3.0 and 4.0 have gradually introduced service robotics and automation into several agricultural processes, mostly improving crops quality and seasonal yield. Row-based crops are the perfect settings to test and deploy smart machines capable of monitoring and manage the harvest. In this context, global path generation is essential either for ground or aerial vehicles, and it is the starting point for every type of mission plan. Nevertheless, little attention has been currently given to this problem by the research community and global path generation automation is still far to be solved. In order to generate a viable path for an autonomous machine, the presented research proposes a feature learning fully convolutional model capable of estimating waypoints given an occupancy grid map. In particular, we apply the proposed data-driven methodology to the specific case of row-based crops with the general objective to generate a global path able to cover the extension of the crop completely. Extensive experimentation with a custom made synthetic dataset and real satellite-derived images of different scenarios have proved the effectiveness of our methodology and demonstrated the feasibility of an end-to-end and completely autonomous global path planner.



### Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents
- **Arxiv ID**: http://arxiv.org/abs/2010.16363v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16363v1)
- **Published**: 2020-10-30 16:39:49+00:00
- **Updated**: 2020-10-30 16:39:49+00:00
- **Authors**: Gregory Yauney, Jack Hessel, David Mimno
- **Comment**: None
- **Journal**: Published in EMNLP 2020
- **Summary**: Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as "kitchen" and "bedroom", and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating "granite" with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.



### Development and Evaluation of a Deep Neural Network for Histologic Classification of Renal Cell Carcinoma on Biopsy and Surgical Resection Slides
- **Arxiv ID**: http://arxiv.org/abs/2010.16380v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.16380v1)
- **Published**: 2020-10-30 17:20:49+00:00
- **Updated**: 2020-10-30 17:20:49+00:00
- **Authors**: Mengdan Zhu, Bing Ren, Ryland Richards, Matthew Suriawinata, Naofumi Tomita, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Renal cell carcinoma (RCC) is the most common renal cancer in adults. The histopathologic classification of RCC is essential for diagnosis, prognosis, and management of patients. Reorganization and classification of complex histologic patterns of RCC on biopsy and surgical resection slides under a microscope remains a heavily specialized, error-prone, and time-consuming task for pathologists. In this study, we developed a deep neural network model that can accurately classify digitized surgical resection slides and biopsy slides into five related classes: clear cell RCC, papillary RCC, chromophobe RCC, renal oncocytoma, and normal. In addition to the whole-slide classification pipeline, we visualized the identified indicative regions and features on slides for classification by reprocessing patch-level classification results to ensure the explainability of our diagnostic model. We evaluated our model on independent test sets of 78 surgical resection whole slides and 79 biopsy slides from our tertiary medical institution, and 69 randomly selected surgical resection slides from The Cancer Genome Atlas (TCGA) database. The average area under the curve (AUC) of our classifier on the internal resection slides, internal biopsy slides, and external TCGA slides is 0.98, 0.98 and 0.99, respectively. Our results suggest that the high generalizability of our approach across different data sources and specimen types. More importantly, our model has the potential to assist pathologists by (1) automatically pre-screening slides to reduce false-negative cases, (2) highlighting regions of importance on digitized slides to accelerate diagnosis, and (3) providing objective and accurate diagnosis as the second opinion.



### Wide-angle Image Rectification: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2011.12108v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12108v2)
- **Published**: 2020-10-30 17:28:40+00:00
- **Updated**: 2021-12-01 12:24:03+00:00
- **Authors**: Jinlong Fan, Jing Zhang, Stephen J. Maybank, Dacheng Tao
- **Comment**: Accepted by the International Journal of Computer Vision (IJCV). Both
  the datasets and source code are available at
  https://github.com/loong8888/WAIR
- **Journal**: None
- **Summary**: Wide field-of-view (FOV) cameras, which capture a larger scene area than narrow FOV cameras, are used in many applications including 3D reconstruction, autonomous driving, and video surveillance. However, wide-angle images contain distortions that violate the assumptions underlying pinhole camera models, resulting in object distortion, difficulties in estimating scene distance, area, and direction, and preventing the use of off-the-shelf deep models trained on undistorted images for downstream computer vision tasks. Image rectification, which aims to correct these distortions, can solve these problems. In this paper, we comprehensively survey progress in wide-angle image rectification from transformation models to rectification methods. Specifically, we first present a detailed description and discussion of the camera models used in different approaches. Then, we summarize several distortion models including radial distortion and projection distortion. Next, we review both traditional geometry-based image rectification methods and deep learning-based methods, where the former formulate distortion parameter estimation as an optimization problem and the latter treat it as a regression problem by leveraging the power of deep neural networks. We evaluate the performance of state-of-the-art methods on public datasets and show that although both kinds of methods can achieve good results, these methods only work well for specific camera models and distortion types. We also provide a strong baseline model and carry out an empirical study of different distortion models on synthetic datasets and real-world wide-angle images. Finally, we discuss several potential research directions that are expected to further advance this area in the future.



### Mutual Information-based Disentangled Neural Networks for Classifying Unseen Categories in Different Domains: Application to Fetal Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2011.00739v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.00739v2)
- **Published**: 2020-10-30 17:32:18+00:00
- **Updated**: 2021-04-06 17:11:52+00:00
- **Authors**: Qingjie Meng, Jacqueline Matthew, Veronika A. Zimmer, Alberto Gomez, David F. A. Lloyd, Daniel Rueckert, Bernhard Kainz
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.00321
- **Journal**: None
- **Summary**: Deep neural networks exhibit limited generalizability across images with different entangled domain features and categorical features. Learning generalizable features that can form universal categorical decision boundaries across domains is an interesting and difficult challenge. This problem occurs frequently in medical imaging applications when attempts are made to deploy and improve deep learning models across different image acquisition devices, across acquisition parameters or if some classes are unavailable in new training databases. To address this problem, we propose Mutual Information-based Disentangled Neural Networks (MIDNet), which extract generalizable categorical features to transfer knowledge to unseen categories in a target domain. The proposed MIDNet adopts a semi-supervised learning paradigm to alleviate the dependency on labeled data. This is important for real-world applications where data annotation is time-consuming, costly and requires training and expertise. We extensively evaluate the proposed method on fetal ultrasound datasets for two different image classification tasks where domain features are respectively defined by shadow artifacts and image acquisition devices. Experimental results show that the proposed method outperforms the state-of-the-art on the classification of unseen categories in a target domain with sparsely labeled training data.



### Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss
- **Arxiv ID**: http://arxiv.org/abs/2010.16396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16396v1)
- **Published**: 2020-10-30 17:48:42+00:00
- **Updated**: 2020-10-30 17:48:42+00:00
- **Authors**: Panagiotis Paraskevas Filntisis, Niki Efthymiou, Gerasimos Potamianos, Petros Maragos
- **Comment**: Winner of the First International Workshop on Bodily Expressed
  Emotion Understanding Challenge (ECCVW-2020)
- **Journal**: None
- **Summary**: We present our winning submission to the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Based on recent literature on the effect of context/environment on emotion, as well as visual representations with semantic meaning using word embeddings, we extend the framework of Temporal Segment Network to accommodate these. Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.



### Why Do Better Loss Functions Lead to Less Transferable Features?
- **Arxiv ID**: http://arxiv.org/abs/2010.16402v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.16402v2)
- **Published**: 2020-10-30 17:50:31+00:00
- **Updated**: 2021-11-03 18:32:53+00:00
- **Authors**: Simon Kornblith, Ting Chen, Honglak Lee, Mohammad Norouzi
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.



### Unsupervised Monocular Depth Learning in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.16404v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.16404v2)
- **Published**: 2020-10-30 17:52:27+00:00
- **Updated**: 2020-11-07 19:19:10+00:00
- **Authors**: Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova
- **Comment**: Accepted at 4th Conference on Robot Learning (CoRL 2020)
- **Journal**: None
- **Summary**: We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input. Code is at https://github.com/google-research/google-research/tree/master/depth_and_motion_learning .



### On the Performance of Convolutional Neural Networks under High and Low Frequency Information
- **Arxiv ID**: http://arxiv.org/abs/2011.06496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06496v1)
- **Published**: 2020-10-30 17:54:45+00:00
- **Updated**: 2020-10-30 17:54:45+00:00
- **Authors**: Roshan Reddy Yedla, Shiv Ram Dubey
- **Comment**: Accepted in Fifth IAPR International Conference on Computer Vision
  and Image Processing (CVIP), 2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown very promising performance in recent years for different problems, including object recognition, face recognition, medical image analysis, etc. However, generally the trained CNN models are tested over the test set which is very similar to the trained set. The generalizability and robustness of the CNN models are very important aspects to make it to work for the unseen data. In this letter, we study the performance of CNN models over the high and low frequency information of the images. We observe that the trained CNN fails to generalize over the high and low frequency images. In order to make the CNN robust against high and low frequency images, we propose the stochastic filtering based data augmentation during training. A satisfactory performance improvement has been observed in terms of the high and low frequency generalization and robustness with the proposed stochastic filtering based data augmentation approach. The experimentations are performed using ResNet50 model over the CIFAR-10 dataset and ResNet101 model over Tiny-ImageNet dataset.



### MichiGAN: Multi-Input-Conditioned Hair Image Generation for Portrait Editing
- **Arxiv ID**: http://arxiv.org/abs/2010.16417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.16417v1)
- **Published**: 2020-10-30 17:59:10+00:00
- **Updated**: 2020-10-30 17:59:10+00:00
- **Authors**: Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, Nenghai Yu
- **Comment**: Siggraph 2020, code is available at
  https://github.com/tzt101/MichiGAN
- **Journal**: None
- **Summary**: Despite the recent success of face image generation with GANs, conditional hair editing remains challenging due to the under-explored complexity of its geometry and appearance. In this paper, we present MichiGAN (Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation method for interactive portrait hair manipulation. To provide user control over every major hair visual factor, we explicitly disentangle hair into four orthogonal attributes, including shape, structure, appearance, and background. For each of them, we design a corresponding condition module to represent, process, and convert user inputs, and modulate the image generation pipeline in ways that respect the natures of different visual attributes. All these condition modules are integrated with the backbone generator to form the final end-to-end network, which allows fully-conditioned hair generation from multiple user inputs. Upon it, we also build an interactive portrait hair editing system that enables straightforward manipulation of hair by projecting intuitive and high-level user inputs such as painted masks, guiding strokes, or reference photos to well-defined condition representations. Through extensive experiments and evaluations, we demonstrate the superiority of our method regarding both result quality and user controllability. The code is available at https://github.com/tzt101/MichiGAN.



### Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2011.00043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.00043v1)
- **Published**: 2020-10-30 18:45:16+00:00
- **Updated**: 2020-10-30 18:45:16+00:00
- **Authors**: Zhengyuan Yang, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the human ability to infer emotions from body language, we propose an automated framework for body language based emotion recognition starting from regular RGB videos. In collaboration with psychologists, we further extend the framework for psychiatric symptom prediction. Because a specific application domain of the proposed framework may only supply a limited amount of data, the framework is designed to work on a small training set and possess a good transferability. The proposed system in the first stage generates sequences of body language predictions based on human poses estimated from input videos. In the second stage, the predicted sequences are fed into a temporal network for emotion interpretation and psychiatric symptom prediction. We first validate the accuracy and transferability of the proposed body language recognition method on several public action recognition datasets. We then evaluate the framework on a proposed URMC dataset, which consists of conversations between a standardized patient and a behavioral health professional, along with expert annotations of body language, emotions, and potential psychiatric symptoms. The proposed framework outperforms other methods on the URMC dataset.



### (Un)Masked COVID-19 Trends from Social Media
- **Arxiv ID**: http://arxiv.org/abs/2011.00052v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00052v3)
- **Published**: 2020-10-30 19:02:42+00:00
- **Updated**: 2021-07-09 15:36:12+00:00
- **Authors**: Asmit Kumar Singh, Paras Mehan, Divyanshu Sharma, Rohan Pandey, Tavpritesh Sethi, Ponnurangam Kumaraguru
- **Comment**: None
- **Journal**: None
- **Summary**: Wearing masks is a useful protection method against COVID-19, which has caused widespread economic and social impact worldwide. Across the globe, governments have put mandates for the use of face masks, which have received both positive and negative reaction. Online social media provides an exciting platform to study the use of masks and analyze underlying mask-wearing patterns. In this article, we analyze 2.04 million social media images for six US cities. An increase in masks worn in images is seen as the COVID-19 cases rose, particularly when their respective states imposed strict regulations. We also found a decrease in the posting of group pictures as stay-at-home laws were put into place. Furthermore, mask compliance in the Black Lives Matter protest was analyzed, eliciting that 40% of the people in group photos wore masks, and 45% of them wore the masks with a fit score of greater than 80%. We introduce two new datasets, VAriety MAsks - Classification (VAMA-C) and VAriety MAsks - Segmentation (VAMA-S), for mask detection and mask fit analysis tasks, respectively. For the analysis, we create two frameworks, face mask detector (for classifying masked and unmasked faces) and mask fit analyzer (a semantic segmentation based model to calculate a mask-fit score). The face mask detector achieved a classification accuracy of 98%, and the semantic segmentation model for the mask fit analyzer achieved an Intersection Over Union (IOU) score of 98%. We conclude that such a framework can be used to evaluate the effectiveness of such public health strategies using social media platforms in times of pandemic.



### Adversarial Robust Training of Deep Learning MRI Reconstruction Models
- **Arxiv ID**: http://arxiv.org/abs/2011.00070v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00070v3)
- **Published**: 2020-10-30 19:26:14+00:00
- **Updated**: 2021-04-27 05:51:44+00:00
- **Authors**: Francesco Calivá, Kaiyang Cheng, Rutwik Shah, Valentina Pedoia
- **Comment**: 32 pages, 9 figures, 6 tables, accepted at MELBA (MIDL 2020 special
  issue)
- **Journal**: None
- **Summary**: Deep Learning (DL) has shown potential in accelerating Magnetic Resonance Image acquisition and reconstruction. Nevertheless, there is a dearth of tailored methods to guarantee that the reconstruction of small features is achieved with high fidelity. In this work, we employ adversarial attacks to generate small synthetic perturbations, which are difficult to reconstruct for a trained DL reconstruction network. Then, we use robust training to increase the network's sensitivity to these small features and encourage their reconstruction. Next, we investigate the generalization of said approach to real world features. For this, a musculoskeletal radiologist annotated a set of cartilage and meniscal lesions from the knee Fast-MRI dataset, and a classification network was devised to assess the reconstruction of the features. Experimental results show that by introducing robust training to a reconstruction network, the rate of false negative features (4.8\%) in image reconstruction can be reduced. These results are encouraging, and highlight the necessity for attention to this problem by the image reconstruction community, as a milestone for the introduction of DL reconstruction in clinical practice. To support further research, we make our annotations and code publicly available at https://github.com/fcaliva/fastMRI_BB_abnormalities_annotation.



### Training EfficientNets at Supercomputer Scale: 83% ImageNet Top-1 Accuracy in One Hour
- **Arxiv ID**: http://arxiv.org/abs/2011.00071v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2011.00071v2)
- **Published**: 2020-10-30 19:27:11+00:00
- **Updated**: 2020-11-05 02:17:22+00:00
- **Authors**: Arissa Wongpanich, Hieu Pham, James Demmel, Mingxing Tan, Quoc Le, Yang You, Sameer Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: EfficientNets are a family of state-of-the-art image classification models based on efficiently scaled convolutional neural networks. Currently, EfficientNets can take on the order of days to train; for example, training an EfficientNet-B0 model takes 23 hours on a Cloud TPU v2-8 node. In this paper, we explore techniques to scale up the training of EfficientNets on TPU-v3 Pods with 2048 cores, motivated by speedups that can be achieved when training at such scales. We discuss optimizations required to scale training to a batch size of 65536 on 1024 TPU-v3 cores, such as selecting large batch optimizers and learning rate schedules as well as utilizing distributed evaluation and batch normalization techniques. Additionally, we present timing and performance benchmarks for EfficientNet models trained on the ImageNet dataset in order to analyze the behavior of EfficientNets at scale. With our optimizations, we are able to train EfficientNet on ImageNet to an accuracy of 83% in 1 hour and 4 minutes.



### C-Net: A Reliable Convolutional Neural Network for Biomedical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.00081v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00081v2)
- **Published**: 2020-10-30 20:03:20+00:00
- **Updated**: 2021-08-19 02:44:51+00:00
- **Authors**: Hosein Barzekar, Zeyun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Cancers are the leading cause of death in many countries. Early diagnosis plays a crucial role in having proper treatment for this debilitating disease. The automated classification of the type of cancer is a challenging task since pathologists must examine a huge number of histopathological images to detect infinitesimal abnormalities. In this study, we propose a novel convolutional neural network (CNN) architecture composed of a Concatenation of multiple Networks, called C-Net, to classify biomedical images. The model incorporates multiple CNNs including Outer, Middle, and Inner. The first two parts of the architecture contain six networks that serve as feature extractors to feed into the Inner network to classify the images in terms of malignancy and benignancy. The C-Net is applied for histopathological image classification on two public datasets, including BreakHis and Osteosarcoma. To evaluate the performance, the model is tested using several evaluation metrics for its reliability. The C-Net model outperforms all other models on the individual metrics for both datasets and achieves zero misclassification. This approach has the potential to be extended to additional classification tasks, as experimental results demonstrate utilizing extensive evaluation metrics.



### Multi-stage transfer learning for lung segmentation using portable X-ray devices for patients with COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2011.00133v2
- **DOI**: 10.1016/j.eswa.2021.114677
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00133v2)
- **Published**: 2020-10-30 22:51:06+00:00
- **Updated**: 2021-03-07 00:29:42+00:00
- **Authors**: Plácido L Vidal, Joaquim de Moura, Jorge Novo, Marcos Ortega
- **Comment**: None
- **Journal**: Expert Systems with Applications 173 (2021) 114677
- **Summary**: One of the main challenges in times of sanitary emergency is to quickly develop computer aided diagnosis systems with a limited number of available samples due to the novelty, complexity of the case and the urgency of its implementation. This is the case during the current pandemic of COVID-19. This pathogen primarily infects the respiratory system of the afflicted, resulting in pneumonia and in a severe case of acute respiratory distress syndrome. This results in the formation of different pathological structures in the lungs that can be detected by the use of chest X-rays. Due to the overload of the health services, portable X-ray devices are recommended during the pandemic, preventing the spread of the disease. However, these devices entail different complications (such as capture quality) that, together with the subjectivity of the clinician, make the diagnostic process more difficult and suggest the necessity for computer-aided diagnosis methodologies despite the scarcity of samples available to do so. To solve this problem, we propose a methodology that allows to adapt the knowledge from a well-known domain with a high number of samples to a new domain with a significantly reduced number and greater complexity. We took advantage of a pre-trained segmentation model from brain magnetic resonance imaging of a unrelated pathology and performed two stages of knowledge transfer to obtain a robust system able to segment lung regions from portable X-ray devices despite the scarcity of samples and lesser quality. This way, our methodology obtained a satisfactory accuracy of $0.9761 \pm 0.0100$ for patients with COVID-19, $0.9801 \pm 0.0104$ for normal patients and $0.9769 \pm 0.0111$ for patients with pulmonary diseases with similar characteristics as COVID-19 (such as pneumonia) but not genuine COVID-19.



### EDCNN: Edge enhancement-based Densely Connected Network with Compound Loss for Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.00139v1
- **DOI**: 10.1109/ICSP48669.2020.9320928
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.00139v1)
- **Published**: 2020-10-30 23:12:09+00:00
- **Updated**: 2020-10-30 23:12:09+00:00
- **Authors**: Tengfei Liang, Yi Jin, Yidong Li, Tao Wang, Songhe Feng, Congyan Lang
- **Comment**: 8 pages, 7 figures, 3 tables
- **Journal**: 2020 15th IEEE International Conference on Signal Processing
  (ICSP). 1 (2020) 193-198
- **Summary**: In the past few decades, to reduce the risk of X-ray in computed tomography (CT), low-dose CT image denoising has attracted extensive attention from researchers, which has become an important research issue in the field of medical images. In recent years, with the rapid development of deep learning technology, many algorithms have emerged to apply convolutional neural networks to this task, achieving promising results. However, there are still some problems such as low denoising efficiency, over-smoothed result, etc. In this paper, we propose the Edge enhancement based Densely connected Convolutional Neural Network (EDCNN). In our network, we design an edge enhancement module using the proposed novel trainable Sobel convolution. Based on this module, we construct a model with dense connections to fuse the extracted edge information and realize end-to-end image denoising. Besides, when training the model, we introduce a compound loss that combines MSE loss and multi-scales perceptual loss to solve the over-smoothed problem and attain a marked improvement in image quality after denoising. Compared with the existing low-dose CT image denoising algorithms, our proposed model has a better performance in preserving details and suppressing noise.



### Integer Programming-based Error-Correcting Output Code Design for Robust Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.00144v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.00144v1)
- **Published**: 2020-10-30 23:35:18+00:00
- **Updated**: 2020-10-30 23:35:18+00:00
- **Authors**: Samarth Gupta, Saurabh Amin
- **Comment**: None
- **Journal**: None
- **Summary**: Error-Correcting Output Codes (ECOCs) offer a principled approach for combining simple binary classifiers into multiclass classifiers. In this paper, we investigate the problem of designing optimal ECOCs to achieve both nominal and adversarial accuracy using Support Vector Machines (SVMs) and binary deep learning models. In contrast to previous literature, we present an Integer Programming (IP) formulation to design minimal codebooks with desirable error correcting properties. Our work leverages the advances in IP solvers to generate codebooks with optimality guarantees. To achieve tractability, we exploit the underlying graph-theoretic structure of the constraint set in our IP formulation. This enables us to use edge clique covers to substantially reduce the constraint set. Our codebooks achieve a high nominal accuracy relative to standard codebooks (e.g., one-vs-all, one-vs-one, and dense/sparse codes). We also estimate the adversarial accuracy of our ECOC-based classifiers in a white-box setting. Our IP-generated codebooks provide non-trivial robustness to adversarial perturbations even without any adversarial training.



