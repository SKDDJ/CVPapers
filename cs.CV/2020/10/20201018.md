# Arxiv Papers in cs.CV on 2020-10-18
### Hierarchical Conditional Relation Networks for Multimodal Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.10019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10019v2)
- **Published**: 2020-10-18 02:31:06+00:00
- **Updated**: 2021-01-03 07:11:23+00:00
- **Authors**: Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran
- **Comment**: Major extension of our CVPR'20 paper to handle long video with text.
  arXiv admin note: substantial text overlap with arXiv:2002.10698
- **Journal**: None
- **Summary**: Video QA challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity - selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking with flexibility in accommodating input modalities and conditioning features across both different domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video and its accompanying channels in terms of compositionality, hierarchy, and near and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content, and long-form where associated information, such as subtitles, presented. Our rigorous evaluations show consistent improvements over SOTAs on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA.



### Boosting High-Level Vision with Joint Compression Artifacts Reduction and Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.08919v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08919v2)
- **Published**: 2020-10-18 04:17:08+00:00
- **Updated**: 2020-12-18 03:26:40+00:00
- **Authors**: Xiaoyu Xiang, Qian Lin, Jan P. Allebach
- **Comment**: 8 pages, 6 figures, 5 tables. Accepted by the 25th ICPR (2020)
- **Journal**: None
- **Summary**: Due to the limits of bandwidth and storage space, digital images are usually down-scaled and compressed when transmitted over networks, resulting in loss of details and jarring artifacts that can lower the performance of high-level visual tasks. In this paper, we aim to generate an artifact-free high-resolution image from a low-resolution one compressed with an arbitrary quality factor by exploring joint compression artifacts reduction (CAR) and super-resolution (SR) tasks. First, we propose a context-aware joint CAR and SR neural network (CAJNN) that integrates both local and non-local features to solve CAR and SR in one-stage. Finally, a deep reconstruction network is adopted to predict high quality and high-resolution images. Evaluation on CAR and SR benchmark datasets shows that our CAJNN model outperforms previous methods and also takes 26.2% shorter runtime. Based on this model, we explore addressing two critical challenges in high-level computer vision: optical character recognition of low-resolution texts, and extremely tiny face detection. We demonstrate that CAJNN can serve as an effective image preprocessing method and improve the accuracy for real-scene text recognition (from 85.30% to 85.75%) and the average precision for tiny face detection (from 0.317 to 0.611).



### What do CNN neurons learn: Visualization & Clustering
- **Arxiv ID**: http://arxiv.org/abs/2010.11725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11725v1)
- **Published**: 2020-10-18 05:29:22+00:00
- **Updated**: 2020-10-18 05:29:22+00:00
- **Authors**: Haoyue Dai
- **Comment**: 9 pages, 10 figures, tech report
- **Journal**: None
- **Summary**: In recent years convolutional neural networks (CNN) have shown striking progress in various tasks. However, despite the high performance, the training and prediction process remains to be a black box, leaving it a mystery to extract what neurons learn in CNN. In this paper, we address the problem of interpreting a CNN from the aspects of the input image's focus and preference, and the neurons' domination, activation and contribution to a concrete final prediction. Specifically, we use two techniques - visualization and clustering - to tackle the problems above. Visualization means the method of gradient descent on image pixel, and in clustering section two algorithms are proposed to cluster respectively over image categories and network neurons. Experiments and quantitative analyses have demonstrated the effectiveness of the two methods in explaining the question: what do neurons learn.



### Distortion-aware Monocular Depth Estimation for Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/2010.08942v2
- **DOI**: 10.1109/LSP.2021.3050712
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08942v2)
- **Published**: 2020-10-18 08:47:57+00:00
- **Updated**: 2020-11-30 01:41:15+00:00
- **Authors**: Hong-Xiang Chen, Kunhong Li, Zhiheng Fu, Mengyi Liu, Zonghao Chen, Yulan Guo
- **Comment**: Preprint
- **Journal**: None
- **Summary**: A main challenge for tasks on panorama lies in the distortion of objects among images. In this work, we propose a Distortion-Aware Monocular Omnidirectional (DAMO) dense depth estimation network to address this challenge on indoor panoramas with two steps. First, we introduce a distortion-aware module to extract calibrated semantic features from omnidirectional images. Specifically, we exploit deformable convolution to adjust its sampling grids to geometric variations of distorted objects on panoramas and then utilize a strip pooling module to sample against horizontal distortion introduced by inverse gnomonic projection. Second, we further introduce a plug-and-play spherical-aware weight matrix for our objective function to handle the uneven distribution of areas projected from a sphere. Experiments on the 360D dataset show that the proposed method can effectively extract semantic features from distorted panoramas and alleviate the supervision bias caused by distortion. It achieves state-of-the-art performance on the 360D dataset with high efficiency.



### Temporal Binary Representation for Event-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.08946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08946v1)
- **Published**: 2020-10-18 09:20:45+00:00
- **Updated**: 2020-10-18 09:20:45+00:00
- **Authors**: Simone Undri Innocenti, Federico Becattini, Federico Pernici, Alberto Del Bimbo
- **Comment**: Accepted at ICPR2020
- **Journal**: None
- **Summary**: In this paper we present an event aggregation strategy to convert the output of an event camera into frames processable by traditional Computer Vision algorithms. The proposed method first generates sequences of intermediate binary representations, which are then losslessly transformed into a compact format by simply applying a binary-to-decimal conversion. This strategy allows us to encode temporal information directly into pixel values, which are then interpreted by deep learning models. We apply our strategy, called Temporal Binary Representation, to the task of Gesture Recognition, obtaining state of the art results on the popular DVS128 Gesture Dataset. To underline the effectiveness of the proposed method compared to existing ones, we also collect an extension of the dataset under more challenging conditions on which to perform experiments.



### Multiple Future Prediction Leveraging Synthetic Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2010.08948v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.08948v1)
- **Published**: 2020-10-18 09:33:23+00:00
- **Updated**: 2020-10-18 09:33:23+00:00
- **Authors**: Lorenzo Berlincioni, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: Accepted at ICPR2020
- **Journal**: None
- **Summary**: Trajectory prediction is an important task, especially in autonomous driving. The ability to forecast the position of other moving agents can yield to an effective planning, ensuring safety for the autonomous vehicle as well for the observed entities. In this work we propose a data driven approach based on Markov Chains to generate synthetic trajectories, which are useful for training a multiple future trajectory predictor. The advantages are twofold: on the one hand synthetic samples can be used to augment existing datasets and train more effective predictors; on the other hand, it allows to generate samples with multiple ground truths, corresponding to diverse equally likely outcomes of the observed trajectory. We define a trajectory prediction model and a loss that explicitly address the multimodality of the problem and we show that combining synthetic and real data leads to prediction improvements, obtaining state of the art results.



### Shape Constrained CNN for Cardiac MR Segmentation with Simultaneous Prediction of Shape and Pose Parameters
- **Arxiv ID**: http://arxiv.org/abs/2010.08952v1
- **DOI**: 10.1007/978-3-030-68107-4_13
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08952v1)
- **Published**: 2020-10-18 09:51:04+00:00
- **Updated**: 2020-10-18 09:51:04+00:00
- **Authors**: Sofie Tilborghs, Tom Dresselaers, Piet Claus, Jan Bogaert, Frederik Maes
- **Comment**: Presented at the 11th Workshop on Statistical Atlases and
  Computational Modeling of the Heart (STACOM 2020), held in conjunction with
  MICCAI 2020
- **Journal**: None
- **Summary**: Semantic segmentation using convolutional neural networks (CNNs) is the state-of-the-art for many medical segmentation tasks including left ventricle (LV) segmentation in cardiac MR images. However, a drawback is that these CNNs lack explicit shape constraints, occasionally resulting in unrealistic segmentations. In this paper, we perform LV and myocardial segmentation by regression of pose and shape parameters derived from a statistical shape model. The integrated shape model regularizes predicted segmentations and guarantees realistic shapes. Furthermore, in contrast to semantic segmentation, it allows direct calculation of regional measures such as myocardial thickness. We enforce robustness of shape and pose prediction by simultaneously constructing a segmentation distance map during training. We evaluated the proposed method in a fivefold cross validation on a in-house clinical dataset with 75 subjects containing a total of 1539 delineated short-axis slices covering LV from apex to base, and achieved a correlation of 99% for LV area, 94% for myocardial area, 98% for LV dimensions and 88% for regional wall thicknesses. The method was additionally validated on the LVQuan18 and LVQuan19 public datasets and achieved state-of-the-art results.



### Feature Importance Ranking for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.08973v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08973v1)
- **Published**: 2020-10-18 12:20:27+00:00
- **Updated**: 2020-10-18 12:20:27+00:00
- **Authors**: Maksymilian Wojtas, Ke Chen
- **Comment**: Accepted by NeurIPS 2020, 5 Figures and 1 Table in Main text, 10
  Figures and 5 Tables in Supplementary Materials
- **Journal**: None
- **Summary**: Feature importance ranking has become a powerful tool for explainable AI. However, its nature of combinatorial optimization poses a great challenge for deep learning. In this paper, we propose a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset of a fixed size and ranking the importance of those features in the optimal subset simultaneously. During learning, the operator is trained for a supervised learning task via optimal feature subset candidates generated by the selector that learns predicting the learning performance of the operator working on different optimal subset candidates. We develop an alternate learning algorithm that trains two nets jointly and incorporates a stochastic local search procedure into learning to address the combinatorial optimization challenge. In deployment, the selector generates an optimal feature subset and ranks feature importance, while the operator makes predictions based on the optimal subset for test data. A thorough evaluation on synthetic, benchmark and real data sets suggests that our approach outperforms several state-of-the-art feature importance ranking and supervised feature selection methods. (Our source code is available: https://github.com/maksym33/FeatureImportanceDL)



### Image-based Automated Species Identification: Can Virtual Data Augmentation Overcome Problems of Insufficient Sampling?
- **Arxiv ID**: http://arxiv.org/abs/2010.09009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09009v1)
- **Published**: 2020-10-18 15:44:45+00:00
- **Updated**: 2020-10-18 15:44:45+00:00
- **Authors**: Morris Klasen, Dirk Ahrens, Jonas Eberle, Volker Steinhage
- **Comment**: None
- **Journal**: None
- **Summary**: Automated species identification and delimitation is challenging, particularly in rare and thus often scarcely sampled species, which do not allow sufficient discrimination of infraspecific versus interspecific variation. Typical problems arising from either low or exaggerated interspecific morphological differentiation are best met by automated methods of machine learning that learn efficient and effective species identification from training samples. However, limited infraspecific sampling remains a key challenge also in machine learning. 1In this study, we assessed whether a two-level data augmentation approach may help to overcome the problem of scarce training data in automated visual species identification. The first level of visual data augmentation applies classic approaches of data augmentation and generation of faked images using a GAN approach. Descriptive feature vectors are derived from bottleneck features of a VGG-16 convolutional neural network (CNN) that are then stepwise reduced in dimensionality using Global Average Pooling and PCA to prevent overfitting. The second level of data augmentation employs synthetic additional sampling in feature space by an oversampling algorithm in vector space (SMOTE). Applied on two challenging datasets of scarab beetles (Coleoptera), our augmentation approach outperformed a non-augmented deep learning baseline approach as well as a traditional 2D morphometric approach (Procrustes analysis).



### Tracklets Predicting Based Adaptive Graph Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.09015v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09015v3)
- **Published**: 2020-10-18 16:16:49+00:00
- **Updated**: 2020-11-19 05:46:35+00:00
- **Authors**: Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Xiaoliang Cheng, Kewei Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing tracking methods link the detected boxes to the tracklets using a linear combination of feature cosine distances and box overlap. But the problem of inconsistent features of an object in two different frames still exists. In addition, when extracting features, only appearance information is utilized, neither the location relationship nor the information of the tracklets is considered. We present an accurate and end-to-end learning framework for multi-object tracking, namely \textbf{TPAGT}. It re-extracts the features of the tracklets in the current frame based on motion predicting, which is the key to solve the problem of features inconsistent. The adaptive graph neural network in TPAGT is adopted to fuse locations, appearance, and historical information, and plays an important role in distinguishing different objects. In the training phase, we propose the balanced MSE LOSS to successfully overcome the unbalanced samples. Experiments show that our method reaches state-of-the-art performance. It achieves 76.5\% MOTA on the MOT16 challenge and 76.2\% MOTA on the MOT17 challenge.



### Covapixels
- **Arxiv ID**: http://arxiv.org/abs/2010.09016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09016v1)
- **Published**: 2020-10-18 16:16:50+00:00
- **Updated**: 2020-10-18 16:16:50+00:00
- **Authors**: Jeffrey Uhlmann
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and discuss the summarization of superpixel-type image tiles/patches using mean and covariance information. We refer to the resulting objects as covapixels.



### Deep Structured Prediction for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.09035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09035v1)
- **Published**: 2020-10-18 17:09:24+00:00
- **Updated**: 2020-10-18 17:09:24+00:00
- **Authors**: Lisha Chen, Hui Su, Qiang Ji
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Existing deep learning based facial landmark detection methods have achieved excellent performance. These methods, however, do not explicitly embed the structural dependencies among landmark points. They hence cannot preserve the geometric relationships between landmark points or generalize well to challenging conditions or unseen data. This paper proposes a method for deep structured facial landmark detection based on combining a deep Convolutional Network with a Conditional Random Field. We demonstrate its superior performance to existing state-of-the-art techniques in facial landmark detection, especially a better generalization ability on challenging datasets that include large pose and occlusion.



### Exploiting Context for Robustness to Label Noise in Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.09066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09066v1)
- **Published**: 2020-10-18 18:59:44+00:00
- **Updated**: 2020-10-18 18:59:44+00:00
- **Authors**: Sudipta Paul, Shivkumar Chandrasekaran, B. S. Manjunath, Amit K. Roy-Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: Several works in computer vision have demonstrated the effectiveness of active learning for adapting the recognition model when new unlabeled data becomes available. Most of these works consider that labels obtained from the annotator are correct. However, in a practical scenario, as the quality of the labels depends on the annotator, some of the labels might be wrong, which results in degraded recognition performance. In this paper, we address the problems of i) how a system can identify which of the queried labels are wrong and ii) how a multi-class active learning system can be adapted to minimize the negative impact of label noise. Towards solving the problems, we propose a noisy label filtering based learning approach where the inter-relationship (context) that is quite common in natural data is utilized to detect the wrong labels. We construct a graphical representation of the unlabeled data to encode these relationships and obtain new beliefs on the graph when noisy labels are available. Comparing the new beliefs with the prior relational information, we generate a dissimilarity score to detect the incorrect labels and update the recognition model with correct labels which result in better recognition performance. This is demonstrated in three different applications: scene classification, activity classification, and document classification.



### Multimodal semantic forecasting based on conditional generation of future features
- **Arxiv ID**: http://arxiv.org/abs/2010.09067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09067v1)
- **Published**: 2020-10-18 18:59:52+00:00
- **Updated**: 2020-10-18 18:59:52+00:00
- **Authors**: Kristijan Fugošić, Josip Šarić, Siniša Šegvić
- **Comment**: Accepted to German Conference on Pattern Recognition 2020. 24 pages,
  11 figures, 5 tables
- **Journal**: None
- **Summary**: This paper considers semantic forecasting in road-driving scenes. Most existing approaches address this problem as deterministic regression of future features or future predictions given observed frames. However, such approaches ignore the fact that future can not always be guessed with certainty. For example, when a car is about to turn around a corner, the road which is currently occluded by buildings may turn out to be either free to drive, or occupied by people, other vehicles or roadworks. When a deterministic model confronts such situation, its best guess is to forecast the most likely outcome. However, this is not acceptable since it defeats the purpose of forecasting to improve security. It also throws away valuable training data, since a deterministic model is unable to learn any deviation from the norm. We address this problem by providing more freedom to the model through allowing it to forecast different futures. We propose to formulate multimodal forecasting as sampling of a multimodal generative model conditioned on the observed frames. Experiments on the Cityscapes dataset reveal that our multimodal model outperforms its deterministic counterpart in short-term forecasting while performing slightly worse in the mid-term case.



### RADIATE: A Radar Dataset for Automotive Perception in Bad Weather
- **Arxiv ID**: http://arxiv.org/abs/2010.09076v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.09076v3)
- **Published**: 2020-10-18 19:33:27+00:00
- **Updated**: 2021-04-05 14:00:22+00:00
- **Authors**: Marcel Sheeny, Emanuele De Pellegrin, Saptarshi Mukherjee, Alireza Ahrabian, Sen Wang, Andrew Wallace
- **Comment**: Accepted at IEEE International Conference on Robotics and Automation
  2021 (ICRA 2021)
- **Journal**: None
- **Summary**: Datasets for autonomous cars are essential for the development and benchmarking of perception systems. However, most existing datasets are captured with camera and LiDAR sensors in good weather conditions. In this paper, we present the RAdar Dataset In Adverse weaThEr (RADIATE), aiming to facilitate research on object detection, tracking and scene understanding using radar sensing for safe autonomous driving. RADIATE includes 3 hours of annotated radar images with more than 200K labelled road actors in total, on average about 4.6 instances per radar image. It covers 8 different categories of actors in a variety of weather conditions (e.g., sun, night, rain, fog and snow) and driving scenarios (e.g., parked, urban, motorway and suburban), representing different levels of challenge. To the best of our knowledge, this is the first public radar dataset which provides high-resolution radar images on public roads with a large amount of road actors labelled. The data collected in adverse weather, e.g., fog and snowfall, is unique. Some baseline results of radar based object detection and recognition are given to show that the use of radar data is promising for automotive applications in bad weather, where vision and LiDAR can fail. RADIATE also has stereo images, 32-channel LiDAR and GPS data, directed at other applications such as sensor fusion, localisation and mapping. The public dataset can be accessed at http://pro.hw.ac.uk/radiate/.



### Graphite: GRAPH-Induced feaTure Extraction for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2010.09079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.09079v1)
- **Published**: 2020-10-18 19:41:09+00:00
- **Updated**: 2020-10-18 19:41:09+00:00
- **Authors**: Mahdi Saleh, Shervin Dehghani, Benjamin Busam, Nassir Navab, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: 3D Point clouds are a rich source of information that enjoy growing popularity in the vision community. However, due to the sparsity of their representation, learning models based on large point clouds is still a challenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure Extraction pipeline, a simple yet powerful feature transform and keypoint detector. Graphite enables intensive down-sampling of point clouds with keypoint detection accompanied by a descriptor. We construct a generic graph-based learning scheme to describe point cloud regions and extract salient points. To this end, we take advantage of 6D pose information and metric learning to learn robust descriptions and keypoints across different scans. We Reformulate the 3D keypoint pipeline with graph neural networks which allow efficient processing of the point set while boosting its descriptive power which ultimately results in more accurate 3D registrations. We demonstrate our lightweight descriptor on common 3D descriptor matching and point cloud registration benchmarks and achieve comparable results with the state of the art. Describing 100 patches of a point cloud and detecting their keypoints takes only ~0.018 seconds with our proposed network.



### Gait Recognition using Multi-Scale Partial Representation Transformation with Capsules
- **Arxiv ID**: http://arxiv.org/abs/2010.09084v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09084v1)
- **Published**: 2020-10-18 19:47:38+00:00
- **Updated**: 2020-10-18 19:47:38+00:00
- **Authors**: Alireza Sepas-Moghaddam, Saeed Ghorbani, Nikolaus F. Troje, Ali Etemad
- **Comment**: Accepted to International Conference on Pattern Recognition (ICPR)
  2020
- **Journal**: None
- **Summary**: Gait recognition, referring to the identification of individuals based on the manner in which they walk, can be very challenging due to the variations in the viewpoint of the camera and the appearance of individuals. Current methods for gait recognition have been dominated by deep learning models, notably those based on partial feature representations. In this context, we propose a novel deep network, learning to transfer multi-scale partial gait representations using capsules to obtain more discriminative gait features. Our network first obtains multi-scale partial representations using a state-of-the-art deep partial feature extractor. It then recurrently learns the correlations and co-occurrences of the patterns among the partial features in forward and backward directions using Bi-directional Gated Recurrent Units (BGRU). Finally, a capsule network is adopted to learn deeper part-whole relationships and assigns more weights to the more relevant features while ignoring the spurious dimensions. That way, we obtain final features that are more robust to both viewing and appearance changes. The performance of our method has been extensively tested on two gait recognition datasets, CASIA-B and OU-MVLP, using four challenging test protocols. The results of our method have been compared to the state-of-the-art gait recognition solutions, showing the superiority of our model, notably when facing challenging viewing and carrying conditions.



### View-Invariant Gait Recognition with Attentive Recurrent Learning of Partial Representations
- **Arxiv ID**: http://arxiv.org/abs/2010.09092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09092v1)
- **Published**: 2020-10-18 20:20:43+00:00
- **Updated**: 2020-10-18 20:20:43+00:00
- **Authors**: Alireza Sepas-Moghaddam, Ali Etemad
- **Comment**: Accepted in IEEE Transactions on Biometrics, Behavior, and Identity
  Science (T-BIOM)
- **Journal**: None
- **Summary**: Gait recognition refers to the identification of individuals based on features acquired from their body movement during walking. Despite the recent advances in gait recognition with deep learning, variations in data acquisition and appearance, namely camera angles, subject pose, occlusions, and clothing, are challenging factors that need to be considered for achieving accurate gait recognition systems. In this paper, we propose a network that first learns to extract gait convolutional energy maps (GCEM) from frame-level convolutional features. It then adopts a bidirectional recurrent neural network to learn from split bins of the GCEM, thus exploiting the relations between learned partial spatiotemporal representations. We then use an attention mechanism to selectively focus on important recurrently learned partial representations as identity information in different scenarios may lie in different GCEM bins. Our proposed model has been extensively tested on two large-scale CASIA-B and OU-MVLP gait datasets using four different test protocols and has been compared to a number of state-of-the-art and baseline solutions. Additionally, a comprehensive experiment has been performed to study the robustness of our model in the presence of six different synthesized occlusions. The experimental results show the superiority of our proposed method, outperforming the state-of-the-art, especially in scenarios where different clothing and carrying conditions are encountered. The results also revealed that our model is more robust against different occlusions as compared to the state-of-the-art methods.



### Variational Capsule Encoder
- **Arxiv ID**: http://arxiv.org/abs/2010.09102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09102v1)
- **Published**: 2020-10-18 20:52:16+00:00
- **Updated**: 2020-10-18 20:52:16+00:00
- **Authors**: Harish RaviPrakash, Syed Muhammad Anwar, Ulas Bagci
- **Comment**: Accepted for publication in the 2020 25th International Conference on
  Pattern Recognition (ICPR)
- **Journal**: None
- **Summary**: We propose a novel capsule network based variational encoder architecture, called Bayesian capsules (B-Caps), to modulate the mean and standard deviation of the sampling distribution in the latent space. We hypothesized that this approach can learn a better representation of features in the latent space than traditional approaches. Our hypothesis was tested by using the learned latent variables for image reconstruction task, where for MNIST and Fashion-MNIST datasets, different classes were separated successfully in the latent space using our proposed model. Our experimental results have shown improved reconstruction and classification performances for both datasets adding credence to our hypothesis. We also showed that by increasing the latent space dimension, the proposed B-Caps was able to learn a better representation when compared to the traditional variational auto-encoders (VAE). Hence our results indicate the strength of capsule networks in representation learning which has never been examined under the VAE settings before.



### Unsupervised Foveal Vision Neural Networks with Top-Down Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.09103v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09103v1)
- **Published**: 2020-10-18 20:55:49+00:00
- **Updated**: 2020-10-18 20:55:49+00:00
- **Authors**: Ryan Burt, Nina N. Thigpen, Andreas Keil, Jose C. Principe
- **Comment**: 29 pages, 15 figures
- **Journal**: None
- **Summary**: Deep learning architectures are an extremely powerful tool for recognizing and classifying images. However, they require supervised learning and normally work on vectors the size of image pixels and produce the best results when trained on millions of object images. To help mitigate these issues, we propose the fusion of bottom-up saliency and top-down attention employing only unsupervised learning techniques, which helps the object recognition module to focus on relevant data and learn important features that can later be fine-tuned for a specific task. In addition, by utilizing only relevant portions of the data, the training speed can be greatly improved. We test the performance of the proposed Gamma saliency technique on the Toronto and CAT2000 databases, and the foveated vision in the Street View House Numbers (SVHN) database. The results in foveated vision show that Gamma saliency is comparable to the best and computationally faster. The results in SVHN show that our unsupervised cognitive architecture is comparable to fully supervised methods and that the Gamma saliency also improves CNN performance if desired. We also develop a topdown attention mechanism based on the Gamma saliency applied to the top layer of CNNs to improve scene understanding in multi-object images or images with strong background clutter. When we compare the results with human observers in an image dataset of animals occluded in natural scenes, we show that topdown attention is capable of disambiguating object from background and improves system performance beyond the level of human observers.



### Movement-induced Priors for Deep Stereo
- **Arxiv ID**: http://arxiv.org/abs/2010.09105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09105v1)
- **Published**: 2020-10-18 21:02:25+00:00
- **Updated**: 2020-10-18 21:02:25+00:00
- **Authors**: Yuxin Hou, Muhammad Kamran Janjua, Juho Kannala, Arno Solin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for fusing stereo disparity estimation with movement-induced prior information. Instead of independent inference frame-by-frame, we formulate the problem as a non-parametric learning task in terms of a temporal Gaussian process prior with a movement-driven kernel for inter-frame reasoning. We present a hierarchy of three Gaussian process kernels depending on the availability of motion information, where our main focus is on a new gyroscope-driven kernel for handheld devices with low-quality MEMS sensors, thus also relaxing the requirement of having full 6D camera poses available. We show how our method can be combined with two state-of-the-art deep stereo methods. The method either work in a plug-and-play fashion with pre-trained deep stereo networks, or further improved by jointly training the kernels together with encoder-decoder architectures, leading to consistent improvement.



### Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2010.09125v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09125v2)
- **Published**: 2020-10-18 22:29:07+00:00
- **Updated**: 2021-04-20 18:06:17+00:00
- **Authors**: Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, Sanja Fidler
- **Comment**: Accepted to ICLR 2021 as an Oral paper
- **Journal**: None
- **Summary**: Differentiable rendering has paved the way to training neural networks to perform "inverse graphics" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D "neural renderer", complementing traditional graphics renderers.



### Localized Interactive Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.09140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09140v2)
- **Published**: 2020-10-18 23:24:09+00:00
- **Updated**: 2020-10-20 09:57:07+00:00
- **Authors**: Soumajit Majumder, Angela Yao
- **Comment**: Preprint of the accepted paper at GCPR 2019
- **Journal**: None
- **Summary**: In current interactive instance segmentation works, the user is granted a free hand when providing clicks to segment an object; clicks are allowed on background pixels and other object instances far from the target object. This form of interaction is highly inconsistent with the end goal of efficiently isolating objects of interest. In our work, we propose a clicking scheme wherein user interactions are restricted to the proximity of the object. In addition, we propose a novel transformation of the user-provided clicks to generate a weak localization prior on the object which is consistent with image structures such as edges, textures etc. We demonstrate the effectiveness of our proposed clicking scheme and localization strategy through detailed experimentation in which we raise state-of-the-art on several standard interactive segmentation benchmarks.



### Vision-Based Layout Detection from Scientific Literature using Recurrent Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10; I.4.9; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2010.11727v1)
- **Published**: 2020-10-18 23:50:28+00:00
- **Updated**: 2020-10-18 23:50:28+00:00
- **Authors**: Huichen Yang, William H. Hsu
- **Comment**: 8 pages
- **Journal**: 25th International Conference on Pattern Recognition (ICPR2020)
- **Summary**: We present an approach for adapting convolutional neural networks for object recognition and classification to scientific literature layout detection (SLLD), a shared subtask of several information extraction problems. Scientific publications contain multiple types of information sought by researchers in various disciplines, organized into an abstract, bibliography, and sections documenting related work, experimental methods, and results; however, there is no effective way to extract this information due to their diverse layout. In this paper, we present a novel approach to developing an end-to-end learning framework to segment and classify major regions of a scientific document. We consider scientific document layout analysis as an object detection task over digital images, without any additional text features that need to be added into the network during the training process. Our technical objective is to implement transfer learning via fine-tuning of pre-trained networks and thereby demonstrate that this deep learning architecture is suitable for tasks that lack very large document corpora for training ab initio. As part of the experimental test bed for empirical evaluation of this approach, we created a merged multi-corpus data set for scientific publication layout detection tasks. Our results show good improvement with fine-tuning of a pre-trained base network using this merged data set, compared to the baseline convolutional neural network architecture.



