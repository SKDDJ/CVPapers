# Arxiv Papers in cs.CV on 2020-10-14
### Linking average- and worst-case perturbation robustness via class selectivity and dimensionality
- **Arxiv ID**: http://arxiv.org/abs/2010.07693v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.07693v2)
- **Published**: 2020-10-14 00:45:29+00:00
- **Updated**: 2021-03-29 22:49:58+00:00
- **Authors**: Matthew L. Leavitt, Ari Morcos
- **Comment**: arXiv admin note: text overlap with arXiv:2007.04440
- **Journal**: None
- **Summary**: Representational sparsity is known to affect robustness to input perturbations in deep neural networks (DNNs), but less is known about how the semantic content of representations affects robustness. Class selectivity-the variability of a unit's responses across data classes or dimensions-is one way of quantifying the sparsity of semantic representations. Given recent evidence that class selectivity may not be necessary for, and in some cases can impair generalization, we investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that networks regularized to have lower levels of class selectivity were more robust to average-case (naturalistic) perturbations, while networks with higher class selectivity are more vulnerable. In contrast, class selectivity increases robustness to multiple types of worst-case (i.e. white box adversarial) perturbations, suggesting that while decreasing class selectivity is helpful for average-case perturbations, it is harmful for worst-case perturbations. To explain this difference, we studied the dimensionality of the networks' representations: we found that the dimensionality of early-layer representations is inversely proportional to a network's class selectivity, and that adversarial samples cause a larger increase in early-layer dimensionality than corrupted samples. Furthermore, the input-unit gradient is more variable across samples and units in high-selectivity networks compared to low-selectivity networks. These results lead to the conclusion that units participate more consistently in low-selectivity regimes compared to high-selectivity regimes, effectively creating a larger attack surface and hence vulnerability to worst-case perturbations.



### Rotation Averaging with Attention Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.06773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06773v1)
- **Published**: 2020-10-14 02:07:19+00:00
- **Updated**: 2020-10-14 02:07:19+00:00
- **Authors**: Joshua Thorpe, Ruwan Tennakoon, Alireza Bab-Hadiashar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a real-time and robust solution to large-scale multiple rotation averaging. Until recently, Multiple rotation averaging problem had been solved using conventional iterative optimization algorithms. Such methods employed robust cost functions that were chosen based on assumptions made about the sensor noise and outlier distribution. In practice, these assumptions do not always fit real datasets very well. A recent work showed that the noise distribution could be learnt using a graph neural network. This solution required a second network for outlier detection and removal as the averaging network was sensitive to a poor initialization. In this paper we propose a single-stage graph neural network that can robustly perform rotation averaging in the presence of noise and outliers. Our method uses all observations, suppressing outliers effects through the use of weighted averaging and an attention mechanism within the network design. The result is a network that is faster, more robust and can be trained with less samples than the previous neural approach, ultimately outperforming conventional iterative algorithms in accuracy and in inference times.



### Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision
- **Arxiv ID**: http://arxiv.org/abs/2010.06775v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06775v1)
- **Published**: 2020-10-14 02:11:51+00:00
- **Updated**: 2020-10-14 02:11:51+00:00
- **Authors**: Hao Tan, Mohit Bansal
- **Comment**: EMNLP 2020 (15 pages)
- **Journal**: None
- **Summary**: Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named "vokenization" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call "vokens"). The "vokenizer" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at https://github.com/airsplay/vokenization



### Ferrograph image classification
- **Arxiv ID**: http://arxiv.org/abs/2010.06777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06777v1)
- **Published**: 2020-10-14 02:19:32+00:00
- **Updated**: 2020-10-14 02:19:32+00:00
- **Authors**: Peng Peng, Jiugen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: It has been challenging to identify ferrograph images with a small dataset and various scales of wear particle. A novel model is proposed in this study to cope with these challenging problems. For the problem of insufficient samples, we first proposed a data augmentation algorithm based on the permutation of image patches. Then, an auxiliary loss function of image patch permutation recognition was proposed to identify the image generated by the data augmentation algorithm. Moreover, we designed a feature extraction loss function to force the proposed model to extract more abundant features and to reduce redundant representations. As for the challenge of large change range of wear particle size, we proposed a multi-scale feature extraction block to obtain the multi-scale representations of wear particles. We carried out experiments on a ferrograph image dataset and a mini-CIFAR-10 dataset. Experimental results show that the proposed model can improve the accuracy of the two datasets by 9% and 20% respectively compared with the baseline.



### Low-rank Convex/Sparse Thermal Matrix Approximation for Infrared-based Diagnostic System
- **Arxiv ID**: http://arxiv.org/abs/2010.06784v1
- **DOI**: 10.1109/TIM.2020.3031129
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06784v1)
- **Published**: 2020-10-14 02:53:19+00:00
- **Updated**: 2020-10-14 02:53:19+00:00
- **Authors**: Bardia Yousefi, Clemente Ibarra Castanedo, Xavier P. V. Maldague
- **Comment**: Authors version
- **Journal**: IEEE Transactions on Instrumentation and Measurement 2020
- **Summary**: Active and passive thermography are two efficient techniques extensively used to measure heterogeneous thermal patterns leading to subsurface defects for diagnostic evaluations. This study conducts a comparative analysis on low-rank matrix approximation methods in thermography with applications of semi-, convex-, and sparse- non-negative matrix factorization (NMF) methods for detecting subsurface thermal patterns. These methods inherit the advantages of principal component thermography (PCT) and sparse PCT, whereas tackle negative bases in sparse PCT with non-negative constraints, and exhibit clustering property in processing data. The practicality and efficiency of these methods are demonstrated by the experimental results for subsurface defect detection in three specimens (for different depth and size defects) and preserving thermal heterogeneity for distinguishing breast abnormality in breast cancer screening dataset (accuracy of 74.1%, 75.8%, and 77.8%).



### A Patch-based Image Denoising Method Using Eigenvectors of the Geodesics' Gramian Matrix
- **Arxiv ID**: http://arxiv.org/abs/2010.07769v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68U10, 94A08, 68T10, I.4.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.07769v2)
- **Published**: 2020-10-14 04:07:24+00:00
- **Updated**: 2022-02-02 04:35:52+00:00
- **Authors**: Kelum Gajamannage, Randy Paffenroth, Anura P. Jayasumana
- **Comment**: 25 pages, 5 figures, submitted into Pattern Recognition
- **Journal**: None
- **Summary**: With the proliferation of sophisticated cameras in modern society, the demand for accurate and visually pleasing images is increasing. However, the quality of an image captured by a camera may be degraded by noise. Thus, some processing of images is required to filter out the noise without losing vital image features. Even though the current literature offers a variety of denoising methods, the fidelity and efficacy of their denoising are sometimes uncertain. Thus, here we propose a novel and computationally efficient image denoising method that is capable of producing accurate images. To preserve image smoothness, this method inputs patches partitioned from the image rather than pixels. Then, it performs denoising on the manifold underlying the patch-space rather than that in the image domain to better preserve the features across the whole image. We validate the performance of this method against benchmark image processing methods.



### Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign Dropout
- **Arxiv ID**: http://arxiv.org/abs/2010.06808v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06808v1)
- **Published**: 2020-10-14 04:42:18+00:00
- **Updated**: 2020-10-14 04:42:18+00:00
- **Authors**: Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, Dragomir Anguelov
- **Comment**: Conference on Neural Information Processing Systems (NeurIPS) 2020
- **Journal**: None
- **Summary**: The vast majority of deep models use multiple gradient signals, typically corresponding to a sum of multiple loss terms, to update a shared set of trainable weights. However, these multiple updates can impede optimal training by pulling the model in conflicting directions. We present Gradient Sign Dropout (GradDrop), a probabilistic masking procedure which samples gradients at an activation layer based on their level of consistency. GradDrop is implemented as a simple deep layer that can be used in any deep net and synergizes with other gradient balancing approaches. We show that GradDrop outperforms the state-of-the-art multiloss methods within traditional multitask and transfer learning settings, and we discuss how GradDrop reveals links between optimal multiloss training and gradient stochasticity.



### Towards Optimal Filter Pruning with Balanced Performance and Pruning Speed
- **Arxiv ID**: http://arxiv.org/abs/2010.06821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06821v1)
- **Published**: 2020-10-14 06:17:09+00:00
- **Updated**: 2020-10-14 06:17:09+00:00
- **Authors**: Dong Li, Sitong Chen, Xudong Liu, Yunda Sun, Li Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Filter pruning has drawn more attention since resource constrained platform requires more compact model for deployment. However, current pruning methods suffer either from the inferior performance of one-shot methods, or the expensive time cost of iterative training methods. In this paper, we propose a balanced filter pruning method for both performance and pruning speed. Based on the filter importance criteria, our method is able to prune a layer with approximate layer-wise optimal pruning rate at preset loss variation. The network is pruned in the layer-wise way without the time consuming prune-retrain iteration. If a pre-defined pruning rate for the entire network is given, we also introduce a method to find the corresponding loss variation threshold with fast converging speed. Moreover, we propose the layer group pruning and channel selection mechanism for channel alignment in network with short connections. The proposed pruning method is widely applicable to common architectures and does not involve any additional training except the final fine-tuning. Comprehensive experiments show that our method outperforms many state-of-the-art approaches.



### Differential diagnosis and molecular stratification of gastrointestinal stromal tumors on CT images using a radiomics approach
- **Arxiv ID**: http://arxiv.org/abs/2010.06824v2
- **DOI**: 10.1007/s10278-022-00590-2
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06824v2)
- **Published**: 2020-10-14 06:27:45+00:00
- **Updated**: 2020-10-15 06:12:55+00:00
- **Authors**: Martijn P. A. Starmans, Milea J. M. Timbergen, Melissa Vos, Michel Renckens, Dirk J. Grünhagen, Geert J. L. H. van Leenders, Roy S. Dwarkasing, François E. J. A. Willemssen, Wiro J. Niessen, Cornelis Verhoef, Stefan Sleijfer, Jacob J. Visser, Stefan Klein
- **Comment**: Martijn P.A. Starmans and Milea J.M. Timbergen contributed equally
- **Journal**: J Digit Imaging (2022)
- **Summary**: Distinguishing gastrointestinal stromal tumors (GISTs) from other intra-abdominal tumors and GISTs molecular analysis is necessary for treatment planning, but challenging due to its rarity. The aim of this study was to evaluate radiomics for distinguishing GISTs from other intra-abdominal tumors, and in GISTs, predict the c-KIT, PDGFRA,BRAF mutational status and mitotic index (MI). All 247 included patients (125 GISTS, 122 non-GISTs) underwent a contrast-enhanced venous phase CT. The GIST vs. non-GIST radiomics model, including imaging, age, sex and location, had a mean area under the curve (AUC) of 0.82. Three radiologists had an AUC of 0.69, 0.76, and 0.84, respectively. The radiomics model had an AUC of 0.52 for c-KIT, 0.56 for c-KIT exon 11, and 0.52 for the MI. Hence, our radiomics model was able to distinguish GIST from non-GISTS with a performance similar to three radiologists, but was not able to predict the c-KIT mutation or MI.



### GreedyFool: Multi-Factor Imperceptibility and Its Application to Designing a Black-box Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2010.06855v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06855v4)
- **Published**: 2020-10-14 07:45:06+00:00
- **Updated**: 2021-11-29 03:10:31+00:00
- **Authors**: Hui Liu, Bo Zhao, Minzhi Ji, Peng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are well-designed input samples, in which perturbations are imperceptible to the human eyes, but easily mislead the output of deep neural networks (DNNs). Existing works synthesize adversarial examples by leveraging simple metrics to penalize perturbations, that lack sufficient consideration of the human visual system (HVS), which produces noticeable artifacts. To explore why the perturbations are visible, this paper summarizes four primary factors affecting the perceptibility of human eyes. Based on this investigation, we design a multi-factor metric MulFactorLoss for measuring the perceptual loss between benign examples and adversarial ones. In order to test the imperceptibility of the multi-factor metric, we propose a novel black-box adversarial attack that is referred to as GreedyFool. GreedyFool applies differential evolution to evaluate the effects of perturbed pixels on the confidence of a target DNN, and introduces greedy approximation to automatically generate adversarial perturbations. We conduct extensive experiments on the ImageNet and CIFRA-10 datasets and a comprehensive user study with 60 participants. The experimental results demonstrate that MulFactorLoss is a more imperceptible metric than the existing pixelwise metrics, and GreedyFool achieves a 100% success rate in a black-box manner.



### Deep Ensembles for Low-Data Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.06866v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.06866v2)
- **Published**: 2020-10-14 07:59:00+00:00
- **Updated**: 2020-10-19 10:59:20+00:00
- **Authors**: Basil Mustafa, Carlos Riquelme, Joan Puigcerver, André Susano Pinto, Daniel Keysers, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: In the low-data regime, it is difficult to train good supervised models from scratch. Instead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.



### Semantic Flow-guided Motion Removal Method for Robust Mapping
- **Arxiv ID**: http://arxiv.org/abs/2010.06876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06876v1)
- **Published**: 2020-10-14 08:40:16+00:00
- **Updated**: 2020-10-14 08:40:16+00:00
- **Authors**: Xudong Lv, Boya Wang, Dong Ye, Shuo Wang
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Moving objects in scenes are still a severe challenge for the SLAM system. Many efforts have tried to remove the motion regions in the images by detecting moving objects. In this way, the keypoints belonging to motion regions will be ignored in the later calculations. In this paper, we proposed a novel motion removal method, leveraging semantic information and optical flow to extract motion regions. Different from previous works, we don't predict moving objects or motion regions directly from image sequences. We computed rigid optical flow, synthesized by the depth and pose, and compared it against the estimated optical flow to obtain initial motion regions. Then, we utilized K-means to finetune the motion region masks with instance segmentation masks. The ORB-SLAM2 integrated with the proposed motion removal method achieved the best performance in both indoor and outdoor dynamic environments.



### Semantic Segmentation for Partially Occluded Apple Trees Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.06879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06879v1)
- **Published**: 2020-10-14 08:43:22+00:00
- **Updated**: 2020-10-14 08:43:22+00:00
- **Authors**: Zijue Chen, David Ting, Rhys Newbury, Chao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Fruit tree pruning and fruit thinning require a powerful vision system that can provide high resolution segmentation of the fruit trees and their branches. However, recent works only consider the dormant season, where there are minimal occlusions on the branches or fit a polynomial curve to reconstruct branch shape and hence, losing information about branch thickness. In this work, we apply two state-of-the-art supervised learning models U-Net and DeepLabv3, and a conditional Generative Adversarial Network Pix2Pix (with and without the discriminator) to segment partially occluded 2D-open-V apple trees. Binary accuracy, Mean IoU, Boundary F1 score and Occluded branch recall were used to evaluate the performances of the models. DeepLabv3 outperforms the other models at Binary accuracy, Mean IoU and Boundary F1 score, but is surpassed by Pix2Pix (without discriminator) and U-Net in Occluded branch recall. We define two difficulty indices to quantify the difficulty of the task: (1) Occlusion Difficulty Index and (2) Depth Difficulty Index. We analyze the worst 10 images in both difficulty indices by means of Branch Recall and Occluded Branch Recall. U-Net outperforms the other two models in the current metrics. On the other hand, Pix2Pix (without discriminator) provides more information on branch paths, which are not reflected by the metrics. This highlights the need for more specific metrics on recovering occluded information. Furthermore, this shows the usefulness of image-transfer networks for hallucination behind occlusions. Future work is required to further enhance the models to recover more information from occlusions such that this technology can be applied to automating agricultural tasks in a commercial environment.



### Identifying Wrongly Predicted Samples: A Method for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.06890v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.06890v1)
- **Published**: 2020-10-14 09:00:42+00:00
- **Updated**: 2020-10-14 09:00:42+00:00
- **Authors**: Rahaf Aljundi, Nikolay Chumerin, Daniel Olmeda Reino
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art machine learning models require access to significant amount of annotated data in order to achieve the desired level of performance. While unlabelled data can be largely available and even abundant, annotation process can be quite expensive and limiting. Under the assumption that some samples are more important for a given task than others, active learning targets the problem of identifying the most informative samples that one should acquire annotations for. Instead of the conventional reliance on model uncertainty as a proxy to leverage new unknown labels, in this work we propose a simple sample selection criterion that moves beyond uncertainty. By first accepting the model prediction and then judging its effect on the generalization error, we can better identify wrongly predicted samples. We further present an approximation to our criterion that is very efficient and provides a similarity based interpretation. In addition to evaluating our method on the standard benchmarks of active learning, we consider the challenging yet realistic scenario of imbalanced data where categories are not equally represented. We show state-of-the-art results and better rates at identifying wrongly predicted samples. Our method is simple, model agnostic and relies on the current model status without the need for re-training from scratch.



### Adaptive-Attentive Geolocalization from few queries: a hybrid approach
- **Arxiv ID**: http://arxiv.org/abs/2010.06897v2
- **DOI**: 10.1109/WACV48630.2021.00296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06897v2)
- **Published**: 2020-10-14 09:14:02+00:00
- **Updated**: 2021-01-06 20:05:53+00:00
- **Authors**: Gabriele Moreno Berton, Valerio Paolicelli, Carlo Masone, Barbara Caputo
- **Comment**: The pytorch code is available at
  https://github.com/valeriopaolicelli/AdAGeo
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2021, pp. 2918-2927
- **Summary**: We address the task of cross-domain visual place recognition, where the goal is to geolocalize a given query image against a labeled gallery, in the case where the query and the gallery belong to different visual domains. To achieve this, we focus on building a domain robust deep network by leveraging over an attention mechanism combined with few-shot unsupervised domain adaptation techniques, where we use a small number of unlabeled target domain images to learn about the target distribution. With our method, we are able to outperform the current state of the art while using two orders of magnitude less target domain images. Finally we propose a new large-scale dataset for cross-domain visual place recognition, called SVOX. The pytorch code is available at https://github.com/valeriopaolicelli/AdAGeo .



### Development of Open Informal Dataset Affecting Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2010.06900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06900v1)
- **Published**: 2020-10-14 09:21:45+00:00
- **Updated**: 2020-10-14 09:21:45+00:00
- **Authors**: Yong-Gu Lee, Seong-Jae Lee, Sang-Jin Lee, Tae-Seung Baek, Dong-Whan Lee, Kyeong-Chan Jang, Ho-Jin Sohn, Jin-Soo Kim
- **Comment**: 26 pages, 16 figures
- **Journal**: None
- **Summary**: This document is a document that has written procedures and methods for collecting objects and unstructured dynamic data on the road for the development of object recognition technology for self-driving cars, and outlines the methods of collecting data, annotation data, object classifier criteria, and data processing methods. On-road object and unstructured dynamic data were collected in various environments, such as weather, time and traffic conditions, and additional reception calls for police and safety personnel were collected. Finally, 100,000 images of various objects existing on pedestrians and roads, 200,000 images of police and traffic safety personnel, 5,000 images of police and traffic safety personnel, and data sets consisting of 5,000 image data were collected and built.



### PlenoptiCam v1.0: A light-field imaging framework
- **Arxiv ID**: http://arxiv.org/abs/2010.11687v5
- **DOI**: 10.1109/TIP.2021.3095671
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11687v5)
- **Published**: 2020-10-14 09:23:18+00:00
- **Updated**: 2021-07-25 17:38:22+00:00
- **Authors**: Christopher Hahne, Amar Aggoun
- **Comment**: final author version
- **Journal**: None
- **Summary**: Light-field cameras play a vital role for rich 3-D information retrieval in narrow range depth sensing applications. The key obstacle in composing light-fields from exposures taken by a plenoptic camera is to computationally calibrate, align and rearrange four-dimensional image data. Several attempts have been proposed to enhance the overall image quality by tailoring pipelines dedicated to particular plenoptic cameras and improving the consistency across viewpoints at the expense of high computational loads. The framework presented herein advances prior outcomes thanks to its novel micro image scale-space analysis for generic camera calibration independent of the lens specifications and its parallax-invariant, cost-effective viewpoint color equalization from optimal transport theory. Artifacts from the sensor and micro lens grid are compensated in an innovative way to enable superior quality in sub-aperture image extraction, computational refocusing and Scheimpflug rendering with sub-sampling capabilities. Benchmark comparisons using established image metrics suggest that our proposed pipeline outperforms state-of-the-art tool chains in the majority of cases. Results from a Wasserstein distance further show that our color transfer outdoes the existing transport methods. Our algorithms are released under an open-source license, offer cross-platform compatibility with few dependencies and different user interfaces. This makes the reproduction of results and experimentation with plenoptic camera technology convenient for peer researchers, developers, photographers, data scientists and others working in this field.



### AMPA-Net: Optimization-Inspired Attention Neural Network for Deep Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2010.06907v6
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06907v6)
- **Published**: 2020-10-14 09:39:22+00:00
- **Updated**: 2020-11-16 09:57:39+00:00
- **Authors**: Nanyu Li, Charles C. Zhou
- **Comment**: 7 pages,7 figures
- **Journal**: 2020 IEEE 20th International Conference on Communication
  Technology(Oral)
- **Summary**: Compressed sensing (CS) is a challenging problem in image processing due to reconstructing an almost complete image from a limited measurement. To achieve fast and accurate CS reconstruction, we synthesize the advantages of two well-known methods (neural network and optimization algorithm) to propose a novel optimization inspired neural network which dubbed AMP-Net. AMP-Net realizes the fusion of the Approximate Message Passing (AMP) algorithm and neural network. All of its parameters are learned automatically. Furthermore, we propose an AMPA-Net which uses three attention networks to improve the representation ability of AMP-Net. Finally, We demonstrate the effectiveness of AMP-Net and AMPA-Net on four standard CS reconstruction benchmark data sets. Our code is available on https://github.com/puallee/AMPA-Net.



### Automotive Radar Interference Mitigation with Unfolded Robust PCA based on Residual Overcomplete Auto-Encoder Blocks
- **Arxiv ID**: http://arxiv.org/abs/2010.10357v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10357v2)
- **Published**: 2020-10-14 09:41:06+00:00
- **Updated**: 2021-04-17 11:37:44+00:00
- **Authors**: Nicolae-Cătălin Ristea, Andrei Anghel, Radu Tudor Ionescu, Yonina C. Eldar
- **Comment**: Accepted at the CVPR 2021 Embedded Vision Workshop
- **Journal**: None
- **Summary**: In autonomous driving, radar systems play an important role in detecting targets such as other vehicles on the road. Radars mounted on different cars can interfere with each other, degrading the detection performance. Deep learning methods for automotive radar interference mitigation can succesfully estimate the amplitude of targets, but fail to recover the phase of the respective targets. In this paper, we propose an efficient and effective technique based on unfolded robust Principal Component Analysis (RPCA) that is able to estimate both amplitude and phase in the presence of interference. Our contribution consists in introducing residual overcomplete auto-encoder (ROC-AE) blocks into the recurrent architecture of unfolded RPCA, which results in a deeper model that significantly outperforms unfolded RPCA as well as other deep learning models.



### Efficient and high accuracy 3-D OCT angiography motion correction in pathology
- **Arxiv ID**: http://arxiv.org/abs/2010.06931v1
- **DOI**: 10.1364/BOE.411117
- **Categories**: **eess.IV**, cs.CV, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.06931v1)
- **Published**: 2020-10-14 10:20:17+00:00
- **Updated**: 2020-10-14 10:20:17+00:00
- **Authors**: Stefan B. Ploner, Martin F. Kraus, Eric M. Moult, Lennart Husvogt, Julia Schottenhamml, A. Yasin Alibhai, Nadia K. Waheed, Jay S. Duker, James G. Fujimoto, Andreas K. Maier
- **Comment**: 22 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: We propose a novel method for non-rigid 3-D motion correction of orthogonally raster-scanned optical coherence tomography angiography volumes. This is the first approach that aligns predominantly axial structural features like retinal layers and transverse angiographic vascular features in a joint optimization. Combined with the use of orthogonal scans and favorization of kinematically more plausible displacements, the approach allows subpixel alignment and micrometer-scale distortion correction in all 3 dimensions. As no specific structures or layers are segmented, the approach is by design robust to pathologic changes. It is furthermore designed for highly parallel implementation and brief runtime, allowing its integration in clinical routine even for high density or wide-field scans. We evaluated the algorithm with metrics related to clinically relevant features in a large-scale quantitative evaluation based on 204 volumetric scans of 17 subjects including both a wide range of pathologies and healthy controls. Using this method, we achieve state-of-the-art axial performance and show significant advances in both transverse co-alignment and distortion correction, especially in the pathologic subgroup.



### PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training
- **Arxiv ID**: http://arxiv.org/abs/2010.06932v1
- **DOI**: 10.1145/3423323.3423407
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06932v1)
- **Published**: 2020-10-14 10:23:48+00:00
- **Updated**: 2020-10-14 10:23:48+00:00
- **Authors**: An Tran, Ali Zonoozi, Jagannadan Varadarajan, Hannes Kruppa
- **Comment**: None
- **Journal**: None
- **Summary**: Road network and building footprint extraction is essential for many applications such as updating maps, traffic regulations, city planning, ride-hailing, disaster response \textit{etc}. Mapping road networks is currently both expensive and labor-intensive. Recently, improvements in image segmentation through the application of deep neural networks has shown promising results in extracting road segments from large scale, high resolution satellite imagery. However, significant challenges remain due to lack of enough labeled training data needed to build models for industry grade applications. In this paper, we propose a two-stage transfer learning technique to improve robustness of semantic segmentation for satellite images that leverages noisy pseudo ground truth masks obtained automatically (without human labor) from crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation that uses focal loss, poly learning rate, and context module. We demonstrate the strengths of our approach through evaluations done on three popular datasets over two tasks, namely, road extraction and building foot-print detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet and DeepGlobe road extraction dataset, respectively.



### Practical Deep Raw Image Denoising on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2010.06935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06935v1)
- **Published**: 2020-10-14 10:30:32+00:00
- **Updated**: 2020-10-14 10:30:32+00:00
- **Authors**: Yuzhi Wang, Haibin Huang, Qin Xu, Jiaming Liu, Yiqun Liu, Jue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image denoising approaches have been extensively studied in recent years, prevailing in many public benchmark datasets. However, the stat-of-the-art networks are computationally too expensive to be directly applied on mobile devices. In this work, we propose a light-weight, efficient neural network-based raw image denoiser that runs smoothly on mainstream mobile devices, and produces high quality denoising results. Our key insights are twofold: (1) by measuring and estimating sensor noise level, a smaller network trained on synthetic sensor-specific data can out-perform larger ones trained on general data; (2) the large noise level variation under different ISO settings can be removed by a novel k-Sigma Transform, allowing a small network to efficiently handle a wide range of noise levels. We conduct extensive experiments to demonstrate the efficiency and accuracy of our approach. Our proposed mobile-friendly denoising model runs at ~70 milliseconds per megapixel on Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot feature of several flagship smartphones released in 2019.



### Deep Learning from Small Amount of Medical Data with Noisy Labels: A Meta-Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2010.06939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06939v2)
- **Published**: 2020-10-14 10:39:44+00:00
- **Updated**: 2021-02-15 08:42:57+00:00
- **Authors**: Görkem Algan, Ilkay Ulusoy, Şaban Gönül, Banu Turgut, Berker Bakbak
- **Comment**: Accepted by ICRPOA 2021: International Conference on Retinopathy of
  Prematurity Ophthalmologic Approach
- **Journal**: None
- **Summary**: Computer vision systems recently made a big leap thanks to deep neural networks. However, these systems require correctly labeled large datasets in order to be trained properly, which is very difficult to obtain for medical applications. Two main reasons for label noise in medical applications are the high complexity of the data and conflicting opinions of experts. Moreover, medical imaging datasets are commonly tiny, which makes each data very important in learning. As a result, if not handled properly, label noise significantly degrades the performance. Therefore, a label-noise-robust learning algorithm that makes use of the meta-learning paradigm is proposed in this article. The proposed solution is tested on retinopathy of prematurity (ROP) dataset with a very high label noise of 68%. Results show that the proposed algorithm significantly improves the classification algorithm's performance in the presence of noisy labels.



### Relative Depth Estimation as a Ranking Problem
- **Arxiv ID**: http://arxiv.org/abs/2010.06944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06944v1)
- **Published**: 2020-10-14 10:46:33+00:00
- **Updated**: 2020-10-14 10:46:33+00:00
- **Authors**: Alican Mertan, Damien Jade Duff, Gozde Unal
- **Comment**: Accepted at SIU 2020
- **Journal**: None
- **Summary**: We present a formulation of the relative depth estimation from a single image problem, as a ranking problem. By reformulating the problem this way, we were able to utilize literature on the ranking problem, and apply the existing knowledge to achieve better results. To this end, we have introduced a listwise ranking loss borrowed from ranking literature, weighted ListMLE, to the relative depth estimation problem. We have also brought a new metric which considers pixel depth ranking accuracy, on which our method is stronger.



### FC-DCNN: A densely connected neural network for stereo estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.06950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.06950v1)
- **Published**: 2020-10-14 10:59:09+00:00
- **Updated**: 2020-10-14 10:59:09+00:00
- **Authors**: Dominik Hirner, Friedrich Fraundorfer
- **Comment**: This paper has been accepted to the ICPR 2020 conference in Milan
  which will be held on the 10-15 January 2021. Therefore this work has not yet
  been presented
- **Journal**: None
- **Summary**: We propose a novel lightweight network for stereo estimation. Our network consists of a fully-convolutional densely connected neural network (FC-DCNN) that computes matching costs between rectified image pairs. Our FC-DCNN method learns expressive features and performs some simple but effective post-processing steps. The densely connected layer structure connects the output of each layer to the input of each subsequent layer. This network structure and the fact that we do not use any fully-connected layers or 3D convolutions leads to a very lightweight network. The output of this network is used in order to calculate matching costs and create a cost-volume. Instead of using time and memory-inefficient cost-aggregation methods such as semi-global matching or conditional random fields in order to improve the result, we rely on filtering techniques, namely median filter and guided filter. By computing a left-right consistency check we get rid of inconsistent values. Afterwards we use a watershed foreground-background segmentation on the disparity image with removed inconsistencies. This mask is then used to refine the final prediction. We show that our method works well for both challenging indoor and outdoor scenes by evaluating it on the Middlebury, KITTI and ETH3D benchmarks respectively. Our full framework is available at https://github.com/thedodo/FC-DCNN



### Fast meningioma segmentation in T1-weighted MRI volumes using a lightweight 3D deep learning architecture
- **Arxiv ID**: http://arxiv.org/abs/2010.07002v1
- **DOI**: 10.1117/1.JMI.8.2.024002
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.07002v1)
- **Published**: 2020-10-14 12:26:53+00:00
- **Updated**: 2020-10-14 12:26:53+00:00
- **Authors**: David Bouget, André Pedersen, Sayied Abdol Mohieb Hosainey, Johanna Vanel, Ole Solheim, Ingerid Reinertsen
- **Comment**: 15 pages, 7 figures, submitted to SPIE journal of Medical Imaging
- **Journal**: J. of Medical Imaging, 8(2), 024002 (2021)
- **Summary**: Automatic and consistent meningioma segmentation in T1-weighted MRI volumes and corresponding volumetric assessment is of use for diagnosis, treatment planning, and tumor growth evaluation. In this paper, we optimized the segmentation and processing speed performances using a large number of both surgically treated meningiomas and untreated meningiomas followed at the outpatient clinic. We studied two different 3D neural network architectures: (i) a simple encoder-decoder similar to a 3D U-Net, and (ii) a lightweight multi-scale architecture (PLS-Net). In addition, we studied the impact of different training schemes. For the validation studies, we used 698 T1-weighted MR volumes from St. Olav University Hospital, Trondheim, Norway. The models were evaluated in terms of detection accuracy, segmentation accuracy and training/inference speed. While both architectures reached a similar Dice score of 70% on average, the PLS-Net was more accurate with an F1-score of up to 88%. The highest accuracy was achieved for the largest meningiomas. Speed-wise, the PLS-Net architecture tended to converge in about 50 hours while 130 hours were necessary for U-Net. Inference with PLS-Net takes less than a second on GPU and about 15 seconds on CPU. Overall, with the use of mixed precision training, it was possible to train competitive segmentation models in a relatively short amount of time using the lightweight PLS-Net architecture. In the future, the focus should be brought toward the segmentation of small meningiomas (less than 2ml) to improve clinical relevance for automatic and early diagnosis as well as speed of growth estimates.



### Better Patch Stitching for Parametric Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2010.07021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07021v1)
- **Published**: 2020-10-14 12:37:57+00:00
- **Updated**: 2020-10-14 12:37:57+00:00
- **Authors**: Zhantao Deng, Jan Bednařík, Mathieu Salzmann, Pascal Fua
- **Comment**: Accepted to 3DV 2020
- **Journal**: None
- **Summary**: Recently, parametric mappings have emerged as highly effective surface representations, yielding low reconstruction error. In particular, the latest works represent the target shape as an atlas of multiple mappings, which can closely encode object parts. Atlas representations, however, suffer from one major drawback: The individual mappings are not guaranteed to be consistent, which results in holes in the reconstructed shape or in jagged surface areas.   We introduce an approach that explicitly encourages global consistency of the local mappings. To this end, we introduce two novel loss terms. The first term exploits the surface normals and requires that they remain locally consistent when estimated within and across the individual mappings. The second term further encourages better spatial configuration of the mappings by minimizing novel stitching error. We show on standard benchmarks that the use of normal consistency requirement outperforms the baselines quantitatively while enforcing better stitching leads to much better visual quality of the reconstructed objects as compared to the state-of-the-art.



### 3D Segmentation Networks for Excessive Numbers of Classes: Distinct Bone Segmentation in Upper Bodies
- **Arxiv ID**: http://arxiv.org/abs/2010.07045v1
- **DOI**: 10.1007/978-3-030-59861-7_5
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07045v1)
- **Published**: 2020-10-14 12:54:15+00:00
- **Updated**: 2020-10-14 12:54:15+00:00
- **Authors**: Eva Schnider, Antal Horváth, Georg Rauter, Azhar Zam, Magdalena Müller-Gerbl, Philippe C. Cattin
- **Comment**: 10 pages, 3 figures, 2 tables, accepted into MICCAI 2020
  International Workshop on Machine Learning in Medical Imaging
- **Journal**: Machine Learning in Medical Imaging. MLMI 2020. Lecture Notes in
  Computer Science, vol 12436. Springer, Cham
- **Summary**: Segmentation of distinct bones plays a crucial role in diagnosis, planning, navigation, and the assessment of bone metastasis. It supplies semantic knowledge to visualisation tools for the planning of surgical interventions and the education of health professionals. Fully supervised segmentation of 3D data using Deep Learning methods has been extensively studied for many tasks but is usually restricted to distinguishing only a handful of classes. With 125 distinct bones, our case includes many more labels than typical 3D segmentation tasks. For this reason, the direct adaptation of most established methods is not possible. This paper discusses the intricacies of training a 3D segmentation network in a many-label setting and shows necessary modifications in network architecture, loss function, and data augmentation. As a result, we demonstrate the robustness of our method by automatically segmenting over one hundred distinct bones simultaneously in an end-to-end learnt fashion from a CT-scan.



### A New Distributional Ranking Loss With Uncertainty: Illustrated in Relative Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.07091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07091v1)
- **Published**: 2020-10-14 13:47:18+00:00
- **Updated**: 2020-10-14 13:47:18+00:00
- **Authors**: Alican Mertan, Yusuf Huseyin Sahin, Damien Jade Duff, Gozde Unal
- **Comment**: Accepted at 3DV 2020
- **Journal**: None
- **Summary**: We propose a new approach for the problem of relative depth estimation from a single image. Instead of directly regressing over depth scores, we formulate the problem as estimation of a probability distribution over depth and aim to learn the parameters of the distributions which maximize the likelihood of the given data. To train our model, we propose a new ranking loss, Distributional Loss, which tries to increase the probability of farther pixel's depth being greater than the closer pixel's depth. Our proposed approach allows our model to output confidence in its estimation in the form of standard deviation of the distribution. We achieve state of the art results against a number of baselines while providing confidence in our estimations. Our analysis show that estimated confidence is actually a good indicator of accuracy. We investigate the usage of confidence information in a downstream task of metric depth estimation, to increase its performance.



### Data Augmentation for Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07092v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07092v2)
- **Published**: 2020-10-14 13:48:22+00:00
- **Updated**: 2021-06-22 16:06:36+00:00
- **Authors**: Renkun Ni, Micah Goldblum, Amr Sharaf, Kezhi Kong, Tom Goldstein
- **Comment**: 15 pages, 3 figures, Accepted to ICML 2021
- **Journal**: None
- **Summary**: Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.



### Multi-class segmentation under severe class imbalance: A case study in roof damage assessment
- **Arxiv ID**: http://arxiv.org/abs/2010.07151v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07151v2)
- **Published**: 2020-10-14 15:08:54+00:00
- **Updated**: 2020-11-18 19:00:48+00:00
- **Authors**: Jean-Baptiste Boin, Nat Roth, Jigar Doshi, Pablo Llueca, Nicolas Borensztein
- **Comment**: Submitted to the Artificial Intelligence for Humanitarian Assistance
  and Disaster Response Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: The task of roof damage classification and segmentation from overhead imagery presents unique challenges. In this work we choose to address the challenge posed due to strong class imbalance. We propose four distinct techniques that aim at mitigating this problem. Through a new scheme that feeds the data to the network by oversampling the minority classes, and three other network architectural improvements, we manage to boost the macro-averaged F1-score of a model by 39.9 percentage points, thus achieving improved segmentation performance, especially on the minority classes.



### WeightAlign: Normalizing Activations by Weight Alignment
- **Arxiv ID**: http://arxiv.org/abs/2010.07160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07160v1)
- **Published**: 2020-10-14 15:25:39+00:00
- **Updated**: 2020-10-14 15:25:39+00:00
- **Authors**: Xiangwei Shi, Yunqiang Li, Xin Liu, Jan van Gemert
- **Comment**: The first three authors contributed equally; accepted by ICPR 2020
- **Journal**: None
- **Summary**: Batch normalization (BN) allows training very deep networks by normalizing activations by mini-batch sample statistics which renders BN unstable for small batch sizes. Current small-batch solutions such as Instance Norm, Layer Norm, and Group Norm use channel statistics which can be computed even for a single sample. Such methods are less stable than BN as they critically depend on the statistics of a single input sample. To address this problem, we propose a normalization of activation without sample statistics. We present WeightAlign: a method that normalizes the weights by the mean and scaled standard derivation computed within a filter, which normalizes activations without computing any sample statistics. Our proposed method is independent of batch size and stable over a wide range of batch sizes. Because weight statistics are orthogonal to sample statistics, we can directly combine WeightAlign with any method for activation normalization. We experimentally demonstrate these benefits for classification on CIFAR-10, CIFAR-100, ImageNet, for semantic segmentation on PASCAL VOC 2012 and for domain adaptation on Office-31.



### A Vector-based Representation to Enhance Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.07184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07184v2)
- **Published**: 2020-10-14 15:57:29+00:00
- **Updated**: 2020-12-08 21:44:34+00:00
- **Authors**: Zhiwen Cao, Zongcheng Chu, Dongfang Liu, Yingjie Chen
- **Comment**: Proceeding with IEEE Winter Conference on Applications of Computer
  Vision (WACV 2021); 10 pages, 8 figures
- **Journal**: None
- **Summary**: This paper proposes to use the three vectors in a rotation matrix as the representation in head pose estimation and develops a new neural network based on the characteristic of such representation. We address two potential issues existed in current head pose estimation works: 1. Public datasets for head pose estimation use either Euler angles or quaternions to annotate data samples. However, both of these annotations have the issue of discontinuity and thus could result in some performance issues in neural network training. 2. Most research works report Mean Absolute Error (MAE) of Euler angles as the measurement of performance. We show that MAE may not reflect the actual behavior especially for the cases of profile views. To solve these two problems, we propose a new annotation method which uses three vectors to describe head poses and a new measurement Mean Absolute Error of Vectors (MAEV) to assess the performance. We also train a new neural network to predict the three vectors with the constraints of orthogonality. Our proposed method achieves state-of-the-art results on both AFLW2000 and BIWI datasets. Experiments show our vector-based annotation method can effectively reduce prediction errors for large pose angles.



### Learning Propagation Rules for Attribution Map Generation
- **Arxiv ID**: http://arxiv.org/abs/2010.07210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07210v1)
- **Published**: 2020-10-14 16:23:58+00:00
- **Updated**: 2020-10-14 16:23:58+00:00
- **Authors**: Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Prior gradient-based attribution-map methods rely on handcrafted propagation rules for the non-linear/activation layers during the backward pass, so as to produce gradients of the input and then the attribution map. Despite the promising results achieved, such methods are sensitive to the non-informative high-frequency components and lack adaptability for various models and samples. In this paper, we propose a dedicated method to generate attribution maps that allow us to learn the propagation rules automatically, overcoming the flaws of the handcrafted ones. Specifically, we introduce a learnable plugin module, which enables adaptive propagation rules for each pixel, to the non-linear layers during the backward pass for mask generating. The masked input image is then fed into the model again to obtain new output that can be used as a guidance when combined with the original one. The introduced learnable module can be trained under any auto-grad framework with higher-order differential support. As demonstrated on five datasets and six network architectures, the proposed method yields state-of-the-art results and gives cleaner and more visually plausible attribution maps.



### PointManifold: Using Manifold Learning for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.07215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07215v2)
- **Published**: 2020-10-14 16:28:19+00:00
- **Updated**: 2020-10-16 06:32:05+00:00
- **Authors**: Dinghao Yang, Wei Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a point cloud classification method based on graph neural network and manifold learning. Different from the conventional point cloud analysis methods, this paper uses manifold learning algorithms to embed point cloud features for better considering the geometric continuity on the surface. Then, the nature of point cloud can be acquired in low dimensional space, and after being concatenated with features in the original three-dimensional (3D)space, both the capability of feature representation and the classification network performance can be improved. We pro-pose two manifold learning modules, where one is based on locally linear embedding algorithm, and the other is a non-linear projection method based on neural network architecture. Both of them can obtain better performances than the state-of-the-art baseline. Afterwards, the graph model is constructed by using the k nearest neighbors algorithm, where the edge features are effectively aggregated for the implementation of point cloud classification. Experiments show that the proposed point cloud classification methods obtain the mean class accuracy (mA) of 90.2% and the overall accuracy (oA)of 93.2%, which reach competitive performances compared with the existing state-of-the-art related methods.



### Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07217v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07217v5)
- **Published**: 2020-10-14 16:31:12+00:00
- **Updated**: 2021-10-24 07:58:40+00:00
- **Authors**: Xinyu Yang, Majid Mirmehdi, Tilo Burghardt
- **Comment**: accepted at BMVC
- **Journal**: None
- **Summary**: In this paper we show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach termed Cycle Encoding Prediction (CEP) that is able to effectively represent high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backward as well as backward-forward temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of the video stream and applies loss functions that encourage both temporal cycle closure as well as contrastive feature separation. Architecturally, the underpinning network structure utilises a single feature encoder for all video snippets, adding two predictive modules that learn temporal forward and backward transitions. We apply our framework for pretext training of networks for action recognition tasks. We report significantly improved results for the standard datasets UCF101 and HMDB51. Detailed ablation studies support the effectiveness of the proposed components. We publish source code for the CEP components in full with this paper.



### Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07219v3
- **DOI**: 10.1109/ACCESS.2020.3038926
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07219v3)
- **Published**: 2020-10-14 16:32:22+00:00
- **Updated**: 2020-12-14 20:47:52+00:00
- **Authors**: Vinicius M. de Pinho, Marcello L. R. de Campos, Luis Uzeda Garcia, Dalia Popescu
- **Comment**: Accepted for publication in the IEEE Access
- **Journal**: in IEEE Access, vol. 8, pp. 209619-209629, 2020
- **Summary**: 5G is designed to be an essential enabler and a leading infrastructure provider in the communication technology industry by supporting the demand for the growing data traffic and a variety of services with distinct requirements. The use of deep learning and computer vision tools has the means to increase the environmental awareness of the network with information from visual data. Information extracted via computer vision tools such as user position, movement direction, and speed can be promptly available for the network. However, the network must have a mechanism to match the identity of a user in both visual and radio systems. This mechanism is absent in the present literature. Therefore, we propose a framework to match the information from both visual and radio domains. This is an essential step to practical applications of computer vision tools in communications. We detail the proposed framework training and deployment phases for a presented setup. We carried out practical experiments using data collected in different types of environments. The work compares the use of Deep Neural Network and Random Forest classifiers and shows that the former performed better across all experiments, achieving classification accuracy greater than 99%.



### Domain Shift in Computer Vision models for MRI data analysis: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2010.07222v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07222v1)
- **Published**: 2020-10-14 16:34:21+00:00
- **Updated**: 2020-10-14 16:34:21+00:00
- **Authors**: Ekaterina Kondrateva, Marina Pominova, Elena Popova, Maxim Sharaev, Alexander Bernstein, Evgeny Burnaev
- **Comment**: 8 pages, 1 figure
- **Journal**: ICMV2020
- **Summary**: Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yetonly a few applications are now in clinical use and one of the reasons for that is poor transferability of themodels to data from different sources or acquisition domains. Development of new methods and algorithms forthe transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for thedevelopment of accurate models and their use in clinics. In present work, we overview methods used to tackle thedomain shift problem in machine learning and computer vision. The algorithms discussed in this survey includeadvanced data processing, model architecture enhancing and featured training, as well as predicting in domaininvariant latent space. The application of the autoencoding neural networks and their domain-invariant variationsare heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging(MRI) data analysis and conclude on their performance as well as propose directions for further research.



### Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video with Applications for Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2010.07704v2
- **DOI**: 10.1142/S1793351X20400139
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.07704v2)
- **Published**: 2020-10-14 16:41:33+00:00
- **Updated**: 2020-11-10 00:35:33+00:00
- **Authors**: Alisha Sharma, Ryan Nett, Jonathan Ventura
- **Comment**: Expansion on arXiv:1901.00979 for IJSC SI; correct table 1 and 3
  headings, reduce file size
- **Journal**: Int.J.Semantic Computing 14(3) (2020) 315-322
- **Summary**: We introduce a convolutional neural network model for unsupervised learning of depth and ego-motion from cylindrical panoramic video. Panoramic depth estimation is an important technology for applications such as virtual reality, 3D modeling, and autonomous robotic navigation. In contrast to previous approaches for applying convolutional neural networks to panoramic imagery, we use the cylindrical panoramic projection which allows for the use of the traditional CNN layers such as convolutional filters and max pooling without modification. Our evaluation of synthetic and real data shows that unsupervised learning of depth and ego-motion on cylindrical panoramic images can produce high-quality depth maps and that an increased field-of-view improves ego-motion estimation accuracy. We create two new datasets to evaluate our approach: a synthetic dataset created using the CARLA simulator, and Headcam, a novel dataset of panoramic video collected from a helmet-mounted camera while biking in an urban setting. We also apply our network to the problem of converting monocular panoramas to stereo panoramas.



### Fader Networks for domain adaptation on fMRI: ABIDE-II study
- **Arxiv ID**: http://arxiv.org/abs/2010.07233v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07233v1)
- **Published**: 2020-10-14 16:50:50+00:00
- **Updated**: 2020-10-14 16:50:50+00:00
- **Authors**: Marina Pominova, Ekaterina Kondrateva, Maxim Sharaev, Alexander Bernstein, Evgeny Burnaev
- **Comment**: None
- **Journal**: ICMV2020
- **Summary**: ABIDE is the largest open-source autism spectrum disorder database with both fMRI data and full phenotype description. These data were extensively studied based on functional connectivity analysis as well as with deep learning on raw data, with top models accuracy close to 75\% for separate scanning sites. Yet there is still a problem of models transferability between different scanning sites within ABIDE. In the current paper, we for the first time perform domain adaptation for brain pathology classification problem on raw neuroimaging data. We use 3D convolutional autoencoders to build the domain irrelevant latent space image representation and demonstrate this method to outperform existing approaches on ABIDE data.



### Self-Supervised Ranking for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07258v2)
- **Published**: 2020-10-14 17:24:56+00:00
- **Updated**: 2020-11-20 15:20:30+00:00
- **Authors**: Ali Varamesh, Ali Diba, Tinne Tuytelaars, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new framework for self-supervised representation learning by formulating it as a ranking problem in an image retrieval context on a large number of random views (augmentations) obtained from images. Our work is based on two intuitions: first, a good representation of images must yield a high-quality image ranking in a retrieval task; second, we would expect random views of an image to be ranked closer to a reference view of that image than random views of other images. Hence, we model representation learning as a learning to rank problem for image retrieval. We train a representation encoder by maximizing average precision (AP) for ranking, where random views of an image are considered positively related, and that of the other images considered negatives. The new framework, dubbed S2R2, enables computing a global objective on multiple views, compared to the local objective in the popular contrastive learning framework, which is calculated on pairs of views. In principle, by using a ranking criterion, we eliminate reliance on object-centric curated datasets. When trained on STL10 and MS-COCO, S2R2 outperforms SimCLR and the clustering-based contrastive learning model, SwAV, while being much simpler both conceptually and at implementation. On MS-COCO, S2R2 outperforms both SwAV and SimCLR with a larger margin than on STl10. This indicates that S2R2 is more effective on diverse scenes and could eliminate the need for an object-centric large training dataset for self-supervised representation learning.



### Privacy-Preserving Object Detection & Localization Using Distributed Machine Learning: A Case Study of Infant Eyeblink Conditioning
- **Arxiv ID**: http://arxiv.org/abs/2010.07259v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07259v1)
- **Published**: 2020-10-14 17:33:28+00:00
- **Updated**: 2020-10-14 17:33:28+00:00
- **Authors**: Stefan Zwaard, Henk-Jan Boele, Hani Alers, Christos Strydis, Casey Lew-Williams, Zaid Al-Ars
- **Comment**: This is a preprint version of "Privacy-Preserving Object Detection &
  Localization Using Distributed Machine Learning: A Case Study of Infant
  Eyeblink Conditioning". This work consists of 12 pages including refs and, 4
  tables and 7 figures
- **Journal**: None
- **Summary**: Distributed machine learning is becoming a popular model-training method due to privacy, computational scalability, and bandwidth capacities. In this work, we explore scalable distributed-training versions of two algorithms commonly used in object detection. A novel distributed training algorithm using Mean Weight Matrix Aggregation (MWMA) is proposed for Linear Support Vector Machine (L-SVM) object detection based in Histogram of Orientated Gradients (HOG). In addition, a novel Weighted Bin Aggregation (WBA) algorithm is proposed for distributed training of Ensemble of Regression Trees (ERT) landmark localization. Both algorithms do not restrict the location of model aggregation and allow custom architectures for model distribution. For this work, a Pool-Based Local Training and Aggregation (PBLTA) architecture for both algorithms is explored. The application of both algorithms in the medical field is examined using a paradigm from the fields of psychology and neuroscience - eyeblink conditioning with infants - where models need to be trained on facial images while protecting participant privacy. Using distributed learning, models can be trained without sending image data to other nodes. The custom software has been made available for public use on GitHub: https://github.com/SLWZwaard/DMT. Results show that the aggregation of models for the HOG algorithm using MWMA not only preserves the accuracy of the model but also allows for distributed learning with an accuracy increase of 0.9% compared with traditional learning. Furthermore, WBA allows for ERT model aggregation with an accuracy increase of 8% when compared to single-node models.



### A spatial model checker in GPU (extended version)
- **Arxiv ID**: http://arxiv.org/abs/2010.07284v1
- **DOI**: None
- **Categories**: **cs.LO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07284v1)
- **Published**: 2020-10-14 17:58:28+00:00
- **Updated**: 2020-10-14 17:58:28+00:00
- **Authors**: Laura Bussi, Vincenzo Ciancia, Fabio Gadducci
- **Comment**: None
- **Journal**: None
- **Summary**: The tool voxlogica merges the state-of-the-art library of computational imaging algorithms ITK with the combination of declarative specification and optimised execution provided by spatial logic model checking. The analysis of an existing benchmark for segmentation of brain tumours via a simple logical specification reached state-of-the-art accuracy. We present a new, GPU-based version of voxlogica and discuss its implementation, scalability, and applications.



### Towards Accurate Quantization and Pruning via Data-free Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.07334v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07334v1)
- **Published**: 2020-10-14 18:02:55+00:00
- **Updated**: 2020-10-14 18:02:55+00:00
- **Authors**: Chen Zhu, Zheng Xu, Ali Shafahi, Manli Shu, Amin Ghiasi, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: When large scale training data is available, one can obtain compact and accurate networks to be deployed in resource-constrained environments effectively through quantization and pruning. However, training data are often protected due to privacy concerns and it is challenging to obtain compact networks without data. We study data-free quantization and pruning by transferring knowledge from trained large networks to compact networks. Auxiliary generators are simultaneously and adversarially trained with the targeted compact networks to generate synthetic inputs that maximize the discrepancy between the given large network and its quantized or pruned version. We show theoretically that the alternating optimization for the underlying minimax problem converges under mild conditions for pruning and quantization. Our data-free compact networks achieve competitive accuracy to networks trained and fine-tuned with training data. Our quantized and pruned networks achieve good performance while being more compact and lightweight. Further, we demonstrate that the compact structure and corresponding initialization from the Lottery Ticket Hypothesis can also help in data-free training.



### Matching-space Stereo Networks for Cross-domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2010.07347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07347v1)
- **Published**: 2020-10-14 18:29:20+00:00
- **Updated**: 2020-10-14 18:29:20+00:00
- **Authors**: Changjiang Cai, Matteo Poggi, Stefano Mattoccia, Philippos Mordohai
- **Comment**: 14 pages, 8 figures, International Conference on 3D Vision
  (3DV'2020), Github code at https://github.com/ccj5351/MS-Nets
- **Journal**: None
- **Summary**: End-to-end deep networks represent the state of the art for stereo matching. While excelling on images framing environments similar to the training set, major drops in accuracy occur in unseen domains (e.g., when moving from synthetic to real scenes). In this paper we introduce a novel family of architectures, namely Matching-Space Networks (MS-Nets), with improved generalization properties. By replacing learning-based feature extraction from image RGB values with matching functions and confidence measures from conventional wisdom, we move the learning process from the color space to the Matching Space, avoiding over-specialization to domain specific features. Extensive experimental results on four real datasets highlight that our proposal leads to superior generalization to unseen environments over conventional deep architectures, keeping accuracy on the source domain almost unaltered. Our code is available at https://github.com/ccj5351/MS-Nets.



### Do End-to-end Stereo Algorithms Under-utilize Information?
- **Arxiv ID**: http://arxiv.org/abs/2010.07350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07350v1)
- **Published**: 2020-10-14 18:32:39+00:00
- **Updated**: 2020-10-14 18:32:39+00:00
- **Authors**: Changjiang Cai, Philippos Mordohai
- **Comment**: 13 pages, 10 figures, International Conference on 3D Vision
  (3DV'2020)
- **Journal**: None
- **Summary**: Deep networks for stereo matching typically leverage 2D or 3D convolutional encoder-decoder architectures to aggregate cost and regularize the cost volume for accurate disparity estimation. Due to content-insensitive convolutions and down-sampling and up-sampling operations, these cost aggregation mechanisms do not take full advantage of the information available in the images. Disparity maps suffer from over-smoothing near occlusion boundaries, and erroneous predictions in thin structures. In this paper, we show how deep adaptive filtering and differentiable semi-global aggregation can be integrated in existing 2D and 3D convolutional networks for end-to-end stereo matching, leading to improved accuracy. The improvements are due to utilizing RGB information from the images as a signal to dynamically guide the matching process, in addition to being the signal we attempt to match across the images. We show extensive experimental results on the KITTI 2015 and Virtual KITTI 2 datasets comparing four stereo networks (DispNetC, GCNet, PSMNet and GANet) after integrating four adaptive filters (segmentation-aware bilateral filtering, dynamic filtering networks, pixel adaptive convolution and semi-global aggregation) into their architectures. Our code is available at https://github.com/ccj5351/DAFStereoNets.



### Photovoltaic module segmentation and thermal analysis tool from thermal images
- **Arxiv ID**: http://arxiv.org/abs/2010.07356v1
- **DOI**: 10.1109/ROPEC50909.2020.9258760
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.07356v1)
- **Published**: 2020-10-14 18:43:01+00:00
- **Updated**: 2020-10-14 18:43:01+00:00
- **Authors**: L. E. Montañez, L. M. Valentín-Coronado, D. Moctezuma, G. Flores
- **Comment**: 7 pages, 12 Figures
- **Journal**: None
- **Summary**: The growing interest in the use of clean energy has led to the construction of increasingly large photovoltaic systems. Consequently, monitoring the proper functioning of these systems has become a highly relevant issue.In this paper, automatic detection, and analysis of photovoltaic modules are proposed. To perform the analysis, a module identification step, based on a digital image processing algorithm, is first carried out. This algorithm consists of image enhancement (contrast enhancement, noise reduction, etc.), followed by segmentation of the photovoltaic module. Subsequently, a statistical analysis based on the temperature values of the segmented module is performed.Besides, a graphical user interface has been designed as a potential tool that provides relevant information of the photovoltaic modules.



### Deep Learning in Ultrasound Elastography Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.07360v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07360v2)
- **Published**: 2020-10-14 18:50:40+00:00
- **Updated**: 2020-10-31 18:59:13+00:00
- **Authors**: Hongliang Li, Manish Bhatt, Zhen Qu, Shiming Zhang, Martin C. Hartel, Ali Khademhosseini, Guy Cloutier
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that changes in the mechanical properties of tissues are associated with the onset and progression of certain diseases. Ultrasound elastography is a technique to characterize tissue stiffness using ultrasound imaging either by measuring tissue strain using quasi-static elastography or natural organ pulsation elastography, or by tracing a propagated shear wave induced by a source or a natural vibration using dynamic elastography. In recent years, deep learning has begun to emerge in ultrasound elastography research. In this review, several common deep learning frameworks in the computer vision community, such as multilayer perceptron, convolutional neural network, and recurrent neural network are described. Then, recent advances in ultrasound elastography using such deep learning techniques are revisited in terms of algorithm development and clinical diagnosis. Finally, the current challenges and future developments of deep learning in ultrasound elastography are prospected.



### Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.07367v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07367v2)
- **Published**: 2020-10-14 19:06:23+00:00
- **Updated**: 2021-01-18 16:15:31+00:00
- **Authors**: Shijie Li, Jinhui Yi, Yazan Abu Farha, Juergen Gall
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)
- **Journal**: None
- **Summary**: With the advances in capturing 2D or 3D skeleton data, skeleton-based action recognition has received an increasing interest over the last years. As skeleton data is commonly represented by graphs, graph convolutional networks have been proposed for this task. While current graph convolutional networks accurately recognize actions, they are too expensive for robotics applications where limited computational resources are available. In this paper, we therefore propose a highly efficient graph convolutional network that addresses the limitations of previous works. This is achieved by a parallel structure that gradually fuses motion and spatial information and by reducing the temporal resolution as early as possible. Furthermore, we explicitly address the issue that human poses can contain errors. To this end, the network first refines the poses before they are further processed to recognize the action. We therefore call the network Pose Refinement Graph Convolutional Network. Compared to other graph convolutional networks, our network requires 86\%-93\% less parameters and reduces the floating point operations by 89%-96% while achieving a comparable accuracy. It therefore provides a much better trade-off between accuracy, memory footprint and processing time, which makes it suitable for robotics applications.



### Harnessing Uncertainty in Domain Adaptation for MRI Prostate Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.07411v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07411v2)
- **Published**: 2020-10-14 21:30:27+00:00
- **Updated**: 2021-01-18 18:54:13+00:00
- **Authors**: Eleni Chiou, Francesco Giganti, Shonit Punwani, Iasonas Kokkinos, Eleftheria Panagiotaki
- **Comment**: Accepted at MICCAI 2020. Code is available at
  https://github.com/elchiou/DA
- **Journal**: None
- **Summary**: The need for training data can impede the adoption of novel imaging modalities for learning-based medical image analysis. Domain adaptation methods partially mitigate this problem by translating training data from a related source domain to a novel target domain, but typically assume that a one-to-one translation is possible. Our work addresses the challenge of adapting to a more informative target domain where multiple target samples can emerge from a single source sample. In particular we consider translating from mp-MRI to VERDICT, a richer MRI modality involving an optimized acquisition protocol for cancer characterization. We explicitly account for the inherent uncertainty of this mapping and exploit it to generate multiple outputs conditioned on a single input. Our results show that this allows us to extract systematically better image representations for the target domain, when used in tandem with both simple, CycleGAN-based baselines, as well as more powerful approaches that integrate discriminative segmentation losses and/or residual adapters. When compared to its deterministic counterparts, our approach yields substantial improvements across a broad range of dataset sizes, increasingly strong baselines, and evaluation measures.



### Skeleton-bridged Point Completion: From Global Inference to Local Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2010.07428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07428v1)
- **Published**: 2020-10-14 22:49:30+00:00
- **Updated**: 2020-10-14 22:49:30+00:00
- **Authors**: Yinyu Nie, Yiqun Lin, Xiaoguang Han, Shihui Guo, Jian Chang, Shuguang Cui, Jian Jun Zhang
- **Comment**: Accepted by NeurIPS 2020; Project Page:
  https://yinyunie.github.io/SKPCN-page/
- **Journal**: None
- **Summary**: Point completion refers to complete the missing geometries of objects from partial point clouds. Existing works usually estimate the missing shape by decoding a latent feature encoded from the input points. However, real-world objects are usually with diverse topologies and surface details, which a latent feature may fail to represent to recover a clean and complete surface. To this end, we propose a skeleton-bridged point completion network (SK-PCN) for shape completion. Given a partial scan, our method first predicts its 3D skeleton to obtain the global structure, and completes the surface by learning displacements from skeletal points. We decouple the shape completion into structure estimation and surface reconstruction, which eases the learning difficulty and benefits our method to obtain on-surface details. Besides, considering the missing features during encoding input points, SK-PCN adopts a local adjustment strategy that merges the input point cloud to our predictions for surface refinement. Comparing with previous methods, our skeleton-bridged manner better supports point normal estimation to obtain the full surface mesh beyond point clouds. The qualitative and quantitative experiments on both point cloud and mesh completion show that our approach outperforms the existing methods on various object categories.



### Viewmaker Networks: Learning Views for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07432v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07432v2)
- **Published**: 2020-10-14 23:03:31+00:00
- **Updated**: 2021-03-29 06:49:09+00:00
- **Authors**: Alex Tamkin, Mike Wu, Noah Goodman
- **Comment**: ICLR 2021
- **Journal**: None
- **Summary**: Many recent methods for unsupervised representation learning train models to be invariant to different "views," or distorted versions of an input. However, designing these views requires considerable trial and error by human experts, hindering widespread adoption of unsupervised representation learning methods across domains and modalities. To address this, we propose viewmaker networks: generative models that learn to produce useful views from a given input. Viewmakers are stochastic bounded adversaries: they produce views by generating and then adding an $\ell_p$-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. Remarkably, when pretraining on CIFAR-10, our learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations -- despite not including transformations like cropping or color jitter. Furthermore, our learned views significantly outperform baseline augmentations on speech recordings (+9% points, on average) and wearable sensor data (+17% points). Viewmakers can also be combined with handcrafted views: they improve robustness to common image corruptions and can increase transfer performance in cases where handcrafted views are less explored. These results suggest that viewmakers may provide a path towards more general representation learning algorithms -- reducing the domain expertise and effort needed to pretrain on a much wider set of domains. Code is available at https://github.com/alextamkin/viewmaker.



### Auto-calibration Method Using Stop Signs for Urban Autonomous Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2010.07441v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.07441v2)
- **Published**: 2020-10-14 23:56:47+00:00
- **Updated**: 2021-03-18 19:01:05+00:00
- **Authors**: Yunhai Han, Yuhan Liu, David Paz, Henrik Christensen
- **Comment**: 7 pages, 7 figures, 1 table, Accepted to ICRA 2021
- **Journal**: None
- **Summary**: Calibration of sensors is fundamental to robust performance for intelligent vehicles. In natural environments, disturbances can easily challenge calibration. One possibility is to use natural objects of known shape to recalibrate sensors. An approach based on recognition of traffic signs, such as stop signs, and use of them for recalibration of cameras is presented. The approach is based on detection, geometry estimation, calibration, and recursive updating. Results from natural environments are presented that clearly show convergence and improved performance.



