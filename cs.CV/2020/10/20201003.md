# Arxiv Papers in cs.CV on 2020-10-03
### A Deep Genetic Programming based Methodology for Art Media Classification Robust to Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2010.01238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2010.01238v1)
- **Published**: 2020-10-03 00:36:34+00:00
- **Updated**: 2020-10-03 00:36:34+00:00
- **Authors**: Gustavo Olague, Gerardo Ibarra-Vazquez, Mariana Chan-Ley, Cesar Puente, Carlos Soubervielle-Montalvo, Axel Martinez
- **Comment**: 13 pages, 3 figures, International Symposium on Visual Computing 2020
- **Journal**: None
- **Summary**: Art Media Classification problem is a current research area that has attracted attention due to the complex extraction and analysis of features of high-value art pieces. The perception of the attributes can not be subjective, as humans sometimes follow a biased interpretation of artworks while ensuring automated observation's trustworthiness. Machine Learning has outperformed many areas through its learning process of artificial feature extraction from images instead of designing handcrafted feature detectors. However, a major concern related to its reliability has brought attention because, with small perturbations made intentionally in the input image (adversarial attack), its prediction can be completely changed. In this manner, we foresee two ways of approaching the situation: (1) solve the problem of adversarial attacks in current neural networks methodologies, or (2) propose a different approach that can challenge deep learning without the effects of adversarial attacks. The first one has not been solved yet, and adversarial attacks have become even more complex to defend. Therefore, this work presents a Deep Genetic Programming method, called Brain Programming, that competes with deep learning and studies the transferability of adversarial attacks using two artworks databases made by art experts. The results show that the Brain Programming method preserves its performance in comparison with AlexNet, making it robust to these perturbations and competing to the performance of Deep Learning.



### Improving Network Slimming with Nonconvex Regularization
- **Arxiv ID**: http://arxiv.org/abs/2010.01242v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01242v4)
- **Published**: 2020-10-03 01:04:02+00:00
- **Updated**: 2021-08-18 23:51:15+00:00
- **Authors**: Kevin Bui, Fredrick Park, Shuai Zhang, Yingyong Qi, Jack Xin
- **Comment**: version1 published in ISVC'20; version 2: fixed typo; version3 is the
  extended version and submitted to a journal; version 4: more typos fixed,
  official version will be on IEEE Access
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have developed to become powerful models for various computer vision tasks ranging from object detection to semantic segmentation. However, most of the state-of-the-art CNNs cannot be deployed directly on edge devices such as smartphones and drones, which need low latency under limited power and memory bandwidth. One popular, straightforward approach to compressing CNNs is network slimming, which imposes $\ell_1$ regularization on the channel-associated scaling factors via the batch normalization layers during training. Network slimming thereby identifies insignificant channels that can be pruned for inference. In this paper, we propose replacing the $\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty in order to yield a more compressed and/or accurate CNN architecture. We investigate $\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to their recent successes and popularity in solving sparse optimization problems, such as compressed sensing and variable selection. We demonstrate the effectiveness of network slimming with nonconvex penalties on three neural network architectures -- VGG-19, DenseNet-40, and ResNet-164 -- on standard image classification datasets. Based on the numerical experiments, T$\ell_1$ preserves model accuracy against channel pruning, $\ell_{1/2, 3/4}$ yield better compressed models with similar accuracies after retraining as $\ell_1$, and MCP and SCAD provide more accurate models after retraining with similar compression as $\ell_1$. Network slimming with T$\ell_1$ regularization also outperforms the latest Bayesian modification of network slimming in compressing a CNN architecture in terms of memory storage while preserving its model accuracy after channel pruning.



### Consensus Clustering With Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01245v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01245v2)
- **Published**: 2020-10-03 01:16:46+00:00
- **Updated**: 2021-07-08 17:20:52+00:00
- **Authors**: Jayanth Reddy Regatti, Aniket Anand Deshmukh, Eren Manavoglu, Urun Dogan
- **Comment**: Accepted by the 2021 International Joint Conference on Neural
  Networks (IJCNN 2021)
- **Journal**: None
- **Summary**: Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must either be closer in the representation space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL) is one such representation learning algorithm that has achieved state-of-the-art results in self-supervised image classification on ImageNet under the linear evaluation protocol. However, the utility of the learnt features of BYOL to perform clustering is not explored. In this work, we study the clustering ability of BYOL and observe that features learnt using BYOL may not be optimal for clustering. We propose a novel consensus clustering based loss function, and train BYOL with the proposed loss in an end-to-end way that improves the clustering ability and outperforms similar clustering based methods on some popular computer vision datasets.



### 3D-Aided Data Augmentation for Robust Face Understanding
- **Arxiv ID**: http://arxiv.org/abs/2010.01246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01246v2)
- **Published**: 2020-10-03 01:18:07+00:00
- **Updated**: 2020-10-06 02:55:36+00:00
- **Authors**: Yifan Xing, Yuanjun Xiong, Wei Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation has been highly effective in narrowing the data gap and reducing the cost for human annotation, especially for tasks where ground truth labels are difficult and expensive to acquire. In face recognition, large pose and illumination variation of face images has been a key factor for performance degradation. However, human annotation for the various face understanding tasks including face landmark localization, face attributes classification and face recognition under these challenging scenarios are highly costly to acquire. Therefore, it would be desirable to perform data augmentation for these cases. But simple 2D data augmentation techniques on the image domain are not able to satisfy the requirement of these challenging cases. As such, 3D face modeling, in particular, single image 3D face modeling, stands a feasible solution for these challenging conditions beyond 2D based data augmentation. To this end, we propose a method that produces realistic 3D augmented images from multiple viewpoints with different illumination conditions through 3D face modeling, each associated with geometrically accurate face landmarks, attributes and identity information. Experiments demonstrate that the proposed 3D data augmentation method significantly improves the performance and robustness of various face understanding tasks while achieving state-of-arts on multiple benchmarks.



### CorrAttack: Black-box Adversarial Attack with Structured Search
- **Arxiv ID**: http://arxiv.org/abs/2010.01250v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01250v1)
- **Published**: 2020-10-03 01:44:16+00:00
- **Updated**: 2020-10-03 01:44:16+00:00
- **Authors**: Zhichao Huang, Yaowei Huang, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for score-based adversarial attack, where the attacker queries the loss-oracle of the target model. Our method employs a parameterized search space with a structure that captures the relationship of the gradient of the loss function. We show that searching over the structured space can be approximated by a time-varying contextual bandits problem, where the attacker takes feature of the associated arm to make modifications of the input, and receives an immediate reward as the reduction of the loss function. The time-varying contextual bandits problem can then be solved by a Bayesian optimization procedure, which can take advantage of the features of the structured action space. The experiments on ImageNet and the Google Cloud Vision API demonstrate that the proposed method achieves the state of the art success rates and query efficiencies for both undefended and defended models.



### UCP: Uniform Channel Pruning for Deep Convolutional Neural Networks Compression and Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2010.01251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01251v1)
- **Published**: 2020-10-03 01:51:06+00:00
- **Updated**: 2020-10-03 01:51:06+00:00
- **Authors**: Jingfei Chang, Yang Lu, Ping Xue, Xing Wei, Zhen Wei
- **Comment**: 21 pages,7 figures and 5 tables
- **Journal**: None
- **Summary**: To apply deep CNNs to mobile terminals and portable devices, many scholars have recently worked on the compressing and accelerating deep convolutional neural networks. Based on this, we propose a novel uniform channel pruning (UCP) method to prune deep CNN, and the modified squeeze-and-excitation blocks (MSEB) is used to measure the importance of the channels in the convolutional layers. The unimportant channels, including convolutional kernels related to them, are pruned directly, which greatly reduces the storage cost and the number of calculations. There are two types of residual blocks in ResNet. For ResNet with bottlenecks, we use the pruning method with traditional CNN to trim the 3x3 convolutional layer in the middle of the blocks. For ResNet with basic residual blocks, we propose an approach to consistently prune all residual blocks in the same stage to ensure that the compact network structure is dimensionally correct. Considering that the network loses considerable information after pruning and that the larger the pruning amplitude is, the more information that will be lost, we do not choose fine-tuning but retrain from scratch to restore the accuracy of the network after pruning. Finally, we verified our method on CIFAR-10, CIFAR-100 and ILSVRC-2012 for image classification. The results indicate that the performance of the compact network after retraining from scratch, when the pruning rate is small, is better than the original network. Even when the pruning amplitude is large, the accuracy can be maintained or decreased slightly. On the CIFAR-100, when reducing the parameters and FLOPs up to 82% and 62% respectively, the accuracy of VGG-19 even improved by 0.54% after retraining.



### WeMix: How to Better Utilize Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.01267v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01267v1)
- **Published**: 2020-10-03 03:12:18+00:00
- **Updated**: 2020-10-03 03:12:18+00:00
- **Authors**: Yi Xu, Asaf Noy, Ming Lin, Qi Qian, Hao Li, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a widely used training trick in deep learning to improve the network generalization ability. Despite many encouraging results, several recent studies did point out limitations of the conventional data augmentation scheme in certain scenarios, calling for a better theoretical understanding of data augmentation. In this work, we develop a comprehensive analysis that reveals pros and cons of data augmentation. The main limitation of data augmentation arises from the data bias, i.e. the augmented data distribution can be quite different from the original one. This data bias leads to a suboptimal performance of existing data augmentation methods. To this end, we develop two novel algorithms, termed "AugDrop" and "MixLoss", to correct the data bias in the data augmentation. Our theoretical analysis shows that both algorithms are guaranteed to improve the effect of data augmentation through the bias correction, which is further validated by our empirical studies. Finally, we propose a generic algorithm "WeMix" by combining AugDrop and MixLoss, whose effectiveness is observed from extensive empirical evaluations.



### TCLNet: Learning to Locate Typhoon Center Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2010.01282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01282v2)
- **Published**: 2020-10-03 05:34:00+00:00
- **Updated**: 2021-05-30 07:19:32+00:00
- **Authors**: Chao Tan
- **Comment**: None
- **Journal**: None
- **Summary**: The task of typhoon center location plays an important role in typhoon intensity analysis and typhoon path prediction. Conventional typhoon center location algorithms mostly rely on digital image processing and mathematical morphology operation, which achieve limited performance. In this paper, we proposed an efficient fully convolutional end-to-end deep neural network named TCLNet to automatically locate the typhoon center position. We design the network structure carefully so that our TCLNet can achieve remarkable performance base on its lightweight architecture. In addition, we also present a brand new large-scale typhoon center location dataset (TCLD) so that the TCLNet can be trained in a supervised manner. Furthermore, we propose to use a novel TCL+ piecewise loss function to further improve the performance of TCLNet. Extensive experimental results and comparison demonstrate the performance of our model, and our TCLNet achieve a 14.4% increase in accuracy on the basis of a 92.7% reduction in parameters compared with SOTA deep learning based typhoon center location methods.



### Generating the Cloud Motion Winds Field from Satellite Cloud Imagery Using Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2010.01283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01283v2)
- **Published**: 2020-10-03 05:40:36+00:00
- **Updated**: 2021-05-30 07:21:57+00:00
- **Authors**: Chao Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Cloud motion winds (CMW) are routinely derived by tracking features in sequential geostationary satellite infrared cloud imagery. In this paper, we explore the cloud motion winds algorithm based on data-driven deep learning approach, and different from conventional hand-craft feature tracking and correlation matching algorithms, we use deep learning model to automatically learn the motion feature representations and directly output the field of cloud motion winds. In addition, we propose a novel large-scale cloud motion winds dataset (CMWD) for training deep learning models. We also try to use a single cloud imagery to predict the cloud motion winds field in a fixed region, which is impossible to achieve using traditional algorithms. The experimental results demonstrate that our algorithm can predict the cloud motion winds field efficiently, and even with a single cloud imagery as input.



### Unsupervised Shadow Removal Using Target Consistency Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2010.01291v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01291v2)
- **Published**: 2020-10-03 06:55:26+00:00
- **Updated**: 2021-05-30 08:07:03+00:00
- **Authors**: Chao Tan, Xin Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised shadow removal aims to learn a non-linear function to map the original image from shadow domain to non-shadow domain in the absence of paired shadow and non-shadow data. In this paper, we develop a simple yet efficient target-consistency generative adversarial network (TC-GAN) for the shadow removal task in the unsupervised manner. Compared with the bidirectional mapping in cycle-consistency GAN based methods for shadow removal, TC-GAN tries to learn a one-sided mapping to cast shadow images into shadow-free ones. With the proposed target-consistency constraint, the correlations between shadow images and the output shadow-free image are strictly confined. Extensive comparison experiments results show that TC-GAN outperforms the state-of-the-art unsupervised shadow removal methods by 14.9% in terms of FID and 31.5% in terms of KID. It is rather remarkable that TC-GAN achieves comparable performance with supervised shadow removal methods.



### Deep Convolutional Neural Networks Model-based Brain Tumor Detection in Brain MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2010.11978v1
- **DOI**: 10.1109/I-SMAC49090.2020.9243461
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.11978v1)
- **Published**: 2020-10-03 07:42:17+00:00
- **Updated**: 2020-10-03 07:42:17+00:00
- **Authors**: Md. Abu Bakr Siddique, Shadman Sakib, Mohammad Mahmudur Rahman Khan, Abyaz Kader Tanzeem, Madiha Chowdhury, Nowrin Yasmin
- **Comment**: 4th International conference on I-SMAC (IoT in Social, Mobile,
  Analytics and Cloud) (I-SMAC 2020), IEEE, 7-9 October 2020, TamilNadu, INDIA
- **Journal**: 2020 Fourth International Conference on I-SMAC (IoT in Social,
  Mobile, Analytics and Cloud) (I-SMAC)
- **Summary**: Diagnosing Brain Tumor with the aid of Magnetic Resonance Imaging (MRI) has gained enormous prominence over the years, primarily in the field of medical science. Detection and/or partitioning of brain tumors solely with the aid of MR imaging is achieved at the cost of immense time and effort and demands a lot of expertise from engaged personnel. This substantiates the necessity of fabricating an autonomous model brain tumor diagnosis. Our work involves implementing a deep convolutional neural network (DCNN) for diagnosing brain tumors from MR images. The dataset used in this paper consists of 253 brain MR images where 155 images are reported to have tumors. Our model can single out the MR images with tumors with an overall accuracy of 96%. The model outperformed the existing conventional methods for the diagnosis of brain tumor in the test dataset (Precision = 0.93, Sensitivity = 1.00, and F1-score = 0.97). Moreover, the proposed model's average precision-recall score is 0.93, Cohen's Kappa 0.91, and AUC 0.95. Therefore, the proposed model can help clinical experts verify whether the patient has a brain tumor and, consequently, accelerate the treatment procedure.



### Deep Convolutional Neural Network Based Facial Expression Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2010.01301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01301v1)
- **Published**: 2020-10-03 08:17:00+00:00
- **Updated**: 2020-10-03 08:17:00+00:00
- **Authors**: Hafiq Anas, Bacha Rehman, Wee Hong Ong
- **Comment**: 3 pages, 1 figure
- **Journal**: None
- **Summary**: This paper describes the proposed methodology, data used and the results of our participation in the ChallengeTrack 2 (Expr Challenge Track) of the Affective Behavior Analysis in-the-wild (ABAW) Competition 2020. In this competition, we have used a proposed deep convolutional neural network (CNN) model to perform automatic facial expression recognition (AFER) on the given dataset. Our proposed model has achieved an accuracy of 50.77% and an F1 score of 29.16% on the validation set.



### Bounding Boxes Are All We Need: Street View Image Classification via Context Encoding of Detected Buildings
- **Arxiv ID**: http://arxiv.org/abs/2010.01305v2
- **DOI**: 10.1109/TGRS.2021.3064316
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01305v2)
- **Published**: 2020-10-03 08:49:51+00:00
- **Updated**: 2020-10-12 05:52:29+00:00
- **Authors**: Kun Zhao, Yongkun Liu, Siyuan Hao, Shaoxing Lu, Hongbin Liu, Lijian Zhou
- **Comment**: Figure 1 has been added, and the order of the rest of the figures
  continues. Figure 6 (Figure 5 of the previous version) and Figure 7 (Figure 6
  of the previous version) have been modified. Figure 7, Figure 15, and Figure
  16 of the previous versionhave have been removed. The structure of Section 4
  has been adjusted
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing 2021 (Early
  Access)
- **Summary**: Street view images classification aiming at urban land use analysis is difficult because the class labels (e.g., commercial area), are concepts with higher abstract level compared to the ones of general visual tasks (e.g., persons and cars). Therefore, classification models using only visual features often fail to achieve satisfactory performance. In this paper, a novel approach based on a "Detector-Encoder-Classifier" framework is proposed. Instead of using visual features of the whole image directly as common image-level models based on convolutional neural networks (CNNs) do, the proposed framework firstly obtains the bounding boxes of buildings in street view images from a detector. Their contextual information such as the co-occurrence patterns of building classes and their layout are then encoded into metadata by the proposed algorithm "CODING" (Context encOding of Detected buildINGs). Finally, these bounding box metadata are classified by a recurrent neural network (RNN). In addition, we made a dual-labeled dataset named "BEAUTY" (Building dEtection And Urban funcTional-zone portraYing) of 19,070 street view images and 38,857 buildings based on the existing BIC GSV [1]. The dataset can be used not only for street view image classification, but also for multi-class building detection. Experiments on "BEAUTY" show that the proposed approach achieves a 12.65% performance improvement on macro-precision and 12% on macro-recall over image-level CNN based models. Our code and dataset are available at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/



### A simulation environment for drone cinematography
- **Arxiv ID**: http://arxiv.org/abs/2010.01315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01315v1)
- **Published**: 2020-10-03 09:57:56+00:00
- **Updated**: 2020-10-03 09:57:56+00:00
- **Authors**: Fan Zhang, David Hall, Tao Xu, Stephen Boyle, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a workflow for the simulation of drone operations exploiting realistic background environments constructed within Unreal Engine 4 (UE4). Methods for environmental image capture, 3D reconstruction (photogrammetry) and the creation of foreground assets are presented along with a flexible and user-friendly simulation interface. Given the geographical location of the selected area and the camera parameters employed, the scanning strategy and its associated flight parameters are first determined for image capture. Source imagery can be extracted from virtual globe software or obtained through aerial photography of the scene (e.g. using drones). The latter case is clearly more time consuming but can provide enhanced detail, particularly where coverage of virtual globe software is limited. The captured images are then used to generate 3D background environment models employing photogrammetry software. The reconstructed 3D models are then imported into the simulation interface as background environment assets together with appropriate foreground object models as a basis for shot planning and rehearsal. The tool supports both free-flight and parameterisable standard shot types along with programmable scenarios associated with foreground assets and event dynamics. It also supports the exporting of flight plans. Camera shots can also be designed to provide suitable coverage of any landmarks which need to appear in-shot. This simulation tool will contribute to enhanced productivity, improved safety (awareness and mitigations for crowds and buildings), improved confidence of operators and directors and ultimately enhanced quality of viewer experience.



### Gaussian Vector: An Efficient Solution for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.01318v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2010.01318v1)
- **Published**: 2020-10-03 10:15:41+00:00
- **Updated**: 2020-10-03 10:15:41+00:00
- **Authors**: Yilin Xiong, Zijian Zhou, Yuhao Dou, Zhizhong Su
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Significant progress has been made in facial landmark detection with the development of Convolutional Neural Networks. The widely-used algorithms can be classified into coordinate regression methods and heatmap based methods. However, the former loses spatial information, resulting in poor performance while the latter suffers from large output size or high post-processing complexity. This paper proposes a new solution, Gaussian Vector, to preserve the spatial information as well as reduce the output size and simplify the post-processing. Our method provides novel vector supervision and introduces Band Pooling Module to convert heatmap into a pair of vectors for each landmark. This is a plug-and-play component which is simple and effective. Moreover, Beyond Box Strategy is proposed to handle the landmarks out of the face bounding box. We evaluate our method on 300W, COFW, WFLW and JD-landmark. That the results significantly surpass previous works demonstrates the effectiveness of our approach.



### End-to-End Training of CNN Ensembles for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2010.01342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01342v1)
- **Published**: 2020-10-03 12:40:13+00:00
- **Updated**: 2020-10-03 12:40:13+00:00
- **Authors**: Ayse Serbetci, Yusuf Sinan Akgul
- **Comment**: None
- **Journal**: Pattern Recognition, 104, 107319 (2020)
- **Summary**: We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.



### A Variational Information Bottleneck Based Method to Compress Sequential Networks for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.01343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2010.01343v2)
- **Published**: 2020-10-03 12:41:51+00:00
- **Updated**: 2020-11-09 14:36:53+00:00
- **Authors**: Ayush Srivastava, Oshin Dutta, Prathosh AP, Sumeet Agarwal, Jigyasa Gupta
- **Comment**: Accepted at IEEE WACV 2021
- **Journal**: None
- **Summary**: In the last few years, compression of deep neural networks has become an important strand of machine learning and computer vision research. Deep models require sizeable computational complexity and storage, when used for instance for Human Action Recognition (HAR) from videos, making them unsuitable to be deployed on edge devices. In this paper, we address this issue and propose a method to effectively compress Recurrent Neural Networks (RNNs) such as Gated Recurrent Units (GRUs) and Long-Short-Term-Memory Units (LSTMs) that are used for HAR. We use a Variational Information Bottleneck (VIB) theory-based pruning approach to limit the information flow through the sequential cells of RNNs to a small subset. Further, we combine our pruning method with a specific group-lasso regularization technique that significantly improves compression. The proposed techniques reduce model parameters and memory footprint from latent representations, with little or no reduction in the validation accuracy while increasing the inference speed several-fold. We perform experiments on the three widely used Action Recognition datasets, viz. UCF11, HMDB51, and UCF101, to validate our approach. It is shown that our method achieves over 70 times greater compression than the nearest competitor with comparable accuracy for the task of action recognition on UCF11.



### Actionet: An Interactive End-To-End Platform For Task-Based Data Collection And Augmentation In 3D Environment
- **Arxiv ID**: http://arxiv.org/abs/2010.01357v1
- **DOI**: 10.1109/ICIP40778.2020.9191324
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01357v1)
- **Published**: 2020-10-03 13:37:01+00:00
- **Updated**: 2020-10-03 13:37:01+00:00
- **Authors**: Jiafei Duan, Samson Yu, Hui Li Tan, Cheston Tan
- **Comment**: https://github.com/SamsonYuBaiJian/actionet
- **Journal**: None
- **Summary**: The problem of task planning for artificial agents remains largely unsolved. While there has been increasing interest in data-driven approaches for the study of task planning for artificial agents, a significant remaining bottleneck is the dearth of large-scale comprehensive task-based datasets. In this paper, we present ActioNet, an interactive end-to-end platform for data collection and augmentation of task-based dataset in 3D environment. Using ActioNet, we collected a large-scale comprehensive task-based dataset, comprising over 3000 hierarchical task structures and videos. Using the hierarchical task structures, the videos are further augmented across 50 different scenes to give over 150,000 video. To our knowledge, ActioNet is the first interactive end-to-end platform for such task-based dataset generation and the accompanying dataset is the largest task-based dataset of such comprehensive nature. The ActioNet platform and dataset will be made available to facilitate research in hierarchical task planning.



### COVID-19 Classification of X-ray Images Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01362v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01362v2)
- **Published**: 2020-10-03 13:57:08+00:00
- **Updated**: 2020-10-07 08:28:41+00:00
- **Authors**: Elisha Goldstein, Daphna Keidar, Daniel Yaron, Yair Shachar, Ayelet Blass, Leonid Charbinsky, Israel Aharony, Liza Lifshitz, Dimitri Lumelsky, Ziv Neeman, Matti Mizrachi, Majd Hajouj, Nethanel Eizenbach, Eyal Sela, Chedva S Weiss, Philip Levin, Ofer Benjaminov, Gil N Bachar, Shlomit Tamir, Yael Rapson, Dror Suhami, Amiel A Dror, Naama R Bogot, Ahuva Grubstein, Nogah Shabshin, Yishai M Elyada, Yonina C Eldar
- **Comment**: Elisha Goldstein, Daphna Keidar, and Daniel Yaron have made an equal
  contribution and are equal first authors, listed alphabetically
- **Journal**: None
- **Summary**: In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray (CXR) imaging is playing an important role in the diagnosis and monitoring of patients with COVID-19. Machine learning solutions have been shown to be useful for X-ray analysis and classification in a range of medical contexts. The purpose of this study is to create and evaluate a machine learning model for diagnosis of COVID-19, and to provide a tool for searching for similar patients according to their X-ray scans. In this retrospective study, a classifier was built using a pre-trained deep learning model (ReNet50) and enhanced by data augmentation and lung segmentation to detect COVID-19 in frontal CXR images collected between January 2018 and July 2020 in four hospitals in Israel. A nearest-neighbors algorithm was implemented based on the network results that identifies the images most similar to a given image. The model was evaluated using accuracy, sensitivity, area under the curve (AUC) of receiver operating characteristic (ROC) curve and of the precision-recall (P-R) curve. The dataset sourced for this study includes 2362 CXRs, balanced for positive and negative COVID-19, from 1384 patients (63 +/- 18 years, 552 men). Our model achieved 89.7% (314/350) accuracy and 87.1% (156/179) sensitivity in classification of COVID-19 on a test dataset comprising 15% (350 of 2326) of the original data, with AUC of ROC 0.95 and AUC of the P-R curve 0.94. For each image we retrieve images with the most similar DNN-based image embeddings; these can be used to compare with previous cases.



### Adversarial and Natural Perturbations for General Robustness
- **Arxiv ID**: http://arxiv.org/abs/2010.01401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01401v1)
- **Published**: 2020-10-03 17:53:18+00:00
- **Updated**: 2020-10-03 17:53:18+00:00
- **Authors**: Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders
- **Comment**: Currently under review
- **Journal**: None
- **Summary**: In this paper we aim to explore the general robustness of neural network classifiers by utilizing adversarial as well as natural perturbations. Different from previous works which mainly focus on studying the robustness of neural networks against adversarial perturbations, we also evaluate their robustness on natural perturbations before and after robustification. After standardizing the comparison between adversarial and natural perturbations, we demonstrate that although adversarial training improves the performance of the networks against adversarial perturbations, it leads to drop in the performance for naturally perturbed samples besides clean samples. In contrast, natural perturbations like elastic deformations, occlusions and wave does not only improve the performance against natural perturbations, but also lead to improvement in the performance for the adversarial perturbations. Additionally they do not drop the accuracy on the clean images.



### Unsupervised Monocular Depth Estimation for Night-time Images using Adversarial Domain Feature Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.01402v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01402v1)
- **Published**: 2020-10-03 17:55:16+00:00
- **Updated**: 2020-10-03 17:55:16+00:00
- **Authors**: Madhu Vankadari, Sourav Garg, Anima Majumder, Swagat Kumar, Ardhendu Behera
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we look into the problem of estimating per-pixel depth maps from unconstrained RGB monocular night-time images which is a difficult task that has not been addressed adequately in the literature. The state-of-the-art day-time depth estimation methods fail miserably when tested with night-time images due to a large domain shift between them. The usual photo metric losses used for training these networks may not work for night-time images due to the absence of uniform lighting which is commonly present in day-time images, making it a difficult problem to solve. We propose to solve this problem by posing it as a domain adaptation problem where a network trained with day-time images is adapted to work for night-time images. Specifically, an encoder is trained to generate features from night-time images that are indistinguishable from those obtained from day-time images by using a PatchGAN-based adversarial discriminative learning method. Unlike the existing methods that directly adapt depth prediction (network output), we propose to adapt feature maps obtained from the encoder network so that a pre-trained day-time depth decoder can be directly used for predicting depth from these adapted features. Hence, the resulting method is termed as "Adversarial Domain Feature Adaptation (ADFA)" and its efficacy is demonstrated through experimentation on the challenging Oxford night driving dataset. Also, The modular encoder-decoder architecture for the proposed ADFA method allows us to use the encoder module as a feature extractor which can be used in many other applications. One such application is demonstrated where the features obtained from our adapted encoder network are shown to outperform other state-of-the-art methods in a visual place recognition problem, thereby, further establishing the usefulness and effectiveness of the proposed approach.



### Early Bird: Loop Closures from Opposing Viewpoints for Perceptually-Aliased Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2010.01421v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01421v3)
- **Published**: 2020-10-03 20:18:55+00:00
- **Updated**: 2020-12-20 13:41:24+00:00
- **Authors**: Satyajit Tourani, Dhagash Desai, Udit Singh Parihar, Sourav Garg, Ravi Kiran Sarvadevabhatla, Michael Milford, K. Madhava Krishna
- **Comment**: Accepted to VISAPP 2021. Video Link: https://youtu.be/q6cKYW0kX4s
- **Journal**: None
- **Summary**: Significant advances have been made recently in Visual Place Recognition (VPR), feature correspondence, and localization due to the proliferation of deep-learning-based methods. However, existing approaches tend to address, partially or fully, only one of two key challenges: viewpoint change and perceptual aliasing. In this paper, we present novel research that simultaneously addresses both challenges by combining deep-learned features with geometric transformations based on reasonable domain assumptions about navigation on a ground-plane, whilst also removing the requirement for specialized hardware setup (e.g. lighting, downwards facing cameras). In particular, our integration of VPR with SLAM by leveraging the robustness of deep-learned features and our homography-based extreme viewpoint invariance significantly boosts the performance of VPR, feature correspondence, and pose graph submodules of the SLAM pipeline. For the first time, we demonstrate a localization system capable of state-of-the-art performance despite perceptual aliasing and extreme 180-degree-rotated viewpoint change in a range of real-world and simulated experiments. Our system is able to achieve early loop closures that prevent significant drifts in SLAM trajectories. We also compare extensively several deep architectures for VPR and descriptor matching. We also show that superior place recognition and descriptor matching across opposite views results in a similar performance gain in back-end pose graph optimization.



### MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2010.01424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.01424v1)
- **Published**: 2020-10-03 20:56:16+00:00
- **Updated**: 2020-10-03 20:56:16+00:00
- **Authors**: Yi Wei, Zhe Gan, Wenbo Li, Siwei Lyu, Ming-Ching Chang, Lei Zhang, Jianfeng Gao, Pengchuan Zhang
- **Comment**: published at ACCV2020
- **Journal**: None
- **Summary**: We present Mask-guided Generative Adversarial Network (MagGAN) for high-resolution face attribute editing, in which semantic facial masks from a pre-trained face parser are used to guide the fine-grained image editing process. With the introduction of a mask-guided reconstruction loss, MagGAN learns to only edit the facial parts that are relevant to the desired attribute changes, while preserving the attribute-irrelevant regions (e.g., hat, scarf for modification `To Bald'). Further, a novel mask-guided conditioning strategy is introduced to incorporate the influence region of each attribute change into the generator. In addition, a multi-level patch-wise discriminator structure is proposed to scale our model for high-resolution ($1024 \times 1024$) face editing. Experiments on the CelebA benchmark show that the proposed method significantly outperforms prior state-of-the-art approaches in terms of both image quality and editing performance.



### autoTICI: Automatic Brain Tissue Reperfusion Scoring on 2D DSA Images of Acute Ischemic Stroke Patients
- **Arxiv ID**: http://arxiv.org/abs/2010.01432v3
- **DOI**: 10.1109/TMI.2021.3077113
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01432v3)
- **Published**: 2020-10-03 22:06:55+00:00
- **Updated**: 2021-05-07 10:51:07+00:00
- **Authors**: Ruisheng Su, Sandra A. P. Cornelissen, Matthijs van der Sluijs, Adriaan C. G. M. van Es, Wim H. van Zwam, Diederik W. J. Dippel, Geert Lycklama, Pieter Jan van Doormaal, Wiro J. Niessen, Aad van der Lugt, Theo van Walsum
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
  (IEEE TMI)
- **Journal**: None
- **Summary**: The Thrombolysis in Cerebral Infarction (TICI) score is an important metric for reperfusion therapy assessment in acute ischemic stroke. It is commonly used as a technical outcome measure after endovascular treatment (EVT). Existing TICI scores are defined in coarse ordinal grades based on visual inspection, leading to inter- and intra-observer variation. In this work, we present autoTICI, an automatic and quantitative TICI scoring method. First, each digital subtraction angiography (DSA) acquisition is separated into four phases (non-contrast, arterial, parenchymal and venous phase) using a multi-path convolutional neural network (CNN), which exploits spatio-temporal features. The network also incorporates sequence level label dependencies in the form of a state-transition matrix. Next, a minimum intensity map (MINIP) is computed using the motion corrected arterial and parenchymal frames. On the MINIP image, vessel, perfusion and background pixels are segmented. Finally, we quantify the autoTICI score as the ratio of reperfused pixels after EVT. On a routinely acquired multi-center dataset, the proposed autoTICI shows good correlation with the extended TICI (eTICI) reference with an average area under the curve (AUC) score of 0.81. The AUC score is 0.90 with respect to the dichotomized eTICI. In terms of clinical outcome prediction, we demonstrate that autoTICI is overall comparable to eTICI.



### Assessing Automated Machine Learning service to detect COVID-19 from X-Ray and CT images: A Real-time Smartphone Application case study
- **Arxiv ID**: http://arxiv.org/abs/2010.02715v1
- **DOI**: 10.20944/preprints202009.0647.v1
- **Categories**: **eess.IV**, cs.CV, 03, F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/2010.02715v1)
- **Published**: 2020-10-03 23:18:05+00:00
- **Updated**: 2020-10-03 23:18:05+00:00
- **Authors**: Razib Mustafiz, Khaled Mohsin
- **Comment**: 21 Pages, 6 Tables
- **Journal**: IJCSI-2020-17-6-12569
- **Summary**: The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a non interventional and sustainable AI solution. Lung disease remains a major healthcare challenge with high morbidity and mortality worldwide. The predominant lung disease was lung cancer. Until recently, the world has witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We have experienced how viral infection of lung and heart claimed thousands of lives worldwide. With the unprecedented advancement of Artificial Intelligence in recent years, Machine learning can be used to easily detect and classify medical imagery. It is much faster and most of the time more accurate than human radiologists. Once implemented, it is more cost-effective and time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive Service to detect and classify COVID19 induced pneumonia from other Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the implication and accuracy of the Automated ML-based Rapid Application Development (RAD) environment in the field of Medical Image diagnosis. This study will better equip us to respond with an ML-based diagnostic Decision Support System(DSS) for a Pandemic situation like COVID19. After optimization, the trained network achieved 96.8% Average Precision which was implemented as a Web Application for consumption. However, the same trained network did not perform the same like Web Application when ported to Smartphone for Real-time inference. Which was our main interest of study. The authors believe, there is scope for further study on this issue. One of the main goal of this study was to develop and evaluate the performance of AI-powered Smartphone-based Real-time Application. Facilitating primary diagnostic services in less equipped and understaffed rural healthcare centers of the world with unreliable internet service.



### Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors
- **Arxiv ID**: http://arxiv.org/abs/2010.01446v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01446v1)
- **Published**: 2020-10-03 23:55:36+00:00
- **Updated**: 2020-10-03 23:55:36+00:00
- **Authors**: Yu Sun, Jiaming Liu, Yiran Sun, Brendt Wohlberg, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new asynchronous RED (ASYNC-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of ASYNC-RED is further reduced by using a random subset of measurements at every iteration. We present complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate ASYNC-RED on image recovery using pre-trained deep denoisers as priors.



