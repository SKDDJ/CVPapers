# Arxiv Papers in cs.CV on 2020-10-23
### R-TOD: Real-Time Object Detector with Minimized End-to-End Delay for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.06372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06372v1)
- **Published**: 2020-10-23 01:03:46+00:00
- **Updated**: 2020-10-23 01:03:46+00:00
- **Authors**: Wonseok Jang, Hansaem Jeong, Kyungtae Kang, Nikil Dutt, Jong-Chan Kim
- **Comment**: 14 pages, 16 figures. Accepted to the 41st IEEE Real-Time Systems
  Symposium (RTSS), 2020
- **Journal**: None
- **Summary**: For realizing safe autonomous driving, the end-to-end delays of real-time object detection systems should be thoroughly analyzed and minimized. However, despite recent development of neural networks with minimized inference delays, surprisingly little attention has been paid to their end-to-end delays from an object's appearance until its detection is reported. With this motivation, this paper aims to provide more comprehensive understanding of the end-to-end delay, through which precise best- and worst-case delay predictions are formulated, and three optimization methods are implemented: (i) on-demand capture, (ii) zero-slack pipeline, and (iii) contention-free pipeline. Our experimental results show a 76% reduction in the end-to-end delay of Darknet YOLO (You Only Look Once) v3 (from 1070 ms to 261 ms), thereby demonstrating the great potential of exploiting the end-to-end delay analysis for autonomous driving. Furthermore, as we only modify the system architecture and do not change the neural network architecture itself, our approach incurs no penalty on the detection accuracy.



### Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with Adversarial Discriminative Domain Regularization
- **Arxiv ID**: http://arxiv.org/abs/2010.12126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12126v2)
- **Published**: 2020-10-23 01:48:37+00:00
- **Updated**: 2020-10-27 23:42:21+00:00
- **Authors**: Li Ren, Kai Li, LiQiang Wang, Kien Hua
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Matching information across image and text modalities is a fundamental challenge for many applications that involve both vision and natural language processing. The objective is to find efficient similarity metrics to compare the similarity between visual and textual information. Existing approaches mainly match the local visual objects and the sentence words in a shared space with attention mechanisms. The matching performance is still limited because the similarity computation is based on simple comparisons of the matching features, ignoring the characteristics of their distribution in the data. In this paper, we address this limitation with an efficient learning objective that considers the discriminative feature distributions between the visual objects and sentence words. Specifically, we propose a novel Adversarial Discriminative Domain Regularization (ADDR) learning framework, beyond the paradigm metric learning objective, to construct a set of discriminative data domains within each image-text pairs. Our approach can generally improve the learning efficiency and the performance of existing metrics learning frameworks by regulating the distribution of the hidden space between the matching pairs. The experimental results show that this new approach significantly improves the overall performance of several popular cross-modal matching techniques (SCAN, VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.



### Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2010.12136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12136v1)
- **Published**: 2020-10-23 02:43:02+00:00
- **Updated**: 2020-10-23 02:43:02+00:00
- **Authors**: Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We propose a novel lightweight generative adversarial network for efficient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with fine-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on specific visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simplified to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modification using natural language descriptions.



### Rethinking the competition between detection and ReID in Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.12138v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12138v3)
- **Published**: 2020-10-23 02:44:59+00:00
- **Updated**: 2022-05-24 11:48:32+00:00
- **Authors**: Chao Liang, Zhipeng Zhang, Xue Zhou, Bing Li, Shuyuan Zhu, Weiming Hu
- **Comment**: Accepted by TIP
- **Journal**: None
- **Summary**: Due to balanced accuracy and speed, one-shot models which jointly learn detection and identification embeddings, have drawn great attention in multi-object tracking (MOT). However, the inherent differences and relations between detection and re-identification (ReID) are unconsciously overlooked because of treating them as two isolated tasks in the one-shot tracking paradigm. This leads to inferior performance compared with existing two-stage methods. In this paper, we first dissect the reasoning process for these two tasks, which reveals that the competition between them inevitably would destroy task-dependent representations learning. To tackle this problem, we propose a novel reciprocal network (REN) with a self-relation and cross-relation design so that to impel each branch to better learn task-dependent representations. The proposed model aims to alleviate the deleterious tasks competition, meanwhile improve the cooperation between detection and ReID. Furthermore, we introduce a scale-aware attention network (SAAN) that prevents semantic level misalignment to improve the association capability of ID embeddings. By integrating the two delicately designed networks into a one-shot online MOT system, we construct a strong MOT tracker, namely CSTrack. Our tracker achieves the state-of-the-art performance on MOT16, MOT17 and MOT20 datasets, without other bells and whistles. Moreover, CSTrack is efficient and runs at 16.4 FPS on a single modern GPU, and its lightweight version even runs at 34.6 FPS. The complete code has been released at https://github.com/JudasDie/SOTS.



### AdaCrowd: Unlabeled Scene Adaptation for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2010.12141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12141v2)
- **Published**: 2020-10-23 03:20:42+00:00
- **Updated**: 2021-02-23 06:05:48+00:00
- **Authors**: Mahesh Kumar Krishna Reddy, Mrigank Rochan, Yiwei Lu, Yang Wang
- **Comment**: Accepted for publication in IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: We address the problem of image-based crowd counting. In particular, we propose a new problem called unlabeled scene-adaptive crowd counting. Given a new target scene, we would like to have a crowd counting model specifically adapted to this particular scene based on the target data that capture some information about the new scene. In this paper, we propose to use one or more unlabeled images from the target scene to perform the adaptation. In comparison with the existing problem setups (e.g. fully supervised), our proposed problem setup is closer to the real-world applications of crowd counting systems. We introduce a novel AdaCrowd framework to solve this problem. Our framework consists of a crowd counting network and a guiding network. The guiding network predicts some parameters in the crowd counting network based on the unlabeled images from a particular scene. This allows our model to adapt to different target scenes. The experimental results on several challenging benchmark datasets demonstrate the effectiveness of our proposed approach compared with other alternative methods. Code is available at https://github.com/maheshkkumar/adacrowd.



### Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.12176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12176v1)
- **Published**: 2020-10-23 05:40:53+00:00
- **Updated**: 2020-10-23 05:40:53+00:00
- **Authors**: Yuxi Li, Ning Xu, Jinlong Peng, John See, Weiyao Lin
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we address several inadequacies of current video object segmentation pipelines. Firstly, a cyclic mechanism is incorporated to the standard semi-supervised process to produce more robust representations. By relying on the accurate reference mask in the starting frame, we show that the error propagation problem can be mitigated. Next, we introduce a simple gradient correction module, which extends the offline pipeline to an online method while maintaining the efficiency of the former. Finally we develop cycle effective receptive field (cycle-ERF) based on gradient correction to provide a new perspective into analyzing object-specific regions of interests. We conduct comprehensive experiments on challenging benchmarks of DAVIS17 and Youtube-VOS, demonstrating that the cyclic mechanism is beneficial to segmentation quality.



### Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.12184v3
- **DOI**: 10.1109/TIP.2021.3113576
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12184v3)
- **Published**: 2020-10-23 06:29:09+00:00
- **Updated**: 2021-10-10 22:28:08+00:00
- **Authors**: Taotao Jing, Bingrong Xu, Jingjing Li, Zhengming Ding
- **Comment**: None
- **Journal**: in IEEE Transactions on Image Processing, vol. 30, pp. 8200-8211,
  2021
- **Summary**: Domain adaptation (DA) becomes an up-and-coming technique to address the insufficient or no annotation issue by exploiting external source knowledge. Existing DA algorithms mainly focus on practical knowledge transfer through domain alignment. Unfortunately, they ignore the fairness issue when the auxiliary source is extremely imbalanced across different categories, which results in severe under-presented knowledge adaptation of minority source set. To this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to handle the fairness challenge in imbalanced cross-domain learning. Specifically, a novel cross-domain mixup generation is exploited to augment the minority source set with target information to enhance fairness. Moreover, dual distinct classifiers and cross-domain prototype alignment are developed to seek a more robust classifier boundary and mitigate the domain shift. Such three strategies are formulated into a unified framework to address the fairness issue and domain shift challenge. Extensive experiments over two popular benchmarks have verified the effectiveness of our proposed model by comparing to existing state-of-the-art DA models, and especially our model significantly improves over 20% on two benchmarks in terms of the overall accuracy.



### Towards Robust Neural Networks via Orthogonal Diversity
- **Arxiv ID**: http://arxiv.org/abs/2010.12190v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12190v4)
- **Published**: 2020-10-23 06:40:56+00:00
- **Updated**: 2022-11-20 12:28:11+00:00
- **Authors**: Kun Fang, Qinghua Tao, Yingwen Wu, Tao Li, Jia Cai, Feipeng Cai, Xiaolin Huang, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks, which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Despite of the efficiency in defending specific attacks, adversarial training is benefited from the data augmentation, which does not contribute to the robustness of DNN itself and usually suffers from accuracy drop on clean data as well as inefficiency in unknown attacks. Towards the robustness of DNN itself, we propose a novel defense that aims at augmenting the model in order to learn features adaptive to diverse inputs, including adversarial examples. Specifically, we introduce multiple paths to augment the network, and impose orthogonality constraints on these paths. In addition, a margin-maximization loss is designed to further boost DIversity via Orthogonality (DIO). Extensive empirical results on various data sets, architectures, and attacks demonstrate the adversarial robustness of the proposed DIO.



### The Analysis of Facial Feature Deformation using Optical Flow Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2010.12199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12199v2)
- **Published**: 2020-10-23 07:14:02+00:00
- **Updated**: 2020-11-13 04:14:33+00:00
- **Authors**: Dayang Nur Zulhijah Awang Jesemi, Hamimah Ujir, Irwandi Hipiny, Sarah Flora Samson Juan
- **Comment**: 9 pages
- **Journal**: IJEECS, Vol. 15, No. 2, pp. 769-777 (2019)
- **Summary**: Facial features deformed according to the intended facial expression. Specific facial features are associated with specific facial expression, i.e. happy means the deformation of mouth. This paper presents the study of facial feature deformation for each facial expression by using an optical flow algorithm and segmented into three different regions of interest. The deformation of facial features shows the relation between facial the and facial expression. Based on the experiments, the deformations of eye and mouth are significant in all expressions except happy. For happy expression, cheeks and mouths are the significant regions. This work also suggests that different facial features' intensity varies in the way that they contribute to the recognition of the different facial expression intensity. The maximum magnitude across all expressions is shown by the mouth for surprise expression which is 9x10-4. While the minimum magnitude is shown by the mouth for angry expression which is 0.4x10-4.



### Feature matching in Ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/2010.12216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.12216v1)
- **Published**: 2020-10-23 07:43:27+00:00
- **Updated**: 2020-10-23 07:43:27+00:00
- **Authors**: Hang Zhu, Zihao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Feature matching is an important technique to identify a single object in different images. It helps machines to construct recognition of a specific object from multiple perspectives. For years, feature matching has been commonly used in various computer vision applications, like traffic surveillance, self-driving, and other systems. With the arise of Computer-Aided Diagnosis(CAD), the need for feature matching techniques also emerges in the medical imaging field. In this paper, we present a deep learning-based method specially for ultrasound images. It will be examined against existing methods that have outstanding results on regular images. As the ultrasound images are different from regular images in many fields like texture, noise type, and dimension, traditional methods will be evaluated and optimized to be applied to ultrasound images.



### A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision
- **Arxiv ID**: http://arxiv.org/abs/2010.12219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12219v1)
- **Published**: 2020-10-23 07:58:20+00:00
- **Updated**: 2020-10-23 07:58:20+00:00
- **Authors**: Liyan Sun, Jianxiong Wu, Xinghao Ding, Yue Huang, Guisheng Wang, Yizhou Yu
- **Comment**: 10 pages, 11 figure
- **Journal**: None
- **Summary**: Standard segmentation of medical images based on full-supervised convolutional networks demands accurate dense annotations. Such learning framework is built on laborious manual annotation with restrict demands for expertise, leading to insufficient high-quality labels. To overcome such limitation and exploit massive weakly labeled data, we relaxed the rigid labeling requirement and developed a semi-supervised learning framework based on a teacher-student fashion for organ and lesion segmentation with partial dense-labeled supervision and supplementary loose bounding-box supervision which are easier to acquire. Observing the geometrical relation of an organ and its inner lesions in most cases, we propose a hierarchical organ-to-lesion (O2L) attention module in a teacher segmentor to produce pseudo-labels. Then a student segmentor is trained with combinations of manual-labeled and pseudo-labeled annotations. We further proposed a localization branch realized via an aggregation of high-level features in a deep decoder to predict locations of organ and lesion, which enriches student segmentor with precise localization information. We validated each design in our model on LiTS challenge datasets by ablation study and showed its state-of-the-art performance compared with recent methods. We show our model is robust to the quality of bounding box and achieves comparable performance compared with full-supervised learning methods.



### Temporal Attention-Augmented Graph Convolutional Network for Efficient Skeleton-Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.12221v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12221v3)
- **Published**: 2020-10-23 08:01:55+00:00
- **Updated**: 2021-04-22 18:15:32+00:00
- **Authors**: Negar Heidari, Alexandros Iosifidis
- **Comment**: Accepted by the 2020 International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) have been very successful in modeling non-Euclidean data structures, like sequences of body skeletons forming actions modeled as spatio-temporal graphs. Most GCN-based action recognition methods use deep feed-forward networks with high computational complexity to process all skeletons in an action. This leads to a high number of floating point operations (ranging from 16G to 100G FLOPs) to process a single sample, making their adoption in restricted computation application scenarios infeasible. In this paper, we propose a temporal attention module (TAM) for increasing the efficiency in skeleton-based action recognition by selecting the most informative skeletons of an action at the early layers of the network. We incorporate the TAM in a light-weight GCN topology to further reduce the overall number of computations. Experimental results on two benchmark datasets show that the proposed method outperforms with a large margin the baseline GCN-based method while having 2.9 times less number of computations. Moreover, it performs on par with the state-of-the-art with up to 9.6 times less number of computations.



### Coping with Label Shift via Distributionally Robust Optimisation
- **Arxiv ID**: http://arxiv.org/abs/2010.12230v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2010.12230v3)
- **Published**: 2020-10-23 08:33:04+00:00
- **Updated**: 2021-08-17 05:36:25+00:00
- **Authors**: Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, Suvrit Sra
- **Comment**: None
- **Journal**: None
- **Summary**: The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an \emph{unlabelled} test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in \emph{multiple} test environments. Can one instead learn a \emph{single} classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. %, and establish its convergence. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present.



### Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.12238v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12238v2)
- **Published**: 2020-10-23 08:52:00+00:00
- **Updated**: 2020-10-27 02:12:58+00:00
- **Authors**: Huan Fu, Shunming Li, Rongfei Jia, Mingming Gong, Binqiang Zhao, Dacheng Tao
- **Comment**: Accepted to NeurlPS 2020
- **Journal**: None
- **Summary**: Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape of a given 2D image from a large 3D shape database. The common routine is to map 2D images and 3D shapes into an embedding space and define (or learn) a shape similarity measure. While metric learning with some adaptation techniques seems to be a natural solution to shape similarity learning, the performance is often unsatisfactory for fine-grained shape retrieval. In the paper, we identify the source of the poor performance and propose a practical solution to this problem. We find that the shape difference between a negative pair is entangled with the texture gap, making metric learning ineffective in pushing away negative pairs. To tackle this issue, we develop a geometry-focused multi-view metric learning framework empowered by texture synthesis. The synthesis of textures for 3D shape models creates hard triplets, which suppress the adverse effects of rich texture in 2D images, thereby push the network to focus more on discovering geometric characteristics. Our approach shows state-of-the-art performance on a recently released large-scale 3D-FUTURE[1] repository, as well as three widely studied benchmarks, including Pix3D[2], Stanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at: https://github.com/3D-FRONT-FUTURE/IBSR-texture



### Domain Adaptation in LiDAR Semantic Segmentation by Aligning Class Distributions
- **Arxiv ID**: http://arxiv.org/abs/2010.12239v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12239v3)
- **Published**: 2020-10-23 08:52:15+00:00
- **Updated**: 2021-12-03 15:45:33+00:00
- **Authors**: Inigo Alonso, Luis Riazuelo, Luis Montesano, Ana C. Murillo
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: LiDAR semantic segmentation provides 3D semantic information about the environment, an essential cue for intelligent systems during their decision making processes. Deep neural networks are achieving state-of-the-art results on large public benchmarks on this task. Unfortunately, finding models that generalize well or adapt to additional domains, where data distribution is different, remains a major challenge. This work addresses the problem of unsupervised domain adaptation for LiDAR semantic segmentation models. Our approach combines novel ideas on top of the current state-of-the-art approaches and yields new state-of-the-art results. We propose simple but effective strategies to reduce the domain shift by aligning the data distribution on the input space. Besides, we propose a learning-based approach that aligns the distribution of the semantic classes of the target domain to the source domain. The presented ablation study shows how each part contributes to the final performance. Our strategy is shown to outperform previous approaches for domain adaptation with comparisons run on three different domains.



### Multi Scale Identity-Preserving Image-to-Image Translation Network for Low-Resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.12249v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12249v4)
- **Published**: 2020-10-23 09:21:06+00:00
- **Updated**: 2022-07-03 19:25:15+00:00
- **Authors**: Vahid Reza Khazaie, Nicky Bayat, Yalda Mohsenzadeh
- **Comment**: Accepted in the 35th Canadian Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: State-of-the-art deep neural network models have reached near perfect face recognition accuracy rates on controlled high-resolution face images. However, their performance is drastically degraded when they are tested with very low-resolution face images. This is particularly critical in surveillance systems, where a low-resolution probe image is to be matched with high-resolution gallery images. super-resolution techniques aim at producing high-resolution face images from low-resolution counterparts. While they are capable of reconstructing images that are visually appealing, the identity-related information is not preserved. Here, we propose an identity-preserving end-to-end image-to-image translation deep neural network which is capable of super-resolving very low-resolution faces to their high-resolution counterparts while preserving identity-related information. We achieved this by training a very deep convolutional encoder-decoder network with a symmetric contracting path between corresponding layers. This network was trained with a combination of a reconstruction and an identity-preserving loss, on multi-scale low-resolution conditions. Extensive quantitative evaluations of our proposed model demonstrated that it outperforms competing super-resolution and low-resolution face recognition methods on natural and artificial low-resolution face data sets and even unseen identities.



### Population Gradients improve performance across data-sets and architectures in object classification
- **Arxiv ID**: http://arxiv.org/abs/2010.12260v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.12260v1)
- **Published**: 2020-10-23 09:40:23+00:00
- **Updated**: 2020-10-23 09:40:23+00:00
- **Authors**: Yurika Sakai, Andrey Kormilitzin, Qiang Liu, Alejo Nevado-Holgado
- **Comment**: None
- **Journal**: None
- **Summary**: The most successful methods such as ReLU transfer functions, batch normalization, Xavier initialization, dropout, learning rate decay, or dynamic optimizers, have become standards in the field due, particularly, to their ability to increase the performance of Neural Networks (NNs) significantly and in almost all situations. Here we present a new method to calculate the gradients while training NNs, and show that it significantly improves final performance across architectures, data-sets, hyper-parameter values, training length, and model sizes, including when it is being combined with other common performance-improving methods (such as the ones mentioned above). Besides being effective in the wide array situations that we have tested, the increase in performance (e.g. F1) it provides is as high or higher than this one of all the other widespread performance-improving methods that we have compared against. We call our method Population Gradients (PG), and it consists on using a population of NNs to calculate a non-local estimation of the gradient, which is closer to the theoretical exact gradient (i.e. this one obtainable only with an infinitely big data-set) of the error function than the empirical gradient (i.e. this one obtained with the real finite data-set).



### Show and Speak: Directly Synthesize Spoken Description of Images
- **Arxiv ID**: http://arxiv.org/abs/2010.12267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.12267v2)
- **Published**: 2020-10-23 09:53:01+00:00
- **Updated**: 2020-11-17 10:58:19+00:00
- **Authors**: Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette Scharenborg
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new model, referred to as the show and speak (SAS) model that, for the first time, is able to directly synthesize spoken descriptions of images, bypassing the need for any text or phonemes. The basic structure of SAS is an encoder-decoder architecture that takes an image as input and predicts the spectrogram of speech that describes this image. The final speech audio is obtained from the predicted spectrogram via WaveNet. Extensive experiments on the public benchmark database Flickr8k demonstrate that the proposed SAS is able to synthesize natural spoken descriptions for images, indicating that synthesizing spoken descriptions for images while bypassing text and phonemes is feasible.



### Self-Learning Transformations for Improving Gaze and Head Redirection
- **Arxiv ID**: http://arxiv.org/abs/2010.12307v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.6
- **Links**: [PDF](http://arxiv.org/pdf/2010.12307v1)
- **Published**: 2020-10-23 11:18:37+00:00
- **Updated**: 2020-10-23 11:18:37+00:00
- **Authors**: Yufeng Zheng, Seonwook Park, Xucong Zhang, Shalini De Mello, Otmar Hilliges
- **Comment**: Accepted at NeurIPS 2020. Check our supplementary video at:
  https://ait.ethz.ch/projects/2020/STED-gaze/
- **Journal**: None
- **Summary**: Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/



### Matching the Clinical Reality: Accurate OCT-Based Diagnosis From Few Labels
- **Arxiv ID**: http://arxiv.org/abs/2010.12316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12316v1)
- **Published**: 2020-10-23 11:47:28+00:00
- **Updated**: 2020-10-23 11:47:28+00:00
- **Authors**: Valentyn Melnychuk, Evgeniy Faerman, Ilja Manakov, Thomas Seidl
- **Comment**: KDAH-CIKM-2020
- **Journal**: None
- **Summary**: Unlabeled data is often abundant in the clinic, making machine learning methods based on semi-supervised learning a good match for this setting. Despite this, they are currently receiving relatively little attention in medical image analysis literature. Instead, most practitioners and researchers focus on supervised or transfer learning approaches. The recently proposed MixMatch and FixMatch algorithms have demonstrated promising results in extracting useful representations while requiring very few labels. Motivated by these recent successes, we apply MixMatch and FixMatch in an ophthalmological diagnostic setting and investigate how they fare against standard transfer learning. We find that both algorithms outperform the transfer learning baseline on all fractions of labelled data. Furthermore, our experiments show that exponential moving average (EMA) of model parameters, which is a component of both algorithms, is not needed for our classification problem, as disabling it leaves the outcome unchanged. Our code is available online: https://github.com/Valentyn1997/oct-diagn-semi-supervised



### Error Bounds of Projection Models in Weakly Supervised 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.12317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12317v1)
- **Published**: 2020-10-23 11:48:13+00:00
- **Updated**: 2020-10-23 11:48:13+00:00
- **Authors**: Nikolas Klug, Moritz Einfalt, Stephan Brehm, Rainer Lienhart
- **Comment**: Accepted at 3DV 2020
- **Journal**: None
- **Summary**: The current state-of-the-art in monocular 3D human pose estimation is heavily influenced by weakly supervised methods. These allow 2D labels to be used to learn effective 3D human pose recovery either directly from images or via 2D-to-3D pose uplifting. In this paper we present a detailed analysis of the most commonly used simplified projection models, which relate the estimated 3D pose representation to 2D labels: normalized perspective and weak perspective projections. Specifically, we derive theoretical lower bound errors for those projection models under the commonly used mean per-joint position error (MPJPE). Additionally, we show how the normalized perspective projection can be replaced to avoid this guaranteed minimal error. We evaluate the derived lower bounds on the most commonly used 3D human pose estimation benchmark datasets. Our results show that both projection models lead to an inherent minimal error between 19.3mm and 54.7mm, even after alignment in position and scale. This is a considerable share when comparing with recent state-of-the-art results. Our paper thus establishes a theoretical baseline that shows the importance of suitable projection models in weakly supervised 3D human pose estimation.



### Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2010.12320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12320v2)
- **Published**: 2020-10-23 11:52:06+00:00
- **Updated**: 2020-10-26 01:22:55+00:00
- **Authors**: Feng Liu, Xiaoming Liu
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: The goal of this paper is to learn dense 3D shape correspondence for topology-varying objects in an unsupervised manner. Conventional implicit functions estimate the occupancy of a 3D point given a shape latent code. Instead, our novel implicit function produces a part embedding vector for each 3D point, which is assumed to be similar to its densely corresponded point in another 3D shape of the same object category. Furthermore, we implement dense correspondence through an inverse function mapping from the part embedding to a corresponded 3D point. Both functions are jointly learned with several effective loss functions to realize our assumption, together with the encoder generating the shape latent code. During inference, if a user selects an arbitrary point on the source shape, our algorithm can automatically generate a confidence score indicating whether there is a correspondence on the target shape, as well as the corresponding semantic point if there is one. Such a mechanism inherently benefits man-made objects with different part constitutions. The effectiveness of our approach is demonstrated through unsupervised 3D semantic correspondence and shape segmentation.



### Tele-operative Robotic Lung Ultrasound Scanning Platform for Triage of COVID-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2010.12335v3
- **DOI**: 10.1109/LRA.2021.3068702
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12335v3)
- **Published**: 2020-10-23 12:17:42+00:00
- **Updated**: 2020-11-12 02:36:53+00:00
- **Authors**: Ryosuke Tsumura, John W. Hardin, Keshav Bimbraw, Olushola S. Odusanya, Yihao Zheng, Jeffrey C. Hill, Beatrice Hoffmann, Winston Soboyejo, Haichong K. Zhang
- **Comment**: The demonstration video of our robotic platform can be watched below
  the link <https://youtu.be/BbNCvESTYik>
- **Journal**: IEEE Robotics and Automation Letters (2021)
- **Summary**: Novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has become a pandemic of epic proportions and a global response to prepare health systems worldwide is of utmost importance. In addition to its cost-effectiveness in a resources-limited setting, lung ultrasound (LUS) has emerged as a rapid noninvasive imaging tool for the diagnosis of COVID-19 infected patients. Concerns surrounding LUS include the disparity of infected patients and healthcare providers, relatively small number of physicians and sonographers capable of performing LUS, and most importantly, the requirement for substantial physical contact between the patient and operator, increasing the risk of transmission. Mitigation of the spread of the virus is of paramount importance. A 2-dimensional (2D) tele-operative robotic platform capable of performing LUS in for COVID-19 infected patients may be of significant benefit. The authors address the aforementioned issues surrounding the use of LUS in the application of COVID- 19 infected patients. In addition, first time application, feasibility and safety were validated in three healthy subjects, along with 2D image optimization and comparison for overall accuracy. Preliminary results demonstrate that the proposed platform allows for successful acquisition and application of LUS in humans.



### Fusion of Dual Spatial Information for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.12337v1
- **DOI**: 10.1109/TGRS.2020.3031928
- **Categories**: **cs.CV**, cs.IT, eess.IV, math.IT, 68T45, J.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.12337v1)
- **Published**: 2020-10-23 12:20:18+00:00
- **Updated**: 2020-10-23 12:20:18+00:00
- **Authors**: Puhong Duan, Pedram Ghamisi, Xudong Kang, Behnood Rasti, Shutao Li, Richard Gloaguen
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: The inclusion of spatial information into spectral classifiers for fine-resolution hyperspectral imagery has led to significant improvements in terms of classification performance. The task of spectral-spatial hyperspectral image classification has remained challenging because of high intraclass spectrum variability and low interclass spectral variability. This fact has made the extraction of spatial information highly active. In this work, a novel hyperspectral image classification framework using the fusion of dual spatial information is proposed, in which the dual spatial information is built by both exploiting pre-processing feature extraction and post-processing spatial optimization. In the feature extraction stage, an adaptive texture smoothing method is proposed to construct the structural profile (SP), which makes it possible to precisely extract discriminative features from hyperspectral images. The SP extraction method is used here for the first time in the remote sensing community. Then, the extracted SP is fed into a spectral classifier. In the spatial optimization stage, a pixel-level classifier is used to obtain the class probability followed by an extended random walker-based spatial optimization technique. Finally, a decision fusion rule is utilized to fuse the class probabilities obtained by the two different stages. Experiments performed on three data sets from different scenes illustrate that the proposed method can outperform other state-of-the-art classification techniques. In addition, the proposed feature extraction method, i.e., SP, can effectively improve the discrimination between different land covers.



### Spherical Harmonics for Shape-Constrained 3D Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.12369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12369v1)
- **Published**: 2020-10-23 12:58:26+00:00
- **Updated**: 2020-10-23 12:58:26+00:00
- **Authors**: Dennis Eschweiler, Malte Rethwisch, Simon Koppers, Johannes Stegmaier
- **Comment**: None
- **Journal**: None
- **Summary**: Recent microscopy imaging techniques allow to precisely analyze cell morphology in 3D image data. To process the vast amount of image data generated by current digitized imaging techniques, automated approaches are demanded more than ever. Segmentation approaches used for morphological analyses, however, are often prone to produce unnaturally shaped predictions, which in conclusion could lead to inaccurate experimental outcomes. In order to minimize further manual interaction, shape priors help to constrain the predictions to the set of natural variations. In this paper, we show how spherical harmonics can be used as an alternative way to inherently constrain the predictions of neural networks for the segmentation of cells in 3D microscopy image data. Benefits and limitations of the spherical harmonic representation are analyzed and final results are compared to other state-of-the-art approaches on two different data sets.



### Efficient grouping for keypoint detection
- **Arxiv ID**: http://arxiv.org/abs/2010.12390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12390v1)
- **Published**: 2020-10-23 13:25:45+00:00
- **Updated**: 2020-10-23 13:25:45+00:00
- **Authors**: Alexey Sidnev, Ekaterina Krasikova, Maxim Kazakov
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: The success of deep neural networks in the traditional keypoint detection task encourages researchers to solve new problems and collect more complex datasets. The size of the DeepFashion2 dataset poses a new challenge on the keypoint detection task, as it comprises 13 clothing categories that span a wide range of keypoints (294 in total). The direct prediction of all keypoints leads to huge memory consumption, slow training, and a slow inference time. This paper studies the keypoint grouping approach and how it affects the performance of the CenterNet architecture. We propose a simple and efficient automatic grouping technique with a powerful post-processing method and apply it to the DeepFashion2 fashion landmark task and the MS COCO pose estimation task. This reduces memory consumption and processing time during inference by up to 19% and 30% respectively, and during the training stage by 28% and 26% respectively, without compromising accuracy.



### Segmentation of the cortical plate in fetal brain MRI with a topological loss
- **Arxiv ID**: http://arxiv.org/abs/2010.12391v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12391v2)
- **Published**: 2020-10-23 13:25:45+00:00
- **Updated**: 2020-11-24 12:38:57+00:00
- **Authors**: Priscille de Dumast, Hamza Kebiri, Chirine Atat, Vincent Dunet, Mériam Koob, Meritxell Bach Cuadra
- **Comment**: 4 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: The fetal cortical plate undergoes drastic morphological changes throughout early in utero development that can be observed using magnetic resonance (MR) imaging. An accurate MR image segmentation, and more importantly a topologically correct delineation of the cortical gray matter, is a key baseline to perform further quantitative analysis of brain development. In this paper, we propose for the first time the integration of a topological constraint, as an additional loss function, to enhance the morphological consistency of a deep learning-based segmentation of the fetal cortical plate. We quantitatively evaluate our method on 18 fetal brain atlases ranging from 21 to 38 weeks of gestation, showing the significant benefits of our method through all gestational ages as compared to a baseline method. Furthermore, qualitative evaluation by three different experts on 130 randomly selected slices from 26 clinical MRIs evidences the out-performance of our method independently of the MR reconstruction quality.



### RSKDD-Net: Random Sample-based Keypoint Detector and Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2010.12394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12394v1)
- **Published**: 2020-10-23 13:29:29+00:00
- **Updated**: 2020-10-23 13:29:29+00:00
- **Authors**: Fan Lu, Guang Chen, Yinlong Liu, Zhongnan Qu, Alois Knoll
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Keypoint detector and descriptor are two main components of point cloud registration. Previous learning-based keypoint detectors rely on saliency estimation for each point or farthest point sample (FPS) for candidate points selection, which are inefficient and not applicable in large scale scenes. This paper proposes Random Sample-based Keypoint Detector and Descriptor Network (RSKDD-Net) for large scale point cloud registration. The key idea is using random sampling to efficiently select candidate points and using a learning-based method to jointly generate keypoints and descriptors. To tackle the information loss of random sampling, we exploit a novel random dilation cluster strategy to enlarge the receptive field of each sampled point and an attention mechanism to aggregate the positions and features of neighbor points. Furthermore, we propose a matching loss to train the descriptor in a weakly supervised manner. Extensive experiments on two large scale outdoor LiDAR datasets show that the proposed RSKDD-Net achieves state-of-the-art performance with more than 15 times faster than existing methods. Our code is available at https://github.com/ispc-lab/RSKDD-Net.



### SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.12416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12416v1)
- **Published**: 2020-10-23 14:07:00+00:00
- **Updated**: 2020-10-23 14:07:00+00:00
- **Authors**: Shuai Shao, Rui Xu, Yan-Jiang Wang, Weifeng Liu, Bao-Di Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the attention mechanism contributes significantly to hypergraph based neural networks. However, these methods update the attention weights with the network propagating. That is to say, this type of attention mechanism is only suitable for deep learning-based methods while not applicable to the traditional machine learning approaches. In this paper, we propose a hypergraph based sparse attention mechanism to tackle this issue and embed it into dictionary learning. More specifically, we first construct a sparse attention hypergraph, asset attention weights to samples by employing the $\ell_1$-norm sparse regularization to mine the high-order relationship among sample features. Then, we introduce the hypergraph Laplacian operator to preserve the local structure for subspace transformation in dictionary learning. Besides, we incorporate the discriminative information into the hypergraph as the guidance to aggregate samples. Unlike previous works, our method updates attention weights independently, does not rely on the deep network. We demonstrate the efficacy of our approach on four benchmark datasets.



### DLDL: Dynamic Label Dictionary Learning via Hypergraph Regularization
- **Arxiv ID**: http://arxiv.org/abs/2010.12417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12417v1)
- **Published**: 2020-10-23 14:07:07+00:00
- **Updated**: 2020-10-23 14:07:07+00:00
- **Authors**: Shuai Shao, Mengke Wang, Rui Xu, Yan-Jiang Wang, Bao-Di Liu
- **Comment**: None
- **Journal**: None
- **Summary**: For classification tasks, dictionary learning based methods have attracted lots of attention in recent years. One popular way to achieve this purpose is to introduce label information to generate a discriminative dictionary to represent samples. However, compared with traditional dictionary learning, this category of methods only achieves significant improvements in supervised learning, and has little positive influence on semi-supervised or unsupervised learning. To tackle this issue, we propose a Dynamic Label Dictionary Learning (DLDL) algorithm to generate the soft label matrix for unlabeled data. Specifically, we employ hypergraph manifold regularization to keep the relations among original data, transformed data, and soft labels consistent. We demonstrate the efficiency of the proposed DLDL approach on two remote sensing datasets.



### Progressive Training of Multi-level Wavelet Residual Networks for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2010.12422v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12422v1)
- **Published**: 2020-10-23 14:14:00+00:00
- **Updated**: 2020-10-23 14:14:00+00:00
- **Authors**: Yali Peng, Yue Cao, Shigang Liu, Jian Yang, Wangmeng Zuo
- **Comment**: 13 pages, 7 figures code: https://github.com/happycaoyue/PT-MWRN
- **Journal**: None
- **Summary**: Recent years have witnessed the great success of deep convolutional neural networks (CNNs) in image denoising. Albeit deeper network and larger model capacity generally benefit performance, it remains a challenging practical issue to train a very deep image denoising network. Using multilevel wavelet-CNN (MWCNN) as an example, we empirically find that the denoising performance cannot be significantly improved by either increasing wavelet decomposition levels or increasing convolution layers within each level. To cope with this issue, this paper presents a multi-level wavelet residual network (MWRN) architecture as well as a progressive training (PTMWRN) scheme to improve image denoising performance. In contrast to MWCNN, our MWRN introduces several residual blocks after each level of discrete wavelet transform (DWT) and before inverse discrete wavelet transform (IDWT). For easing the training difficulty, scale-specific loss is applied to each level of MWRN by requiring the intermediate output to approximate the corresponding wavelet subbands of ground-truth clean image. To ensure the effectiveness of scale-specific loss, we also take the wavelet subbands of noisy image as the input to each scale of the encoder. Furthermore, progressive training scheme is adopted for better learning of MWRN by beigining with training the lowest level of MWRN and progressively training the upper levels to bring more fine details to denoising results. Experiments on both synthetic and real-world noisy images show that our PT-MWRN performs favorably against the state-of-the-art denoising methods in terms both quantitative metrics and visual quality.



### Casting a BAIT for Offline and Online Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.12427v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12427v5)
- **Published**: 2020-10-23 14:18:42+00:00
- **Updated**: 2023-06-10 05:24:15+00:00
- **Authors**: Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui
- **Comment**: Accepted by Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: We address the source-free domain adaptation (SFDA) problem, where only the source model is available during adaptation to the target domain. We consider two settings: the offline setting where all target data can be visited multiple times (epochs) to arrive at a prediction for each target sample, and the online setting where the target data needs to be directly classified upon arrival. Inspired by diverse classifier based domain adaptation methods, in this paper we introduce a second classifier, but with another classifier head fixed. When adapting to the target domain, the additional classifier initialized from source classifier is expected to find misclassified features. Next, when updating the feature extractor, those features will be pushed towards the right side of the source decision boundary, thus achieving source-free domain adaptation. Experimental results show that the proposed method achieves competitive results for offline SFDA on several benchmark datasets compared with existing DA and SFDA methods, and our method surpasses by a large margin other SFDA methods under online source-free domain adaptation setting.



### BP-MVSNet: Belief-Propagation-Layers for Multi-View-Stereo
- **Arxiv ID**: http://arxiv.org/abs/2010.12436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12436v1)
- **Published**: 2020-10-23 14:30:07+00:00
- **Updated**: 2020-10-23 14:30:07+00:00
- **Authors**: Christian Sormann, Patrick Knöbelreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, Friedrich Fraundorfer
- **Comment**: accepted at 3DV 2020
- **Journal**: None
- **Summary**: In this work, we propose BP-MVSNet, a convolutional neural network (CNN)-based Multi-View-Stereo (MVS) method that uses a differentiable Conditional Random Field (CRF) layer for regularization. To this end, we propose to extend the BP layer and add what is necessary to successfully use it in the MVS setting. We therefore show how we can calculate a normalization based on the expected 3D error, which we can then use to normalize the label jumps in the CRF. This is required to make the BP layer invariant to different scales in the MVS setting. In order to also enable fractional label jumps, we propose a differentiable interpolation step, which we embed into the computation of the pairwise term. These extensions allow us to integrate the BP layer into a multi-scale MVS network, where we continuously improve a rough initial estimate until we get high quality depth maps as a result. We evaluate the proposed BP-MVSNet in an ablation study and conduct extensive experiments on the DTU, Tanks and Temples and ETH3D data sets. The experiments show that we can significantly outperform the baseline and achieve state-of-the-art results.



### Estimation of Cardiac Valve Annuli Motion with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.12446v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12446v1)
- **Published**: 2020-10-23 14:39:47+00:00
- **Updated**: 2020-10-23 14:39:47+00:00
- **Authors**: Eric Kerfoot, Carlos Escudero King, Tefvik Ismail, David Nordsletten, Renee Miller
- **Comment**: 10 pages, STACOM abstract
- **Journal**: None
- **Summary**: Valve annuli motion and morphology, measured from non-invasive imaging, can be used to gain a better understanding of healthy and pathological heart function. Measurements such as long-axis strain as well as peak strain rates provide markers of systolic function. Likewise, early and late-diastolic filling velocities are used as indicators of diastolic function. Quantifying global strains, however, requires a fast and precise method of tracking long-axis motion throughout the cardiac cycle. Valve landmarks such as the insertion of leaflets into the myocardial wall provide features that can be tracked to measure global long-axis motion. Feature tracking methods require initialisation, which can be time-consuming in studies with large cohorts. Therefore, this study developed and trained a neural network to identify ten features from unlabeled long-axis MR images: six mitral valve points from three long-axis views, two aortic valve points and two tricuspid valve points. This study used manual annotations of valve landmarks in standard 2-, 3- and 4-chamber long-axis images collected in clinical scans to train the network. The accuracy in the identification of these ten features, in pixel distance, was compared with the accuracy of two commonly used feature tracking methods as well as the inter-observer variability of manual annotations. Clinical measures, such as valve landmark strain and motion between end-diastole and end-systole, are also presented to illustrate the utility and robustness of the method.



### LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration
- **Arxiv ID**: http://arxiv.org/abs/2010.12447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12447v2)
- **Published**: 2020-10-23 14:39:50+00:00
- **Updated**: 2021-11-26 05:21:11+00:00
- **Authors**: Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll
- **Comment**: NeurIPS'20 (Oral)
- **Journal**: NeurIPS 2020
- **Summary**: We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.



### Primal-Dual Mesh Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.12455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12455v1)
- **Published**: 2020-10-23 14:49:02+00:00
- **Updated**: 2020-10-23 14:49:02+00:00
- **Authors**: Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, Luca Carlone
- **Comment**: Accepted to the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020), Vancouver, Canada. Code available at:
  https://github.com/MIT-SPARK/PD-MeshNet
- **Journal**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020)
- **Summary**: Recent works in geometric deep learning have introduced neural networks that allow performing inference tasks on three-dimensional geometric data by defining convolution, and sometimes pooling, operations on triangle meshes. These methods, however, either consider the input mesh as a graph, and do not exploit specific geometric properties of meshes for feature aggregation and downsampling, or are specialized for meshes, but rely on a rigid definition of convolution that does not properly capture the local topology of the mesh. We propose a method that combines the advantages of both types of approaches, while addressing their limitations: we extend a primal-dual framework drawn from the graph-neural-network literature to triangle meshes, and define convolutions on two types of graphs constructed from an input mesh. Our method takes features for both edges and faces of a 3D mesh as input and dynamically aggregates them using an attention mechanism. At the same time, we introduce a pooling operation with a precise geometric interpretation, that allows handling variations in the mesh connectivity by clustering mesh faces in a task-driven fashion. We provide theoretical insights of our approach using tools from the mesh-simplification literature. In addition, we validate experimentally our method in the tasks of shape classification and shape segmentation, where we obtain comparable or superior performance to the state of the art.



### CLOUD: Contrastive Learning of Unsupervised Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2010.12488v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12488v1)
- **Published**: 2020-10-23 15:42:57+00:00
- **Updated**: 2020-10-23 15:42:57+00:00
- **Authors**: Jianren Wang, Yujie Lu, Hang Zhao
- **Comment**: None
- **Journal**: CORL 2020
- **Summary**: Developing agents that can perform complex control tasks from high dimensional observations such as pixels is challenging due to difficulties in learning dynamics efficiently. In this work, we propose to learn forward and inverse dynamics in a fully unsupervised manner via contrastive estimation. Specifically, we train a forward dynamics model and an inverse dynamics model in the feature space of states and actions with data collected from random exploration. Unlike most existing deterministic models, our energy-based model takes into account the stochastic nature of agent-environment interactions. We demonstrate the efficacy of our approach across a variety of tasks including goal-directed planning and imitation from observations. Project videos and code are at https://jianrenw.github.io/cloud/.



### ResNet or DenseNet? Introducing Dense Shortcuts to ResNet
- **Arxiv ID**: http://arxiv.org/abs/2010.12496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12496v1)
- **Published**: 2020-10-23 16:00:15+00:00
- **Updated**: 2020-10-23 16:00:15+00:00
- **Authors**: Chaoning Zhang, Philipp Benz, Dawit Mureja Argaw, Seokju Lee, Junsik Kim, Francois Rameau, Jean-Charles Bazin, In So Kweon
- **Comment**: Accepted to WACV2021 first round
- **Journal**: None
- **Summary**: ResNet or DenseNet? Nowadays, most deep learning based approaches are implemented with seminal backbone networks, among them the two arguably most famous ones are ResNet and DenseNet. Despite their competitive performance and overwhelming popularity, inherent drawbacks exist for both of them. For ResNet, the identity shortcut that stabilizes training also limits its representation capacity, while DenseNet has a higher capacity with multi-layer feature concatenation. However, the dense concatenation causes a new problem of requiring high GPU memory and more training time. Partially due to this, it is not a trivial choice between ResNet and DenseNet. This paper provides a unified perspective of dense summation to analyze them, which facilitates a better understanding of their core difference. We further propose dense weighted normalized shortcuts as a solution to the dilemma between them. Our proposed dense shortcut inherits the design philosophy of simple design in ResNet and DenseNet. On several benchmark datasets, the experimental results show that the proposed DSNet achieves significantly better results than ResNet, and achieves comparable performance as DenseNet but requiring fewer computation resources.



### High-Throughput Image-Based Plant Stand Count Estimation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.12552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2010.12552v1)
- **Published**: 2020-10-23 17:28:29+00:00
- **Updated**: 2020-10-23 17:28:29+00:00
- **Authors**: Saeed Khaki, Hieu Pham, Ye Han, Wade Kent, Lizhi Wang
- **Comment**: 15 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: The future landscape of modern farming and plant breeding is rapidly changing due to the complex needs of our society. The explosion of collectable data has started a revolution in agriculture to the point where innovation must occur. To a commercial organization, the accurate and efficient collection of information is necessary to ensure that optimal decisions are made at key points of the breeding cycle. However, due to the shear size of a breeding program and current resource limitations, the ability to collect precise data on individual plants is not possible. In particular, efficient phenotyping of crops to record its color, shape, chemical properties, disease susceptibility, etc. is severely limited due to labor requirements and, oftentimes, expert domain knowledge. In this paper, we propose a deep learning based approach, named DeepStand, for image-based corn stand counting at early phenological stages. The proposed method adopts a truncated VGG-16 network as a backbone feature extractor and merges multiple feature maps with different scales to make the network robust against scale variation. Our extensive computational experiments suggest that our proposed method can successfully count corn stands and out-perform other state-of-the-art methods. It is the goal of our work to be used by the larger agricultural community as a way to enable high-throughput phenotyping without the use of extensive time and labor requirements.



### Object-aware Feature Aggregation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.12573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12573v1)
- **Published**: 2020-10-23 17:56:25+00:00
- **Updated**: 2020-10-23 17:56:25+00:00
- **Authors**: Qichuan Geng, Hong Zhang, Na Jiang, Xiaojuan Qi, Liangjun Zhang, Zhong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We present an Object-aware Feature Aggregation (OFA) module for video object detection (VID). Our approach is motivated by the intriguing property that video-level object-aware knowledge can be employed as a powerful semantic prior to help object recognition. As a consequence, augmenting features with such prior knowledge can effectively improve the classification and localization performance. To make features get access to more content about the whole video, we first capture the object-aware knowledge of proposals and incorporate such knowledge with the well-established pair-wise contexts. With extensive experimental results on the ImageNet VID dataset, our approach demonstrates the effectiveness of object-aware knowledge with the superior performance of 83.93% and 86.09% mAP with ResNet-101 and ResNeXt-101, respectively. When further equipped with Sequence DIoU NMS, we obtain the best-reported mAP of 85.07% and 86.88% upon the paper submitted. The code to reproduce our results will be released after acceptance.



### View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose
- **Arxiv ID**: http://arxiv.org/abs/2010.13321v3
- **DOI**: 10.1007/s11263-021-01529-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13321v3)
- **Published**: 2020-10-23 17:58:35+00:00
- **Updated**: 2021-11-18 10:03:27+00:00
- **Authors**: Ting Liu, Jennifer J. Sun, Long Zhao, Jiaping Zhao, Liangzhe Yuan, Yuxiao Wang, Liang-Chieh Chen, Florian Schroff, Hartwig Adam
- **Comment**: Accepted to International Journal of Computer Vision (IJCV). Code is
  available at
  https://github.com/google-research/google-research/tree/master/poem. Video
  synchronization results are available at
  https://drive.google.com/corp/drive/folders/1nhPuEcX4Lhe6iK3nv84cvSCov2eJ52Xy.
  arXiv admin note: text overlap with arXiv:1912.01001
- **Journal**: None
- **Summary**: Recognition of human poses and actions is crucial for autonomous systems to interact smoothly with people. However, cameras generally capture human poses in 2D as images and videos, which can have significant appearance variations across viewpoints that make the recognition tasks challenging. To address this, we explore recognizing similarity in 3D human body poses from 2D information, which has not been well-studied in existing works. Here, we propose an approach to learning a compact view-invariant embedding space from 2D body joint keypoints, without explicitly predicting 3D poses. Input ambiguities of 2D poses from projection and occlusion are difficult to represent through a deterministic mapping, and therefore we adopt a probabilistic formulation for our embedding space. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 3D pose estimation models. We also show that by training a simple temporal embedding model, we achieve superior performance on pose sequence retrieval and largely reduce the embedding dimension from stacking frame-based embeddings for efficient large-scale retrieval. Furthermore, in order to enable our embeddings to work with partially visible input, we further investigate different keypoint occlusion augmentation strategies during training. We demonstrate that these occlusion augmentations significantly improve retrieval performance on partial 2D input poses. Results on action recognition and video alignment demonstrate that using our embeddings without any additional training achieves competitive performance relative to other models specifically trained for each task.



### Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2011.08065v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08065v1)
- **Published**: 2020-10-23 18:14:36+00:00
- **Updated**: 2020-10-23 18:14:36+00:00
- **Authors**: Debesh Jha, Sharib Ali, Krister Emanuelsen, Steven A. Hicks, VajiraThambawita, Enrique Garcia-Ceja, Michael A. Riegler, Thomas de Lange, Peter T. Schmidt, Håvard D. Johansen, Dag Johansen, Pål Halvorsen
- **Comment**: None
- **Journal**: None
- **Summary**: Gastrointestinal (GI) pathologies are periodically screened, biopsied, and resected using surgical tools. Usually the procedures and the treated or resected areas are not specifically tracked or analysed during or after colonoscopies. Information regarding disease borders, development and amount and size of the resected area get lost. This can lead to poor follow-up and bothersome reassessment difficulties post-treatment. To improve the current standard and also to foster more research on the topic we have released the ``Kvasir-Instrument'' dataset which consists of $590$ annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists. Additionally, we provide a baseline for the segmentation of the GI tools to promote research and algorithm development. We obtained a dice coefficient score of 0.9158 and a Jaccard index of 0.8578 using a classical U-Net architecture. A similar dice coefficient score was observed for DoubleUNet. The qualitative results showed that the model did not work for the images with specularity and the frames with multiple instruments, while the best result for both methods was observed on all other types of images. Both, qualitative and quantitative results show that the model performs reasonably good, but there is a large potential for further improvements. Benchmarking using the dataset provides an opportunity for researchers to contribute to the field of automatic endoscopic diagnostic and therapeutic tool segmentation for GI endoscopy.



### Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization
- **Arxiv ID**: http://arxiv.org/abs/2010.12606v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12606v3)
- **Published**: 2020-10-23 18:31:13+00:00
- **Updated**: 2021-05-02 19:27:06+00:00
- **Authors**: Judy Borowski, Roland S. Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel
- **Comment**: Published at ICLR 2021. Joint first and last authors. Code is
  available at https://bethgelab.github.io/testing_visualizations/
- **Journal**: None
- **Summary**: Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiments are designed to maximize participants' performance, and are the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations ($82\pm4\%$ accuracy; chance would be $50\%$). However, natural images - originally intended as a baseline - outperform synthetic images by a wide margin ($92\pm2\%$). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images ($65\pm5\%$ vs. $73\pm4\%$). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this baseline.



### S2cGAN: Semi-Supervised Training of Conditional GANs with Fewer Labels
- **Arxiv ID**: http://arxiv.org/abs/2010.12622v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.12622v1)
- **Published**: 2020-10-23 19:13:44+00:00
- **Updated**: 2020-10-23 19:13:44+00:00
- **Authors**: Arunava Chakraborty, Rahul Ragesh, Mahir Shah, Nipun Kwatra
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have been remarkably successful in learning complex high dimensional real word distributions and generating realistic samples. However, they provide limited control over the generation process. Conditional GANs (cGANs) provide a mechanism to control the generation process by conditioning the output on a user defined input. Although training GANs requires only unsupervised data, training cGANs requires labelled data which can be very expensive to obtain. We propose a framework for semi-supervised training of cGANs which utilizes sparse labels to learn the conditional mapping, and at the same time leverages a large amount of unsupervised data to learn the unconditional distribution. We demonstrate effectiveness of our method on multiple datasets and different conditional tasks.



### Attention-Guided Network for Iris Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.12631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12631v1)
- **Published**: 2020-10-23 19:23:51+00:00
- **Updated**: 2020-10-23 19:23:51+00:00
- **Authors**: Cunjian Chen, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are being increasingly used to address the problem of iris presentation attack detection. In this work, we propose attention-guided iris presentation attack detection (AG-PAD) to augment CNNs with attention mechanisms. Two types of attention modules are independently appended on top of the last convolutional layer of the backbone network. Specifically, the channel attention module is used to model the inter-channel relationship between features, while the position attention module is used to model inter-spatial relationship between features. An element-wise sum is employed to fuse these two attention modules. Further, a novel hierarchical attention mechanism is introduced. Experiments involving both a JHU-APL proprietary dataset and the benchmark LivDet-Iris-2017 dataset suggest that the proposed method achieves promising results. To the best of our knowledge, this is the first work that exploits the use of attention mechanisms in iris presentation attack detection.



### The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation
- **Arxiv ID**: http://arxiv.org/abs/2010.12639v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12639v1)
- **Published**: 2020-10-23 19:58:17+00:00
- **Updated**: 2020-10-23 19:58:17+00:00
- **Authors**: Shurjo Banerjee, Jesse Thomason, Jason J. Corso
- **Comment**: Conference on Robot Learning 2020
- **Journal**: None
- **Summary**: Autonomous robot systems for applications from search and rescue to assistive guidance should be able to engage in natural language dialog with people. To study such cooperative communication, we introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a benchmark of 169 natural language dialogs between a human Driver controlling a robot and a human Commander providing guidance towards navigation goals. In each trial, the pair first cooperates to localize the robot on a global map visible to the Commander, then the Driver follows Commander instructions to move the robot to a sequence of target objects. We introduce a Localization from Dialog History (LDH) and a Navigation from Dialog History (NDH) task where a learned agent is given dialog and visual observations from the robot platform as input and must localize in the global map or navigate towards the next target object, respectively. RobotSlang is comprised of nearly 5k utterances and over 1k minutes of robot camera and control streams. We present an initial model for the NDH task, and show that an agent trained in simulation can follow the RobotSlang dialog-based navigation instructions for controlling a physical robot platform. Code and data are available at https://umrobotslang.github.io/.



### Position and Rotation Invariant Sign Language Recognition from 3D Kinect Data with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.12669v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2010.12669v2)
- **Published**: 2020-10-23 21:07:40+00:00
- **Updated**: 2023-03-14 15:20:15+00:00
- **Authors**: Prasun Roy, Saumik Bhattacharya, Partha Pratim Roy, Umapada Pal
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Sign language is a gesture-based symbolic communication medium among speech and hearing impaired people. It also serves as a communication bridge between non-impaired and impaired populations. Unfortunately, in most situations, a non-impaired person is not well conversant in such symbolic languages restricting the natural information flow between these two categories. Therefore, an automated translation mechanism that seamlessly translates sign language into natural language can be highly advantageous. In this paper, we attempt to perform recognition of 30 basic Indian sign gestures. Gestures are represented as temporal sequences of 3D maps (RGB + depth), each consisting of 3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent neural network (RNN) is employed as the classifier. To improve the classifier's performance, we use geometric transformation for the alignment correction of depth frames. In our experiments, the model achieves 84.81% accuracy.



### 3DBooSTeR: 3D Body Shape and Texture Recovery
- **Arxiv ID**: http://arxiv.org/abs/2010.12670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12670v1)
- **Published**: 2020-10-23 21:07:59+00:00
- **Updated**: 2020-10-23 21:07:59+00:00
- **Authors**: Alexandre Saint, Anis Kacem, Kseniya Cherenkova, Djamila Aouada
- **Comment**: None
- **Journal**: SHARP Workshop, European Conference on Computer Vision (ECCV),
  2020
- **Summary**: We propose 3DBooSTeR, a novel method to recover a textured 3D body mesh from a textured partial 3D scan. With the advent of virtual and augmented reality, there is a demand for creating realistic and high-fidelity digital 3D human representations. However, 3D scanning systems can only capture the 3D human body shape up to some level of defects due to its complexity, including occlusion between body parts, varying levels of details, shape deformations and the articulated skeleton. Textured 3D mesh completion is thus important to enhance 3D acquisitions. The proposed approach decouples the shape and texture completion into two sequential tasks. The shape is recovered by an encoder-decoder network deforming a template body mesh. The texture is subsequently obtained by projecting the partial texture onto the template mesh before inpainting the corresponding texture map with a novel approach. The approach is validated on the 3DBodyTex.v2 dataset.



### Unsupervised Dense Shape Correspondence using Heat Kernels
- **Arxiv ID**: http://arxiv.org/abs/2010.12682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12682v1)
- **Published**: 2020-10-23 21:54:10+00:00
- **Updated**: 2020-10-23 21:54:10+00:00
- **Authors**: Mehmet Aygün, Zorah Lähner, Daniel Cremers
- **Comment**: In International Conference on 3D Vision (3DV), 2020
- **Journal**: None
- **Summary**: In this work, we propose an unsupervised method for learning dense correspondences between shapes using a recent deep functional map framework. Instead of depending on ground-truth correspondences or the computationally expensive geodesic distances, we use heat kernels. These can be computed quickly during training as the supervisor signal. Moreover, we propose a curriculum learning strategy using different heat diffusion times which provide different levels of difficulty during optimization without any sampling mechanism or hard example mining. We present the results of our method on different benchmarks which have various challenges like partiality, topological noise and different connectivity.



### Investigating Saturation Effects in Integrated Gradients
- **Arxiv ID**: http://arxiv.org/abs/2010.12697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12697v1)
- **Published**: 2020-10-23 22:48:02+00:00
- **Updated**: 2020-10-23 22:48:02+00:00
- **Authors**: Vivek Miglani, Narine Kokhlikyan, Bilal Alsallakh, Miguel Martin, Orion Reblitz-Richardson
- **Comment**: Presented at ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2020)
- **Journal**: None
- **Summary**: Integrated Gradients has become a popular method for post-hoc model interpretability. De-spite its popularity, the composition and relative impact of different regions of the integral path are not well understood. We explore these effects and find that gradients in saturated regions of this path, where model output changes minimally, contribute disproportionately to the computed attribution. We propose a variant of IntegratedGradients which primarily captures gradients in unsaturated regions and evaluate this method on ImageNet classification networks. We find that this attribution technique shows higher model faithfulness and lower sensitivity to noise com-pared with standard Integrated Gradients. A note-book illustrating our computations and results is available at https://github.com/vivekmig/captum-1/tree/ExpandedIG.



