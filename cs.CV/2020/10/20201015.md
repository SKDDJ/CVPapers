# Arxiv Papers in cs.CV on 2020-10-15
### AI-based BMI Inference from Facial Images: An Application to Weight Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2010.07442v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07442v1)
- **Published**: 2020-10-15 00:00:40+00:00
- **Updated**: 2020-10-15 00:00:40+00:00
- **Authors**: Hera Siddiqui, Ajita Rattani, Dakshina Ranjan Kisku, Tanner Dean
- **Comment**: None
- **Journal**: None
- **Summary**: Self-diagnostic image-based methods for healthy weight monitoring is gaining increased interest following the alarming trend of obesity. Only a handful of academic studies exist that investigate AI-based methods for Body Mass Index (BMI) inference from facial images as a solution to healthy weight monitoring and management. To promote further research and development in this area, we evaluate and compare the performance of five different deep-learning based Convolutional Neural Network (CNN) architectures i.e., VGG19, ResNet50, DenseNet, MobileNet, and lightCNN for BMI inference from facial images. Experimental results on the three publicly available BMI annotated facial image datasets assembled from social media, namely, VisualBMI, VIP-Attributes, and Bollywood datasets, suggest the efficacy of the deep learning methods in BMI inference from face images with minimum Mean Absolute Error (MAE) of $1.04$ obtained using ResNet50.



### Deep Learning Models for Predicting Wildfires from Historical Remote-Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2010.07445v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07445v3)
- **Published**: 2020-10-15 00:27:22+00:00
- **Updated**: 2021-02-10 14:52:42+00:00
- **Authors**: Fantine Huot, R. Lily Hu, Matthias Ihme, Qing Wang, John Burge, Tianjian Lu, Jason Hickey, Yi-Fan Chen, John Anderson
- **Comment**: Presented at 34th Conference on Neural Information Processing Systems
  (NeurIPS 2020), Artificial Intelligence for Humani- tarian Assistance and
  Disaster Response Workshop, Vancouver, Canada
- **Journal**: None
- **Summary**: Identifying regions that have high likelihood for wildfires is a key component of land and forestry management and disaster preparedness. We create a data set by aggregating nearly a decade of remote-sensing data and historical fire records to predict wildfires. This prediction problem is framed as three machine learning tasks. Results are compared and analyzed for four different deep learning models to estimate wildfire likelihood. The results demonstrate that deep learning models can successfully identify areas of high fire likelihood using aggregated data about vegetation, weather, and topography with an AUC of 83%.



### AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients
- **Arxiv ID**: http://arxiv.org/abs/2010.07468v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.07468v5)
- **Published**: 2020-10-15 01:46:13+00:00
- **Updated**: 2020-12-20 22:30:36+00:00
- **Authors**: Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan
- **Comment**: None
- **Journal**: NeurIPS 2020
- **Summary**: Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer



### Unsupervised Self-training Algorithm Based on Deep Learning for Optical Aerial Images Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.07469v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.07469v2)
- **Published**: 2020-10-15 01:51:46+00:00
- **Updated**: 2020-10-22 07:28:12+00:00
- **Authors**: Yuan Zhou, Xiangrui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Optical aerial images change detection is an important task in earth observation and has been extensively investigated in the past few decades. Generally, the supervised change detection methods with superior performance require a large amount of labeled training data which is obtained by manual annotation with high cost. In this paper, we present a novel unsupervised self-training algorithm (USTA) for optical aerial images change detection. The traditional method such as change vector analysis is used to generate the pseudo labels. We use these pseudo labels to train a well designed convolutional neural network. The network is used as a teacher to classify the original multitemporal images to generate another set of pseudo labels. Then two set of pseudo labels are used to jointly train a student network with the same structure as the teacher. The final change detection result can be obtained by the trained student network. Besides, we design an image filter to control the usage of change information in the pseudo labels in the training process of the network. The whole process of the algorithm is an unsupervised process without manually marked labels. Experimental results on the real datasets demonstrate competitive performance of our proposed method.



### Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2010.08145v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08145v3)
- **Published**: 2020-10-15 02:47:06+00:00
- **Updated**: 2021-04-06 03:44:34+00:00
- **Authors**: Suncheng Xiang, Yuzhuo Fu, Guanjie You, Ting Liu
- **Comment**: Accepted as conference paper in ICASSP 2021. arXiv admin note: text
  overlap with arXiv:2006.07139
- **Journal**: None
- **Summary**: Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has achieved remarkable performance. However, in pursuit of high accuracy, researchers in the academic always focus on training with large-scale datasets at a high cost of time and label expenses, while neglect to explore the potential of performing efficient training from millions of synthetic data. To facilitate development in this field, we reviewed the previously developed synthetic dataset GPR and built an improved one (GPR+) with larger number of identities and distinguished attributes. Based on it, we quantitatively analyze the influence of dataset attribute on re-ID system. To our best knowledge, we are among the first attempts to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. This research helps us have a deeper understanding of the fundamental problems in person re-ID, which also provides useful insights for dataset building and future practical usage.



### Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation
- **Arxiv ID**: http://arxiv.org/abs/2010.07485v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07485v5)
- **Published**: 2020-10-15 03:03:36+00:00
- **Updated**: 2021-01-12 08:46:08+00:00
- **Authors**: Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation aims at obtaining a compact and effective model by learning the mapping function from a much larger one. Due to the limited capacity of the student, the student would underfit the teacher. Therefore, student performance would unexpectedly drop when distilling from an oversized teacher, termed the capacity gap problem. We investigate this problem by study the gap of confidence between teacher and student. We find that the magnitude of confidence is not necessary for knowledge distillation and could harm the student performance if the student are forced to learn confidence. We propose Spherical Knowledge Distillation to eliminate this gap explicitly, which eases the underfitting problem. We find this novel knowledge representation can improve compact models with much larger teachers and is robust to temperature. We conducted experiments on both CIFAR100 and ImageNet, and achieve significant improvement. Specifically, we train ResNet18 to 73.0 accuracy, which is a substantial improvement over previous SOTA and is on par with resnet34 almost twice the student size. The implementation has been shared at https://github.com/forjiuzhou/Spherical-Knowledge-Distillation.



### CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.07486v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07486v2)
- **Published**: 2020-10-15 03:06:37+00:00
- **Updated**: 2020-10-19 14:39:41+00:00
- **Authors**: Lei Mou, Yitian Zhao, Huazhu Fu, Yonghuai Liu, Jun Cheng, Yalin Zheng, Pan Su, Jianlong Yang, Li Chen, Alejandro F Frang, Masahiro Akiba, Jiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Automated detection of curvilinear structures, e.g., blood vessels or nerve fibres, from medical and biomedical images is a crucial early step in automatic image interpretation associated to the management of many diseases. Precise measurement of the morphological changes of these curvilinear organ structures informs clinicians for understanding the mechanism, diagnosis, and treatment of e.g. cardiovascular, kidney, eye, lung, and neurological conditions. In this work, we propose a generic and unified convolution neural network for the segmentation of curvilinear structures and illustrate in several 2D/3D medical imaging modalities. We introduce a new curvilinear structure segmentation network (CS2-Net), which includes a self-attention mechanism in the encoder and decoder to learn rich hierarchical representations of curvilinear structures. Two types of attention modules - spatial attention and channel attention - are utilized to enhance the inter-class discrimination and intra-class responsiveness, to further integrate local features with their global dependencies and normalization, adaptively. Furthermore, to facilitate the segmentation of curvilinear structures in medical images, we employ a 1x3 and a 3x1 convolutional kernel to capture boundary features. ...



### RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure
- **Arxiv ID**: http://arxiv.org/abs/2010.07488v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07488v2)
- **Published**: 2020-10-15 03:09:08+00:00
- **Updated**: 2021-06-20 00:00:13+00:00
- **Authors**: Shounak Datta, Eduardo B. Mariottoni, David Dov, Alessandro A. Jammal, Lawrence Carin, Felipe A. Medeiros
- **Comment**: None
- **Journal**: None
- **Summary**: Glaucoma is the leading cause of irreversible blindness in the world, affecting over 70 million people. The cumbersome Standard Automated Perimetry (SAP) test is most frequently used to detect visual loss due to glaucoma. Due to the SAP test's innate difficulty and its high test-retest variability, we propose the RetiNerveNet, a deep convolutional recursive neural network for obtaining estimates of the SAP visual field. RetiNerveNet uses information from the more objective Spectral-Domain Optical Coherence Tomography (SDOCT). RetiNerveNet attempts to trace-back the arcuate convergence of the retinal nerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness around the optic disc, to estimate individual age-corrected 24-2 SAP values. Recursive passes through the proposed network sequentially yield estimates of the visual locations progressively farther from the optic disc. While all the methods used for our experiments exhibit lower performance for the advanced disease group, the proposed network is observed to be more accurate than all the baselines for estimating the individual visual field values. We further augment RetiNerveNet to additionally predict the SAP Mean Deviation values and also create an ensemble of RetiNerveNets that further improves the performance, by increasingly weighting-up underrepresented parts of the training data.



### NeRF++: Analyzing and Improving Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2010.07492v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07492v2)
- **Published**: 2020-10-15 03:24:14+00:00
- **Updated**: 2020-10-21 18:53:21+00:00
- **Authors**: Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun
- **Comment**: Code is available at https://github.com/Kai-46/nerfplusplus; fix a
  minor formatting issue in Fig. 4
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.



### A Human Eye-based Text Color Scheme Generation Method for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2010.07510v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07510v2)
- **Published**: 2020-10-15 04:24:08+00:00
- **Updated**: 2022-11-01 03:13:05+00:00
- **Authors**: Shao Wei Wang, Guan Jie Huang, Xiang Yu Luo
- **Comment**: Accepted by EITCE 2022, No.QJE77JVOLU
- **Journal**: None
- **Summary**: Synthetic data used for scene text detection and recognition tasks have proven effective. However, there are still two problems: First, the color schemes used for text coloring in the existing methods are relatively fixed color key-value pairs learned from real datasets. The dirty data in real datasets may cause the problem that the colors of text and background are too similar to be distinguished from each other. Second, the generated texts are uniformly limited to the same depth of a picture, while there are special cases in the real world that text may appear across depths. To address these problems, in this paper we design a novel method to generate color schemes, which are consistent with the characteristics of human eyes to observe things. The advantages of our method are as follows: (1) overcomes the color confusion problem between text and background caused by dirty data; (2) the texts generated are allowed to appear in most locations of any image, even across depths; (3) avoids analyzing the depth of background, such that the performance of our method exceeds the state-of-the-art methods; (4) the speed of generating images is fast, nearly one picture generated per three milliseconds. The effectiveness of our method is verified on several public datasets.



### Unsupervised Video Anomaly Detection via Normalizing Flows with Implicit Latent Features
- **Arxiv ID**: http://arxiv.org/abs/2010.07524v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07524v3)
- **Published**: 2020-10-15 05:02:02+00:00
- **Updated**: 2022-08-04 02:31:16+00:00
- **Authors**: MyeongAh Cho, Taeoh Kim, Woo Jin Kim, Suhwan Cho, Sangyoun Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos, is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion information-for example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out of distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.



### Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs
- **Arxiv ID**: http://arxiv.org/abs/2010.07526v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07526v1)
- **Published**: 2020-10-15 05:08:56+00:00
- **Updated**: 2020-10-15 05:08:56+00:00
- **Authors**: Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A. Smith, Yejin Choi
- **Comment**: Accepted to Findings of EMNLP
- **Journal**: None
- **Summary**: Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.



### Self-Supervised Domain Adaptation with Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2010.07539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07539v1)
- **Published**: 2020-10-15 06:03:47+00:00
- **Updated**: 2020-10-15 06:03:47+00:00
- **Authors**: L. Xiao, J. Xu, D. Zhao, Z. Wang, L. Wang, Y. Nie, B. Dai
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: We consider the problem of unsupervised domain adaptation for image classification. To learn target-domain-aware features from the unlabeled data, we create a self-supervised pretext task by augmenting the unlabeled data with a certain type of transformation (specifically, image rotation) and ask the learner to predict the properties of the transformation. However, the obtained feature representation may contain a large amount of irrelevant information with respect to the main task. To provide further guidance, we force the feature representation of the augmented data to be consistent with that of the original data. Intuitively, the consistency introduces additional constraints to representation learning, therefore, the learned representation is more likely to focus on the right information about the main task. Our experimental results validate the proposed method and demonstrate state-of-the-art performance on classical domain adaptation benchmarks. Code is available at https://github.com/Jiaolong/ss-da-consistency.



### FOSS: Multi-Person Age Estimation with Focusing on Objects and Still Seeing Surroundings
- **Arxiv ID**: http://arxiv.org/abs/2010.07544v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07544v2)
- **Published**: 2020-10-15 06:38:16+00:00
- **Updated**: 2021-02-19 03:47:55+00:00
- **Authors**: Masakazu Yoshimura, Satoshi Ogata
- **Comment**: The precision and speed are improved from previous version with
  modified loss function and image size
- **Journal**: None
- **Summary**: Age estimation from images can be used in many practical scenes. Most of the previous works targeted on the estimation from images in which only one face exists. Also, most of the open datasets for age estimation contain images like that. However, in some situations, age estimation in the wild and for multi-person is needed. Usually, such situations were solved by two separate models; one is a face detector model which crops facial regions and the other is an age estimation model which estimates from cropped images. In this work, we propose a method that can detect and estimate the age of multi-person with a single model which estimates age with focusing on faces and still seeing surroundings. Also, we propose a training method which enables the model to estimate multi-person well despite trained with images in which only one face is photographed. In the experiments, we evaluated our proposed method compared with the traditional approach using two separate models. As the result, the accuracy could be enhanced with our proposed method. We also adapted our proposed model to commonly used single person photographed age estimation datasets and it is proved that our method is also effective to those images and outperforms the state of the art accuracy.



### MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.07548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07548v2)
- **Published**: 2020-10-15 06:52:16+00:00
- **Updated**: 2020-12-08 09:10:53+00:00
- **Authors**: Patrick Dendorfer, Aljoša Ošep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, Laura Leal-Taixé
- **Comment**: Accepted at IJCV
- **Journal**: None
- **Summary**: Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data, and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shed some light on potential future research directions.



### Encoder-decoder semantic segmentation models for electroluminescence images of thin-film photovoltaic modules
- **Arxiv ID**: http://arxiv.org/abs/2010.07556v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07556v1)
- **Published**: 2020-10-15 07:09:16+00:00
- **Updated**: 2020-10-15 07:09:16+00:00
- **Authors**: Evgenii Sovetkin, Elbert Jan Achterberg, Thomas Weber, Bart E. Pieters
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a series of image segmentation methods based on the deep neural networks in order to perform semantic segmentation of electroluminescence (EL) images of thin-film modules. We utilize the encoder-decoder deep neural network architecture. The framework is general such that it can easily be extended to other types of images (e.g. thermography) or solar cell technologies (e.g. crystalline silicon modules). The networks are trained and tested on a sample of images from a database with 6000 EL images of Copper Indium Gallium Diselenide (CIGS) thin film modules. We selected two types of features to extract, shunts and so called "droplets". The latter feature is often observed in the set of images. Several models are tested using various combinations of encoder-decoder layers, and a procedure is proposed to select the best model. We show exemplary results with the best selected model. Furthermore, we applied the best model to the full set of 6000 images and demonstrate that the automated segmentation of EL images can reveal many subtle features which cannot be inferred from studying a small sample of images. We believe these features can contribute to process optimization and quality control.



### Interactive Latent Interpolation on MNIST Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.07581v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07581v1)
- **Published**: 2020-10-15 08:04:48+00:00
- **Updated**: 2020-10-15 08:04:48+00:00
- **Authors**: Mazeyar Moeini Feizabadi, Ali Mohammed Shujjat, Sarah Shahid, Zainab Hasnain
- **Comment**: For associated demonstration and code repository, see
  https://mazy1998.github.io/browserGAN/ and
  https://github.com/mazy1998/browserGAN respectively
- **Journal**: None
- **Summary**: This paper will discuss the potential of dimensionality reduction with a web-based use of GANs. Throughout a variety of experiments, we show synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with latent vectors. GANs have proved to be a remarkable technique to produce computer-generated images, very similar to an original image. This is primarily useful when coupled with dimensionality reduction as an effective application of our algorithm. We proposed a new architecture for GANs, which ended up not working for mathematical reasons later explained. We then proposed a new web-based GAN that still takes advantage of dimensionality reduction to speed generation in the browser to .2 milliseconds. Lastly, we made a modern UI with linear interpolation to present the work. With the speedy generation, we can generate so fast that we can create an animation type effect that we have never seen before that works on both web and mobile.



### Respecting Domain Relations: Hypothesis Invariance for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2010.07591v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07591v1)
- **Published**: 2020-10-15 08:26:08+00:00
- **Updated**: 2020-10-15 08:26:08+00:00
- **Authors**: Ziqi Wang, Marco Loog, Jan van Gemert
- **Comment**: accepted at ICPR 2020
- **Journal**: None
- **Summary**: In domain generalization, multiple labeled non-independent and non-identically distributed source domains are available during training while neither the data nor the labels of target domains are. Currently, learning so-called domain invariant representations (DIRs) is the prevalent approach to domain generalization. In this work, we define DIRs employed by existing works in probabilistic terms and show that by learning DIRs, overly strict requirements are imposed concerning the invariance. Particularly, DIRs aim to perfectly align representations of different domains, i.e. their input distributions. This is, however, not necessary for good generalization to a target domain and may even dispose of valuable classification information. We propose to learn so-called hypothesis invariant representations (HIRs), which relax the invariance assumptions by merely aligning posteriors, instead of aligning representations. We report experimental results on public domain generalization datasets to show that learning HIRs is more effective than learning DIRs. In fact, our approach can even compete with approaches using prior knowledge about domains.



### DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding
- **Arxiv ID**: http://arxiv.org/abs/2010.11685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11685v1)
- **Published**: 2020-10-15 08:54:17+00:00
- **Updated**: 2020-10-15 08:54:17+00:00
- **Authors**: Zilong Wang, Mingjie Zhan, Xuebo Liu, Ding Liang
- **Comment**: Accepted to EMNLP 2020 Findings
- **Journal**: None
- **Summary**: Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well, it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works cannot apply to all forms because of their requirements on formats. Therefore, we concentrate on the most elementary components, the key-value pairs, and adopt multimodal methods to extract features. We consider the form structure as a tree-like or graph-like hierarchy of text fragments. The parent-child relation corresponds to the key-value pairs in forms. We utilize the state-of-the-art models and design targeted extraction modules to extract multimodal features from semantic contents, layout information, and visual images. A hybrid fusion method of concatenation and feature shifting is designed to fuse the heterogeneous features and provide an informative joint representation. We adopt an asymmetric algorithm and negative sampling in our model as well. We validate our method on two benchmarks, MedForm and FUNSD, and extensive experiments demonstrate the effectiveness of our method.



### Object Tracking Using Spatio-Temporal Future Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.07605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07605v1)
- **Published**: 2020-10-15 09:02:50+00:00
- **Updated**: 2020-10-15 09:02:50+00:00
- **Authors**: Yuan Liu, Ruoteng Li, Robby T. Tan, Yu Cheng, Xiubao Sui
- **Comment**: 12 pages, 11 figures, journal
- **Journal**: None
- **Summary**: Occlusion is a long-standing problem that causes many modern tracking methods to be erroneous. In this paper, we address the occlusion problem by exploiting the current and future possible locations of the target object from its past trajectory. To achieve this, we introduce a learning-based tracking method that takes into account background motion modeling and trajectory prediction. Our trajectory prediction module predicts the target object's locations in the current and future frames based on the object's past trajectory. Since, in the input video, the target object's trajectory is not only affected by the object motion but also the camera motion, our background motion module estimates the camera motion. So that the object's trajectory can be made independent from it. To dynamically switch between the appearance-based tracker and the trajectory prediction, we employ a network that can assess how good a tracking prediction is, and we use the assessment scores to choose between the appearance-based tracker's prediction and the trajectory-based prediction. Comprehensive evaluations show that the proposed method sets a new state-of-the-art performance on commonly used tracking benchmarks.



### Fully Unsupervised Person Re-identification viaSelective Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07608v2)
- **Published**: 2020-10-15 09:09:23+00:00
- **Updated**: 2021-03-04 02:37:28+00:00
- **Authors**: Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Unsupervised person ReID attracts a lot of attention recently, due to it works without intensive manual annotation and thus shows great potential of adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively sampled negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic dictionaries, among which the global and local memory banks are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state-of-the-arts.



### THIN: THrowable Information Networks and Application for Facial Expression Recognition In The Wild
- **Arxiv ID**: http://arxiv.org/abs/2010.07614v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07614v3)
- **Published**: 2020-10-15 09:20:31+00:00
- **Updated**: 2022-01-14 14:58:52+00:00
- **Authors**: Estephe Arnaud, Arnaud Dapogny, Kevin Bailly
- **Comment**: None
- **Journal**: None
- **Summary**: For a number of machine learning problems, an exogenous variable can be identified such that it heavily influences the appearance of the different classes, and an ideal classifier should be invariant to this variable. An example of such exogenous variable is identity if facial expression recognition (FER) is considered. In this paper, we propose a dual exogenous/endogenous representation. The former captures the exogenous variable whereas the second one models the task at hand (e.g. facial expression). We design a prediction layer that uses a tree-gated deep ensemble conditioned by the exogenous representation. We also propose an exogenous dispelling loss to remove the exogenous information from the endogenous representation. Thus, the exogenous information is used two times in a throwable fashion, first as a conditioning variable for the target task, and second to create invariance within the endogenous representation. We call this method THIN, standing for THrowable Information Networks. We experimentally validate THIN in several contexts where an exogenous information can be identified, such as digit recognition under large rotations and shape recognition at multiple scales. We also apply it to FER with identity as the exogenous variable. We demonstrate that THIN significantly outperforms state-of-the-art approaches on several challenging datasets.



### HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2010.07621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07621v1)
- **Published**: 2020-10-15 09:32:38+00:00
- **Updated**: 2020-10-15 09:32:38+00:00
- **Authors**: Pengcheng Yuan, Shufei Lin, Cheng Cui, Yuning Du, Ruoyu Guo, Dongliang He, Errui Ding, Shumin Han
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses representational block named Hierarchical-Split Block, which can be taken as a plug-and-play block to upgrade existing convolutional neural networks, improves model performance significantly in a network. Hierarchical-Split Block contains many hierarchical split and concatenate connections within one single residual block. We find multi-scale features is of great importance for numerous vision tasks. Moreover, Hierarchical-Split block is very flexible and efficient, which provides a large space of potential network architectures for different applications. In this work, we present a common backbone based on Hierarchical-Split block for tasks: image classification, object detection, instance segmentation and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baseline. As shown in Figure1, for image classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1 accuracy with competitive latency on ImageNet-1k dataset. It also outperforms most state-of-the-art models. The source code and models will be available on: https://github.com/PaddlePaddle/PaddleClas



### Empty Cities: a Dynamic-Object-Invariant Space for Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2010.07646v1
- **DOI**: 10.1109/TRO.2020.3031267
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.07646v1)
- **Published**: 2020-10-15 10:31:12+00:00
- **Updated**: 2020-10-15 10:31:12+00:00
- **Authors**: Berta Bescos, Cesar Cadena, Jose Neira
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a data-driven approach to obtain the static image of a scene, eliminating dynamic objects that might have been present at the time of traversing the scene with a camera. The general objective is to improve vision-based localization and mapping tasks in dynamic environments, where the presence (or absence) of different dynamic objects in different moments makes these tasks less robust. We introduce an end-to-end deep learning framework to turn images of an urban environment that include dynamic content, such as vehicles or pedestrians, into realistic static frames suitable for localization and mapping. This objective faces two main challenges: detecting the dynamic objects, and inpainting the static occluded back-ground. The first challenge is addressed by the use of a convolutional network that learns a multi-class semantic segmentation of the image. The second challenge is approached with a generative adversarial model that, taking as input the original dynamic image and the computed dynamic/static binary mask, is capable of generating the final static image. This framework makes use of two new losses, one based on image steganalysis techniques, useful to improve the inpainting quality, and another one based on ORB features, designed to enhance feature matching between real and hallucinated image regions. To validate our approach, we perform an extensive evaluation on different tasks that are affected by dynamic entities, i.e., visual odometry, place recognition and multi-view stereo, with the hallucinated images. Code has been made available on https://github.com/bertabescos/EmptyCities_SLAM.



### A Deep Drift-Diffusion Model for Image Aesthetic Score Distribution Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.07661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07661v1)
- **Published**: 2020-10-15 11:01:46+00:00
- **Updated**: 2020-10-15 11:01:46+00:00
- **Authors**: Xin Jin, Xiqiao Li, Heng Huang, Xiaodong Li, Xinghui Zhou
- **Comment**: 13 pages, 8 pages
- **Journal**: None
- **Summary**: The task of aesthetic quality assessment is complicated due to its subjectivity. In recent years, the target representation of image aesthetic quality has changed from a one-dimensional binary classification label or numerical score to a multi-dimensional score distribution. According to current methods, the ground truth score distributions are straightforwardly regressed. However, the subjectivity of aesthetics is not taken into account, that is to say, the psychological processes of human beings are not taken into consideration, which limits the performance of the task. In this paper, we propose a Deep Drift-Diffusion (DDD) model inspired by psychologists to predict aesthetic score distribution from images. The DDD model can describe the psychological process of aesthetic perception instead of traditional modeling of the results of assessment. We use deep convolution neural networks to regress the parameters of the drift-diffusion model. The experimental results in large scale aesthetic image datasets reveal that our novel DDD model is simple but efficient, which outperforms the state-of-the-art methods in aesthetic score distribution prediction. Besides, different psychological processes can also be predicted by our model.



### Convolutional Neural Network for Blur Images Detection as an Alternative for Laplacian Method
- **Arxiv ID**: http://arxiv.org/abs/2010.07936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07936v1)
- **Published**: 2020-10-15 11:13:22+00:00
- **Updated**: 2020-10-15 11:13:22+00:00
- **Authors**: Tomasz Szandala
- **Comment**: None
- **Journal**: None
- **Summary**: With the prevalence of digital cameras, the number of digital images increases quickly, which raises the demand for non-manual image quality assessment. While there are many methods considered useful for detecting blurriness, in this paper we propose and evaluate a new method that uses a deep convolutional neural network, which can determine whether an image is blurry or not. Experimental results demonstrate the effectiveness of the proposed scheme and are compared to deterministic methods using the confusion matrix.



### Integrating Coarse Granularity Part-level Features with Supervised Global-level Features for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2010.07675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07675v1)
- **Published**: 2020-10-15 11:49:20+00:00
- **Updated**: 2020-10-15 11:49:20+00:00
- **Authors**: Xiaofei Mao, Jiahao Cao, Dongfang Li, Xia Jia, Qingfang Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Holistic person re-identification (Re-ID) and partial person re-identification have achieved great progress respectively in recent years. However, scenarios in reality often include both holistic and partial pedestrian images, which makes single holistic or partial person Re-ID hard to work. In this paper, we propose a robust coarse granularity part-level person Re-ID network (CGPN), which not only extracts robust regional level body features, but also integrates supervised global features for both holistic and partial person images. CGPN gains two-fold benefit toward higher accuracy for person Re-ID. On one hand, CGPN learns to extract effective body part features for both holistic and partial person images. On the other hand, compared with extracting global features directly by backbone network, CGPN learns to extract more accurate global features with a supervision strategy. The single model trained on three Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK03 achieves state-of-the-art performances and outperforms any existing approaches. Especially on CUHK03, which is the most challenging dataset for person Re-ID, in single query mode, we obtain a top result of Rank-1/mAP=87.1\%/83.6\% with this method without re-ranking, outperforming the current best method by +7.0\%/+6.7\%.



### LiteDepthwiseNet: An Extreme Lightweight Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.07726v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07726v1)
- **Published**: 2020-10-15 13:12:17+00:00
- **Updated**: 2020-10-15 13:12:17+00:00
- **Authors**: Benlei Cui, XueMei Dong, Qiaoqiao Zhan, Jiangtao Peng, Weiwei Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have shown considerable potential for hyperspectral image (HSI) classification, which can achieve high accuracy compared with traditional methods. However, they often need a large number of training samples and have a lot of parameters and high computational overhead. To solve these problems, this paper proposes a new network architecture, LiteDepthwiseNet, for HSI classification. Based on 3D depthwise convolution, LiteDepthwiseNet can decompose standard convolution into depthwise convolution and pointwise convolution, which can achieve high classification performance with minimal parameters. Moreover, we remove the ReLU layer and Batch Normalization layer in the original 3D depthwise convolution, which significantly improves the overfitting phenomenon of the model on small sized datasets. In addition, focal loss is used as the loss function to improve the model's attention on difficult samples and unbalanced data, and its training performance is significantly better than that of cross-entropy loss or balanced cross-entropy loss. Experiment results on three benchmark hyperspectral datasets show that LiteDepthwiseNet achieves state-of-the-art performance with a very small number of parameters and low computational cost.



### Self-training for Few-shot Transfer Across Extreme Task Differences
- **Arxiv ID**: http://arxiv.org/abs/2010.07734v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07734v2)
- **Published**: 2020-10-15 13:23:59+00:00
- **Updated**: 2021-03-17 16:11:57+00:00
- **Authors**: Cheng Perng Phoo, Bharath Hariharan
- **Comment**: Published as a conference paper at ICLR 2021(oral)
- **Journal**: None
- **Summary**: Most few-shot learning techniques are pre-trained on a large, labeled "base dataset". In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different "source" problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains. Our code is available at https://github.com/cpphoo/STARTUP.



### Improved Multi-Source Domain Adaptation by Preservation of Factors
- **Arxiv ID**: http://arxiv.org/abs/2010.07783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07783v2)
- **Published**: 2020-10-15 14:19:57+00:00
- **Updated**: 2020-10-16 07:15:58+00:00
- **Authors**: Sebastian Schrom, Stephan Hasler, Jürgen Adamy
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Adaptation (DA) is a highly relevant research topic when it comes to image classification with deep neural networks. Combining multiple source domains in a sophisticated way to optimize a classification model can improve the generalization to a target domain. Here, the difference in data distributions of source and target image datasets plays a major role. In this paper, we describe based on a theory of visual factors how real-world scenes appear in images in general and how recent DA datasets are composed of such. We show that different domains can be described by a set of so called domain factors, whose values are consistent within a domain, but can change across domains. Many DA approaches try to remove all domain factors from the feature representation to be domain invariant. In this paper we show that this can lead to negative transfer since task-informative factors can get lost as well. To address this, we propose Factor-Preserving DA (FP-DA), a method to train a deep adversarial unsupervised DA model, which is able to preserve specific task relevant factors in a multi-domain scenario. We demonstrate on CORe50, a dataset with many domains, how such factors can be identified by standard one-to-one transfer experiments between single domains combined with PCA. By applying FP-DA, we show that the highest average and minimum performance can be achieved.



### Generalizing Universal Adversarial Attacks Beyond Additive Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2010.07788v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07788v2)
- **Published**: 2020-10-15 14:25:58+00:00
- **Updated**: 2020-10-29 18:20:21+00:00
- **Authors**: Yanghao Zhang, Wenjie Ruan, Fu Wang, Xiaowei Huang
- **Comment**: A short version of this work will appear in the ICDM 2020 conference
  proceedings
- **Journal**: None
- **Summary**: The previous study has shown that universal adversarial attacks can fool deep neural networks over a large set of input images with a single human-invisible perturbation. However, current methods for universal adversarial attacks are based on additive perturbation, which cause misclassification when the perturbation is directly added to the input images. In this paper, for the first time, we show that a universal adversarial attack can also be achieved via non-additive perturbation (e.g., spatial transformation). More importantly, to unify both additive and non-additive perturbations, we propose a novel unified yet flexible framework for universal adversarial attacks, called GUAP, which is able to initiate attacks by additive perturbation, non-additive perturbation, or the combination of both. Extensive experiments are conducted on CIFAR-10 and ImageNet datasets with six deep neural network models including GoogleLeNet, VGG16/19, ResNet101/152, and DenseNet121. The empirical experiments demonstrate that GUAP can obtain up to 90.9% and 99.24% successful attack rates on CIFAR-10 and ImageNet datasets, leading to over 15% and 19% improvements respectively than current state-of-the-art universal adversarial attacks. The code for reproducing the experiments in this paper is available at https://github.com/TrustAI/GUAP.



### XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge
- **Arxiv ID**: http://arxiv.org/abs/2010.07290v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.07290v2)
- **Published**: 2020-10-15 14:45:00+00:00
- **Updated**: 2021-07-07 08:57:52+00:00
- **Authors**: Zaccharie Ramzi, Philippe Ciuciu, Jean-Luc Starck
- **Comment**: 8 pages, 3 figures, presented as an oral to the 2021 ISMRM conference
- **Journal**: None
- **Summary**: We present a new neural network, the XPDNet, for MRI reconstruction from periodically under-sampled multi-coil data. We inform the design of this network by taking best practices from MRI reconstruction and computer vision. We show that this network can achieve state-of-the-art reconstruction results, as shown by its ranking of second in the fastMRI 2020 challenge.



### CIMON: Towards High-quality Hash Codes
- **Arxiv ID**: http://arxiv.org/abs/2010.07804v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07804v4)
- **Published**: 2020-10-15 14:47:14+00:00
- **Updated**: 2021-08-21 04:13:07+00:00
- **Authors**: Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Minghua Deng, Jinwen Ma, Zhongming Jin, Jianqiang Huang, Xian-Sheng Hua
- **Comment**: Accepted by IJCAI 21
- **Journal**: None
- **Summary**: Recently, hashing is widely used in approximate nearest neighbor search for its storage and computational efficiency. Most of the unsupervised hashing methods learn to map images into semantic similarity-preserving hash codes by constructing local semantic similarity structure from the pre-trained model as the guiding information, i.e., treating each point pair similar if their distance is small in feature space. However, due to the inefficient representation ability of the pre-trained model, many false positives and negatives in local semantic similarity will be introduced and lead to error propagation during the hash code learning. Moreover, few of the methods consider the robustness of models, which will cause instability of hash codes to disturbance. In this paper, we propose a new method named {\textbf{C}}omprehensive s{\textbf{I}}milarity {\textbf{M}}ining and c{\textbf{O}}nsistency lear{\textbf{N}}ing (CIMON). First, we use global refinement and similarity statistical distribution to obtain reliable and smooth guidance. Second, both semantic and contrastive consistency learning are introduced to derive both disturb-invariant and discriminative hash codes. Extensive experiments on several benchmark datasets show that the proposed method outperforms a wide range of state-of-the-art methods in both retrieval performance and robustness.



### Does Data Augmentation Benefit from Split BatchNorms
- **Arxiv ID**: http://arxiv.org/abs/2010.07810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.07810v1)
- **Published**: 2020-10-15 15:00:43+00:00
- **Updated**: 2020-10-15 15:00:43+00:00
- **Authors**: Amil Merchant, Barret Zoph, Ekin Dogus Cubuk
- **Comment**: 9 pages (+ 3 for references)
- **Journal**: None
- **Summary**: Data augmentation has emerged as a powerful technique for improving the performance of deep neural networks and led to state-of-the-art results in computer vision. However, state-of-the-art data augmentation strongly distorts training images, leading to a disparity between examples seen during training and inference. In this work, we explore a recently proposed training paradigm in order to correct for this disparity: using an auxiliary BatchNorm for the potentially out-of-distribution, strongly augmented images. Our experiments then focus on how to define the BatchNorm parameters that are used at evaluation. To eliminate the train-test disparity, we experiment with using the batch statistics defined by clean training images only, yet surprisingly find that this does not yield improvements in model performance. Instead, we investigate using BatchNorm parameters defined by weak augmentations and find that this method significantly improves the performance of common image classification benchmarks such as CIFAR-10, CIFAR-100, and ImageNet. We then explore a fundamental trade-off between accuracy and robustness coming from using different BatchNorm parameters, providing greater insight into the benefits of data augmentation on model performance.



### Boosting Image-based Mutual Gaze Detection using Pseudo 3D Gaze
- **Arxiv ID**: http://arxiv.org/abs/2010.07811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07811v2)
- **Published**: 2020-10-15 15:01:41+00:00
- **Updated**: 2020-12-22 17:20:59+00:00
- **Authors**: Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, Xuhui Jia, Yukun Zhu, Bradley Green
- **Comment**: None
- **Journal**: None
- **Summary**: Mutual gaze detection, i.e., predicting whether or not two people are looking at each other, plays an important role in understanding human interactions. In this work, we focus on the task of image-based mutual gaze detection, and propose a simple and effective approach to boost the performance by using an auxiliary 3D gaze estimation task during the training phase. We achieve the performance boost without additional labeling cost by training the 3D gaze estimation branch using pseudo 3D gaze labels deduced from mutual gaze labels. By sharing the head image encoder between the 3D gaze estimation and the mutual gaze detection branches, we achieve better head features than learned by training the mutual gaze detection branch alone. Experimental results on three image datasets show that the proposed approach improves the detection performance significantly without additional annotations. This work also introduces a new image dataset that consists of 33.1K pairs of humans annotated with mutual gaze labels in 29.2K images.



### DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM
- **Arxiv ID**: http://arxiv.org/abs/2010.07820v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07820v1)
- **Published**: 2020-10-15 15:25:30+00:00
- **Updated**: 2020-10-15 15:25:30+00:00
- **Authors**: Berta Bescos, Carlos Campos, Juan D. Tardós, José Neira
- **Comment**: None
- **Journal**: None
- **Summary**: The assumption of scene rigidity is common in visual SLAM algorithms. However, it limits their applicability in populated real-world environments. Furthermore, most scenarios including autonomous driving, multi-robot collaboration and augmented/virtual reality, require explicit motion information of the surroundings to help with decision making and scene understanding. We present in this paper DynaSLAM II, a visual SLAM system for stereo and RGB-D configurations that tightly integrates the multi-object tracking capability.   DynaSLAM II makes use of instance semantic segmentation and of ORB features to track dynamic objects. The structure of the static scene and of the dynamic objects is optimized jointly with the trajectories of both the camera and the moving agents within a novel bundle adjustment proposal. The 3D bounding boxes of the objects are also estimated and loosely optimized within a fixed temporal window. We demonstrate that tracking dynamic objects does not only provide rich clues for scene understanding but is also beneficial for camera tracking.   The project code will be released upon acceptance.



### Interpretation of Swedish Sign Language using Convolutional Neural Networks and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.07827v1)
- **Published**: 2020-10-15 15:34:09+00:00
- **Updated**: 2020-10-15 15:34:09+00:00
- **Authors**: Gustaf Halvardsson, Johanna Peterson, César Soto-Valero, Benoit Baudry
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic interpretation of sign languages is a challenging task, as it requires the usage of high-level vision and high-level motion processing systems for providing accurate image perception. In this paper, we use Convolutional Neural Networks (CNNs) and transfer learning in order to make computers able to interpret signs of the Swedish Sign Language (SSL) hand alphabet. Our model consist of the implementation of a pre-trained InceptionV3 network, and the usage of the mini-batch gradient descent optimization algorithm. We rely on transfer learning during the pre-training of the model and its data. The final accuracy of the model, based on 8 study subjects and 9,400 images, is 85%. Our results indicate that the usage of CNNs is a promising approach to interpret sign languages, and transfer learning can be used to achieve high testing accuracy despite using a small training dataset. Furthermore, we describe the implementation details of our model to interpret signs as a user-friendly web application.



### Semi-Supervised Semantic Segmentation in Earth Observation: The MiniFrance Suite, Dataset Analysis and Multi-task Network Study
- **Arxiv ID**: http://arxiv.org/abs/2010.07830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07830v1)
- **Published**: 2020-10-15 15:36:58+00:00
- **Updated**: 2020-10-15 15:36:58+00:00
- **Authors**: Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre Boulch, Nicolas Audebert, Sébastien Lefèvre
- **Comment**: None
- **Journal**: None
- **Summary**: The development of semi-supervised learning techniques is essential to enhance the generalization capacities of machine learning algorithms. Indeed, raw image data are abundant while labels are scarce, therefore it is crucial to leverage unlabeled inputs to build better models. The availability of large databases have been key for the development of learning algorithms with high level performance.   Despite the major role of machine learning in Earth Observation to derive products such as land cover maps, datasets in the field are still limited, either because of modest surface coverage, lack of variety of scenes or restricted classes to identify. We introduce a novel large-scale dataset for semi-supervised semantic segmentation in Earth Observation, the MiniFrance suite. MiniFrance has several unprecedented properties: it is large-scale, containing over 2000 very high resolution aerial images, accounting for more than 200 billions samples (pixels); it is varied, covering 16 conurbations in France, with various climates, different landscapes, and urban as well as countryside scenes; and it is challenging, considering land use classes with high-level semantics. Nevertheless, the most distinctive quality of MiniFrance is being the only dataset in the field especially designed for semi-supervised learning: it contains labeled and unlabeled images in its training partition, which reproduces a life-like scenario. Along with this dataset, we present tools for data representativeness analysis in terms of appearance similarity and a thorough study of MiniFrance data, demonstrating that it is suitable for learning and generalizes well in a semi-supervised setting. Finally, we present semi-supervised deep architectures based on multi-task learning and the first experiments on MiniFrance.



### Deep image prior for undersampling high-speed photoacoustic microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.12041v2
- **DOI**: 10.1016/j.pacs.2021.100266
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12041v2)
- **Published**: 2020-10-15 15:46:19+00:00
- **Updated**: 2021-04-07 20:02:10+00:00
- **Authors**: Tri Vu, Anthony DiSpirito III, Daiwei Li, Zixuan Zhang, Xiaoyi Zhu, Maomao Chen, Laiming Jiang, Dong Zhang, Jianwen Luo, Yu Shrike Zhang, Qifa Zhou, Roarke Horstmeyer, Junjie Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Photoacoustic microscopy (PAM) is an emerging imaging method combining light and sound. However, limited by the laser's repetition rate, state-of-the-art high-speed PAM technology often sacrifices spatial sampling density (i.e., undersampling) for increased imaging speed over a large field-of-view. Deep learning (DL) methods have recently been used to improve sparsely sampled PAM images; however, these methods often require time-consuming pre-training and large training dataset with ground truth. Here, we propose the use of deep image prior (DIP) to improve the image quality of undersampled PAM images. Unlike other DL approaches, DIP requires neither pre-training nor fully-sampled ground truth, enabling its flexible and fast implementation on various imaging targets. Our results have demonstrated substantial improvement in PAM images with as few as 1.4$\%$ of the fully sampled pixels on high-speed PAM. Our approach outperforms interpolation, is competitive with pre-trained supervised DL method, and is readily translated to other high-speed, undersampling imaging modalities.



### A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack and Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.07849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07849v1)
- **Published**: 2020-10-15 16:07:26+00:00
- **Updated**: 2020-10-15 16:07:26+00:00
- **Authors**: Hongjun Wang, Guanbin Li, Xiaobai Liu, Liang Lin
- **Comment**: Accepted as a Regular Paper in IEEE Transactions on Pattern Analysis
  and Machine Intelligence
- **Journal**: None
- **Summary**: Although deep convolutional neural networks (CNNs) have demonstrated remarkable performance on multiple computer vision tasks, researches on adversarial learning have shown that deep models are vulnerable to adversarial examples, which are crafted by adding visually imperceptible perturbations to the input images. Most of the existing adversarial attack methods only create a single adversarial example for the input, which just gives a glimpse of the underlying data manifold of adversarial examples. An attractive solution is to explore the solution space of the adversarial examples and generate a diverse bunch of them, which could potentially improve the robustness of real-world systems and help prevent severe security threats and vulnerabilities. In this paper, we present an effective method, called Hamiltonian Monte Carlo with Accumulated Momentum (HMCAM), aiming to generate a sequence of adversarial examples. To improve the efficiency of HMC, we propose a new regime to automatically control the length of trajectories, which allows the algorithm to move with adaptive step sizes along the search direction at different positions. Moreover, we revisit the reason for high computational cost of adversarial training under the view of MCMC and design a new generative method called Contrastive Adversarial Training (CAT), which approaches equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD) and achieve a trade-off between efficiency and accuracy. Both quantitative and qualitative analysis on several natural image datasets and practical systems have confirmed the superiority of the proposed algorithm.



### An Empirical Analysis of Visual Features for Multiple Object Tracking in Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.07881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07881v1)
- **Published**: 2020-10-15 16:57:13+00:00
- **Updated**: 2020-10-15 16:57:13+00:00
- **Authors**: Mehdi Miah, Justine Pepin, Nicolas Saunier, Guillaume-Alexandre Bilodeau
- **Comment**: Accepted on 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: This paper addresses the problem of selecting appearance features for multiple object tracking (MOT) in urban scenes. Over the years, a large number of features has been used for MOT. However, it is not clear whether some of them are better than others. Commonly used features are color histograms, histograms of oriented gradients, deep features from convolutional neural networks and re-identification (ReID) features. In this study, we assess how good these features are at discriminating objects enclosed by a bounding box in urban scene tracking scenarios. Several affinity measures, namely the $\mathrm{L}_1$, $\mathrm{L}_2$ and the Bhattacharyya distances, Rank-1 counts and the cosine similarity, are also assessed for their impact on the discriminative power of the features. Results on several datasets show that features from ReID networks are the best for discriminating instances from one another regardless of the quality of the detector. If a ReID model is not available, color histograms may be selected if the detector has a good recall and there are few occlusions; otherwise, deep features are more robust to detectors with lower recall. The project page is http://www.mehdimiah.com/visual_features.



### Representation Learning via Invariant Causal Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2010.07922v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.07922v1)
- **Published**: 2020-10-15 17:53:37+00:00
- **Updated**: 2020-10-15 17:53:37+00:00
- **Authors**: Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, Charles Blundell
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on $51$ out of $57$ games.



### Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.07930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07930v2)
- **Published**: 2020-10-15 17:59:08+00:00
- **Updated**: 2020-12-03 05:05:15+00:00
- **Authors**: Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released.



### LTN: Long-Term Network for Long-Term Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.07931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07931v1)
- **Published**: 2020-10-15 17:59:09+00:00
- **Updated**: 2020-10-15 17:59:09+00:00
- **Authors**: YingQiao Wang
- **Comment**: Under Review For ICRA/RA-L 2021
- **Journal**: None
- **Summary**: Making accurate motion prediction of surrounding agents such as pedestrians and vehicles is a critical task when robots are trying to perform autonomous navigation tasks. Recent research on multi-modal trajectory prediction, including regression and classification approaches, perform very well at short-term prediction. However, when it comes to long-term prediction, most Long Short-Term Memory (LSTM) based models tend to diverge far away from the ground truth. Therefore, in this work, we present a two-stage framework for long-term trajectory prediction, which is named as Long-Term Network (LTN). Our Long-Term Network integrates both the regression and classification approaches. We first generate a set of proposed trajectories with our proposed distribution using a Conditional Variational Autoencoder (CVAE), and then classify them with binary labels, and output the trajectories with the highest score. We demonstrate our Long-Term Network's performance with experiments on two real-world pedestrian datasets: ETH/UCY, Stanford Drone Dataset (SDD), and one challenging real-world driving forecasting dataset: nuScenes. The results show that our method outperforms multiple state-of-the-art approaches in long-term trajectory prediction in terms of accuracy.



### Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2010.07954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.07954v1)
- **Published**: 2020-10-15 18:01:15+00:00
- **Updated**: 2020-10-15 18:01:15+00:00
- **Authors**: Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge
- **Comment**: EMNLP 2020
- **Journal**: None
- **Summary**: We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations. We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in simulated, photo-realistic environments.



### Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement
- **Arxiv ID**: http://arxiv.org/abs/2010.07958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07958v1)
- **Published**: 2020-10-15 18:04:46+00:00
- **Updated**: 2020-10-15 18:04:46+00:00
- **Authors**: Yongqing Liang, Xin Li, Navid Jafari, Qin Chen
- **Comment**: Preprint version. Accepted by NeurIPS 2020
- **Journal**: NeurIPS 2020
- **Summary**: We propose a new matching-based framework for semi-supervised video object segmentation (VOS). Recently, state-of-the-art VOS performance has been achieved by matching-based algorithms, in which feature banks are created to store features for region matching and classification. However, how to effectively organize information in the continuously growing feature bank remains under-explored, and this leads to inefficient design of the bank. We introduce an adaptive feature bank update scheme to dynamically absorb new features and discard obsolete features. We also design a new confidence loss and a fine-grained segmentation module to enhance the segmentation accuracy in uncertain regions. On public benchmarks, our algorithm outperforms existing state-of-the-arts.



### Quantifying the Extent to Which Race and Gender Features Determine Identity in Commercial Face Recognition Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2010.07979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07979v1)
- **Published**: 2020-10-15 18:52:36+00:00
- **Updated**: 2020-10-15 18:52:36+00:00
- **Authors**: John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton, Arun R. Vemury
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Human face features can be used to determine individual identity as well as demographic information like gender and race. However, the extent to which black-box commercial face recognition algorithms (CFRAs) use gender and race features to determine identity is poorly understood despite increasing deployments by government and industry. In this study, we quantified the degree to which gender and race features influenced face recognition similarity scores between different people, i.e. non-mated scores. We ran this study using five different CFRAs and a sample of 333 diverse test subjects. As a control, we compared the behavior of these non-mated distributions to a commercial iris recognition algorithm (CIRA). Confirming prior work, all CFRAs produced higher similarity scores for people of the same gender and race, an effect known as "broad homogeneity". No such effect was observed for the CIRA. Next, we applied principal components analysis (PCA) to similarity score matrices. We show that some principal components (PCs) of CFRAs cluster people by gender and race, but the majority do not. Demographic clustering in the PCs accounted for only 10 % of the total CFRA score variance. No clustering was observed for the CIRA. This demonstrates that, although CFRAs use some gender and race features to establish identity, most features utilized by current CFRAs are unrelated to gender and race, similar to the iris texture patterns utilized by the CIRA. Finally, reconstruction of similarity score matrices using only PCs that showed no demographic clustering reduced broad homogeneity effects, but also decreased the separation between mated and non-mated scores. This suggests it's possible for CFRAs to operate on features unrelated to gender and race, albeit with somewhat lower recognition accuracy, but that this is not the current commercial practice.



### Impact of Action Unit Occurrence Patterns on Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.07982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.07982v1)
- **Published**: 2020-10-15 19:03:05+00:00
- **Updated**: 2020-10-15 19:03:05+00:00
- **Authors**: Saurabh Hinduja, Shaun Canavan, Saandeep Aathreya
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting action units is an important task in face analysis, especially in facial expression recognition. This is due, in part, to the idea that expressions can be decomposed into multiple action units. In this paper we investigate the impact of action unit occurrence patterns on detection of action units. To facilitate this investigation, we review state of the art literature, for AU detection, on 2 state-of-the-art face databases that are commonly used for this task, namely DISFA, and BP4D. Our findings, from this literature review, suggest that action unit occurrence patterns strongly impact evaluation metrics (e.g. F1-binary). Along with the literature review, we also conduct multi and single action unit detection, as well as propose a new approach to explicitly train deep neural networks using the occurrence patterns to boost the accuracy of action unit detection. These experiments validate that action unit patterns directly impact the evaluation metrics.



### What is More Likely to Happen Next? Video-and-Language Future Event Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.07999v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.07999v1)
- **Published**: 2020-10-15 19:56:47+00:00
- **Updated**: 2020-10-15 19:56:47+00:00
- **Authors**: Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal
- **Comment**: EMNLP 2020 (17 pages)
- **Journal**: None
- **Summary**: Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work. Our dataset and code are available at: https://github.com/jayleicn/VideoLanguageFuturePred



### Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2010.08001v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08001v2)
- **Published**: 2020-10-15 20:02:23+00:00
- **Updated**: 2020-12-18 03:37:02+00:00
- **Authors**: Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas
- **Comment**: Accepted to NeurIPS 2020. Code is available at
  https://github.com/garyzhao/ME-ADA
- **Journal**: None
- **Summary**: Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing "hard" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated "hard" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.



### Data Valuation for Medical Imaging Using Shapley Value: Application on A Large-scale Chest X-ray Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.08006v1
- **DOI**: 10.1038/s41598-021-87762-2
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08006v1)
- **Published**: 2020-10-15 20:18:35+00:00
- **Updated**: 2020-10-15 20:18:35+00:00
- **Authors**: Siyi Tang, Amirata Ghorbani, Rikiya Yamashita, Sameer Rehman, Jared A. Dunnmon, James Zou, Daniel L. Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: The reliability of machine learning models can be compromised when trained on low quality data. Many large-scale medical imaging datasets contain low quality labels extracted from sources such as medical reports. Moreover, images within a dataset may have heterogeneous quality due to artifacts and biases arising from equipment or measurement errors. Therefore, algorithms that can automatically identify low quality data are highly desired. In this study, we used data Shapley, a data valuation metric, to quantify the value of training data to the performance of a pneumonia detection algorithm in a large chest X-ray dataset. We characterized the effectiveness of data Shapley in identifying low quality versus valuable data for pneumonia detection. We found that removing training data with high Shapley values decreased the pneumonia detection performance, whereas removing data with low Shapley values improved the model performance. Furthermore, there were more mislabeled examples in low Shapley value data and more true pneumonia cases in high Shapley value data. Our results suggest that low Shapley value indicates mislabeled or poor quality images, whereas high Shapley value indicates data that are valuable for pneumonia detection. Our method can serve as a framework for using data Shapley to denoise large-scale medical imaging datasets.



### On the Exploration of Incremental Learning for Fine-grained Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2010.08020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08020v1)
- **Published**: 2020-10-15 21:07:44+00:00
- **Updated**: 2020-10-15 21:07:44+00:00
- **Authors**: Wei Chen, Yu Liu, Weiping Wang, Tinne Tuytelaars, Erwin M. Bakker, Michael Lew
- **Comment**: BMVC2020
- **Journal**: None
- **Summary**: In this paper, we consider the problem of fine-grained image retrieval in an incremental setting, when new categories are added over time. On the one hand, repeatedly training the representation on the extended dataset is time-consuming. On the other hand, fine-tuning the learned representation only with the new classes leads to catastrophic forgetting. To this end, we propose an incremental learning method to mitigate retrieval performance degradation caused by the forgetting issue. Without accessing any samples of the original classes, the classifier of the original network provides soft "labels" to transfer knowledge to train the adaptive network, so as to preserve the previous capability for classification. More importantly, a regularization function based on Maximum Mean Discrepancy is devised to minimize the discrepancy of new classes features from the original network and the adaptive network, respectively. Extensive experiments on two datasets show that our method effectively mitigates the catastrophic forgetting on the original classes while achieving high performance on the new classes.



### MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.08021v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.08021v1)
- **Published**: 2020-10-15 21:08:20+00:00
- **Updated**: 2020-10-15 21:08:20+00:00
- **Authors**: Aman Khullar, Udit Arora
- **Comment**: To appear in the first EMNLP Workshop on NLP Beyond Text, 2020. Aman
  Khullar and Udit Arora have equal contribution
- **Journal**: None
- **Summary**: This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities -- text, audio and video -- in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.



### QReLU and m-QReLU: Two novel quantum activation functions to aid medical diagnostics
- **Arxiv ID**: http://arxiv.org/abs/2010.08031v1
- **DOI**: 10.1016/j.eswa.2021.115892
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, 68T07, 68T10, 68T45, 68U35, I.2.1; I.2.10; I.4.9; I.5.1; I.5.4; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.08031v1)
- **Published**: 2020-10-15 21:38:36+00:00
- **Updated**: 2020-10-15 21:38:36+00:00
- **Authors**: L. Parisi, D. Neagu, R. Ma, F. Campean
- **Comment**: 30 pages, 4 listings/Python code snippets, 2 figures, 8 tables
- **Journal**: None
- **Summary**: The ReLU activation function (AF) has been extensively applied in deep neural networks, in particular Convolutional Neural Networks (CNN), for image classification despite its unresolved dying ReLU problem, which poses challenges to reliable applications. This issue has obvious important implications for critical applications, such as those in healthcare. Recent approaches are just proposing variations of the activation function within the same unresolved dying ReLU challenge. This contribution reports a different research direction by investigating the development of an innovative quantum approach to the ReLU AF that avoids the dying ReLU problem by disruptive design. The Leaky ReLU was leveraged as a baseline on which the two quantum principles of entanglement and superposition were applied to derive the proposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation functions. Both QReLU and m-QReLU are implemented and made freely available in TensorFlow and Keras. This original approach is effective and validated extensively in case studies that facilitate the detection of COVID-19 and Parkinson Disease (PD) from medical images. The two novel AFs were evaluated in a two-layered CNN against nine ReLU-based AFs on seven benchmark datasets, including images of spiral drawings taken via graphic tablets from patients with Parkinson Disease and healthy subjects, and point-of-care ultrasound images on the lungs of patients with COVID-19, those with pneumonia and healthy controls. Despite a higher computational cost, results indicated an overall higher classification accuracy, precision, recall and F1-score brought about by either quantum AFs on five of the seven bench-mark datasets, thus demonstrating its potential to be the new benchmark or gold standard AF in CNNs and aid image classification tasks involved in critical applications, such as medical diagnoses of COVID-19 and PD.



### Overfitting or Underfitting? Understand Robustness Drop in Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2010.08034v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08034v1)
- **Published**: 2020-10-15 21:43:07+00:00
- **Updated**: 2020-10-15 21:43:07+00:00
- **Authors**: Zichao Li, Liyuan Liu, Chengyu Dong, Jingbo Shang
- **Comment**: Work in Progress
- **Journal**: None
- **Summary**: Our goal is to understand why the robustness drops after conducting adversarial training for too long. Although this phenomenon is commonly explained as overfitting, our analysis suggest that its primary cause is perturbation underfitting. We observe that after training for too long, FGSM-generated perturbations deteriorate into random noise. Intuitively, since no parameter updates are made to strengthen the perturbation generator, once this process collapses, it could be trapped in such local optima. Also, sophisticating this process could mostly avoid the robustness drop, which supports that this phenomenon is caused by underfitting instead of overfitting. In the light of our analyses, we propose APART, an adaptive adversarial training framework, which parameterizes perturbation generation and progressively strengthens them. Shielding perturbations from underfitting unleashes the potential of our framework. In our experiments, APART provides comparable or even better robustness than PGD-10, with only about 1/4 of its computational cost.



### Why Layer-Wise Learning is Hard to Scale-up and a Possible Solution via Accelerated Downsampling
- **Arxiv ID**: http://arxiv.org/abs/2010.08038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08038v1)
- **Published**: 2020-10-15 21:51:43+00:00
- **Updated**: 2020-10-15 21:51:43+00:00
- **Authors**: Wenchi Ma, Miao Yu, Kaidong Li, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Layer-wise learning, as an alternative to global back-propagation, is easy to interpret, analyze, and it is memory efficient. Recent studies demonstrate that layer-wise learning can achieve state-of-the-art performance in image classification on various datasets. However, previous studies of layer-wise learning are limited to networks with simple hierarchical structures, and the performance decreases severely for deeper networks like ResNet. This paper, for the first time, reveals the fundamental reason that impedes the scale-up of layer-wise learning is due to the relatively poor separability of the feature space in shallow layers. This argument is empirically verified by controlling the intensity of the convolution operation in local layers. We discover that the poorly-separable features from shallow layers are mismatched with the strong supervision constraint throughout the entire network, making the layer-wise learning sensitive to network depth. The paper further proposes a downsampling acceleration approach to weaken the poor learning of shallow layers so as to transfer the learning emphasis to deep feature space where the separability matches better with the supervision restraint. Extensive experiments have been conducted to verify the new finding and demonstrate the advantages of the proposed downsampling acceleration in improving the performance of layer-wise learning.



### Revisiting Optical Flow Estimation in 360 Videos
- **Arxiv ID**: http://arxiv.org/abs/2010.08045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08045v1)
- **Published**: 2020-10-15 22:22:21+00:00
- **Updated**: 2020-10-15 22:22:21+00:00
- **Authors**: Keshav Bhandari, Ziliang Zong, Yan Yan
- **Comment**: 8 Pages, 7 figures, 1 Table, 5 Equations, 25th International
  Conference on Pattern Recognition Milan, Italy
- **Journal**: None
- **Summary**: Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.



### Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.08054v3
- **DOI**: 10.1109/LRA.2022.3151981
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.08054v3)
- **Published**: 2020-10-15 22:38:37+00:00
- **Updated**: 2022-02-08 00:26:18+00:00
- **Authors**: Jingpei Lu, Florian Richter, Michael Yip
- **Comment**: 8 pages, 9 figures. Accepted to IEEE Robotics and Automation Letters
  January, 2022
- **Journal**: in IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.
  4622-4629, April 2022
- **Summary**: Keypoint detection is an essential building block for many robotic applications like motion capture and pose estimation. Historically, keypoints are detected using uniquely engineered markers such as checkerboards or fiducials. More recently, deep learning methods have been explored as they have the ability to detect user-defined keypoints in a marker-less manner. However, different manually selected keypoints can have uneven performance when it comes to detection and localization. An example of this can be found on symmetric robotic tools where DNN detectors cannot solve the correspondence problem correctly. In this work, we propose a new and autonomous way to define the keypoint locations that overcomes these challenges. The approach involves finding the optimal set of keypoints on robotic manipulators for robust visual detection and localization. Using a robotic simulator as a medium, our algorithm utilizes synthetic data for DNN training, and the proposed algorithm is used to optimize the selection of keypoints through an iterative approach. The results show that when using the optimized keypoints, the detection performance of the DNNs improved significantly. We further use the optimized keypoints for real robotic applications by using domain randomization to bridge the reality gap between the simulator and the physical world. The physical world experiments show how the proposed method can be applied to the wide-breadth of robotic applications that require visual feedback, such as camera-to-robot calibration, robotic tool tracking, and end-effector pose estimation.



### Egok360: A 360 Egocentric Kinetic Human Activity Video Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.08055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.08055v1)
- **Published**: 2020-10-15 22:40:55+00:00
- **Updated**: 2020-10-15 22:40:55+00:00
- **Authors**: Keshav Bhandari, Mario A. DeLaGarza, Ziliang Zong, Hugo Latapie, Yan Yan
- **Comment**: 5 pages, 5 figures, 1 table, 2020 IEEE International Conference on
  Image Processing (ICIP)
- **Journal**: None
- **Summary**: Recently, there has been a growing interest in wearable sensors which provides new research perspectives for 360 {\deg} video analysis. However, the lack of 360 {\deg} datasets in literature hinders the research in this field. To bridge this gap, in this paper we propose a novel Egocentric (first-person) 360{\deg} Kinetic human activity video dataset (EgoK360). The EgoK360 dataset contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve. To the best of our knowledge, EgoK360 is the first dataset in the domain of first-person activity recognition with a 360{\deg} environmental setup, which will facilitate the egocentric 360 {\deg} video understanding. We provide experimental results and comprehensive analysis of variants of the two-stream network for 360 egocentric activity recognition. The EgoK360 dataset can be downloaded from https://egok360.github.io/.



### TextMage: The Automated Bangla Caption Generator Based On Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.08066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.08066v1)
- **Published**: 2020-10-15 23:24:15+00:00
- **Updated**: 2020-10-15 23:24:15+00:00
- **Authors**: Abrar Hasin Kamal, Md. Asifuzzaman Jishan, Nafees Mansoor
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Neural Networks and Deep Learning have seen an upsurge of research in the past decade due to the improved results. Generates text from the given image is a crucial task that requires the combination of both sectors which are computer vision and natural language processing in order to understand an image and represent it using a natural language. However existing works have all been done on a particular lingual domain and on the same set of data. This leads to the systems being developed to perform poorly on images that belong to specific locales' geographical context. TextMage is a system that is capable of understanding visual scenes that belong to the Bangladeshi geographical context and use its knowledge to represent what it understands in Bengali. Hence, we have trained a model on our previously developed and published dataset named BanglaLekhaImageCaptions. This dataset contains 9,154 images along with two annotations for each image. In order to access performance, the proposed model has been implemented and evaluated.



