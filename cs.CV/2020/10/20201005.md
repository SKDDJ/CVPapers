# Arxiv Papers in cs.CV on 2020-10-05
### Attention Guided Semantic Relationship Parsing for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.01725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01725v1)
- **Published**: 2020-10-05 00:23:49+00:00
- **Updated**: 2020-10-05 00:23:49+00:00
- **Authors**: Moshiur Farazi, Salman Khan, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Humans explain inter-object relationships with semantic labels that demonstrate a high-level understanding required to perform complex Vision-Language tasks such as Visual Question Answering (VQA). However, existing VQA models represent relationships as a combination of object-level visual features which constrain a model to express interactions between objects in a single domain, while the model is trying to solve a multi-modal task. In this paper, we propose a general purpose semantic relationship parser which generates a semantic feature vector for each subject-predicate-object triplet in an image, and a Mutual and Self Attention (MSA) mechanism that learns to identify relationship triplets that are important to answer the given question. To motivate the significance of semantic relationships, we show an oracle setting with ground-truth relationship triplets, where our model achieves a ~25% accuracy gain over the closest state-of-the-art model on the challenging GQA dataset. Further, with our semantic parser, we show that our model outperforms other comparable approaches on VQA and GQA datasets.



### Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch
- **Arxiv ID**: http://arxiv.org/abs/2010.01729v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.01729v5)
- **Published**: 2020-10-05 00:49:30+00:00
- **Updated**: 2021-11-10 21:23:44+00:00
- **Authors**: Youngeun Kim, Priyadarshini Panda
- **Comment**: Accepted to Frontiers in Neuroscience (2021)
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have recently emerged as an alternative to deep learning owing to sparse, asynchronous and binary event (or spike) driven processing, that can yield huge energy efficiency benefits on neuromorphic hardware. However, training high-accuracy and low-latency SNNs from scratch suffers from non-differentiable nature of a spiking neuron. To address this training issue in SNNs, we revisit batch normalization and propose a temporal Batch Normalization Through Time (BNTT) technique. Most prior SNN works till now have disregarded batch normalization deeming it ineffective for training temporal SNNs. Different from previous works, our proposed BNTT decouples the parameters in a BNTT layer along the time axis to capture the temporal dynamics of spikes. The temporally evolving learnable parameters in BNTT allow a neuron to control its spike rate through different time-steps, enabling low-latency and low-energy training from scratch. We conduct experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and event-driven DVS-CIFAR10 datasets. BNTT allows us to train deep SNN architectures from scratch, for the first time, on complex datasets with just few 25-30 time-steps. We also propose an early exit algorithm using the distribution of parameters in BNTT to reduce the latency at inference, that further improves the energy-efficiency.



### Lipschitz Bounded Equilibrium Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01732v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, eess.SY, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01732v1)
- **Published**: 2020-10-05 01:00:40+00:00
- **Updated**: 2020-10-05 01:00:40+00:00
- **Authors**: Max Revay, Ruigang Wang, Ian R. Manchester
- **Comment**: Conference submission, 19 pages
- **Journal**: None
- **Summary**: This paper introduces new parameterizations of equilibrium neural networks, i.e. networks defined by implicit equations. This model class includes standard multilayer and residual networks as special cases. The new parameterization admits a Lipschitz bound during training via unconstrained optimization: no projections or barrier functions are required. Lipschitz bounds are a common proxy for robustness and appear in many generalization bounds. Furthermore, compared to previous works we show well-posedness (existence of solutions) under less restrictive conditions on the network weights and more natural assumptions on the activation functions: that they are monotone and slope restricted. These results are proved by establishing novel connections with convex optimization, operator splitting on non-Euclidean spaces, and contracting neural ODEs. In image classification experiments we show that the Lipschitz bounds are very accurate and improve robustness to adversarial attacks.



### A Review of Vegetation Encroachment Detection in Power Transmission Lines using Optical Sensing Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.01757v1
- **DOI**: 10.30534/ijatcse/2020/8691.42020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01757v1)
- **Published**: 2020-10-05 03:24:31+00:00
- **Updated**: 2020-10-05 03:24:31+00:00
- **Authors**: Fathi Mahdi Elsiddig Haroun, Siti Noratiqah Mohamad Deros, Norashidah Md Din
- **Comment**: 7 pages, 7 figures , 2 Tables
- **Journal**: None
- **Summary**: Vegetation encroachment in power transmission lines can cause outages, which may result in severe impact on economic of power utilities companies as well as the consumer. Vegetation detection and monitoring along the power line corridor right-of-way (ROW) are implemented to protect power transmission lines from vegetation penetration. There were various methods used to monitor the vegetation penetration, however, most of them were too expensive and time consuming. Satellite images can play a major role in vegetation monitoring, because it can cover high spatial area with relatively low cost. In this paper, the current techniques used to detect the vegetation encroachment using satellite images are reviewed and categorized into four sectors; Vegetation Index based method, object-based detection method, stereo matching based and other current techniques. However, the current methods depend usually on setting manually serval threshold values and parameters which make the detection process very static. Machine Learning (ML) and deep learning (DL) algorithms can provide a very high accuracy with flexibility in the detection process. Hence, in addition to review the current technique of vegetation penetration monitoring in power transmission, the potential of using Machine Learning based algorithms are also included.



### Learning Manifold Implicitly via Explicit Heat-Kernel Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01761v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01761v3)
- **Published**: 2020-10-05 03:39:58+00:00
- **Updated**: 2021-03-15 00:12:39+00:00
- **Authors**: Yufan Zhou, Changyou Chen, Jinhui Xu
- **Comment**: Accepted by NeurIPS 2020. Some typos have been corrected
- **Journal**: None
- **Summary**: Manifold learning is a fundamental problem in machine learning with numerous applications. Most of the existing methods directly learn the low-dimensional embedding of the data in some high-dimensional space, and usually lack the flexibility of being directly applicable to down-stream applications. In this paper, we propose the concept of implicit manifold learning, where manifold information is implicitly obtained by learning the associated heat kernel. A heat kernel is the solution of the corresponding heat equation, which describes how "heat" transfers on the manifold, thus containing ample geometric information of the manifold. We provide both practical algorithm and theoretical analysis of our framework. The learned heat kernel can be applied to various kernel-based machine learning models, including deep generative models (DGM) for data generation and Stein Variational Gradient Descent for Bayesian inference. Extensive experiments show that our framework can achieve state-of-the-art results compared to existing methods for the two tasks.



### OLALA: Object-Level Active Learning for Efficient Document Layout Annotation
- **Arxiv ID**: http://arxiv.org/abs/2010.01762v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01762v3)
- **Published**: 2020-10-05 03:48:07+00:00
- **Updated**: 2021-03-29 19:32:25+00:00
- **Authors**: Zejiang Shen, Jian Zhao, Melissa Dell, Yaoliang Yu, Weining Li
- **Comment**: 12 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Document images often have intricate layout structures, with numerous content regions (e.g. texts, figures, tables) densely arranged on each page. This makes the manual annotation of layout datasets expensive and inefficient. These characteristics also challenge existing active learning methods, as image-level scoring and selection suffer from the overexposure of common objects.Inspired by recent progresses in semi-supervised learning and self-training, we propose an Object-Level Active Learning framework for efficient document layout Annotation, OLALA. In this framework, only regions with the most ambiguous object predictions within an image are selected for annotators to label, optimizing the use of the annotation budget. For unselected predictions, the semi-automatic correction algorithm is proposed to identify certain errors based on prior knowledge of layout structures and rectifies them with minor supervision. Additionally, we carefully design a perturbation-based object scoring function for document images. It governs the object selection process via evaluating prediction ambiguities, and considers both the positions and categories of predicted layout objects. Extensive experiments show that OLALA can significantly boost model performance and improve annotation efficiency, given the same labeling budget. Code for this paper can be accessed via https://github.com/lolipopshock/detectron2_al.



### MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement
- **Arxiv ID**: http://arxiv.org/abs/2010.01773v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01773v3)
- **Published**: 2020-10-05 04:41:03+00:00
- **Updated**: 2021-03-06 04:37:54+00:00
- **Authors**: Xin Liu, Ziheng Jiang, Josh Fromm, Xuhai Xu, Shwetak Patel, Daniel McDuff
- **Comment**: None
- **Journal**: None
- **Summary**: There are large individual differences in physiological processes, making designing personalized health sensing algorithms challenging. Existing machine learning systems struggle to generalize well to unseen subjects or contexts and can often contain problematic biases. Video-based physiological measurement is not an exception. Therefore, learning personalized or customized models from a small number of unlabeled samples is very attractive as it would allow fast calibrations to improve generalization and help correct biases. In this paper, we present a novel meta-learning approach called MetaPhys for personalized video-based cardiac measurement for contactless pulse and heart rate monitoring. Our method uses only 18-seconds of video for customization and works effectively in both supervised and unsupervised manners. We evaluate our proposed approach on two benchmark datasets and demonstrate superior performance in cross-dataset evaluation with substantial reductions (42% to 44%) in errors compared with state-of-the-art approaches. We have also demonstrated our proposed method significantly helps reduce the bias in skin type.



### Photon-Driven Neural Path Guiding
- **Arxiv ID**: http://arxiv.org/abs/2010.01775v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01775v1)
- **Published**: 2020-10-05 04:54:01+00:00
- **Updated**: 2020-10-05 04:54:01+00:00
- **Authors**: Shilin Zhu, Zexiang Xu, Tiancheng Sun, Alexandr Kuznetsov, Mark Meyer, Henrik Wann Jensen, Hao Su, Ravi Ramamoorthi
- **Comment**: Keywords: computer graphics, rendering, path tracing, path guiding,
  machine learning, neural networks, denoising, reconstruction
- **Journal**: None
- **Summary**: Although Monte Carlo path tracing is a simple and effective algorithm to synthesize photo-realistic images, it is often very slow to converge to noise-free results when involving complex global illumination. One of the most successful variance-reduction techniques is path guiding, which can learn better distributions for importance sampling to reduce pixel noise. However, previous methods require a large number of path samples to achieve reliable path guiding. We present a novel neural path guiding approach that can reconstruct high-quality sampling distributions for path guiding from a sparse set of samples, using an offline trained neural network. We leverage photons traced from light sources as the input for sampling density reconstruction, which is highly effective for challenging scenes with strong global illumination. To fully make use of our deep neural network, we partition the scene space into an adaptive hierarchical grid, in which we apply our network to reconstruct high-quality sampling distributions for any local region in the scene. This allows for highly efficient path guiding for any path bounce at any location in path tracing. We demonstrate that our photon-driven neural path guiding method can generalize well on diverse challenging testing scenes that are not seen in training. Our approach achieves significantly better rendering results of testing scenes than previous state-of-the-art path guiding methods.



### Can we Generalize and Distribute Private Representation Learning?
- **Arxiv ID**: http://arxiv.org/abs/2010.01792v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01792v5)
- **Published**: 2020-10-05 05:43:47+00:00
- **Updated**: 2022-01-30 21:21:18+00:00
- **Authors**: Sheikh Shams Azam, Taejin Kim, Seyyedali Hosseinalipour, Carlee Joe-Wong, Saurabh Bagchi, Christopher Brinton
- **Comment**: In Proceedings of the 25th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2022
- **Journal**: None
- **Summary**: We study the problem of learning representations that are private yet informative, i.e., provide information about intended "ally" targets while hiding sensitive "adversary" attributes. We propose Exclusion-Inclusion Generative Adversarial Network (EIGAN), a generalized private representation learning (PRL) architecture that accounts for multiple ally and adversary attributes unlike existing PRL solutions. While centrally-aggregated dataset is a prerequisite for most PRL techniques, data in real-world is often siloed across multiple distributed nodes unwilling to share the raw data because of privacy concerns. We address this practical constraint by developing D-EIGAN, the first distributed PRL method that learns representations at each node without transmitting the source data. We theoretically analyze the behavior of adversaries under the optimal EIGAN and D-EIGAN encoders and the impact of dependencies among ally and adversary tasks on the optimization objective. Our experiments on various datasets demonstrate the advantages of EIGAN in terms of performance, robustness, and scalability. In particular, EIGAN outperforms the previous state-of-the-art by a significant accuracy margin (47% improvement), and D-EIGAN's performance is consistently on par with EIGAN under different network settings.



### DCT-SNN: Using DCT to Distribute Spatial Information over Time for Learning Low-Latency Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01795v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.01795v1)
- **Published**: 2020-10-05 05:55:34+00:00
- **Updated**: 2020-10-05 05:55:34+00:00
- **Authors**: Isha Garg, Sayeed Shafayet Chowdhury, Kaushik Roy
- **Comment**: The first two authors contributed equally to this paper
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) offer a promising alternative to traditional deep learning frameworks, since they provide higher computational efficiency due to event-driven information processing. SNNs distribute the analog values of pixel intensities into binary spikes over time. However, the most widely used input coding schemes, such as Poisson based rate-coding, do not leverage the additional temporal learning capability of SNNs effectively. Moreover, these SNNs suffer from high inference latency which is a major bottleneck to their deployment. To overcome this, we propose a scalable time-based encoding scheme that utilizes the Discrete Cosine Transform (DCT) to reduce the number of timesteps required for inference. DCT decomposes an image into a weighted sum of sinusoidal basis images. At each time step, the Hadamard product of the DCT coefficients and a single frequency base, taken in order, is given to an accumulator that generates spikes upon crossing a threshold. We use the proposed scheme to learn DCT-SNN, a low-latency deep SNN with leaky-integrate-and-fire neurons, trained using surrogate gradient descent based backpropagation. We achieve top-1 accuracy of 89.94%, 68.3% and 52.43% on CIFAR-10, CIFAR-100 and TinyImageNet, respectively using VGG architectures. Notably, DCT-SNN performs inference with 2-14X reduced latency compared to other state-of-the-art SNNs, while achieving comparable accuracy to their standard deep learning counterparts. The dimension of the transform allows us to control the number of timesteps required for inference. Additionally, we can trade-off accuracy with latency in a principled manner by dropping the highest frequency components during inference.



### Long-tailed Recognition by Routing Diverse Distribution-Aware Experts
- **Arxiv ID**: http://arxiv.org/abs/2010.01809v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01809v4)
- **Published**: 2020-10-05 06:53:44+00:00
- **Updated**: 2022-05-01 18:00:20+00:00
- **Authors**: Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella X. Yu
- **Comment**: Accepted at ICLR 2021 (Spotlight); Add experiments on Swin
  Transformer
- **Journal**: None
- **Summary**: Natural data are often long-tail distributed over semantic classes. Existing recognition methods tackle this imbalanced classification by placing more emphasis on the tail data, through class re-balancing/re-weighting or ensembling over different data groups, resulting in increased tail accuracies but reduced head accuracies.   We take a dynamic view of the training data and provide a principled model bias and variance analysis as the training data fluctuates: Existing long-tail classifiers invariably increase the model variance and the head-tail model bias gap remains large, due to more and larger confusion with hard negatives for the tail.   We propose a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces the model variance with multiple experts, reduces the model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is also a universal framework that is applicable to various backbone networks, long-tailed algorithms, and training mechanisms for consistent performance gains. Our code is available at: https://github.com/frank-xwang/RIDE-LongTailRecognition.



### Painting Outside as Inside: Edge Guided Image Outpainting via Bidirectional Rearrangement with Progressive Step Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01810v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01810v2)
- **Published**: 2020-10-05 06:53:55+00:00
- **Updated**: 2020-11-09 05:18:19+00:00
- **Authors**: Kyunghun Kim, Yeohun Yun, Keon-Woo Kang, Kyeongbo Kong, Siyeong Lee, Suk-Ju Kang
- **Comment**: Paper accepted in WACV 2021
- **Journal**: None
- **Summary**: Image outpainting is a very intriguing problem as the outside of a given image can be continuously filled by considering as the context of the image. This task has two main challenges. The first is to maintain the spatial consistency in contents of generated regions and the original input. The second is to generate a high-quality large image with a small amount of adjacent information. Conventional image outpainting methods generate inconsistent, blurry, and repeated pixels. To alleviate the difficulty of an outpainting problem, we propose a novel image outpainting method using bidirectional boundary region rearrangement. We rearrange the image to benefit from the image inpainting task by reflecting more directional information. The bidirectional boundary region rearrangement enables the generation of the missing region using bidirectional information similar to that of the image inpainting task, thereby generating the higher quality than the conventional methods using unidirectional information. Moreover, we use the edge map generator that considers images as original input with structural information and hallucinates the edges of unknown regions to generate the image. Our proposed method is compared with other state-of-the-art outpainting and inpainting methods both qualitatively and quantitatively. We further compared and evaluated them using BRISQUE, one of the No-Reference image quality assessment (IQA) metrics, to evaluate the naturalness of the output. The experimental results demonstrate that our method outperforms other methods and generates new images with 360{\deg}panoramic characteristics.



### Quantifying Statistical Significance of Neural Network-based Image Segmentation by Selective Inference
- **Arxiv ID**: http://arxiv.org/abs/2010.01823v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01823v3)
- **Published**: 2020-10-05 07:16:40+00:00
- **Updated**: 2022-12-14 09:08:49+00:00
- **Authors**: Vo Nguyen Le Duy, Shogo Iwazaki, Ichiro Takeuchi
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Although a vast body of literature relates to image segmentation methods that use deep neural networks (DNNs), less attention has been paid to assessing the statistical reliability of segmentation results. In this study, we interpret the segmentation results as hypotheses driven by DNN (called DNN-driven hypotheses) and propose a method by which to quantify the reliability of these hypotheses within a statistical hypothesis testing framework. Specifically, we consider a statistical hypothesis test for the difference between the object and background regions. This problem is challenging, as the difference would be falsely large because of the adaptation of the DNN to the data. To overcome this difficulty, we introduce a conditional selective inference (SI) framework -- a new statistical inference framework for data-driven hypotheses that has recently received considerable attention -- to compute exact (non-asymptotic) valid p-values for the segmentation results. To use the conditional SI framework for DNN-based segmentation, we develop a new SI algorithm based on the homotopy method, which enables us to derive the exact (non-asymptotic) sampling distribution of DNN-driven hypothesis. We conduct experiments on both synthetic and real-world datasets, through which we offer evidence that our proposed method can successfully control the false positive rate, has good performance in terms of computational efficiency, and provides good results when applied to medical image data.



### Class-Wise Difficulty-Balanced Loss for Solving Class-Imbalance
- **Arxiv ID**: http://arxiv.org/abs/2010.01824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01824v1)
- **Published**: 2020-10-05 07:19:19+00:00
- **Updated**: 2020-10-05 07:19:19+00:00
- **Authors**: Saptarshi Sinha, Hiroki Ohashi, Katsuyuki Nakamura
- **Comment**: Accepted for ACCV 2020 oral presentation
- **Journal**: None
- **Summary**: Class-imbalance is one of the major challenges in real world datasets, where a few classes (called majority classes) constitute much more data samples than the rest (called minority classes). Learning deep neural networks using such datasets leads to performances that are typically biased towards the majority classes. Most of the prior works try to solve class-imbalance by assigning more weights to the minority classes in various manners (e.g., data re-sampling, cost-sensitive learning). However, we argue that the number of available training data may not be always a good clue to determine the weighting strategy because some of the minority classes might be sufficiently represented even by a small number of training data. Overweighting samples of such classes can lead to drop in the model's overall performance. We claim that the 'difficulty' of a class as perceived by the model is more important to determine the weighting. In this light, we propose a novel loss function named Class-wise Difficulty-Balanced loss, or CDB loss, which dynamically distributes weights to each sample according to the difficulty of the class that the sample belongs to. Note that the assigned weights dynamically change as the 'difficulty' for the model may change with the learning progress. Extensive experiments are conducted on both image (artificially induced class-imbalanced MNIST, long-tailed CIFAR and ImageNet-LT) and video (EGTEA) datasets. The results show that CDB loss consistently outperforms the recently proposed loss functions on class-imbalanced datasets irrespective of the data type (i.e., video or image).



### Depth-wise layering of 3d images using dense depth maps: a threshold based approach
- **Arxiv ID**: http://arxiv.org/abs/2010.01841v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01841v1)
- **Published**: 2020-10-05 07:55:18+00:00
- **Updated**: 2020-10-05 07:55:18+00:00
- **Authors**: Seyedsaeid Mirkamali, P. Nagabhushan
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation has long been a basic problem in computer vision. Depth-wise Layering is a kind of segmentation that slices an image in a depth-wise sequence unlike the conventional image segmentation problems dealing with surface-wise decomposition. The proposed Depth-wise Layering technique uses a single depth image of a static scene to slice it into multiple layers. The technique employs a thresholding approach to segment rows of the dense depth map into smaller partitions called Line-Segments in this paper. Then, it uses the line-segment labelling method to identify number of objects and layers of the scene independently. The final stage is to link objects of the scene to their respective object-layers. We evaluate the efficiency of the proposed technique by applying that on many images along with their dense depth maps. The experiments have shown promising results of layering.



### Characterization of surface motion patterns in highly deformable soft tissue organs from dynamic MRI: An application to assess 4D bladder motion
- **Arxiv ID**: http://arxiv.org/abs/2010.02746v3
- **DOI**: 10.1016/j.cmpb.2022.106708
- **Categories**: **cs.CV**, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02746v3)
- **Published**: 2020-10-05 08:38:08+00:00
- **Updated**: 2021-11-14 15:56:39+00:00
- **Authors**: Karim Makki, Amine Bohi, Augustin . C Ogier, Marc Emmanuel Bellemare
- **Comment**: arXiv admin note: text overlap with arXiv:2003.08332
- **Journal**: None
- **Summary**: Dynamic MRI may capture temporal anatomical changes in soft tissue organs with high contrast but the obtained sequences usually suffer from limited volume coverage which makes the high resolution reconstruction of organ shape trajectories a major challenge in temporal studies. Because of the variability of abdominal organ shapes across time and subjects, the objective of this study is to go towards 3D dense velocity measurements to fully cover the entire surface and to extract meaningful features characterizing the observed organ deformations and enabling clinical action or decision. We present a pipeline for characterization of bladder surface dynamics during deep respiratory movements. For a compact shape representation, the reconstructed temporal volumes were first used to establish subject-specific dynamical 4D mesh sequences using the LDDMM framework. Then, we performed a statistical characterization of organ dynamics from mechanical parameters such as mesh elongations and distortions. Since we refer to organs as non flat surfaces, we have also used the mean curvature changes as metric to quantify surface evolution. However, the numerical computation of curvature is strongly dependant on the surface parameterization. To cope with this dependency, we employed a new method for surface deformation analysis. Independent of parameterization and minimizing the length of the geodesic curves, it stretches smoothly the surface curves towards a sphere by minimizing a Dirichlet energy. An Eulerian PDE approach is used to derive a shape descriptor from the curve-shortening flow. Intercorrelations between individual motion patterns are computed using the Laplace Beltrami operator eigenfunctions for spherical mapping. Application to extracting characterization correlation curves for locally controlled simulated shape trajectories demonstrates the stability of the proposed shape descriptor.



### AE-Netv2: Optimization of Image Fusion Efficiency and Network Architecture
- **Arxiv ID**: http://arxiv.org/abs/2010.01863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01863v2)
- **Published**: 2020-10-05 08:58:10+00:00
- **Updated**: 2020-10-06 07:58:49+00:00
- **Authors**: Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Beibei Qin, Yanning Zhang
- **Comment**: Some mistakes have been fixed
- **Journal**: None
- **Summary**: Existing image fusion methods pay few research attention to image fusion efficiency and network architecture. However, the efficiency and accuracy of image fusion has an important impact in practical applications. To solve this problem, we propose an \textit{efficient autonomous evolution image fusion method, dubed by AE-Netv2}. Different from other image fusion methods based on deep learning, AE-Netv2 is inspired by human brain cognitive mechanism. Firstly, we discuss the influence of different network architecture on image fusion quality and fusion efficiency, which provides a reference for the design of image fusion architecture. Secondly, we explore the influence of pooling layer on image fusion task and propose an image fusion method with pooling layer. Finally, we explore the commonness and characteristics of different image fusion tasks, which provides a research basis for further research on the continuous learning characteristics of human brain in the field of image fusion. Comprehensive experiments demonstrate the superiority of AE-Netv2 compared with state-of-the-art methods in different fusion tasks at a real time speed of 100+ FPS on GTX 2070. Among all tested methods based on deep learning, AE-Netv2 has the faster speed, the smaller model size and the better robustness.



### Image Translation for Medical Image Generation -- Ischemic Stroke Lesions
- **Arxiv ID**: http://arxiv.org/abs/2010.02745v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02745v2)
- **Published**: 2020-10-05 09:12:28+00:00
- **Updated**: 2021-10-31 21:35:10+00:00
- **Authors**: Moritz Platscher, Jonathan Zopes, Christian Federau
- **Comment**: 15 pages; 9 figures; 2 tables; content matches published version
- **Journal**: None
- **Summary**: Deep learning based disease detection and segmentation algorithms promise to improve many clinical processes. However, such algorithms require vast amounts of annotated training data, which are typically not available in the medical context due to data privacy, legal obstructions, and non-uniform data acquisition protocols. Synthetic databases with annotated pathologies could provide the required amounts of training data. We demonstrate with the example of ischemic stroke that an improvement in lesion segmentation is feasible using deep learning based augmentation. To this end, we train different image-to-image translation models to synthesize magnetic resonance images of brain volumes with and without stroke lesions from semantic segmentation maps. In addition, we train a generative adversarial network to generate synthetic lesion masks. Subsequently, we combine these two components to build a large database of synthetic stroke images. The performance of the various models is evaluated using a U-Net which is trained to segment stroke lesions on a clinical test set. We report a Dice score of $\mathbf{72.8}$% [$\mathbf{70.8\pm1.0}$%] for the model with the best performance, which outperforms the model trained on the clinical images alone $\mathbf{67.3}$% [$\mathbf{63.2\pm1.9}$%], and is close to the human inter-reader Dice score of $\mathbf{76.9}$%. Moreover, we show that for a small database of only 10 or 50 clinical cases, synthetic data augmentation yields significant improvement compared to a setting where no synthetic data is used. To the best of our knowledge, this presents the first comparative analysis of synthetic data augmentation based on image-to-image translation, and first application to ischemic stroke.



### Monocular Rotational Odometry with Incremental Rotation Averaging and Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/2010.01872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01872v1)
- **Published**: 2020-10-05 09:19:06+00:00
- **Updated**: 2020-10-05 09:19:06+00:00
- **Authors**: Chee-Kheng Chng, Alvaro Parra, Tat-Jun Chin, Yasir Latif
- **Comment**: Accepted to DICTA 2020
- **Journal**: None
- **Summary**: Estimating absolute camera orientations is essential for attitude estimation tasks. An established approach is to first carry out visual odometry (VO) or visual SLAM (V-SLAM), and retrieve the camera orientations (3 DOF) from the camera poses (6 DOF) estimated by VO or V-SLAM. One drawback of this approach, besides the redundancy in estimating full 6 DOF camera poses, is the dependency on estimating a map (3D scene points) jointly with the 6 DOF poses due to the basic constraint on structure-and-motion. To simplify the task of absolute orientation estimation, we formulate the monocular rotational odometry problem and devise a fast algorithm to accurately estimate camera orientations with 2D-2D feature matches alone. Underpinning our system is a new incremental rotation averaging method for fast and constant time iterative updating. Furthermore, our system maintains a view-graph that 1) allows solving loop closure to remove camera orientation drift, and 2) can be used to warm start a V-SLAM system. We conduct extensive quantitative experiments on real-world datasets to demonstrate the accuracy of our incremental camera orientation solver. Finally, we showcase the benefit of our algorithm to V-SLAM: 1) solving the known rotation problem to estimate the trajectory of the camera and the surrounding map, and 2)enabling V-SLAM systems to track pure rotational motions.



### MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps
- **Arxiv ID**: http://arxiv.org/abs/2010.01884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01884v1)
- **Published**: 2020-10-05 09:36:47+00:00
- **Updated**: 2020-10-05 09:36:47+00:00
- **Authors**: Pascal Colling, Lutz Roese-Koerner, Hanno Gottschalk, Matthias Rottmann
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel region based active learning method for semantic image segmentation, called MetaBox+. For acquisition, we train a meta regression model to estimate the segment-wise Intersection over Union (IoU) of each predicted segment of unlabeled images. This can be understood as an estimation of segment-wise prediction quality. Queried regions are supposed to minimize to competing targets, i.e., low predicted IoU values / segmentation quality and low estimated annotation costs. For estimating the latter we propose a simple but practical method for annotation cost estimation. We compare our method to entropy based methods, where we consider the entropy as uncertainty of the prediction. The comparison and analysis of the results provide insights into annotation costs as well as robustness and variance of the methods. Numerical experiments conducted with two different networks on the Cityscapes dataset clearly demonstrate a reduction of annotation effort compared to random acquisition. Noteworthily, we achieve 95%of the mean Intersection over Union (mIoU), using MetaBox+ compared to when training with the full dataset, with only 10.47% / 32.01% annotation effort for the two networks, respectively.



### Joint Pruning & Quantization for Extremely Sparse Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.01892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01892v1)
- **Published**: 2020-10-05 10:04:29+00:00
- **Updated**: 2020-10-05 10:04:29+00:00
- **Authors**: Po-Hsiang Yu, Sih-Sian Wu, Jan P. Klopp, Liang-Gee Chen, Shao-Yi Chien
- **Comment**: 13 page, 16 figures
- **Journal**: None
- **Summary**: We investigate pruning and quantization for deep neural networks. Our goal is to achieve extremely high sparsity for quantized networks to enable implementation on low cost and low power accelerator hardware. In a practical scenario, there are particularly many applications for dense prediction tasks, hence we choose stereo depth estimation as target.   We propose a two stage pruning and quantization pipeline and introduce a Taylor Score alongside a new fine-tuning mode to achieve extreme sparsity without sacrificing performance.   Our evaluation does not only show that pruning and quantization should be investigated jointly, but also shows that almost 99% of memory demand can be cut while hardware costs can be reduced up to 99.9%. In addition, to compare with other works, we demonstrate that our pruning stage alone beats the state-of-the-art when applied to ResNet on CIFAR10 and ImageNet.



### Joint Scene and Object Tracking for Cost-Effective Augmented Reality Assisted Patient Positioning in Radiation Therapy
- **Arxiv ID**: http://arxiv.org/abs/2010.01895v2
- **DOI**: 10.1016/j.cmpb.2021.106296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01895v2)
- **Published**: 2020-10-05 10:20:46+00:00
- **Updated**: 2021-01-11 07:40:18+00:00
- **Authors**: Hamid Sarmadi, Rafael Muñoz-Salinas, M. Álvaro Berbís, Antonio Luna, R. Medina-Carnicer
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: BACKGROUND AND OBJECTIVE: The research done in the field of Augmented Reality (AR) for patient positioning in radiation therapy is scarce. We propose an efficient and cost-effective algorithm for tracking the scene and the patient to interactively assist the patient's positioning process by providing visual feedback to the operator. Up to our knowledge, this is the first framework that can be employed for mobile interactive AR to guide patient positioning. METHODS: We propose a point cloud processing method that combined with a fiducial marker-mapper algorithm and the generalized ICP algorithm tracks the patient and the camera precisely and efficiently only using the CPU unit. The alignment between the 3D reference model and body marker map is calculated employing an efficient body reconstruction algorithm. RESULTS: Our quantitative evaluation shows that the proposed method achieves a translational and rotational error of 4.17 mm/0.82 deg at 9 fps. Furthermore, the qualitative results demonstrate the usefulness of our algorithm in patient positioning on different human subjects. CONCLUSION: Since our algorithm achieves a relatively high frame rate and accuracy employing a regular laptop (without the usage of a dedicated GPU), it is a very cost-effective AR-based patient positioning method. It also opens the way for other researchers by introducing a framework that could be improved upon for better mobile interactive AR patient positioning solutions in the future.



### Best Buddies Registration for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.01912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01912v1)
- **Published**: 2020-10-05 10:49:18+00:00
- **Updated**: 2020-10-05 10:49:18+00:00
- **Authors**: Amnon Drory, Tal Shomer, Shai Avidan, Raja Giryes
- **Comment**: Accepted to ACCV 2020
- **Journal**: None
- **Summary**: We propose new, and robust, loss functions for the point cloud registration problem. Our loss functions are inspired by the Best Buddies Similarity (BBS) measure that counts the number of mutual nearest neighbors between two point sets. This measure has been shown to be robust to outliers and missing data in the case of template matching for images. We present several algorithms, collectively named Best Buddy Registration (BBR), where each algorithm consists of optimizing one of these loss functions with Adam gradient descent. The loss functions differ in several ways, including the distance function used (point-to-point vs. point-to-plane), and how the BBS measure is combined with the actual distances between pairs of points. Experiments on various data sets, both synthetic and real, demonstrate the effectiveness of the BBR algorithms, showing that they are quite robust to noise, outliers, and distractors, and cope well with extremely sparse point clouds. One variant, BBR-F, achieves state-of-the-art accuracy in the registration of automotive lidar scans taken up to several seconds apart, from the KITTI and Apollo-Southbay datasets.



### Local Label Point Correction for Edge Detection of Overlapping Cervical Cells
- **Arxiv ID**: http://arxiv.org/abs/2010.01919v3
- **DOI**: 10.3389/fninf.2022.895290
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01919v3)
- **Published**: 2020-10-05 11:01:45+00:00
- **Updated**: 2023-03-08 04:09:23+00:00
- **Authors**: Jiawei Liu, Huijie Fan, Qiang Wang, Wentao Li, Yandong Tang, Danbo Wang, Mingyi Zhou, Li Chen
- **Comment**: Published on Frontiers in Neuroinformatics. Official paper:
  https://www.frontiersin.org/articles/10.3389/fninf.2022.895290/full. Code and
  dataset: https://github.com/nachifur/LLPC
- **Journal**: None
- **Summary**: Accurate labeling is essential for supervised deep learning methods. However, it is almost impossible to accurately and manually annotate thousands of images, which results in many labeling errors for most datasets. We proposes a local label point correction (LLPC) method to improve annotation quality for edge detection and image segmentation tasks. Our algorithm contains three steps: gradient-guided point correction, point interpolation and local point smoothing. We correct the labels of object contours by moving the annotated points to the pixel gradient peaks. This can improve the edge localization accuracy, but it also causes unsmooth contours due to the interference of image noise. Therefore, we design a point smoothing method based on local linear fitting to smooth the corrected edge. To verify the effectiveness of our LLPC, we construct a largest overlapping cervical cell edge detection dataset (CCEDD) with higher precision label corrected by our label correction method. Our LLPC only needs to set three parameters, but yields 30-40$\%$ average precision improvement on multiple networks. The qualitative and quantitative experimental results show that our LLPC can improve the quality of manual labels and the accuracy of overlapping cell edge detection. We hope that our study will give a strong boost to the development of the label correction for edge detection and image segmentation. We will release the dataset and code at https://github.com/nachifur/LLPC.



### Test-time Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.01926v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01926v1)
- **Published**: 2020-10-05 11:30:36+00:00
- **Updated**: 2020-10-05 11:30:36+00:00
- **Authors**: Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H. Sudre, Mark S. Graham, Parashkev Nachev, M. Jorge Cardoso
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Convolutional neural networks trained on publicly available medical imaging datasets (source domain) rarely generalise to different scanners or acquisition protocols (target domain). This motivates the active field of domain adaptation. While some approaches to the problem require labeled data from the target domain, others adopt an unsupervised approach to domain adaptation (UDA). Evaluating UDA methods consists of measuring the model's ability to generalise to unseen data in the target domain. In this work, we argue that this is not as useful as adapting to the test set directly. We therefore propose an evaluation framework where we perform test-time UDA on each subject separately. We show that models adapted to a specific target subject from the target domain outperform a domain adaptation method which has seen more data of the target domain but not this specific target subject. This result supports the thesis that unsupervised domain adaptation should be used at test-time, even if only using a single target-domain subject



### EqCo: Equivalent Rules for Self-supervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01929v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01929v4)
- **Published**: 2020-10-05 11:39:04+00:00
- **Updated**: 2023-02-26 17:43:27+00:00
- **Authors**: Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose EqCo (Equivalent Rules for Contrastive Learning) to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can use only a few negative pairs (e.g., 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCov2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples is not a critical factor in contrastive learning frameworks.



### Unsupervised Region-based Anomaly Detection in Brain MRI with Adversarial Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2010.01942v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.5.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.01942v1)
- **Published**: 2020-10-05 12:13:44+00:00
- **Updated**: 2020-10-05 12:13:44+00:00
- **Authors**: Bao Nguyen, Adam Feldman, Sarath Bethapudi, Andrew Jennings, Chris G. Willcocks
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Medical segmentation is performed to determine the bounds of regions of interest (ROI) prior to surgery. By allowing the study of growth, structure, and behaviour of the ROI in the planning phase, critical information can be obtained, increasing the likelihood of a successful operation. Usually, segmentations are performed manually or via machine learning methods trained on manual annotations. In contrast, this paper proposes a fully automatic, unsupervised inpainting-based brain tumour segmentation system for T1-weighted MRI. First, a deep convolutional neural network (DCNN) is trained to reconstruct missing healthy brain regions. Then, upon application, anomalous regions are determined by identifying areas of highest reconstruction loss. Finally, superpixel segmentation is performed to segment those regions. We show the proposed system is able to segment various sized and abstract tumours and achieves a mean and standard deviation Dice score of 0.771 and 0.176, respectively.



### A Comparative Study of Existing and New Deep Learning Methods for Detecting Knee Injuries using the MRNet Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.01947v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01947v1)
- **Published**: 2020-10-05 12:27:18+00:00
- **Updated**: 2020-10-05 12:27:18+00:00
- **Authors**: David Azcona, Kevin McGuinness, Alan F. Smeaton
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a comparative study of existing and new techniques to detect knee injuries by leveraging Stanford's MRNet Dataset. All approaches are based on deep learning and we explore the comparative performances of transfer learning and a deep residual network trained from scratch. We also exploit some characteristics of Magnetic Resonance Imaging (MRI) data by, for example, using a fixed number of slices or 2D images from each of the axial, coronal and sagittal planes as well as combining the three planes into one multi-plane network. Overall we achieved a performance of 93.4% AUC on the validation data by using the more recent deep learning architectures and data augmentation strategies. More flexible architectures are also proposed that might help with the development and training of models that process MRIs. We found that transfer learning and a carefully tuned data augmentation strategy were the crucial factors in determining best performance.



### A Novel Actor Dual-Critic Model for Remote Sensing Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2010.01999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01999v1)
- **Published**: 2020-10-05 13:35:02+00:00
- **Updated**: 2020-10-05 13:35:02+00:00
- **Authors**: Ruchika Chavhan, Biplab Banerjee, Xiao Xiang Zhu, Subhasis Chaudhuri
- **Comment**: 8 pages, 21 figures Accepted at the International Conference on
  Pattern Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: We deal with the problem of generating textual captions from optical remote sensing (RS) images using the notion of deep reinforcement learning. Due to the high inter-class similarity in reference sentences describing remote sensing data, jointly encoding the sentences and images encourages prediction of captions that are semantically more precise than the ground truth in many cases. To this end, we introduce an Actor Dual-Critic training strategy where a second critic model is deployed in the form of an encoder-decoder RNN to encode the latent information corresponding to the original and generated captions. While all actor-critic methods use an actor to predict sentences for an image and a critic to provide rewards, our proposed encoder-decoder RNN guarantees high-level comprehension of images by sentence-to-image translation. We observe that the proposed model generates sentences on the test data highly similar to the ground truth and is successful in generating even better captions in many critical cases. Extensive experiments on the benchmark Remote Sensing Image Captioning Dataset (RSICD) and the UCM-captions dataset confirm the superiority of the proposed approach in comparison to the previous state-of-the-art where we obtain a gain of sharp increments in both the ROUGE-L and CIDEr measures.



### Adversarial Boot Camp: label free certified robustness in one epoch
- **Arxiv ID**: http://arxiv.org/abs/2010.02508v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.02508v1)
- **Published**: 2020-10-05 13:47:45+00:00
- **Updated**: 2020-10-05 13:47:45+00:00
- **Authors**: Ryan Campbell, Chris Finlay, Adam M Oberman
- **Comment**: 13 pages, 5 figures, 5 tables. Under review as a conference paper at
  ICLR 2021. arXiv admin note: substantial text overlap with arXiv:2006.06061
- **Journal**: None
- **Summary**: Machine learning models are vulnerable to adversarial attacks. One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size. A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given input. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.



### BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.02036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02036v2)
- **Published**: 2020-10-05 14:16:41+00:00
- **Updated**: 2021-06-05 14:24:50+00:00
- **Authors**: Or Patashnik, Dov Danon, Hao Zhang, Daniel Cohen-Or
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops 2021
- **Summary**: State-of-the-art image-to-image translation methods tend to struggle in an imbalanced domain setting, where one image domain lacks richness and diversity. We introduce a new unsupervised translation network, BalaGAN, specifically designed to tackle the domain imbalance problem. We leverage the latent modalities of the richer domain to turn the image-to-image translation problem, between two imbalanced domains, into a balanced, multi-class, and conditional translation problem, more resembling the style transfer setting. Specifically, we analyze the source domain and learn a decomposition of it into a set of latent modes or classes, without any supervision. This leaves us with a multitude of balanced cross-domain translation tasks, between all pairs of classes, including the target domain. During inference, the trained network takes as input a source image, as well as a reference or style image from one of the modes as a condition, and produces an image which resembles the source on the pixel-wise level, but shares the same mode as the reference. We show that employing modalities within the dataset improves the quality of the translated images, and that BalaGAN outperforms strong baselines of both unconditioned and style-transfer-based image-to-image translation methods, in terms of image quality and diversity.



### Probabilistic 3D surface reconstruction from sparse MRI information
- **Arxiv ID**: http://arxiv.org/abs/2010.02041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02041v1)
- **Published**: 2020-10-05 14:18:52+00:00
- **Updated**: 2020-10-05 14:18:52+00:00
- **Authors**: Katarína Tóthová, Sarah Parisot, Matthew Lee, Esther Puyol-Antón, Andrew King, Marc Pollefeys, Ender Konukoglu
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Surface reconstruction from magnetic resonance (MR) imaging data is indispensable in medical image analysis and clinical research. A reliable and effective reconstruction tool should: be fast in prediction of accurate well localised and high resolution models, evaluate prediction uncertainty, work with as little input data as possible. Current deep learning state of the art (SOTA) 3D reconstruction methods, however, often only produce shapes of limited variability positioned in a canonical position or lack uncertainty evaluation. In this paper, we present a novel probabilistic deep learning approach for concurrent 3D surface reconstruction from sparse 2D MR image data and aleatoric uncertainty prediction. Our method is capable of reconstructing large surface meshes from three quasi-orthogonal MR imaging slices from limited training sets whilst modelling the location of each mesh vertex through a Gaussian distribution. Prior shape information is encoded using a built-in linear principal component analysis (PCA) model. Extensive experiments on cardiac MR data show that our probabilistic approach successfully assesses prediction uncertainty while at the same time qualitatively and quantitatively outperforms SOTA methods in shape prediction. Compared to SOTA, we are capable of properly localising and orientating the prediction via the use of a spatially aware neural network.



### Non-anchor-based vehicle detection for traffic surveillance using bounding ellipses
- **Arxiv ID**: http://arxiv.org/abs/2010.02059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02059v2)
- **Published**: 2020-10-05 14:48:18+00:00
- **Updated**: 2021-03-05 04:05:41+00:00
- **Authors**: Byeonghyeop Yu, Johyun Shin, Gyeongjun Kim, Seungbin Roh, Keemin Sohn
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Cameras for traffic surveillance are usually pole-mounted and produce images that reflect a birds-eye view. Vehicles in such images, in general, assume an ellipse form. A bounding box for the vehicles usually includes a large empty space when the vehicle orientation is not parallel to the edges of the box. To circumvent this problem, the present study applied bounding ellipses to a non-anchor-based, single-shot detection model (CenterNet). Since this model does not depend on anchor boxes, non-max suppression (NMS) that requires computing the intersection over union (IOU) between predicted bounding boxes is unnecessary for inference. The SpotNet that extends the CenterNet model by adding a segmentation head was also tested with bounding ellipses. Two other anchor-based, single-shot detection models (YOLO4 and SSD) were chosen as references for comparison. The model performance was compared based on a local dataset that was doubly annotated with bounding boxes and ellipses. As a result, the performance of the two models with bounding ellipses exceeded that of the reference models with bounding boxes. When the backbone of the ellipse models was pretrained on an open dataset (UA-DETRAC), the performance was further enhanced. The data augmentation schemes developed for YOLO4 also improved the performance of the proposed models. As a result, the best mAP score of a CenterNet with bounding ellipses exceeds 0.9.



### Invertible DenseNets
- **Arxiv ID**: http://arxiv.org/abs/2010.02125v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.02125v3)
- **Published**: 2020-10-05 16:11:39+00:00
- **Updated**: 2021-01-08 18:33:17+00:00
- **Authors**: Yura Perugachi-Diaz, Jakub M. Tomczak, Sandjai Bhulai
- **Comment**: Accepted at 3rd Symposium on Advances in Approximate Bayesian
  Inference (AABI)
- **Journal**: None
- **Summary**: We introduce Invertible Dense Networks (i-DenseNets), a more parameter efficient alternative to Residual Flows. The method relies on an analysis of the Lipschitz continuity of the concatenation in DenseNets, where we enforce the invertibility of the network by satisfying the Lipschitz constraint. Additionally, we extend this method by proposing a learnable concatenation, which not only improves the model performance but also indicates the importance of the concatenated representation. We demonstrate the performance of i-DenseNets and Residual Flows on toy, MNIST, and CIFAR10 data. Both i-DenseNets outperform Residual Flows evaluated in negative log-likelihood, on all considered datasets under an equal parameter budget.



### Ego-Motion Alignment from Face Detections for Collaborative Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2010.02153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02153v1)
- **Published**: 2020-10-05 16:57:48+00:00
- **Updated**: 2020-10-05 16:57:48+00:00
- **Authors**: Branislav Micusik, Georgios Evangelidis
- **Comment**: None
- **Journal**: None
- **Summary**: Sharing virtual content among multiple smart glasses wearers is an essential feature of a seamless Collaborative Augmented Reality experience. To enable the sharing, local coordinate systems of the underlying 6D ego-pose trackers, running independently on each set of glasses, have to be spatially and temporally aligned with respect to each other. In this paper, we propose a novel lightweight solution for this problem, which is referred as ego-motion alignment. We show that detecting each other's face or glasses together with tracker ego-poses sufficiently conditions the problem to spatially relate local coordinate systems. Importantly, the detected glasses can serve as reliable anchors to bring sufficient accuracy for the targeted practical use. The proposed idea allows us to abandon the traditional visual localization step with fiducial markers or scene points as anchors. A novel closed form minimal solver which solves a Quadratic Eigenvalue Problem is derived and its refinement with Gaussian Belief Propagation is introduced. Experiments validate the presented approach and show its high practical potential.



### Generative Model-Enhanced Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.11699v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11699v3)
- **Published**: 2020-10-05 17:04:34+00:00
- **Updated**: 2020-11-25 10:16:28+00:00
- **Authors**: Anthony Bourached, Ryan-Rhys Griffiths, Robert Gray, Ashwani Jha, Parashkev Nachev
- **Comment**: 8 pages + 5 pages supplementary materials, under review at ICLR
- **Journal**: None
- **Summary**: The task of predicting human motion is complicated by the natural heterogeneity and compositionality of actions, necessitating robustness to distributional shifts as far as out-of-distribution (OoD). Here we formulate a new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and introduce a hybrid framework for hardening discriminative architectures to OoD failure by augmenting them with a generative model. When applied to current state-of-the-art discriminative models, we show that the proposed approach improves OoD robustness without sacrificing in-distribution performance, and can theoretically facilitate model interpretability. We suggest human motion predictors ought to be constructed with OoD challenges in mind, and provide an extensible general framework for hardening diverse discriminative architectures to extreme distributional shift. The code is available at https://github.com/bouracha/OoDMotion.



### Mind the Pad -- CNNs can Develop Blind Spots
- **Arxiv ID**: http://arxiv.org/abs/2010.02178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.02178v1)
- **Published**: 2020-10-05 17:24:48+00:00
- **Updated**: 2020-10-05 17:24:48+00:00
- **Authors**: Bilal Alsallakh, Narine Kokhlikyan, Vivek Miglani, Jun Yuan, Orion Reblitz-Richardson
- **Comment**: Appendix E available at
  https://drive.google.com/file/d/1bIvRQJIBwJbKTfpg0hNaFX2ThuuDO8PU/view?usp=sharing
- **Journal**: None
- **Summary**: We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We propose solutions to mitigate spatial bias and demonstrate how they can improve model accuracy.



### CO2: Consistent Contrast for Unsupervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.02217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02217v1)
- **Published**: 2020-10-05 18:00:01+00:00
- **Updated**: 2020-10-05 18:00:01+00:00
- **Authors**: Chen Wei, Huiyu Wang, Wei Shen, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has been adopted as a core method for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, this task labels crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment strategy is that it can not reflect the heterogeneous similarity between the query crop and each crop from other images, taking them as equally negative, while some of them may even belong to the same semantic class as the query. To address this issue, inspired by consistency regularization in semi-supervised learning on unlabeled data, we propose Consistent Contrast (CO2), which introduces a consistency regularization term into the current contrastive learning framework. Regarding the similarity of the query crop to each crop from other images as "unlabeled", the consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for these downstream tasks.



### Smoother Network Tuning and Interpolation for Continuous-level Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2010.02270v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02270v1)
- **Published**: 2020-10-05 18:29:52+00:00
- **Updated**: 2020-10-05 18:29:52+00:00
- **Authors**: Hyeongmin Lee, Taeoh Kim, Hanbin Son, Sangwook Baek, Minsu Cheon, Sangyoun Lee
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.05145
- **Journal**: None
- **Summary**: In Convolutional Neural Network (CNN) based image processing, most studies propose networks that are optimized to single-level (or single-objective); thus, they underperform on other levels and must be retrained for delivery of optimal performance. Using multiple models to cover multiple levels involves very high computational costs. To solve these problems, recent approaches train networks on two different levels and propose their own interpolation methods to enable arbitrary intermediate levels. However, many of them fail to generalize or have certain side effects in practical usage. In this paper, we define these frameworks as network tuning and interpolation and propose a novel module for continuous-level learning, called Filter Transition Network (FTN). This module is a structurally smoother module than existing ones. Therefore, the frameworks with FTN generalize well across various tasks and networks and cause fewer undesirable side effects. For stable learning of FTN, we additionally propose a method to initialize non-linear neural network layers with identity mappings. Extensive results for various image processing tasks indicate that the performance of FTN is comparable in multiple continuous levels, and is significantly smoother and lighter than that of other frameworks.



### Early Detection of Myocardial Infarction in Low-Quality Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2010.02281v2
- **DOI**: 10.1109/ACCESS.2021.3059595
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02281v2)
- **Published**: 2020-10-05 18:47:04+00:00
- **Updated**: 2021-03-15 10:01:30+00:00
- **Authors**: Aysen Degerli, Morteza Zabihi, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Ridha Hamila, Moncef Gabbouj
- **Comment**: None
- **Journal**: IEEE Access (2021)
- **Summary**: Myocardial infarction (MI), or commonly known as heart attack, is a life-threatening health problem worldwide from which 32.4 million people suffer each year. Early diagnosis and treatment of MI are crucial to prevent further heart tissue damages or death. The earliest and most reliable sign of ischemia is regional wall motion abnormality (RWMA) of the affected part of the ventricular muscle. Echocardiography can easily, inexpensively, and non-invasively exhibit the RWMA. In this article, we introduce a three-phase approach for early MI detection in low-quality echocardiography: 1) segmentation of the entire left ventricle (LV) wall using a state-of-the-art deep learning model, 2) analysis of the segmented LV wall by feature engineering, and 3) early MI detection. The main contributions of this study are highly accurate segmentation of the LV wall from low-quality echocardiography, pseudo labeling approach for ground-truth formation of the unannotated LV wall, and the first public echocardiographic dataset (HMC-QU)* for MI detection. Furthermore, the outputs of the proposed approach can significantly help cardiologists for a better assessment of the LV wall characteristics. The proposed approach has achieved 95.72% sensitivity and 99.58% specificity for the LV wall segmentation, and 85.97% sensitivity, 74.03% specificity, and 86.85% precision for MI detection on the HMC-QU dataset. *The benchmark HMC-QU dataset is publicly shared at the repository https://www.kaggle.com/aysendegerli/hmcqu-dataset



### A Study on Trees's Knots Prediction from their Bark Outer-Shape
- **Arxiv ID**: http://arxiv.org/abs/2010.03173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03173v1)
- **Published**: 2020-10-05 19:13:10+00:00
- **Updated**: 2020-10-05 19:13:10+00:00
- **Authors**: Mejri Mohamed, Antoine Richard, Cedric Pradalier
- **Comment**: arXiv admin note: text overlap with arXiv:2002.04571
- **Journal**: None
- **Summary**: In the industry, the value of wood-logs strongly depends on their internal structure and more specifically on the knots' distribution inside the trees. As of today, CT-scanners are the prevalent tool to acquire accurate images of the trees internal structure. However, CT-scanners are expensive, and slow, making their use impractical for most industrial applications. Knowing where the knots are within a tree could improve the efficiency of the overall tree industry by reducing waste and improving the quality of wood-logs by-products. In this paper we evaluate different deep-learning based architectures to predict the internal knots distribution of a tree from its outer-shape, something that has never been done before. Three types of techniques based on Convolutional Neural Networks (CNN) will be studied.   The architectures are tested on both real and synthetic CT-scanned trees. With these experiments, we demonstrate that CNNs can be used to predict internal knots distribution based on the external surface of the trees. The goal being to show that these inexpensive and fast methods could be used to replace the CT-scanners.   Additionally, we look into the performance of several off-the-shelf object-detectors to detect knots inside CT-scanned images. This method is used to autonomously label part of our real CT-scanned trees alleviating the need to manually segment the whole of the images.



### SMILE: Semantically-guided Multi-attribute Image and Layout Editing
- **Arxiv ID**: http://arxiv.org/abs/2010.02315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02315v1)
- **Published**: 2020-10-05 20:15:21+00:00
- **Updated**: 2020-10-05 20:15:21+00:00
- **Authors**: Andrés Romero, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Attribute image manipulation has been a very active topic since the introduction of Generative Adversarial Networks (GANs). Exploring the disentangled attribute space within a transformation is a very challenging task due to the multiple and mutually-inclusive nature of the facial images, where different labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the same time. Several works address this issue either by exploiting the modality of each domain/attribute using a conditional random vector noise, or extracting the modality from an exemplary image. However, existing methods cannot handle both random and reference transformations for multiple attributes, which limits the generality of the solutions. In this paper, we successfully exploit a multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain. We present extensive qualitative and quantitative results for facial datasets and several different attributes that show the superiority of our method. Additionally, our method is capable of adding, removing or changing either fine-grained or coarse attributes by using an image as a reference or by exploring the style distribution space, and it can be easily extended to head-swapping and face-reenactment applications without being trained on videos.



### Tensor Fields for Data Extraction from Chart Images: Bar Charts and Scatter Plots
- **Arxiv ID**: http://arxiv.org/abs/2010.02319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.NA, math.NA, 68U10, 68U05
- **Links**: [PDF](http://arxiv.org/pdf/2010.02319v1)
- **Published**: 2020-10-05 20:19:40+00:00
- **Updated**: 2020-10-05 20:19:40+00:00
- **Authors**: Jaya Sreevalsan-Nair, Komal Dadhich, Siri Chandana Daggubati
- **Comment**: 17 pages, 7 figures, 1 table, peer-reviewed and accepted for
  publication in "Topological Methods in Visualization: Theory, Software and
  Applications," Ingrid Hotz, Talha Bin Masood, Filip Sadlo, and Julien Tierny
  (Eds.). Springer-Verlag
- **Journal**: None
- **Summary**: Charts are an essential part of both graphicacy (graphical literacy), and statistical literacy. As chart understanding has become increasingly relevant in data science, automating chart analysis by processing raster images of the charts has become a significant problem. Automated chart reading involves data extraction and contextual understanding of the data from chart images. In this paper, we perform the first step of determining the computational model of chart images for data extraction for selected chart types, namely, bar charts, and scatter plots. We demonstrate the use of positive semidefinite second-order tensor fields as an effective model. We identify an appropriate tensor field as the model and propose a methodology for the use of its degenerate point extraction for data extraction from chart images. Our results show that tensor voting is effective for data extraction from bar charts and scatter plots, and histograms, as a special case of bar charts.



### Exploring the Interchangeability of CNN Embedding Spaces
- **Arxiv ID**: http://arxiv.org/abs/2010.02323v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02323v4)
- **Published**: 2020-10-05 20:32:40+00:00
- **Updated**: 2021-02-12 01:59:35+00:00
- **Authors**: David McNeely-White, Benjamin Sattelberg, Nathaniel Blanchard, Ross Beveridge
- **Comment**: Preprint, 11 pages, 2 figures, 6 tables
- **Journal**: None
- **Summary**: CNN feature spaces can be linearly mapped and consequently are often interchangeable. This equivalence holds across variations in architectures, training datasets, and network tasks. Specifically, we mapped between 10 image-classification CNNs and between 4 facial-recognition CNNs. When image embeddings generated by one CNN are transformed into embeddings corresponding to the feature space of a second CNN trained on the same task, their respective image classification or face verification performance is largely preserved. For CNNs trained to the same classes and sharing a common backend-logit (soft-max) architecture, a linear-mapping may always be calculated directly from the backend layer weights. However, the case of a closed-set analysis with perfect knowledge of classifiers is limiting. Therefore, empirical methods of estimating mappings are presented for both the closed-set image classification task and the open-set task of face recognition. The results presented expose the essentially interchangeable nature of CNNs embeddings for two important and common recognition tasks. The implications are far-reaching, suggesting an underlying commonality between representations learned by networks designed and trained for a common task. One practical implication is that face embeddings from some commonly used CNNs can be compared using these mappings.



### Understanding bias in facial recognition technologies
- **Arxiv ID**: http://arxiv.org/abs/2010.07023v1
- **DOI**: 10.5281/zenodo.4050457
- **Categories**: **cs.CY**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2010.07023v1)
- **Published**: 2020-10-05 20:45:46+00:00
- **Updated**: 2020-10-05 20:45:46+00:00
- **Authors**: David Leslie
- **Comment**: 49 pages
- **Journal**: None
- **Summary**: Over the past couple of years, the growing debate around automated facial recognition has reached a boiling point. As developers have continued to swiftly expand the scope of these kinds of technologies into an almost unbounded range of applications, an increasingly strident chorus of critical voices has sounded concerns about the injurious effects of the proliferation of such systems. Opponents argue that the irresponsible design and use of facial detection and recognition technologies (FDRTs) threatens to violate civil liberties, infringe on basic human rights and further entrench structural racism and systemic marginalisation. They also caution that the gradual creep of face surveillance infrastructures into every domain of lived experience may eventually eradicate the modern democratic forms of life that have long provided cherished means to individual flourishing, social solidarity and human self-creation. Defenders, by contrast, emphasise the gains in public safety, security and efficiency that digitally streamlined capacities for facial identification, identity verification and trait characterisation may bring. In this explainer, I focus on one central aspect of this debate: the role that dynamics of bias and discrimination play in the development and deployment of FDRTs. I examine how historical patterns of discrimination have made inroads into the design and implementation of FDRTs from their very earliest moments. And, I explain the ways in which the use of biased FDRTs can lead distributional and recognitional injustices. The explainer concludes with an exploration of broader ethical questions around the potential proliferation of pervasive face-based surveillance infrastructures and makes some recommendations for cultivating more responsible approaches to the development and governance of these technologies.



### A Benchmark and Baseline for Language-Driven Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2010.02330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02330v1)
- **Published**: 2020-10-05 20:51:16+00:00
- **Updated**: 2020-10-05 20:51:16+00:00
- **Authors**: Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, Chenliang Xu
- **Comment**: Accepted by ACCV 2020
- **Journal**: None
- **Summary**: Language-driven image editing can significantly save the laborious image editing work and be friendly to the photography novice. However, most similar work can only deal with a specific image domain or can only do global retouching. To solve this new task, we first present a new language-driven image editing dataset that supports both local and global editing with editing operation and mask annotations. Besides, we also propose a baseline method that fully utilizes the annotation to solve this problem. Our new method treats each editing operation as a sub-module and can automatically predict operation parameters. Not only performing well on challenging user data, but such an approach is also highly interpretable. We believe our work, including both the benchmark and the baseline, will advance the image editing area towards a more general and free-form level.



### Multi-level Feature Learning on Embedding Layer of Convolutional Autoencoders and Deep Inverse Feature Learning for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2010.02343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02343v1)
- **Published**: 2020-10-05 21:24:10+00:00
- **Updated**: 2020-10-05 21:24:10+00:00
- **Authors**: Behzad Ghazanfari, Fatemeh Afghah
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces Multi-Level feature learning alongside the Embedding layer of Convolutional Autoencoder (CAE-MLE) as a novel approach in deep clustering. We use agglomerative clustering as the multi-level feature learning that provides a hierarchical structure on the latent feature space. It is shown that applying multi-level feature learning considerably improves the basic deep convolutional embedding clustering (DCEC). CAE-MLE considers the clustering loss of agglomerative clustering simultaneously alongside the learning latent feature of CAE. In the following of the previous works in inverse feature learning, we show that the representation of learning of error as a general strategy can be applied on different deep clustering approaches and it leads to promising results. We develop deep inverse feature learning (deep IFL) on CAE-MLE as a novel approach that leads to the state-of-the-art results among the same category methods. The experimental results show that the CAE-MLE improves the results of the basic method, DCEC, around 7% -14% on two well-known datasets of MNIST and USPS. Also, it is shown that the proposed deep IFL improves the primary results about 9%-17%. Therefore, both proposed approaches of CAE-MLE and deep IFL based on CAE-MLE can lead to notable performance improvement in comparison to the majority of existing techniques. The proposed approaches while are based on a basic convolutional autoencoder lead to outstanding results even in comparison to variational autoencoders or generative adversarial networks.



### Deep Generative Modelling of Human Reach-and-Place Action
- **Arxiv ID**: http://arxiv.org/abs/2010.02345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02345v1)
- **Published**: 2020-10-05 21:36:20+00:00
- **Updated**: 2020-10-05 21:36:20+00:00
- **Authors**: Connor Daly, Yuzuko Nakamura, Tobias Ritschel
- **Comment**: Accompanying video found here: https://youtu.be/o1SICcMZiiU 9 pages
- **Journal**: None
- **Summary**: The motion of picking up and placing an object in 3D space is full of subtle detail. Typically these motions are formed from the same constraints, optimizing for swiftness, energy efficiency, as well as physiological limits. Yet, even for identical goals, the motion realized is always subject to natural variation. To capture these aspects computationally, we suggest a deep generative model for human reach-and-place action, conditioned on a start and end position.We have captured a dataset of 600 such human 3D actions, to sample the 2x3-D space of 3D source and targets. While temporal variation is often modeled with complex learning machinery like recurrent neural networks or networks with memory or attention, we here demonstrate a much simpler approach that is convolutional in time and makes use of(periodic) temporal encoding. Provided a latent code and conditioned on start and end position, the model generates a complete 3D character motion in linear time as a sequence of convolutions. Our evaluation includes several ablations, analysis of generative diversity and applications.



### Winning Lottery Tickets in Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2010.02350v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02350v2)
- **Published**: 2020-10-05 21:45:39+00:00
- **Updated**: 2021-01-29 18:44:21+00:00
- **Authors**: Neha Mukund Kalibhat, Yogesh Balaji, Soheil Feizi
- **Comment**: Published at AAAI 2021
- **Journal**: None
- **Summary**: The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding "winning tickets" in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late rewinding) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called "early-bird tickets". Through early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models.



### VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach
- **Arxiv ID**: http://arxiv.org/abs/2010.02358v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02358v5)
- **Published**: 2020-10-05 21:58:19+00:00
- **Updated**: 2021-07-04 21:25:52+00:00
- **Authors**: Mohamed Kerroumi, Othmane Sayem, Aymen Shabou
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel approach for scanned document representation to perform field extraction. It allows the simultaneous encoding of the textual, visual and layout information in a 3-axis tensor used as an input to a segmentation model. We improve the recent Chargrid and Wordgrid \cite{chargrid} models in several ways, first by taking into account the visual modality, then by boosting its robustness in regards to small datasets while keeping the inference time low. Our approach is tested on public and private document-image datasets, showing higher performances compared to the recent state-of-the-art methods.



### Automotive Radar Data Acquisition using Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.02367v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02367v2)
- **Published**: 2020-10-05 22:22:08+00:00
- **Updated**: 2021-03-01 18:51:22+00:00
- **Authors**: Madhumitha Sakthi, Ahmed Tewfik
- **Comment**: Submitted to EUSIPCO 2021
- **Journal**: None
- **Summary**: The growing urban complexity demands an efficient algorithm to acquire and process various sensor information from autonomous vehicles. In this paper, we introduce an algorithm to utilize object detection results from the image to adaptively sample and acquire radar data using Compressed Sensing (CS). This novel algorithm is motivated by the hypothesis that with a limited sampling budget, allocating more sampling budget to areas with the object as opposed to a uniform sampling ultimately improves relevant object detection performance. We improve detection performance by dynamically allocating a lower sampling rate to objects such as buses than pedestrians leading to better reconstruction than baseline across areas with objects of interest. We automate the sampling rate allocation using linear programming and show significant time savings while reducing the radar block size by a factor of 2. We also analyze a Binary Permuted Diagonal measurement matrix for radar acquisition which is hardware-efficient and show its performance is similar to Gaussian and Binary Permuted Block Diagonal matrix. Our experiments on the Oxford radar dataset show an effective reconstruction of objects of interest with 10% sampling rate. Finally, we develop a transformer-based 2D object detection network using the NuScenes radar and image data.



### Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD Construction from Human Design Sequences
- **Arxiv ID**: http://arxiv.org/abs/2010.02392v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.02392v2)
- **Published**: 2020-10-05 23:18:21+00:00
- **Updated**: 2021-05-17 03:58:04+00:00
- **Authors**: Karl D. D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G. Lambourne, Armando Solar-Lezama, Wojciech Matusik
- **Comment**: Accepted to SIGGRAPH 2021; data/code available at
  https://github.com/AutodeskAILab/Fusion360GalleryDataset
- **Journal**: None
- **Summary**: Parametric computer-aided design (CAD) is a standard paradigm used to design manufactured objects, where a 3D shape is represented as a program supported by the CAD software. Despite the pervasiveness of parametric CAD and a growing interest from the research community, currently there does not exist a dataset of realistic CAD models in a concise programmatic form. In this paper we present the Fusion 360 Gallery, consisting of a simple language with just the sketch and extrude modeling operations, and a dataset of 8,625 human design sequences expressed in this language. We also present an interactive environment called the Fusion 360 Gym, which exposes the sequential construction of a CAD program as a Markov decision process, making it amendable to machine learning approaches. As a use case for our dataset and environment, we define the CAD reconstruction task of recovering a CAD program from a target geometry. We report results of applying state-of-the-art methods of program synthesis with neurally guided search on this task.



