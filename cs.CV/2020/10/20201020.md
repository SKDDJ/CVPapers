# Arxiv Papers in cs.CV on 2020-10-20
### A Cluster-Matching-Based Method for Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.11732v1
- **DOI**: 10.1145/3428658.3430967
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.11732v1)
- **Published**: 2020-10-20 00:44:54+00:00
- **Updated**: 2020-10-20 00:44:54+00:00
- **Authors**: Paulo R C Mendes, Antonio J G Busson, Sérgio Colcher, Daniel Schwabe, Álan L V Guedes, Carlos Laufer
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Face recognition systems are present in many modern solutions and thousands of applications in our daily lives. However, current solutions are not easily scalable, especially when it comes to the addition of new targeted people. We propose a cluster-matching-based approach for face recognition in video. In our approach, we use unsupervised learning to cluster the faces present in both the dataset and targeted videos selected for face recognition. Moreover, we design a cluster matching heuristic to associate clusters in both sets that is also capable of identifying when a face belongs to a non-registered person. Our method has achieved a recall of 99.435% and a precision of 99.131% in the task of video face recognition. Besides performing face recognition, it can also be used to determine the video segments where each person is present.



### Region-specific Dictionary Learning-based Low-dose Thoracic CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2010.09953v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09953v1)
- **Published**: 2020-10-20 01:41:45+00:00
- **Updated**: 2020-10-20 01:41:45+00:00
- **Authors**: Qiong Xu, Jeff Wang, Hiroki Shirato, Lei Xing
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a dictionary learning-based method with region-specific image patches to maximize the utility of the powerful sparse data processing technique for CT image reconstruction. Considering heterogeneous distributions of image features and noise in CT, region-specific customization of dictionaries is utilized in iterative reconstruction. Thoracic CT images are partitioned into several regions according to their structural and noise characteristics. Dictionaries specific to each region are then learned from the segmented thoracic CT images and applied to subsequent image reconstruction of the region. Parameters for dictionary learning and sparse representation are determined according to the structural and noise properties of each region. The proposed method results in better performance than the conventional reconstruction based on a single dictionary in recovering structures and suppressing noise in both simulation and human CT imaging. Quantitatively, the simulation study shows maximum improvement of image quality for the whole thorax can achieve 4.88% and 11.1% in terms of the Structure-SIMilarity (SSIM) and Root-Mean-Square Error (RMSE) indices, respectively. For human imaging data, it is found that the structures in the lungs and heart can be better recovered, while simultaneously decreasing noise around the vertebra effectively. The proposed strategy takes into account inherent regional differences inside of the reconstructed object and leads to improved images. The method can be readily extended to CT imaging of other anatomical regions and other applications.



### Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.09978v1
- **DOI**: 10.1145/3394171.3413802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09978v1)
- **Published**: 2020-10-20 02:56:58+00:00
- **Updated**: 2020-10-20 02:56:58+00:00
- **Authors**: Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang
- **Comment**: Accepted by ACM MultiMedia 2020, 9 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the State-Of-The-Art (SOTA) models of this task tends to be exceedingly sophisticated and over-parameterized, where the low efficiency in model training and inference has obstructed the development in the field, especially for large-scale action datasets. In this work, we propose an efficient but strong baseline based on Graph Convolutional Network (GCN), where three main improvements are aggregated, i.e., early fused Multiple Input Branches (MIB), Residual GCN (ResGCN) with bottleneck structure and Part-wise Attention (PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton features and remain compact representations at an early fusion stage. Then, inspired by the success of the ResNet architecture in Convolutional Neural Network (CNN), a ResGCN module is introduced in GCN to alleviate computational costs and reduce learning difficulties in model training while maintain the model accuracy. Finally, a PartAtt block is proposed to discover the most essential body parts over a whole action sequence and obtain more explainable representations for different skeleton action sequences. Extensive experiments on two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the proposed baseline slightly outperforms other SOTA models and meanwhile requires much fewer parameters during training and inference procedures, e.g., at most 34 times less than DGNN, which is one of the best SOTA methods.



### Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.09982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.09982v1)
- **Published**: 2020-10-20 03:06:20+00:00
- **Updated**: 2020-10-20 03:06:20+00:00
- **Authors**: Yuqian Fu, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang
- **Comment**: accepted by ACM Multimedia 2020
- **Journal**: None
- **Summary**: Humans can easily recognize actions with only a few examples given, while the existing video recognition models still heavily rely on the large-scale labeled data inputs. This observation has motivated an increasing interest in few-shot video action recognition, which aims at learning new actions with only very few labeled samples. In this paper, we propose a depth guided Adaptive Meta-Fusion Network for few-shot video recognition which is termed as AMeFu-Net. Concretely, we tackle the few-shot recognition problem from three aspects: firstly, we alleviate this extremely data-scarce problem by introducing depth information as a carrier of the scene, which will bring extra visual information to our model; secondly, we fuse the representation of original RGB clips with multiple non-strictly corresponding depth clips sampled by our temporal asynchronization augmentation mechanism, which synthesizes new instances at feature-level; thirdly, a novel Depth Guided Adaptive Instance Normalization (DGAdaIN) fusion module is proposed to fuse the two-stream modalities efficiently. Additionally, to better mimic the few-shot recognition process, our model is trained in the meta-learning way. Extensive experiments on several action recognition benchmarks demonstrate the effectiveness of our model.



### ivadomed: A Medical Imaging Deep Learning Toolbox
- **Arxiv ID**: http://arxiv.org/abs/2010.09984v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.09984v1)
- **Published**: 2020-10-20 03:08:53+00:00
- **Updated**: 2020-10-20 03:08:53+00:00
- **Authors**: Charley Gros, Andreanne Lemay, Olivier Vincent, Lucas Rouhier, Anthime Bucquet, Joseph Paul Cohen, Julien Cohen-Adad
- **Comment**: None
- **Journal**: None
- **Summary**: ivadomed is an open-source Python package for designing, end-to-end training, and evaluating deep learning models applied to medical imaging data. The package includes APIs, command-line tools, documentation, and tutorials. ivadomed also includes pre-trained models such as spinal tumor segmentation and vertebral labeling. Original features of ivadomed include a data loader that can parse image metadata (e.g., acquisition parameters, image contrast, resolution) and subject metadata (e.g., pathology, age, sex) for custom data splitting or extra information during training and evaluation. Any dataset following the Brain Imaging Data Structure (BIDS) convention will be compatible with ivadomed without the need to manually organize the data, which is typically a tedious task. Beyond the traditional deep learning methods, ivadomed features cutting-edge architectures, such as FiLM and HeMis, as well as various uncertainty estimation methods (aleatoric and epistemic), and losses adapted to imbalanced classes and non-binary predictions. Each step is conveniently configurable via a single file. At the same time, the code is highly modular to allow addition/modification of an architecture or pre/post-processing steps. Example applications of ivadomed include MRI object detection, segmentation, and labeling of anatomical and pathological structures. Overall, ivadomed enables easy and quick exploration of the latest advances in deep learning for medical imaging applications. ivadomed's main project page is available at https://ivadomed.org.



### L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set
- **Arxiv ID**: http://arxiv.org/abs/2010.09987v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.09987v2)
- **Published**: 2020-10-20 03:17:20+00:00
- **Updated**: 2020-10-21 23:32:01+00:00
- **Authors**: Zhen Xiang, David J. Miller, George Kesidis
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attacks (BAs) are an emerging form of adversarial attack typically against deep neural network image classifiers. The attacker aims to have the classifier learn to classify to a target class when test images from one or more source classes contain a backdoor pattern, while maintaining high accuracy on all clean test images. Reverse-Engineering-based Defenses (REDs) against BAs do not require access to the training set but only to an independent clean dataset. Unfortunately, most existing REDs rely on an unrealistic assumption that all classes except the target class are source classes of the attack. REDs that do not rely on this assumption often require a large set of clean images and heavy computation. In this paper, we propose a Lagrangian-based RED (L-RED) that does not require knowledge of the number of source classes (or whether an attack is present). Our defense requires very few clean images to effectively detect BAs and is computationally efficient. Notably, we detect 56 out of 60 BAs using only two clean images per class in our experiments on CIFAR-10.



### Wasserstein K-Means for Clustering Tomographic Projections
- **Arxiv ID**: http://arxiv.org/abs/2010.09989v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 62H30 (Primary) 92C55, 68U10 (Secondary), I.5.3; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.09989v1)
- **Published**: 2020-10-20 03:28:17+00:00
- **Updated**: 2020-10-20 03:28:17+00:00
- **Authors**: Rohan Rao, Amit Moscovich, Amit Singer
- **Comment**: 11 pages, 5 figures, 1 table
- **Journal**: Machine Learning for Structural Biology Workshop, NeurIPS 2020
- **Summary**: Motivated by the 2D class averaging problem in single-particle cryo-electron microscopy (cryo-EM), we present a k-means algorithm based on a rotationally-invariant Wasserstein metric for images. Unlike existing methods that are based on Euclidean ($L_2$) distances, we prove that the Wasserstein metric better accommodates for the out-of-plane angular differences between different particle views. We demonstrate on a synthetic dataset that our method gives superior results compared to an $L_2$ baseline. Furthermore, there is little computational overhead, thanks to the use of a fast linear-time approximation to the Wasserstein-1 metric, also known as the Earthmover's distance.



### Explorable Tone Mapping Operators
- **Arxiv ID**: http://arxiv.org/abs/2010.10000v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10000v1)
- **Published**: 2020-10-20 04:18:54+00:00
- **Updated**: 2020-10-20 04:18:54+00:00
- **Authors**: Chien-Chuan Su, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, Chia-Ping Chen, Yu-Lin Chang, Soo-Chang Pei
- **Comment**: To appear in ICPR 2020
- **Journal**: None
- **Summary**: Tone-mapping plays an essential role in high dynamic range (HDR) imaging. It aims to preserve visual information of HDR images in a medium with a limited dynamic range. Although many works have been proposed to provide tone-mapped results from HDR images, most of them can only perform tone-mapping in a single pre-designed way. However, the subjectivity of tone-mapping quality varies from person to person, and the preference of tone-mapping style also differs from application to application. In this paper, a learning-based multimodal tone-mapping method is proposed, which not only achieves excellent visual quality but also explores the style diversity. Based on the framework of BicycleGAN, the proposed method can provide a variety of expert-level tone-mapped results by manipulating different latent codes. Finally, we show that the proposed method performs favorably against state-of-the-art tone-mapping algorithms both quantitatively and qualitatively.



### Contextual Heterogeneous Graph Network for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.10001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10001v1)
- **Published**: 2020-10-20 04:20:33+00:00
- **Updated**: 2020-10-20 04:20:33+00:00
- **Authors**: Hai Wang, Wei-Shi Zheng, Ling Yingbiao
- **Comment**: Published on ECCV-2020
- **Journal**: None
- **Summary**: Human-object interaction(HOI) detection is an important task for understanding human activity. Graph structure is appropriate to denote the HOIs in the scene. Since there is an subordination between human and object---human play subjective role and object play objective role in HOI, the relations between homogeneous entities and heterogeneous entities in the scene should also not be equally the same. However, previous graph models regard human and object as the same kind of nodes and do not consider that the messages are not equally the same between different entities. In this work, we address such a problem for HOI task by proposing a heterogeneous graph network that models humans and objects as different kinds of nodes and incorporates intra-class messages between homogeneous nodes and inter-class messages between heterogeneous nodes. In addition, a graph attention mechanism based on the intra-class context and inter-class context is exploited to improve the learning. Extensive experiments on the benchmark datasets V-COCO and HICO-DET demonstrate that the intra-class and inter-class messages are very important in HOI detection and verify the effectiveness of our method.



### Convolutional-LSTM for Multi-Image to Single Output Medical Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.10004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10004v1)
- **Published**: 2020-10-20 04:30:09+00:00
- **Updated**: 2020-10-20 04:30:09+00:00
- **Authors**: Luis Leal, Marvin Castillo, Fernando Juarez, Erick Ramirez, Mildred Aspuac, Diana Letona
- **Comment**: None
- **Journal**: None
- **Summary**: Medical head CT-scan imaging has been successfully combined with deep learning for medical diagnostics of head diseases and lesions[1]. State of the art classification models and algorithms for this task usually are based on 3d convolution layers for volumetric data on a supervised learning setting (1 input volume, 1 prediction per patient) or 2d convolution layers in a supervised setting (1 input image, 1 prediction per image). However a very common scenario in developing countries is to have the volume metadata lost due multiple reasons for example formatting conversion in images (for example .dicom to jpg), in this scenario the doctor analyses the collection of images and then emits a single diagnostic for the patient (with possibly an unfixed and variable number of images per patient) , this prevents it from being possible to use state of the art 3d models, but also is not possible to convert it to a supervised problem in a (1 image,1 diagnostic) setting because different angles or positions of the images for a single patient may not contain the disease or lesion. In this study we propose a solution for this scenario by combining 2d convolutional[2] models with sequence models which generate a prediction only after all images have been processed by the model for a given patient \(i\), this creates a multi-image to single-diagnostic setting \(y^i=f(x_1,x_2,..,x_n)\) where \(n\) may be different between patients. The experimental results demonstrate that it is possible to get a multi-image to single diagnostic model which mimics human doctor diagnostic process: evaluate the collection of patient images and then use important information in memory to decide a single diagnostic for the patient.



### Fast Video Salient Object Detection via Spatiotemporal Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2010.10027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10027v2)
- **Published**: 2020-10-20 04:48:36+00:00
- **Updated**: 2021-03-17 09:51:51+00:00
- **Authors**: Yi Tang, Yuanman Li, Wenbin Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Since the wide employment of deep learning frameworks in video salient object detection, the accuracy of the recent approaches has made stunning progress. These approaches mainly adopt the sequential modules, based on optical flow or recurrent neural network (RNN), to learn robust spatiotemporal features. These modules are effective but significantly increase the computational burden of the corresponding deep models. In this paper, to simplify the network and maintain the accuracy, we present a lightweight network tailored for video salient object detection through the spatiotemporal knowledge distillation. Specifically, in the spatial aspect, we combine a saliency guidance feature embedding structure and spatial knowledge distillation to refine the spatial features. In the temporal aspect, we propose a temporal knowledge distillation strategy, which allows the network to learn the robust temporal features through the infer-frame feature encoding and distilling information from adjacent frames. The experiments on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2) prove that our approach achieves competitive performance. Furthermore, without the employment of the complex sequential modules, the proposed network can obtain high efficiency with 0.01s per frame.



### TTPLA: An Aerial-Image Dataset for Detection and Segmentation of Transmission Towers and Power Lines
- **Arxiv ID**: http://arxiv.org/abs/2010.10032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10032v1)
- **Published**: 2020-10-20 04:58:05+00:00
- **Updated**: 2020-10-20 04:58:05+00:00
- **Authors**: Rabab Abdelfattah, Xiaofeng Wang, Song Wang
- **Comment**: 17 pages, 9 figures, 4 Tables
- **Journal**: None
- **Summary**: Accurate detection and segmentation of transmission towers~(TTs) and power lines~(PLs) from aerial images plays a key role in protecting power-grid security and low-altitude UAV safety. Meanwhile, aerial images of TTs and PLs pose a number of new challenges to the computer vision researchers who work on object detection and segmentation -- PLs are long and thin, and may show similar color as the background; TTs can be of various shapes and most likely made up of line structures of various sparsity; The background scene, lighting, and object sizes can vary significantly from one image to another. In this paper we collect and release a new TT/PL Aerial-image (TTPLA) dataset, consisting of 1,100 images with the resolution of 3,840$\times$2,160 pixels, as well as manually labeled 8,987 instances of TTs and PLs. We develop novel policies for collecting, annotating, and labeling the images in TTPLA. Different from other relevant datasets, TTPLA supports evaluation of instance segmentation, besides detection and semantic segmentation. To build a baseline for detection and segmentation tasks on TTPLA, we report the performance of several state-of-the-art deep learning models on our dataset. TTPLA dataset is publicly available at https://github.com/r3ab/ttpla_dataset



### SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency
- **Arxiv ID**: http://arxiv.org/abs/2010.10038v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10038v2)
- **Published**: 2020-10-20 05:15:48+00:00
- **Updated**: 2020-12-01 02:11:13+00:00
- **Authors**: Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath R. Selvaraju
- **Comment**: Accepted to the NeurIPS 2020 workshop on Interpretable Inductive
  Biases and Physically Structured Learning
- **Journal**: None
- **Summary**: Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -- they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the higher level question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an <image, reasoning-question> pair. We show that SOrT improves model consistency by upto 6.5% points over existing baselines, while also improving visual grounding.



### Robust Neural Networks inspired by Strong Stability Preserving Runge-Kutta methods
- **Arxiv ID**: http://arxiv.org/abs/2010.10047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10047v1)
- **Published**: 2020-10-20 05:55:58+00:00
- **Updated**: 2020-10-20 05:55:58+00:00
- **Authors**: Byungjoo Kim, Bryce Chudomelka, Jinyoung Park, Jaewoo Kang, Youngjoon Hong, Hyunwoo J. Kim
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Deep neural networks have achieved state-of-the-art performance in a variety of fields. Recent works observe that a class of widely used neural networks can be viewed as the Euler method of numerical discretization. From the numerical discretization perspective, Strong Stability Preserving (SSP) methods are more advanced techniques than the explicit Euler method that produce both accurate and stable solutions. Motivated by the SSP property and a generalized Runge-Kutta method, we propose Strong Stability Preserving networks (SSP networks) which improve robustness against adversarial attacks. We empirically demonstrate that the proposed networks improve the robustness against adversarial examples without any defensive methods. Further, the SSP networks are complementary with a state-of-the-art adversarial training scheme. Lastly, our experiments show that SSP networks suppress the blow-up of adversarial perturbations. Our results open up a way to study robust architectures of neural networks leveraging rich knowledge from numerical discretization literature.



### Tracking from Patterns: Learning Corresponding Patterns in Point Clouds for 3D Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.10051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10051v1)
- **Published**: 2020-10-20 06:07:20+00:00
- **Updated**: 2020-10-20 06:07:20+00:00
- **Authors**: Jieqi Shi, Peiliang Li, Shaojie Shen
- **Comment**: 4 pages, ECCV2020 Workshop on Perception for Autonomous
  Driving(PAD2020)
- **Journal**: ECCV2020 Workshop on Perception for Autonomous Driving(PAD2020)
- **Summary**: A robust 3D object tracker which continuously tracks surrounding objects and estimates their trajectories is key for self-driving vehicles. Most existing tracking methods employ a tracking-by-detection strategy, which usually requires complex pair-wise similarity computation and neglects the nature of continuous object motion. In this paper, we propose to directly learn 3D object correspondences from temporal point cloud data and infer the motion information from correspondence patterns. We modify the standard 3D object detector to process two lidar frames at the same time and predict bounding box pairs for the association and motion estimation tasks. We also equip our pipeline with a simple yet effective velocity smoothing module to estimate consistent object motion. Benifiting from the learned correspondences and motion refinement, our method exceeds the existing 3D tracking methods on both the KITTI and larger scale Nuscenes dataset.



### Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair
- **Arxiv ID**: http://arxiv.org/abs/2010.10052v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10052v2)
- **Published**: 2020-10-20 06:08:42+00:00
- **Updated**: 2020-11-13 10:06:06+00:00
- **Authors**: S Anupama, Prasan Shedligeri, Abhishek Pal, Kaushik Mitra
- **Comment**: 8 pages, 7 figures, 3 tables, To appear at ICPR 2020
- **Journal**: None
- **Summary**: Learning-based methods have enabled the recovery of a video sequence from a single motion-blurred image or a single coded exposure image. Recovering video from a single motion-blurred image is a very ill-posed problem and the recovered video usually has many artifacts. In addition to this, the direction of motion is lost and it results in motion ambiguity. However, it has the advantage of fully preserving the information in the static parts of the scene. The traditional coded exposure framework is better-posed but it only samples a fraction of the space-time volume, which is at best 50% of the space-time volume. Here, we propose to use the complementary information present in the fully-exposed (blurred) image along with the coded exposure image to recover a high fidelity video without any motion ambiguity. Our framework consists of a shared encoder followed by an attention module to selectively combine the spatial information from the fully-exposed image with the temporal information from the coded image, which is then super-resolved to recover a non-ambiguous high-quality video. The input to our algorithm is a fully-exposed and coded image pair. Such an acquisition system already exists in the form of a Coded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning approach using blurred-coded image pair produces much better results than those from just a blurred image or just a coded image.



### Real-time Localized Photorealistic Video Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.10056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10056v1)
- **Published**: 2020-10-20 06:21:09+00:00
- **Updated**: 2020-10-20 06:21:09+00:00
- **Authors**: Xide Xia, Tianfan Xue, Wei-sheng Lai, Zheng Sun, Abby Chang, Brian Kulis, Jiawen Chen
- **Comment**: 16 pages, 15 figures
- **Journal**: None
- **Summary**: We present a novel algorithm for transferring artistic styles of semantically meaningful local regions of an image onto local regions of a target video while preserving its photorealism. Local regions may be selected either fully automatically from an image, through using video segmentation algorithms, or from casual user guidance such as scribbles. Our method, based on a deep neural network architecture inspired by recent work in photorealistic style transfer, is real-time and works on arbitrary inputs without runtime optimization once trained on a diverse dataset of artistic styles. By augmenting our video dataset with noisy semantic labels and jointly optimizing over style, content, mask, and temporal losses, our method can cope with a variety of imperfections in the input and produce temporally coherent videos without visual artifacts. We demonstrate our method on a variety of style images and target videos, including the ability to transfer different styles onto multiple objects simultaneously, and smoothly transition between styles in time.



### BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues
- **Arxiv ID**: http://arxiv.org/abs/2010.10095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10095v1)
- **Published**: 2020-10-20 07:43:00+00:00
- **Updated**: 2020-10-20 07:43:00+00:00
- **Authors**: Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.



### Two-stage generative adversarial networks for document image binarization with color noise and background removal
- **Arxiv ID**: http://arxiv.org/abs/2010.10103v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10103v3)
- **Published**: 2020-10-20 07:51:50+00:00
- **Updated**: 2021-04-27 08:17:44+00:00
- **Authors**: Sungho Suh, Jihun Kim, Paul Lukowicz, Yong Oh Lee
- **Comment**: Submitted to Pattern Recognition, Elsevier
- **Journal**: None
- **Summary**: Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network-based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks. In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks, we formulate loss functions between a discriminator and generators having an encoder-decoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/.



### Identification of deep breath while moving forward based on multiple body regions and graph signal analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.11734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2010.11734v1)
- **Published**: 2020-10-20 08:26:50+00:00
- **Updated**: 2020-10-20 08:26:50+00:00
- **Authors**: Yunlu Wang, Cheng Yang, Menghan Hu, Jian Zhang, Qingli Li, Guangtao Zhai, Xiao-Ping Zhang
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: This paper presents an unobtrusive solution that can automatically identify deep breath when a person is walking past the global depth camera. Existing non-contact breath assessments achieve satisfactory results under restricted conditions when human body stays relatively still. When someone moves forward, the breath signals detected by depth camera are hidden within signals of trunk displacement and deformation, and the signal length is short due to the short stay time, posing great challenges for us to establish models. To overcome these challenges, multiple region of interests (ROIs) based signal extraction and selection method is proposed to automatically obtain the signal informative to breath from depth video. Subsequently, graph signal analysis (GSA) is adopted as a spatial-temporal filter to wipe the components unrelated to breath. Finally, a classifier for identifying deep breath is established based on the selected breath-informative signal. In validation experiments, the proposed approach outperforms the comparative methods with the accuracy, precision, recall and F1 of 75.5%, 76.2%, 75.0% and 75.2%, respectively. This system can be extended to public places to provide timely and ubiquitous help for those who may have or are going through physical or mental trouble.



### Claw U-Net: A Unet-based Network with Deep Feature Concatenation for Scleral Blood Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.10163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10163v1)
- **Published**: 2020-10-20 09:55:29+00:00
- **Updated**: 2020-10-20 09:55:29+00:00
- **Authors**: Chang Yao, Jingyu Tang, Menghan Hu, Yue Wu, Wenyi Guo, Qingli Li, Xiao-Ping Zhang
- **Comment**: 5 pages,4 figures
- **Journal**: None
- **Summary**: Sturge-Weber syndrome (SWS) is a vascular malformation disease, and it may cause blindness if the patient's condition is severe. Clinical results show that SWS can be divided into two types based on the characteristics of scleral blood vessels. Therefore, how to accurately segment scleral blood vessels has become a significant problem in computer-aided diagnosis. In this research, we propose to continuously upsample the bottom layer's feature maps to preserve image details, and design a novel Claw UNet based on UNet for scleral blood vessel segmentation. Specifically, the residual structure is used to increase the number of network layers in the feature extraction stage to learn deeper features. In the decoding stage, by fusing the features of the encoding, upsampling, and decoding parts, Claw UNet can achieve effective segmentation in the fine-grained regions of scleral blood vessels. To effectively extract small blood vessels, we use the attention mechanism to calculate the attention coefficient of each position in images. Claw UNet outperforms other UNet-based networks on scleral blood vessel image dataset.



### ICFHR 2020 Competition on Image Retrieval for Historical Handwritten Fragments
- **Arxiv ID**: http://arxiv.org/abs/2010.10197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10197v1)
- **Published**: 2020-10-20 11:12:35+00:00
- **Updated**: 2020-10-20 11:12:35+00:00
- **Authors**: Mathias Seuret, Anguelos Nicolaou, Dominique Stutzmann, Andreas Maier, Vincent Christlein
- **Comment**: ICFHR 2020
- **Journal**: None
- **Summary**: This competition succeeds upon a line of competitions for writer and style analysis of historical document images. In particular, we investigate the performance of large-scale retrieval of historical document fragments in terms of style and writer identification. The analysis of historic fragments is a difficult challenge commonly solved by trained humanists. In comparison to previous competitions, we make the results more meaningful by addressing the issue of sample granularity and moving from writer to page fragment retrieval. The two approaches, style and author identification, provide information on what kind of information each method makes better use of and indirectly contribute to the interpretability of the participating method. Therefore, we created a large dataset consisting of more than 120 000 fragments. Although the most teams submitted methods based on convolutional neural networks, the winning entry achieves an mAP below 40%.



### Self-Supervised Learning of Part Mobility from Point Cloud Sequence
- **Arxiv ID**: http://arxiv.org/abs/2010.11735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11735v2)
- **Published**: 2020-10-20 11:29:46+00:00
- **Updated**: 2021-03-02 09:34:11+00:00
- **Authors**: Yahao Shi, Xinyu Cao, Bin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Part mobility analysis is a significant aspect required to achieve a functional understanding of 3D objects. It would be natural to obtain part mobility from the continuous part motion of 3D objects. In this study, we introduce a self-supervised method for segmenting motion parts and predicting their motion attributes from a point cloud sequence representing a dynamic object. To sufficiently utilize spatiotemporal information from the point cloud sequence, we generate trajectories by using correlations among successive frames of the sequence instead of directly processing the point clouds. We propose a novel neural network architecture called PointRNN to learn feature representations of trajectories along with their part rigid motions. We evaluate our method on various tasks including motion part segmentation, motion axis prediction and motion range estimation. The results demonstrate that our method outperforms previous techniques on both synthetic and real datasets. Moreover, our method has the ability to generalize to new and unseen objects. It is important to emphasize that it is not required to know any prior shape structure, prior shape category information, or shape orientation. To the best of our knowledge, this is the first study on deep learning to extract part mobility from point cloud sequence of a dynamic object.



### Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT Images Utilizing Synthesized Training Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.10207v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10207v1)
- **Published**: 2020-10-20 11:40:24+00:00
- **Updated**: 2020-10-20 11:40:24+00:00
- **Authors**: Tong Zheng, Hirohisa Oda, Masahiro Oda, Shota Nakamura, Masaki Mori, Hirotsugu Takabatake, Hiroshi Natori, Kensaku Mori
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel, unsupervised super-resolution (SR) approach for performing the SR of a clinical CT into the resolution level of a micro CT ($\mu$CT). The precise non-invasive diagnosis of lung cancer typically utilizes clinical CT data. Due to the resolution limitations of clinical CT (about $0.5 \times 0.5 \times 0.5$ mm$^3$), it is difficult to obtain enough pathological information such as the invasion area at alveoli level. On the other hand, $\mu$CT scanning allows the acquisition of volumes of lung specimens with much higher resolution ($50 \times 50 \times 50 \mu {\rm m}^3$ or higher). Thus, super-resolution of clinical CT volume may be helpful for diagnosis of lung cancer. Typical SR methods require aligned pairs of low-resolution (LR) and high-resolution (HR) images for training. Unfortunately, obtaining paired clinical CT and $\mu$CT volumes of human lung tissues is infeasible. Unsupervised SR methods are required that do not need paired LR and HR images. In this paper, we create corresponding clinical CT-$\mu$CT pairs by simulating clinical CT images from $\mu$CT images by modified CycleGAN. After this, we use simulated clinical CT-$\mu$CT image pairs to train an SR network based on SRGAN. Finally, we use the trained SR network to perform SR of the clinical CT images. We compare our proposed method with another unsupervised SR method for clinical CT images named SR-CycleGAN. Experimental results demonstrate that the proposed method can successfully perform SR of clinical CT images of lung cancer patients with $\mu$CT level resolution, and quantitatively and qualitatively outperformed conventional method (SR-CycleGAN), improving the SSIM (structure similarity) form 0.40 to 0.51.



### BYOL works even without batch statistics
- **Arxiv ID**: http://arxiv.org/abs/2010.10241v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10241v1)
- **Published**: 2020-10-20 13:05:05+00:00
- **Updated**: 2020-10-20 13:05:05+00:00
- **Authors**: Pierre H. Richemond, Jean-Bastien Grill, Florent Altché, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, Michal Valko
- **Comment**: None
- **Journal**: None
- **Summary**: Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach for image representation. From an augmented view of an image, BYOL trains an online network to predict a target network representation of a different augmented view of the same image. Unlike contrastive methods, BYOL does not explicitly use a repulsion term built from negative pairs in its training objective. Yet, it avoids collapse to a trivial, constant representation. Thus, it has recently been hypothesized that batch normalization (BN) is critical to prevent collapse in BYOL. Indeed, BN flows gradients across batch elements, and could leak information about negative views in the batch, which could act as an implicit negative (contrastive) term. However, we experimentally show that replacing BN with a batch-independent normalization scheme (namely, a combination of group normalization and weight standardization) achieves performance comparable to vanilla BYOL ($73.9\%$ vs. $74.3\%$ top-1 accuracy under the linear evaluation protocol on ImageNet with ResNet-$50$). Our finding disproves the hypothesis that the use of batch statistics is a crucial ingredient for BYOL to learn useful representations.



### Ulixes: Facial Recognition Privacy with Adversarial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.10242v2
- **DOI**: 10.2478/popets-2022-0008
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10242v2)
- **Published**: 2020-10-20 13:05:51+00:00
- **Updated**: 2022-02-01 18:10:16+00:00
- **Authors**: Thomas Cilloni, Wei Wang, Charles Walter, Charles Fleming
- **Comment**: None
- **Journal**: PoPETS Proceedings on Privacy Enhancing Technologies 2022;
  (1):148-165
- **Summary**: Facial recognition tools are becoming exceptionally accurate in identifying people from images. However, this comes at the cost of privacy for users of online services with photo management (e.g. social media platforms). Particularly troubling is the ability to leverage unsupervised learning to recognize faces even when the user has not labeled their images. In this paper we propose Ulixes, a strategy to generate visually non-invasive facial noise masks that yield adversarial examples, preventing the formation of identifiable user clusters in the embedding space of facial encoders. This is applicable even when a user is unmasked and labeled images are available online. We demonstrate the effectiveness of Ulixes by showing that various classification and clustering methods cannot reliably label the adversarial examples we generate. We also study the effects of Ulixes in various black-box settings and compare it to the current state of the art in adversarial machine learning. Finally, we challenge the effectiveness of Ulixes against adversarially trained models and show that it is robust to countermeasures.



### AutoBSS: An Efficient Algorithm for Block Stacking Style Search
- **Arxiv ID**: http://arxiv.org/abs/2010.10261v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10261v2)
- **Published**: 2020-10-20 13:32:10+00:00
- **Updated**: 2021-03-29 05:08:52+00:00
- **Authors**: Yikang Zhang, Jian Zhang, Zhao Zhong
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Neural network architecture design mostly focuses on the new convolutional operator or special topological structure of network block, little attention is drawn to the configuration of stacking each block, called Block Stacking Style (BSS). Recent studies show that BSS may also have an unneglectable impact on networks, thus we design an efficient algorithm to search it automatically. The proposed method, AutoBSS, is a novel AutoML algorithm based on Bayesian optimization by iteratively refining and clustering Block Stacking Style Code (BSSC), which can find optimal BSS in a few trials without biased evaluation. On ImageNet classification task, ResNet50/MobileNetV2/EfficientNet-B0 with our searched BSS achieve 79.29%/74.5%/77.79%, which outperform the original baselines by a large margin. More importantly, experimental results on model compression, object detection and instance segmentation show the strong generalizability of the proposed AutoBSS, and further verify the unneglectable impact of BSS on neural networks.



### Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2010.10266v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10266v1)
- **Published**: 2020-10-20 13:37:40+00:00
- **Updated**: 2020-10-20 13:37:40+00:00
- **Authors**: Hasib Zunair, A. Ben Hamza
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the lack of publicly available datasets of chest radiographs of positive patients with Coronavirus disease 2019 (COVID-19), we build the first-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high fidelity using an unsupervised domain adaptation approach by leveraging class conditioning and adversarial training. Our contributions are twofold. First, we show considerable performance improvements on COVID-19 detection using various deep learning architectures when employing synthetic images as additional training set. Second, we show how our image synthesis method can serve as a data anonymization tool by achieving comparable detection performance when trained only on synthetic data. In addition, the proposed data generation framework offers a viable solution to the COVID-19 detection in particular, and to medical image classification tasks in general. Our publicly available benchmark dataset consists of 21,295 synthetic COVID-19 chest X-ray images. The insights gleaned from this dataset can be used for preventive actions in the fight against the COVID-19 pandemic.



### Pedestrian Intention Prediction: A Multi-task Perspective
- **Arxiv ID**: http://arxiv.org/abs/2010.10270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.10270v2)
- **Published**: 2020-10-20 13:42:31+00:00
- **Updated**: 2021-05-20 11:14:35+00:00
- **Authors**: Smail Ait Bouhsain, Saeed Saadatnejad, Alexandre Alahi
- **Comment**: Accepted and published in hEART2020 (the 9th Symposium of the
  European Association for Research in Transportation):
  http://www.heart-web.org/
- **Journal**: None
- **Summary**: In order to be globally deployed, autonomous cars must guarantee the safety of pedestrians. This is the reason why forecasting pedestrians' intentions sufficiently in advance is one of the most critical and challenging tasks for autonomous vehicles. This work tries to solve this problem by jointly predicting the intention and visual states of pedestrians. In terms of visual states, whereas previous work focused on x-y coordinates, we will also predict the size and indeed the whole bounding box of the pedestrian. The method is a recurrent neural network in a multi-task learning approach. It has one head that predicts the intention of the pedestrian for each one of its future position and another one predicting the visual states of the pedestrian. Experiments on the JAAD dataset show the superiority of the performance of our method compared to previous works for intention prediction. Also, although its simple architecture (more than 2 times faster), the performance of the bounding box prediction is comparable to the ones yielded by much more complex architectures. Our code is available online.



### Edge Bias in Federated Learning and its Solution by Buffered Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2010.10338v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10338v3)
- **Published**: 2020-10-20 15:00:43+00:00
- **Updated**: 2021-02-09 07:26:33+00:00
- **Authors**: Sangho Lee, Kiyoon Yoo, Nojun Kwak
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Federated learning (FL), which utilizes communication between the server (core) and local devices (edges) to indirectly learn from more data, is an emerging field in deep learning research. Recently, Knowledge Distillation-based FL methods with notable performance and high applicability have been suggested. In this paper, we choose knowledge distillation-based FL method as our baseline and tackle a challenging problem that ensues from using these methods. Especially, we focus on the problem incurred in the server model that tries to mimic different datasets, each of which is unique to an individual edge device. We dub the problem 'edge bias', which occurs when multiple teacher models trained on different datasets are used individually to distill knowledge. We introduce this nuisance that occurs in certain scenarios of FL, and to alleviate it, we propose a simple yet effective distillation scheme named 'buffered distillation'. In addition, we also experimentally show that this scheme is effective in mitigating the straggler problem caused by delayed edges.



### Leveraging SLIC Superpixel Segmentation and Cascaded Ensemble SVM for Fully Automated Mass Detection In Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2010.10340v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10340v1)
- **Published**: 2020-10-20 15:02:25+00:00
- **Updated**: 2020-10-20 15:02:25+00:00
- **Authors**: Jaime Simarro, Zohaib Salahuddin, Ahmed Gouda, Anindo Saha
- **Comment**: None
- **Journal**: None
- **Summary**: Identification and segmentation of breast masses in mammograms face complex challenges, owing to the highly variable nature of malignant densities with regards to their shape, contours, texture and orientation. Additionally, classifiers typically suffer from high class imbalance in region candidates, where normal tissue regions vastly outnumber malignant masses. This paper proposes a rigorous segmentation method, supported by morphological enhancement using grayscale linear filters. A novel cascaded ensemble of support vector machines (SVM) is used to effectively tackle the class imbalance and provide significant predictions. For True Positive Rate (TPR) of 0.35, 0.69 and 0.82, the system generates only 0.1, 0.5 and 1.0 False Positives/Image (FPI), respectively.



### A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.10368v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10368v2)
- **Published**: 2020-10-20 15:22:29+00:00
- **Updated**: 2020-10-26 19:29:06+00:00
- **Authors**: Ali Akbari, Muhammad Awais, Zhen-Hua Feng, Ammarah Farooq, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: The most existing studies in the facial age estimation assume training and test images are captured under similar shooting conditions. However, this is rarely valid in real-world applications, where training and test sets usually have different characteristics. In this paper, we advocate a cross-dataset protocol for age estimation benchmarking. In order to improve the cross-dataset age estimation performance, we mitigate the inherent bias caused by the learning algorithm itself. To this end, we propose a novel loss function that is more effective for neural network training. The relative smoothness of the proposed loss function is its advantage with regards to the optimisation process performed by stochastic gradient descent (SGD). Compared with existing loss functions, the lower gradient of the proposed loss function leads to the convergence of SGD to a better optimum point, and consequently a better generalisation. The cross-dataset experimental results demonstrate the superiority of the proposed method over the state-of-the-art algorithms in terms of accuracy and generalisation capability.



### Convolutional neural networks for automatic detection of Focal Cortical Dysplasia
- **Arxiv ID**: http://arxiv.org/abs/2010.10373v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10373v1)
- **Published**: 2020-10-20 15:30:37+00:00
- **Updated**: 2020-10-20 15:30:37+00:00
- **Authors**: Ruslan Aliev, Ekaterina Kondrateva, Maxim Sharaev, Oleg Bronov, Alexey Marinets, Sergey Subbotin, Alexander Bernstein, Evgeny Burnaev
- **Comment**: MRI, Deep learning, CNN, computer vision, medical detection,
  epilepsy, FCD, focal cortical dysplasia
- **Journal**: None
- **Summary**: Focal cortical dysplasia (FCD) is one of the most common epileptogenic lesions associated with cortical development malformations. However, the accurate detection of the FCD relies on the radiologist professionalism, and in many cases, the lesion could be missed. In this work, we solve the problem of automatic identification of FCD on magnetic resonance images (MRI). For this task, we improve recent methods of Deep Learning-based FCD detection and apply it for a dataset of 15 labeled FCD patients. The model results in the successful detection of FCD on 11 out of 15 subjects.



### On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR Application
- **Arxiv ID**: http://arxiv.org/abs/2010.11700v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11700v1)
- **Published**: 2020-10-20 17:05:11+00:00
- **Updated**: 2020-10-20 17:05:11+00:00
- **Authors**: Fadi Boutros, Naser Damer, Kiran Raja, Raghavendra Ramachandra, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at International Join Conference on Biometrics (IJCB 2020)
- **Journal**: None
- **Summary**: Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication.



### Tilting at windmills: Data augmentation for deep pose estimation does not help with occlusions
- **Arxiv ID**: http://arxiv.org/abs/2010.10451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10451v1)
- **Published**: 2020-10-20 17:06:46+00:00
- **Updated**: 2020-10-20 17:06:46+00:00
- **Authors**: Rafal Pytel, Osman Semih Kayhan, Jan C. van Gemert
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Occlusion degrades the performance of human pose estimation. In this paper, we introduce targeted keypoint and body part occlusion attacks. The effects of the attacks are systematically analyzed on the best performing methods. In addition, we propose occlusion specific data augmentation techniques against keypoint and part attacks. Our extensive experiments show that human pose estimation methods are not robust to occlusion and data augmentation does not solve the occlusion problems.



### FishNet: A Unified Embedding for Salmon Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.10475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10475v1)
- **Published**: 2020-10-20 17:35:01+00:00
- **Updated**: 2020-10-20 17:35:01+00:00
- **Authors**: Bjørn Magnus Mathisen, Kerstin Bach, Espen Meidell, Håkon Måløy, Edvard Schreiner Sjøblom
- **Comment**: ECAI 2020
- **Journal**: None
- **Summary**: Identifying individual salmon can be very beneficial for the aquaculture industry as it enables monitoring and analyzing fish behavior and welfare. For aquaculture researchers identifying individual salmon is imperative to their research. The current methods of individual salmon tagging and tracking rely on physical interaction with the fish. This process is inefficient and can cause physical harm and stress for the salmon. In this paper we propose FishNet, based on a deep learning technique that has been successfully used for identifying humans, to identify salmon.We create a dataset of labeled fish images and then test the performance of the FishNet architecture. Our experiments show that this architecture learns a useful representation based on images of salmon heads. Further, we show that good performance can be achieved with relatively small neural network models: FishNet achieves a false positive rate of 1\% and a true positive rate of 96\%.



### SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images
- **Arxiv ID**: http://arxiv.org/abs/2010.10505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10505v1)
- **Published**: 2020-10-20 17:59:47+00:00
- **Updated**: 2020-10-20 17:59:47+00:00
- **Authors**: Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey
- **Comment**: Accepted to NeurIPS 2020. Project page & code:
  https://chenhsuanlin.bitbucket.io/signed-distance-SRN/
- **Journal**: None
- **Summary**: Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.



### Image-Driven Furniture Style for Interactive 3D Scene Modeling
- **Arxiv ID**: http://arxiv.org/abs/2010.10557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10557v1)
- **Published**: 2020-10-20 18:19:28+00:00
- **Updated**: 2020-10-20 18:19:28+00:00
- **Authors**: Tomer Weiss, Ilkay Yildiz, Nitin Agarwal, Esra Ataer-Cansizoglu, Jae-Woo Choi
- **Comment**: Accepted to Pacific Graphics 2020
- **Journal**: None
- **Summary**: Creating realistic styled spaces is a complex task, which involves design know-how for what furniture pieces go well together. Interior style follows abstract rules involving color, geometry and other visual elements. Following such rules, users manually select similar-style items from large repositories of 3D furniture models, a process which is both laborious and time-consuming. We propose a method for fast-tracking style-similarity tasks, by learning a furniture's style-compatibility from interior scene images. Such images contain more style information than images depicting single furniture. To understand style, we train a deep learning network on a classification task. Based on image embeddings extracted from our network, we measure stylistic compatibility of furniture. We demonstrate our method with several 3D model style-compatibility results, and with an interactive system for modeling style-consistent scenes.



### A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2010.10563v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10563v2)
- **Published**: 2020-10-20 18:48:37+00:00
- **Updated**: 2022-01-08 15:21:49+00:00
- **Authors**: Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia Besa, Sergio Uribe, Marcelo andía, Cristian Tejos, Claudia Prieto, Daniel Capurro
- **Comment**: Accepted for publication in ACM CSUR
- **Journal**: None
- **Summary**: Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to: (1) Datasets, (2) Architecture Design, (3) Explainability and (4) Evaluation Metrics. Our survey identifies interesting developments, but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.



### Incandescent Bulb and LED Brake Lights:Novel Analysis of Reaction Times
- **Arxiv ID**: http://arxiv.org/abs/2010.10584v1
- **DOI**: 10.1109/ACCESS.2021.3058579
- **Categories**: **cs.HC**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.10584v1)
- **Published**: 2020-10-20 19:41:52+00:00
- **Updated**: 2020-10-20 19:41:52+00:00
- **Authors**: Ramaswamy Palaniappan, Surej Mouli, Evangelina Fringi, Howard Bowman, Ian McLoughlin
- **Comment**: 10 pages, 18 figures
- **Journal**: For a revised version and its published version refer to IEEE
  Access journal, 2021
- **Summary**: Rear-end collision accounts for around 8% of all vehicle crashes in the UK, with the failure to notice or react to a brake light signal being a major contributory cause. Meanwhile traditional incandescent brake light bulbs on vehicles are increasingly being replaced by a profusion of designs featuring LEDs. In this paper, we investigate the efficacy of brake light design using a novel approach to recording subject reaction times in a simulation setting using physical brake light assemblies. The reaction times of 22 subjects were measured for ten pairs of LED and incandescent bulb brake lights. Three events were investigated for each subject, namely the latency of brake light activation to accelerator release (BrakeAcc), the latency of accelerator release to brake pedal depression (AccPdl), and the cumulative time from light activation to brake pedal depression (BrakePdl). To our knowledge, this is the first study in which reaction times have been split into BrakeAcc and AccPdl. Results indicate that the two brake lights containing incandescent bulbs led to significantly slower reaction times compared to the tested eight LED lights. BrakeAcc results also show that experienced subjects were quicker to respond to the activation of brake lights by releasing the accelerator pedal. Interestingly, the analysis also revealed that the type of brake light influenced the AccPdl time, although experienced subjects did not always act quicker than inexperienced subjects. Overall, the study found that different designs of brake light can significantly influence driver response times.



### American Sign Language Identification Using Hand Trackpoint Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.10590v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10590v3)
- **Published**: 2020-10-20 19:59:16+00:00
- **Updated**: 2021-01-19 21:11:21+00:00
- **Authors**: Yugam Bajaj, Puru Malhotra
- **Comment**: 12 Pages, 6 Images
- **Journal**: None
- **Summary**: Sign Language helps people with Speaking and Hearing Disabilities communicate with others efficiently. Sign Language identification is a challenging area in the field of computer vision and recent developments have been able to achieve near perfect results for the task, though some challenges are yet to be solved. In this paper we propose a novel machine learning based pipeline for American Sign Language identification using hand track points. We convert a hand gesture into a series of hand track point coordinates that serve as an input to our system. In order to make the solution more efficient, we experimented with 28 different combinations of pre-processing techniques, each run on three different machine learning algorithms namely k-Nearest Neighbours, Random Forests and a Neural Network. Their performance was contrasted to determine the best pre-processing scheme and algorithm pair. Our system achieved an Accuracy of 95.66% to identify American sign language gestures.



### Cross-Modal Information Maximization for Medical Imaging: CMIM
- **Arxiv ID**: http://arxiv.org/abs/2010.10593v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.10593v3)
- **Published**: 2020-10-20 20:05:35+00:00
- **Updated**: 2021-02-01 21:10:37+00:00
- **Authors**: Tristan Sylvain, Francis Dutil, Tess Berthier, Lisa Di Jorio, Margaux Luck, Devon Hjelm, Yoshua Bengio
- **Comment**: ICASSP 2021
- **Journal**: None
- **Summary**: In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.   In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.



### Convolutional 3D to 2D Patch Conversion for Pixel-wise Glioma Segmentation in MRI Scans
- **Arxiv ID**: http://arxiv.org/abs/2010.10612v1
- **DOI**: 10.1007/978-3-030-46640-4_1
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10612v1)
- **Published**: 2020-10-20 20:42:52+00:00
- **Updated**: 2020-10-20 20:42:52+00:00
- **Authors**: Mohammad Hamghalam, Baiying Lei, Tianfu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Structural magnetic resonance imaging (MRI) has been widely utilized for analysis and diagnosis of brain diseases. Automatic segmentation of brain tumors is a challenging task for computer-aided diagnosis due to low-tissue contrast in the tumor subregions. To overcome this, we devise a novel pixel-wise segmentation framework through a convolutional 3D to 2D MR patch conversion model to predict class labels of the central pixel in the input sliding patches. Precisely, we first extract 3D patches from each modality to calibrate slices through the squeeze and excitation (SE) block. Then, the output of the SE block is fed directly into subsequent bottleneck layers to reduce the number of channels. Finally, the calibrated 2D slices are concatenated to obtain multimodal features through a 2D convolutional neural network (CNN) for prediction of the central pixel. In our architecture, both local inter-slice and global intra-slice features are jointly exploited to predict class label of the central voxel in a given patch through the 2D CNN classifier. We implicitly apply all modalities through trainable parameters to assign weights to the contributions of each sequence for segmentation. Experimental results on the segmentation of brain tumors in multimodal MRI scans (BraTS'19) demonstrate that our proposed method can efficiently segment the tumor regions.



### ENSURE: A General Approach for Unsupervised Training of Deep Image Reconstruction Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2010.10631v4
- **DOI**: 10.1109/TMI.2022.3224359
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.10631v4)
- **Published**: 2020-10-20 21:18:33+00:00
- **Updated**: 2022-12-02 14:13:14+00:00
- **Authors**: Hemant Kumar Aggarwal, Aniket Pramanik, Maneesh John, Mathews Jacob
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 2022
- **Summary**: Image reconstruction using deep learning algorithms offers improved reconstruction quality and lower reconstruction time than classical compressed sensing and model-based algorithms. Unfortunately, clean and fully sampled ground-truth data to train the deep networks is often unavailable in several applications, restricting the applicability of the above methods. We introduce a novel metric termed the ENsemble Stein's Unbiased Risk Estimate (ENSURE) framework, which can be used to train deep image reconstruction algorithms without fully sampled and noise-free images. The proposed framework is the generalization of the classical SURE and GSURE formulation to the setting where the images are sampled by different measurement operators, chosen randomly from a set. We evaluate the expectation of the GSURE loss functions over the sampling patterns to obtain the ENSURE loss function. We show that this loss is an unbiased estimate for the true mean-square error, which offers a better alternative to GSURE, which only offers an unbiased estimate for the projected error. Our experiments show that the networks trained with this loss function can offer reconstructions comparable to the supervised setting. While we demonstrate this framework in the context of MR image recovery, the ENSURE framework is generally applicable to arbitrary inverse problems.



### Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/2010.10637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.10637v2)
- **Published**: 2020-10-20 21:42:18+00:00
- **Updated**: 2021-06-05 15:09:55+00:00
- **Authors**: Xiaofeng Liu, Linghao Jin, Xu Han, Jane You
- **Comment**: Published in Pattern Recognition
- **Journal**: None
- **Summary**: How to extract effective expression representations that invariant to the identity-specific attributes is a long-lasting problem for facial expression recognition (FER). Most of the previous methods process the RGB images of a sequence, while we argue that the off-the-shelf and valuable expression-related muscle movement is already embedded in the compression format. In this paper, we target to explore the inter-subject variations eliminated facial expression representation in the compressed video domain. In the up to two orders of magnitude compressed domain, we can explicitly infer the expression from the residual frames and possibly extract identity factors from the I frame with a pre-trained face recognition network. By enforcing the marginal independence of them, the expression feature is expected to be purer for the expression and be robust to identity shifts. Specifically, we propose a novel collaborative min-min game for mutual information (MI) minimization in latent space. We do not need the identity label or multiple expression samples from the same person for identity elimination. Moreover, when the apex frame is annotated in the dataset, the complementary constraint can be further added to regularize the feature-level game. In testing, only the compressed residual frames are required to achieve expression prediction. Our solution can achieve comparable or better performance than the recent decoded image-based methods on the typical FER benchmarks with about 3 times faster inference.



### Towards End-to-End In-Image Neural Machine Translation
- **Arxiv ID**: http://arxiv.org/abs/2010.10648v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.10648v1)
- **Published**: 2020-10-20 22:20:04+00:00
- **Updated**: 2020-10-20 22:20:04+00:00
- **Authors**: Elman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob Uszkoreit, Puneet Jain
- **Comment**: Accepted as an oral presentation at EMNLP, NLP Beyond Text workshop,
  2020
- **Journal**: None
- **Summary**: In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.



### Exploring Overcomplete Representations for Single Image Deraining using CNNs
- **Arxiv ID**: http://arxiv.org/abs/2010.10661v1
- **DOI**: 10.1109/JSTSP.2020.3039393
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.10661v1)
- **Published**: 2020-10-20 22:55:02+00:00
- **Updated**: 2020-10-20 22:55:02+00:00
- **Authors**: Rajeev Yasarla, Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2020
- **Summary**: Removal of rain streaks from a single image is an extremely challenging problem since the rainy images often contain rain streaks of different size, shape, direction and density. Most recent methods for deraining use a deep network following a generic "encoder-decoder" architecture which captures low-level features across the initial layers and high-level features in the deeper layers. For the task of deraining, the rain streaks which are to be removed are relatively small and focusing much on global features is not an efficient way to solve the problem. To this end, we propose using an overcomplete convolutional network architecture which gives special attention in learning local structures by restraining the receptive field of filters. We combine it with U-Net so that it does not lose out on the global structures as well while focusing more on low-level features, to compute the derained image. The proposed network called, Over-and-Under Complete Deraining Network (OUCD), consists of two branches: overcomplete branch which is confined to small receptive field size in order to focus on the local structures and an undercomplete branch that has larger receptive fields to primarily focus on global structures. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods.



