# Arxiv Papers in cs.CV on 2020-10-06
### Video Anomaly Detection Using Pre-Trained Deep Convolutional Neural Nets and Context Mining
- **Arxiv ID**: http://arxiv.org/abs/2010.02406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02406v1)
- **Published**: 2020-10-06 00:26:14+00:00
- **Updated**: 2020-10-06 00:26:14+00:00
- **Authors**: Chongke Wu, Sicong Shao, Cihan Tunc, Salim Hariri
- **Comment**: Accepted by AICCSA 2020
- **Journal**: None
- **Summary**: Anomaly detection is critically important for intelligent surveillance systems to detect in a timely manner any malicious activities. Many video anomaly detection approaches using deep learning methods focus on a single camera video stream with a fixed scenario. These deep learning methods use large-scale training data with large complexity. As a solution, in this paper, we show how to use pre-trained convolutional neural net models to perform feature extraction and context mining, and then use denoising autoencoder with relatively low model complexity to provide efficient and accurate surveillance anomaly detection, which can be useful for the resource-constrained devices such as edge devices of the Internet of Things (IoT). Our anomaly detection model makes decisions based on the high-level features derived from the selected embedded computer vision models such as object classification and object detection. Additionally, we derive contextual properties from the high-level features to further improve the performance of our video anomaly detection method. We use two UCSD datasets to demonstrate that our approach with relatively low model complexity can achieve comparable performance compared to the state-of-the-art approaches.



### Pathological Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.12435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12435v1)
- **Published**: 2020-10-06 00:36:55+00:00
- **Updated**: 2020-10-06 00:36:55+00:00
- **Authors**: Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, Pengtao Xie
- **Comment**: arXiv admin note: text overlap with arXiv:2003.10286
- **Journal**: None
- **Summary**: Is it possible to develop an "AI Pathologist" to pass the board-certified examination of the American Board of Pathology (ABP)? To build such a system, three challenges need to be addressed. First, we need to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Due to privacy concerns, pathology images are usually not publicly available. Besides, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. The second challenge is: since it is difficult to hire highly experienced pathologists to create pathology visual questions and answers, the resulting pathology VQA dataset may contain errors. Training pathology VQA models using these noisy or even erroneous data will lead to problematic models that cannot generalize well on unseen images. The third challenge is: the medical concepts and knowledge covered in pathology question-answer (QA) pairs are very diverse while the number of QA pairs available for modeling training is limited. How to learn effective representations of diverse medical concepts based on limited data is technically demanding. In this paper, we aim to address these three challenges. To our best knowledge, our work represents the first one addressing the pathology VQA problem. To deal with the issue that a publicly available pathology VQA dataset is lacking, we create PathVQA dataset. To address the second challenge, we propose a learning-by-ignoring approach. To address the third challenge, we propose to use cross-modal self-supervised learning. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed learning-by-ignoring method and cross-modal self-supervised learning methods.



### ASDN: A Deep Convolutional Network for Arbitrary Scale Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.02414v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02414v1)
- **Published**: 2020-10-06 01:18:46+00:00
- **Updated**: 2020-10-06 01:18:46+00:00
- **Authors**: Jialiang Shen, Yucheng Wang, Jian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have significantly improved the peak signal-to-noise ratio of SuperResolution (SR). However, image viewer applications commonly allow users to zoom the images to arbitrary magnification scales, thus far imposing a large number of required training scales at a tremendous computational cost. To obtain a more computationally efficient model for arbitrary scale SR, this paper employs a Laplacian pyramid method to reconstruct any-scale high-resolution (HR) images using the high-frequency image details in a Laplacian Frequency Representation. For SR of small-scales (between 1 and 2), images are constructed by interpolation from a sparse set of precalculated Laplacian pyramid levels. SR of larger scales is computed by recursion from small scales, which significantly reduces the computational cost. For a full comparison, fixed- and any-scale experiments are conducted using various benchmarks. At fixed scales, ASDN outperforms predefined upsampling methods (e.g., SRCNN, VDSR, DRRN) by about 1 dB in PSNR. At any-scale, ASDN generally exceeds Meta-SR on many scales.



### The Effectiveness of Memory Replay in Large Scale Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.02418v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02418v1)
- **Published**: 2020-10-06 01:23:12+00:00
- **Updated**: 2020-10-06 01:23:12+00:00
- **Authors**: Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, Ang Li
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: We study continual learning in the large scale setting where tasks in the input sequence are not limited to classification, and the outputs can be of high dimension. Among multiple state-of-the-art methods, we found vanilla experience replay (ER) still very competitive in terms of both performance and scalability, despite its simplicity. However, a degraded performance is observed for ER with small memory. A further visualization of the feature space reveals that the intermediate representation undergoes a distributional drift. While existing methods usually replay only the input-output pairs, we hypothesize that their regularization effect is inadequate for complex deep models and diverse tasks with small replay buffer size. Following this observation, we propose to replay the activation of the intermediate layers in addition to the input-output pairs. Considering that saving raw activation maps can dramatically increase memory and compute cost, we propose the Compressed Activation Replay technique, where compressed representations of layer activation are saved to the replay buffer. We show that this approach can achieve superior regularization effect while adding negligible memory overhead to replay method. Experiments on both the large-scale Taskonomy benchmark with a diverse set of tasks and standard common datasets (Split-CIFAR and Split-miniImageNet) demonstrate the effectiveness of the proposed method.



### Shot in the Dark: Few-Shot Learning with No Base-Class Labels
- **Arxiv ID**: http://arxiv.org/abs/2010.02430v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02430v2)
- **Published**: 2020-10-06 02:05:27+00:00
- **Updated**: 2021-04-23 01:16:11+00:00
- **Authors**: Zitian Chen, Subhransu Maji, Erik Learned-Miller
- **Comment**: Learning from Limited or Imperfect Data (L2ID) Workshop @ CVPR 2021
- **Journal**: None
- **Summary**: Few-shot learning aims to build classifiers for new classes from a small number of labeled examples and is commonly facilitated by access to examples from a distinct set of 'base classes'. The difference in data distribution between the test set (novel classes) and the base classes used to learn an inductive bias often results in poor generalization on the novel classes. To alleviate problems caused by the distribution shift, previous research has explored the use of unlabeled examples from the novel classes, in addition to labeled examples of the base classes, which is known as the transductive setting. In this work, we show that, surprisingly, off-the-shelf self-supervised learning outperforms transductive few-shot methods by 3.9% for 5-shot accuracy on miniImageNet without using any base class labels. This motivates us to examine more carefully the role of features learned through self-supervision in few-shot learning. Comprehensive experiments are conducted to compare the transferability, robustness, efficiency, and the complementarity of supervised and self-supervised features.



### Collaboratively boosting data-driven deep learning and knowledge-guided ontological reasoning for semantic segmentation of remote sensing imagery
- **Arxiv ID**: http://arxiv.org/abs/2010.02451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.02451v1)
- **Published**: 2020-10-06 03:32:17+00:00
- **Updated**: 2020-10-06 03:32:17+00:00
- **Authors**: Yansheng Li, Song Ouyang, Yongjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As one kind of architecture from the deep learning family, deep semantic segmentation network (DSSN) achieves a certain degree of success on the semantic segmentation task and obviously outperforms the traditional methods based on hand-crafted features. As a classic data-driven technique, DSSN can be trained by an end-to-end mechanism and competent for employing the low-level and mid-level cues (i.e., the discriminative image structure) to understand images, but lacks the high-level inference ability. By contrast, human beings have an excellent inference capacity and can be able to reliably interpret the RS imagery only when human beings master the basic RS domain knowledge. In literature, ontological modeling and reasoning is an ideal way to imitate and employ the domain knowledge of human beings, but is still rarely explored and adopted in the RS domain. To remedy the aforementioned critical limitation of DSSN, this paper proposes a collaboratively boosting framework (CBF) to combine data-driven deep learning module and knowledge-guided ontological reasoning module in an iterative way.



### Downscaling Attack and Defense: Turning What You See Back Into What You Get
- **Arxiv ID**: http://arxiv.org/abs/2010.02456v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02456v2)
- **Published**: 2020-10-06 03:41:05+00:00
- **Updated**: 2020-10-07 18:24:29+00:00
- **Authors**: Andrew J. Lohn
- **Comment**: None
- **Journal**: None
- **Summary**: The resizing of images, which is typically a required part of preprocessing for computer vision systems, is vulnerable to attack. Images can be created such that the image is completely different at machine-vision scales than at other scales and the default settings for some common computer vision and machine learning systems are vulnerable. We show that defenses exist and are trivial to administer provided that defenders are aware of the threat. These attacks and defenses help to establish the role of input sanitization in machine learning.



### Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2010.02467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.02467v1)
- **Published**: 2020-10-06 04:18:18+00:00
- **Updated**: 2020-10-06 04:18:18+00:00
- **Authors**: Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley
- **Comment**: 7 pages, 2 figures, to be published in Findings of EMNLP 2020
- **Journal**: None
- **Summary**: Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists' workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g.~label imbalance) and face common issues inherent in text generation models (e.g.~repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.



### Visualizing Color-wise Saliency of Black-Box Image Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2010.02468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02468v1)
- **Published**: 2020-10-06 04:27:18+00:00
- **Updated**: 2020-10-06 04:27:18+00:00
- **Authors**: Yuhki Hatakeyama, Hiroki Sakuma, Yoshinori Konishi, Kohei Suenaga
- **Comment**: To appear in ACCV 2020
- **Journal**: None
- **Summary**: Image classification based on machine learning is being commonly used. However, a classification result given by an advanced method, including deep learning, is often hard to interpret. This problem of interpretability is one of the major obstacles in deploying a trained model in safety-critical systems. Several techniques have been proposed to address this problem; one of which is RISE, which explains a classification result by a heatmap, called a saliency map, which explains the significance of each pixel. We propose MC-RISE (Multi-Color RISE), which is an enhancement of RISE to take color information into account in an explanation. Our method not only shows the saliency of each pixel in a given image as the original RISE does, but the significance of color components of each pixel; a saliency map with color information is useful especially in the domain where the color information matters (e.g., traffic-sign recognition). We implemented MC-RISE and evaluate them using two datasets (GTSRB and ImageNet) to demonstrate the effectiveness of our methods in comparison with existing techniques for interpreting image classification results.



### Joint COCO and Mapillary Workshop at ICCV 2019: COCO Instance Segmentation Challenge Track
- **Arxiv ID**: http://arxiv.org/abs/2010.02475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02475v1)
- **Published**: 2020-10-06 04:49:37+00:00
- **Updated**: 2020-10-06 04:49:37+00:00
- **Authors**: Zeming Li, Yuchen Ma, Yukang Chen, Xiangyu Zhang, Jian Sun
- **Comment**: 1st Place Technical Report in ICCV2019/ ECCV2020: MegDetV2
- **Journal**: None
- **Summary**: In this report, we present our object detection/instance segmentation system, MegDetV2, which works in a two-pass fashion, first to detect instances then to obtain segmentation. Our baseline detector is mainly built on a new designed RPN, called RPN++. On the COCO-2019 detection/instance-segmentation test-dev dataset, our system achieves 61.0/53.1 mAP, which surpassed our 2018 winning results by 5.0/4.2 respectively. We achieve the best results in COCO Challenge 2019 and 2020.



### Exposure Trajectory Recovery from Motion Blur
- **Arxiv ID**: http://arxiv.org/abs/2010.02484v2
- **DOI**: 10.1109/TPAMI.2021.3116135
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02484v2)
- **Published**: 2020-10-06 05:23:33+00:00
- **Updated**: 2021-10-04 09:10:55+00:00
- **Authors**: Youjian Zhang, Chaoyue Wang, Stephen J. Maybank, Dacheng Tao
- **Comment**: Accpeted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Motion blur in dynamic scenes is an important yet challenging research topic. Recently, deep learning methods have achieved impressive performance for dynamic scene deblurring. However, the motion information contained in a blurry image has yet to be fully explored and accurately formulated because: (i) the ground truth of dynamic motion is difficult to obtain; (ii) the temporal ordering is destroyed during the exposure; and (iii) the motion estimation from a blurry image is highly ill-posed. By revisiting the principle of camera exposure, motion blur can be described by the relative motions of sharp content with respect to each exposed position. In this paper, we define exposure trajectories, which represent the motion information contained in a blurry image and explain the causes of motion blur. A novel motion offset estimation framework is proposed to model pixel-wise displacements of the latent sharp image at multiple timepoints. Under mild constraints, our method can recover dense, (non-)linear exposure trajectories, which significantly reduce temporal disorder and ill-posed problems. Finally, experiments demonstrate that the recovered exposure trajectories not only capture accurate and interpretable motion information from a blurry image, but also benefit motion-aware image deblurring and warping-based video extraction tasks. Codes are available on https://github.com/yjzhang96/Motion-ETR.



### RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/2010.02488v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02488v3)
- **Published**: 2020-10-06 05:34:39+00:00
- **Updated**: 2020-10-25 11:52:31+00:00
- **Authors**: Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley
- **Comment**: International Conference on 3D Vision (3DV), 2020 (Oral)
- **Journal**: None
- **Summary**: Although 3D Convolutional Neural Networks (CNNs) are essential for most learning based applications involving dense 3D data, their applicability is limited due to excessive memory and computational requirements. Compressing such networks by pruning therefore becomes highly desirable. However, pruning 3D CNNs is largely unexplored possibly because of the complex nature of typical pruning algorithms that embeds pruning into an iterative optimization paradigm. In this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes 3D CNNs at initialization to high sparsity levels. Specifically, the core idea is to obtain an importance score for each neuron based on their sensitivity to the loss function. This neuron importance is then reweighted according to the neuron resource consumption related to FLOPs or memory. We demonstrate the effectiveness of our pruning method on 3D semantic segmentation with widely used 3D-UNets on ShapeNet and BraTS'18 as well as on video classification with MobileNetV2 and I3D on UCF101 dataset. In these experiments, our RANP leads to roughly 50-95 reduction in FLOPs and 35-80 reduction in memory with negligible loss in accuracy compared to the unpruned networks. This significantly reduces the computational resources required to train 3D CNNs. The pruned network obtained by our algorithm can also be easily scaled up and transferred to another dataset for training.



### Descriptive analysis of computational methods for automating mammograms with practical applications
- **Arxiv ID**: http://arxiv.org/abs/2010.03378v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03378v1)
- **Published**: 2020-10-06 05:40:26+00:00
- **Updated**: 2020-10-06 05:40:26+00:00
- **Authors**: Aparna Bhale, Manish Joshi
- **Comment**: 33 pages and 2 Figures. A review paper of the research work related
  to mamography
- **Journal**: None
- **Summary**: Mammography is a vital screening technique for early revealing and identification of breast cancer in order to assist to decrease mortality rate. Practical applications of mammograms are not limited to breast cancer revealing, identification ,but include task based lens design, image compression, image classification, content based image retrieval and a host of others. Mammography computational analysis methods are a useful tool for specialists to reveal hidden features and extract significant information in mammograms. Digital mammograms are mammography images available along with the conventional screen-film mammography to make automation of mammograms easier. In this paper, we descriptively discuss computational advancement in digital mammograms to serve as a compass for research and practice in the domain of computational mammography and related fields. The discussion focuses on research aiming at a variety of applications and automations of mammograms. It covers different perspectives on image pre-processing, feature extraction, application of mammograms, screen-film mammogram, digital mammogram and development of benchmark corpora for experimenting with digital mammograms.



### Denoising Diffusion Implicit Models
- **Arxiv ID**: http://arxiv.org/abs/2010.02502v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02502v4)
- **Published**: 2020-10-06 06:15:51+00:00
- **Updated**: 2022-10-05 20:19:21+00:00
- **Authors**: Jiaming Song, Chenlin Meng, Stefano Ermon
- **Comment**: ICLR 2021; updated connections with ODEs at page 6, fixed some typos
  in the proof
- **Journal**: None
- **Summary**: Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \times$ to $50 \times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.



### PCAL: A Privacy-preserving Intelligent Credit Risk Modeling Framework Based on Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.02529v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02529v1)
- **Published**: 2020-10-06 07:04:59+00:00
- **Updated**: 2020-10-06 07:04:59+00:00
- **Authors**: Yuli Zheng, Zhenyu Wu, Ye Yuan, Tianlong Chen, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Credit risk modeling has permeated our everyday life. Most banks and financial companies use this technique to model their clients' trustworthiness. While machine learning is increasingly used in this field, the resulting large-scale collection of user private information has reinvigorated the privacy debate, considering dozens of data breach incidents every year caused by unauthorized hackers, and (potentially even more) information misuse/abuse by authorized parties. To address those critical concerns, this paper proposes a framework of Privacy-preserving Credit risk modeling based on Adversarial Learning (PCAL). PCAL aims to mask the private information inside the original dataset, while maintaining the important utility information for the target prediction task performance, by (iteratively) weighing between a privacy-risk loss and a utility-oriented loss. PCAL is compared against off-the-shelf options in terms of both utility and privacy protection. Results indicate that PCAL can learn an effective, privacy-free representation from user data, providing a solid foundation towards privacy-preserving machine learning for credit risk analysis.



### Secure 3D medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.03367v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03367v1)
- **Published**: 2020-10-06 07:21:11+00:00
- **Updated**: 2020-10-06 07:21:11+00:00
- **Authors**: Shadi Al-Zu'bi
- **Comment**: 24 Pages, 4 Tables, 6 Figures
- **Journal**: None
- **Summary**: Image segmentation has proved its importance and plays an important role in various domains such as health systems and satellite-oriented military applications. In this context, accuracy, image quality, and execution time deem to be the major issues to always consider. Although many techniques have been applied, and their experimental results have shown appealing achievements for 2D images in real-time environments, however, there is a lack of works about 3D image segmentation despite its importance in improving segmentation accuracy. Specifically, HMM was used in this domain. However, it suffers from the time complexity, which was updated using different accelerators. As it is important to have efficient 3D image segmentation, we propose in this paper a novel system for partitioning the 3D segmentation process across several distributed machines. The concepts behind distributed multi-media network segmentation were employed to accelerate the segmentation computational time of training Hidden Markov Model (HMMs). Furthermore, a secure transmission has been considered in this distributed environment and various bidirectional multimedia security algorithms have been applied. The contribution of this work lies in providing an efficient and secure algorithm for 3D image segmentation. Through a number of extensive experiments, it was proved that our proposed system is of comparable efficiency to the state of art methods in terms of segmentation accuracy, security and execution time.



### Deep Neural Network: An Efficient and Optimized Machine Learning Paradigm for Reducing Genome Sequencing Error
- **Arxiv ID**: http://arxiv.org/abs/2010.03420v1
- **DOI**: None
- **Categories**: **q-bio.GN**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03420v1)
- **Published**: 2020-10-06 08:16:35+00:00
- **Updated**: 2020-10-06 08:16:35+00:00
- **Authors**: Ferdinand Kartriku, Dr. Robert Sowah, Charles Saah
- **Comment**: for associated mpeg file, see https://ijettjournal.org/
- **Journal**: None
- **Summary**: Genomic data I used in many fields but, it has become known that most of the platforms used in the sequencing process produce significant errors. This means that the analysis and inferences generated from these data may have some errors that need to be corrected. On the two main types of genome errors - substitution and indels - our work is focused on correcting indels. A deep learning approach was used to correct the errors in sequencing the chosen dataset



### Training Deep Neural Networks for Wireless Sensor Networks Using Loosely and Weakly Labeled Images
- **Arxiv ID**: http://arxiv.org/abs/2010.02546v1
- **DOI**: 10.1016/j.neucom.2020.09.040
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02546v1)
- **Published**: 2020-10-06 08:28:34+00:00
- **Updated**: 2020-10-06 08:28:34+00:00
- **Authors**: Qianwei Zhou, Yuhang Chen, Baoqing Li, Xiaoxin Li, Chen Zhou, Jingchang Huang, Haigen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning has achieved remarkable successes over the past years, few reports have been published about applying deep neural networks to Wireless Sensor Networks (WSNs) for image targets recognition where data, energy, computation resources are limited. In this work, a Cost-Effective Domain Generalization (CEDG) algorithm has been proposed to train an efficient network with minimum labor requirements. CEDG transfers networks from a publicly available source domain to an application-specific target domain through an automatically allocated synthetic domain. The target domain is isolated from parameters tuning and used for model selection and testing only. The target domain is significantly different from the source domain because it has new target categories and is consisted of low-quality images that are out of focus, low in resolution, low in illumination, low in photographing angle. The trained network has about 7M (ResNet-20 is about 41M) multiplications per prediction that is small enough to allow a digital signal processor chip to do real-time recognitions in our WSN. The category-level averaged error on the unseen and unbalanced target domain has been decreased by 41.12%.



### OCT-GAN: Single Step Shadow and Noise Removal from Optical Coherence Tomography Images of the Human Optic Nerve Head
- **Arxiv ID**: http://arxiv.org/abs/2010.11698v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11698v1)
- **Published**: 2020-10-06 08:32:32+00:00
- **Updated**: 2020-10-06 08:32:32+00:00
- **Authors**: Haris Cheong, Sripad Krishna Devalla, Thanadet Chuangsuwanich, Tin A. Tun, Xiaofei Wang, Tin Aung, Leopold Schmetterer, Martin L. Buist, Craig Boote, Alexandre H. Thiéry, Michaël J. A. Girard
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: Speckle noise and retinal shadows within OCT B-scans occlude important edges, fine textures and deep tissues, preventing accurate and robust diagnosis by algorithms and clinicians. We developed a single process that successfully removed both noise and retinal shadows from unseen single-frame B-scans within 10.4ms. Mean average gradient magnitude (AGM) for the proposed algorithm was 57.2% higher than current state-of-the-art, while mean peak signal to noise ratio (PSNR), contrast to noise ratio (CNR), and structural similarity index metric (SSIM) increased by 11.1%, 154% and 187% respectively compared to single-frame B-scans. Mean intralayer contrast (ILC) improvement for the retinal nerve fiber layer (RNFL), photoreceptor layer (PR) and retinal pigment epithelium (RPE) layers decreased from 0.362 \pm 0.133 to 0.142 \pm 0.102, 0.449 \pm 0.116 to 0.0904 \pm 0.0769, 0.381 \pm 0.100 to 0.0590 \pm 0.0451 respectively. The proposed algorithm reduces the necessity for long image acquisition times, minimizes expensive hardware requirements and reduces motion artifacts in OCT images.



### Memory-efficient GAN-based Domain Translation of High Resolution 3D Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2010.03396v1
- **DOI**: 10.1016/j.compmedimag.2020.101801
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03396v1)
- **Published**: 2020-10-06 08:43:27+00:00
- **Updated**: 2020-10-06 08:43:27+00:00
- **Authors**: Hristina Uzunova, Jan Ehrhardt, Heinz Handels
- **Comment**: Accepted for Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are currently rarely applied on 3D medical images of large size, due to their immense computational demand. The present work proposes a multi-scale patch-based GAN approach for establishing unpaired domain translation by generating 3D medical image volumes of high resolution in a memory-efficient way. The key idea to enable memory-efficient image generation is to first generate a low-resolution version of the image followed by the generation of patches of constant sizes but successively growing resolutions. To avoid patch artifacts and incorporate global information, the patch generation is conditioned on patches from previous resolution scales. Those multi-scale GANs are trained to generate realistically looking images from image sketches in order to perform an unpaired domain translation. This allows to preserve the topology of the test data and generate the appearance of the training domain data. The evaluation of the domain translation scenarios is performed on brain MRIs of size 155x240x240 and thorax CTs of size up to 512x512x512. Compared to common patch-based approaches, the multi-resolution scheme enables better image quality and prevents patch artifacts. Also, it ensures constant GPU memory demand independent from the image size, allowing for the generation of arbitrarily large images.



### Arbitrary Style Transfer using Graph Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/2010.02560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02560v1)
- **Published**: 2020-10-06 09:07:20+00:00
- **Updated**: 2020-10-06 09:07:20+00:00
- **Authors**: Dongki Jung, Seunghan Yang, Jaehoon Choi, Changick Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer is the image synthesis task, which applies a style of one image to another while preserving the content. In statistical methods, the adaptive instance normalization (AdaIN) whitens the source images and applies the style of target images through normalizing the mean and variance of features. However, computing feature statistics for each instance would neglect the inherent relationship between features, so it is hard to learn global styles while fitting to the individual training dataset. In this paper, we present a novel learnable normalization technique for style transfer using graph convolutional networks, termed Graph Instance Normalization (GrIN). This algorithm makes the style transfer approach more robust by taking into account similar information shared between instances. Besides, this simple module is also applicable to other tasks like image-to-image translation or domain adaptation.



### Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.02582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.02582v1)
- **Published**: 2020-10-06 09:46:20+00:00
- **Updated**: 2020-10-06 09:46:20+00:00
- **Authors**: Wei Han, Hantao Huang, Tao Han
- **Comment**: Accepted in COLING2020
- **Journal**: None
- **Summary**: Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.



### Comprehensive Online Network Pruning via Learnable Scaling Factors
- **Arxiv ID**: http://arxiv.org/abs/2010.02623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02623v1)
- **Published**: 2020-10-06 11:04:17+00:00
- **Updated**: 2020-10-06 11:04:17+00:00
- **Authors**: Muhammad Umair Haider, Murtaza Taj
- **Comment**: Submitted to WACV2021
- **Journal**: None
- **Summary**: One of the major challenges in deploying deep neural network architectures is their size which has an adverse effect on their inference time and memory requirements. Deep CNNs can either be pruned width-wise by removing filters based on their importance or depth-wise by removing layers and blocks. Width wise pruning (filter pruning) is commonly performed via learnable gates or switches and sparsity regularizers whereas pruning of layers has so far been performed arbitrarily by manually designing a smaller network usually referred to as a student network. We propose a comprehensive pruning strategy that can perform both width-wise as well as depth-wise pruning. This is achieved by introducing gates at different granularities (neuron, filter, layer, block) which are then controlled via an objective function that simultaneously performs pruning at different granularity during each forward pass. Our approach is applicable to wide-variety of architectures without any constraints on spatial dimensions or connection type (sequential, residual, parallel or inception). Our method has resulted in a compression ratio of 70% to 90% without noticeable loss in accuracy when evaluated on benchmark datasets.



### Unfolding the Alternating Optimization for Blind Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.02631v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02631v4)
- **Published**: 2020-10-06 11:27:27+00:00
- **Updated**: 2020-11-25 11:02:44+00:00
- **Authors**: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan
- **Comment**: Conference on Neural Information Processing Systems (NeurIPS) 2020
- **Journal**: None
- **Summary**: Previous methods decompose blind super resolution (SR) problem into two sequential steps: \textit{i}) estimating blur kernel from given low-resolution (LR) image and \textit{ii}) restoring SR image based on estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. Small estimation error of the first step could cause severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from LR image, which makes it difficult to predict highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate blur kernel and restore SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores SR image based on predicted kernel, and \textit{Estimator} estimates blur kernel with the help of restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of blur kernel easier. More importantly, \textit{Restorer} is trained with the kernel estimated by \textit{Estimator}, instead of ground-truth kernel, thus \textit{Restorer} could be more tolerant to the estimation error of \textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at much higher speed. The source code is available at https://github.com/greatlog/DAN.git.



### How Convolutional Neural Network Architecture Biases Learned Opponency and Colour Tuning
- **Arxiv ID**: http://arxiv.org/abs/2010.02634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2010.02634v1)
- **Published**: 2020-10-06 11:33:48+00:00
- **Updated**: 2020-10-06 11:33:48+00:00
- **Authors**: Ethan Harris, Daniela Mihai, Jonathon Hare
- **Comment**: Final version; Accepted for publication in Neural Computation
- **Journal**: None
- **Summary**: Recent work suggests that changing Convolutional Neural Network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterising visual systems which permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and colour tuning curves for convolutional neurons, which can be used to classify cells in terms of their spatial and colour opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organisation: almost all cells in the bottleneck layer become both spatially and colour opponent, cells in the layer following the bottleneck become non-opponent. The colour tuning data can further be used to form a rich understanding of how colour is encoded by a network. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex non-linear colour system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We further develop a method of obtaining a hue sensitivity curve for a trained CNN which enables high level insights that complement the low level findings from the colour tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately, our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Code for all experiments is available at https://github.com/ecs-vlc/opponency.



### A Method for Tumor Treating Fields Fast Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.02644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02644v1)
- **Published**: 2020-10-06 11:45:06+00:00
- **Updated**: 2020-10-06 11:45:06+00:00
- **Authors**: Reuben R Shamir, Zeev Bomzon
- **Comment**: None
- **Journal**: None
- **Summary**: Tumor Treating Fields (TTFields) is an FDA approved treatment for specific types of cancer and significantly extends patients life. The intensity of the TTFields within the tumor was associated with the treatment outcomes: the larger the intensity the longer the patients are likely to survive. Therefore, it was suggested to optimize TTFields transducer array location such that their intensity is maximized. Such optimization requires multiple computations of TTFields in a simulation framework. However, these computations are typically performed using finite element methods or similar approaches that are time consuming. Therefore, only a limited number of transducer array locations can be examined in practice. To overcome this issue, we have developed a method for fast estimation of TTFields intensity. We have designed and implemented a method that inputs a segmentation of the patients head, a table of tissues electrical properties and the location of the transducer array. The method outputs a spatial estimation of the TTFields intensity by incorporating a few relevant parameters in a random-forest regressor. The method was evaluated on 10 patients (20 TA layouts) in a leave-one-out framework. The computation time was 1.5 minutes using the suggested method, and 180-240 minutes using the commercial simulation. The average error was 0.14 V/cm (SD = 0.06 V/cm) in comparison to the result of the commercial simulation. These results suggest that a fast estimation of TTFields based on a few parameters is feasible. The presented method may facilitate treatment optimization and further extend patients life.



### Histopathological Stain Transfer using Style Transfer Network with Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/2010.02659v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02659v1)
- **Published**: 2020-10-06 12:10:50+00:00
- **Updated**: 2020-10-06 12:10:50+00:00
- **Authors**: Harshal Nishar, Nikhil Chavanke, Nitin Singhal
- **Comment**: Accepted at MICCAI 2020 Main Track Oral Presentation
- **Journal**: None
- **Summary**: Deep learning models that are trained on histopathological images obtained from a single lab and/or scanner give poor inference performance on images obtained from another scanner/lab with a different staining protocol. In recent years, there has been a good amount of research done for image stain normalization to address this issue. In this work, we present a novel approach for the stain normalization problem using fast neural style transfer coupled with adversarial loss. We also propose a novel stain transfer generator network based on High-Resolution Network (HRNet) which requires less training time and gives good generalization with few paired training images of reference stain and test stain. This approach has been tested on Whole Slide Images (WSIs) obtained from 8 different labs, where images from one lab were treated as a reference stain. A deep learning model was trained on this stain and the rest of the images were transferred to it using the corresponding stain transfer generator network. Experimentation suggests that this approach is able to successfully perform stain normalization with good visual quality and provides better inference performance compared to not applying stain normalization.



### A Data Set and a Convolutional Model for Iconography Classification in Paintings
- **Arxiv ID**: http://arxiv.org/abs/2010.11697v3
- **DOI**: 10.1145/3458885
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11697v3)
- **Published**: 2020-10-06 12:40:46+00:00
- **Updated**: 2021-07-26 12:27:36+00:00
- **Authors**: Federico Milani, Piero Fraternali
- **Comment**: Published at ACM Journal on Computing and Cultural Heritage (JOCCH)
  https://doi.org/10.1145/3458885
- **Journal**: Journal on Computing and Cultural Heritage (JOCCH), 2021, 14.4:
  1-18
- **Summary**: Iconography in art is the discipline that studies the visual content of artworks to determine their motifs and themes andto characterize the way these are represented. It is a subject of active research for a variety of purposes, including the interpretation of meaning, the investigation of the origin and diffusion in time and space of representations, and the study of influences across artists and art works. With the proliferation of digital archives of art images, the possibility arises of applying Computer Vision techniques to the analysis of art images at an unprecedented scale, which may support iconography research and education. In this paper we introduce a novel paintings data set for iconography classification and present the quantitativeand qualitative results of applying a Convolutional Neural Network (CNN) classifier to the recognition of the iconography of artworks. The proposed classifier achieves good performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73% Average Precision) in the task of identifying saints in Christian religious paintings, a task made difficult by the presence of classes with very similar visual features. Qualitative analysis of the results shows that the CNN focuses on the traditional iconic motifs that characterize the representation of each saint and exploits such hints to attain correct identification. The ultimate goal of our work is to enable the automatic extraction, decomposition, and comparison of iconography elements to support iconographic studies and automatic art work annotation.



### Parallax Motion Effect Generation Through Instance Segmentation And Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.02680v1
- **DOI**: 10.1109/ICIP40778.2020.9191168
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02680v1)
- **Published**: 2020-10-06 12:56:59+00:00
- **Updated**: 2020-10-06 12:56:59+00:00
- **Authors**: Allan Pinto, Manuel A. Córdova, Luis G. L. Decker, Jose L. Flores-Campana, Marcos R. Souza, Andreza A. dos Santos, Jhonatas S. Conceição, Henrique F. Gagliardi, Diogo C. Luvizon, Ricardo da S. Torres, Helio Pedrini
- **Comment**: 2020 IEEE International Conference on Image Processing (ICIP), Abu
  Dhabi, United Arab Emirates
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP), Abu
  Dhabi, United Arab Emirates, 2020, pp. 1621-1625
- **Summary**: Stereo vision is a growing topic in computer vision due to the innumerable opportunities and applications this technology offers for the development of modern solutions, such as virtual and augmented reality applications. To enhance the user's experience in three-dimensional virtual environments, the motion parallax estimation is a promising technique to achieve this objective. In this paper, we propose an algorithm for generating parallax motion effects from a single image, taking advantage of state-of-the-art instance segmentation and depth estimation approaches. This work also presents a comparison against such algorithms to investigate the trade-off between efficiency and quality of the parallax motion effects, taking into consideration a multi-task learning network capable of estimating instance segmentation and depth estimation at once. Experimental results and visual quality assessment indicate that the PyD-Net network (depth estimation) combined with Mask R-CNN or FBNet networks (instance segmentation) can produce parallax motion effects with good visual quality.



### Vec2Instance: Parameterization for Deep Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.02725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02725v1)
- **Published**: 2020-10-06 13:51:02+00:00
- **Updated**: 2020-10-06 13:51:02+00:00
- **Authors**: N. Lakmal Deshapriya, Matthew N. Dailey, Manzul Kumar Hazarika, Hiroyuki Miyazaki
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Current advances in deep learning is leading to human-level accuracy in computer vision tasks such as object classification, localization, semantic segmentation, and instance segmentation. In this paper, we describe a new deep convolutional neural network architecture called Vec2Instance for instance segmentation. Vec2Instance provides a framework for parametrization of instances, allowing convolutional neural networks to efficiently estimate the complex shapes of instances around their centroids. We demonstrate the feasibility of the proposed architecture with respect to instance segmentation tasks on satellite images, which have a wide range of applications. Moreover, we demonstrate the usefulness of the new method for extracting building foot-prints from satellite images. Total pixel-wise accuracy of our approach is 89\%, near the accuracy of the state-of-the-art Mask RCNN (91\%). Vec2Instance is an alternative approach to complex instance segmentation pipelines, offering simplicity and intuitiveness. The code developed under this study is available in the Vec2Instance GitHub repository, https://github.com/lakmalnd/Vec2Instance



### Assisted Probe Positioning for Ultrasound Guided Radiotherapy Using Image Sequence Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.02732v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.02732v1)
- **Published**: 2020-10-06 13:55:02+00:00
- **Updated**: 2020-10-06 13:55:02+00:00
- **Authors**: Alexander Grimwood, Helen McNair, Yipeng Hu, Ester Bonmati, Dean Barratt, Emma Harris
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Effective transperineal ultrasound image guidance in prostate external beam radiotherapy requires consistent alignment between probe and prostate at each session during patient set-up. Probe placement and ultrasound image inter-pretation are manual tasks contingent upon operator skill, leading to interoperator uncertainties that degrade radiotherapy precision. We demonstrate a method for ensuring accurate probe placement through joint classification of images and probe position data. Using a multi-input multi-task algorithm, spatial coordinate data from an optically tracked ultrasound probe is combined with an image clas-sifier using a recurrent neural network to generate two sets of predictions in real-time. The first set identifies relevant prostate anatomy visible in the field of view using the classes: outside prostate, prostate periphery, prostate centre. The second set recommends a probe angular adjustment to achieve alignment between the probe and prostate centre with the classes: move left, move right, stop. The algo-rithm was trained and tested on 9,743 clinical images from 61 treatment sessions across 32 patients. We evaluated classification accuracy against class labels de-rived from three experienced observers at 2/3 and 3/3 agreement thresholds. For images with unanimous consensus between observers, anatomical classification accuracy was 97.2% and probe adjustment accuracy was 94.9%. The algorithm identified optimal probe alignment within a mean (standard deviation) range of 3.7$^{\circ}$ (1.2$^{\circ}$) from angle labels with full observer consensus, comparable to the 2.8$^{\circ}$ (2.6$^{\circ}$) mean interobserver range. We propose such an algorithm could assist ra-diotherapy practitioners with limited experience of ultrasound image interpreta-tion by providing effective real-time feedback during patient set-up.



### High Speed Event Camera TRacking
- **Arxiv ID**: http://arxiv.org/abs/2010.02771v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.02771v2)
- **Published**: 2020-10-06 14:41:06+00:00
- **Updated**: 2020-10-13 11:15:22+00:00
- **Authors**: William Chamorro, Juan Andrade-Cetto, Joan Solà
- **Comment**: Presented in BMVC 2020, oral session # 5
- **Journal**: None
- **Summary**: Event cameras are bioinspired sensors with reaction times in the order of microseconds. This property makes them appealing for use in highly-dynamic computer vision applications. In this work,we explore the limits of this sensing technology and present an ultra-fast tracking algorithm able to estimate six-degree-of-freedom motion with dynamics over 25.8 g, at a throughput of 10 kHz,processing over a million events per second. Our method is capable of tracking either camera motion or the motion of an object in front of it, using an error-state Kalman filter formulated in a Lie-theoretic sense. The method includes a robust mechanism for the matching of events with projected line segments with very fast outlier rejection. Meticulous treatment of sparse matrices is applied to achieve real-time performance. Different motion models of varying complexity are considered for the sake of comparison and performance analysis



### Compressing Deep Convolutional Neural Networks by Stacking Low-dimensional Binary Convolution Filters
- **Arxiv ID**: http://arxiv.org/abs/2010.02778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02778v1)
- **Published**: 2020-10-06 14:49:22+00:00
- **Updated**: 2020-10-06 14:49:22+00:00
- **Authors**: Weichao Lan, Liang Lan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) have been successfully applied to many real-life problems. However, the huge memory cost of deep CNN models poses a great challenge of deploying them on memory-constrained devices (e.g., mobile phones). One popular way to reduce the memory cost of deep CNN model is to train binary CNN where the weights in convolution filters are either 1 or -1 and therefore each weight can be efficiently stored using a single bit. However, the compression ratio of existing binary CNN models is upper bounded by around 32. To address this limitation, we propose a novel method to compress deep CNN model by stacking low-dimensional binary convolution filters. Our proposed method approximates a standard convolution filter by selecting and stacking filters from a set of low-dimensional binary convolution filters. This set of low-dimensional binary convolution filters is shared across all filters for a given convolution layer. Therefore, our method will achieve much larger compression ratio than binary CNN models. In order to train our proposed model, we have theoretically shown that our proposed model is equivalent to select and stack intermediate feature maps generated by low-dimensional binary filters. Therefore, our proposed model can be efficiently trained using the split-transform-merge strategy. We also provide detailed analysis of the memory and computation cost of our model in model inference. We compared the proposed method with other five popular model compression techniques on two benchmark datasets. Our experimental results have demonstrated that our proposed method achieves much higher compression ratio than existing methods while maintains comparable accuracy.



### Representation learning from videos in-the-wild: An object-centric approach
- **Arxiv ID**: http://arxiv.org/abs/2010.02808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02808v2)
- **Published**: 2020-10-06 15:17:45+00:00
- **Updated**: 2021-02-09 17:08:17+00:00
- **Authors**: Rob Romijnders, Aravindh Mahendran, Michael Tschannen, Josip Djolonga, Marvin Ritter, Neil Houlsby, Mario Lucic
- **Comment**: Published at WACV 2021
- **Journal**: None
- **Summary**: We propose a method to learn image representations from uncurated videos. We combine a supervised loss from off-the-shelf object detectors and self-supervised losses which naturally arise from the video-shot-frame-object hierarchy present in each video. We report competitive results on 19 transfer learning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8 out-of-distribution-generalization tasks, and discuss the benefits and shortcomings of the proposed approach. In particular, it improves over the baseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution generalization tasks. Finally, we perform several ablation studies and analyze the impact of the pretrained object detector on the performance across this suite of tasks.



### Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2010.02814v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02814v2)
- **Published**: 2020-10-06 15:21:31+00:00
- **Updated**: 2021-04-13 19:54:28+00:00
- **Authors**: Shehroz S. Khan, Faraz Khoshbakhtian, Ahmed Bilal Ashraf
- **Comment**: 9 pages, 3 tables, 3 figures
- **Journal**: None
- **Summary**: The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such asCOVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of data from infected persons could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate the problem of identifying early cases in a pandemic as an anomaly detection problem, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present several unsupervised deep learning approaches, including convolutional and adversarially trained autoencoder. We tested two settings on a publicly available dataset (COVIDx)by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. Afterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays



### Microscopic fine-grained instance classification through deep attention
- **Arxiv ID**: http://arxiv.org/abs/2010.02818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02818v1)
- **Published**: 2020-10-06 15:29:58+00:00
- **Updated**: 2020-10-06 15:29:58+00:00
- **Authors**: Mengran Fan, Tapabrata Chakrabort, Eric I-Chao Chang, Yan Xu, Jens Rittscher
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained classification of microscopic image data with limited samples is an open problem in computer vision and biomedical imaging. Deep learning based vision systems mostly deal with high number of low-resolution images, whereas subtle detail in biomedical images require higher resolution. To bridge this gap, we propose a simple yet effective deep network that performs two tasks simultaneously in an end-to-end manner. First, it utilises a gated attention module that can focus on multiple key instances at high resolution without extra annotations or region proposals. Second, the global structural features and local instance features are fused for final image level classification. The result is a robust but lightweight end-to-end trainable deep network that yields state-of-the-art results in two separate fine-grained multi-instance biomedical image classification tasks: a benchmark breast cancer histology dataset and our new fungi species mycology dataset. In addition, we demonstrate the interpretability of the proposed model by visualising the concordance of the learned features with clinically relevant features.



### Support-set bottlenecks for video-text representation learning
- **Arxiv ID**: http://arxiv.org/abs/2010.02824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02824v2)
- **Published**: 2020-10-06 15:38:54+00:00
- **Updated**: 2021-01-14 10:34:56+00:00
- **Authors**: Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, João Henriques, Andrea Vedaldi
- **Comment**: Accepted as spotlight paper at the International Conference on
  Learning Representations (ICLR) 2021
- **Journal**: None
- **Summary**: The dominant paradigm for learning video-text representations -- noise contrastive learning -- increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related -- for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of other support samples' visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX and ActivityNet, and MSVD for video-to-text and text-to-video retrieval.



### RANDGAN: Randomized Generative Adversarial Network for Detection of COVID-19 in Chest X-ray
- **Arxiv ID**: http://arxiv.org/abs/2010.06418v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06418v1)
- **Published**: 2020-10-06 15:58:09+00:00
- **Updated**: 2020-10-06 15:58:09+00:00
- **Authors**: Saman Motamed, Patrik Rogalla, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19 spread across the globe at an immense rate has left healthcare systems incapacitated to diagnose and test patients at the needed rate. Studies have shown promising results for detection of COVID-19 from viral bacterial pneumonia in chest X-rays. Automation of COVID-19 testing using medical images can speed up the testing process of patients where health care systems lack sufficient numbers of the reverse-transcription polymerase chain reaction (RT-PCR) tests. Supervised deep learning models such as convolutional neural networks (CNN) need enough labeled data for all classes to correctly learn the task of detection. Gathering labeled data is a cumbersome task and requires time and resources which could further strain health care systems and radiologists at the early stages of a pandemic such as COVID-19. In this study, we propose a randomized generative adversarial network (RANDGAN) that detects images of an unknown class (COVID-19) from known and labelled classes (Normal and Viral Pneumonia) without the need for labels and training data from the unknown class of images (COVID-19). We used the largest publicly available COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia, and COVID-19 images from multiple public databases. In this work, we use transfer learning to segment the lungs in the COVIDx dataset. Next, we show why segmentation of the region of interest (lungs) is vital to correctly learn the task of classification, specifically in datasets that contain images from different resources as it is the case for the COVIDx dataset. Finally, we show improved results in detection of COVID-19 cases using our generative model (RANDGAN) compared to conventional generative adversarial networks (GANs) for anomaly detection in medical images, improving the area under the ROC curve from 0.71 to 0.77.



### CoRe: Color Regression for Multicolor Fashion Garments
- **Arxiv ID**: http://arxiv.org/abs/2010.02849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02849v2)
- **Published**: 2020-10-06 16:12:30+00:00
- **Updated**: 2022-05-31 14:39:41+00:00
- **Authors**: Alexandre Rame, Arthur Douillard, Charles Ollion
- **Comment**: 6 pages,3 figures,1 table
- **Journal**: CVPR 2022, Workshop on Computer Vision for Fashion, Art, and
  Design
- **Summary**: Developing deep networks that analyze fashion garments has many real-world applications. Among all fashion attributes, color is one of the most important yet challenging to detect. Existing approaches are classification-based and thus cannot go beyond the list of discrete predefined color names. In this paper, we handle color detection as a regression problem to predict the exact RGB values. That's why in addition to a first color classifier, we include a second regression stage for refinement in our newly proposed architecture. This second step combines two attention models: the first depends on the type of clothing, the second depends on the color previously detected by the classifier. Our final prediction is the weighted spatial pooling over the image pixels RGB values, where the illumination has been corrected. This architecture is modular and easily expanded to detect the RGBs of all colors in a multicolor garment. In our experiments, we show the benefits of each component of our architecture.



### BlendTorch: A Real-Time, Adaptive Domain Randomization Library
- **Arxiv ID**: http://arxiv.org/abs/2010.11696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11696v1)
- **Published**: 2020-10-06 16:13:39+00:00
- **Updated**: 2020-10-06 16:13:39+00:00
- **Authors**: Christoph Heindl, Lukas Brunner, Sebastian Zambal, Josef Scharinger
- **Comment**: None
- **Journal**: None
- **Summary**: Solving complex computer vision tasks by deep learning techniques relies on large amounts of (supervised) image data, typically unavailable in industrial environments. The lack of training data starts to impede the successful transfer of state-of-the-art methods in computer vision to industrial applications. We introduce BlendTorch, an adaptive Domain Randomization (DR) library, to help creating infinite streams of synthetic training data. BlendTorch generates data by massively randomizing low-fidelity simulations and takes care of distributing artificial training data for model learning in real-time. We show that models trained with BlendTorch repeatedly perform better in an industrial object detection task than those trained on real or photo-realistic datasets.



### LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU
- **Arxiv ID**: http://arxiv.org/abs/2010.02871v2
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02871v2)
- **Published**: 2020-10-06 16:51:35+00:00
- **Updated**: 2021-07-27 17:27:45+00:00
- **Authors**: Evgeny Ponomarev, Sergey Matveev, Ivan Oseledets
- **Comment**: Please, don't use this draft version because there are some mistakes
  in data
- **Journal**: None
- **Summary**: A lot of deep learning applications are desired to be run on mobile devices. Both accuracy and inference time are meaningful for a lot of them. While the number of FLOPs is usually used as a proxy for neural network latency, it may be not the best choice. In order to obtain a better approximation of latency, research community uses look-up tables of all possible layers for latency calculation for the final prediction of the inference on mobile CPU. It requires only a small number of experiments. Unfortunately, on mobile GPU this method is not applicable in a straight-forward way and shows low precision. In this work, we consider latency approximation on mobile GPU as a data and hardware-specific problem. Our main goal is to construct a convenient latency estimation tool for investigation(LETI) of neural network inference and building robust and accurate latency prediction models for each specific task. To achieve this goal, we build open-source tools which provide a convenient way to conduct massive experiments on different target devices focusing on mobile GPU. After evaluation of the dataset, we learn the regression model on experimental data and use it for future latency prediction and analysis. We experimentally demonstrate the applicability of such an approach on a subset of popular NAS-Benchmark 101 dataset and also evaluate the most popular neural network architectures for two mobile GPUs. As a result, we construct latency prediction model with good precision on the target evaluation subset. We consider LETI as a useful tool for neural architecture search or massive latency evaluation. The project is available at https://github.com/leti-ai



### Iterative Methods for Computing Eigenvectors of Nonlinear Operators
- **Arxiv ID**: http://arxiv.org/abs/2010.02890v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.AP, math.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.02890v1)
- **Published**: 2020-10-06 17:16:38+00:00
- **Updated**: 2020-10-06 17:16:38+00:00
- **Authors**: Guy Gilboa
- **Comment**: None
- **Journal**: None
- **Summary**: In this chapter we are examining several iterative methods for solving nonlinear eigenvalue problems. These arise in variational image-processing, graph partition and classification, nonlinear physics and more. The canonical eigenproblem we solve is $T(u)=\lambda u$, where $T:\R^n\to \R^n$ is some bounded nonlinear operator. Other variations of eigenvalue problems are also discussed. We present a progression of 5 algorithms, coauthored in recent years by the author and colleagues. Each algorithm attempts to solve a unique problem or to improve the theoretical foundations. The algorithms can be understood as nonlinear PDE's which converge to an eigenfunction in the continuous time domain. This allows a unique view and understanding of the discrete iterative process. Finally, it is shown how to evaluate numerically the results, along with some examples and insights related to priors of nonlinear denoisers, both classical algorithms and ones based on deep networks.



### SAFENet: Self-Supervised Monocular Depth Estimation with Semantic-Aware Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2010.02893v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.02893v3)
- **Published**: 2020-10-06 17:22:25+00:00
- **Updated**: 2020-12-29 07:54:37+00:00
- **Authors**: Jaehoon Choi, Dongki Jung, Donghwan Lee, Changick Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation has emerged as a promising method because it does not require groundtruth depth maps during training. As an alternative for the groundtruth depth map, the photometric loss enables to provide self-supervision on depth prediction by matching the input image frames. However, the photometric loss causes various problems, resulting in less accurate depth values compared with supervised approaches. In this paper, we propose SAFENet that is designed to leverage semantic information to overcome the limitations of the photometric loss. Our key idea is to exploit semantic-aware depth features that integrate the semantic and geometric knowledge. Therefore, we introduce multi-task learning schemes to incorporate semantic-awareness into the representation of depth features. Experiments on KITTI dataset demonstrate that our methods compete or even outperform the state-of-the-art methods. Furthermore, extensive experiments on different datasets show its better generalization ability and robustness to various conditions, such as low-light or adverse weather.



### A Contrastive Learning Approach for Training Variational Autoencoder Priors
- **Arxiv ID**: http://arxiv.org/abs/2010.02917v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.02917v3)
- **Published**: 2020-10-06 17:59:02+00:00
- **Updated**: 2021-11-03 20:01:38+00:00
- **Authors**: Jyoti Aneja, Alexander Schwing, Jan Kautz, Arash Vahdat
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.



### Learning to Represent Image and Text with Denotation Graph
- **Arxiv ID**: http://arxiv.org/abs/2010.02949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.02949v1)
- **Published**: 2020-10-06 18:00:58+00:00
- **Updated**: 2020-10-06 18:00:58+00:00
- **Authors**: Bowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, Fei Sha
- **Comment**: to appear at EMNLP 2020
- **Journal**: None
- **Summary**: Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.



### Using Sentences as Semantic Representations in Large Scale Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.02959v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.02959v1)
- **Published**: 2020-10-06 18:22:21+00:00
- **Updated**: 2020-10-06 18:22:21+00:00
- **Authors**: Yannick Le Cacheux, Hervé Le Borgne, Michel Crucianu
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning aims to recognize instances of unseen classes, for which no visual instance is available during training, by learning multimodal relations between samples from seen classes and corresponding class semantic representations. These class representations usually consist of either attributes, which do not scale well to large datasets, or word embeddings, which lead to poorer performance. A good trade-off could be to employ short sentences in natural language as class descriptions. We explore different solutions to use such short descriptions in a ZSL setting and show that while simple methods cannot achieve very good results with sentences alone, a combination of usual word embeddings and sentences can significantly outperform current state-of-the-art.



### Motion Prediction Using Temporal Inception Module
- **Arxiv ID**: http://arxiv.org/abs/2010.03006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03006v1)
- **Published**: 2020-10-06 20:26:01+00:00
- **Updated**: 2020-10-06 20:26:01+00:00
- **Authors**: Tim Lebailly, Sena Kiciroglu, Mathieu Salzmann, Pascal Fua, Wei Wang
- **Comment**: 16 pages, 4 figures. To appear in the proceedings of the 15th Asian
  Conference on Computer Vision, ACCV 2020
- **Journal**: None
- **Summary**: Human motion prediction is a necessary component for many applications in robotics and autonomous driving. Recent methods propose using sequence-to-sequence deep learning models to tackle this problem. However, they do not focus on exploiting different temporal scales for different length inputs. We argue that the diverse temporal scales are important as they allow us to look at the past frames with different receptive fields, which can lead to better predictions. In this paper, we propose a Temporal Inception Module (TIM) to encode human motion. Making use of TIM, our framework produces input embeddings using convolutional layers, by using different kernel sizes for different input lengths. The experimental results on standard motion prediction benchmark datasets Human3.6M and CMU motion capture dataset show that our approach consistently outperforms the state of the art methods.



### Online Action Detection in Streaming Videos with Time Buffers
- **Arxiv ID**: http://arxiv.org/abs/2010.03016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03016v1)
- **Published**: 2020-10-06 20:43:50+00:00
- **Updated**: 2020-10-06 20:43:50+00:00
- **Authors**: Bowen Zhang, Hao Chen, Meng Wang, Yuanjun Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: We formulate the problem of online temporal action detection in live streaming videos, acknowledging one important property of live streaming videos that there is normally a broadcast delay between the latest captured frame and the actual frame viewed by the audience. The standard setting of the online action detection task requires immediate prediction after a new frame is captured. We illustrate that its lack of consideration of the delay is imposing unnecessary constraints on the models and thus not suitable for this problem. We propose to adopt the problem setting that allows models to make use of the small `buffer time' incurred by the delay in live streaming videos. We design an action start and end detection framework for this online with buffer setting with two major components: flattened I3D and window-based suppression. Experiments on three standard temporal action detection benchmarks under the proposed setting demonstrate the effectiveness of the proposed framework. We show that by having a suitable problem setting for this problem with wide-applications, we can achieve much better detection accuracy than off-the-shelf online action detection models.



### Global Self-Attention Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.03019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.03019v2)
- **Published**: 2020-10-06 20:55:34+00:00
- **Updated**: 2020-10-14 05:46:59+00:00
- **Authors**: Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, Ching-Hui Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a series of works in computer vision have shown promising results on various image and video understanding tasks using self-attention. However, due to the quadratic computational and memory complexities of self-attention, these works either apply attention only to low-resolution feature maps in later stages of a deep network or restrict the receptive field of attention in each layer to a small local region. To overcome these limitations, this work introduces a new global self-attention module, referred to as the GSA module, which is efficient enough to serve as the backbone component of a deep network. This module consists of two parallel layers: a content attention layer that attends to pixels based only on their content and a positional attention layer that attends to pixels based on their spatial locations. The output of this module is the sum of the outputs of the two layers. Based on the proposed GSA module, we introduce new standalone global attention-based deep networks that use GSA modules instead of convolutions to model pixel interactions. Due to the global extent of the proposed GSA module, a GSA network has the ability to model long-range pixel interactions throughout the network. Our experimental results show that GSA networks outperform the corresponding convolution-based networks significantly on the CIFAR-100 and ImageNet datasets while using less parameters and computations. The proposed GSA networks also outperform various existing attention-based networks on the ImageNet dataset.



### IS-CAM: Integrated Score-CAM for axiomatic-based explanations
- **Arxiv ID**: http://arxiv.org/abs/2010.03023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03023v1)
- **Published**: 2020-10-06 21:03:03+00:00
- **Updated**: 2020-10-06 21:03:03+00:00
- **Authors**: Rakshit Naidu, Ankita Ghosh, Yash Maurya, Shamanth R Nayak K, Soumya Snigdha Kundu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks have been known as black-box models as humans cannot interpret their inner functionalities. With an attempt to make CNNs more interpretable and trustworthy, we propose IS-CAM (Integrated Score-CAM), where we introduce the integration operation within the Score-CAM pipeline to achieve visually sharper attribution maps quantitatively. Our method is evaluated on 2000 randomly selected images from the ILSVRC 2012 Validation dataset, which proves the versatility of IS-CAM to account for different models and methods.



### Rotate to Attend: Convolutional Triplet Attention Module
- **Arxiv ID**: http://arxiv.org/abs/2010.03045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03045v2)
- **Published**: 2020-10-06 21:31:00+00:00
- **Updated**: 2020-11-05 19:08:52+00:00
- **Authors**: Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, Qibin Hou
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Benefiting from the capability of building inter-dependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly used in a variety of computer vision tasks recently. In this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For an input tensor, triplet attention builds inter-dimensional dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial information with negligible computational overhead. Our method is simple as well as efficient and can be easily plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive in-sight into the performance of triplet attention by visually inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our method supports our intuition on the importance of capturing dependencies across dimensions when computing attention weights. Code for this paper can be publicly accessed at https://github.com/LandskapeAI/triplet-attention



### Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.03060v5
- **DOI**: 10.1109/JBHI.2021.3110805
- **Categories**: **cs.LG**, cs.CL, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03060v5)
- **Published**: 2020-10-06 22:20:29+00:00
- **Updated**: 2021-09-09 00:25:53+00:00
- **Authors**: Gongbo Liang, Connor Greenwell, Yu Zhang, Xiaoqin Wang, Ramakanth Kavuluru, Nathan Jacobs
- **Comment**: This work is accepted to the IEEE Journal of Biomedical and Health
  Informatics
- **Journal**: None
- **Summary**: A key challenge in training neural networks for a given medical imaging task is often the difficulty of obtaining a sufficient number of manually labeled examples. In contrast, textual imaging reports, which are often readily available in medical records, contain rich but unstructured interpretations written by experts as part of standard clinical practice. We propose using these textual reports as a form of weak supervision to improve the image interpretation performance of a neural network without requiring additional manually labeled examples. We use an image-text matching task to train a feature extractor and then fine-tune it in a transfer learning setting for a supervised task using a small labeled dataset. The end result is a neural network that automatically interprets imagery without requiring textual reports during inference. This approach can be applied to any task for which text-image pairs are readily available. We evaluate our method on three classification tasks and find consistent performance improvements, reducing the need for labeled data by 67%-98%.



### Domain Adaptive Transfer Learning on Visual Attention Aware Data Augmentation for Fine-grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2010.03071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03071v1)
- **Published**: 2020-10-06 22:47:57+00:00
- **Updated**: 2020-10-06 22:47:57+00:00
- **Authors**: Ashiq Imran, Vassilis Athitsos
- **Comment**: 18 pages, 12 figures, 4 tables
- **Journal**: Will be published in ISVC 2020
- **Summary**: Fine-Grained Visual Categorization (FGVC) is a challenging topic in computer vision. It is a problem characterized by large intra-class differences and subtle inter-class differences. In this paper, we tackle this problem in a weakly supervised manner, where neural network models are getting fed with additional data using a data augmentation technique through a visual attention mechanism. We perform domain adaptive knowledge transfer via fine-tuning on our base network model. We perform our experiment on six challenging and commonly used FGVC datasets, and we show competitive improvement on accuracies by using attention-aware data augmentation techniques with features derived from deep learning model InceptionV3, pre-trained on large scale datasets. Our method outperforms competitor methods on multiple FGVC datasets and showed competitive results on other datasets. Experimental studies show that transfer learning from large scale datasets can be utilized effectively with visual attention based data augmentation, which can obtain state-of-the-art results on several FGVC datasets. We present a comprehensive analysis of our experiments. Our method achieves state-of-the-art results in multiple fine-grained classification datasets including challenging CUB200-2011 bird, Flowers-102, and FGVC-Aircrafts datasets.



### Adversarial Patch Attacks on Monocular Depth Estimation Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.03072v1
- **DOI**: 10.1109/ACCESS.2020.3027372
- **Categories**: **cs.CV**, cs.CR, eess.IV, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2010.03072v1)
- **Published**: 2020-10-06 22:56:22+00:00
- **Updated**: 2020-10-06 22:56:22+00:00
- **Authors**: Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii
- **Comment**: Publisher's Open Access PDF with the CC-BY copyright. Associated
  video, data and programs are available at
  https://www.fujii.nuee.nagoya-u.ac.jp/Research/MonoDepth/
- **Journal**: IEEE Access, vol.8, pp.179094-179104, October 2020
- **Summary**: Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.



