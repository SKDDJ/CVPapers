# Arxiv Papers in cs.CV on 2020-10-09
### Addressing the Real-world Class Imbalance Problem in Dermatology
- **Arxiv ID**: http://arxiv.org/abs/2010.04308v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04308v2)
- **Published**: 2020-10-09 00:24:55+00:00
- **Updated**: 2020-11-14 03:45:48+00:00
- **Authors**: Wei-Hung Weng, Jonathan Deaton, Vivek Natarajan, Gamaleldin F. Elsayed, Yuan Liu
- **Comment**: Machine Learning for Health Workshop at NeurIPS 2020; 14 pages + 4
  pages appendix, 8 figures, 6 appendix tables
- **Journal**: None
- **Summary**: Class imbalance is a common problem in medical diagnosis, causing a standard classifier to be biased towards the common classes and perform poorly on the rare classes. This is especially true for dermatology, a specialty with thousands of skin conditions but many of which have low prevalence in the real world. Motivated by recent advances, we explore few-shot learning methods as well as conventional class imbalance techniques for the skin condition recognition problem and propose an evaluation setup to fairly assess the real-world utility of such approaches. We find the performance of few-show learning methods does not reach that of conventional class imbalance techniques, but combining the two approaches using a novel ensemble improves model performance, especially for rare classes. We conclude that ensembling can be useful to address the class imbalance problem, yet progress can further be accelerated by real-world evaluation setups for benchmarking new methods.



### Deep-Masking Generative Network: A Unified Framework for Background Restoration from Superimposed Images
- **Arxiv ID**: http://arxiv.org/abs/2010.04324v2
- **DOI**: 10.1109/TIP.2021.3076589
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04324v2)
- **Published**: 2020-10-09 01:47:52+00:00
- **Updated**: 2021-04-12 09:47:26+00:00
- **Authors**: Xin Feng, Wenjie Pei, Zihui Jia, Fanglin Chen, David Zhang, Guangming Lu
- **Comment**: 16 pages, accepted for publication in IEEE Transactions on Image
  Processing (TIP)
- **Journal**: None
- **Summary**: Restoring the clean background from the superimposed images containing a noisy layer is the common crux of a classical category of tasks on image restoration such as image reflection removal, image deraining and image dehazing. These tasks are typically formulated and tackled individually due to the diverse and complicated appearance patterns of noise layers within the image. In this work we present the Deep-Masking Generative Network (DMGN), which is a unified framework for background restoration from the superimposed images and is able to cope with different types of noise. Our proposed DMGN follows a coarse-to-fine generative process: a coarse background image and a noise image are first generated in parallel, then the noise image is further leveraged to refine the background image to achieve a higher-quality background image. In particular, we design the novel Residual Deep-Masking Cell as the core operating unit for our DMGN to enhance the effective information and suppress the negative information during image generation via learning a gating mask to control the information flow. By iteratively employing this Residual Deep-Masking Cell, our proposed DMGN is able to generate both high-quality background image and noisy image progressively. Furthermore, we propose a two-pronged strategy to effectively leverage the generated noise image as contrasting cues to facilitate the refinement of the background image. Extensive experiments across three typical tasks for image background restoration, including image reflection removal, image rain steak removal and image dehazing, show that our DMGN consistently outperforms state-of-the-art methods specifically designed for each single task.



### Targeted Physical-World Attention Attack on Deep Learning Models in Road Sign Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.04331v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04331v3)
- **Published**: 2020-10-09 02:31:34+00:00
- **Updated**: 2021-08-13 01:29:14+00:00
- **Authors**: Xinghao Yang, Weifeng Liu, Shengli Zhang, Wei Liu, Dacheng Tao
- **Comment**: IEEE Internet of Things Journal 8 (6), pp. 4980-4990, 2021
- **Journal**: None
- **Summary**: Real world traffic sign recognition is an important step towards building autonomous vehicles, most of which highly dependent on Deep Neural Networks (DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to adversarial examples. Many attack methods have been proposed to understand and generate adversarial examples, such as gradient based attack, score based attack, decision based attack, and transfer based attacks. However, most of these algorithms are ineffective in real-world road sign attack, because (1) iteratively learning perturbations for each frame is not realistic for a fast moving car and (2) most optimization algorithms traverse all pixels equally without considering their diverse contribution. To alleviate these problems, this paper proposes the targeted attention attack (TAA) method for real world road sign attack. Specifically, we have made the following contributions: (1) we leverage the soft attention map to highlight those important pixels and skip those zero-contributed areas - this also helps to generate natural perturbations, (2) we design an efficient universal attack that optimizes a single perturbation/noise based on a set of training images under the guidance of the pre-trained attention map, (3) we design a simple objective function that can be easily optimized, (4) we evaluate the effectiveness of TAA on real world data sets. Experimental results validate that the TAA method improves the attack successful rate (nearly 10%) and reduces the perturbation loss (about a quarter) compared with the popular RP2 method. Additionally, our TAA also provides good properties, e.g., transferability and generalization capability. We provide code and data to ensure the reproducibility: https://github.com/AdvAttack/RoadSignAttack.



### MMGSD: Multi-Modal Gaussian Shape Descriptors for Correspondence Matching in 1D and 2D Deformable Objects
- **Arxiv ID**: http://arxiv.org/abs/2010.04339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.04339v1)
- **Published**: 2020-10-09 03:11:13+00:00
- **Updated**: 2020-10-09 03:11:13+00:00
- **Authors**: Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan, Ashwin Balakrishna, Daniel Seita, Ryan Hoque, Joseph E. Gonzalez, Ken Goldberg
- **Comment**: IROS 2020 Workshop on Managing Deformation: A Step Towards Higher
  Robot Autonomy
- **Journal**: None
- **Summary**: We explore learning pixelwise correspondences between images of deformable objects in different configurations. Traditional correspondence matching approaches such as SIFT, SURF, and ORB can fail to provide sufficient contextual information for fine-grained manipulation. We propose Multi-Modal Gaussian Shape Descriptor (MMGSD), a new visual representation of deformable objects which extends ideas from dense object descriptors to predict all symmetric correspondences between different object configurations. MMGSD is learned in a self-supervised manner from synthetic data and produces correspondence heatmaps with measurable uncertainty. In simulation, experiments suggest that MMGSD can achieve an RMSE of 32.4 and 31.3 for square cloth and braided synthetic nylon rope respectively. The results demonstrate an average of 47.7% improvement over a provided baseline based on contrastive learning, symmetric pixel-wise contrastive loss (SPCL), as opposed to MMGSD which enforces distributional continuity.



### Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2010.04354v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04354v3)
- **Published**: 2020-10-09 03:52:16+00:00
- **Updated**: 2021-09-28 06:53:15+00:00
- **Authors**: Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei Yu, Junjie Yan, Wanli Ouyang
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Quantization Neural Networks (QNN) have attracted a lot of attention due to their high efficiency. To enhance the quantization accuracy, prior works mainly focus on designing advanced quantization algorithms but still fail to achieve satisfactory results under the extremely low-bit case. In this work, we take an architecture perspective to investigate the potential of high-performance QNN. Therefore, we propose to combine Network Architecture Search methods with quantization to enjoy the merits of the two sides. However, a naive combination inevitably faces unacceptable time consumption or unstable training problem. To alleviate these problems, we first propose the joint training of architecture and quantization with a shared step size to acquire a large number of quantized models. Then a bit-inheritance scheme is introduced to transfer the quantized models to the lower bit, which further reduces the time cost and meanwhile improves the quantization accuracy. Equipped with this overall framework, dubbed as Once Quantization-Aware Training~(OQAT), our searched model family, OQATNets, achieves a new state-of-the-art compared with various architectures under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet Top-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin of 9% with 10% less computation cost. A series of quantization-friendly architectures are identified easily and extensive analysis can be made to summarize the interaction between quantization and neural architectures. Codes and models are released at https://github.com/LaVieEnRoseSMZ/OQA



### Refining Semantic Segmentation with Superpixel by Transparent Initialization and Sparse Encoder
- **Arxiv ID**: http://arxiv.org/abs/2010.04363v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04363v3)
- **Published**: 2020-10-09 04:20:54+00:00
- **Updated**: 2020-11-24 10:14:58+00:00
- **Authors**: Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning greatly improves the performance of semantic segmentation, its success mainly lies in object central areas without accurate edges. As superpixels are a popular and effective auxiliary to preserve object edges, in this paper, we jointly learn semantic segmentation with trainable superpixels. We achieve it with fully-connected layers with Transparent Initialization (TI) and efficient logit consistency using a sparse encoder. The proposed TI preserves the effects of learned parameters of pretrained networks. This avoids a significant increase of the loss of pretrained networks, which otherwise may be caused by inappropriate parameter initialization of the additional layers. Meanwhile, consistent pixel labels in each superpixel are guaranteed by logit consistency. The sparse encoder with sparse matrix operations substantially reduces both the memory requirement and the computational complexity. We demonstrated the superiority of TI over other parameter initialization methods and tested its numerical stability. The effectiveness of our proposal was validated on PASCAL VOC 2012, ADE20K, and PASCAL Context showing enhanced semantic segmentation edges. With quantitative evaluations on segmentation edges using performance ratio and F-measure, our method outperforms the state-of-the-art.



### DeepStreet: A deep learning powered urban street network generation module
- **Arxiv ID**: http://arxiv.org/abs/2010.04365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.04365v1)
- **Published**: 2020-10-09 04:27:41+00:00
- **Updated**: 2020-10-09 04:27:41+00:00
- **Authors**: Zhou Fang, Tianren Yang, Ying Jin
- **Comment**: None
- **Journal**: None
- **Summary**: In countries experiencing unprecedented waves of urbanization, there is a need for rapid and high quality urban street design. Our study presents a novel deep learning powered approach, DeepStreet (DS), for automatic street network generation that can be applied to the urban street design with local characteristics. DS is driven by a Convolutional Neural Network (CNN) that enables the interpolation of streets based on the areas of immediate vicinity. Specifically, the CNN is firstly trained to detect, recognize and capture the local features as well as the patterns of the existing street network sourced from the OpenStreetMap. With the trained CNN, DS is able to predict street networks' future expansion patterns within the predefined region conditioned on its surrounding street networks. To test the performance of DS, we apply it to an area in and around the Eixample area in the City of Barcelona, a well known example in the fields of urban and transport planning with iconic grid like street networks in the centre and irregular road alignments farther afield. The results show that DS can (1) detect and self cluster different types of complex street patterns in Barcelona; (2) predict both gridiron and irregular street and road networks. DS proves to have a great potential as a novel tool for designers to efficiently design the urban street network that well maintains the consistency across the existing and newly generated urban street network. Furthermore, the generated networks can serve as a benchmark to guide the local plan-making especially in rapidly developing cities.



### Robust Instance Tracking via Uncertainty Flow
- **Arxiv ID**: http://arxiv.org/abs/2010.04367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04367v1)
- **Published**: 2020-10-09 04:39:19+00:00
- **Updated**: 2020-10-09 04:39:19+00:00
- **Authors**: Jianing Qian, Junyu Nan, Siddharth Ancha, Brian Okorn, David Held
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art trackers often fail due to distractorsand large object appearance changes. In this work, we explore the use ofdense optical flow to improve tracking robustness. Our main insight is that, because flow estimation can also have errors, we need to incorporate an estimate of flow uncertainty for robust tracking. We present a novel tracking framework which combines appearance and flow uncertainty information to track objects in challenging scenarios. We experimentally verify that our framework improves tracking robustness, leading to new state-of-the-art results. Further, our experimental ablations shows the importance of flow uncertainty for robust tracking.



### Deep Sequence Learning for Video Anticipation: From Discrete and Deterministic to Continuous and Stochastic
- **Arxiv ID**: http://arxiv.org/abs/2010.04368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04368v1)
- **Published**: 2020-10-09 04:40:58+00:00
- **Updated**: 2020-10-09 04:40:58+00:00
- **Authors**: Sadegh Aliakbarian
- **Comment**: The draft of my PhD thesis
- **Journal**: None
- **Summary**: Video anticipation is the task of predicting one/multiple future representation(s) given limited, partial observation. This is a challenging task due to the fact that given limited observation, the future representation can be highly ambiguous. Based on the nature of the task, video anticipation can be considered from two viewpoints: the level of details and the level of determinism in the predicted future. In this research, we start from anticipating a coarse representation of a deterministic future and then move towards predicting continuous and fine-grained future representations of a stochastic process. The example of the former is video action anticipation in which we are interested in predicting one action label given a partially observed video and the example of the latter is forecasting multiple diverse continuations of human motion given partially observed one. In particular, in this thesis, we make several contributions to the literature of video anticipation...



### Weaponizing Unicodes with Deep Learning -- Identifying Homoglyphs with Weakly Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2010.04382v4
- **DOI**: 10.1109/ISI49825.2020.9280538
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04382v4)
- **Published**: 2020-10-09 06:03:18+00:00
- **Updated**: 2020-12-22 18:11:46+00:00
- **Authors**: Perry Deng, Cooper Linsky, Matthew Wright
- **Comment**: Updated DOI
- **Journal**: None
- **Summary**: Visually similar characters, or homoglyphs, can be used to perform social engineering attacks or to evade spam and plagiarism detectors. It is thus important to understand the capabilities of an attacker to identify homoglyphs -- particularly ones that have not been previously spotted -- and leverage them in attacks. We investigate a deep-learning model using embedding learning, transfer learning, and augmentation to determine the visual similarity of characters and thereby identify potential homoglyphs. Our approach uniquely takes advantage of weak labels that arise from the fact that most characters are not homoglyphs. Our model drastically outperforms the Normalized Compression Distance approach on pairwise homoglyph identification, for which we achieve an average precision of 0.97. We also present the first attempt at clustering homoglyphs into sets of equivalence classes, which is more efficient than pairwise information for security practitioners to quickly lookup homoglyphs or to normalize confusable string encodings. To measure clustering performance, we propose a metric (mBIOU) building on the classic Intersection-Over-Union (IOU) metric. Our clustering method achieves 0.592 mBIOU, compared to 0.430 for the naive baseline. We also use our model to predict over 8,000 previously unknown homoglyphs, and find good early indications that many of these may be true positives. Source code and list of predicted homoglyphs are uploaded to Github: https://github.com/PerryXDeng/weaponizing_unicode



### Learning 3D Face Reconstruction with a Pose Guidance Network
- **Arxiv ID**: http://arxiv.org/abs/2010.04384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.04384v1)
- **Published**: 2020-10-09 06:11:17+00:00
- **Updated**: 2020-10-09 06:11:17+00:00
- **Authors**: Pengpeng Liu, Xintong Han, Michael Lyu, Irwin King, Jia Xu
- **Comment**: ACCV 2020 (Oral)
- **Journal**: None
- **Summary**: We present a self-supervised learning approach to learning monocular 3D face reconstruction with a pose guidance network (PGN). First, we unveil the bottleneck of pose estimation in prior parametric 3D face learning methods, and propose to utilize 3D face landmarks for estimating pose parameters. With our specially designed PGN, our model can learn from both faces with fully labeled 3D landmarks and unlimited unlabeled in-the-wild face images. Our network is further augmented with a self-supervised learning scheme, which exploits face geometry information embedded in multiple frames of the same person, to alleviate the ill-posed nature of regressing 3D face geometry from a single image. These three insights yield a single approach that combines the complementary strengths of parametric model learning and data-driven learning techniques. We conduct a rigorous evaluation on the challenging AFLW2000-3D, Florence and FaceWarehouse datasets, and show that our method outperforms the state-of-the-art for all metrics.



### Generating Novel Glyph without Human Data by Learning to Communicate
- **Arxiv ID**: http://arxiv.org/abs/2010.04402v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.04402v2)
- **Published**: 2020-10-09 07:18:36+00:00
- **Updated**: 2020-11-22 12:57:16+00:00
- **Authors**: Seung-won Park
- **Comment**: To appear at 4th Workshop on Machine Learning for Creativity and
  Design at NeurIPS 2020; 6 pages with 4 figures and 1 table
- **Journal**: None
- **Summary**: In this paper, we present Neural Glyph, a system that generates novel glyph without any training data. The generator and the classifier are trained to communicate via visual symbols as a medium, which enforces the generator to come up with a set of distinctive symbols. Our method results in glyphs that resemble the human-made glyphs, which may imply that the visual appearances of existing glyphs can be attributed to constraints of communication via writing. Important tricks that enable this framework are described and the code is made available.



### A deep learning based interactive sketching system for fashion images design
- **Arxiv ID**: http://arxiv.org/abs/2010.04413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04413v1)
- **Published**: 2020-10-09 07:50:56+00:00
- **Updated**: 2020-10-09 07:50:56+00:00
- **Authors**: Yao Li, Xianggang Yu, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: In this work, we propose an interactive system to design diverse high-quality garment images from fashion sketches and the texture information. The major challenge behind this system is to generate high-quality and detailed texture according to the user-provided texture information. Prior works mainly use the texture patch representation and try to map a small texture patch to a whole garment image, hence unable to generate high-quality details. In contrast, inspired by intrinsic image decomposition, we decompose this task into texture synthesis and shading enhancement. In particular, we propose a novel bi-colored edge texture representation to synthesize textured garment images and a shading enhancer to render shading based on the grayscale edges. The bi-colored edge representation provides simple but effective texture cues and color constraints, so that the details can be better reconstructed. Moreover, with the rendered shading, the synthesized garment image becomes more vivid.



### Long-distance tiny face detection based on enhanced YOLOv3 for unmanned system
- **Arxiv ID**: http://arxiv.org/abs/2010.04421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04421v1)
- **Published**: 2020-10-09 08:12:58+00:00
- **Updated**: 2020-10-09 08:12:58+00:00
- **Authors**: Jia-Yi Chang, Yan-Feng Lu, Ya-Jun Liu, Bo Zhou, Hong Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Remote tiny face detection applied in unmanned system is a challeng-ing work. The detector cannot obtain sufficient context semantic information due to the relatively long distance. The received poor fine-grained features make the face detection less accurate and robust. To solve the problem of long-distance detection of tiny faces, we propose an enhanced network model (YOLOv3-C) based on the YOLOv3 algorithm for unmanned platform. In this model, we bring in multi-scale features from feature pyramid networks and make the features fu-sion to adjust prediction feature map of the output, which improves the sensitivity of the entire algorithm for tiny target faces. The enhanced model improves the accuracy of tiny face detection in the cases of long-distance and high-density crowds. The experimental evaluation results demonstrated the superior perfor-mance of the proposed YOLOv3-C in comparison with other relevant detectors in remote tiny face detection. It is worth mentioning that our proposed method achieves comparable performance with the state of the art YOLOv4[1] in the tiny face detection tasks.



### WHO 2016 subtyping and automated segmentation of glioma using multi-task deep learning
- **Arxiv ID**: http://arxiv.org/abs/2010.04425v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04425v1)
- **Published**: 2020-10-09 08:18:53+00:00
- **Updated**: 2020-10-09 08:18:53+00:00
- **Authors**: Sebastian R. van der Voort, Fatih Incekara, Maarten M. J. Wijnenga, Georgios Kapsas, Renske Gahrmann, Joost W. Schouten, Rishi Nandoe Tewarie, Geert J. Lycklama, Philip C. De Witt Hamer, Roelant S. Eijgelaar, Pim J. French, Hendrikus J. Dubbink, Arnaud J. P. E. Vincent, Wiro J. Niessen, Martin J. van den Bent, Marion Smits, Stefan Klein
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate characterization of glioma is crucial for clinical decision making. A delineation of the tumor is also desirable in the initial decision stages but is a time-consuming task. Leveraging the latest GPU capabilities, we developed a single multi-task convolutional neural network that uses the full 3D, structural, pre-operative MRI scans to can predict the IDH mutation status, the 1p/19q co-deletion status, and the grade of a tumor, while simultaneously segmenting the tumor. We trained our method using the largest, most diverse patient cohort to date containing 1508 glioma patients from 16 institutes. We tested our method on an independent dataset of 240 patients from 13 different institutes, and achieved an IDH-AUC of 0.90, 1p/19q-AUC of 0.85, grade-AUC of 0.81, and a mean whole tumor DICE score of 0.84. Thus, our method non-invasively predicts multiple, clinically relevant parameters and generalizes well to the broader clinical population.



### Real-time Mask Detection on Google Edge TPU
- **Arxiv ID**: http://arxiv.org/abs/2010.04427v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04427v1)
- **Published**: 2020-10-09 08:21:34+00:00
- **Updated**: 2020-10-09 08:21:34+00:00
- **Authors**: Keondo Park, Wonyoung Jang, Woochul Lee, Kisung Nam, Kihong Seong, Kyuwook Chai, Wen-Syan Li
- **Comment**: None
- **Journal**: None
- **Summary**: After the COVID-19 outbreak, it has become important to automatically detect whether people are wearing masks in order to reduce risk of front-line workers. In addition, processing user data locally is a great way to address both privacy and network bandwidth issues. In this paper, we present a light-weighted model for detecting whether people in a particular area wear masks, which can also be deployed on Coral Dev Board, a commercially available development board containing Google Edge TPU. Our approach combines the object detecting network based on MobileNetV2 plus SSD and the quantization scheme for integer-only hardware. As a result, the lighter model in the Edge TPU has a significantly lower latency which is more appropriate for real-time execution while maintaining accuracy comparable to a floating point device.



### Rethinking the Extraction and Interaction of Multi-Scale Features for Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.04428v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04428v1)
- **Published**: 2020-10-09 08:22:54+00:00
- **Updated**: 2020-10-09 08:22:54+00:00
- **Authors**: Yicheng Wu, Chengwei Pan, Shuqi Wang, Ming Zhang, Yong Xia, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing the morphological attributes of blood vessels plays a critical role in the computer-aided diagnosis of many cardiovascular and ophthalmologic diseases. Although being extensively studied, segmentation of blood vessels, particularly thin vessels and capillaries, remains challenging mainly due to the lack of an effective interaction between local and global features. In this paper, we propose a novel deep learning model called PC-Net to segment retinal vessels and major arteries in 2D fundus image and 3D computed tomography angiography (CTA) scans, respectively. In PC-Net, the pyramid squeeze-and-excitation (PSE) module introduces spatial information to each convolutional block, boosting its ability to extract more effective multi-scale features, and the coarse-to-fine (CF) module replaces the conventional decoder to enhance the details of thin vessels and process hard-to-classify pixels again. We evaluated our PC-Net on the Digital Retinal Images for Vessel Extraction (DRIVE) database and an in-house 3D major artery (3MA) database against several recent methods. Our results not only demonstrate the effectiveness of the proposed PSE module and CF module, but also suggest that our proposed PC-Net sets new state of the art in the segmentation of retinal vessels (AUC: 98.31%) and major arteries (AUC: 98.35%) on both databases, respectively.



### Face Mask Assistant: Detection of Face Mask Service Stage Based on Mobile Phone
- **Arxiv ID**: http://arxiv.org/abs/2010.06421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06421v1)
- **Published**: 2020-10-09 08:49:52+00:00
- **Updated**: 2020-10-09 08:49:52+00:00
- **Authors**: Yuzhen Chen, Menghan Hu, Chunjun Hua, Guangtao Zhai, Jian Zhang, Qingli Li, Simon X. Yang
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Coronavirus Disease 2019 (COVID-19) has spread all over the world since it broke out massively in December 2019, which has caused a large loss to the whole world. Both the confirmed cases and death cases have reached a relatively frightening number. Syndrome coronaviruses 2 (SARS-CoV-2), the cause of COVID-19, can be transmitted by small respiratory droplets. To curb its spread at the source, wearing masks is a convenient and effective measure. In most cases, people use face masks in a high-frequent but short-time way. Aimed at solving the problem that we don't know which service stage of the mask belongs to, we propose a detection system based on the mobile phone. We first extract four features from the GLCMs of the face mask's micro-photos. Next, a three-result detection system is accomplished by using KNN algorithm. The results of validation experiments show that our system can reach a precision of 82.87% (standard deviation=8.5%) on the testing dataset. In future work, we plan to expand the detection objects to more mask types. This work demonstrates that the proposed mobile microscope system can be used as an assistant for face mask being used, which may play a positive role in fighting against COVID-19.



### Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2010.04456v6
- **DOI**: 10.1088/1742-5468/ac3ae5
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04456v6)
- **Published**: 2020-10-09 09:31:03+00:00
- **Updated**: 2022-05-10 12:56:21+00:00
- **Authors**: Yuan Yin, Vincent Le Guen, Jérémie Dona, Emmanuel de Bézenac, Ibrahim Ayed, Nicolas Thome, Patrick Gallinari
- **Comment**: Accepted at ICLR 2021 (Oral)
- **Journal**: J. Stat. Mech. (2021) 124012
- **Summary**: Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. Code is available at https://github.com/yuan-yin/APHYNITY .



### gundapusunil at SemEval-2020 Task 8: Multimodal Memotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2010.04470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04470v1)
- **Published**: 2020-10-09 09:53:14+00:00
- **Updated**: 2020-10-09 09:53:14+00:00
- **Authors**: Sunil Gundapu, Radhika Mamidi
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Recent technological advancements in the Internet and Social media usage have resulted in the evolution of faster and efficient platforms of communication. These platforms include visual, textual and speech mediums and have brought a unique social phenomenon called Internet memes. Internet memes are in the form of images with witty, catchy, or sarcastic text descriptions. In this paper, we present a multi-modal sentiment analysis system using deep neural networks combining Computer Vision and Natural Language Processing. Our aim is different than the normal sentiment analysis goal of predicting whether a text expresses positive or negative sentiment; instead, we aim to classify the Internet meme as a positive, negative, or neutral, identify the type of humor expressed and quantify the extent to which a particular effect is being expressed. Our system has been developed using CNN and LSTM and outperformed the baseline score.



### Contralaterally Enhanced Networks for Thoracic Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.04483v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04483v1)
- **Published**: 2020-10-09 10:15:26+00:00
- **Updated**: 2020-10-09 10:15:26+00:00
- **Authors**: Gangming Zhao, Chaowei Fang, Guanbin Li, Licheng Jiao, Yizhou Yu
- **Comment**: submitted to TMI
- **Journal**: None
- **Summary**: Identifying and locating diseases in chest X-rays are very challenging, due to the low visual contrast between normal and abnormal regions, and distortions caused by other overlapping tissues. An interesting phenomenon is that there exist many similar structures in the left and right parts of the chest, such as ribs, lung fields and bronchial tubes. This kind of similarities can be used to identify diseases in chest X-rays, according to the experience of broad-certificated radiologists. Aimed at improving the performance of existing detection methods, we propose a deep end-to-end module to exploit the contralateral context information for enhancing feature representations of disease proposals. First of all, under the guidance of the spine line, the spatial transformer network is employed to extract local contralateral patches, which can provide valuable context information for disease proposals. Then, we build up a specific module, based on both additive and subtractive operations, to fuse the features of the disease proposal and the contralateral patch. Our method can be integrated into both fully and weakly supervised disease detection frameworks. It achieves 33.17 AP50 on a carefully annotated private chest X-ray dataset which contains 31,000 images. Experiments on the NIH chest X-ray dataset indicate that our method achieves state-of-the-art performance in weakly-supervised disease localization.



### Linear Mode Connectivity in Multitask and Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.04495v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04495v1)
- **Published**: 2020-10-09 10:53:25+00:00
- **Updated**: 2020-10-09 10:53:25+00:00
- **Authors**: Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, Hassan Ghasemzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.



### Background Learnable Cascade for Zero-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.04502v1
- **DOI**: 10.1007/978-3-030-69535-4_7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04502v1)
- **Published**: 2020-10-09 11:20:02+00:00
- **Updated**: 2020-10-09 11:20:02+00:00
- **Authors**: Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, Li Cui
- **Comment**: 18 pages, 5figures
- **Journal**: None
- **Summary**: Zero-shot detection (ZSD) is crucial to large-scale object detection with the aim of simultaneously localizing and recognizing unseen objects. There remain several challenges for ZSD, including reducing the ambiguity between background and unseen objects as well as improving the alignment between visual and semantic concept. In this work, we propose a novel framework named Background Learnable Cascade (BLC) to improve ZSD performance. The major contributions for BLC are as follows: (i) we propose a multi-stage cascade structure named Cascade Semantic R-CNN to progressively refine the alignment between visual and semantic of ZSD; (ii) we develop the semantic information flow structure and directly add it between each stage in Cascade Semantic RCNN to further improve the semantic feature learning; (iii) we propose the background learnable region proposal network (BLRPN) to learn an appropriate word vector for background class and use this learned vector in Cascade Semantic R CNN, this design makes \Background Learnable" and reduces the confusion between background and unseen classes. Our extensive experiments show BLC obtains significantly performance improvements for MS-COCO over state-of-the-art methods.



### Sickle-cell disease diagnosis support selecting the most appropriate machinelearning method: Towards a general and interpretable approach for cellmorphology analysis from microscopy images
- **Arxiv ID**: http://arxiv.org/abs/2010.04511v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04511v1)
- **Published**: 2020-10-09 11:46:38+00:00
- **Updated**: 2020-10-09 11:46:38+00:00
- **Authors**: Nataša Petrović, Gabriel Moyà-Alcover, Antoni Jaume-i-Capó, Manuel González-Hidalgo
- **Comment**: 35 pages, 10 tables
- **Journal**: Computers in Biology and Medicine, 2020, pending publication
- **Summary**: In this work we propose an approach to select the classification method and features, based on the state-of-the-art, with best performance for diagnostic support through peripheral blood smear images of red blood cells. In our case we used samples of patients with sickle-cell disease which can be generalized for other study cases. To trust the behavior of the proposed system, we also analyzed the interpretability.   We pre-processed and segmented microscopic images, to ensure high feature quality. We applied the methods used in the literature to extract the features from blood cells and the machine learning methods to classify their morphology. Next, we searched for their best parameters from the resulting data in the feature extraction phase. Then, we found the best parameters for every classifier using Randomized and Grid search.   For the sake of scientific progress, we published parameters for each classifier, the implemented code library, the confusion matrices with the raw data, and we used the public erythrocytesIDB dataset for validation. We also defined how to select the most important features for classification to decrease the complexity and the training time, and for interpretability purpose in opaque models. Finally, comparing the best performing classification methods with the state-of-the-art, we obtained better results even with interpretable model classifiers.



### Controllable Continuous Gaze Redirection
- **Arxiv ID**: http://arxiv.org/abs/2010.04513v1
- **DOI**: 10.1145/3394171.3413868
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04513v1)
- **Published**: 2020-10-09 11:50:06+00:00
- **Updated**: 2020-10-09 11:50:06+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue, Wensen Feng
- **Comment**: accepted by ACM International Conference on Multimedia (ACM MM), 2020
- **Journal**: None
- **Summary**: In this work, we present interpGaze, a novel framework for controllable gaze redirection that achieves both precise redirection and continuous interpolation. Given two gaze images with different attributes, our goal is to redirect the eye gaze of one person into any gaze direction depicted in the reference image or to generate continuous intermediate results. To accomplish this, we design a model including three cooperative components: an encoder, a controller and a decoder. The encoder maps images into a well-disentangled and hierarchically-organized latent space. The controller adjusts the magnitudes of latent vectors to the desired strength of corresponding attributes by altering a control vector. The decoder converts the desired representations from the attribute space to the image space. To facilitate covering the full space of gaze directions, we introduce a high-quality gaze image dataset with a large range of directions, which also benefits researchers in related areas. Extensive experimental validation and comparisons to several baseline methods show that the proposed interpGaze outperforms state-of-the-art methods in terms of image quality and redirection precision.



### Be Your Own Best Competitor! Multi-Branched Adversarial Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2010.04516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04516v1)
- **Published**: 2020-10-09 11:57:45+00:00
- **Updated**: 2020-10-09 11:57:45+00:00
- **Authors**: Mahdi Ghorbani, Fahimeh Fooladgar, Shohreh Kasaei
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural network architectures have attained remarkable improvements in scene understanding tasks. Utilizing an efficient model is one of the most important constraints for limited-resource devices. Recently, several compression methods have been proposed to diminish the heavy computational burden and memory consumption. Among them, the pruning and quantizing methods exhibit a critical drop in performances by compressing the model parameters. While the knowledge distillation methods improve the performance of compact models by focusing on training lightweight networks with the supervision of cumbersome networks. In the proposed method, the knowledge distillation has been performed within the network by constructing multiple branches over the primary stream of the model, known as the self-distillation method. Therefore, the ensemble of sub-neural network models has been proposed to transfer the knowledge among themselves with the knowledge distillation policies as well as an adversarial learning strategy. Hence, The proposed ensemble of sub-models is trained against a discriminator model adversarially. Besides, their knowledge is transferred within the ensemble by four different loss functions. The proposed method has been devoted to both lightweight image classification and encoder-decoder architectures to boost the performance of small and compact models without incurring extra computational overhead at the inference process. Extensive experimental results on the main challenging datasets show that the proposed network outperforms the primary model in terms of accuracy at the same number of parameters and computational cost. The obtained results show that the proposed model has achieved significant improvement over earlier ideas of self-distillation methods. The effectiveness of the proposed models has also been illustrated in the encoder-decoder model.



### Real Time Face Recognition Using Convoluted Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.04517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04517v1)
- **Published**: 2020-10-09 12:04:49+00:00
- **Updated**: 2020-10-09 12:04:49+00:00
- **Authors**: Rohith Pudari, Sunil Bhutada, Sai Pavan Mudavath
- **Comment**: None
- **Journal**: International Journal of Sciences and Technology (2019) volume - 7
- **Summary**: Face Recognition is one of the process of identifying people using their face, it has various applications like authentication systems, surveillance systems and law enforcement. Convolutional Neural Networks are proved to be best for facial recognition. Detecting faces using core-ml api and processing the extracted face through a coreML model, which is trained to recognize specific persons. The creation of dataset is done by converting face videos of the persons to be recognized into Hundreds of images of person, which is further used for training and validation of the model to provide accurate real-time results.



### Uncertainty-Aware Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.04525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04525v2)
- **Published**: 2020-10-09 12:26:27+00:00
- **Updated**: 2021-06-03 13:48:34+00:00
- **Authors**: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shih-Fu Chang
- **Comment**: Accepted by IJCAI2021
- **Journal**: None
- **Summary**: Few-shot image classification learns to recognize new categories from limited labelled data. Metric learning based approaches have been widely investigated, where a query sample is classified by finding the nearest prototype from the support set based on their feature similarities. A neural network has different uncertainties on its calculated similarities of different pairs. Understanding and modeling the uncertainty on the similarity could promote the exploitation of limited samples in few-shot optimization. In this work, we propose Uncertainty-Aware Few-Shot framework for image classification by modeling uncertainty of the similarities of query-support pairs and performing uncertainty-aware optimization. Particularly, we exploit such uncertainty by converting observed similarities to probabilistic representations and incorporate them to the loss for more effective optimization. In order to jointly consider the similarities between a query and the prototypes in a support set, a graph-based model is utilized to estimate the uncertainty of the pairs. Extensive experiments show our proposed method brings significant improvements on top of a strong baseline and achieves the state-of-the-art performance.



### Incorporating planning intelligence into deep learning: A planning support tool for street network design
- **Arxiv ID**: http://arxiv.org/abs/2010.04536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.04536v1)
- **Published**: 2020-10-09 12:57:05+00:00
- **Updated**: 2020-10-09 12:57:05+00:00
- **Authors**: Zhou Fang, Ying Jin, Tianren Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning applications in shaping ad hoc planning proposals are limited by the difficulty in integrating professional knowledge about cities with artificial intelligence. We propose a novel, complementary use of deep neural networks and planning guidance to automate street network generation that can be context-aware, example-based and user-guided. The model tests suggest that the incorporation of planning knowledge (e.g., road junctions and neighborhood types) in the model training leads to a more realistic prediction of street configurations. Furthermore, the new tool provides both professional and lay users an opportunity to systematically and intuitively explore benchmark proposals for comparisons and further evaluations.



### Table Structure Recognition using Top-Down and Bottom-Up Cues
- **Arxiv ID**: http://arxiv.org/abs/2010.04565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04565v1)
- **Published**: 2020-10-09 13:32:53+00:00
- **Updated**: 2020-10-09 13:32:53+00:00
- **Authors**: Sachin Raja, Ajoy Mondal, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Tables are information-rich structured objects in document images. While significant work has been done in localizing tables as graphic objects in document images, only limited attempts exist on table structure recognition. Most existing literature on structure recognition depends on extraction of meta-features from the PDF document or on the optical character recognition (OCR) models to extract low-level layout features from the image. However, these methods fail to generalize well because of the absence of meta-features or errors made by the OCR when there is a significant variance in table layouts and text organization. In our work, we focus on tables that have complex structures, dense content, and varying layouts with no dependency on meta-features and/or OCR.   We present an approach for table structure recognition that combines cell detection and interaction modules to localize the cells and predict their row and column associations with other detected cells. We incorporate structural constraints as additional differential components to the loss function for cell detection. We empirically validate our method on the publicly available real-world datasets - ICDAR-2013, ICDAR-2019 (cTDaR) archival, UNLV, SciTSR, SciTSR-COMP, TableBank, and PubTabNet. Our attempt opens up a new direction for table structure recognition by combining top-down (table cells detection) and bottom-up (structure recognition) cues in visually understanding the tables.



### A Novel ANN Structure for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.04586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/2010.04586v1)
- **Published**: 2020-10-09 14:07:29+00:00
- **Updated**: 2020-10-09 14:07:29+00:00
- **Authors**: Shilpa Mayannavar, Uday Wali, V M Aparanji
- **Comment**: 9 pages, 10 images
- **Journal**: None
- **Summary**: The paper presents Multi-layer Auto Resonance Networks (ARN), a new neural model, for image recognition. Neurons in ARN, called Nodes, latch on to an incoming pattern and resonate when the input is within its 'coverage.' Resonance allows the neuron to be noise tolerant and tunable. Coverage of nodes gives them an ability to approximate the incoming pattern. Its latching characteristics allow it to respond to episodic events without disturbing the existing trained network. These networks are capable of addressing problems in varied fields but have not been sufficiently explored. Implementation of an image classification and identification system using two-layer ARN is discussed in this paper. Recognition accuracy of 94% has been achieved for MNIST dataset with only two layers of neurons and just 50 samples per numeral, making it useful in computing at the edge of cloud infrastructure.



### GRF: Learning a General Radiance Field for 3D Representation and Rendering
- **Arxiv ID**: http://arxiv.org/abs/2010.04595v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.04595v3)
- **Published**: 2020-10-09 14:21:43+00:00
- **Updated**: 2021-08-11 07:09:11+00:00
- **Authors**: Alex Trevithick, Bo Yang
- **Comment**: ICCV 2021. Code and data are available at:
  https://github.com/alextrevithick/GRF
- **Journal**: None
- **Summary**: We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.



### Handwriting Quality Analysis using Online-Offline Models
- **Arxiv ID**: http://arxiv.org/abs/2010.06693v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06693v1)
- **Published**: 2020-10-09 14:33:56+00:00
- **Updated**: 2020-10-09 14:33:56+00:00
- **Authors**: Yahia Hamdi, Hanen Akouaydi, Houcine Boubaker, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: This work is part of an innovative e-learning project allowing the development of an advanced digital educational tool that provides feedback during the process of learning handwriting for young school children (three to eight years old). In this paper, we describe a new method for children handwriting quality analysis. It automatically detects mistakes, gives real-time on-line feedback for children's writing, and helps teachers comprehend and evaluate children's writing skills. The proposed method adjudges five main criteria shape, direction, stroke order, position respect to the reference lines, and kinematics of the trace. It analyzes the handwriting quality and automatically gives feedback based on the combination of three extracted models: Beta-Elliptic Model (BEM) using similarity detection (SD) and dissimilarity distance (DD) measure, Fourier Descriptor Model (FDM), and perceptive Convolutional Neural Network (CNN) with Support Vector Machine (SVM) comparison engine. The originality of our work lies partly in the system architecture which apprehends complementary dynamic, geometric, and visual representation of the examined handwritten scripts and in the efficient selected features adapted to various handwriting styles and multiple script languages such as Arabic, Latin, digits, and symbol drawing. The application offers two interactive interfaces respectively dedicated to learners, educators, experts or teachers and allows them to adapt it easily to the specificity of their disciples. The evaluation of our framework is enhanced by a database collected in Tunisia primary school with 400 children. Experimental results show the efficiency and robustness of our suggested framework that helps teachers and children by offering positive feedback throughout the handwriting learning process using tactile digital devices.



### Explaining Clinical Decision Support Systems in Medical Imaging using Cycle-Consistent Activation Maximization
- **Arxiv ID**: http://arxiv.org/abs/2010.05759v3
- **DOI**: 10.1016/j.neucom.2021.05.081
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, 62M45, I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2010.05759v3)
- **Published**: 2020-10-09 14:39:27+00:00
- **Updated**: 2022-06-09 07:28:00+00:00
- **Authors**: Alexander Katzmann, Oliver Taubmann, Stephen Ahmad, Alexander Mühlberg, Michael Sühling, Horst-Michael Groß
- **Comment**: 49 pages, 13 figures, 18 pages appendix, accepted manuscript
- **Journal**: Neurocomputing 458 (2021), 141-156
- **Summary**: Clinical decision support using deep neural networks has become a topic of steadily growing interest. While recent work has repeatedly demonstrated that deep learning offers major advantages for medical image classification over traditional methods, clinicians are often hesitant to adopt the technology because its underlying decision-making process is considered to be intransparent and difficult to comprehend. In recent years, this has been addressed by a variety of approaches that have successfully contributed to providing deeper insight. Most notably, additive feature attribution methods are able to propagate decisions back into the input space by creating a saliency map which allows the practitioner to "see what the network sees." However, the quality of the generated maps can become poor and the images noisy if only limited data is available - a typical scenario in clinical contexts. We propose a novel decision explanation scheme based on CycleGAN activation maximization which generates high-quality visualizations of classifier decisions even in smaller data sets. We conducted a user study in which we evaluated our method on the LIDC dataset for lung lesion malignancy classification, the BreastMNIST dataset for ultrasound image breast cancer detection, as well as two subsets of the CIFAR-10 dataset for RBG image object recognition. Within this user study, our method clearly outperformed existing approaches on the medical imaging datasets and ranked second in the natural image setting. With our approach we make a significant contribution towards a better understanding of clinical decision support systems based on deep neural networks and thus aim to foster overall clinical acceptance.



### Hyperspectral Unmixing via Nonnegative Matrix Factorization with Handcrafted and Learnt Priors
- **Arxiv ID**: http://arxiv.org/abs/2010.04611v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04611v1)
- **Published**: 2020-10-09 14:40:20+00:00
- **Updated**: 2020-10-09 14:40:20+00:00
- **Authors**: Min Zhao, Tiande Gao, Jie Chen, Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, nonnegative matrix factorization (NMF) based methods have been widely applied to blind spectral unmixing. Introducing proper regularizers to NMF is crucial for mathematically constraining the solutions and physically exploiting spectral and spatial properties of images. Generally, properly handcrafting regularizers and solving the associated complex optimization problem are non-trivial tasks. In our work, we propose an NMF based unmixing framework which jointly uses a handcrafting regularizer and a learnt regularizer from data. we plug learnt priors of abundances where the associated subproblem can be addressed using various image denoisers, and we consider an l_2,1-norm regularizer to the abundance matrix to promote sparse unmixing results. The proposed framework is flexible and extendable. Both synthetic data and real airborne data are conducted to confirm the effectiveness of our method.



### Attaining Real-Time Super-Resolution for Microscopic Images Using GAN
- **Arxiv ID**: http://arxiv.org/abs/2010.04634v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.04634v1)
- **Published**: 2020-10-09 15:26:21+00:00
- **Updated**: 2020-10-09 15:26:21+00:00
- **Authors**: Vibhu Bhatia, Yatender Kumar
- **Comment**: 10 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: In the last few years, several deep learning models, especially Generative Adversarial Networks have received a lot of attention for the task of Single Image Super-Resolution (SISR). These methods focus on building an end-to-end framework, which produce a high resolution(SR) image from a given low resolution(LR) image in a single step to achieve state-of-the-art performance. This paper focuses on improving an existing deep-learning based method to perform Super-Resolution Microscopy in real-time using a standard GPU. For this, we first propose a tiling strategy, which takes advantage of parallelism provided by a GPU to speed up the network training process. Further, we suggest simple changes to the architecture of the generator and the discriminator of SRGAN. Subsequently, We compare the quality and the running time for the outputs produced by our model, opening its applications in different areas like low-end benchtop and even mobile microscopy. Finally, we explore the possibility of the trained network to produce High-Resolution HR outputs for different domains.



### Baseline and Triangulation Geometry in a Standard Plenoptic Camera
- **Arxiv ID**: http://arxiv.org/abs/2010.04638v2
- **DOI**: 10.1007/s11263-017-1036-4
- **Categories**: **cs.IR**, cs.CG, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2010.04638v2)
- **Published**: 2020-10-09 15:31:14+00:00
- **Updated**: 2021-01-20 12:02:36+00:00
- **Authors**: Christopher Hahne, Amar Aggoun, Vladan Velisavljevic, Susanne Fiebig, Matthias Pesch
- **Comment**: clarified remarks around Eqs.(16-17)
- **Journal**: International Journal of Computer Vision, volume 126, pages 21-35
  (2018)
- **Summary**: In this paper, we demonstrate light field triangulation to determine depth distances and baselines in a plenoptic camera. Advances in micro lenses and image sensors have enabled plenoptic cameras to capture a scene from different viewpoints with sufficient spatial resolution. While object distances can be inferred from disparities in a stereo viewpoint pair using triangulation, this concept remains ambiguous when applied in the case of plenoptic cameras. We present a geometrical light field model allowing the triangulation to be applied to a plenoptic camera in order to predict object distances or specify baselines as desired. It is shown that distance estimates from our novel method match those of real objects placed in front of the camera. Additional benchmark tests with an optical design software further validate the model's accuracy with deviations of less than +-0.33 % for several main lens types and focus settings. A variety of applications in the automotive and robotics field can benefit from this estimation model.



### Torch-Points3D: A Modular Multi-Task Frameworkfor Reproducible Deep Learning on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.04642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML, 68T07, 68T45, I.4.8; I.4.6; I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2010.04642v1)
- **Published**: 2020-10-09 15:34:32+00:00
- **Updated**: 2020-10-09 15:34:32+00:00
- **Authors**: Thomas Chaton, Nicolas Chaulet, Sofiane Horache, Loic Landrieu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Torch-Points3D, an open-source framework designed to facilitate the use of deep networks on3D data. Its modular design, efficient implementation, and user-friendly interfaces make it a relevant tool for research and productization alike. Beyond multiple quality-of-life features, our goal is to standardize a higher level of transparency and reproducibility in 3D deep learning research, and to lower its barrier to entry. In this paper, we present the design principles of Torch-Points3D, as well as extensive benchmarks of multiple state-of-the-art algorithms and inference schemes across several datasets and tasks. The modularity of Torch-Points3D allows us to design fair and rigorous experimental protocols in which all methods are evaluated in the same conditions. The Torch-Points3D repository :https://github.com/nicolas-chaulet/torch-points3d



### Learning Invariant Representations and Risks for Semi-supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.04647v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04647v3)
- **Published**: 2020-10-09 15:42:35+00:00
- **Updated**: 2021-04-04 18:10:56+00:00
- **Authors**: Bo Li, Yezhen Wang, Shanghang Zhang, Dongsheng Li, Trevor Darrell, Kurt Keutzer, Han Zhao
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: The success of supervised learning hinges on the assumption that the training and test data come from the same underlying distribution, which is often not valid in practice due to potential distribution shift. In light of this, most existing methods for unsupervised domain adaptation focus on achieving domain-invariant representations and small source domain error. However, recent works have shown that this is not sufficient to guarantee good generalization on the target domain, and in fact, is provably detrimental under label distribution shift. Furthermore, in many real-world applications it is often feasible to obtain a small amount of labeled data from the target domain and use them to facilitate model training with source data. Inspired by the above observations, in this paper we propose the first method that aims to simultaneously learn invariant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA). First, we provide a finite sample bound for both classification and regression problems under Semi-DA. The bound suggests a principled way to obtain target generalization, i.e. by aligning both the marginal and conditional distributions across domains in feature space. Motivated by this, we then introduce the LIRR algorithm for jointly \textbf{L}earning \textbf{I}nvariant \textbf{R}epresentations and \textbf{R}isks. Finally, extensive experiments are conducted on both classification and regression tasks, which demonstrates LIRR consistently achieves state-of-the-art performance and significant improvements compared with the methods that only learn invariant representations or invariant risks.



### Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.05785v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.05785v3)
- **Published**: 2020-10-09 16:38:38+00:00
- **Updated**: 2021-06-23 17:04:49+00:00
- **Authors**: Oren Nuriel, Sagie Benaim, Lior Wolf
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Recent work has shown that convolutional neural network classifiers overly rely on texture at the expense of shape cues. We make a similar but different distinction between shape and local image cues, on the one hand, and global image statistics, on the other. Our method, called Permuted Adaptive Instance Normalization (pAdaIN), reduces the representation of global statistics in the hidden layers of image classifiers. pAdaIN samples a random permutation $\pi$ that rearranges the samples in a given batch. Adaptive Instance Normalization (AdaIN) is then applied between the activations of each (non-permuted) sample $i$ and the corresponding activations of the sample $\pi(i)$, thus swapping statistics between the samples of the batch. Since the global image statistics are distorted, this swapping procedure causes the network to rely on cues, such as shape or texture. By choosing the random permutation with probability $p$ and the identity permutation otherwise, one can control the effect's strength.   With the correct choice of $p$, fixed apriori for all experiments and selected without considering test data, our method consistently outperforms baselines in multiple settings. In image classification, our method improves on both CIFAR100 and ImageNet using multiple architectures. In the setting of robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple architectures. In the setting of domain adaptation and domain generalization, our method achieves state of the art results on the transfer learning task from GTAV to Cityscapes and on the PACS benchmark.



### Video Quality Enhancement Using Deep Learning-Based Prediction Models for Quantized DCT Coefficients in MPEG I-frames
- **Arxiv ID**: http://arxiv.org/abs/2010.05760v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.05760v1)
- **Published**: 2020-10-09 16:41:18+00:00
- **Updated**: 2020-10-09 16:41:18+00:00
- **Authors**: Antonio J G Busson, Paulo R C Mendes, Daniel de S Moraes, Álvaro M da Veiga, Álan L V Guedes, Sérgio Colcher
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have successfully applied some types of Convolutional Neural Networks (CNNs) to reduce the noticeable distortion resulting from the lossy JPEG/MPEG compression technique. Most of them are built upon the processing made on the spatial domain. In this work, we propose a MPEG video decoder that is purely based on the frequency-to-frequency domain: it reads the quantized DCT coefficients received from a low-quality I-frames bitstream and, using a deep learning-based model, predicts the missing coefficients in order to recompose the same frames with enhanced quality. In experiments with a video dataset, our best model was able to improve from frames with quantized DCT coefficients corresponding to a Quality Factor (QF) of 10 to enhanced quality frames with QF slightly near to 20.



### Smooth Variational Graph Embeddings for Efficient Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2010.04683v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.04683v3)
- **Published**: 2020-10-09 17:05:41+00:00
- **Updated**: 2021-05-12 12:44:54+00:00
- **Authors**: Jovita Lukasik, David Friede, Arber Zela, Frank Hutter, Margret Keuper
- **Comment**: 8 pages, 3 figures, 5 tables. Camera-Ready Version for IJCNN 2021
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has recently been addressed from various directions, including discrete, sampling-based methods and efficient differentiable approaches. While the former are notoriously expensive, the latter suffer from imposing strong constraints on the search space. Architecture optimization from a learned embedding space for example through graph neural network based variational autoencoders builds a middle ground and leverages advantages from both sides. Such approaches have recently shown good performance on several benchmarks. Yet, their stability and predictive power heavily depends on their capacity to reconstruct networks from the embedding space. In this paper, we propose a two-sided variational graph autoencoder, which allows to smoothly encode and accurately reconstruct neural architectures from various search spaces. We evaluate the proposed approach on neural architectures defined by the ENAS approach, the NAS-Bench-101 and the NAS-Bench-201 search space and show that our smooth embedding space allows to directly extrapolate the performance prediction to architectures outside the seen domain (e.g. with more operations). Thus, it facilitates to predict good network architectures even without expensive Bayesian optimization or reinforcement learning.



### LaND: Learning to Navigate from Disengagements
- **Arxiv ID**: http://arxiv.org/abs/2010.04689v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04689v1)
- **Published**: 2020-10-09 17:21:42+00:00
- **Updated**: 2020-10-09 17:21:42+00:00
- **Authors**: Gregory Kahn, Pieter Abbeel, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Consistently testing autonomous mobile robots in real world scenarios is a necessary aspect of developing autonomous navigation systems. Each time the human safety monitor disengages the robot's autonomy system due to the robot performing an undesirable maneuver, the autonomy developers gain insight into how to improve the autonomy system. However, we believe that these disengagements not only show where the system fails, which is useful for troubleshooting, but also provide a direct learning signal by which the robot can learn to navigate. We present a reinforcement learning approach for learning to navigate from disengagements, or LaND. LaND learns a neural network model that predicts which actions lead to disengagements given the current sensory observation, and then at test time plans and executes actions that avoid disengagements. Our results demonstrate LaND can successfully learn to navigate in diverse, real world sidewalk environments, outperforming both imitation learning and reinforcement learning approaches. Videos, code, and other material are available on our website https://sites.google.com/view/sidewalk-learning



### Robust Isometric Non-Rigid Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2010.04690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04690v2)
- **Published**: 2020-10-09 17:25:00+00:00
- **Updated**: 2021-06-02 12:23:42+00:00
- **Authors**: Shaifali Parashar, Adrien Bartoli, Daniel Pizarro
- **Comment**: Accepted in TPAMI 2021
- **Journal**: None
- **Summary**: Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object from the correspondences established between monocular 2D images. Current NRSfM methods lack statistical robustness, which is the ability to cope with correspondence errors.This prevents one to use automatically established correspondences, which are prone to errors, thereby strongly limiting the scope of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by exploiting isometry. Step 1 computes the optical flow from correspondences, step 2 reconstructs each 3D point's normal vector using multiple reference images and integrates them to form surfaces with the best reference and step 3 rejects the 3D points that break isometry in their local neighborhood. Importantly, each step is designed to discard or flag erroneous correspondences. Our contributions include the robustification of optical flow by warp estimation, new fast analytic solutions to local normal reconstruction and their robustification, and a new scale-independent measure of 3D local isometric coherence. Experimental results show that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.



### PathoNet: Deep learning assisted evaluation of Ki-67 and tumor infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A large dataset and baseline
- **Arxiv ID**: http://arxiv.org/abs/2010.04713v3
- **DOI**: 10.1038/s41598-021-86912-w
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04713v3)
- **Published**: 2020-10-09 17:55:52+00:00
- **Updated**: 2021-04-28 22:20:20+00:00
- **Authors**: Farzin Negahbani, Rasool Sabzi, Bita Pakniyat Jahromi, Fateme Movahedi, Mahsa Kohandel Shirazi, Shayan Majidi, Dena Firouzabadi, Amirreza Dehghanian
- **Comment**: This is a preprint of an article published in Nature Publishing
  Group, Scientific Reports. The final authenticated version is available
  online at: https://www.nature.com/articles/s41598-021-86912-w
- **Journal**: Sci Rep 11, 8489 (2021)
- **Summary**: The nuclear protein Ki-67 and Tumor infiltrating lymphocytes (TILs) have been introduced as prognostic factors in predicting tumor progression and its treatment response. The value of the Ki-67 index and TILs in approach to heterogeneous tumors such as Breast cancer (BC), known as the most common cancer in women worldwide, has been highlighted in the literature. Due to the indeterminable and subjective nature of Ki-67 as well as TILs scoring, automated methods using machine learning, specifically approaches based on deep learning, have attracted attention. Yet, deep learning methods need considerable annotated data. In the absence of publicly available benchmarks for BC Ki-67 stained cell detection and further annotated classification of cells, we propose SHIDC-BC-Ki-67 as a dataset for the aforementioned purpose. We also introduce a novel pipeline and a backend, namely PathoNet for Ki-67 immunostained cell detection and classification and simultaneous determination of intratumoral TILs score. Further, we show that despite facing challenges, our proposed backend, PathoNet, outperforms the state of the art methods proposed to date in the harmonic mean measure.



### Unsupervised 3D Brain Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.04717v2
- **DOI**: 10.1007/978-3-030-72084-1_13
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04717v2)
- **Published**: 2020-10-09 17:59:17+00:00
- **Updated**: 2021-04-09 11:43:09+00:00
- **Authors**: Jaime Simarro, Ezequiel de la Rosa, Thijs Vande Vyvere, David Robben, Diana M. Sima
- **Comment**: Accepted at BrainLes Workshop in MICCAI 2020
- **Journal**: In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries. BrainLes 2020. Lecture Notes in Computer Science, vol 12658.
  Springer, Cham (2021)
- **Summary**: Anomaly detection (AD) is the identification of data samples that do not fit a learned data distribution. As such, AD systems can help physicians to determine the presence, severity, and extension of a pathology. Deep generative models, such as Generative Adversarial Networks (GANs), can be exploited to capture anatomical variability. Consequently, any outlier (i.e., sample falling outside of the learned distribution) can be detected as an abnormality in an unsupervised fashion. By using this method, we can not only detect expected or known lesions, but we can even unveil previously unrecognized biomarkers. To the best of our knowledge, this study exemplifies the first AD approach that can efficiently handle volumetric data and detect 3D brain anomalies in one single model. Our proposal is a volumetric and high-detail extension of the 2D f-AnoGAN model obtained by combining a state-of-the-art 3D GAN with refinement training steps. In experiments using non-contrast computed tomography images from traumatic brain injury (TBI) patients, the model detects and localizes TBI abnormalities with an area under the ROC curve of ~75%. Moreover, we test the potential of the method for detecting other anomalies such as low quality images, preprocessing inaccuracies, artifacts, and even the presence of post-operative signs (such as a craniectomy or a brain shunt). The method has potential for rapidly labeling abnormalities in massive imaging datasets, as well as identifying new biomarkers.



### Efficient Generalized Spherical CNNs
- **Arxiv ID**: http://arxiv.org/abs/2010.11661v3
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11661v3)
- **Published**: 2020-10-09 18:00:05+00:00
- **Updated**: 2021-03-08 11:55:27+00:00
- **Authors**: Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d'Avezac, Jason D. McEwen
- **Comment**: 20 pages, 4 figures, accepted by ICLR, code at
  https://www.kagenova.com/products/fourpiAI/
- **Journal**: None
- **Summary**: Many problems across computer vision and the natural sciences require the analysis of spherical data, for which representations may be learned efficiently by encoding equivariance to rotational symmetries. We present a generalized spherical CNN framework that encompasses various existing approaches and allows them to be leveraged alongside each other. The only existing non-linear spherical CNN layer that is strictly equivariant has complexity $\mathcal{O}(C^2L^5)$, where $C$ is a measure of representational capacity and $L$ the spherical harmonic bandlimit. Such a high computational cost often prohibits the use of strictly equivariant spherical CNNs. We develop two new strictly equivariant layers with reduced complexity $\mathcal{O}(CL^4)$ and $\mathcal{O}(CL^3 \log L)$, making larger, more expressive models computationally feasible. Moreover, we adopt efficient sampling theory to achieve further computational savings. We show that these developments allow the construction of more expressive hybrid models that achieve state-of-the-art accuracy and parameter efficiency on spherical benchmark problems.



### Predictive Modeling of Anatomy with Genetic and Clinical Data
- **Arxiv ID**: http://arxiv.org/abs/2010.04757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04757v1)
- **Published**: 2020-10-09 18:30:15+00:00
- **Updated**: 2020-10-09 18:30:15+00:00
- **Authors**: Adrian V. Dalca, Ramesh Sridharan, Mert R. Sabuncu, Polina Golland
- **Comment**: MICCAI 2015. Keywords: Neuroimaging, Anatomical prediction,
  Synthesis, Simulation, Genetics, Generative model, Linear mixed effects,
  Kernel Machines
- **Journal**: None
- **Summary**: We present a semi-parametric generative model for predicting anatomy of a patient in subsequent scans following a single baseline image. Such predictive modeling promises to facilitate novel analyses in both voxel-level studies and longitudinal biomarker evaluation. We capture anatomical change through a combination of population-wide regression and a non-parametric model of the subject's health based on individual genetic and clinical indicators. In contrast to classical correlation and longitudinal analysis, we focus on predicting new observations from a single subject observation. We demonstrate prediction of follow-up anatomical scans in the ADNI cohort, and illustrate a novel analysis approach that compares a patient's scans to the predicted subject-specific healthy anatomical trajectory. The code is available at https://github.com/adalca/voxelorb.



### Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.04767v4
- **DOI**: 10.4271/12-04-03-0023
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2010.04767v4)
- **Published**: 2020-10-09 19:03:15+00:00
- **Updated**: 2021-08-05 10:31:35+00:00
- **Authors**: Tanmay Vilas Samak, Chinmay Vilas Samak, Sivanathan Kandhasamy
- **Comment**: Accepted at SAE International Journal of Connected and Automated
  Vehicles
- **Journal**: SAE International Journal of Connected and Automated Vehicles,
  vol. 4, no. 3, pp. 279-295, 2021
- **Summary**: In this work, we present a lightweight pipeline for robust behavioral cloning of a human driver using end-to-end imitation learning. The proposed pipeline was employed to train and deploy three distinct driving behavior models onto a simulated vehicle. The training phase comprised of data collection, balancing, augmentation, preprocessing and training a neural network, following which, the trained model was deployed onto the ego vehicle to predict steering commands based on the feed from an onboard camera. A novel coupled control law was formulated to generate longitudinal control commands on-the-go based on the predicted steering angle and other parameters such as actual speed of the ego vehicle and the prescribed constraints for speed and steering. We analyzed computational efficiency of the pipeline and evaluated robustness of the trained models through exhaustive experimentation during the deployment phase. We also compared our approach against state-of-the-art implementation in order to comment on its validity.



### Cross-Spectral Iris Matching Using Conditional Coupled GAN
- **Arxiv ID**: http://arxiv.org/abs/2010.11689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11689v1)
- **Published**: 2020-10-09 19:13:24+00:00
- **Updated**: 2020-10-09 19:13:24+00:00
- **Authors**: Moktari Mostofa, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: International Joint Conference on Biometrics (IJCB-2020)
- **Journal**: None
- **Summary**: Cross-spectral iris recognition is emerging as a promising biometric approach to authenticating the identity of individuals. However, matching iris images acquired at different spectral bands shows significant performance degradation when compared to single-band near-infrared (NIR) matching due to the spectral gap between iris images obtained in the NIR and visual-light (VIS) spectra. Although researchers have recently focused on deep-learning-based approaches to recover invariant representative features for more accurate recognition performance, the existing methods cannot achieve the expected accuracy required for commercial applications. Hence, in this paper, we propose a conditional coupled generative adversarial network (CpGAN) architecture for cross-spectral iris recognition by projecting the VIS and NIR iris images into a low-dimensional embedding domain to explore the hidden relationship between them. The conditional CpGAN framework consists of a pair of GAN-based networks, one responsible for retrieving images in the visible domain and other responsible for retrieving images in the NIR domain. Both networks try to map the data into a common embedding subspace to ensure maximum pair-wise similarity between the feature vectors from the two iris modalities of the same subject. To prove the usefulness of our proposed approach, extensive experimental results obtained on the PolyU dataset are compared to existing state-of-the-art cross-spectral recognition methods.



### Cluster Activation Mapping with Applications to Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.04794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.04794v1)
- **Published**: 2020-10-09 20:37:09+00:00
- **Updated**: 2020-10-09 20:37:09+00:00
- **Authors**: Sarah Ryan, Nichole Carlson, Harris Butler, Tasha Fingerlin, Lisa Maier, Fuyong Xing
- **Comment**: None
- **Journal**: None
- **Summary**: An open question in deep clustering is how to understand what in the image is creating the cluster assignments. This visual understanding is essential to be able to trust the results of an inherently complex algorithm like deep learning, especially when the derived cluster assignments may be used to inform decision-making or create new disease sub-types. In this work, we developed novel methodology to generate CLuster Activation Mapping (CLAM) which combines an unsupervised deep clustering framework with a modification of Score-CAM, an approach for discriminative localization in the supervised setting. We evaluated our approach using a simulation study based on computed tomography scans of the lung, and applied it to 3D CT scans from a sarcoidosis population to identify new clusters of sarcoidosis based purely on CT scan presentation.



### Locally Linear Region Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2010.04812v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.04812v2)
- **Published**: 2020-10-09 21:23:53+00:00
- **Updated**: 2020-10-19 08:47:58+00:00
- **Authors**: Xiang Deng, Zhongfei, Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is an effective technique to transfer knowledge from one neural network (teacher) to another (student), thus improving the performance of the student. To make the student better mimic the behavior of the teacher, the existing work focuses on designing different criteria to align their logits or representations. Different from these efforts, we address knowledge distillation from a novel data perspective. We argue that transferring knowledge at sparse training data points cannot enable the student to well capture the local shape of the teacher function. To address this issue, we propose locally linear region knowledge distillation ($\rm L^2$RKD) which transfers the knowledge in local, linear regions from a teacher to a student. This is achieved by enforcing the student to mimic the outputs of the teacher function in local, linear regions. To the end, the student is able to better capture the local shape of the teacher function and thus achieves a better performance. Despite its simplicity, extensive experiments demonstrate that $\rm L^2$RKD is superior to the original KD in many aspects as it outperforms KD and the other state-of-the-art approaches by a large margin, shows robustness and superiority under few-shot settings, and is more compatible with the existing distillation approaches to further improve their performances significantly.



### Understanding Local Robustness of Deep Neural Networks under Natural Variations
- **Arxiv ID**: http://arxiv.org/abs/2010.04821v2
- **DOI**: None
- **Categories**: **cs.SE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.04821v2)
- **Published**: 2020-10-09 21:42:16+00:00
- **Updated**: 2021-01-23 02:46:18+00:00
- **Authors**: Ziyuan Zhong, Yuchi Tian, Baishakhi Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are being deployed in a wide range of settings today, from safety-critical applications like autonomous driving to commercial applications involving image classifications. However, recent research has shown that DNNs can be brittle to even slight variations of the input data. Therefore, rigorous testing of DNNs has gained widespread attention.   While DNN robustness under norm-bound perturbation got significant attention over the past few years, our knowledge is still limited when natural variants of the input images come. These natural variants, e.g. a rotated or a rainy version of the original input, are especially concerning as they can occur naturally in the field without any active adversary and may lead to undesirable consequences. Thus, it is important to identify the inputs whose small variations may lead to erroneous DNN behaviors. The very few studies that looked at DNN's robustness under natural variants, however, focus on estimating the overall robustness of DNNs across all the test data rather than localizing such error-producing points. This work aims to bridge this gap.   To this end, we study the local per-input robustness properties of the DNNs and leverage those properties to build a white-box (DeepRobust-W) and a black-box (DeepRobust-B) tool to automatically identify the non-robust points. Our evaluation of these methods on three DNN models spanning three widely used image classification datasets shows that they are effective in flagging points of poor robustness. In particular, DeepRobust-W and DeepRobust-B are able to achieve an F1 score of up to 91.4% and 99.1%, respectively. We further show that DeepRobust-W can be applied to a regression problem in another domain. Our evaluation on three self-driving car models demonstrates that DeepRobust-W is effective in identifying points of poor robustness with F1 score up to 78.9%.



### CurbScan: Curb Detection and Tracking Using Multi-Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/2010.04837v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.04837v2)
- **Published**: 2020-10-09 22:48:20+00:00
- **Updated**: 2020-10-13 00:28:21+00:00
- **Authors**: Iljoo Baek, Tzu-Chieh Tai, Manoj Bhat, Karun Ellango, Tarang Shah, Kamal Fuseini, Ragunathan, Rajkumar
- **Comment**: Accepted to IEEE ITSC-2020 conference
- **Journal**: None
- **Summary**: Reliable curb detection is critical for safe autonomous driving in urban contexts. Curb detection and tracking are also useful in vehicle localization and path planning. Past work utilized a 3D LiDAR sensor to determine accurate distance information and the geometric attributes of curbs. However, such an approach requires dense point cloud data and is also vulnerable to false positives from obstacles present on both road and off-road areas. In this paper, we propose an approach to detect and track curbs by fusing together data from multiple sensors: sparse LiDAR data, a mono camera and low-cost ultrasonic sensors. The detection algorithm is based on a single 3D LiDAR and a mono camera sensor used to detect candidate curb features and it effectively removes false positives arising from surrounding static and moving obstacles. The detection accuracy of the tracking algorithm is boosted by using Kalman filter-based prediction and fusion with lateral distance information from low-cost ultrasonic sensors. We next propose a line-fitting algorithm that yields robust results for curb locations. Finally, we demonstrate the practical feasibility of our solution by testing in different road environments and evaluating our implementation in a real vehicle\footnote{Demo video clips demonstrating our algorithm have been uploaded to Youtube: https://www.youtube.com/watch?v=w5MwsdWhcy4, https://www.youtube.com/watch?v=Gd506RklfG8.}. Our algorithm maintains over 90\% accuracy within 4.5-22 meters and 0-14 meters for the KITTI dataset and our dataset respectively, and its average processing time per frame is approximately 10 ms on Intel i7 x86 and 100ms on NVIDIA Xavier board.



