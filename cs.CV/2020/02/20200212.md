# Arxiv Papers in cs.CV on 2020-02-12
### Progressive Object Transfer Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.04741v2
- **DOI**: 10.1109/TIP.2019.2938680
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04741v2)
- **Published**: 2020-02-12 00:16:24+00:00
- **Updated**: 2020-02-13 05:06:51+00:00
- **Authors**: Hao Chen, Yali Wang, Guoyou Wang, Xiang Bai, Yu Qiao
- **Comment**: TIP 2019
- **Journal**: None
- **Summary**: Recent development of object detection mainly depends on deep learning with large-scale benchmarks. However, collecting such fully-annotated data is often difficult or expensive for real-world applications, which restricts the power of deep neural networks in practice. Alternatively, humans can detect new objects with little annotation burden, since humans often use the prior knowledge to identify new objects with few elaborately-annotated examples, and subsequently generalize this capacity by exploiting objects from wild images. Inspired by this procedure of learning to detect, we propose a novel Progressive Object Transfer Detection (POTD) framework. Specifically, we make three main contributions in this paper. First, POTD can leverage various object supervision of different domains effectively into a progressive detection procedure. Via such human-like learning, one can boost a target detection task with few annotations. Second, POTD consists of two delicate transfer stages, i.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer Detection (WSTD). In LSTD, we distill the implicit object knowledge of source detector to enhance target detector with few annotations. It can effectively warm up WSTD later on. In WSTD, we design a recurrent object labelling mechanism for learning to annotate weakly-labeled images. More importantly, we exploit the reliable object supervision from LSTD, which can further enhance the robustness of target detector in the WSTD stage. Finally, we perform extensive experiments on a number of challenging detection benchmarks with different settings. The results demonstrate that, our POTD outperforms the recent state-of-the-art approaches.



### Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale Chest Computed Tomography Volumes
- **Arxiv ID**: http://arxiv.org/abs/2002.04752v3
- **DOI**: 10.1016/j.media.2020.101857
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.04752v3)
- **Published**: 2020-02-12 00:59:23+00:00
- **Updated**: 2020-10-12 23:57:17+00:00
- **Authors**: Rachel Lea Draelos, David Dov, Maciej A. Mazurowski, Joseph Y. Lo, Ricardo Henao, Geoffrey D. Rubin, Lawrence Carin
- **Comment**: 20 pages, 3 figures, 5 tables (appendices additional). Published in
  Medical Image Analysis (October 2020)
- **Journal**: None
- **Summary**: Machine learning models for radiology benefit from large-scale data sets with high quality labels for abnormalities. We curated and analyzed a chest computed tomography (CT) data set of 36,316 volumes from 19,993 unique patients. This is the largest multiply-annotated volumetric medical imaging data set reported. To annotate this data set, we developed a rule-based method for automatically extracting abnormality labels from free-text radiology reports with an average F-score of 0.976 (min 0.941, max 1.0). We also developed a model for multi-organ, multi-disease classification of chest CT volumes that uses a deep convolutional neural network (CNN). This model reached a classification performance of AUROC greater than 0.90 for 18 abnormalities, with an average AUROC of 0.773 for all 83 abnormalities, demonstrating the feasibility of learning from unfiltered whole volume CT data. We show that training on more labels improves performance significantly: for a subset of 9 labels - nodule, opacity, atelectasis, pleural effusion, consolidation, mass, pericardial effusion, cardiomegaly, and pneumothorax - the model's average AUROC increased by 10% when the number of training labels was increased from 9 to all 83. All code for volume preprocessing, automated label extraction, and the volume abnormality prediction model will be made publicly available. The 36,316 CT volumes and labels will also be made publicly available pending institutional approval.



### Efficient Training of Deep Convolutional Neural Networks by Augmentation in Embedding Space
- **Arxiv ID**: http://arxiv.org/abs/2002.04776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.04776v1)
- **Published**: 2020-02-12 03:26:33+00:00
- **Updated**: 2020-02-12 03:26:33+00:00
- **Authors**: Mohammad Saeed Abrishami, Amir Erfan Eshratifar, David Eigen, Yanzhi Wang, Shahin Nazarian, Massoud Pedram
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in the field of artificial intelligence have been made possible by deep neural networks. In applications where data are scarce, transfer learning and data augmentation techniques are commonly used to improve the generalization of deep learning models. However, fine-tuning a transfer model with data augmentation in the raw input space has a high computational cost to run the full network for every augmented input. This is particularly critical when large models are implemented on embedded devices with limited computational and energy resources. In this work, we propose a method that replaces the augmentation in the raw input space with an approximate one that acts purely in the embedding space. Our experimental results show that the proposed method drastically reduces the computation, while the accuracy of models is negligibly compromised.



### MFFW: A new dataset for multi-focus image fusion
- **Arxiv ID**: http://arxiv.org/abs/2002.04780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.04780v1)
- **Published**: 2020-02-12 03:35:37+00:00
- **Updated**: 2020-02-12 03:35:37+00:00
- **Authors**: Shuang Xu, Xiaoli Wei, Chunxia Zhang, Junmin Liu, Jiangshe Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-focus image fusion (MFF) is a fundamental task in the field of computational photography. Current methods have achieved significant performance improvement. It is found that current methods are evaluated on simulated image sets or Lytro dataset. Recently, a growing number of researchers pay attention to defocus spread effect, a phenomenon of real-world multi-focus images. Nonetheless, defocus spread effect is not obvious in simulated or Lytro datasets, where popular methods perform very similar. To compare their performance on images with defocus spread effect, this paper constructs a new dataset called MFF in the wild (MFFW). It contains 19 pairs of multi-focus images collected on the Internet. We register all pairs of source images, and provide focus maps and reference images for part of pairs. Compared with Lytro dataset, images in MFFW significantly suffer from defocus spread effect. In addition, the scenes of MFFW are more complex. The experiments demonstrate that most state-of-the-art methods on MFFW dataset cannot robustly generate satisfactory fusion images. MFFW can be a new baseline dataset to test whether an MMF algorithm is able to deal with defocus spread effect.



### A Visual-inertial Navigation Method for High-Speed Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2002.04791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, cs.SY, eess.SY, math.DS, math.NA, math.OC, 65H17, 65J15, 65K05, 65L05
- **Links**: [PDF](http://arxiv.org/pdf/2002.04791v1)
- **Published**: 2020-02-12 04:28:11+00:00
- **Updated**: 2020-02-12 04:28:11+00:00
- **Authors**: Xin-long Luo, Jia-hui Lv, Geng Sun
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the localization problem of high-speed high-altitude unmanned aerial vehicle (UAV) with a monocular camera and inertial navigation system. It proposes a navigation method utilizing the complementarity of vision and inertial devices to overcome the singularity which arises from the horizontal flight of UAV. Furthermore, it modifies the mathematical model of localization problem via separating linear parts from nonlinear parts and replaces a nonlinear least-squares problem with a linearly equality-constrained optimization problem. In order to avoid the ill-condition property near the optimal point of sequential unconstrained minimization techniques(penalty methods), it constructs a semi-implicit continuous method with a trust-region technique based on a differential-algebraic dynamical system to solve the linearly equality-constrained optimization problem. It also analyzes the global convergence property of the semi-implicit continuous method in an infinity integrated interval other than the traditional convergence analysis of numerical methods for ordinary differential equations in a finite integrated interval. Finally, the promising numerical results are also presented.



### Fast and Scalable Complex Network Descriptor Using PageRank and Persistent Homology
- **Arxiv ID**: http://arxiv.org/abs/2002.05158v2
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.LG, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05158v2)
- **Published**: 2020-02-12 05:08:48+00:00
- **Updated**: 2020-09-12 03:33:20+00:00
- **Authors**: Mustafa Hajij, Elizabeth Munch, Paul Rosen
- **Comment**: None
- **Journal**: None
- **Summary**: The PageRank of a graph is a scalar function defined on the node set of the graph which encodes nodes centrality information of the graph. In this article, we use the PageRank function along with persistent homology to obtain a scalable graph descriptor and utilize it to compare the similarities between graphs. For a given graph $G(V,E)$, our descriptor can be computed in $O(|E|\alpha(|V|))$, where $\alpha$ is the inverse Ackermann function which makes it scalable and computable on massive graphs. We show the effectiveness of our method by utilizing it on multiple shape mesh datasets.



### Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis of Femoral Head
- **Arxiv ID**: http://arxiv.org/abs/2002.05536v2
- **DOI**: 10.1109/JBHI.2020.3037079
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2002.05536v2)
- **Published**: 2020-02-12 05:55:50+00:00
- **Updated**: 2020-11-10 14:06:56+00:00
- **Authors**: Yang Li, Yan Li, Hua Tian
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: As the first diagnostic imaging modality of avascular necrosis of the femoral head (AVNFH), accurately staging AVNFH from a plain radiograph is critical yet challenging for orthopedists. Thus, we propose a deep learning-based AVNFH diagnosis system (AVN-net). The proposed AVN-net reads plain radiographs of the pelvis, conducts diagnosis, and visualizes results automatically. Deep convolutional neural networks are trained to provide an end-to-end diagnosis solution, covering tasks of femoral head detection, exam-view identification, side classification, AVNFH diagnosis, and key clinical notes generation. AVN-net is able to obtain state-of-the-art testing AUC of 0.97 (95% CI: 0.97-0.98) in AVNFH detection and significantly greater F1 scores than less-to-moderately experienced orthopedists in all diagnostic tests (p<0.01). Furthermore, two real-world pilot studies were conducted for diagnosis support and education assistance, respectively, to assess the utility of AVN-net. The experimental results are promising. With the AVN-net diagnosis as a reference, the diagnostic accuracy and consistency of all orthopedists considerably improved while requiring only 1/4 of the time. Students self-studying the AVNFH diagnosis using AVN-net can learn better and faster than the control group. To the best of our knowledge, this study is the first research on the prospective use of a deep learning-based diagnosis system for AVNFH by conducting two pilot studies representing real-world application scenarios. We have demonstrated that the proposed AVN-net achieves expert-level AVNFH diagnosis performance, provides efficient support in clinical decision-making, and effectively passes clinical experience to students.



### FPGA Implementation of Minimum Mean Brightness Error Bi-Histogram Equalization
- **Arxiv ID**: http://arxiv.org/abs/2003.00840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00840v1)
- **Published**: 2020-02-12 06:42:19+00:00
- **Updated**: 2020-02-12 06:42:19+00:00
- **Authors**: Abhishek Saroha, Avichal Rakesh, Rajiv Kumar Tripathi
- **Comment**: None
- **Journal**: None
- **Summary**: Histogram Equalization (HE) is a popular method for contrast enhancement. Generally, mean brightness is not conserved in Histogram Equalization. Initially, Bi-Histogram Equalization (BBHE) was proposed to enhance contrast while maintaining a the mean brightness. However, when mean brightness is primary concern, Minimum Mean Brightness Error Bi-Histogram Equalization (MMBEBHE) is the best technique. There are several implementations of Histogram Equalization on FPGA, however to our knowledge MMBEBHE has not been implemented on FPGAs before. Therefore, we present an implementation of MMBEBHE on FPGA.



### Deep Variational Luenberger-type Observer for Stochastic Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00835v1)
- **Published**: 2020-02-12 06:59:04+00:00
- **Updated**: 2020-02-12 06:59:04+00:00
- **Authors**: Dong Wang, Feng Zhou, Zheng Yan, Guang Yao, Zongxuan Liu, Wennan Ma, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Considering the inherent stochasticity and uncertainty, predicting future video frames is exceptionally challenging. In this work, we study the problem of video prediction by combining interpretability of stochastic state space models and representation learning of deep neural networks. Our model builds upon an variational encoder which transforms the input video into a latent feature space and a Luenberger-type observer which captures the dynamic evolution of the latent features. This enables the decomposition of videos into static features and dynamics in an unsupervised manner. By deriving the stability theory of the nonlinear Luenberger-type observer, the hidden states in the feature space become insensitive with respect to the initial values, which improves the robustness of the overall model. Furthermore, the variational lower bound on the data log-likelihood can be derived to obtain the tractable posterior prediction distribution based on the variational principle. Finally, the experiments such as the Bouncing Balls dataset and the Pendulum dataset are provided to demonstrate the proposed model outperforms concurrent works.



### Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic Conditions
- **Arxiv ID**: http://arxiv.org/abs/2002.04821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04821v1)
- **Published**: 2020-02-12 07:00:07+00:00
- **Updated**: 2020-02-12 07:00:07+00:00
- **Authors**: Mohammad Sabokrou, Masoud Pourreza, Xiaobai Li, Mahmood Fathy, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method for remote heart rate (HR) estimation. Recent studies have proved that blood pumping by the heart is highly correlated to the intense color of face pixels, and surprisingly can be utilized for remote HR estimation. Researchers successfully proposed several methods for this task, but making it work in realistic situations is still a challenging problem in computer vision community. Furthermore, learning to solve such a complex task on a dataset with very limited annotated samples is not reasonable. Consequently, researchers do not prefer to use the deep learning approaches for this problem. In this paper, we propose a simple yet efficient approach to benefit the advantages of the Deep Neural Network (DNN) by simplifying HR estimation from a complex task to learning from very correlated representation to HR. Inspired by previous work, we learn a component called Front-End (FE) to provide a discriminative representation of face videos, afterward a light deep regression auto-encoder as Back-End (BE) is learned to map the FE representation to HR. Regression task on the informative representation is simple and could be learned efficiently on limited training samples. Beside of this, to be more accurate and work well on low-quality videos, two deep encoder-decoder networks are trained to refine the output of FE. We also introduce a challenging dataset (HR-D) to show that our method can efficiently work in realistic conditions. Experimental results on HR-D and MAHNOB datasets confirm that our method could run as a real-time method and estimate the average HR better than state-of-the-art ones.



### Uniform Interpolation Constrained Geodesic Learning on Data Manifold
- **Arxiv ID**: http://arxiv.org/abs/2002.04829v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.04829v4)
- **Published**: 2020-02-12 07:47:41+00:00
- **Updated**: 2020-08-14 05:32:56+00:00
- **Authors**: Cong Geng, Jia Wang, Li Chen, Wenbo Bao, Chu Chu, Zhiyong Gao
- **Comment**: submitted to NIPS 2020
- **Journal**: None
- **Summary**: In this paper, we propose a method to learn a minimizing geodesic within a data manifold. Along the learned geodesic, our method can generate high-quality interpolations between two given data samples. Specifically, we use an autoencoder network to map data samples into latent space and perform interpolation via an interpolation network. We add prior geometric information to regularize our autoencoder for the convexity of representations so that for any given interpolation approach, the generated interpolations remain within the distribution of the data manifold. Before the learning of a geodesic, a proper Riemannianmetric should be defined. Therefore, we induce a Riemannian metric by the canonical metric in the Euclidean space which the data manifold is isometrically immersed in. Based on this defined Riemannian metric, we introduce a constant speed loss and a minimizing geodesic loss to regularize the interpolation network to generate uniform interpolation along the learned geodesic on the manifold. We provide a theoretical analysis of our model and use image translation as an example to demonstrate the effectiveness of our method.



### End-to-End Face Parsing via Interlinked Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.04831v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04831v2)
- **Published**: 2020-02-12 08:03:03+00:00
- **Updated**: 2020-06-23 19:27:52+00:00
- **Authors**: Zi Yin, Valentin Yiu, Xiaolin Hu, Liang Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Face parsing is an important computer vision task that requires accurate pixel segmentation of facial parts (such as eyes, nose, mouth, etc.), providing a basis for further face analysis, modification, and other applications. Interlinked Convolutional Neural Networks (iCNN) was proved to be an effective two-stage model for face parsing. However, the original iCNN was trained separately in two stages, limiting its performance. To solve this problem, we introduce a simple, end-to-end face parsing framework: STN-aided iCNN(STN-iCNN), which extends the iCNN by adding a Spatial Transformer Network (STN) between the two isolated stages. The STN-iCNN uses the STN to provide a trainable connection to the original two-stage iCNN pipeline, making end-to-end joint training possible. Moreover, as a by-product, STN also provides more precise cropped parts than the original cropper. Due to these two advantages, our approach significantly improves the accuracy of the original model. Our model achieved competitive performance on the Helen Dataset, the standard face parsing dataset. It also achieved superior performance on CelebAMask-HQ dataset, proving its good generalization. Our code has been released at https://github.com/aod321/STN-iCNN.



### Analysis Of Multi Field Of View Cnn And Attention Cnn On H&E Stained Whole-slide Images On Hepatocellular Carcinoma
- **Arxiv ID**: http://arxiv.org/abs/2002.04836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.04836v2)
- **Published**: 2020-02-12 08:18:40+00:00
- **Updated**: 2020-02-18 23:25:48+00:00
- **Authors**: Mehmet Burak Sayıcı, Rikiya Yamashita, Jeanne Shen
- **Comment**: This paper has been withdrawn by the authors due to need for heavy
  revise
- **Journal**: None
- **Summary**: Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death worldwide. Whole-slide imaging which is a method of scanning glass slides have been employed for diagnosis of HCC. Using high resolution Whole-slide images is infeasible for Convolutional Neural Network applications. Hence tiling the Whole-slide images is a common methodology for assigning Convolutional Neural Networks for classification and segmentation. Determination of the tile size affects the performance of the algorithms since small field of view can not capture the information on a larger scale and large field of view can not capture the information on a cellular scale. In this work, the effect of tile size on performance for classification problem is analysed. In addition, Multi Field of View CNN is assigned for taking advantage of the information provided by different tile sizes and Attention CNN is assigned for giving the capability of voting most contributing tile size. It is found that employing more than one tile size significantly increases the performance of the classification by 3.97% and both algorithms are found successful over the algorithm which uses only one tile size.



### Abnormal respiratory patterns classifier may contribute to large-scale screening of people infected with COVID-19 in an accurate and unobtrusive manner
- **Arxiv ID**: http://arxiv.org/abs/2002.05534v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2002.05534v2)
- **Published**: 2020-02-12 09:42:57+00:00
- **Updated**: 2020-12-21 03:57:55+00:00
- **Authors**: Yunlu Wang, Menghan Hu, Qingli Li, Xiao-Ping Zhang, Guangtao Zhai, Nan Yao
- **Comment**: 6 page, 3 figure
- **Journal**: None
- **Summary**: Research significance: The extended version of this paper has been accepted by IEEE Internet of Things journal (DOI: 10.1109/JIOT.2020.2991456), please cite the journal version. During the epidemic prevention and control period, our study can be helpful in prognosis, diagnosis and screening for the patients infected with COVID-19 (the novel coronavirus) based on breathing characteristics. According to the latest clinical research, the respiratory pattern of COVID-19 is different from the respiratory patterns of flu and the common cold. One significant symptom that occurs in the COVID-19 is Tachypnea. People infected with COVID-19 have more rapid respiration. Our study can be utilized to distinguish various respiratory patterns and our device can be preliminarily put to practical use. Demo videos of this method working in situations of one subject and two subjects can be downloaded online. Research details: Accurate detection of the unexpected abnormal respiratory pattern of people in a remote and unobtrusive manner has great significance. In this work, we innovatively capitalize on depth camera and deep learning to achieve this goal. The challenges in this task are twofold: the amount of real-world data is not enough for training to get the deep model; and the intra-class variation of different types of respiratory patterns is large and the outer-class variation is small. In this paper, considering the characteristics of actual respiratory signals, a novel and efficient Respiratory Simulation Model (RSM) is first proposed to fill the gap between the large amount of training data and scarce real-world data. The proposed deep model and the modeling ideas have the great potential to be extended to large scale applications such as public places, sleep scenario, and office environment.



### Bi-Directional Generation for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.04869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04869v1)
- **Published**: 2020-02-12 09:45:39+00:00
- **Updated**: 2020-02-12 09:45:39+00:00
- **Authors**: Guanglei Yang, Haifeng Xia, Mingli Ding, Zhengming Ding
- **Comment**: 9 pages, 4 figures
- **Journal**: Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI),
  2020
- **Summary**: Unsupervised domain adaptation facilitates the unlabeled target domain relying on well-established source domain information. The conventional methods forcefully reducing the domain discrepancy in the latent space will result in the destruction of intrinsic data structure. To balance the mitigation of domain gap and the preservation of the inherent structure, we propose a Bi-Directional Generation domain adaptation model with consistent classifiers interpolating two intermediate domains to bridge source and target domains. Specifically, two cross-domain generators are employed to synthesize one domain conditioned on the other. The performance of our proposed method can be further enhanced by the consistent classifiers and the cross-domain alignment constraints. We also design two classifiers which are jointly optimized to maximize the consistency on target sample prediction. Extensive experiments verify that our proposed model outperforms the state-of-the-art on standard cross domain visual benchmarks.



### Boosting rare benthic macroinvertebrates taxa identification with one-class classification
- **Arxiv ID**: http://arxiv.org/abs/2002.10420v1
- **DOI**: 10.1109/SSCI47803.2020.9308359
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10420v1)
- **Published**: 2020-02-12 09:46:24+00:00
- **Updated**: 2020-02-12 09:46:24+00:00
- **Authors**: Fahad Sohrab, Jenni Raitoharju
- **Comment**: 5 pages, 1 figure, 2 tables
- **Journal**: 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Summary**: Insect monitoring is crucial for understanding the consequences of rapid ecological changes, but taxa identification currently requires tedious manual expert work and cannot be scaled-up efficiently. Deep convolutional neural networks (CNNs), provide a viable way to significantly increase the biomonitoring volumes. However, taxa abundances are typically very imbalanced and the amounts of training images for the rarest classes are simply too low for deep CNNs. As a result, the samples from the rare classes are often completely missed, while detecting them has biological importance. In this paper, we propose combining the trained deep CNN with one-class classifiers to improve the rare species identification. One-class classification models are traditionally trained with much fewer samples and they can provide a mechanism to indicate samples potentially belonging to the rare classes for human inspection. Our experiments confirm that the proposed approach may indeed support moving towards partial automation of the taxa identification task.



### CALVIS: chest, waist and pelvis circumference from 3D human body meshes as ground truth for deep learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00834v1)
- **Published**: 2020-02-12 10:36:15+00:00
- **Updated**: 2020-02-12 10:36:15+00:00
- **Authors**: Yansel Gonzalez Tejeda, Helmut Mayer
- **Comment**: 14 pages, 6 figures. To appear in the Proceedings of the VIII
  International Workshop on Representation, analysis and recognition of shape
  and motion FroM Imaging data (RFMI 2019), 11-13 December 2019, Sidi Bou Said,
  Tunisia
- **Journal**: None
- **Summary**: In this paper we present CALVIS, a method to calculate $\textbf{C}$hest, w$\textbf{A}$ist and pe$\textbf{LVIS}$ circumference from 3D human body meshes. Our motivation is to use this data as ground truth for training convolutional neural networks (CNN). Previous work had used the large scale CAESAR dataset or determined these anthropometrical measurements $\textit{manually}$ from a person or human 3D body meshes. Unfortunately, acquiring these data is a cost and time consuming endeavor. In contrast, our method can be used on 3D meshes automatically. We synthesize eight human body meshes and apply CALVIS to calculate chest, waist and pelvis circumference. We evaluate the results qualitatively and observe that the measurements can indeed be used to estimate the shape of a person. We then asses the plausibility of our approach by generating ground truth with CALVIS to train a small CNN. After having trained the network with our data, we achieve competitive validation error. Furthermore, we make the implementation of CALVIS publicly available to advance the field.



### A Zero-Shot based Fingerprint Presentation Attack Detection System
- **Arxiv ID**: http://arxiv.org/abs/2002.04908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04908v1)
- **Published**: 2020-02-12 10:52:38+00:00
- **Updated**: 2020-02-12 10:52:38+00:00
- **Authors**: Haozhe Liu, Wentian Zhang, Guojie Liu, Feng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of presentation attacks, Automated Fingerprint Recognition Systems(AFRSs) are vulnerable to presentation attack. Thus, numerous methods of presentation attack detection(PAD) have been proposed to ensure the normal utilization of AFRS. However, the demand of large-scale presentation attack images and the low-level generalization ability always astrict existing PAD methods' actual performances. Therefore, we propose a novel Zero-Shot Presentation Attack Detection Model to guarantee the generalization of the PAD model. The proposed ZSPAD-Model based on generative model does not utilize any negative samples in the process of establishment, which ensures the robustness for various types or materials based presentation attack. Different from other auto-encoder based model, the Fine-grained Map architecture is proposed to refine the reconstruction error of the auto-encoder networks and a task-specific gaussian model is utilized to improve the quality of clustering. Meanwhile, in order to improve the performance of the proposed model, 9 confidence scores are discussed in this article. Experimental results showed that the ZSPAD-Model is the state of the art for ZSPAD, and the MS-Score is the best confidence score. Compared with existing methods, the proposed ZSPAD-Model performs better than the feature-based method and under the multi-shot setting, the proposed method overperforms the learning based method with little training data. When large training data is available, their results are similar.



### Synaptic Integration of Spatiotemporal Features with a Dynamic Neuromorphic Processor
- **Arxiv ID**: http://arxiv.org/abs/2002.04924v2
- **DOI**: 10.1109/IJCNN48605.2020.9207210
- **Categories**: **cs.NE**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2002.04924v2)
- **Published**: 2020-02-12 11:26:35+00:00
- **Updated**: 2021-06-01 14:05:32+00:00
- **Authors**: Mattias Nilsson, Foteini Liwicki, Fredrik Sandin
- **Comment**: Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN),
  2020, pp. 1-7
- **Summary**: Spiking neurons can perform spatiotemporal feature detection by nonlinear synaptic and dendritic integration of presynaptic spike patterns. Multicompartment models of non-linear dendrites and related neuromorphic circuit designs enable faithful imitation of such dynamic integration processes, but these approaches are also associated with a relatively high computing cost or circuit size. Here, we investigate synaptic integration of spatiotemporal spike patterns with multiple dynamic synapses on point-neurons in the DYNAP-SE neuromorphic processor, which offers a complementary resource-efficient, albeit less flexible, approach to feature detection. We investigate how previously proposed excitatory--inhibitory pairs of dynamic synapses can be combined to integrate multiple inputs, and we generalize that concept to a case in which one inhibitory synapse is combined with multiple excitatory synapses. We characterize the resulting delayed excitatory postsynaptic potentials (EPSPs) by measuring and analyzing the membrane potentials of the neuromorphic neuronal circuits. We find that biologically relevant EPSP delays, with variability of order 10 milliseconds per neuron, can be realized in the proposed manner by selecting different synapse combinations, thanks to device mismatch. Based on these results, we demonstrate that a single point-neuron with dynamic synapses in the DYNAP-SE can respond selectively to presynaptic spikes with a particular spatiotemporal structure, which enables, for instance, visual feature tuning of single neurons.



### Towards Precise Intra-camera Supervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2002.04932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04932v2)
- **Published**: 2020-02-12 11:56:30+00:00
- **Updated**: 2020-12-11 07:14:23+00:00
- **Authors**: Menglin Wang, Baisheng Lai, Haokun Chen, Jianqiang Huang, Xiaojin Gong, Xian-Sheng Hua
- **Comment**: Accepted by WACV2021
- **Journal**: None
- **Summary**: Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes that identity labels are independently annotated within each camera view and no inter-camera identity association is labeled. It is a new setting proposed recently to reduce the burden of annotation while expect to maintain desirable Re-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID problem much more challenging than the fully supervised counterpart. By investigating the characteristics of ICS, this paper proposes camera-specific non-parametric classifiers, together with a hybrid mining quintuplet loss, to perform intra-camera learning. Then, an inter-camera learning module consisting of a graph-based ID association step and a Re-ID model updating step is conducted. Extensive experiments on three large-scale Re-ID datasets show that our approach outperforms all existing ICS works by a great margin. Our approach performs even comparable to state-of-the-art fully supervised methods in two of the datasets.



### Saliency Driven Perceptual Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2002.04988v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04988v2)
- **Published**: 2020-02-12 13:43:17+00:00
- **Updated**: 2020-11-08 17:03:14+00:00
- **Authors**: Yash Patel, Srikar Appalaraju, R. Manmatha
- **Comment**: WACV 2021 camera-ready version
- **Journal**: None
- **Summary**: This paper proposes a new end-to-end trainable model for lossy image compression, which includes several novel components. The method incorporates 1) an adequate perceptual similarity metric; 2) saliency in the images; 3) a hierarchical auto-regressive model. This paper demonstrates that the popularly used evaluations metrics such as MS-SSIM and PSNR are inadequate for judging the performance of image compression techniques as they do not align with the human perception of similarity. Alternatively, a new metric is proposed, which is learned on perceptual similarity data specific to image compression. The proposed compression model incorporates the salient regions and optimizes on the proposed perceptual similarity metric. The model not only generates images which are visually better but also gives superior performance for subsequent computer vision tasks such as object detection and segmentation when compared to existing engineered or learned compression techniques.



### Real-Time Semantic Background Subtraction
- **Arxiv ID**: http://arxiv.org/abs/2002.04993v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04993v3)
- **Published**: 2020-02-12 13:46:01+00:00
- **Updated**: 2020-05-27 08:49:42+00:00
- **Authors**: Anthony Cioppa, Marc Van Droogenbroeck, Marc Braham
- **Comment**: Accepted and Published at ICIP 2020
- **Journal**: None
- **Summary**: Semantic background subtraction SBS has been shown to improve the performance of most background subtraction algorithms by combining them with semantic information, derived from a semantic segmentation network. However, SBS requires high-quality semantic segmentation masks for all frames, which are slow to compute. In addition, most state-of-the-art background subtraction algorithms are not real-time, which makes them unsuitable for real-world applications. In this paper, we present a novel background subtraction algorithm called Real-Time Semantic Background Subtraction (denoted RT-SBS) which extends SBS for real-time constrained applications while keeping similar performances. RT-SBS effectively combines a real-time background subtraction algorithm with high-quality semantic information which can be provided at a slower pace, independently for each pixel. We show that RT-SBS coupled with ViBe sets a new state of the art for real-time background subtraction algorithms and even competes with the non real-time state-of-the-art ones. Note that we provide python CPU and GPU implementations of RT-SBS at https://github.com/cioppaanthony/rt-sbs.



### Learning light field synthesis with Multi-Plane Images: scene encoding as a recurrent segmentation task
- **Arxiv ID**: http://arxiv.org/abs/2002.05028v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05028v3)
- **Published**: 2020-02-12 14:35:54+00:00
- **Updated**: 2020-05-19 11:25:09+00:00
- **Authors**: Tomás Völker, Guillaume Boisson, Bertrand Chupeau
- **Comment**: Accepted to ICIP 2020
- **Journal**: None
- **Summary**: In this paper we address the problem of view synthesis from large baseline light fields, by turning a sparse set of input views into a Multi-plane Image (MPI). Because available datasets are scarce, we propose a lightweight network that does not require extensive training. Unlike latest approaches, our model does not learn to estimate RGB layers but only encodes the scene geometry within MPI alpha layers, which comes down to a segmentation task. A Learned Gradient Descent (LGD) framework is used to cascade the same convolutional network in a recurrent fashion in order to refine the volumetric representation obtained. Thanks to its low number of parameters, our model trains successfully on a small light field video dataset and provides visually appealing results. It also exhibits convenient generalization properties regarding both the number of input views, the number of depth planes in the MPI, and the number of refinement iterations.



### CNN Hyperparameter tuning applied to Iris Liveness Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00833v1)
- **Published**: 2020-02-12 15:00:46+00:00
- **Updated**: 2020-02-12 15:00:46+00:00
- **Authors**: Gabriela Y. Kimura, Diego R. Lucio, Alceu S. Britto Jr., David Menotti
- **Comment**: Accepted for presentation at the International Conference on Computer
  Vision Theory and Applications (VISAPP 2020)
- **Journal**: None
- **Summary**: The iris pattern has significantly improved the biometric recognition field due to its high level of stability and uniqueness. Such physical feature has played an important role in security and other related areas. However, presentation attacks, also known as spoofing techniques, can be used to bypass the biometric system with artifacts such as printed images, artificial eyes, and textured contact lenses. To improve the security of these systems, many liveness detection methods have been proposed, and the first Internacional Iris Liveness Detection competition was launched in 2013 to evaluate their effectiveness. In this paper, we propose a hyperparameter tuning of the CASIA algorithm, submitted by the Chinese Academy of Sciences to the third competition of Iris Liveness Detection, in 2017. The modifications proposed promoted an overall improvement, with an 8.48% Attack Presentation Classification Error Rate (APCER) and 0.18% Bonafide Presentation Classification Error Rate (BPCER) for the evaluation of the combined datasets. Other threshold values were evaluated in an attempt to reduce the trade-off between the APCER and the BPCER on the evaluated datasets and worked out successfully.



### Intra-Camera Supervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2002.05046v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05046v3)
- **Published**: 2020-02-12 15:26:33+00:00
- **Updated**: 2021-01-16 06:55:06+00:00
- **Authors**: Xiangping Zhu, Xiatian Zhu, Minxian Li, Pietro Morerio, Vittorio Murino, Shaogang Gong
- **Comment**: Accepted to IJCV
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors.



### Detect and Correct Bias in Multi-Site Neuroimaging Datasets
- **Arxiv ID**: http://arxiv.org/abs/2002.05049v2
- **DOI**: 10.1016/j.media.2020.101879
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05049v2)
- **Published**: 2020-02-12 15:32:24+00:00
- **Updated**: 2020-10-27 20:11:25+00:00
- **Authors**: Christian Wachinger, Anna Rieckmann, Sebastian Pölsterl
- **Comment**: None
- **Journal**: Medical Image Analysis, 2020
- **Summary**: The desire to train complex machine learning algorithms and to increase the statistical power in association studies drives neuroimaging research to use ever-larger datasets. The most obvious way to increase sample size is by pooling scans from independent studies. However, simple pooling is often ill-advised as selection, measurement, and confounding biases may creep in and yield spurious correlations. In this work, we combine 35,320 magnetic resonance images of the brain from 17 studies to examine bias in neuroimaging. In the first experiment, Name That Dataset, we provide empirical evidence for the presence of bias by showing that scans can be correctly assigned to their respective dataset with 71.5% accuracy. Given such evidence, we take a closer look at confounding bias, which is often viewed as the main shortcoming in observational studies. In practice, we neither know all potential confounders nor do we have data on them. Hence, we model confounders as unknown, latent variables. Kolmogorov complexity is then used to decide whether the confounded or the causal model provides the simplest factorization of the graphical model. Finally, we present methods for dataset harmonization and study their ability to remove bias in imaging features. In particular, we propose an extension of the recently introduced ComBat algorithm to control for global variation across image features, inspired by adjusting for population stratification in genetics. Our results demonstrate that harmonization can reduce dataset-specific information in image features. Further, confounding bias can be reduced and even turned into a causal relationship. However, harmonziation also requires caution as it can easily remove relevant subject-specific information. Code is available at https://github.com/ai-med/Dataset-Bias.



### An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.00832v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.00832v1)
- **Published**: 2020-02-12 15:33:59+00:00
- **Updated**: 2020-02-12 15:33:59+00:00
- **Authors**: Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Emotion recognition in user-generated videos plays an important role in human-centered computing. Existing methods mainly employ traditional two-stage shallow pipeline, i.e. extracting visual and/or audio features and training classifiers. In this paper, we propose to recognize video emotions in an end-to-end manner based on convolutional neural networks (CNNs). Specifically, we develop a deep Visual-Audio Attention Network (VAANet), a novel architecture that integrates spatial, channel-wise, and temporal attentions into a visual 3D CNN and temporal attentions into an audio 2D CNN. Further, we design a special classification loss, i.e. polarity-consistent cross-entropy loss, based on the polarity-emotion hierarchy constraint to guide the attention generation. Extensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6 datasets demonstrate that the proposed VAANet outperforms the state-of-the-art approaches for video emotion recognition. Our source code is released at: https://github.com/maysonma/VAANet.



### Fast Generation of High Fidelity RGB-D Images by Deep-Learning with Adaptive Convolution
- **Arxiv ID**: http://arxiv.org/abs/2002.05067v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05067v3)
- **Published**: 2020-02-12 16:14:38+00:00
- **Updated**: 2020-06-12 04:10:34+00:00
- **Authors**: Chuhua Xian, Dongjiu Zhang, Chengkai Dai, Charlie C. L. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Using the raw data from consumer-level RGB-D cameras as input, we propose a deep-learning based approach to efficiently generate RGB-D images with completed information in high resolution. To process the input images in low resolution with missing regions, new operators for adaptive convolution are introduced in our deep-learning network that consists of three cascaded modules -- the completion module, the refinement module and the super-resolution module. The completion module is based on an architecture of encoder-decoder, where the features of input raw RGB-D will be automatically extracted by the encoding layers of a deep neural-network. The decoding layers are applied to reconstruct the completed depth map, which is followed by a refinement module to sharpen the boundary of different regions. For the super-resolution module, we generate RGB-D images in high resolution by multiple layers for feature extraction and a layer for up-sampling. Benefited from the adaptive convolution operators newly proposed in this paper, our results outperform the existing deep-learning based approaches for RGB-D image complete and super-resolution. As an end-to-end approach, high fidelity RGB-D images can be generated efficiently at the rate of around 21 frames per second.



### AlignNet: A Unifying Approach to Audio-Visual Alignment
- **Arxiv ID**: http://arxiv.org/abs/2002.05070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.05070v1)
- **Published**: 2020-02-12 16:19:28+00:00
- **Updated**: 2020-02-12 16:19:28+00:00
- **Authors**: Jianren Wang, Zhaoyuan Fang, Hang Zhao
- **Comment**: WACV2020. Project video and code are available at
  https://jianrenw.github.io/AlignNet
- **Journal**: None
- **Summary**: We present AlignNet, a model that synchronizes videos with reference audios under non-uniform and irregular misalignments. AlignNet learns the end-to-end dense correspondence between each frame of a video and an audio. Our method is designed according to simple and well-established principles: attention, pyramidal processing, warping, and affinity function. Together with the model, we release a dancing dataset Dance50 for training and evaluation. Qualitative, quantitative and subjective evaluation results on dance-music alignment and speech-lip alignment demonstrate that our method far outperforms the state-of-the-art methods. Project video and code are available at https://jianrenw.github.io/AlignNet.



### Component Analysis for Visual Question Answering Architectures
- **Arxiv ID**: http://arxiv.org/abs/2002.05104v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05104v2)
- **Published**: 2020-02-12 17:25:50+00:00
- **Updated**: 2020-03-27 01:08:38+00:00
- **Authors**: Camila Kolling, Jônatas Wehrmann, Rodrigo C. Barros
- **Comment**: None
- **Journal**: 2020 - The International Joint Conference on Neural Networks
  (IJCNN)
- **Summary**: Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.



### Analysis of Dutch Master Paintings with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.05107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05107v3)
- **Published**: 2020-02-12 17:32:18+00:00
- **Updated**: 2020-08-16 13:29:19+00:00
- **Authors**: Steven J. Frank, Andrea M. Frank
- **Comment**: None
- **Journal**: None
- **Summary**: Trained on the works of an artist under study and visually comparable works of other artists, convolutional neural networks can identify forgeries and provide attributions. They can also assign classification probabilities within a painting, revealing mixed authorship and identifying regions painted by different hands.



### Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.05123v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05123v4)
- **Published**: 2020-02-12 17:58:12+00:00
- **Updated**: 2021-06-04 22:11:54+00:00
- **Authors**: Roi Pony, Itay Naeh, Shie Mannor
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks for video classification, just like image classification networks, may be subjected to adversarial manipulation. The main difference between image classifiers and video classifiers is that the latter usually use temporal information contained within the video. In this work we present a manipulation scheme for fooling video classifiers by introducing a flickering temporal perturbation that in some cases may be unnoticeable by human observers and is implementable in the real world. After demonstrating the manipulation of action classification of single videos, we generalize the procedure to make universal adversarial perturbation, achieving high fooling ratio. In addition, we generalize the universal perturbation and produce a temporal-invariant perturbation, which can be applied to the video without synchronizing the perturbation to the input. The attack was implemented on several target models and the transferability of the attack was demonstrated. These properties allow us to bridge the gap between simulated environment and real-world application, as will be demonstrated in this paper for the first time for an over-the-air flickering attack.



### Real or Not Real, that is the Question
- **Arxiv ID**: http://arxiv.org/abs/2002.05512v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05512v1)
- **Published**: 2020-02-12 18:41:55+00:00
- **Updated**: 2020-02-12 18:41:55+00:00
- **Authors**: Yuanbo Xiangli, Yubin Deng, Bo Dai, Chen Change Loy, Dahua Lin
- **Comment**: ICLR2020 spotlight. 1) train GAN by maximizing kl-divergence. 2)
  train non-progressive GAN (DCGAN) architecture at 1024*1024 resolution
- **Journal**: None
- **Summary**: While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. Compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch.



### Constrained Dominant sets and Its applications in computer vision
- **Arxiv ID**: http://arxiv.org/abs/2002.06028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06028v1)
- **Published**: 2020-02-12 20:19:44+00:00
- **Updated**: 2020-02-12 20:19:44+00:00
- **Authors**: Alemu Leulseged Tesfaye
- **Comment**: PhD dissertation. arXiv admin note: substantial text overlap with
  arXiv:1608.00641 by other authors
- **Journal**: None
- **Summary**: In this thesis, we present new schemes which leverage a constrained clustering method to solve several computer vision tasks ranging from image retrieval, image segmentation and co-segmentation, to person re-identification. In the last decades clustering methods have played a vital role in computer vision applications; herein, we focus on the extension, reformulation, and integration of a well-known graph and game theoretic clustering method known as Dominant Sets. Thus, we have demonstrated the validity of the proposed methods with extensive experiments which are conducted on several benchmark datasets.



### Image-to-Image Translation with Text Guidance
- **Arxiv ID**: http://arxiv.org/abs/2002.05235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05235v1)
- **Published**: 2020-02-12 21:09:15+00:00
- **Updated**: 2020-02-12 21:09:15+00:00
- **Authors**: Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions.



### Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System
- **Arxiv ID**: http://arxiv.org/abs/2002.05242v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05242v2)
- **Published**: 2020-02-12 21:30:34+00:00
- **Updated**: 2022-04-08 20:58:23+00:00
- **Authors**: Nataniel Ruiz, Hao Yu, Danielle A. Allessio, Mona Jalal, Ajjen Joshi, Thomas Murray, John J. Magee, Jacob R. Whitehill, Vitaly Ablavsky, Ivon Arroyo, Beverly P. Woolf, Stan Sclaroff, Margrit Betke
- **Comment**: Published at IEEE International Conference on Automatic Face and
  Gesture Recognition (FG), 2021 - Best Poster Award (4% award rate)
- **Journal**: None
- **Summary**: In this work, we propose a video-based transfer learning approach for predicting problem outcomes of students working with an intelligent tutoring system (ITS). By analyzing a student's face and gestures, our method predicts the outcome of a student answering a problem in an ITS from a video feed. Our work is motivated by the reasoning that the ability to predict such outcomes enables tutoring systems to adjust interventions, such as hints and encouragement, and to ultimately yield improved student learning. We collected a large labeled dataset of student interactions with an intelligent online math tutor consisting of 68 sessions, where 54 individual students solved 2,749 problems. The dataset is public and available at https://www.cs.bu.edu/faculty/betke/research/learning/ . Working with this dataset, our transfer-learning challenge was to design a representation in the source domain of pictures obtained "in the wild" for the task of facial expression analysis, and transferring this learned representation to the task of human behavior prediction in the domain of webcam videos of students in a classroom environment. We developed a novel facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We designed several variants of a recurrent neural network that models the temporal structure of video sequences of students solving math problems. Our final model, named ATL-BP for Affect Transfer Learning for Behavior Prediction, achieves a relative increase in mean F-score of 50% over the state-of-the-art method on this new dataset.



### HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2002.05271v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05271v1)
- **Published**: 2020-02-12 23:03:44+00:00
- **Updated**: 2020-02-12 23:03:44+00:00
- **Authors**: Qianwen Wang, William Alexander, Jack Pegg, Huamin Qu, Min Chen
- **Comment**: This article was submitted to EuroVis 2020 on 5 December 2020. It was
  not accepted. Because the reviews have not identified any technical problems
  that would undermine the novelty and validity of this work, we think that the
  article is ready to be released as an arXiv report. The EuroVis 2020 reviews
  and authors' short feedback can be found in the anc folder
- **Journal**: IEEE Transactions on Visualization and Computer Graphics, Feb.
  2021
- **Summary**: In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a "concept" or "feature" may benefit or hinder a ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing data is transformed to a visual representation for rapid observation of the conclusions and the logical flow between the testing data and hypotheses.We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.



### Solving Missing-Annotation Object Detection with Background Recalibration Loss
- **Arxiv ID**: http://arxiv.org/abs/2002.05274v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/2002.05274v2)
- **Published**: 2020-02-12 23:11:46+00:00
- **Updated**: 2020-08-03 19:21:26+00:00
- **Authors**: Han Zhang, Fangyi Chen, Zhiqiang Shen, Qiqi Hao, Chenchen Zhu, Marios Savvides
- **Comment**: 5 pages. Paper has been accepted by ICASSP 2020 for presentation in a
  lecture (oral) session
- **Journal**: None
- **Summary**: This paper focuses on a novel and challenging detection scenario: A majority of true objects/instances is unlabeled in the datasets, so these missing-labeled areas will be regarded as the background during training. Previous art on this problem has proposed to use soft sampling to re-weight the gradients of RoIs based on the overlaps with positive instances, while their method is mainly based on the two-stage detector (i.e. Faster RCNN) which is more robust and friendly for the missing label scenario. In this paper, we introduce a superior solution called Background Recalibration Loss (BRL) that can automatically re-calibrate the loss signals according to the pre-defined IoU threshold and input image. Our design is built on the one-stage detector which is faster and lighter. Inspired by the Focal Loss formulation, we make several significant modifications to fit on the missing-annotation circumstance. We conduct extensive experiments on the curated PASCAL VOC and MS COCO datasets. The results demonstrate that our proposed method outperforms the baseline and other state-of-the-arts by a large margin. Code available: https://github.com/Dwrety/mmdetection-selective-iou.



### Stabilizing Differentiable Architecture Search via Perturbation-based Regularization
- **Arxiv ID**: http://arxiv.org/abs/2002.05283v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05283v3)
- **Published**: 2020-02-12 23:46:58+00:00
- **Updated**: 2021-01-12 19:17:24+00:00
- **Authors**: Xiangning Chen, Cho-Jui Hsieh
- **Comment**: ICML 2020, code is available at
  https://github.com/xiangning-chen/SmoothDARTS
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability has been challenged for yielding deteriorating architectures as the search proceeds. We find that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the final architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbation-based regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS-based methods. In particular, our new formulations stabilize DARTS-based methods by either random smoothing or adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.



