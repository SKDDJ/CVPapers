# Arxiv Papers in cs.CV on 2020-02-03
### Novelty Detection via Non-Adversarial Generative Network
- **Arxiv ID**: http://arxiv.org/abs/2002.00522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00522v1)
- **Published**: 2020-02-03 01:05:59+00:00
- **Updated**: 2020-02-03 01:05:59+00:00
- **Authors**: Chengwei Chen, Wang Yuan, Yuan Xie, Yanyun Qu, Yiqing Tao, Haichuan Song, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: One-class novelty detection is the process of determining if a query example differs from the training examples (the target class). Most of previous strategies attempt to learn the real characteristics of target sample by using generative adversarial networks (GANs) methods. However, the training process of GANs remains challenging, suffering from instability issues such as mode collapse and vanishing gradients. In this paper, by adopting non-adversarial generative networks, a novel decoder-encoder framework is proposed for novelty detection task, insteading of classical encoder-decoder style. Under the non-adversarial framework, both latent space and image reconstruction space are jointly optimized, leading to a more stable training process with super fast convergence and lower training losses. During inference, inspired by cycleGAN, we design a new testing scheme to conduct image reconstruction, which is the reverse way of training sequence. Experiments show that our model has the clear superiority over cutting-edge novelty detectors and achieves the state-of-the-art results on the datasets.



### Automatic Pruning for Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00523v1)
- **Published**: 2020-02-03 01:10:13+00:00
- **Updated**: 2020-02-03 01:10:13+00:00
- **Authors**: Luis Guerra, Bohan Zhuang, Ian Reid, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization and pruning are two techniques commonly used to reduce the computational complexity and memory footprint of these models for deployment. However, most existing pruning strategies operate on full-precision and cannot be directly applied to discrete parameter distributions after quantization. In contrast, we study a combination of these two techniques to achieve further network compression. In particular, we propose an effective pruning strategy for selecting redundant low-precision filters. Furthermore, we leverage Bayesian optimization to efficiently determine the pruning ratio for each layer. We conduct extensive experiments on CIFAR-10 and ImageNet with various architectures and precisions. In particular, for ResNet-18 on ImageNet, we prune 26.12% of the model size with Binarized Neural Network quantization, achieving a top-1 classification accuracy of 47.32% in a model of 2.47 MB and 59.30% with a 2-bit DoReFa-Net in 4.36 MB.



### Towards High Performance Human Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.00537v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00537v2)
- **Published**: 2020-02-03 02:24:51+00:00
- **Updated**: 2021-05-23 02:23:25+00:00
- **Authors**: Jing Zhang, Zhe Chen, Dacheng Tao
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM's representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.



### DWM: A Decomposable Winograd Method for Convolution Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2002.00552v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00552v1)
- **Published**: 2020-02-03 03:42:56+00:00
- **Updated**: 2020-02-03 03:42:56+00:00
- **Authors**: Di Huang, Xishan Zhang, Rui Zhang, Tian Zhi, Deyuan He, Jiaming Guo, Chang Liu, Qi Guo, Zidong Du, Shaoli Liu, Tianshi Chen, Yunji Chen
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Winograd's minimal filtering algorithm has been widely used in Convolutional Neural Networks (CNNs) to reduce the number of multiplications for faster processing. However, it is only effective on convolutions with kernel size as 3x3 and stride as 1, because it suffers from significantly increased FLOPs and numerical accuracy problem for kernel size larger than 3x3 and fails on convolution with stride larger than 1. In this paper, we propose a novel Decomposable Winograd Method (DWM), which breaks through the limitation of original Winograd's minimal filtering algorithm to a wide and general convolutions. DWM decomposes kernels with large size or large stride to several small kernels with stride as 1 for further applying Winograd method, so that DWM can reduce the number of multiplications while keeping the numerical accuracy. It enables the fast exploring of larger kernel size and larger stride value in CNNs for high performance and accuracy and even the potential for new CNNs. Comparing against the original Winograd, the proposed DWM is able to support all kinds of convolutions with a speedup of ~2, without affecting the numerical accuracy.



### Widening and Squeezing: Towards Accurate and Efficient QNNs
- **Arxiv ID**: http://arxiv.org/abs/2002.00555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00555v2)
- **Published**: 2020-02-03 04:11:13+00:00
- **Updated**: 2020-02-12 09:44:24+00:00
- **Authors**: Chuanjian Liu, Kai Han, Yunhe Wang, Hanting Chen, Qi Tian, Chunjing Xu
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Quantization neural networks (QNNs) are very attractive to the industry because their extremely cheap calculation and storage overhead, but their performance is still worse than that of networks with full-precision parameters. Most of existing methods aim to enhance performance of QNNs especially binary neural networks by exploiting more effective training techniques. However, we find the representation capability of quantization features is far weaker than full-precision features by experiments. We address this problem by projecting features in original full-precision networks to high-dimensional quantization features. Simultaneously, redundant quantization features will be eliminated in order to avoid unrestricted growth of dimensions for some datasets. Then, a compact quantization neural network but with sufficient representation ability will be established. Experimental results on benchmark datasets demonstrate that the proposed method is able to establish QNNs with much less parameters and calculations but almost the same performance as that of full-precision baseline models, e.g. $29.9\%$ top-1 error of binary ResNet-18 on the ImageNet ILSVRC 2012 dataset.



### DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data
- **Arxiv ID**: http://arxiv.org/abs/2002.00569v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00569v3)
- **Published**: 2020-02-03 05:38:33+00:00
- **Updated**: 2020-03-28 08:26:57+00:00
- **Authors**: Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, Dou Renyin
- **Comment**: Fixed typos
- **Journal**: None
- **Summary**: We present a method for depth estimation with monocular images, which can predict high-quality depth on diverse scenes up to an affine transformation, thus preserving accurate shapes of a scene. Previous methods that predict metric depth often work well only for a specific scene. In contrast, learning relative depth (information of being closer or further) can enjoy better generalization, with the price of failing to recover the accurate geometric shape of the scene. In this work, we propose a dataset and methods to tackle this dilemma, aiming to predict accurate depth up to an affine transformation with good generalization to diverse scenes. First we construct a large-scale and diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which has a broad range of scenes and foreground contents. Compared with previous learning objectives, i.e., learning metric depth or relative depth, we propose to learn the affine-invariant depth using our diverse dataset to ensure both generalization and high-quality geometric shapes of scenes. Furthermore, in order to train the model on the complex dataset effectively, we propose a multi-curriculum learning method. Experiments show that our method outperforms previous methods on 8 datasets by a large margin with the zero-shot test setting, demonstrating the excellent generalization capacity of the learned model to diverse scenes. The reconstructed point clouds with the predicted depth show that our method can recover high-quality 3D shapes. Code and dataset are available at: https://tinyurl.com/DiverseDepth



### Revisiting Meta-Learning as Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00573v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.00573v1)
- **Published**: 2020-02-03 06:13:01+00:00
- **Updated**: 2020-02-03 06:13:01+00:00
- **Authors**: Wei-Lun Chao, Han-Jia Ye, De-Chuan Zhan, Mark Campbell, Kilian Q. Weinberger
- **Comment**: An extended version of the paper titled "A Meta Understanding of
  Meta-Learning" presented in ICML 2019 Workshop on Adaptive and Multitask
  Learning: Algorithms & Systems
- **Journal**: None
- **Summary**: Recent years have witnessed an abundance of new publications and approaches on meta-learning. This community-wide enthusiasm has sparked great insights but has also created a plethora of seemingly different frameworks, which can be hard to compare and evaluate. In this paper, we aim to provide a principled, unifying framework by revisiting and strengthening the connection between meta-learning and traditional supervised learning. By treating pairs of task-specific data sets and target models as (feature, label) samples, we can reduce many meta-learning algorithms to instances of supervised learning. This view not only unifies meta-learning into an intuitive and practical framework but also allows us to transfer insights from supervised learning directly to improve meta-learning. For example, we obtain a better understanding of generalization properties, and we can readily transfer well-understood techniques, such as model ensemble, pre-training, joint training, data augmentation, and even nearest neighbor based methods. We provide an intuitive analogy of these methods in the context of meta-learning and show that they give rise to significant improvements in model performance on few-shot learning.



### Unsupervised Domain Adaptive Object Detection using Forward-Backward Cyclic Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.00575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00575v1)
- **Published**: 2020-02-03 06:24:58+00:00
- **Updated**: 2020-02-03 06:24:58+00:00
- **Authors**: Siqi Yang, Lin Wu, Arnold Wiliem, Brian C. Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to perform the unsupervised domain adaptation for object detection through forward-backward cyclic (FBC) training. Recent adversarial training based domain adaptation methods have shown their effectiveness on minimizing domain discrepancy via marginal feature distributions alignment. However, aligning the marginal feature distributions does not guarantee the alignment of class conditional distributions. This limitation is more evident when adapting object detectors as the domain discrepancy is larger compared to the image classification task, e.g. various number of objects exist in one image and the majority of content in an image is the background. This motivates us to learn domain invariance for category level semantics via gradient alignment. Intuitively, if the gradients of two domains point in similar directions, then the learning of one domain can improve that of another domain. To achieve gradient alignment, we propose Forward-Backward Cyclic Adaptation, which iteratively computes adaptation from source to target via backward hopping and from target to source via forward passing. In addition, we align low-level features for adapting holistic color/texture via adversarial training. However, the detector performs well on both domains is not ideal for target domain. As such, in each cycle, domain diversity is enforced by maximum entropy regularization on the source domain to penalize confident source-specific learning and minimum entropy regularization on target domain to intrigue target-specific learning. Theoretical analysis of the training process is provided, and extensive experiments on challenging cross-domain object detection datasets have shown the superiority of our approach over the state-of-the-art.



### Super-resolution of multispectral satellite images using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00580v2
- **DOI**: 10.5194/isprs-annals-V-1-2020-33-2020
- **Categories**: **eess.IV**, cs.CV, 68-06, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2002.00580v2)
- **Published**: 2020-02-03 07:06:36+00:00
- **Updated**: 2020-04-08 06:30:47+00:00
- **Authors**: M. U. Müller, N. Ekhtiari, R. M. Almeida, C. Rieke
- **Comment**: To be published in the ISPRS Annals of the Photogrammetry, Remote
  Sensing and Spatial Information Sciences:
  https://www.isprs.org/publications/annals.aspx, proceedings of the XXIV ISPRS
  Congress, 14-20 June 2020, Nice, France
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., V-1-2020,
  33-40
- **Summary**: Super-resolution aims at increasing image resolution by algorithmic means and has progressed over the recent years due to advances in the fields of computer vision and deep learning. Convolutional Neural Networks based on a variety of architectures have been applied to the problem, e.g. autoencoders and residual networks. While most research focuses on the processing of photographs consisting only of RGB color channels, little work can be found concentrating on multi-band, analytic satellite imagery. Satellite images often include a panchromatic band, which has higher spatial resolution but lower spectral resolution than the other bands. In the field of remote sensing, there is a long tradition of applying pan-sharpening to satellite images, i.e. bringing the multispectral bands to the higher spatial resolution by merging them with the panchromatic band. To our knowledge there are so far no approaches to super-resolution which take advantage of the panchromatic band. In this paper we propose a method to train state-of-the-art CNNs using pairs of lower-resolution multispectral and high-resolution pan-sharpened image tiles in order to create super-resolved analytic images. The derived quality metrics show that the method improves information content of the processed images. We compare the results created by four CNN architectures, with RedNet30 performing best.



### Deriving Emotions and Sentiments from Visual Content: A Disaster Analysis Use Case
- **Arxiv ID**: http://arxiv.org/abs/2002.03773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.03773v1)
- **Published**: 2020-02-03 08:48:52+00:00
- **Updated**: 2020-02-03 08:48:52+00:00
- **Authors**: Kashif Ahmad, Syed Zohaib, Nicola Conci, Ala Al-Fuqaha
- **Comment**: None
- **Journal**: None
- **Summary**: Sentiment analysis aims to extract and express a person's perception, opinions and emotions towards an entity, object, product and a service, enabling businesses to obtain feedback from the consumers. The increasing popularity of the social networks and users' tendency towards sharing their feelings, expressions and opinions in text, visual and audio content has opened new opportunities and challenges in sentiment analysis. While sentiment analysis of text streams has been widely explored in the literature, sentiment analysis of images and videos is relatively new. This article introduces visual sentiment analysis and contrasts it with textual sentiment analysis with emphasis on the opportunities and challenges in this nascent research area. We also propose a deep visual sentiment analyzer for disaster-related images as a use-case, covering different aspects of visual sentiment analysis starting from data collection, annotation, model selection, implementation and evaluations. We believe such rigorous analysis will provide a baseline for future research in the domain.



### Facial Affect Recognition in the Wild Using Multi-Task Learning Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2002.00606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00606v1)
- **Published**: 2020-02-03 09:02:26+00:00
- **Updated**: 2020-02-03 09:02:26+00:00
- **Authors**: Zihang Zhang, Jianping Gu
- **Comment**: submitted to ABAW challenge in FG2020
- **Journal**: None
- **Summary**: This paper presents a neural network based method Multi-Task Affect Net(MTANet) submitted to the Affective Behavior Analysis in-the-Wild Challenge in FG2020. This method is a multi-task network and based on SE-ResNet modules. By utilizing multi-task learning, this network can estimate and recognize three quantified affective models: valence and arousal, action units, and seven basic emotions simultaneously. MTANet achieve Concordance Correlation Coefficient(CCC) rates of 0.28 and 0.34 for valence and arousal, F1-score of 0.427 and 0.32 for AUs detection and categorical emotion classification.



### Regularizers for Single-step Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2002.00614v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00614v1)
- **Published**: 2020-02-03 09:21:04+00:00
- **Updated**: 2020-02-03 09:21:04+00:00
- **Authors**: B. S. Vivek, R. Venkatesh Babu
- **Comment**: None
- **Journal**: None
- **Summary**: The progress in the last decade has enabled machine learning models to achieve impressive performance across a wide range of tasks in Computer Vision. However, a plethora of works have demonstrated the susceptibility of these models to adversarial samples. Adversarial training procedure has been proposed to defend against such adversarial attacks. Adversarial training methods augment mini-batches with adversarial samples, and typically single-step (non-iterative) methods are used for generating these adversarial samples. However, models trained using single-step adversarial training converge to degenerative minima where the model merely appears to be robust. The pseudo robustness of these models is due to the gradient masking effect. Although multi-step adversarial training helps to learn robust models, they are hard to scale due to the use of iterative methods for generating adversarial samples. To address these issues, we propose three different types of regularizers that help to learn robust models using single-step adversarial training methods. The proposed regularizers mitigate the effect of gradient masking by harnessing on properties that differentiate a robust model from that of a pseudo robust model. Performance of models trained using the proposed regularizers is on par with models trained using computationally expensive multi-step adversarial training methods.



### Exponential discretization of weights of neural network connections in pre-trained neural networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00623v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00623v1)
- **Published**: 2020-02-03 09:41:24+00:00
- **Updated**: 2020-02-03 09:41:24+00:00
- **Authors**: Magomed Yu. Malsagov, Emil M. Khayrov, Maria M. Pushkareva, Iakov M. Karandashev
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: Optical Memory and Neural Networks (Inf. Opt.), V.28, N.4,
  pp.262-270 (2019)
- **Summary**: To reduce random access memory (RAM) requirements and to increase speed of recognition algorithms we consider a weight discretization problem for trained neural networks. We show that an exponential discretization is preferable to a linear discretization since it allows one to achieve the same accuracy when the number of bits is 1 or 2 less. The quality of the neural network VGG-16 is already satisfactory (top5 accuracy 69%) in the case of 3 bit exponential discretization. The ResNet50 neural network shows top5 accuracy 84% at 4 bits. Other neural networks perform fairly well at 5 bits (top5 accuracies of Xception, Inception-v3, and MobileNet-v2 top5 were 87%, 90%, and 77%, respectively). At less number of bits, the accuracy decreases rapidly.



### Classification of Chest Diseases using Wavelet Transforms and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00625v1
- **DOI**: 10.1007/978-981-15-5199-4_16
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00625v1)
- **Published**: 2020-02-03 09:44:23+00:00
- **Updated**: 2020-02-03 09:44:23+00:00
- **Authors**: Ahmed Rasheed, Muhammad Shahzad Younis, Muhammad Bilal, Maha Rasheed
- **Comment**: 8 pages, 4 figures, Presented in International Conference On Medical
  Imaging And Computer-Aided Diagnosis (MICAD 2020), proceeding will be
  published with Springer in their "Lecture Notes in Electrical Engineering
  (LNEE)" (ISSN: 1876-1100)
- **Journal**: None
- **Summary**: Chest X-ray scan is a most often used modality by radiologists to diagnose many chest related diseases in their initial stages. The proposed system aids the radiologists in making decision about the diseases found in the scans more efficiently. Our system combines the techniques of image processing for feature enhancement and deep learning for classification among diseases. We have used the ChestX-ray14 database in order to train our deep learning model on the 14 different labeled diseases found in it. The proposed research shows the significant improvement in the results by using wavelet transforms as pre-processing technique.



### Medicine Strip Identification using 2-D Cepstral Feature Extraction and Multiclass Classification Methods
- **Arxiv ID**: http://arxiv.org/abs/2003.00810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00810v1)
- **Published**: 2020-02-03 09:45:01+00:00
- **Updated**: 2020-02-03 09:45:01+00:00
- **Authors**: Anirudh Itagi, Ritam Sil, Saurav Mohapatra, Subham Rout, Bharath K P, Karthik R, Rajesh Kumar Muthu
- **Comment**: None
- **Journal**: None
- **Summary**: Misclassification of medicine is perilous to the health of a patient, more so if the said patient is visually impaired or simply did not recognize the color, shape or type of medicine strip. This paper proposes a method for identification of medicine strips by 2-D cepstral analysis of their images followed by performing classification that has been done using the K-Nearest Neighbor (KNN), Support Vector Machine (SVM) and Logistic Regression (LR) Classifiers. The 2-D cepstral features extracted are extremely distinct to a medicine strip and consequently make identifying them exceptionally accurate. This paper also proposes the Color Gradient and Pill shape Feature (CGPF) extraction procedure and discusses the Binary Robust Invariant Scalable Keypoints (BRISK) algorithm as well. The mentioned algorithms were implemented and their identification results have been compared.



### Pix2Pix-based Stain-to-Stain Translation: A Solution for Robust Stain Normalization in Histopathology Images Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.00647v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00647v1)
- **Published**: 2020-02-03 11:19:01+00:00
- **Updated**: 2020-02-03 11:19:01+00:00
- **Authors**: Pegah Salehi, Abdolah Chalechale
- **Comment**: 7 pages, 6 figures, 4 table, The 11th Iranian and the first
  International Conference on Machine Vision and Image Processing (MVIP 2020)
- **Journal**: None
- **Summary**: The diagnosis of cancer is mainly performed by visual analysis of the pathologists, through examining the morphology of the tissue slices and the spatial arrangement of the cells. If the microscopic image of a specimen is not stained, it will look colorless and textured. Therefore, chemical staining is required to create contrast and help identify specific tissue components. During tissue preparation due to differences in chemicals, scanners, cutting thicknesses, and laboratory protocols, similar tissues are usually varied significantly in appearance. This diversity in staining, in addition to Interpretive disparity among pathologists more is one of the main challenges in designing robust and flexible systems for automated analysis. To address the staining color variations, several methods for normalizing stain have been proposed. In our proposed method, a Stain-to-Stain Translation (STST) approach is used to stain normalization for Hematoxylin and Eosin (H&E) stained histopathology images, which learns not only the specific color distribution but also the preserves corresponding histopathological pattern. We perform the process of translation based on the pix2pix framework, which uses the conditional generator adversarial networks (cGANs). Our approach showed excellent results, both mathematically and experimentally against the state of the art methods. We have made the source code publicly available.



### Single-Stage Object Detection from Top-View Grid Maps on Custom Sensor Setups
- **Arxiv ID**: http://arxiv.org/abs/2002.00667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00667v1)
- **Published**: 2020-02-03 12:05:20+00:00
- **Updated**: 2020-02-03 12:05:20+00:00
- **Authors**: Sascha Wirges, Shuxiao Ding, Christoph Stiller
- **Comment**: 6 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: We present our approach to unsupervised domain adaptation for single-stage object detectors on top-view grid maps in automated driving scenarios. Our goal is to train a robust object detector on grid maps generated from custom sensor data and setups. We first introduce a single-stage object detector for grid maps based on RetinaNet. We then extend our model by image- and instance-level domain classifiers at different feature pyramid levels which are trained in an adversarial manner. This allows us to train robust object detectors for unlabeled domains. We evaluate our approach quantitatively on the nuScenes and KITTI benchmarks and present qualitative domain adaptation results for unlabeled measurements recorded by our experimental vehicle. Our results demonstrate that object detection accuracy for unlabeled domains can be improved by applying our domain adaptation strategy.



### A Novel Incremental Cross-Modal Hashing Approach
- **Arxiv ID**: http://arxiv.org/abs/2002.00677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00677v1)
- **Published**: 2020-02-03 12:34:56+00:00
- **Updated**: 2020-02-03 12:34:56+00:00
- **Authors**: Devraj Mandal, Soma Biswas
- **Comment**: 20 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Cross-modal retrieval deals with retrieving relevant items from one modality, when provided with a search query from another modality. Hashing techniques, where the data is represented as binary bits have specifically gained importance due to the ease of storage, fast computations and high accuracy. In real world, the number of data categories is continuously increasing, which requires algorithms capable of handling this dynamic scenario. In this work, we propose a novel incremental cross-modal hashing algorithm termed "iCMH", which can adapt itself to handle incoming data of new categories. The proposed approach consists of two sequential stages, namely, learning the hash codes and training the hash functions. At every stage, a small amount of old category data termed "exemplars" is is used so as not to forget the old data while trying to learn for the new incoming data, i.e. to avoid catastrophic forgetting. In the first stage, the hash codes for the exemplars is used, and simultaneously, hash codes for the new data is computed such that it maintains the semantic relations with the existing data. For the second stage, we propose both a non-deep and deep architectures to learn the hash functions effectively. Extensive experiments across a variety of cross-modal datasets and comparisons with state-of-the-art cross-modal algorithms shows the usefulness of our approach.



### Modeling the Background for Incremental Learning in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.00718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00718v2)
- **Published**: 2020-02-03 13:30:38+00:00
- **Updated**: 2020-03-30 14:01:26+00:00
- **Authors**: Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Despite their effectiveness in a wide range of tasks, deep architectures suffer from some important limitations. In particular, they are vulnerable to catastrophic forgetting, i.e. they perform poorly when they are required to update their model as new classes are available but the original training set is not retained. This paper addresses this problem in the context of semantic segmentation. Current strategies fail on this task because they do not consider a peculiar aspect of semantic segmentation: since each training step provides annotation only for a subset of all possible classes, pixels of the background class (i.e. pixels that do not belong to any other classes) exhibit a semantic distribution shift. In this work we revisit classical incremental learning methods, proposing a new distillation-based framework which explicitly accounts for this shift. Furthermore, we introduce a novel strategy to initialize classifier's parameters, thus preventing biased predictions toward the background class. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly outperforming state of the art incremental learning methods.



### Improving the Evaluation of Generative Models with Fuzzy Logic
- **Arxiv ID**: http://arxiv.org/abs/2002.03772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03772v1)
- **Published**: 2020-02-03 13:42:53+00:00
- **Updated**: 2020-02-03 13:42:53+00:00
- **Authors**: Julian Niedermeier, Gonçalo Mordido, Christoph Meinel
- **Comment**: AAAI 2020 Meta-Eval
- **Journal**: None
- **Summary**: Objective and interpretable metrics to evaluate current artificial intelligent systems are of great importance, not only to analyze the current state of such systems but also to objectively measure progress in the future. In this work, we focus on the evaluation of image generation tasks. We propose a novel approach, called Fuzzy Topology Impact (FTI), that determines both the quality and diversity of an image set using topology representations combined with fuzzy logic. When compared to current evaluation methods, FTI shows better and more stable performance on multiple experiments evaluating the sensitivity to noise, mode dropping and mode inventing.



### Separation of target anatomical structure and occlusions in chest radiographs
- **Arxiv ID**: http://arxiv.org/abs/2002.00751v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV, stat.ML, J.3; I.4.3; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2002.00751v1)
- **Published**: 2020-02-03 14:01:06+00:00
- **Updated**: 2020-02-03 14:01:06+00:00
- **Authors**: Johannes Hofmanninger, Sebastian Roehrich, Helmut Prosch, Georg Langs
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Chest radiographs are commonly performed low-cost exams for screening and diagnosis. However, radiographs are 2D representations of 3D structures causing considerable clutter impeding visual inspection and automated image analysis. Here, we propose a Fully Convolutional Network to suppress, for a specific task, undesired visual structure from radiographs while retaining the relevant image information such as lung-parenchyma. The proposed algorithm creates reconstructed radiographs and ground-truth data from high resolution CT-scans. Results show that removing visual variation that is irrelevant for a classification task improves the performance of a classifier when only limited training data are available. This is particularly relevant because a low number of ground-truth cases is common in medical imaging.



### Hide-and-Tell: Learning to Bridge Photo Streams for Visual Storytelling
- **Arxiv ID**: http://arxiv.org/abs/2002.00774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00774v1)
- **Published**: 2020-02-03 14:22:18+00:00
- **Updated**: 2020-02-03 14:22:18+00:00
- **Authors**: Yunjae Jung, Dahun Kim, Sanghyun Woo, Kyungsu Kim, Sungjin Kim, In So Kweon
- **Comment**: AAAI 2020 paper
- **Journal**: None
- **Summary**: Visual storytelling is a task of creating a short story based on photo streams. Unlike existing visual captioning, storytelling aims to contain not only factual descriptions, but also human-like narration and semantics. However, the VIST dataset consists only of a small, fixed number of photos per story. Therefore, the main challenge of visual storytelling is to fill in the visual gap between photos with narrative and imaginative story. In this paper, we propose to explicitly learn to imagine a storyline that bridges the visual gap. During training, one or more photos is randomly omitted from the input stack, and we train the network to produce a full plausible story even with missing photo(s). Furthermore, we propose for visual storytelling a hide-and-tell model, which is designed to learn non-local relations across the photo streams and to refine and improve conventional RNN-based models. In experiments, we show that our scheme of hide-and-tell, and the network design are indeed effective at storytelling, and that our model outperforms previous state-of-the-art methods in automatic metrics. Finally, we qualitatively show the learned ability to interpolate storyline over visual gaps.



### Towards Accurate Vehicle Behaviour Classification With Multi-Relational Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00786v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00786v3)
- **Published**: 2020-02-03 14:34:28+00:00
- **Updated**: 2020-05-12 17:49:11+00:00
- **Authors**: Sravan Mylavarapu, Mahtab Sandhu, Priyesh Vijayan, K Madhava Krishna, Balaraman Ravindran, Anoop Namboodiri
- **Comment**: To appear in IV (IEEE Intelligent Vehicles Symposium) 2020
- **Journal**: None
- **Summary**: Understanding on-road vehicle behaviour from a temporal sequence of sensor data is gaining in popularity. In this paper, we propose a pipeline for understanding vehicle behaviour from a monocular image sequence or video. A monocular sequence along with scene semantics, optical flow and object labels are used to get spatial information about the object (vehicle) of interest and other objects (semantically contiguous set of locations) in the scene. This spatial information is encoded by a Multi-Relational Graph Convolutional Network (MR-GCN), and a temporal sequence of such encodings is fed to a recurrent network to label vehicle behaviours. The proposed framework can classify a variety of vehicle behaviours to high fidelity on datasets that are diverse and include European, Chinese and Indian on-road scenes. The framework also provides for seamless transfer of models across datasets without entailing re-annotation, retraining and even fine-tuning. We show comparative performance gain over baseline Spatio-temporal classifiers and detail a variety of ablations to showcase the efficacy of the framework.



### Syn2Real: Forgery Classification via Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.00807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00807v1)
- **Published**: 2020-02-03 15:02:58+00:00
- **Updated**: 2020-02-03 15:02:58+00:00
- **Authors**: Akash Kumar, Arnav Bhavasar
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques. The task of the identification of forged images has become very challenging. Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering. To tackle such problems, publicly available datasets are insufficient. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm. However, models trained on these datasets have a significant drop in performance when tested on more realistic data. To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset. Furthermore, we improvised the F1 score on CASIA and CoMoFoD dataset to 80.3% and 78.8%, respectively. Our approach can be helpful in those cases where the classification of data is unavailable.



### Learning Extremal Representations with Deep Archetypal Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.00815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00815v1)
- **Published**: 2020-02-03 15:13:49+00:00
- **Updated**: 2020-02-03 15:13:49+00:00
- **Authors**: Sebastian Mathias Keller, Maxim Samarin, Fabricio Arend Torres, Mario Wieser, Volker Roth
- **Comment**: Under review for publication at the International Journal of Computer
  Vision (IJCV). Extended version of our GCPR2019 paper "Deep Archetypal
  Analysis"
- **Journal**: None
- **Summary**: Archetypes are typical population representatives in an extremal sense, where typicality is understood as the most extreme manifestation of a trait or feature. In linear feature space, archetypes approximate the data convex hull allowing all data points to be expressed as convex mixtures of archetypes. However, it might not always be possible to identify meaningful archetypes in a given feature space. Learning an appropriate feature space and identifying suitable archetypes simultaneously addresses this problem. This paper introduces a generative formulation of the linear archetype model, parameterized by neural networks. By introducing the distance-dependent archetype loss, the linear archetype model can be integrated into the latent space of a variational autoencoder, and an optimal representation with respect to the unknown archetypes can be learned end-to-end. The reformulation of linear Archetypal Analysis as deep variational information bottleneck, allows the incorporation of arbitrarily complex side information during training. Furthermore, an alternative prior, based on a modified Dirichlet distribution, is proposed. The real-world applicability of the proposed method is demonstrated by exploring archetypes of female facial expressions while using multi-rater based emotion scores of these expressions as side information. A second application illustrates the exploration of the chemical space of small organic molecules. In this experiment, it is demonstrated that exchanging the side information but keeping the same set of molecules, e. g. using as side information the heat capacity of each molecule instead of the band gap energy, will result in the identification of different archetypes. As an application, these learned representations of chemical space might reveal distinct starting points for de novo molecular design.



### Deep Self-Supervised Representation Learning for Free-Hand Sketch
- **Arxiv ID**: http://arxiv.org/abs/2002.00867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00867v1)
- **Published**: 2020-02-03 16:28:29+00:00
- **Updated**: 2020-02-03 16:28:29+00:00
- **Authors**: Peng Xu, Zeyu Song, Qiyue Yin, Yi-Zhe Song, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle for the first time, the problem of self-supervised representation learning for free-hand sketches. This importantly addresses a common problem faced by the sketch community -- that annotated supervisory data are difficult to obtain. This problem is very challenging in that sketches are highly abstract and subject to different drawing styles, making existing solutions tailored for photos unsuitable. Key for the success of our self-supervised learning paradigm lies with our sketch-specific designs: (i) we propose a set of pretext tasks specifically designed for sketches that mimic different drawing styles, and (ii) we further exploit the use of a textual convolution network (TCN) in a dual-branch architecture for sketch feature learning, as means to accommodate the sequential stroke nature of sketches. We demonstrate the superiority of our sketch-specific designs through two sketch-related applications (retrieval and recognition) on a million-scale sketch dataset, and show that the proposed approach outperforms the state-of-the-art unsupervised representation learning methods, and significantly narrows the performance gap between with supervised representation learning.



### Adversarial-based neural networks for affect estimations in the wild
- **Arxiv ID**: http://arxiv.org/abs/2002.00883v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00883v3)
- **Published**: 2020-02-03 16:52:49+00:00
- **Updated**: 2020-02-09 23:00:05+00:00
- **Authors**: Decky Aspandi, Adria Mallol-Ragolta, Björn Schuller, Xavier Binefa
- **Comment**: Paper for FG 2020 Affect Challenge
  https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/
- **Journal**: None
- **Summary**: There is a growing interest in affective computing research nowadays given its crucial role in bridging humans with computers. This progress has been recently accelerated due to the emergence of bigger data. One recent advance in this field is the use of adversarial learning to improve model learning through augmented samples. However, the use of latent features, which is feasible through adversarial learning, is not largely explored, yet. This technique may also improve the performance of affective models, as analogously demonstrated in related fields, such as computer vision. To expand this analysis, in this work, we explore the use of latent features through our proposed adversarial-based networks for valence and arousal recognition in the wild. Specifically, our models operate by aggregating several modalities to our discriminator, which is further conditioned to the extracted latent features by the generator. Our experiments on the recently released SEWA dataset suggest the progressive improvements of our results. Finally, we show our competitive results on the Affective Behavior Analysis in-the-Wild (ABAW) challenge dataset



### Improved inter-scanner MS lesion segmentation by adversarial training on longitudinal data
- **Arxiv ID**: http://arxiv.org/abs/2002.00952v2
- **DOI**: 10.1007/978-3-030-46640-4_10
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.00952v2)
- **Published**: 2020-02-03 16:56:05+00:00
- **Updated**: 2020-10-27 11:11:26+00:00
- **Authors**: Mattias Billast, Maria Ines Meyer, Diana M. Sima, David Robben
- **Comment**: Added link to final authenticated publication
  (https://doi.org/10.1007/978-3-030-46640-4_10)
- **Journal**: Crimi A., Bakas S. (eds) Brainlesion: Glioma, Multiple Sclerosis,
  Stroke and Traumatic Brain Injuries. BrainLes 2019. Lecture Notes in Computer
  Science, vol 11992. Springer, Cham
- **Summary**: The evaluation of white matter lesion progression is an important biomarker in the follow-up of MS patients and plays a crucial role when deciding the course of treatment. Current automated lesion segmentation algorithms are susceptible to variability in image characteristics related to MRI scanner or protocol differences. We propose a model that improves the consistency of MS lesion segmentations in inter-scanner studies. First, we train a CNN base model to approximate the performance of icobrain, an FDA-approved clinically available lesion segmentation software. A discriminator model is then trained to predict if two lesion segmentations are based on scans acquired using the same scanner type or not, achieving a 78% accuracy in this task. Finally, the base model and the discriminator are trained adversarially on multi-scanner longitudinal data to improve the inter-scanner consistency of the base model. The performance of the models is evaluated on an unseen dataset containing manual delineations. The inter-scanner variability is evaluated on test-retest data, where the adversarial network produces improved results over the base model and the FDA-approved solution.



### Effect of top-down connections in Hierarchical Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2002.00892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00892v1)
- **Published**: 2020-02-03 17:12:01+00:00
- **Updated**: 2020-02-03 17:12:01+00:00
- **Authors**: Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent Perrinet
- **Comment**: None
- **Journal**: None
- **Summary**: Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.



### EGO-CH: Dataset and Fundamental Tasks for Visitors BehavioralUnderstanding using Egocentric Vision
- **Arxiv ID**: http://arxiv.org/abs/2002.00899v1
- **DOI**: 10.1016/j.patrec.2019.12.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00899v1)
- **Published**: 2020-02-03 17:25:23+00:00
- **Updated**: 2020-02-03 17:25:23+00:00
- **Authors**: Francesco Ragusa, Antonino Furnari, Sebastiano Battiato, Giovanni Signorello, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: Pattern Recognition Letters 2020
- **Summary**: Equipping visitors of a cultural site with a wearable device allows to easily collect information about their preferences which can be exploited to improve the fruition of cultural goods with augmented reality. Moreover, egocentric video can be processed using computer vision and machine learning to enable an automated analysis of visitors' behavior. The inferred information can be used both online to assist the visitor and offline to support the manager of the site. Despite the positive impact such technologies can have in cultural heritage, the topic is currently understudied due to the limited number of public datasets suitable to study the considered problems. To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first dataset of egocentric videos for visitors' behavior understanding in cultural sites. The dataset has been collected in two cultural sites and includes more than $27$ hours of video acquired by $70$ subjects, with labels for $26$ environments and over $200$ different Points of Interest. A large subset of the dataset, consisting of $60$ videos, is associated with surveys filled out by real visitors. To encourage research on the topic, we propose $4$ challenging tasks (room-based localization, point of interest/object recognition, object retrieval and survey prediction) useful to understand visitors' behavior and report baseline results on the dataset.



### L6DNet: Light 6 DoF Network for Robust and Precise Object Pose Estimation with Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2002.00911v6
- **DOI**: 10.1109/LRA.2021.3062605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00911v6)
- **Published**: 2020-02-03 17:41:29+00:00
- **Updated**: 2022-05-29 20:51:19+00:00
- **Authors**: Mathieu Gonzalez, Amine Kacete, Albert Murienne, Eric Marchand
- **Comment**: This work has been accepted at IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Estimating the 3D pose of an object is a challenging task that can be considered within augmented reality or robotic applications. In this paper, we propose a novel approach to perform 6 DoF object pose estimation from a single RGB-D image. We adopt a hybrid pipeline in two stages: data-driven and geometric respectively. The data-driven step consists of a classification CNN to estimate the object 2D location in the image from local patches, followed by a regression CNN trained to predict the 3D location of a set of keypoints in the camera coordinate system. To extract the pose information, the geometric step consists in aligning the 3D points in the camera coordinate system with the corresponding 3D points in world coordinate system by minimizing a registration error, thus computing the pose. Our experiments on the standard dataset LineMod show that our approach is more robust and accurate than state-of-the-art methods. The approach is also validated to achieve a 6 DoF positioning task by visual servoing.



### End-to-End Models for the Analysis of System 1 and System 2 Interactions based on Eye-Tracking Data
- **Arxiv ID**: http://arxiv.org/abs/2002.11192v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11192v1)
- **Published**: 2020-02-03 17:46:13+00:00
- **Updated**: 2020-02-03 17:46:13+00:00
- **Authors**: Alessandro Rossi, Sara Ermini, Dario Bernabini, Dario Zanca, Marino Todisco, Alessandro Genovese, Antonio Rizzo
- **Comment**: 11 pages, 2 figures, 1 tables
- **Journal**: None
- **Summary**: While theories postulating a dual cognitive system take hold, quantitative confirmations are still needed to understand and identify interactions between the two systems or conflict events. Eye movements are among the most direct markers of the individual attentive load and may serve as an important proxy of information. In this work we propose a computational method, within a modified visual version of the well-known Stroop test, for the identification of different tasks and potential conflicts events between the two systems through the collection and processing of data related to eye movements. A statistical analysis shows that the selected variables can characterize the variation of attentive load within different scenarios. Moreover, we show that Machine Learning techniques allow to distinguish between different tasks with a good classification accuracy and to investigate more in depth the gaze dynamics.



### Radioactive data: tracing through training
- **Arxiv ID**: http://arxiv.org/abs/2002.00937v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00937v1)
- **Published**: 2020-02-03 18:41:08+00:00
- **Updated**: 2020-02-03 18:41:08+00:00
- **Authors**: Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: We want to detect whether a particular image dataset has been used to train a model. We propose a new technique, \emph{radioactive data}, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). Our experiments on large-scale benchmarks (Imagenet), using standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we can detect usage of radioactive data with high confidence (p<10^-4) even when only 1% of the data used to trained our model is radioactive. Our method is robust to data augmentation and the stochasticity of deep network optimization. As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.



### A neural network model that learns differences in diagnosis strategies among radiologists has an improved area under the curve for aneurysm status classification in magnetic resonance angiography image series
- **Arxiv ID**: http://arxiv.org/abs/2002.01891v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01891v1)
- **Published**: 2020-02-03 19:19:57+00:00
- **Updated**: 2020-02-03 19:19:57+00:00
- **Authors**: Yasuhiko Tachibana, Masataka Nishimori, Naoyuki Kitamura, Kensuke Umehara, Junko Ota, Takayuki Obata, Tatsuya Higashi
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To construct a neural network model that can learn the different diagnosing strategies of radiologists to better classify aneurysm status in magnetic resonance angiography images. Materials and methods: This retrospective study included 3423 time-of-flight brain magnetic resonance angiography image series (subjects: male 1843 [mean age, 50.2 +/- 11.7 years], female 1580 [50.8 +/- 11.3 years]) recorded from November 2017 through January 2019. The image series were read independently for aneurysm status by one of four board-certified radiologists, who were assisted by an established deep learning-based computer-assisted diagnosis (CAD) system. The constructed neural networks were trained to classify the aneurysm status of zero to five aneurysm-suspicious areas suggested by the CAD system for each image series, and any additional aneurysm areas added by the radiologists, and this classification was compared with the judgment of the annotating radiologist. Image series were randomly allocated to training and testing data in an 8:2 ratio. The accuracy of the classification was compared by receiver operating characteristic analysis between the control model that accepted only image data as input and the proposed model that additionally accepted the information of who the annotating radiologist was. The DeLong test was used to compare areas under the curves (P < 0.05 was considered significant). Results: The area under the curve was larger in the proposed model (0.845) than in the control model (0.793), and the difference was significant (P < 0.0001). Conclusion: The proposed model improved classification accuracy by learning the diagnosis strategies of individual annotating radiologists.



### Adversarial Color Enhancement: Generating Unrestricted Adversarial Images by Optimizing a Color Filter
- **Arxiv ID**: http://arxiv.org/abs/2002.01008v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01008v3)
- **Published**: 2020-02-03 20:44:29+00:00
- **Updated**: 2020-08-09 18:23:24+00:00
- **Authors**: Zhengyu Zhao, Zhuoran Liu, Martha Larson
- **Comment**: Accepted by BMVC 2020. Code is available at
  https://github.com/ZhengyuZhao/ACE
- **Journal**: None
- **Summary**: We introduce an approach that enhances images using a color filter in order to create adversarial effects, which fool neural networks into misclassification. Our approach, Adversarial Color Enhancement (ACE), generates unrestricted adversarial images by optimizing the color filter via gradient descent. The novelty of ACE is its incorporation of established practice for image enhancement in a transparent manner. Experimental results validate the white-box adversarial strength and black-box transferability of ACE. A range of examples demonstrates the perceptual quality of images that ACE produces. ACE makes an important contribution to recent work that moves beyond $L_p$ imperceptibility and focuses on unrestricted adversarial modifications that yield large perceptible perturbations, but remain non-suspicious, to the human eye. The future potential of filter-based adversaries is also explored in two directions: guiding ACE with common enhancement practices (e.g., Instagram filters) towards specific attractive image styles and adapting ACE to image semantics. Code is available at https://github.com/ZhengyuZhao/ACE.



### Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2002.01014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2002.01014v1)
- **Published**: 2020-02-03 21:03:20+00:00
- **Updated**: 2020-02-03 21:03:20+00:00
- **Authors**: P. Jonathon Phillips, Mark Przybocki
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: Traditionally, researchers in automatic face recognition and biometric technologies have focused on developing accurate algorithms. With this technology being integrated into operational systems, engineers and scientists are being asked, do these systems meet societal norms? The origin of this line of inquiry is `trust' of artificial intelligence (AI) systems. In this paper, we concentrate on adapting explainable AI to face recognition and biometrics, and we present four principles of explainable AI to face recognition and biometrics. The principles are illustrated by $\it{four}$ case studies, which show the challenges and issues in developing algorithms that can produce explanations.



### Stan: Small tumor-aware network for breast ultrasound image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.01034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01034v1)
- **Published**: 2020-02-03 22:25:01+00:00
- **Updated**: 2020-02-03 22:25:01+00:00
- **Authors**: Bryar Shareef, Min Xian, Aleksandar Vakanski
- **Comment**: 5 pages, 3 figures, Accepted to 2020 ISBI conference
- **Journal**: None
- **Summary**: Breast tumor segmentation provides accurate tumor boundary, and serves as a key step toward further cancer quantification. Although deep learning-based approaches have been proposed and achieved promising results, existing approaches have difficulty in detecting small breast tumors. The capacity to detecting small tumors is particularly important in finding early stage cancers using computer-aided diagnosis (CAD) systems. In this paper, we propose a novel deep learning architecture called Small Tumor-Aware Network (STAN), to improve the performance of segmenting tumors with different size. The new architecture integrates both rich context information and high-resolution image features. We validate the proposed approach using seven quantitative metrics on two public breast ultrasound datasets. The proposed approach outperformed the state-of-the-art approaches in segmenting small breast tumors. Index



### Efficient 2D neuron boundary segmentation with local topological constraints
- **Arxiv ID**: http://arxiv.org/abs/2002.01036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01036v1)
- **Published**: 2020-02-03 22:28:39+00:00
- **Updated**: 2020-02-03 22:28:39+00:00
- **Authors**: Thanuja D. Ambegoda, Matthew Cook
- **Comment**: Presented at the Connectomics Workshop - Neural Information
  Processing Systems (NIPS), December 10th, 2016 - Barcelona, Spain
- **Journal**: None
- **Summary**: We present a method for segmenting neuron membranes in 2D electron microscopy imagery. This segmentation task has been a bottleneck to reconstruction efforts of the brain's synaptic circuits. One common problem is the misclassification of blurry membrane fragments as cell interior, which leads to merging of two adjacent neuron sections into one via the blurry membrane region. Human annotators can easily avoid such errors by implicitly performing gap completion, taking into account the continuity of membranes.   Drawing inspiration from these human strategies, we formulate the segmentation task as an edge labeling problem on a graph with local topological constraints. We derive an integer linear program (ILP) that enforces membrane continuity, i.e. the absence of gaps. The cost function of the ILP is the pixel-wise deviation of the segmentation from a priori membrane probabilities derived from the data.   Based on membrane probability maps obtained using random forest classifiers and convolutional neural networks, our method improves the neuron boundary segmentation accuracy compared to a variety of standard segmentation approaches. Our method successfully performs gap completion and leads to fewer topological errors. The method could potentially also be incorporated into other image segmentation pipelines with known topological constraints.



### Learning Numerical Observers using Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.03763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03763v2)
- **Published**: 2020-02-03 22:58:28+00:00
- **Updated**: 2020-02-22 16:56:25+00:00
- **Authors**: Shenghua He, Weimin Zhou, Hua Li, Mark A. Anastasio
- **Comment**: SPIE Medical Imaging 2020 (Oral)
- **Journal**: None
- **Summary**: Medical imaging systems are commonly assessed by use of objective image quality measures. Supervised deep learning methods have been investigated to implement numerical observers for task-based image quality assessment. However, labeling large amounts of experimental data to train deep neural networks is tedious, expensive, and prone to subjective errors. Computer-simulated image data can potentially be employed to circumvent these issues; however, it is often difficult to computationally model complicated anatomical structures, noise sources, and the response of real world imaging systems. Hence, simulated image data will generally possess physical and statistical differences from the experimental image data they seek to emulate. Within the context of machine learning, these differences between the sets of two images is referred to as domain shift. In this study, we propose and investigate the use of an adversarial domain adaptation method to mitigate the deleterious effects of domain shift between simulated and experimental image data for deep learning-based numerical observers (DL-NOs) that are trained on simulated images but applied to experimental ones. In the proposed method, a DL-NO will initially be trained on computer-simulated image data and subsequently adapted for use with experimental image data, without the need for any labeled experimental images. As a proof of concept, a binary signal detection task is considered. The success of this strategy as a function of the degree of domain shift present between the simulated and experimental image data is investigated.



### Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2002.01048v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01048v2)
- **Published**: 2020-02-03 23:17:10+00:00
- **Updated**: 2022-10-06 05:36:32+00:00
- **Authors**: Hao Tang, Philip H. S. Torr, Nicu Sebe
- **Comment**: Accepted to TPAMI, an extended version of a paper published in
  CVPR2019. arXiv admin note: substantial text overlap with arXiv:1904.06807
- **Journal**: None
- **Summary**: We propose a novel model named Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN) for guided image-to-image translation, where we translate an input image into another while respecting an external semantic guidance. The proposed SelectionGAN explicitly utilizes the semantic guidance information and consists of two stages. In the first stage, the input image and the conditional semantic guidance are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using the proposed multi-scale spatial pooling & channel selection module and the multi-channel attention selection module. Moreover, uncertainty maps automatically learned from attention maps are used to guide the pixel loss for better network optimization. Exhaustive experiments on four challenging guided image-to-image translation tasks (face, hand, body, and street view) demonstrate that our SelectionGAN is able to generate significantly better results than the state-of-the-art methods. Meanwhile, the proposed framework and modules are unified solutions and can be applied to solve other generation tasks such as semantic image synthesis. The code is available at https://github.com/Ha0Tang/SelectionGAN.



### Deep-URL: A Model-Aware Approach To Blind Deconvolution Based On Deep Unfolded Richardson-Lucy Network
- **Arxiv ID**: http://arxiv.org/abs/2002.01053v3
- **DOI**: 10.1109/ICIP40778.2020.9190825
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01053v3)
- **Published**: 2020-02-03 23:43:08+00:00
- **Updated**: 2020-06-07 21:19:09+00:00
- **Authors**: Chirag Agarwal, Shahin Khobahi, Arindam Bose, Mojtaba Soltanalian, Dan Schonfeld
- **Comment**: Accepted. 27th IEEE International Conference on Image Processing
  (ICIP), 2020
- **Journal**: None
- **Summary**: The lack of interpretability in current deep learning models causes serious concerns as they are extensively used for various life-critical applications. Hence, it is of paramount importance to develop interpretable deep learning models. In this paper, we consider the problem of blind deconvolution and propose a novel model-aware deep architecture that allows for the recovery of both the blur kernel and the sharp image from the blurred image. In particular, we propose the Deep Unfolded Richardson-Lucy (Deep-URL) framework -- an interpretable deep-learning architecture that can be seen as an amalgamation of classical estimation technique and deep neural network, and consequently leads to improved performance. Our numerical investigations demonstrate significant improvement compared to state-of-the-art algorithms.



