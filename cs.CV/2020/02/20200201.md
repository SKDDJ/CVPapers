# Arxiv Papers in cs.CV on 2020-02-01
### Better Compression with Deep Pre-Editing
- **Arxiv ID**: http://arxiv.org/abs/2002.00113v3
- **DOI**: 10.1109/TIP.2021.3096085
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00113v3)
- **Published**: 2020-02-01 00:35:46+00:00
- **Updated**: 2021-07-23 18:44:52+00:00
- **Authors**: Hossein Talebi, Damien Kelly, Xiyang Luo, Ignacio Garcia Dorado, Feng Yang, Peyman Milanfar, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Could we compress images via standard codecs while avoiding visible artifacts? The answer is obvious -- this is doable as long as the bit budget is generous enough. What if the allocated bit-rate for compression is insufficient? Then unfortunately, artifacts are a fact of life. Many attempts were made over the years to fight this phenomenon, with various degrees of success. In this work we aim to break the unholy connection between bit-rate and image quality, and propose a way to circumvent compression artifacts by pre-editing the incoming image and modifying its content to fit the given bits. We design this editing operation as a learned convolutional neural network, and formulate an optimization problem for its training. Our loss takes into account a proximity between the original image and the edited one, a bit-budget penalty over the proposed image, and a no-reference image quality measure for forcing the outcome to be visually pleasing. The proposed approach is demonstrated on the popular JPEG compression, showing savings in bits and/or improvements in visual quality, obtained with intricate editing effects.



### AdvectiveNet: An Eulerian-Lagrangian Fluidic reservoir for Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2002.00118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00118v3)
- **Published**: 2020-02-01 01:21:05+00:00
- **Updated**: 2020-06-24 19:44:09+00:00
- **Authors**: Xingzhe He, Helen Lu Cao, Bo Zhu
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.



### On the Consistency of Optimal Bayesian Feature Selection in the Presence of Correlations
- **Arxiv ID**: http://arxiv.org/abs/2002.00120v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, 62F15, 62C10, 62F07, 92C37
- **Links**: [PDF](http://arxiv.org/pdf/2002.00120v1)
- **Published**: 2020-02-01 01:41:08+00:00
- **Updated**: 2020-02-01 01:41:08+00:00
- **Authors**: Ali Foroughi pour, Lori A. Dalton
- **Comment**: 33 pages, 1 figure
- **Journal**: None
- **Summary**: Optimal Bayesian feature selection (OBFS) is a multivariate supervised screening method designed from the ground up for biomarker discovery. In this work, we prove that Gaussian OBFS is strongly consistent under mild conditions, and provide rates of convergence for key posteriors in the framework. These results are of enormous importance, since they identify precisely what features are selected by OBFS asymptotically, characterize the relative rates of convergence for posteriors on different types of features, provide conditions that guarantee convergence, justify the use of OBFS when its internal assumptions are invalid, and set the stage for understanding the asymptotic behavior of other algorithms based on the OBFS framework.



### Global Texture Enhancement for Fake Face Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2002.00133v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00133v3)
- **Published**: 2020-02-01 03:46:23+00:00
- **Updated**: 2020-03-19 00:54:37+00:00
- **Authors**: Zhengzhe Liu, Xiaojuan Qi, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) can generate realistic fake face images that can easily fool human beings.On the contrary, a common Convolutional Neural Network(CNN) discriminator can achieve more than 99.9% accuracyin discerning fake/real images. In this paper, we conduct an empirical study on fake/real faces, and have two important observations: firstly, the texture of fake faces is substantially different from real ones; secondly, global texture statistics are more robust to image editing and transferable to fake faces from different GANs and datasets. Motivated by the above observations, we propose a new architecture coined as Gram-Net, which leverages global image texture representations for robust fake image detection. Experimental results on several datasets demonstrate that our Gram-Net outperforms existing approaches. Especially, our Gram-Netis more robust to image editings, e.g. down-sampling, JPEG compression, blur, and noise. More importantly, our Gram-Net generalizes significantly better in detecting fake faces from GAN models not seen in the training phase and can perform decently in detecting fake natural images.



### Training-free Monocular 3D Event Detection System for Traffic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2002.00137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00137v1)
- **Published**: 2020-02-01 04:42:57+00:00
- **Updated**: 2020-02-01 04:42:57+00:00
- **Authors**: Lijun Yu, Peng Chen, Wenhe Liu, Guoliang Kang, Alexander G. Hauptmann
- **Comment**: To be published in 2019 IEEE International Conference on Big Data
  (Big Data), IEEE
- **Journal**: None
- **Summary**: We focus on the problem of detecting traffic events in a surveillance scenario, including the detection of both vehicle actions and traffic collisions. Existing event detection systems are mostly learning-based and have achieved convincing performance when a large amount of training data is available. However, in real-world scenarios, collecting sufficient labeled training data is expensive and sometimes impossible (e.g. for traffic collision detection). Moreover, the conventional 2D representation of surveillance views is easily affected by occlusions and different camera views in nature. To deal with the aforementioned problems, in this paper, we propose a training-free monocular 3D event detection system for traffic surveillance. Our system firstly projects the vehicles into the 3D Euclidean space and estimates their kinematic states. Then we develop multiple simple yet effective ways to identify the events based on the kinematic patterns, which need no further training. Consequently, our system is robust to the occlusions and the viewpoint changes. Exclusive experiments report the superior result of our method on large-scale real-world surveillance datasets, which validates the effectiveness of our proposed system.



### Towards Evaluating Gaussian Blurring in Perceptual Hashing as a Facial Image Filter
- **Arxiv ID**: http://arxiv.org/abs/2002.00140v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2002.00140v2)
- **Published**: 2020-02-01 04:52:41+00:00
- **Updated**: 2020-09-20 22:00:06+00:00
- **Authors**: Yigit Alparslan, Ken Alparslan, Mannika Kshettry, Louis Kratz
- **Comment**: 5 pages, fixed typos, added references in Introduction section, added
  co-author due to post-publication contributions
- **Journal**: None
- **Summary**: With the growth in social media, there is a huge amount of images of faces available on the internet. Often, people use other people's pictures on their own profile. Perceptual hashing is often used to detect whether two images are identical. Therefore, it can be used to detect whether people are misusing others' pictures. In perceptual hashing, a hash is calculated for a given image, and a new test image is mapped to one of the existing hashes if duplicate features are present. Therefore, it can be used as an image filter to flag banned image content or adversarial attacks --which are modifications that are made on purpose to deceive the filter-- even though the content might be changed to deceive the filters. For this reason, it is critical for perceptual hashing to be robust enough to take transformations such as resizing, cropping, and slight pixel modifications into account. In this paper, we would like to propose to experiment with effect of gaussian blurring in perceptual hashing for detecting misuse of personal images specifically for face images. We hypothesize that use of gaussian blurring on the image before calculating its hash will increase the accuracy of our filter that detects adversarial attacks which consist of image cropping, adding text annotation, and image rotation.



### Asymmetric Distribution Measure for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00153v1)
- **Published**: 2020-02-01 06:41:52+00:00
- **Updated**: 2020-02-01 06:41:52+00:00
- **Authors**: Wenbin Li, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao, Jiebo Luo
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: The core idea of metric-based few-shot image classification is to directly measure the relations between query images and support classes to learn transferable feature embeddings. Previous work mainly focuses on image-level feature representations, which actually cannot effectively estimate a class's distribution due to the scarcity of samples. Some recent work shows that local descriptor based representations can achieve richer representations than image-level based representations. However, such works are still based on a less effective instance-level metric, especially a symmetric metric, to measure the relations between query images and support classes. Given the natural asymmetric relation between a query image and a support class, we argue that an asymmetric measure is more suitable for metric-based few-shot learning. To that end, we propose a novel Asymmetric Distribution Measure (ADM) network for few-shot learning by calculating a joint local and global asymmetric measure between two multivariate local distributions of queries and classes. Moreover, a task-aware Contrastive Measure Strategy (CMS) is proposed to further enhance the measure function. On popular miniImageNet and tieredImageNet, we achieve $3.02\%$ and $1.56\%$ gains over the state-of-the-art method on the $5$-way $1$-shot task, respectively, validating our innovative design of asymmetric distribution measures for few-shot learning.



### Deep Multi-View Enhancement Hashing for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.00169v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00169v2)
- **Published**: 2020-02-01 08:32:27+00:00
- **Updated**: 2020-06-16 02:41:20+00:00
- **Authors**: Chenggang Yan, Biao Gong, Yuxuan Wei, Yue Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However, large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore, we try to introduce the multi-view deep neural network into the hash learning field, and design an efficient and innovative retrieval model, which has achieved a significant improvement in retrieval performance. In this paper, we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views, which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval, we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.



### Unbiased Scene Graph Generation via Rich and Fair Semantic Extraction
- **Arxiv ID**: http://arxiv.org/abs/2002.00176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00176v1)
- **Published**: 2020-02-01 09:28:44+00:00
- **Updated**: 2020-02-01 09:28:44+00:00
- **Authors**: Bin Wen, Jie Luo, Xianglong Liu, Lei Huang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Extracting graph representation of visual scenes in image is a challenging task in computer vision. Although there has been encouraging progress of scene graph generation in the past decade, we surprisingly find that the performance of existing approaches is largely limited by the strong biases, which mainly stem from (1) unconsciously assuming relations with certain semantic properties such as symmetric and (2) imbalanced annotations over different relations. To alleviate the negative effects of these biases, we proposed a new and simple architecture named Rich and Fair semantic extraction network (RiFa for short), to not only capture rich semantic properties of the relations, but also fairly predict relations with different scale of annotations. Using pseudo-siamese networks, RiFa embeds the subject and object respectively to distinguish their semantic differences and meanwhile preserve their underlying semantic properties. Then, it further predicts subject-object relations based on both the visual and semantic features of entities under certain contextual area, and fairly ranks the relation predictions for those with a few annotations. Experiments on the popular Visual Genome dataset show that RiFa achieves state-of-the-art performance under several challenging settings of scene graph task. Especially, it performs significantly better on capturing different semantic properties of relations, and obtains the best overall per relation performance.



### AdvJND: Generating Adversarial Examples with Just Noticeable Difference
- **Arxiv ID**: http://arxiv.org/abs/2002.00179v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00179v2)
- **Published**: 2020-02-01 09:55:27+00:00
- **Updated**: 2020-06-23 09:34:17+00:00
- **Authors**: Zifei Zhang, Kai Qiao, Lingyun Jiang, Linyuan Wang, Bin Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with traditional machine learning models, deep neural networks perform better, especially in image classification tasks. However, they are vulnerable to adversarial examples. Adding small perturbations on examples causes a good-performance model to misclassify the crafted examples, without category differences in the human eyes, and fools deep models successfully. There are two requirements for generating adversarial examples: the attack success rate and image fidelity metrics. Generally, perturbations are increased to ensure the adversarial examples' high attack success rate; however, the adversarial examples obtained have poor concealment. To alleviate the tradeoff between the attack success rate and image fidelity, we propose a method named AdvJND, adding visual model coefficients, just noticeable difference coefficients, in the constraint of a distortion function when generating adversarial examples. In fact, the visual subjective feeling of the human eyes is added as a priori information, which decides the distribution of perturbations, to improve the image quality of adversarial examples. We tested our method on the FashionMNIST, CIFAR10, and MiniImageNet datasets. Adversarial examples generated by our AdvJND algorithm yield gradient distributions that are similar to those of the original inputs. Hence, the crafted noise can be hidden in the original inputs, thus improving the attack concealment significantly.



### Deeply Activated Salient Region for Instance Search
- **Arxiv ID**: http://arxiv.org/abs/2002.00185v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00185v3)
- **Published**: 2020-02-01 10:25:59+00:00
- **Updated**: 2020-03-23 02:57:02+00:00
- **Authors**: Hui-Chu Xiao, Wan-Lei Zhao, Jie Lin, Chong-Wah Ngo
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: The performance of instance search depends heavily on the ability to locate and describe a wide variety of object instances in a video/image collection. Due to the lack of proper mechanism in locating instances and deriving feature representation, instance search is generally only effective for retrieving instances of known object categories. In this paper, a simple but effective instance-level feature representation is presented. Different from other approaches, the issues in class-agnostic instance localization and distinctive feature representation are considered. The former is achieved by detecting salient instance regions from an image by a layer-wise back-propagation process. The back-propagation starts from the last convolution layer of a pre-trained CNN that is originally used for classification. The back-propagation proceeds layer-by-layer until it reaches the input layer. This allows the salient instance regions in the input image from both known and unknown categories to be activated. Each activated salient region covers the full or more usually a major range of an instance. The distinctive feature representation is produced by average-pooling on the feature map of certain layer with the detected instance region. Experiments show that such kind of feature representation demonstrates considerably better performance over most of the existing approaches. In addition, we show that the proposed feature descriptor is also suitable for content-based image search.



### Large Hole Image Inpainting With Compress-Decompression Network
- **Arxiv ID**: http://arxiv.org/abs/2002.00199v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00199v1)
- **Published**: 2020-02-01 12:39:13+00:00
- **Updated**: 2020-02-01 12:39:13+00:00
- **Authors**: Zhenghang Wu, Yidong Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting technology can patch images with missing pixels. Existing methods propose convolutional neural networks to repair corrupted images. The networks focus on the valid pixels around the missing pixels, use the encoder-decoder structure to extract valuable information, and use the information to fix the vacancy. However, if the missing part is too large to provide useful information, the result will exist blur, color mixing, and object confusion. In order to patch the large hole image, we study the existing approaches and propose a new network, the compression-decompression network. The compression network takes responsibility for inpainting and generating a down-sample image. The decompression network takes responsibility for extending the down-sample image into the original resolution. We construct the compression network with the residual network and propose a similar texture selection algorithm to extend the image that is better than using the super-resolution network. We evaluate our model over Places2 and CelebA data set and use the similarity ratio as the metric. The result shows that our model has better performance when the inpainting task has many conflicts.



### Leveraging Uncertainties for Deep Multi-modal Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2002.00216v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00216v1)
- **Published**: 2020-02-01 14:24:51+00:00
- **Updated**: 2020-02-01 14:24:51+00:00
- **Authors**: Di Feng, Yifan Cao, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a probabilistic deep neural network that combines LiDAR point clouds and RGB camera images for robust, accurate 3D object detection. We explicitly model uncertainties in the classification and regression tasks, and leverage uncertainties to train the fusion network via a sampling mechanism. We validate our method on three datasets with challenging real-world driving scenarios. Experimental results show that the predicted uncertainties reflect complex environmental uncertainty like difficulties of a human expert to label objects. The results also show that our method consistently improves the Average Precision by up to 7% compared to the baseline method. When sensors are temporally misaligned, the sampling method improves the Average Precision by up to 20%, showing its high robustness against noisy sensor inputs.



### Domain segmentation and adjustment for generalized zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00226v1)
- **Published**: 2020-02-01 15:00:56+00:00
- **Updated**: 2020-02-01 15:00:56+00:00
- **Authors**: Xinsheng Wang, Shanmin Pang, Jihua Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In the generalized zero-shot learning, synthesizing unseen data with generative models has been the most popular method to address the imbalance of training data between seen and unseen classes. However, this method requires that the unseen semantic information is available during the training stage, and training generative models is not trivial. Given that the generator of these models can only be trained with seen classes, we argue that synthesizing unseen data may not be an ideal approach for addressing the domain shift caused by the imbalance of the training data. In this paper, we propose to realize the generalized zero-shot recognition in different domains. Thus, unseen (seen) classes can avoid the effect of the seen (unseen) classes. In practice, we propose a threshold and probabilistic distribution joint method to segment the testing instances into seen, unseen and uncertain domains. Moreover, the uncertain domain is further adjusted to alleviate the domain shift. Extensive experiments on five benchmark datasets show that the proposed method exhibits competitive performance compared with that based on generative models.



### Foreground object segmentation in RGB-D data implemented on GPU
- **Arxiv ID**: http://arxiv.org/abs/2002.00250v1
- **DOI**: 10.1007/978-3-030-50936-1_68
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00250v1)
- **Published**: 2020-02-01 17:53:39+00:00
- **Updated**: 2020-02-01 17:53:39+00:00
- **Authors**: Piotr Janus, Tomasz Kryjak, Marek Gorgon
- **Comment**: 12 pages, 4 figures, submitted to KKA 2020 conference
- **Journal**: None
- **Summary**: This paper presents a GPU implementation of two foreground object segmentation algorithms: Gaussian Mixture Model (GMM) and Pixel Based Adaptive Segmenter (PBAS) modified for RGB-D data support. The simultaneous use of colour (RGB) and depth (D) data allows to improve segmentation accuracy, especially in case of colour camouflage, illumination changes and occurrence of shadows. Three GPUs were used to accelerate calculations: embedded NVIDIA Jetson TX2 (Maxwell architecture), mobile NVIDIA GeForce GTX 1050m (Pascal architecture) and efficient NVIDIA RTX 2070 (Turing architecture). Segmentation accuracy comparable to previously published works was obtained. Moreover, the use of a GPU platform allowed to get real-time image processing. In addition, the system has been adapted to work with two RGB-D sensors: RealSense D415 and D435 from Intel.



### Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with Visual Computing for Improved Music Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.00251v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.00251v1)
- **Published**: 2020-02-01 17:57:14+00:00
- **Updated**: 2020-02-01 17:57:14+00:00
- **Authors**: Alexander Schindler
- **Comment**: Dissertation at TU Wien
- **Journal**: None
- **Summary**: This thesis combines audio-analysis with computer vision to approach Music Information Retrieval (MIR) tasks from a multi-modal perspective. This thesis focuses on the information provided by the visual layer of music videos and how it can be harnessed to augment and improve tasks of the MIR research domain. The main hypothesis of this work is based on the observation that certain expressive categories such as genre or theme can be recognized on the basis of the visual content alone, without the sound being heard. This leads to the hypothesis that there exists a visual language that is used to express mood or genre. In a further consequence it can be concluded that this visual information is music related and thus should be beneficial for the corresponding MIR tasks such as music genre classification or mood recognition. A series of comprehensive experiments and evaluations are conducted which are focused on the extraction of visual information and its application in different MIR tasks. A custom dataset is created, suitable to develop and test visual features which are able to represent music related information. Evaluations range from low-level visual features to high-level concepts retrieved by means of Deep Convolutional Neural Networks. Additionally, new visual features are introduced capturing rhythmic visual patterns. In all of these experiments the audio-based results serve as benchmark for the visual and audio-visual approaches. The experiments are conducted for three MIR tasks Artist Identification, Music Genre Classification and Cross-Genre Classification. Experiments show that an audio-visual approach harnessing high-level semantic information gained from visual concept detection, outperforms audio-only genre-classification accuracy by 16.43%.



### Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00264v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00264v3)
- **Published**: 2020-02-01 19:41:26+00:00
- **Updated**: 2020-06-19 05:54:24+00:00
- **Authors**: Mahesh Kumar Krishna Reddy, Mohammad Hossain, Mrigank Rochan, Yang Wang
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: We consider the problem of few-shot scene adaptive crowd counting. Given a target camera scene, our goal is to adapt a model to this specific scene with only a few labeled images of that scene. The solution to this problem has potential applications in numerous real-world scenarios, where we ideally like to deploy a crowd counting model specially adapted to a target camera. We accomplish this challenge by taking inspiration from the recently introduced learning-to-learn paradigm in the context of few-shot regime. In training, our method learns the model parameters in a way that facilitates the fast adaptation to the target scene. At test time, given a target scene with a small number of labeled data, our method quickly adapts to that scene with a few gradient updates to the learned parameters. Our extensive experimental results show that the proposed approach outperforms other alternatives in few-shot scene adaptive crowd counting. Code is available at https://github.com/maheshkkumar/fscc.



### Deep Feature Fusion for Mitosis Counting
- **Arxiv ID**: http://arxiv.org/abs/2002.03781v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03781v3)
- **Published**: 2020-02-01 20:20:00+00:00
- **Updated**: 2022-02-17 00:15:47+00:00
- **Authors**: Robin Elizabeth Yancey
- **Comment**: None
- **Journal**: None
- **Summary**: Each woman living in the United States has about 1 in 8 chance of developing invasive breast cancer. The mitotic cell count is one of the most common tests to assess the aggressiveness or grade of breast cancer. In this prognosis, histopathology images must be examined by a pathologist using high-resolution microscopes to count the cells. Unfortunately, this can be an exhaustive task with poor reproducibility, especially for non-experts. Deep learning networks have recently been adapted to medical applications which are able to automatically localize these regions of interest. However, these region-based networks lack the ability to take advantage of the segmentation features produced by a full image CNN which are often used as a sole method of detection. Therefore, the proposed method leverages Faster RCNN for object detection while fusing segmentation features generated by a UNet with RGB image features to achieve an F-score of 0.508 on the MITOS-ATYPIA 2014 mitosis counting challenge dataset, outperforming state-of-the-art methods.



