# Arxiv Papers in cs.CV on 2020-02-10
### Semi-Supervised Class Discovery
- **Arxiv ID**: http://arxiv.org/abs/2002.03480v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03480v2)
- **Published**: 2020-02-10 00:29:44+00:00
- **Updated**: 2020-02-22 01:31:08+00:00
- **Authors**: Jeremy Nixon, Jeremiah Liu, David Berthelot
- **Comment**: None
- **Journal**: None
- **Summary**: One promising approach to dealing with datapoints that are outside of the initial training distribution (OOD) is to create new classes that capture similarities in the datapoints previously rejected as uncategorizable. Systems that generate labels can be deployed against an arbitrary amount of data, discovering classification schemes that through training create a higher quality representation of data. We introduce the Dataset Reconstruction Accuracy, a new and important measure of the effectiveness of a model's ability to create labels. We introduce benchmarks against this Dataset Reconstruction metric. We apply a new heuristic, class learnability, for deciding whether a class is worthy of addition to the training dataset. We show that our class discovery system can be successfully applied to vision and language, and we demonstrate the value of semi-supervised learning in automatically discovering novel classes.



### Ultra High Fidelity Image Compression with $\ell_\infty$-constrained Encoding and Deep Decoding
- **Arxiv ID**: http://arxiv.org/abs/2002.03482v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03482v1)
- **Published**: 2020-02-10 00:33:39+00:00
- **Updated**: 2020-02-10 00:33:39+00:00
- **Authors**: Xi Zhang, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In many professional fields, such as medicine, remote sensing and sciences, users often demand image compression methods to be mathematically lossless. But lossless image coding has a rather low compression ratio (around 2:1 for natural images). The only known technique to achieve significant compression while meeting the stringent fidelity requirements is the methodology of $\ell_\infty$-constrained coding that was developed and standardized in nineties. We make a major progress in $\ell_\infty$-constrained image coding after two decades, by developing a novel CNN-based soft $\ell_\infty$-constrained decoding method. The new method repairs compression defects by using a restoration CNN called the $\ell_\infty\mbox{-SDNet}$ to map a conventionally decoded image to the latent image. A unique strength of the $\ell_\infty\mbox{-SDNet}$ is its ability to enforce a tight error bound on a per pixel basis. As such, no small distinctive structures of the original image can be dropped or distorted, even if they are statistical outliers that are otherwise sacrificed by mainstream CNN restoration methods. More importantly, this research ushers in a new image compression system of $\ell_\infty$-constrained encoding and deep soft decoding ($\ell_\infty\mbox{-ED}^2$). The $\ell_\infty \mbox{-ED}^2$ approach beats the best of existing lossy image compression methods (e.g., BPG, WebP, etc.) not only in $\ell_\infty$ but also in $\ell_2$ error metric and perceptual quality, for bit rates near the threshold of perceptually transparent reconstruction. Operationally, the new compression system is practical, with a low-complexity real-time encoder and a cascade decoder consisting of a fast initial decoder and an optional CNN soft decoder.



### Watch out! Motion is Blurring the Vision of Your Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.03500v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03500v3)
- **Published**: 2020-02-10 02:33:08+00:00
- **Updated**: 2020-11-09 05:52:03+00:00
- **Authors**: Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu
- **Comment**: 19 pages, 16 figures. This paper has been accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: The state-of-the-art deep neural networks (DNNs) are vulnerable against adversarial examples with additive random-like noise perturbations. While such examples are hardly found in the physical world, the image blurring effect caused by object motion, on the other hand, commonly occurs in practice, making the study of which greatly important especially for the widely adopted real-time image processing tasks (e.g., object detection, tracking). In this paper, we initiate the first step to comprehensively investigate the potential hazards of the blur effect for DNN, caused by object motion. We propose a novel adversarial attack method that can generate visually natural motion-blurred adversarial examples, named motion-based adversarial blur attack (ABBA). To this end, we first formulate the kernel-prediction-based attack where an input image is convolved with kernels in a pixel-wise way, and the misclassification capability is achieved by tuning the kernel weights. To generate visually more natural and plausible examples, we further propose the saliency-regularized adversarial kernel prediction, where the salient region serves as a moving object, and the predicted kernel is regularized to achieve naturally visual effects. Besides, the attack is further enhanced by adaptively tuning the translations of object and background. A comprehensive evaluation on the NeurIPS'17 adversarial competition dataset demonstrates the effectiveness of ABBA by considering various kernel sizes, translations, and regions. The in-depth study further confirms that our method shows more effective penetrating capability to the state-of-the-art GAN-based deblurring mechanisms compared with other blurring methods. We release the code to https://github.com/tsingqguo/ABBA.



### Segmenting Unseen Industrial Components in a Heavy Clutter Using RGB-D Fusion and Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2002.03501v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03501v3)
- **Published**: 2020-02-10 02:33:21+00:00
- **Updated**: 2020-06-02 00:23:59+00:00
- **Authors**: Seunghyeok Back, Jongwon Kim, Raeyoung Kang, Seungjun Choi, Kyoobin Lee
- **Comment**: 5 pages,6 figures, Accepted to ICIP 2020
- **Journal**: None
- **Summary**: Segmentation of unseen industrial parts is essential for autonomous industrial systems. However, industrial components are texture-less, reflective, and often found in cluttered and unstructured environments with heavy occlusion, which makes it more challenging to deal with unseen objects. To tackle this problem, we present a synthetic data generation pipeline that randomizes textures via domain randomization to focus on the shape information. In addition, we propose an RGB-D Fusion Mask R-CNN with a confidence map estimator, which exploits reliable depth information in multiple feature levels. We transferred the trained model to real-world scenarios and evaluated its performance by making comparisons with baselines and ablation studies. We demonstrate that our methods, which use only synthetic data, could be effective solutions for unseen industrial components segmentation.



### A New Perspective for Flexible Feature Gathering in Scene Text Recognition Via Character Anchor Pooling
- **Arxiv ID**: http://arxiv.org/abs/2002.03509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03509v1)
- **Published**: 2020-02-10 03:01:23+00:00
- **Updated**: 2020-02-10 03:01:23+00:00
- **Authors**: Shangbang Long, Yushuo Guan, Kaigui Bian, Cong Yao
- **Comment**: To appear at ICASSP 2020
- **Journal**: None
- **Summary**: Irregular scene text recognition has attracted much attention from the research community, mainly due to the complexity of shapes of text in natural scene.   However, recent methods either rely on shape-sensitive modules such as bounding box regression, or discard sequence learning.   To tackle these issues, we propose a pair of coupling modules, termed as Character Anchoring Module (CAM) and Anchor Pooling Module (APM), to extract high-level semantics from two-dimensional space to form feature sequences.   The proposed CAM localizes the text in a shape-insensitive way by design by anchoring characters individually.   APM then interpolates and gathers features flexibly along the character anchors which enables sequence learning.   The complementary modules realize a harmonic unification of spatial information and sequence learning.   With the proposed modules, our recognition system surpasses previous state-of-the-art scores on irregular and perspective text datasets, including, ICDAR 2015, CUTE, and Total-Text, while paralleling state-of-the-art performance on regular text datasets.



### UGRWO-Sampling for COVID-19 dataset: A modified random walk under-sampling approach based on graphs to imbalanced data classification
- **Arxiv ID**: http://arxiv.org/abs/2002.03521v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03521v3)
- **Published**: 2020-02-10 03:29:24+00:00
- **Updated**: 2021-12-02 20:43:13+00:00
- **Authors**: Saeideh Roshanfekr, Shahriar Esmaeili, Hassan Ataeian, Ali Amiri
- **Comment**: 43 pages, 4 figures, 9 tables
- **Journal**: None
- **Summary**: This paper proposes a new RWO-Sampling (Random Walk Over-Sampling) based on graphs for imbalanced datasets. In this method, two schemes based on under-sampling and over-sampling methods are introduced to keep the proximity information robust to noises and outliers. After constructing the first graph on minority class, RWO-Sampling will be implemented on selected samples, and the rest will remain unchanged. The second graph is constructed for the majority class, and the samples in a low-density area (outliers) are removed. Finally, in the proposed method, samples of the majority class in a high-density area are selected, and the rest are eliminated. Furthermore, utilizing RWO-sampling, the boundary of minority class is increased though the outliers are not raised. This method is tested, and the number of evaluation measures is compared to previous methods on nine continuous attribute datasets with different over-sampling rates and one data set for the diagnosis of COVID-19 disease. The experimental results indicated the high efficiency and flexibility of the proposed method for the classification of imbalanced data



### Multi-object Monocular SLAM for Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2002.03528v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03528v2)
- **Published**: 2020-02-10 03:49:16+00:00
- **Updated**: 2020-05-11 11:42:42+00:00
- **Authors**: Gokul B. Nair, Swapnil Daga, Rahul Sajnani, Anirudha Ramesh, Junaid Ahmed Ansari, Krishna Murthy Jatavallabhula, K. Madhava Krishna
- **Comment**: Accepted to IEEE Intelligent Vehicles Symposium 2020 (IV2020)
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of multibody SLAM from a monocular camera. The term multibody, implies that we track the motion of the camera, as well as that of other dynamic participants in the scene. The quintessential challenge in dynamic scenes is unobservability: it is not possible to unambiguously triangulate a moving object from a moving monocular camera. Existing approaches solve restricted variants of the problem, but the solutions suffer relative scale ambiguity (i.e., a family of infinitely many solutions exist for each pair of motions in the scene). We solve this rather intractable problem by leveraging single-view metrology, advances in deep learning, and category-level shape estimation. We propose a multi pose-graph optimization formulation, to resolve the relative and absolute scale factor ambiguities involved. This optimization helps us reduce the average error in trajectories of multiple bodies over real-world datasets, such as KITTI. To the best of our knowledge, our method is the first practical monocular multi-body SLAM system to perform dynamic multi-object and ego localization in a unified framework in metric scale.



### Category-wise Attack: Transferable Adversarial Examples for Anchor Free Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04367v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.04367v4)
- **Published**: 2020-02-10 04:49:03+00:00
- **Updated**: 2020-06-23 00:14:15+00:00
- **Authors**: Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbations can completely change the classification results. Their vulnerability has led to a surge of research in this direction. However, most works dedicated to attacking anchor-based object detection models. In this work, we aim to present an effective and efficient algorithm to generate adversarial examples to attack anchor-free object models based on two approaches. First, we conduct category-wise instead of instance-wise attacks on the object detectors. Second, we leverage the high-level semantic information to generate the adversarial examples. Surprisingly, the generated adversarial examples it not only able to effectively attack the targeted anchor-free object detector but also to be transferred to attack other object detectors, even anchor-based detectors such as Faster R-CNN.



### Adversarial TCAV -- Robust and Effective Interpretation of Intermediate Layers in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.03549v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03549v2)
- **Published**: 2020-02-10 05:15:03+00:00
- **Updated**: 2020-02-26 18:15:30+00:00
- **Authors**: Rahul Soni, Naresh Shah, Chua Tat Seng, Jimmy D. Moore
- **Comment**: None
- **Journal**: None
- **Summary**: Interpreting neural network decisions and the information learned in intermediate layers is still a challenge due to the opaque internal state and shared non-linear interactions. Although (Kim et al, 2017) proposed to interpret intermediate layers by quantifying its ability to distinguish a user-defined concept (from random examples), the questions of robustness (variation against the choice of random examples) and effectiveness (retrieval rate of concept images) remain. We investigate these two properties and propose improvements to make concept activations reliable for practical use.   Effectiveness: If the intermediate layer has effectively learned a user-defined concept, it should be able to recall --- at the testing step --- most of the images containing the proposed concept. For instance, we observed that the recall rate of Tiger shark and Great white shark from the ImageNet dataset with "Fins" as a user-defined concept was only 18.35% for VGG16. To increase the effectiveness of concept learning, we propose A-CAV --- the Adversarial Concept Activation Vector --- this results in larger margins between user concepts and (negative) random examples. This approach improves the aforesaid recall to 76.83% for VGG16.   For robustness, we define it as the ability of an intermediate layer to be consistent in its recall rate (the effectiveness) for different random seeds. We observed that TCAV has a large variance in recalling a concept across different random seeds. For example, the recall of cat images (from a layer learning the concept of tail) varies from 18% to 86% with 20.85% standard deviation on VGG16. We propose a simple and scalable modification that employs a Gram-Schmidt process to sample random noise from concepts and learn an average "concept classifier". This approach improves the aforesaid standard deviation from 20.85% to 6.4%.



### From Anchor Generation to Distribution Alignment: Learning a Discriminative Embedding Space for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03554v1)
- **Published**: 2020-02-10 05:25:33+00:00
- **Updated**: 2020-02-10 05:25:33+00:00
- **Authors**: Fuzhen Li, Zhenfeng Zhu, Xingxing Zhang, Jian Cheng, Yao Zhao
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: In zero-shot learning (ZSL), the samples to be classified are usually projected into side information templates such as attributes. However, the irregular distribution of templates makes classification results confused. To alleviate this issue, we propose a novel framework called Discriminative Anchor Generation and Distribution Alignment Model (DAGDA). Firstly, in order to rectify the distribution of original templates, a diffusion based graph convolutional network, which can explicitly model the interaction between class and side information, is proposed to produce discriminative anchors. Secondly, to further align the samples with the corresponding anchors in anchor space, which aims to refine the distribution in a fine-grained manner, we introduce a semantic relation regularization in anchor space. Following the way of inductive learning, our approach outperforms some existing state-of-the-art methods, on several benchmark datasets, for both conventional as well as generalized ZSL setting. Meanwhile, the ablation experiments strongly demonstrate the effectiveness of each component.



### Vehicle Driving Assistant
- **Arxiv ID**: http://arxiv.org/abs/2002.03556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03556v1)
- **Published**: 2020-02-10 05:32:11+00:00
- **Updated**: 2020-02-10 05:32:11+00:00
- **Authors**: Akanksha Dwivedi, Anoop Toffy, Athul Suresh, Tarini Chandrashekhar
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles has been a common term in our day to day life with car manufacturers like Tesla shipping cars that are SAE Level 3. While these vehicles include a slew of features such as parking assistance and cruise control,they have mostly been tailored to foreign roads. Potholes, and the abundance of them, is something that is unique to our Indian roads. We believe that successful detection of potholes from visual images can be applied in a variety of scenarios. Moreover, the sheer variety in the color, shape and size of potholes makes this problem an apt candidate to be solved using modern machine learning and image processing techniques.



### Multitask Emotion Recognition with Incomplete Labels
- **Arxiv ID**: http://arxiv.org/abs/2002.03557v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.03557v2)
- **Published**: 2020-02-10 05:32:12+00:00
- **Updated**: 2020-03-10 11:52:37+00:00
- **Authors**: Didan Deng, Zhaokang Chen, Bertram E. Shi
- **Comment**: Accepted by FG2020
- **Journal**: None
- **Summary**: We train a unified model to perform three tasks: facial action unit detection, expression classification, and valence-arousal estimation. We address two main challenges of learning the three tasks. First, most existing datasets are highly imbalanced. Second, most existing datasets do not contain labels for all three tasks. To tackle the first challenge, we apply data balancing techniques to experimental datasets. To tackle the second challenge, we propose an algorithm for the multitask model to learn from missing (incomplete) labels. This algorithm has two steps. We first train a teacher model to perform all three tasks, where each instance is trained by the ground truth label of its corresponding task. Secondly, we refer to the outputs of the teacher model as the soft labels. We use the soft labels and the ground truth to train the student model. We find that most of the student models outperform their teacher model on all the three tasks. Finally, we use model ensembling to boost performance further on the three tasks.



### Automatic detection and counting of retina cell nuclei using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2002.03563v1
- **DOI**: 10.1117/12.2567454
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03563v1)
- **Published**: 2020-02-10 05:49:10+00:00
- **Updated**: 2020-02-10 05:49:10+00:00
- **Authors**: S. M. Hadi Hosseini, Hao Chen, Monica M. Jablonski
- **Comment**: 13 pages, 11 figures, 2 tables, SPIE. Medical Imaging 2020 Conference
- **Journal**: Proceedings Volume 11317, Medical Imaging 2020: Biomedical
  Applications in Molecular, Structural, and Functional Imaging; 113172I (2020)
- **Summary**: The ability to automatically detect, classify, calculate the size, number, and grade of retinal cells and other biological objects is critically important in eye disease like age-related macular degeneration (AMD). In this paper, we developed an automated tool based on deep learning technique and Mask R-CNN model to analyze large datasets of transmission electron microscopy (TEM) images and quantify retinal cells with high speed and precision. We considered three categories for outer nuclear layer (ONL) cells: live, intermediate, and pyknotic. We trained the model using a dataset of 24 samples. We then optimized the hyper-parameters using another set of 6 samples. The results of this research, after applying to the test datasets, demonstrated that our method is highly accurate for automatically detecting, categorizing, and counting cell nuclei in the ONL of the retina. Performance of our model was tested using general metrics: general mean average precision (mAP) for detection; and precision, recall, F1-score, and accuracy for categorizing and counting.



### Prototype Refinement Network for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.03579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03579v2)
- **Published**: 2020-02-10 07:06:09+00:00
- **Updated**: 2020-05-09 07:17:59+00:00
- **Authors**: Jinlu Liu, Yongqiang Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation targets to segment new classes with few annotated images provided. It is more challenging than traditional semantic segmentation tasks that segment known classes with abundant annotated images. In this paper, we propose a Prototype Refinement Network (PRNet) to attack the challenge of few-shot segmentation. It firstly learns to bidirectionally extract prototypes from both support and query images of the known classes. Furthermore, to extract representative prototypes of the new classes, we use adaptation and fusion for prototype refinement. The step of adaptation makes the model to learn new concepts which is directly implemented by retraining. Prototype fusion is firstly proposed which fuses support prototypes with query prototypes, incorporating the knowledge from both sides. It is effective in prototype refinement without importing extra learnable parameters. In this way, the prototypes become more discriminative in low-data regimes. Experiments on PASAL-$5^i$ and COCO-$20^i$ demonstrate the superiority of our method. Especially on COCO-$20^i$, PRNet significantly outperforms existing methods by a large margin of 13.1\% in 1-shot setting.



### Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization
- **Arxiv ID**: http://arxiv.org/abs/2002.03592v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03592v3)
- **Published**: 2020-02-10 08:17:26+00:00
- **Updated**: 2020-11-05 14:57:23+00:00
- **Authors**: Philipp Terhörst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted in Pattern Recognition Letters
- **Journal**: None
- **Summary**: Current face recognition systems achieve high progress on several benchmark tests. Despite this progress, recent works showed that these systems are strongly biased against demographic sub-groups. Consequently, an easily integrable solution is needed to reduce the discriminatory effect of these biased systems. Previous work mainly focused on learning less biased face representations, which comes at the cost of a strongly degraded overall recognition performance. In this work, we propose a novel unsupervised fair score normalization approach that is specifically designed to reduce the effect of bias in face recognition and subsequently lead to a significant overall performance boost. Our hypothesis is built on the notation of individual fairness by designing a normalization approach that leads to treating similar individuals similarly. Experiments were conducted on three publicly available datasets captured under controlled and in-the-wild circumstances. Results demonstrate that our solution reduces demographic biases, e.g. by up to 82.7% in the case when gender is considered. Moreover, it mitigates the bias more consistently than existing works. In contrast to previous works, our fair normalization approach enhances the overall performance by up to 53.2% at false match rate of 0.001 and up to 82.9% at a false match rate of 0.00001. Additionally, it is easily integrable into existing recognition systems and not limited to face biometrics.



### How well do U-Net-based segmentation trained on adult cardiac magnetic resonance imaging data generalise to rare congenital heart diseases for surgical planning?
- **Arxiv ID**: http://arxiv.org/abs/2002.04392v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.04392v1)
- **Published**: 2020-02-10 08:50:51+00:00
- **Updated**: 2020-02-10 08:50:51+00:00
- **Authors**: Sven Koehler, Animesh Tandon, Tarique Hussain, Heiner Latus, Thomas Pickardt, Samir Sarikouch, Philipp Beerbaum, Gerald Greil, Sandy Engelhardt, Ivo Wolf
- **Comment**: Accepted for SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: Planning the optimal time of intervention for pulmonary valve replacement surgery in patients with the congenital heart disease Tetralogy of Fallot (TOF) is mainly based on ventricular volume and function according to current guidelines. Both of these two biomarkers are most reliably assessed by segmentation of 3D cardiac magnetic resonance (CMR) images. In several grand challenges in the last years, U-Net architectures have shown impressive results on the provided data. However, in clinical practice, data sets are more diverse considering individual pathologies and image properties derived from different scanner properties. Additionally, specific training data for complex rare diseases like TOF is scarce.   For this work, 1) we assessed the accuracy gap when using a publicly available labelled data set (the Automatic Cardiac Diagnosis Challenge (ACDC) data set) for training and subsequent applying it to CMR data of TOF patients and vice versa and 2) whether we can achieve similar results when applying the model to a more heterogeneous data base.   Multiple deep learning models were trained with four-fold cross validation. Afterwards they were evaluated on the respective unseen CMR images from the other collection. Our results confirm that current deep learning models can achieve excellent results (left ventricle dice of $0.951\pm{0.003}$/$0.941\pm{0.007}$ train/validation) within a single data collection. But once they are applied to other pathologies, it becomes apparent how much they overfit to the training pathologies (dice score drops between $0.072\pm{0.001}$ for the left and $0.165\pm{0.001}$ for the right ventricle).



### Object condensation: one-stage grid-free multi-object reconstruction in physics detectors, graph and image data
- **Arxiv ID**: http://arxiv.org/abs/2002.03605v3
- **DOI**: 10.1140/epjc/s10052-020-08461-2
- **Categories**: **physics.data-an**, cs.CV, eess.IV, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2002.03605v3)
- **Published**: 2020-02-10 09:04:02+00:00
- **Updated**: 2020-09-27 07:48:16+00:00
- **Authors**: Jan Kieseler
- **Comment**: None
- **Journal**: Eur. Phys. J. C 80, 886 (2020)
- **Summary**: High-energy physics detectors, images, and point clouds share many similarities in terms of object detection. However, while detecting an unknown number of objects in an image is well established in computer vision, even machine learning assisted object reconstruction algorithms in particle physics almost exclusively predict properties on an object-by-object basis. Traditional approaches from computer vision either impose implicit constraints on the object size or density and are not well suited for sparse detector data or rely on objects being dense and solid. The object condensation method proposed here is independent of assumptions on object size, sorting or object density, and further generalises to non-image-like data structures, such as graphs and point clouds, which are more suitable to represent detector signals. The pixels or vertices themselves serve as representations of the entire object, and a combination of learnable local clustering in a latent space and confidence assignment allows one to collect condensates of the predicted object properties with a simple algorithm. As proof of concept, the object condensation method is applied to a simple object classification problem in images and used to reconstruct multiple particles from detector signals. The latter results are also compared to a classic particle flow approach.



### End-to-End Facial Deep Learning Feature Compression with Teacher-Student Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2002.03627v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03627v1)
- **Published**: 2020-02-10 10:08:44+00:00
- **Updated**: 2020-02-10 10:08:44+00:00
- **Authors**: Shurun Wang, Wenhan Yang, Shiqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end feature compression scheme by leveraging the representation and learning capability of deep neural networks, towards intelligent front-end equipped analysis with promising accuracy and efficiency. In particular, the extracted features are compactly coded in an end-to-end manner by optimizing the rate-distortion cost to achieve feature-in-feature representation. In order to further improve the compression performance, we present a latent code level teacher-student enhancement model, which could efficiently transfer the low bit-rate representation into a high bit rate one. Such a strategy further allows us to adaptively shift the representation cost to decoding computations, leading to more flexible feature compression with enhanced decoding capability. We verify the effectiveness of the proposed model with the facial feature, and experimental results reveal better compression performance in terms of rate-accuracy compared with existing models.



### Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.03642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03642v1)
- **Published**: 2020-02-10 10:43:20+00:00
- **Updated**: 2020-02-10 10:43:20+00:00
- **Authors**: Jongbin Ryu, Jiun Bae, Jongwoo Lim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a collaborative training algorithm of balanced random forests with convolutional neural networks for domain adaptation tasks. In real scenarios, most domain adaptation algorithms face the challenges from noisy, insufficient training data and open set categorization. In such cases, conventional methods suffer from overfitting and fail to successfully transfer the knowledge of the source to the target domain. To address these issues, the following two techniques are proposed. First, we introduce the optimized decision tree construction method with convolutional neural networks, in which the data at each node are split into equal sizes while maximizing the information gain. It generates balanced decision trees on deep features because of the even-split constraint, which contributes to enhanced discrimination power and reduced overfitting problem. Second, to tackle the domain misalignment problem, we propose the domain alignment loss which penalizes uneven splits of the source and target domain data. By collaboratively optimizing the information gain of the labeled source data as well as the entropy of unlabeled target data distributions, the proposed CoBRF algorithm achieves significantly better performance than the state-of-the-art methods.



### CRVOS: Clue Refining Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.03651v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03651v4)
- **Published**: 2020-02-10 10:55:31+00:00
- **Updated**: 2020-06-02 08:15:08+00:00
- **Authors**: Suhwan Cho, MyeongAh Cho, Tae-young Chung, Heansung Lee, Sangyoun Lee
- **Comment**: ICIP 2020. Code: https://github.com/suhwan-cho/CRVOS
- **Journal**: None
- **Summary**: The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios, and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frame's coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsampling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J&F score of 81.6%.



### Hypernetwork approach to generating point clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.00802v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00802v2)
- **Published**: 2020-02-10 11:09:58+00:00
- **Updated**: 2020-10-13 19:18:59+00:00
- **Authors**: Przemysław Spurek, Sebastian Winczowski, Jacek Tabor, Maciej Zamorski, Maciej Zięba, Tomasz Trzciński
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surface. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrization of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows finding mesh-based representation of 3D objects in a generative manner while providing point clouds en pair in quality with the state-of-the-art methods.



### Improving Face Recognition from Hard Samples via Distribution Distillation Loss
- **Arxiv ID**: http://arxiv.org/abs/2002.03662v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03662v3)
- **Published**: 2020-02-10 11:25:22+00:00
- **Updated**: 2020-07-18 15:00:35+00:00
- **Authors**: Yuge Huang, Pengcheng Shen, Ying Tai, Shaoxin Li, Xiaoming Liu, Jilin Li, Feiyue Huang, Rongrong Ji
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Large facial variations are the main challenge in face recognition. To this end, previous variation-specific methods make full use of task-related prior to design special network losses, which are typically not general among different tasks and scenarios. In contrast, the existing generic methods focus on improving the feature discriminability to minimize the intra-class distance while maximizing the interclass distance, which perform well on easy samples but fail on hard samples. To improve the performance on those hard samples for general tasks, we propose a novel Distribution Distillation Loss to narrow the performance gap between easy and hard samples, which is a simple, effective and generic for various types of facial variations. Specifically, we first adopt state-of-the-art classifiers such as ArcFace to construct two similarity distributions: teacher distribution from easy samples and student distribution from hard samples. Then, we propose a novel distribution-driven loss to constrain the student distribution to approximate the teacher distribution, which thus leads to smaller overlap between the positive and negative pairs in the student distribution. We have conducted extensive experiments on both generic large-scale face benchmarks and benchmarks with diverse variations on race, resolution and pose. The quantitative results demonstrate the superiority of our method over strong baselines, e.g., Arcface and Cosface.



### Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via Probabilistic Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.03663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03663v1)
- **Published**: 2020-02-10 11:27:52+00:00
- **Updated**: 2020-02-10 11:27:52+00:00
- **Authors**: Max Mehltretter
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the need to identify erroneous disparity assignments, various approaches for uncertainty and confidence estimation of dense stereo matching have been presented in recent years. As in many other fields, especially deep learning based methods have shown convincing results. However, most of these methods only model the uncertainty contained in the data, while ignoring the uncertainty of the employed dense stereo matching procedure. Additionally modelling the latter, however, is particularly beneficial if the domain of the training data varies from that of the data to be processed. For this purpose, in the present work the idea of probabilistic deep learning is applied to the task of dense stereo matching for the first time. Based on the well-known and commonly employed GC-Net architecture, a novel probabilistic neural network is presented, for the task of joint depth and uncertainty estimation from epipolar rectified stereo image pairs. Instead of learning the network parameters directly, the proposed probabilistic neural network learns a probability distribution from which parameters are sampled for every prediction. The variations between multiple such predictions on the same image pair allow to approximate the model uncertainty. The quality of the estimated depth and uncertainty information is assessed in an extensive evaluation on three different datasets.



### Thermal to Visible Face Recognition Using Deep Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2002.04219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04219v1)
- **Published**: 2020-02-10 11:58:36+00:00
- **Updated**: 2020-02-10 11:58:36+00:00
- **Authors**: Alperen Kantarcı, Hazım Kemal Ekenel
- **Comment**: 5 pages, 3 figures, 2019 International Conference of the Biometrics
  Special Interest Group (BIOSIG)
- **Journal**: None
- **Summary**: Visible face recognition systems achieve nearly perfect recognition accuracies using deep learning. However, in lack of light, these systems perform poorly. A way to deal with this problem is thermal to visible cross-domain face matching. This is a desired technology because of its usefulness in night time surveillance. Nevertheless, due to differences between two domains, it is a very challenging face recognition problem. In this paper, we present a deep autoencoder based system to learn the mapping between visible and thermal face images. Also, we assess the impact of alignment in thermal to visible face recognition. For this purpose, we manually annotate the facial landmarks on the Carl and EURECOM datasets. The proposed approach is extensively tested on three publicly available datasets: Carl, UND-X1, and EURECOM. Experimental results show that the proposed approach improves the state-of-the-art significantly. We observe that alignment increases the performance by around 2%. Annotated facial landmark positions in this study can be downloaded from the following link: github.com/Alpkant/Thermal-to-Visible-Face-Recognition-Using-Deep-Autoencoders .



### Deep Multi-task Multi-label CNN for Effective Facial Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.03683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03683v1)
- **Published**: 2020-02-10 12:34:16+00:00
- **Updated**: 2020-02-10 12:34:16+00:00
- **Authors**: Longbiao Mao, Yan Yan, Jing-Hao Xue, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Attribute Classification (FAC) has attracted increasing attention in computer vision and pattern recognition. However, state-of-the-art FAC methods perform face detection/alignment and FAC independently. The inherent dependencies between these tasks are not fully exploited. In addition, most methods predict all facial attributes using the same CNN network architecture, which ignores the different learning complexities of facial attributes. To address the above problems, we propose a novel deep multi-task multi-label CNN, termed DMM-CNN, for effective FAC. Specifically, DMM-CNN jointly optimizes two closely-related tasks (i.e., facial landmark detection and FAC) to improve the performance of FAC by taking advantage of multi-task learning. To deal with the diverse learning complexities of facial attributes, we divide the attributes into two groups: objective attributes and subjective attributes. Two different network architectures are respectively designed to extract features for two groups of attributes, and a novel dynamic weighting scheme is proposed to automatically assign the loss weight to each facial attribute during training. Furthermore, an adaptive thresholding strategy is developed to effectively alleviate the problem of class imbalance for multi-label learning. Experimental results on the challenging CelebA and LFWA datasets show the superiority of the proposed DMM-CNN method compared with several state-of-the-art FAC methods.



### Knowledge Distillation for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.03688v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03688v1)
- **Published**: 2020-02-10 12:44:07+00:00
- **Updated**: 2020-02-10 12:44:07+00:00
- **Authors**: Dmitrii Lachinov, Elena Shipunova, Vadim Turlapov
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of brain tumors in multimodal MRIs is one of the most challenging tasks in medical image analysis. The recent state of the art algorithms solving this task is based on machine learning approaches and deep learning in particular. The amount of data used for training such models and its variability is a keystone for building an algorithm with high representation power. In this paper, we study the relationship between the performance of the model and the amount of data employed during the training process. On the example of brain tumor segmentation challenge, we compare the model trained with labeled data provided by challenge organizers, and the same model trained in omni-supervised manner using additional unlabeled data annotated with the ensemble of heterogeneous models. As a result, a single model trained with additional data achieves performance close to the ensemble of multiple models and outperforms individual methods.



### Distributed Bayesian Matrix Decomposition for Big Data Mining and Clustering
- **Arxiv ID**: http://arxiv.org/abs/2002.03703v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, I.5.3; I.2.11; I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2002.03703v1)
- **Published**: 2020-02-10 13:10:53+00:00
- **Updated**: 2020-02-10 13:10:53+00:00
- **Authors**: Chihao Zhang, Yang Yang, Wei Zhang, Shihua Zhang
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Matrix decomposition is one of the fundamental tools to discover knowledge from big data generated by modern applications. However, it is still inefficient or infeasible to process very big data using such a method in a single machine. Moreover, big data are often distributedly collected and stored on different machines. Thus, such data generally bear strong heterogeneous noise. It is essential and useful to develop distributed matrix decomposition for big data analytics. Such a method should scale up well, model the heterogeneous noise, and address the communication issue in a distributed system. To this end, we propose a distributed Bayesian matrix decomposition model (DBMD) for big data mining and clustering. Specifically, we adopt three strategies to implement the distributed computing including 1) the accelerated gradient descent, 2) the alternating direction method of multipliers (ADMM), and 3) the statistical inference. We investigate the theoretical convergence behaviors of these algorithms. To address the heterogeneity of the noise, we propose an optimal plug-in weighted average that reduces the variance of the estimation. Synthetic experiments validate our theoretical results, and real-world experiments show that our algorithms scale up well to big data and achieves superior or competing performance compared to other distributed methods.



### Learning End-to-End Lossy Image Compression: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2002.03711v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03711v4)
- **Published**: 2020-02-10 13:13:43+00:00
- **Updated**: 2021-03-26 02:23:55+00:00
- **Authors**: Yueyu Hu, Wenhan Yang, Zhan Ma, Jiaying Liu
- **Comment**: Accepted for publication in IEEE Transactions on Pattern Analysis and
  Machine Intelligence. Website available at
  https://huzi96.github.io/compression-bench.html
- **Journal**: None
- **Summary**: Image compression is one of the most fundamental techniques and commonly used applications in the image and video processing field. Earlier methods built a well-designed pipeline, and efforts were made to improve all modules of the pipeline by handcrafted tuning. Later, tremendous contributions were made, especially when data-driven methods revitalized the domain with their excellent modeling capacities and flexibility in incorporating newly designed modules and constraints. Despite great progress, a systematic benchmark and comprehensive analysis of end-to-end learned image compression methods are lacking. In this paper, we first conduct a comprehensive literature survey of learned image compression methods. The literature is organized based on several aspects to jointly optimize the rate-distortion performance with a neural network, i.e., network architecture, entropy model and rate control. We describe milestones in cutting-edge learned image-compression methods, review a broad range of existing works, and provide insights into their historical development routes. With this survey, the main challenges of image compression methods are revealed, along with opportunities to address the related issues with recent advanced learning methods. This analysis provides an opportunity to take a further step towards higher-efficiency image compression. By introducing a coarse-to-fine hyperprior model for entropy estimation and signal reconstruction, we achieve improved rate-distortion performance, especially on high-resolution images. Extensive benchmark experiments demonstrate the superiority of our model in rate-distortion performance and time complexity on multi-core CPUs and GPUs. Our project website is available at https://huzi96.github.io/compression-bench.html.



### Iterative energy-based projection on a normal data manifold for anomaly localization
- **Arxiv ID**: http://arxiv.org/abs/2002.03734v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03734v1)
- **Published**: 2020-02-10 13:35:41+00:00
- **Updated**: 2020-02-10 13:35:41+00:00
- **Authors**: David Dehaene, Oriel Frigo, Sébastien Combrexelle, Pierre Eline
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder's loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-defined optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset.



### Unsupervised Discovery of Interpretable Directions in the GAN Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2002.03754v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03754v3)
- **Published**: 2020-02-10 13:57:14+00:00
- **Updated**: 2020-06-24 12:12:14+00:00
- **Authors**: Andrey Voynov, Artem Babenko
- **Comment**: None
- **Journal**: None
- **Summary**: The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection.



### Investigating the Importance of Shape Features, Color Constancy, Color Spaces and Similarity Measures in Open-Ended 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03779v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03779v2)
- **Published**: 2020-02-10 14:24:09+00:00
- **Updated**: 2020-09-26 12:18:24+00:00
- **Authors**: S. Hamidreza Kasaei, Maryam Ghorbani, Jits Schilperoort, Wessel van der Rest
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent success of state-of-the-art 3D object recognition approaches, service robots are frequently failed to recognize many objects in real human-centric environments. For these robots, object recognition is a challenging task due to the high demand for accurate and real-time response under changing and unpredictable environmental conditions. Most of the recent approaches use either the shape information only and ignore the role of color information or vice versa. Furthermore, they mainly utilize the $L_n$ Minkowski family functions to measure the similarity of two object views, while there are various distance measures that are applicable to compare two object views. In this paper, we explore the importance of shape information, color constancy, color spaces, and various similarity measures in open-ended 3D object recognition. Towards this goal, we extensively evaluate the performance of object recognition approaches in three different configurations, including \textit{color-only}, \textit{shape-only}, and \textit{ combinations of color and shape}, in both offline and online settings. Experimental results concerning scalability, memory usage, and object recognition performance show that all of the \textit{combinations of color and shape} yields significant improvements over the \textit{shape-only} and \textit{color-only} approaches. The underlying reason is that color information is an important feature to distinguish objects that have very similar geometric properties with different colors and vice versa. Moreover, by combining color and shape information, we demonstrate that the robot can learn new object categories from very few training examples in a real-world setting.



### Vision based body gesture meta features for Affective Computing
- **Arxiv ID**: http://arxiv.org/abs/2003.00809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2003.00809v1)
- **Published**: 2020-02-10 14:38:16+00:00
- **Updated**: 2020-02-10 14:38:16+00:00
- **Authors**: Indigo J. D. Orton
- **Comment**: MPhil thesis; 74 pages
- **Journal**: None
- **Summary**: Early detection of psychological distress is key to effective treatment. Automatic detection of distress, such as depression, is an active area of research. Current approaches utilise vocal, facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially due to the difficulty in extracting bodily representations from videos, and partially due to the lack of viable datasets. Existing body modality approaches use automatic categorization of expressions to represent body language as a series of specific expressions, much like words within natural language. In this dissertation I present a new type of feature, within the body modality, that represents meta information of gestures, such as speed, and use it to predict a non-clinical depression label. This differs to existing work by representing overall behaviour as a small set of aggregated meta features derived from a person's movement. In my method I extract pose estimation from videos, detect gestures within body parts, extract meta information from individual gestures, and finally aggregate these features to generate a small feature vector for use in prediction tasks. I introduce a new dataset of 65 video recordings of interviews with self-evaluated distress, personality, and demographic labels. This dataset enables the development of features utilising the whole body in distress detection tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived stress, somatic stress, five standard personality measures, and gender. A linear regression based classifier using these features achieves a 82.70% F1 score for predicting depression within my novel dataset.



### Real-Time target detection in maritime scenarios based on YOLOv3 model
- **Arxiv ID**: http://arxiv.org/abs/2003.00800v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.00800v1)
- **Published**: 2020-02-10 15:25:19+00:00
- **Updated**: 2020-02-10 15:25:19+00:00
- **Authors**: Alessandro Betti, Benedetto Michelozzi, Andrea Bracci, Andrea Masini
- **Comment**: Paper presented at the 9th International Symposium on Optronics in
  Defence & Security, 28-30 January 2020 (OPTRO2020, Paris). Oral Presentation
- **Journal**: None
- **Summary**: In this work a novel ships dataset is proposed consisting of more than 56k images of marine vessels collected by means of web-scraping and including 12 ship categories. A YOLOv3 single-stage detector based on Keras API is built on top of this dataset. Current results on four categories (cargo ship, naval ship, oil ship and tug ship) show Average Precision up to 96% for Intersection over Union (IoU) of 0.5 and satisfactory detection performances up to IoU of 0.8. A Data Analytics GUI service based on QT framework and Darknet-53 engine is also implemented in order to simplify the deployment process and analyse massive amount of images even for people without Data Science expertise.



### 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss
- **Arxiv ID**: http://arxiv.org/abs/2002.03923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03923v2)
- **Published**: 2020-02-10 16:33:33+00:00
- **Updated**: 2020-05-04 22:24:55+00:00
- **Authors**: Xin Yu, Zheyu Zhuang, Piotr Koniusz, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting loss (DPVL) which mimics the hypothesis selection in the voting procedure. By exploiting our voting loss, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, i.e., LINEMOD and Occlusion LINEMOD, manifest that our DPVL improves pose estimation performance significantly and speeds up the training convergence.



### RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.03933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03933v1)
- **Published**: 2020-02-10 16:44:45+00:00
- **Updated**: 2020-02-10 16:44:45+00:00
- **Authors**: Hossam Isack, Christian Haene, Cem Keskin, Sofien Bouaziz, Yuri Boykov, Shahram Izadi, Sameh Khamis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel efficient and lightweight model for human pose estimation from a single image. Our model is designed to achieve competitive results at a fraction of the number of parameters and computational cost of various state-of-the-art methods. To this end, we explicitly incorporate part-based structural and geometric priors in a hierarchical prediction framework. At the coarsest resolution, and in a manner similar to classical part-based approaches, we leverage the kinematic structure of the human body to propagate convolutional feature updates between the keypoints or body parts. Unlike classical approaches, we adopt end-to-end training to learn this geometric prior through feature updates from data. We then propagate the feature representation at the coarsest resolution up the hierarchy to refine the predicted pose in a coarse-to-fine fashion. The final network effectively models the geometric prior and intuition within a lightweight deep neural network, yielding state-of-the-art results for a model of this size on two standard datasets, Leeds Sports Pose and MPII Human Pose.



### Self-Supervised Joint Encoding of Motion and Appearance for First Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03982v2)
- **Published**: 2020-02-10 17:51:13+00:00
- **Updated**: 2020-12-07 18:50:33+00:00
- **Authors**: Mirco Planamente, Andrea Bottino, Barbara Caputo
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from the first-person point of view. An open challenge in egocentric action recognition is that videos lack detailed information about the main actor's pose and thus tend to record only parts of the movement when focusing on manipulation tasks. Thus, the amount of information about the action itself is limited, making crucial the understanding of the manipulated objects and their context. Many previous works addressed this issue with two-stream architectures, where one stream is dedicated to modeling the appearance of objects involved in the action, and another to extracting motion features from optical flow. In this paper, we argue that learning features jointly from these two information channels is beneficial to capture the spatio-temporal correlations between the two better. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach.



### StickyPillars: Robust and Efficient Feature Matching on Point Clouds using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.03983v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03983v3)
- **Published**: 2020-02-10 17:53:41+00:00
- **Updated**: 2021-02-19 09:18:05+00:00
- **Authors**: Kai Fischer, Martin Simon, Florian Oelsner, Stefan Milz, Horst-Michael Gross, Patrick Maeder
- **Comment**: None
- **Journal**: None
- **Summary**: Robust point cloud registration in real-time is an important prerequisite for many mapping and localization algorithms. Traditional methods like ICP tend to fail without good initialization, insufficient overlap or in the presence of dynamic objects. Modern deep learning based registration approaches present much better results, but suffer from a heavy run-time. We overcome these drawbacks by introducing StickyPillars, a fast, accurate and extremely robust deep middle-end 3D feature matching method on point clouds. It uses graph neural networks and performs context aggregation on sparse 3D key-points with the aid of transformer based multi-head self and cross-attention. The network output is used as the cost for an optimal transport problem whose solution yields the final matching probabilities. The system does not rely on hand crafted feature descriptors or heuristic matching strategies. We present state-of-art art accuracy results on the registration problem demonstrated on the KITTI dataset while being four times faster then leading deep methods. Furthermore, we integrate our matching system into a LiDAR odometry pipeline yielding most accurate results on the KITTI odometry dataset. Finally, we demonstrate robustness on KITTI odometry. Our method remains stable in accuracy where state-of-the-art procedures fail on frame drops and higher speeds.



### Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization
- **Arxiv ID**: http://arxiv.org/abs/2002.03985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03985v1)
- **Published**: 2020-02-10 17:55:55+00:00
- **Updated**: 2020-02-10 17:55:55+00:00
- **Authors**: Luiz A. Zanlorensi, Hugo Proença, David Menotti
- **Comment**: None
- **Journal**: None
- **Summary**: Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the "eyeglasses" and "gaze" factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of "wearing eyeglasses" in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance.



### Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.03989v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03989v1)
- **Published**: 2020-02-10 18:03:44+00:00
- **Updated**: 2020-02-10 18:03:44+00:00
- **Authors**: Jun Liu, Xiangyue Wang, Xue-cheng Tai
- **Comment**: None
- **Journal**: None
- **Summary**: We use Deep Convolutional Neural Networks (DCNNs) for image segmentation problems. DCNNs can well extract the features from natural images. However, the classification functions in the existing network architecture of CNNs are simple and lack capabilities to handle important spatial information in a way that have been done for many well-known traditional variational models. Prior such as spatial regularity, volume prior and object shapes cannot be well handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD) framework which can easily integrate many spatial priors of the classical variational models into the DCNNs for image segmentation. The novelty of our method is to interpret the softmax activation function as a dual variable in a variational problem, and thus many spatial priors can be imposed in the dual space. From this viewpoint, we can build a STD based framework which can enable the outputs of DCNNs to have many special priors such as spatial regularity, volume constraints and star-shape priori. The proposed method is a general mathematical framework and it can be applied to any semantic segmentation DCNNs. To show the efficiency and accuracy of our method, we applied it to the popular DeepLabV3+ image segmentation network, and the experiments results show that our method can work efficiently on data-driven image segmentation DCNNs.



### Upper, Middle and Lower Region Learning for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.04023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.04023v2)
- **Published**: 2020-02-10 18:51:45+00:00
- **Updated**: 2020-02-11 12:58:35+00:00
- **Authors**: Yao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Facial action units (AUs) detection is fundamental to facial expression analysis. As AU occurs only in a small area of the face, region-based learning has been widely recognized useful for AU detection. Most region-based studies focus on a small region where the AU occurs. Focusing on a specific region helps eliminate the influence of identity, but bringing a risk for losing information. It is challenging to find balance. In this study, I propose a simple strategy. I divide the face into three broad regions, upper, middle, and lower region, and group AUs based on where it occurs. I propose a new end-to-end deep learning framework named three regions based attention network (TRA-Net). After extracting the global feature, TRA-Net uses a hard attention module to extract three feature maps, each of which contains only a specific region. Each region-specific feature map is fed to an independent branch. For each branch, three continuous soft attention modules are used to extract higher-level features for final AU detection. In the DISFA dataset, this model achieves the highest F1 scores for the detection of AU1, AU2, and AU4, and produces the highest accuracy in comparison with the state-of-the-art methods.



### Advances in Deep Space Exploration via Simulators & Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.04051v2
- **DOI**: 10.1016/j.newast.2020.101517
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04051v2)
- **Published**: 2020-02-10 19:07:54+00:00
- **Updated**: 2020-06-06 19:21:36+00:00
- **Authors**: James Bird, Linda Petzold, Philip Lubin, Julia Deacon
- **Comment**: 16 pages, 7 figures, 2 tables, currently under review
- **Journal**: None
- **Summary**: The StarLight program conceptualizes fast interstellar travel via small wafer satellites (wafersats) that are propelled by directed energy. This process is wildly different from traditional space travel and trades large and slow spacecraft for small, fast, inexpensive, and fragile ones. The main goal of these wafer satellites is to gather useful images during their deep space journey. We introduce and solve some of the main problems that accompany this concept. First, we need an object detection system that can detect planets that we have never seen before, some containing features that we may not even know exist in the universe. Second, once we have images of exoplanets, we need a way to take these images and rank them by importance. Equipment fails and data rates are slow, thus we need a method to ensure that the most important images to humankind are the ones that are prioritized for data transfer. Finally, the energy on board is minimal and must be conserved and used sparingly. No exoplanet images should be missed, but using energy erroneously would be detrimental. We introduce simulator-based methods that leverage artificial intelligence, mostly in the form of computer vision, in order to solve all three of these issues. Our results confirm that simulators provide an extremely rich training environment that surpasses that of real images, and can be used to train models on features that have yet to be observed by humans. We also show that the immersive and adaptable environment provided by the simulator, combined with deep learning, lets us navigate and save energy in an otherwise implausible way.



### Self-Supervised Linear Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2002.04070v1
- **DOI**: 10.1109/LRA.2020.2972873
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04070v1)
- **Published**: 2020-02-10 20:15:21+00:00
- **Updated**: 2020-02-10 20:15:21+00:00
- **Authors**: Peidong Liu, Joel Janai, Marc Pollefeys, Torsten Sattler, Andreas Geiger
- **Comment**: Accepted by Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: Motion blurry images challenge many computer vision algorithms, e.g, feature detection, motion estimation, or object recognition. Deep convolutional neural networks are state-of-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this paper, we present a differentiable reblur model for self-supervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised single image deblurring is really feasible and leads to visually compelling results.



### Preventing Clean Label Poisoning using Gaussian Mixture Loss
- **Arxiv ID**: http://arxiv.org/abs/2003.00798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00798v1)
- **Published**: 2020-02-10 20:51:59+00:00
- **Updated**: 2020-02-10 20:51:59+00:00
- **Authors**: Muhammad Yaseen, Muneeb Aadil, Maria Sargsyan
- **Comment**: Preliminary v1
- **Journal**: None
- **Summary**: Since 2014 when Szegedy et al. showed that carefully designed perturbations of the input can lead Deep Neural Networks (DNNs) to wrongly classify its label, there has been an ongoing research to make DNNs more robust to such malicious perturbations. In this work, we consider a poisoning attack called Clean Labeling poisoning attack (CLPA). The goal of CLPA is to inject seemingly benign instances which can drastically change decision boundary of the DNNs due to which subsequent queries at test time can be mis-classified. We argue that a strong defense against CLPA can be embedded into the model during the training by imposing features of the network to follow a Large Margin Gaussian Mixture distribution in the penultimate layer. By having such a prior knowledge, we can systematically evaluate how unusual the example is, given the label it is claiming to be. We demonstrate our builtin defense via experiments on MNIST and CIFAR datasets. We train two models on each dataset: one trained via softmax, another via LGM. We show that using LGM can substantially reduce the effectiveness of CLPA while having no additional overhead of data sanitization. The code to reproduce our results is available online.



### Outlier Guided Optimization of Abdominal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.04098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04098v1)
- **Published**: 2020-02-10 21:41:52+00:00
- **Updated**: 2020-02-10 21:41:52+00:00
- **Authors**: Yuchen Xu, Olivia Tang, Yucheng Tang, Ho Hin Lee, Yunqiang Chen, Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G. Abramson, Yuankai Huo, Bennett A. Landman
- **Comment**: SPIE2020 Medical Imaging
- **Journal**: None
- **Summary**: Abdominal multi-organ segmentation of computed tomography (CT) images has been the subject of extensive research interest. It presents a substantial challenge in medical image processing, as the shape and distribution of abdominal organs can vary greatly among the population and within an individual over time. While continuous integration of novel datasets into the training set provides potential for better segmentation performance, collection of data at scale is not only costly, but also impractical in some contexts. Moreover, it remains unclear what marginal value additional data have to offer. Herein, we propose a single-pass active learning method through human quality assurance (QA). We built on a pre-trained 3D U-Net model for abdominal multi-organ segmentation and augmented the dataset either with outlier data (e.g., exemplars for which the baseline algorithm failed) or inliers (e.g., exemplars for which the baseline algorithm worked). The new models were trained using the augmented datasets with 5-fold cross-validation (for outlier data) and withheld outlier samples (for inlier data). Manual labeling of outliers increased Dice scores with outliers by 0.130, compared to an increase of 0.067 with inliers (p<0.001, two-tailed paired t-test). By adding 5 to 37 inliers or outliers to training, we find that the marginal value of adding outliers is higher than that of adding inliers. In summary, improvement on single-organ performance was obtained without diminishing multi-organ performance or significantly increasing training time. Hence, identification and correction of baseline failure cases present an effective and efficient method of selecting training data to improve algorithm performance.



### Validation and Optimization of Multi-Organ Segmentation on Clinical Imaging Archives
- **Arxiv ID**: http://arxiv.org/abs/2002.04102v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04102v1)
- **Published**: 2020-02-10 21:49:42+00:00
- **Updated**: 2020-02-10 21:49:42+00:00
- **Authors**: Yuchen Xu, Olivia Tang, Yucheng Tang, Ho Hin Lee, Yunqiang Chen, Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G. Abramson, Yuankai Huo, Bennett A. Landman
- **Comment**: SPIE2020 Medical Imaging
- **Journal**: None
- **Summary**: Segmentation of abdominal computed tomography(CT) provides spatial context, morphological properties, and a framework for tissue-specific radiomics to guide quantitative Radiological assessment. A 2015 MICCAI challenge spurred substantial innovation in multi-organ abdominal CT segmentation with both traditional and deep learning methods. Recent innovations in deep methods have driven performance toward levels for which clinical translation is appealing. However, continued cross-validation on open datasets presents the risk of indirect knowledge contamination and could result in circular reasoning. Moreover, 'real world' segmentations can be challenging due to the wide variability of abdomen physiology within patients. Herein, we perform two data retrievals to capture clinically acquired deidentified abdominal CT cohorts with respect to a recently published variation on 3D U-Net (baseline algorithm). First, we retrieved 2004 deidentified studies on 476 patients with diagnosis codes involving spleen abnormalities (cohort A). Second, we retrieved 4313 deidentified studies on 1754 patients without diagnosis codes involving spleen abnormalities (cohort B). We perform prospective evaluation of the existing algorithm on both cohorts, yielding 13% and 8% failure rate, respectively. Then, we identified 51 subjects in cohort A with segmentation failures and manually corrected the liver and gallbladder labels. We re-trained the model adding the manual labels, resulting in performance improvement of 9% and 6% failure rate for the A and B cohorts, respectively. In summary, the performance of the baseline on the prospective cohorts was similar to that on previously published datasets. Moreover, adding data from the first cohort substantively improved performance when evaluated on the second withheld validation cohort.



### Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2002.04114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04114v2)
- **Published**: 2020-02-10 22:15:19+00:00
- **Updated**: 2020-02-18 00:03:01+00:00
- **Authors**: Guan-An Wang, Tianzhu Zhang. Yang Yang, Jian Cheng, Jianlong Chang, Xu Liang, Zengguang Hou
- **Comment**: accepted by AAAI'20
- **Journal**: None
- **Summary**: RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. The key solution is to learn aligned features to the bridge RGB and IR modalities. However, due to the lack of correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing the distance between the entire RGB and IR sets. However, this set-level alignment may lead to misalignment of some instances, which limits the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged images. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Extensive experimental results on two standard benchmarks demonstrate that the proposed model favourably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve a gain of 9.2% and 7.7% in terms of Rank-1 and mAP. Code is available at https://github.com/wangguanan/JSIA-ReID.



