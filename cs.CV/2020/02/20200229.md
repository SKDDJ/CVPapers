# Arxiv Papers in cs.CV on 2020-02-29
### Image Hashing by Minimizing Discrete Component-wise Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/2003.00134v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00134v3)
- **Published**: 2020-02-29 00:22:53+00:00
- **Updated**: 2020-05-26 01:33:29+00:00
- **Authors**: Khoa D. Doan, Saurav Manchanda, Sarkhan Badirli, Chandan K. Reddy
- **Comment**: None
- **Journal**: None
- **Summary**: Image hashing is one of the fundamental problems that demand both efficient and effective solutions for various practical scenarios. Adversarial autoencoders are shown to be able to implicitly learn a robust, locality-preserving hash function that generates balanced and high-quality hash codes. However, the existing adversarial hashing methods are inefficient to be employed for large-scale image retrieval applications. Specifically, they require an exponential number of samples to be able to generate optimal hash codes and a significantly high computational cost to train. In this paper, we show that the high sample-complexity requirement often results in sub-optimal retrieval performance of the adversarial hashing methods. To address this challenge, we propose a new adversarial-autoencoder hashing approach that has a much lower sample requirement and computational cost. Specifically, by exploiting the desired properties of the hash function in the low-dimensional, discrete space, our method efficiently estimates a better variant of Wasserstein distance by averaging a set of easy-to-compute one-dimensional Wasserstein distances. The resulting hashing approach has an order-of-magnitude better sample complexity, thus better generalization property, compared to the other adversarial hashing methods. In addition, the computational cost is significantly reduced using our approach. We conduct experiments on several real-world datasets and show that the proposed method outperforms the competing hashing methods, achieving up to 10% improvement over the current state-of-the-art image hashing methods. The code accompanying this paper is available on Github (https://github.com/khoadoan/adversarial-hashing).



### Do CNNs Encode Data Augmentations?
- **Arxiv ID**: http://arxiv.org/abs/2003.08773v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2003.08773v3)
- **Published**: 2020-02-29 00:42:23+00:00
- **Updated**: 2021-10-27 23:58:06+00:00
- **Authors**: Eddie Yan, Yanping Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentations are important ingredients in the recipe for training robust neural networks, especially in computer vision. A fundamental question is whether neural network features encode data augmentation transformations. To answer this question, we introduce a systematic approach to investigate which layers of neural networks are the most predictive of augmentation transformations. Our approach uses features in pre-trained vision models with minimal additional processing to predict common properties transformed by augmentation (scale, aspect ratio, hue, saturation, contrast, and brightness). Surprisingly, neural network features not only predict data augmentation transformations, but they predict many transformations with high accuracy. After validating that neural networks encode features corresponding to augmentation transformations, we show that these features are encoded in the early layers of modern CNNs, though the augmentation signal fades in deeper layers.



### Towards Using Count-level Weak Supervision for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2003.00164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00164v1)
- **Published**: 2020-02-29 02:58:36+00:00
- **Updated**: 2020-02-29 02:58:36+00:00
- **Authors**: Yinjie Lei, Yan Liu, Pingping Zhang, Lingqiao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing crowd counting methods require object location-level annotation, i.e., placing a dot at the center of an object. While being simpler than the bounding-box or pixel-level annotation, obtaining this annotation is still labor-intensive and time-consuming especially for images with highly crowded scenes. On the other hand, weaker annotations that only know the total count of objects can be almost effortless in many practical scenarios. Thus, it is desirable to develop a learning method that can effectively train models from count-level annotations. To this end, this paper studies the problem of weakly-supervised crowd counting which learns a model from only a small amount of location-level annotations (fully-supervised) but a large amount of count-level annotations (weakly-supervised). To perform effective training in this scenario, we observe that the direct solution of regressing the integral of density map to the object count is not sufficient and it is beneficial to introduce stronger regularizations on the predicted density map of weakly-annotated images. We devise a simple-yet-effective training strategy, namely Multiple Auxiliary Tasks Training (MATT), to construct regularizes for restricting the freedom of the generated density maps. Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.



### Two-Level Attention-based Fusion Learning for RGB-D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.00168v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00168v3)
- **Published**: 2020-02-29 03:18:52+00:00
- **Updated**: 2020-10-18 10:20:01+00:00
- **Authors**: Hardik Uppal, Alireza Sepas-Moghaddam, Michael Greenspan, Ali Etemad
- **Comment**: 8 Pages, 4 figure, Accepted to International Conference on Pattern
  Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: With recent advances in RGB-D sensing technologies as well as improvements in machine learning and fusion techniques, RGB-D facial recognition has become an active area of research. A novel attention aware method is proposed to fuse two image modalities, RGB and depth, for enhanced RGB-D facial recognition. The proposed method first extracts features from both modalities using a convolutional feature extractor. These features are then fused using a two-layer attention mechanism. The first layer focuses on the fused feature maps generated by the feature extractor, exploiting the relationship between feature maps using LSTM recurrent learning. The second layer focuses on the spatial features of those maps using convolution. The training database is preprocessed and augmented through a set of geometric transformations, and the learning process is further aided using transfer learning from a pure 2D RGB image training process. Comparative evaluations demonstrate that the proposed method outperforms other state-of-the-art approaches, including both traditional and deep neural network-based methods, on the challenging CurtinFaces and IIIT-D RGB-D benchmark databases, achieving classification accuracies over 98.2% and 99.3% respectively. The proposed attention mechanism is also compared with other attention mechanisms, demonstrating more accurate results.



### HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00186v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.00186v2)
- **Published**: 2020-02-29 05:46:19+00:00
- **Updated**: 2020-03-16 15:51:05+00:00
- **Authors**: Maosheng Ye, Shuangjie Xu, Tongyi Cao
- **Comment**: accepted to CVPR 2020
- **Journal**: None
- **Summary**: We present Hybrid Voxel Network (HVNet), a novel one-stage unified network for point cloud based 3D object detection for autonomous driving. Recent studies show that 2D voxelization with per voxel PointNet style feature extractor leads to accurate and efficient detector for large 3D scenes. Since the size of the feature map determines the computation and memory cost, the size of the voxel becomes a parameter that is hard to balance. A smaller voxel size gives a better performance, especially for small objects, but a longer inference time. A larger voxel can cover the same area with a smaller feature map, but fails to capture intricate features and accurate location for smaller objects. We present a Hybrid Voxel network that solves this problem by fusing voxel feature encoder (VFE) of different scales at point-wise level and project into multiple pseudo-image feature maps. We further propose an attentive voxel feature encoding that outperforms plain VFE and a feature fusion pyramid network to aggregate multi-scale information at feature map level. Experiments on the KITTI benchmark show that a single HVNet achieves the best mAP among all existing methods with a real time inference speed of 31Hz.



### Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.00187v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00187v2)
- **Published**: 2020-02-29 06:20:20+00:00
- **Updated**: 2020-10-12 16:07:23+00:00
- **Authors**: Takehiko Ohkawa, Naoto Inoue, Hirokatsu Kataoka, Nakamasa Inoue
- **Comment**: Accepted to ICPR2020
- **Journal**: None
- **Summary**: Unpaired image-to-image (I2I) translation has received considerable attention in pattern recognition and computer vision because of recent advancements in generative adversarial networks (GANs). However, due to the lack of explicit supervision, unpaired I2I models often fail to generate realistic images, especially in challenging datasets with different backgrounds and poses. Hence, stabilization is indispensable for GANs and applications of I2I translation. Herein, we propose Augmented Cyclic Consistency Regularization (ACCR), a novel regularization method for unpaired I2I translation. Our main idea is to enforce consistency regularization originating from semi-supervised learning on the discriminators leveraging real, fake, reconstructed, and augmented samples. We regularize the discriminators to output similar predictions when fed pairs of original and perturbed images. We qualitatively clarify why consistency regularization on fake and reconstructed samples works well. Quantitatively, our method outperforms the consistency regularized GAN (CR-GAN) in real-world translations and demonstrates efficacy against several data augmentation variants and cycle-consistent constraints.



### Robust 6D Object Pose Estimation by Learning RGB-D Features
- **Arxiv ID**: http://arxiv.org/abs/2003.00188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00188v2)
- **Published**: 2020-02-29 06:24:55+00:00
- **Updated**: 2020-03-09 14:25:38+00:00
- **Authors**: Meng Tian, Liang Pan, Marcelo H Ang Jr, Gim Hee Lee
- **Comment**: Accepted at ICRA 2020
- **Journal**: None
- **Summary**: Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete-continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/mentian/object-posenet.



### First Order Motion Model for Image Animation
- **Arxiv ID**: http://arxiv.org/abs/2003.00196v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00196v3)
- **Published**: 2020-02-29 07:08:56+00:00
- **Updated**: 2020-10-01 15:26:15+00:00
- **Authors**: Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available.



### VideoSSL: Semi-Supervised Learning for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.00197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00197v1)
- **Published**: 2020-02-29 07:13:12+00:00
- **Updated**: 2020-02-29 07:13:12+00:00
- **Authors**: Longlong Jing, Toufiq Parag, Zhe Wu, Yingli Tian, Hongcheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a semi-supervised learning approach for video classification, VideoSSL, using convolutional neural networks (CNN). Like other computer vision tasks, existing supervised video classification methods demand a large amount of labeled data to attain good performance. However, annotation of a large dataset is expensive and time consuming. To minimize the dependence on a large annotated dataset, our proposed semi-supervised method trains from a small number of labeled examples and exploits two regulatory signals from unlabeled data. The first signal is the pseudo-labels of unlabeled examples computed from the confidences of the CNN being trained. The other is the normalized probabilities, as predicted by an image classifier CNN, that captures the information about appearances of the interesting objects in the video. We show that, under the supervision of these guiding signals from unlabeled examples, a video classification CNN can achieve impressive performances utilizing a small fraction of annotated examples on three publicly available datasets: UCF101, HMDB51 and Kinetics.



### Learning to Compare Relation: Semantic Alignment for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00210v2)
- **Published**: 2020-02-29 08:37:02+00:00
- **Updated**: 2022-01-07 08:52:31+00:00
- **Authors**: Congqi Cao, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is a fundamental and challenging problem since it requires recognizing novel categories from only a few examples. The objects for recognition have multiple variants and can locate anywhere in images. Directly comparing query images with example images can not handle content misalignment. The representation and metric for comparison are critical but challenging to learn due to the scarcity and wide variation of the samples in few-shot learning. In this paper, we present a novel semantic alignment model to compare relations, which is robust to content misalignment. We propose to add two key ingredients to existing few-shot learning frameworks for better feature and metric learning ability. First, we introduce a semantic alignment loss to align the relation statistics of the features from samples that belong to the same category. And second, local and global mutual information maximization is introduced, allowing for representations that contain locally-consistent and intra-class shared information across structural locations in an image. Thirdly, we introduce a principled approach to weigh multiple loss functions by considering the homoscedastic uncertainty of each stream. We conduct extensive experiments on several few-shot learning datasets. Experimental results show that the proposed method is capable of comparing relations with semantic alignment strategies, and achieves state-of-the-art performance.



### Cross-Spectrum Dual-Subspace Pairing for RGB-infrared Cross-Modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.00213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00213v1)
- **Published**: 2020-02-29 09:01:39+00:00
- **Updated**: 2020-02-29 09:01:39+00:00
- **Authors**: Xing Fan, Hao Luo, Chi Zhang, Wei Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to its potential wide applications in video surveillance and other computer vision tasks like tracking, person re-identification (ReID) has become popular and been widely investigated. However, conventional person re-identification can only handle RGB color images, which will fail at dark conditions. Thus RGB-infrared ReID (also known as Infrared-Visible ReID or Visible-Thermal ReID) is proposed. Apart from appearance discrepancy in traditional ReID caused by illumination, pose variations and viewpoint changes, modality discrepancy produced by cameras of the different spectrum also exists, which makes RGB-infrared ReID more difficult. To address this problem, we focus on extracting the shared cross-spectrum features of different modalities. In this paper, a novel multi-spectrum image generation method is proposed and the generated samples are utilized to help the network to find discriminative information for re-identifying the same person across modalities. Another challenge of RGB-infrared ReID is that the intra-person (images from the same person) discrepancy is often larger than the inter-person (images from different persons) discrepancy, so a dual-subspace pairing strategy is proposed to alleviate this problem. Combining those two parts together, we also design a one-stream neural network combining the aforementioned methods to extract compact representations of person images, called Cross-spectrum Dual-subspace Pairing (CDP) model. Furthermore, during the training process, we also propose a Dynamic Hard Spectrum Mining method to automatically mine more hard samples from hard spectrum based on the current model state to further boost the performance. Extensive experimental results on two public datasets, SYSU-MM01 with RGB + near-infrared images and RegDB with RGB + far-infrared images, have demonstrated the efficiency and generality of our proposed method.



### Channel Equilibrium Networks for Learning Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/2003.00214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00214v1)
- **Published**: 2020-02-29 09:02:31+00:00
- **Updated**: 2020-02-29 09:02:31+00:00
- **Authors**: Wenqi Shao, Shitao Tang, Xingang Pan, Ping Tan, Xiaogang Wang, Ping Luo
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization (BN) and a rectified linear function such as ReLU. However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of CNNs. Unlike prior arts that simply removed the inhibited channels, we propose to "wake them up" during training by designing a novel neural building block, termed Channel Equilibrium (CE) block, which enables channels at the same layer to contribute equally to the learned representation. We show that CE is able to prevent inhibited channels both empirically and theoretically. CE has several appealing benefits. (1) It can be integrated into many advanced CNN architectures such as ResNet and MobileNet, outperforming their original networks. (2) CE has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game. (3) Extensive experiments show that CE achieves state-of-the-art performance on various challenging benchmarks such as ImageNet and COCO.



### NAS-Count: Counting-by-Density with Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2003.00217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00217v2)
- **Published**: 2020-02-29 09:18:17+00:00
- **Updated**: 2020-08-13 03:54:01+00:00
- **Authors**: Yutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jungong Han, Xianbin Cao, David Doermann
- **Comment**: Accepted to European Conference on Computer Vision(ECCV) 2020
- **Journal**: None
- **Summary**: Most of the recent advances in crowd counting have evolved from hand-designed density estimation networks, where multi-scale features are leveraged to address the scale variation problem, but at the expense of demanding design efforts. In this work, we automate the design of counting models with Neural Architecture Search (NAS) and introduce an end-to-end searched encoder-decoder architecture, Automatic Multi-Scale Network (AMSNet). Specifically, we utilize a counting-specific two-level search space. The encoder and decoder in AMSNet are composed of different cells discovered from micro-level search, while the multi-path architecture is explored through macro-level search. To solve the pixel-level isolation issue in MSE loss, AMSNet is optimized with an auto-searched Scale Pyramid Pooling Loss (SPPLoss) that supervises the multi-scale structural information. Extensive experiments on four datasets show AMSNet produces state-of-the-art results that outperform hand-designed models, fully demonstrating the efficacy of NAS-Count.



### Joint Face Completion and Super-resolution using Multi-scale Feature Relation Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00255v2)
- **Published**: 2020-02-29 13:31:46+00:00
- **Updated**: 2020-08-25 14:35:13+00:00
- **Authors**: Zhilei Liu, Yunpeng Wu, Le Li, Cuicui Zhang, Baoyuan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Previous research on face restoration often focused on repairing a specific type of low-quality facial images such as low-resolution (LR) or occluded facial images. However, in the real world, both the above-mentioned forms of image degradation often coexist. Therefore, it is important to design a model that can repair LR occluded images simultaneously. This paper proposes a multi-scale feature graph generative adversarial network (MFG-GAN) to implement the face restoration of images in which both degradation modes coexist, and also to repair images with a single type of degradation. Based on the GAN, the MFG-GAN integrates the graph convolution and feature pyramid network to restore occluded low-resolution face images to non-occluded high-resolution face images. The MFG-GAN uses a set of customized losses to ensure that high-quality images are generated. In addition, we designed the network in an end-to-end format. Experimental results on the public-domain CelebA and Helen databases show that the proposed approach outperforms state-of-the-art methods in performing face super-resolution (up to 4x or 8x) and face completion simultaneously. Cross-database testing also revealed that the proposed approach has good generalizability.



### "Who is Driving around Me?" Unique Vehicle Instance Classification using Deep Neural Features
- **Arxiv ID**: http://arxiv.org/abs/2003.08771v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2003.08771v1)
- **Published**: 2020-02-29 13:57:46+00:00
- **Updated**: 2020-02-29 13:57:46+00:00
- **Authors**: Tim Oosterhuis, Lambert Schomaker
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Being aware of other traffic is a prerequisite for self-driving cars to operate in the real world. In this paper, we show how the intrinsic feature maps of an object detection CNN can be used to uniquely identify vehicles from a dash-cam feed. Feature maps of a pretrained `YOLO' network are used to create 700 deep integrated feature signatures (DIFS) from 20 different images of 35 vehicles from a high resolution dataset and 340 signatures from 20 different images of 17 vehicles of a lower resolution tracking benchmark dataset. The YOLO network was trained to classify general object categories, e.g. classify a detected object as a `car' or `truck'. 5-Fold nearest neighbor (1NN) classification was used on DIFS created from feature maps in the middle layers of the network to correctly identify unique vehicles at a rate of 96.7\% for the high resolution data and with a rate of 86.8\% for the lower resolution data. We conclude that a deep neural detection network trained to distinguish between different classes can be successfully used to identify different instances belonging to the same class, through the creation of deep integrated feature signatures (DIFS).



### Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.00273v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00273v6)
- **Published**: 2020-02-29 14:55:43+00:00
- **Updated**: 2020-03-28 14:51:33+00:00
- **Authors**: Runfa Chen, Wenbing Huang, Binghui Huang, Fuchun Sun, Bin Fang
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation is a central task in computer vision. Current translation frameworks will abandon the discriminator once the training process is completed. This paper contends a novel role of the discriminator by reusing it for encoding the images of the target domain. The proposed architecture, termed as NICE-GAN, exhibits two advantageous patterns over previous approaches: First, it is more compact since no independent encoding component is required; Second, this plug-in encoder is directly trained by the adversary loss, making it more informative and trained more effectively if a multi-scale discriminator is applied. The main issue in NICE-GAN is the coupling of translation with discrimination along the encoder, which could incur training inconsistency when we play the min-max game via GAN. To tackle this issue, we develop a decoupled training strategy by which the encoder is only trained when maximizing the adversary loss while keeping frozen otherwise. Extensive experiments on four popular benchmarks demonstrate the superior performance of NICE-GAN over state-of-the-art methods in terms of FID, KID, and also human preference. Comprehensive ablation studies are also carried out to isolate the validity of each proposed component. Our codes are available at https://github.com/alpc91/NICE-GAN-pytorch.



### Augmenting Visual Place Recognition with Structural Cues
- **Arxiv ID**: http://arxiv.org/abs/2003.00278v3
- **DOI**: 10.1109/LRA.2020.3009077
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.00278v3)
- **Published**: 2020-02-29 15:28:05+00:00
- **Updated**: 2020-07-16 15:31:14+00:00
- **Authors**: Amadeus Oertel, Titus Cieslewski, Davide Scaramuzza
- **Comment**: 8 pages, published in RA-L & IROS 2020
- **Journal**: IEEE Robotics and Automation Letters, 2020
- **Summary**: In this paper, we propose to augment image-based place recognition with structural cues. Specifically, these structural cues are obtained using structure-from-motion, such that no additional sensors are needed for place recognition. This is achieved by augmenting the 2D convolutional neural network (CNN) typically used for image-based place recognition with a 3D CNN that takes as input a voxel grid derived from the structure-from-motion point cloud. We evaluate different methods for fusing the 2D and 3D features and obtain best performance with global average pooling and simple concatenation. On the Oxford RobotCar dataset, the resulting descriptor exhibits superior recognition performance compared to descriptors extracted from only one of the input modalities, including state-of-the-art image-based descriptors. Especially at low descriptor dimensionalities, we outperform state-of-the-art descriptors by up to 90%.



### Representations, Metrics and Statistics For Shape Analysis of Elastic Graphs
- **Arxiv ID**: http://arxiv.org/abs/2003.00287v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2003.00287v2)
- **Published**: 2020-02-29 16:07:48+00:00
- **Updated**: 2020-05-15 00:34:06+00:00
- **Authors**: Xiaoyang Guo, Anuj Srivastava
- **Comment**: Visualization improved
- **Journal**: None
- **Summary**: Past approaches for statistical shape analysis of objects have focused mainly on objects within the same topological classes, e.g., scalar functions, Euclidean curves, or surfaces, etc. For objects that differ in more complex ways, the current literature offers only topological methods. This paper introduces a far-reaching geometric approach for analyzing shapes of graphical objects, such as road networks, blood vessels, brain fiber tracts, etc. It represents such objects, exhibiting differences in both geometries and topologies, as graphs made of curves with arbitrary shapes (edges) and connected at arbitrary junctions (nodes). To perform statistical analyses, one needs mathematical representations, metrics and other geometrical tools, such as geodesics, means, and covariances. This paper utilizes a quotient structure to develop efficient algorithms for computing these quantities, leading to useful statistical tools, including principal component analysis and analytical statistical testing and modeling of graphical shapes. The efficacy of this framework is demonstrated using various simulated as well as the real data from neurons and brain arterial networks.



### Unsupervised Dictionary Learning for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00293v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2003.00293v2)
- **Published**: 2020-02-29 16:25:56+00:00
- **Updated**: 2020-09-07 12:49:33+00:00
- **Authors**: Paul Irofti, Andra Băltoiu
- **Comment**: in Proceedings of iTWIST'20, Paper-ID: 09, Nantes, France, December,
  2-4, 2020
- **Journal**: None
- **Summary**: We investigate the possibilities of employing dictionary learning to address the requirements of most anomaly detection applications, such as absence of supervision, online formulations, low false positive rates. We present new results of our recent semi-supervised online algorithm, TODDLeR, on a anti-money laundering application. We also introduce a novel unsupervised method of using the performance of the learning algorithm as indication of the nature of the samples.



### Grounded and Controllable Image Completion by Incorporating Lexical Semantics
- **Arxiv ID**: http://arxiv.org/abs/2003.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00303v1)
- **Published**: 2020-02-29 16:54:21+00:00
- **Updated**: 2020-02-29 16:54:21+00:00
- **Authors**: Shengyu Zhang, Tan Jiang, Qinghao Huang, Ziqi Tan, Zhou Zhao, Siliang Tang, Jin Yu, Hongxia Yang, Yi Yang, Fei Wu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we present an approach, namely Lexical Semantic Image Completion (LSIC), that may have potential applications in art, design, and heritage conservation, among several others. Existing image completion procedure is highly subjective by considering only visual context, which may trigger unpredictable results which are plausible but not faithful to a grounded knowledge. To permit both grounded and controllable completion process, we advocate generating results faithful to both visual and lexical semantic context, i.e., the description of leaving holes or blank regions in the image (e.g., hole description). One major challenge for LSIC comes from modeling and aligning the structure of visual-semantic context and translating across different modalities. We term this process as structure completion, which is realized by multi-grained reasoning blocks in our model. Another challenge relates to the unimodal biases, which occurs when the model generates plausible results without using the textual description. This can be true since the annotated captions for an image are often semantically equivalent in existing datasets, and thus there is only one paired text for a masked image in training. We devise an unsupervised unpaired-creation learning path besides the over-explored paired-reconstruction path, as well as a multi-stage training strategy to mitigate the insufficiency of labeled data. We conduct extensive quantitative and qualitative experiments as well as ablation studies, which reveal the efficacy of our proposed LSIC.



### A Bayesian approach to tissue-fraction estimation for oncological PET segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.00317v3
- **DOI**: 10.1088/1361-6560/ac01f4
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00317v3)
- **Published**: 2020-02-29 17:40:04+00:00
- **Updated**: 2022-05-27 22:17:52+00:00
- **Authors**: Ziping Liu, Joyce C. Mhlanga, Richard Laforest, Paul-Robert Derenoncourt, Barry A. Siegel, Abhinav K. Jha
- **Comment**: None
- **Journal**: Phys. Med. Biol. 66 124002 (2021)
- **Summary**: Tumor segmentation in oncological PET is challenging, a major reason being the partial-volume effects that arise due to low system resolution and finite voxel size. The latter results in tissue-fraction effects, i.e. voxels contain a mixture of tissue classes. Conventional segmentation methods are typically designed to assign each voxel in the image as belonging to a certain tissue class. Thus, these methods are inherently limited in modeling tissue-fraction effects. To address the challenge of accounting for partial-volume effects, and in particular, tissue-fraction effects, we propose a Bayesian approach to tissue-fraction estimation for oncological PET segmentation. Specifically, this Bayesian approach estimates the posterior mean of fractional volume that the tumor occupies within each voxel of the image. The proposed method, implemented using a deep-learning-based technique, was first evaluated using clinically realistic 2-D simulation studies with known ground truth, in the context of segmenting the primary tumor in PET images of patients with lung cancer. The evaluation studies demonstrated that the method accurately estimated the tumor-fraction areas and significantly outperformed widely used conventional PET segmentation methods, including a U-net-based method, on the task of segmenting the tumor. In addition, the proposed method was relatively insensitive to partial-volume effects and yielded reliable tumor segmentation for different clinical-scanner configurations. The method was then evaluated using clinical images of patients with stage IIB/III non-small cell lung cancer from ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed that the proposed method significantly outperformed all other considered methods and yielded accurate tumor segmentation on patient images with Dice similarity coefficient (DSC) of 0.82 (95 % CI: [0.78, 0.86]).



### Learning Cross-domain Generalizable Features by Representation Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2003.00321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00321v1)
- **Published**: 2020-02-29 17:53:16+00:00
- **Updated**: 2020-02-29 17:53:16+00:00
- **Authors**: Qingjie Meng, Daniel Rueckert, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models exhibit limited generalizability across different domains. Specifically, transferring knowledge from available entangled domain features(source/target domain) and categorical features to new unseen categorical features in a target domain is an interesting and difficult problem that is rarely discussed in the current literature. This problem is essential for many real-world applications such as improving diagnostic classification or prediction in medical imaging. To address this problem, we propose Mutual-Information-based Disentangled Neural Networks (MIDNet) to extract generalizable features that enable transferring knowledge to unseen categorical features in target domains. The proposed MIDNet is developed as a semi-supervised learning paradigm to alleviate the dependency on labeled data. This is important for practical applications where data annotation requires rare expertise as well as intense time and labor. We demonstrate our method on handwritten digits datasets and a fetal ultrasound dataset for image classification tasks. Experiments show that our method outperforms the state-of-the-art and achieve expected performance with sparsely labeled data.



### Hazard Detection in Supermarkets using Deep Learning on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2003.04116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04116v1)
- **Published**: 2020-02-29 18:43:55+00:00
- **Updated**: 2020-02-29 18:43:55+00:00
- **Authors**: M. G. Sarwar Murshed, Edward Verenich, James J. Carroll, Nazar Khan, Faraz Hussain
- **Comment**: 6 pages, conference
- **Journal**: None
- **Summary**: Supermarkets need to ensure clean and safe environments for both shoppers and employees. Slips, trips, and falls can result in injuries that have a physical as well as financial cost. Timely detection of hazardous conditions such as spilled liquids or fallen items on supermarket floors can reduce the chances of serious injuries. This paper presents EdgeLite, a novel, lightweight deep learning model for easy deployment and inference on resource-constrained devices. We describe the use of EdgeLite on two edge devices for detecting supermarket floor hazards. On a hazard detection dataset that we developed, EdgeLite, when deployed on edge devices, outperformed six state-of-the-art object detection models in terms of accuracy while having comparable memory usage and inference time.



### The Utility of Feature Reuse: Transfer Learning in Data-Starved Regimes
- **Arxiv ID**: http://arxiv.org/abs/2003.04117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04117v1)
- **Published**: 2020-02-29 18:48:58+00:00
- **Updated**: 2020-02-29 18:48:58+00:00
- **Authors**: Edward Verenich, Alvaro Velasquez, M. G. Sarwar Murshed, Faraz Hussain
- **Comment**: 3 pages, 1 figure, conference
- **Journal**: None
- **Summary**: The use of transfer learning with deep neural networks has increasingly become widespread for deploying well-tested computer vision systems to newer domains, especially those with limited datasets. We describe a transfer learning use case for a domain with a data-starved regime, having fewer than 100 labeled target samples. We evaluate the effectiveness of convolutional feature extraction and fine-tuning of overparameterized models with respect to the size of target training data, as well as their generalization performance on data with covariate shift, or out-of-distribution (OOD) data. Our experiments show that both overparameterization and feature reuse contribute to successful application of transfer learning in training image classifiers in data-starved regimes.



### An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data: Experience and Practice
- **Arxiv ID**: http://arxiv.org/abs/2003.00344v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2003.00344v1)
- **Published**: 2020-02-29 20:33:48+00:00
- **Updated**: 2020-02-29 20:33:48+00:00
- **Authors**: Ruwan Wickramarachchi, Cory Henson, Amit Sheth
- **Comment**: 11 pages, To appear in AAAI 2020 Spring Symposium on Combining
  Machine Learning and Knowledge Engineering in Practice (AAAI-MAKE 2020)
- **Journal**: None
- **Summary**: The autonomous driving (AD) industry is exploring the use of knowledge graphs (KGs) to manage the vast amount of heterogeneous data generated from vehicular sensors. The various types of equipped sensors include video, LIDAR and RADAR. Scene understanding is an important topic in AD which requires consideration of various aspects of a scene, such as detected objects, events, time and location. Recent work on knowledge graph embeddings (KGEs) - an approach that facilitates neuro-symbolic fusion - has shown to improve the predictive performance of machine learning models. With the expectation that neuro-symbolic fusion through KGEs will improve scene understanding, this research explores the generation and evaluation of KGEs for autonomous driving data. We also present an investigation of the relationship between the level of informational detail in a KG and the quality of its derivative embeddings. By systematically evaluating KGEs along four dimensions -- i.e. quality metrics, KG informational detail, algorithms, and datasets -- we show that (1) higher levels of informational detail in KGs lead to higher quality embeddings, (2) type and relation semantics are better captured by the semantic transitional distance-based TransE algorithm, and (3) some metrics, such as coherence measure, may not be suitable for intrinsically evaluating KGEs in this domain. Additionally, we also present an (early) investigation of the usefulness of KGEs for two use-cases in the AD domain.



### Emotion Recognition System from Speech and Visual Information based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.00351v1
- **DOI**: 10.1109/SPED.2019.8906538
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2003.00351v1)
- **Published**: 2020-02-29 22:09:46+00:00
- **Updated**: 2020-02-29 22:09:46+00:00
- **Authors**: Nicolae-Catalin Ristea, Liviu Cristian Dutu, Anamaria Radoi
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion recognition has become an important field of research in the human-computer interactions domain. The latest advancements in the field show that combining visual with audio information lead to better results if compared to the case of using a single source of information separately. From a visual point of view, a human emotion can be recognized by analyzing the facial expression of the person. More precisely, the human emotion can be described through a combination of several Facial Action Units. In this paper, we propose a system that is able to recognize emotions with a high accuracy rate and in real time, based on deep Convolutional Neural Networks. In order to increase the accuracy of the recognition system, we analyze also the speech data and fuse the information coming from both sources, i.e., visual and audio. Experimental results show the effectiveness of the proposed scheme for emotion recognition and the importance of combining visual with audio data.



### A Review of Computational Approaches for Evaluation of Rehabilitation Exercises
- **Arxiv ID**: http://arxiv.org/abs/2003.08767v2
- **DOI**: 10.1016/j.compbiomed.2020.103687
- **Categories**: **cs.CV**, cs.LG, J.3; A.1
- **Links**: [PDF](http://arxiv.org/pdf/2003.08767v2)
- **Published**: 2020-02-29 22:18:56+00:00
- **Updated**: 2020-03-20 02:55:34+00:00
- **Authors**: Yalin Liao, Aleksandar Vakanski, Min Xian, David Paul, Russell Baker
- **Comment**: 29 pages, 1 figure
- **Journal**: Computers in Biology and Medicine, vol. 119, 2020
- **Summary**: Recent advances in data analytics and computer-aided diagnostics stimulate the vision of patient-centric precision healthcare, where treatment plans are customized based on the health records and needs of every patient. In physical rehabilitation, the progress in machine learning and the advent of affordable and reliable motion capture sensors have been conducive to the development of approaches for automated assessment of patient performance and progress toward functional recovery. The presented study reviews computational approaches for evaluating patient performance in rehabilitation programs using motion capture systems. Such approaches will play an important role in supplementing traditional rehabilitation assessment performed by trained clinicians, and in assisting patients participating in home-based rehabilitation. The reviewed computational methods for exercise evaluation are grouped into three main categories: discrete movement score, rule-based, and template-based approaches. The review places an emphasis on the application of machine learning methods for movement evaluation in rehabilitation. Related work in the literature on data representation, feature engineering, movement segmentation, and scoring functions is presented. The study also reviews existing sensors for capturing rehabilitation movements and provides an informative listing of pertinent benchmark datasets. The significance of this paper is in being the first to provide a comprehensive review of computational methods for evaluation of patient performance in rehabilitation programs.



