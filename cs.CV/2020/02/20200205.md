# Arxiv Papers in cs.CV on 2020-02-05
### Fine-Grained Urban Flow Inference
- **Arxiv ID**: http://arxiv.org/abs/2002.02318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02318v1)
- **Published**: 2020-02-05 01:11:24+00:00
- **Updated**: 2020-02-05 01:11:24+00:00
- **Authors**: Kun Ouyang, Yuxuan Liang, Ye Liu, Zekun Tong, Sijie Ruan, Yu Zheng, David S. Rosenblum
- **Comment**: 16 pages. arXiv admin note: substantial text overlap with
  arXiv:1902.05377
- **Journal**: None
- **Summary**: The ubiquitous deployment of monitoring devices in urban flow monitoring systems induces a significant cost for maintenance and operation. A technique is required to reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity. In this paper, we present an approach for inferring the real-time and fine-grained crowd flows throughout a city based on coarse-grained observations. This task exhibits two challenges: the spatial correlations between coarse- and fine-grained urban flows, and the complexities of external impacts. To tackle these issues, we develop a model entitled UrbanFM which consists of two major parts: 1) an inference network to generate fine-grained flow distributions from coarse-grained inputs that uses a feature extraction module and a novel distributional upsampling module; 2) a general fusion subnet to further boost the performance by considering the influence of different external factors. This structure provides outstanding effectiveness and efficiency for small scale upsampling. However, the single-pass upsampling used by UrbanFM is insufficient at higher upscaling rates. Therefore, we further present UrbanPy, a cascading model for progressive inference of fine-grained urban flows by decomposing the original tasks into multiple subtasks. Compared to UrbanFM, such an enhanced structure demonstrates favorable performance for larger-scale inference tasks.



### Unsupervised Community Detection with a Potts Model Hamiltonian, an Efficient Algorithmic Solution, and Applications in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2002.01599v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.stat-mech
- **Links**: [PDF](http://arxiv.org/pdf/2002.01599v1)
- **Published**: 2020-02-05 01:20:28+00:00
- **Updated**: 2020-02-05 01:20:28+00:00
- **Authors**: Brendon Lutnick, Wen Dong, Zohar Nussinov, Pinaki Sarder
- **Comment**: 46 pages, 19 Figures
- **Journal**: None
- **Summary**: Unsupervised segmentation of large images using a Potts model Hamiltonian is unique in that segmentation is governed by a resolution parameter which scales the sensitivity to small clusters. Here, the input image is first modeled as a graph, which is then segmented by minimizing a Hamiltonian cost function defined on the graph and the respective segments. However, there exists no closed form solution of this optimization, and using previous iterative algorithmic solution techniques, the problem scales quadratically in the Input Length. Therefore, while Potts model segmentation gives accurate segmentation, it is grossly underutilized as an unsupervised learning technique. We propose a fast statistical down-sampling of input image pixels based on the respective color features, and a new iterative method to minimize the Potts model energy considering pixel to segment relationship. This method is generalizable and can be extended for image pixel texture features as well as spatial features. We demonstrate that this new method is highly efficient, and outperforms existing methods for Potts model based image segmentation. We demonstrate the application of our method in medical microscopy image segmentation; particularly, in segmenting renal glomerular micro-environment in renal pathology. Our method is not limited to image segmentation, and can be extended to any image/data segmentation/clustering task for arbitrary datasets with discrete features.



### Anomaly Detection by One Class Latent Regularized Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01607v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01607v2)
- **Published**: 2020-02-05 02:21:52+00:00
- **Updated**: 2020-07-14 06:30:49+00:00
- **Authors**: Chengwei Chen, Pan Chen, Haichuan Song, Yiqing Tao, Yuan Xie, Shouhong Ding, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection is a fundamental problem in computer vision area with many real-world applications. Given a wide range of images belonging to the normal class, emerging from some distribution, the objective of this task is to construct the model to detect out-of-distribution images belonging to abnormal instances. Semi-supervised Generative Adversarial Networks (GAN)-based methods have been gaining popularity in anomaly detection task recently. However, the training process of GAN is still unstable and challenging. To solve these issues, a novel adversarial dual autoencoder network is proposed, in which the underlying structure of training data is not only captured in latent feature space, but also can be further restricted in the space of latent representation in a discriminant manner, leading to a more accurate detector. In addition, the auxiliary autoencoder regarded as a discriminator could obtain an more stable training process. Experiments show that our model achieves the state-of-the-art results on MNIST and CIFAR10 datasets as well as GTSRB stop signs dataset.



### BABO: Background Activation Black-Out for Efficient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.01609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01609v2)
- **Published**: 2020-02-05 02:25:08+00:00
- **Updated**: 2020-03-23 12:03:31+00:00
- **Authors**: Byungseok Roh, Han-Cheol Cho, Myung-Ho Ju, Soon Hyung Pyo
- **Comment**: 14 pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: Recent advances in deep learning have enabled complex real-world use cases comprised of multiple vision tasks and detection tasks are being shifted to the edge side as a pre-processing step of the entire workload. Since running a deep model on resource-constraint devices is challenging, techniques for efficient inference methods are demanded. In this paper, we present an objectness-aware object detection method to reduce computational cost by sparsifying activation values on background regions where target objects don't exist. Sparsified activation can be exploited to increase inference speed by software or hardware accelerated sparse convolution techniques. To accomplish this goal, we incorporate a light-weight objectness mask generation (OMG) network in front of an object detection (OD) network so that it can zero out unnecessary background areas of an input image before being fed into the OD network. In experiments, by switching background activation values to zero, the average number of zero values increases further from 36% to 68% on MobileNetV2-SSDLite even with ReLU activation while maintaining accuracy on MS-COCO. This result indicates that the total MAC including both OMG and OD networks can be reduced to 62% of the original OD model when only non-zero multiply-accumulate operations are considered. Moreover, we show a similar tendency in heavy networks (VGG and RetinaNet) and an additional dataset (PASCAL VOC).



### Generating Interpretable Poverty Maps using Object Detection in Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2002.01612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01612v2)
- **Published**: 2020-02-05 02:50:01+00:00
- **Updated**: 2020-02-18 02:02:57+00:00
- **Authors**: Kumar Ayush, Burak Uzkent, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate local-level poverty measurement is an essential task for governments and humanitarian organizations to track the progress towards improving livelihoods and distribute scarce resources. Recent computer vision advances in using satellite imagery to predict poverty have shown increasing accuracy, but they do not generate features that are interpretable to policymakers, inhibiting adoption by practitioners. Here we demonstrate an interpretable computational framework to accurately predict poverty at a local level by applying object detectors to high resolution (30cm) satellite images. Using the weighted counts of objects as features, we achieve 0.539 Pearson's r^2 in predicting village-level poverty in Uganda, a 31% improvement over existing (and less interpretable) benchmarks. Feature importance and ablation analysis reveal intuitive relationships between object counts and poverty predictions. Our results suggest that interpretability does not have to come at the cost of performance, at least in this important domain.



### Crowdsourcing the Perception of Machine Teaching
- **Arxiv ID**: http://arxiv.org/abs/2002.01618v1
- **DOI**: 10.1145/3313831.3376428
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01618v1)
- **Published**: 2020-02-05 03:20:25+00:00
- **Updated**: 2020-02-05 03:20:25+00:00
- **Authors**: Jonggi Hong, Kyungjun Lee, June Xu, Hernisa Kacorri
- **Comment**: 10 pages, 8 figures, 5 tables, CHI2020 conference
- **Journal**: Proceedings of the 2020 CHI Conference on Human Factors in
  Computing Systems
- **Summary**: Teachable interfaces can empower end-users to attune machine learning systems to their idiosyncratic characteristics and environment by explicitly providing pertinent training examples. While facilitating control, their effectiveness can be hindered by the lack of expertise or misconceptions. We investigate how users may conceptualize, experience, and reflect on their engagement in machine teaching by deploying a mobile teachable testbed in Amazon Mechanical Turk. Using a performance-based payment scheme, Mechanical Turkers (N = 100) are called to train, test, and re-train a robust recognition model in real-time with a few snapshots taken in their environment. We find that participants incorporate diversity in their examples drawing from parallels to how humans recognize objects independent of size, viewpoint, location, and illumination. Many of their misconceptions relate to consistency and model capabilities for reasoning. With limited variation and edge cases in testing, the majority of them do not change strategies on a second training attempt.



### Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.01619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01619v2)
- **Published**: 2020-02-05 03:25:02+00:00
- **Updated**: 2021-06-09 04:20:56+00:00
- **Authors**: Yingjie Cai, Buyu Li, Zeyu Jiao, Hongsheng Li, Xingyu Zeng, Xiaogang Wang
- **Comment**: 11 pages, 8 figures, AAAI2020
- **Journal**: None
- **Summary**: Monocular 3D object detection task aims to predict the 3D bounding boxes of objects based on monocular RGB images. Since the location recovery in 3D space is quite difficult on account of absence of depth information, this paper proposes a novel unified framework which decomposes the detection problem into a structured polygon prediction task and a depth recovery task. Different from the widely studied 2D bounding boxes, the proposed novel structured polygon in the 2D image consists of several projected surfaces of the target object. Compared to the widely-used 3D bounding box proposals, it is shown to be a better representation for 3D detection. In order to inversely project the predicted 2D structured polygon to a cuboid in the 3D physical world, the following depth recovery task uses the object height prior to complete the inverse projection transformation with the given camera projection matrix. Moreover, a fine-grained 3D box refinement scheme is proposed to further rectify the 3D detection results. Experiments are conducted on the challenging KITTI benchmark, in which our method achieves state-of-the-art detection accuracy.



### Illumination adaptive person reid based on teacher-student model and adversarial training
- **Arxiv ID**: http://arxiv.org/abs/2002.01625v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01625v3)
- **Published**: 2020-02-05 03:49:10+00:00
- **Updated**: 2020-05-26 10:20:21+00:00
- **Authors**: Ziyue Zhang, Richard YD Xu, Shuai Jiang, Yang Li, Congzhentao Huang, Chen Deng
- **Comment**: Accepted by ICIP 2020
- **Journal**: None
- **Summary**: Most existing works in Person Re-identification (ReID) focus on settings where illumination either is kept the same or has very little fluctuation. However, the changes in the illumination degree may affect the robustness of a ReID algorithm significantly. To address this problem, we proposed a Two-Stream Network that can separate ReID features from lighting features to enhance ReID performance. Its innovations are threefold: (1) A discriminative entropy loss to ensure the ReID features contain no lighting information. (2) A ReID Teacher model trained by images under "neutral" lighting conditions to guide ReID classification. (3) An illumination Teacher model trained by the differences between the illumination-adjusted and original images to guide illumination classification. We construct two augmented datasets by synthetically changing a set of predefined lighting conditions in two of the most popular ReID benchmarks: Market1501 and DukeMTMC-ReID. Experiments demonstrate that our algorithm outperforms other state-of-the-art works and particularly potent in handling images under extremely low light.



### Learning Test-time Augmentation for Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.01642v5
- **DOI**: 10.1016/j.cviu.2022.103494
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01642v5)
- **Published**: 2020-02-05 05:08:41+00:00
- **Updated**: 2022-07-05 04:28:32+00:00
- **Authors**: Osman Tursun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Off-the-shelf convolutional neural network features achieve outstanding results in many image retrieval tasks. However, their invariance to target data is pre-defined by the network architecture and training data. Existing image retrieval approaches require fine-tuning or modification of pre-trained networks to adapt to variations unique to the target data. In contrast, our method enhances the invariance of off-the-shelf features by aggregating features extracted from images augmented at test-time, with augmentations guided by a policy learned through reinforcement learning. The learned policy assigns different magnitudes and weights to the selected transformations, which are selected from a list of image transformations. Policies are evaluated using a metric learning protocol to learn the optimal policy. The model converges quickly and the cost of each policy iteration is minimal as we propose an off-line caching technique to greatly reduce the computational cost of extracting features from augmented images. Experimental results on large trademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k and RParis6k scene datasets) tasks show that the learned ensemble of transformations is highly effective for improving performance, and is practical, and transferable.



### Solving Raven's Progressive Matrices with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01646v2)
- **Published**: 2020-02-05 05:18:02+00:00
- **Updated**: 2020-02-06 13:58:52+00:00
- **Authors**: Tao Zhuo, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Raven's Progressive Matrices (RPM) have been widely used for Intelligence Quotient (IQ) test of humans. In this paper, we aim to solve RPM with neural networks in both supervised and unsupervised manners. First, we investigate strategies to reduce over-fitting in supervised learning. We suggest the use of a neural network with deep layers and pre-training on large-scale datasets to improve model generalization. Experiments on the RAVEN dataset show that the overall accuracy of our supervised approach surpasses human-level performance. Second, as an intelligent agent requires to automatically learn new skills to solve new problems, we propose the first unsupervised method, Multilabel Classification with Pseudo Target (MCPT), for RPM problems. Based on the design of the pseudo target, MCPT converts the unsupervised learning problem to a supervised task. Experiments show that MCPT doubles the testing accuracy of random guessing e.g. 28.50% vs. 12.5%. Finally, we discuss the problem of solving RPM with unsupervised and explainable strategies in the future.



### Concept Whitening for Interpretable Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.01650v5
- **DOI**: 10.1038/s42256-020-00265-z
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01650v5)
- **Published**: 2020-02-05 05:28:09+00:00
- **Updated**: 2020-12-07 19:09:35+00:00
- **Authors**: Zhi Chen, Yijie Bei, Cynthia Rudin
- **Comment**: Authors' pre-publication version of a 2020 Nature Machine
  Intelligence article
- **Journal**: Nature Machine Intelligence, Vol 2, Dec 2020, 772-782
- **Summary**: What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can either be misleading, unusable, or rely on the latent space to possess properties that it may not have. In this work, rather than attempting to analyze a neural network posthoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a CNN, the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us a much clearer understanding for how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens) the latent space. CW can be used in any layer of the network without hurting predictive performance.



### CHAIN: Concept-harmonized Hierarchical Inference Interpretation of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01660v1)
- **Published**: 2020-02-05 06:45:23+00:00
- **Updated**: 2020-02-05 06:45:23+00:00
- **Authors**: Dan Wang, Xinrui Cui, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: With the great success of networks, it witnesses the increasing demand for the interpretation of the internal network mechanism, especially for the net decision-making logic. To tackle the challenge, the Concept-harmonized HierArchical INference (CHAIN) is proposed to interpret the net decision-making process. For net-decisions being interpreted, the proposed method presents the CHAIN interpretation in which the net decision can be hierarchically deduced into visual concepts from high to low semantic levels. To achieve it, we propose three models sequentially, i.e., the concept harmonizing model, the hierarchical inference model, and the concept-harmonized hierarchical inference model. Firstly, in the concept harmonizing model, visual concepts from high to low semantic-levels are aligned with net-units from deep to shallow layers. Secondly, in the hierarchical inference model, the concept in a deep layer is disassembled into units in shallow layers. Finally, in the concept-harmonized hierarchical inference model, a deep-layer concept is inferred from its shallow-layer concepts. After several rounds, the concept-harmonized hierarchical inference is conducted backward from the highest semantic level to the lowest semantic level. Finally, net decision-making is explained as a form of concept-harmonized hierarchical inference, which is comparable to human decision-making. Meanwhile, the net layer structure for feature learning can be explained based on the hierarchical visual concepts. In quantitative and qualitative experiments, we demonstrate the effectiveness of CHAIN at the instance and class levels.



### Entropy Minimization vs. Diversity Maximization for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.01690v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01690v1)
- **Published**: 2020-02-05 09:13:19+00:00
- **Updated**: 2020-02-05 09:13:19+00:00
- **Authors**: Xiaofu Wu, Suofei hang, Quan Zhou, Zhen Yang, Chunming Zhao, Longin Jan Latecki
- **Comment**: submitted to IEEE T-IP
- **Journal**: None
- **Summary**: Entropy minimization has been widely used in unsupervised domain adaptation (UDA). However, existing works reveal that entropy minimization only may result into collapsed trivial solutions. In this paper, we propose to avoid trivial solutions by further introducing diversity maximization. In order to achieve the possible minimum target risk for UDA, we show that diversity maximization should be elaborately balanced with entropy minimization, the degree of which can be finely controlled with the use of deep embedded validation in an unsupervised manner. The proposed minimal-entropy diversity maximization (MEDM) can be directly implemented by stochastic gradient descent without use of adversarial learning. Empirical evidence demonstrates that MEDM outperforms the state-of-the-art methods on four popular domain adaptation datasets.



### Geocoding of trees from street addresses and street-level images
- **Arxiv ID**: http://arxiv.org/abs/2002.01708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01708v1)
- **Published**: 2020-02-05 10:13:43+00:00
- **Updated**: 2020-02-05 10:13:43+00:00
- **Authors**: Daniel Laumer, Nico Lang, Natalie van Doorn, Oisin Mac Aodha, Pietro Perona, Jan Dirk Wegner
- **Comment**: Accepted for publication in ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: We introduce an approach for updating older tree inventories with geographic coordinates using street-level panorama images and a global optimization framework for tree instance matching. Geolocations of trees in inventories until the early 2000s where recorded using street addresses whereas newer inventories use GPS. Our method retrofits older inventories with geographic coordinates to allow connecting them with newer inventories to facilitate long-term studies on tree mortality etc. What makes this problem challenging is the different number of trees per street address, the heterogeneous appearance of different tree instances in the images, ambiguous tree positions if viewed from multiple images and occlusions. To solve this assignment problem, we (i) detect trees in Google street-view panoramas using deep learning, (ii) combine multi-view detections per tree into a single representation, (iii) and match detected trees with given trees per street address with a global optimization approach. Experiments for > 50000 trees in 5 cities in California, USA, show that we are able to assign geographic coordinates to 38 % of the street trees, which is a good starting point for long-term studies on the ecosystem services value of street trees at large scale.



### Feature-map-level Online Adversarial Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2002.01775v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01775v3)
- **Published**: 2020-02-05 13:16:37+00:00
- **Updated**: 2020-06-05 18:15:40+00:00
- **Authors**: Inseop Chung, SeongUk Park, Jangho Kim, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Feature maps contain rich information about image intensity and spatial correlation. However, previous online knowledge distillation methods only utilize the class probabilities. Thus in this paper, we propose an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. We train multiple networks simultaneously by employing discriminators to distinguish the feature map distributions of different networks. Each network has its corresponding discriminator which discriminates the feature map from its own as fake while classifying that of the other network as real. By training a network to fool the corresponding discriminator, it can learn the other network's feature map distribution. We show that our method performs better than the conventional direct alignment method such as L1 and is more suitable for online distillation. Also, we propose a novel cyclic learning scheme for training more than two networks together. We have applied our method to various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one.



### Human Posture Recognition and Gesture Imitation with a Humanoid Robot
- **Arxiv ID**: http://arxiv.org/abs/2002.01779v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 14J60 (Robotics), F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/2002.01779v3)
- **Published**: 2020-02-05 13:26:05+00:00
- **Updated**: 2020-03-21 08:56:43+00:00
- **Authors**: Amir Aly
- **Comment**: University of Paris 6 (UPMC), University of Sorbonne, France
- **Journal**: None
- **Summary**: This study proposes different approaches for static and dynamic gesture analysis and imitation with the social robot Nao



### Proximity Preserving Binary Code using Signed Graph-Cut
- **Arxiv ID**: http://arxiv.org/abs/2002.01793v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01793v1)
- **Published**: 2020-02-05 13:58:41+00:00
- **Updated**: 2020-02-05 13:58:41+00:00
- **Authors**: Inbal Lav, Shai Avidan, Yoram Singer, Yacov Hel-Or
- **Comment**: None
- **Journal**: AAAI Conference on Artificial Intelligence , Feb. 2020
- **Summary**: We introduce a binary embedding framework, called Proximity Preserving Code (PPC), which learns similarity and dissimilarity between data points to create a compact and affinity-preserving binary code. This code can be used to apply fast and memory-efficient approximation to nearest-neighbor searches. Our framework is flexible, enabling different proximity definitions between data points. In contrast to previous methods that extract binary codes based on unsigned graph partitioning, our system models the attractive and repulsive forces in the data by incorporating positive and negative graph weights. The proposed framework is shown to boil down to finding the minimal cut of a signed graph, a problem known to be NP-hard. We offer an efficient approximation and achieve superior results by constructing the code bit after bit. We show that the proposed approximation is superior to the commonly used spectral methods with respect to both accuracy and complexity. Thus, it is useful for many other problems that can be translated into signed graph cut.



### Level Three Synthetic Fingerprint Generation
- **Arxiv ID**: http://arxiv.org/abs/2002.03809v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03809v3)
- **Published**: 2020-02-05 14:09:47+00:00
- **Updated**: 2020-08-07 19:18:05+00:00
- **Authors**: André Brasil Vieira Wyzykowski, Mauricio Pamplona Segundo, Rubisley de Paula Lemes
- **Comment**: Database are available at https://andrewyzy.github.io/L3-SF/
- **Journal**: None
- **Summary**: Today's legal restrictions that protect the privacy of biometric data are hampering fingerprint recognition researches. For instance, all high-resolution fingerprint databases ceased to be publicly available. To address this problem, we present a novel hybrid approach to synthesize realistic, high-resolution fingerprints. First, we improved Anguli, a handcrafted fingerprint generator, to obtain dynamic ridge maps with sweat pores and scratches. Then, we trained a CycleGAN to transform these maps into realistic fingerprints. Unlike other CNN-based works, we can generate several images for the same identity. We used our approach to create a synthetic database with 7400 images in an attempt to propel further studies in this field without raising legal issues. We included sweat pore annotations in 740 images to encourage research developments in pore detection. In our experiments, we employed two fingerprint matching approaches to confirm that real and synthetic databases have similar performance. We conducted a human perception analysis where sixty volunteers could hardly differ between real and synthesized fingerprints. Given that we also favorably compare our results with the most advanced works in the literature, our experimentation suggests that our approach is the new state-of-the-art.



### FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains
- **Arxiv ID**: http://arxiv.org/abs/2002.05665v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05665v1)
- **Published**: 2020-02-05 15:08:15+00:00
- **Updated**: 2020-02-05 15:08:15+00:00
- **Authors**: Jeanine Harb, Nicolas Rébéna, Raphaël Chosidow, Grégoire Roblin, Roman Potarusov, Hatem Hajri
- **Comment**: None
- **Journal**: None
- **Summary**: In the realm of autonomous transportation, there have been many initiatives for open-sourcing self-driving cars datasets, but much less for alternative methods of transportation such as trains. In this paper, we aim to bridge the gap by introducing FRSign, a large-scale and accurate dataset for vision-based railway traffic light detection and recognition. Our recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations. An illustrative dataset which corresponds to ten percent of the acquired data to date is published in open source with the paper. It contains more than 100,000 images illustrating six types of French railway traffic lights and their possible color combinations, together with the relevant information regarding their acquisition such as date, time, sensor parameters, and bounding boxes. This dataset is published in open-source at the address \url{https://frsign.irt-systemx.fr}. We compare, analyze various properties of the dataset and provide metrics to express its variability. We also discuss specific challenges and particularities related to autonomous trains in comparison to autonomous cars.



### Analyzing the Dependency of ConvNets on Spatial Information
- **Arxiv ID**: http://arxiv.org/abs/2002.01827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01827v1)
- **Published**: 2020-02-05 15:22:32+00:00
- **Updated**: 2020-02-05 15:22:32+00:00
- **Authors**: Yue Fan, Yongqin Xian, Max Maria Losch, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to further investigate the reliance on spatial information. We propose spatial shuffling and GAP+FC to destroy spatial information during both training and testing phases. Interestingly, we observe that spatial information can be deleted from later layers with small performance drops, which indicates spatial information at later layers is not necessary for good performance. For example, test accuracy of VGG-16 only drops by 0.03% and 2.66% with spatial information completely removed from the last 30% and 53% layers on CIFAR100, respectively. Evaluation on several object recognition datasets (CIFAR100, Small-ImageNet, ImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152) shows an overall consistent pattern.



### Domain Embedded Multi-model Generative Adversarial Networks for Image-based Face Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2002.02909v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02909v2)
- **Published**: 2020-02-05 17:36:13+00:00
- **Updated**: 2020-06-20 05:47:05+00:00
- **Authors**: Xian Zhang, Xin Wang, Bin Kong, Canghong Shi, Youbing Yin, Qi Song, Siwei Lyu, Jiancheng Lv, Canghong Shi, Xiaojie Li
- **Comment**: None
- **Journal**: None
- **Summary**: Prior knowledge of face shape and structure plays an important role in face inpainting. However, traditional face inpainting methods mainly focus on the generated image resolution of the missing portion without consideration of the special particularities of the human face explicitly and generally produce discordant facial parts. To solve this problem, we present a domain embedded multi-model generative adversarial model for inpainting of face images with large cropped regions. We firstly represent only face regions using the latent variable as the domain knowledge and combine it with the non-face parts textures to generate high-quality face images with plausible contents. Two adversarial discriminators are finally used to judge whether the generated distribution is close to the real distribution or not. It can not only synthesize novel image structures but also explicitly utilize the embedded face domain knowledge to generate better predictions with consistency on structures and appearance. Experiments on both CelebA and CelebA-HQ face datasets demonstrate that our proposed approach achieved state-of-the-art performance and generates higher quality inpainting results than existing ones.



### Vehicle Ego-Lane Estimation with Sensor Failure Modeling
- **Arxiv ID**: http://arxiv.org/abs/2002.01913v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01913v2)
- **Published**: 2020-02-05 18:32:00+00:00
- **Updated**: 2020-02-06 15:06:49+00:00
- **Authors**: Augusto Luis Ballardini, Daniele Cattaneo, Rubén Izquierdo, Ignacio Parra Alonso, Andrea Piazzoni, Miguel Ángel Sotelo, Domenico Giorgio Sorrenti
- **Comment**: preprint
- **Journal**: None
- **Summary**: We present a probabilistic ego-lane estimation algorithm for highway-like scenarios that is designed to increase the accuracy of the ego-lane estimate, which can be obtained relying only on a noisy line detector and tracker. The contribution relies on a Hidden Markov Model (HMM) with a transient failure model. The proposed algorithm exploits the OpenStreetMap (or other cartographic services) road property lane number as the expected number of lanes and leverages consecutive, possibly incomplete, observations. The algorithm effectiveness is proven by employing different line detectors and showing we could achieve much more usable, i.e. stable and reliable, ego-lane estimates over more than 100 Km of highway scenarios, recorded both in Italy and Spain. Moreover, as we could not find a suitable dataset for a quantitative comparison with other approaches, we collected datasets and manually annotated the Ground Truth about the vehicle ego-lane. Such datasets are made publicly available for usage from the scientific community.



### Brain Tumor Segmentation by Cascaded Deep Neural Networks Using Multiple Image Scales
- **Arxiv ID**: http://arxiv.org/abs/2002.01975v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01975v1)
- **Published**: 2020-02-05 20:00:40+00:00
- **Updated**: 2020-02-05 20:00:40+00:00
- **Authors**: Zahra Sobhaninia, Safiyeh Rezaei, Nader Karimi, Ali Emami, Shadrokh Samavi
- **Comment**: 5 pages and 4 images
- **Journal**: None
- **Summary**: Intracranial tumors are groups of cells that usually grow uncontrollably. One out of four cancer deaths is due to brain tumors. Early detection and evaluation of brain tumors is an essential preventive medical step that is performed by magnetic resonance imaging (MRI). Many segmentation techniques exist for this purpose. Low segmentation accuracy is the main drawback of existing methods. In this paper, we use a deep learning method to boost the accuracy of tumor segmentation in MR images. Cascade approach is used with multiple scales of images to induce both local and global views and help the network to reach higher accuracies. Our experimental results show that using multiple scales and the utilization of two cascade networks is advantageous.



### Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2002.01981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2002.01981v1)
- **Published**: 2020-02-05 20:30:29+00:00
- **Updated**: 2020-02-05 20:30:29+00:00
- **Authors**: Arie Agranonik, Maya Herman, Mark Last
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper we implemented the algorithm we developed in [1] called 3DPIFCM in a parallel environment by using CUDA on a GPU. In our previous work we introduced 3DPIFCM which performs segmentation of images in noisy conditions and uses particle swarm optimization for finding the optimal algorithm parameters to account for noise. This algorithm achieved state of the art segmentation accuracy when compared to FCM (Fuzzy C-Means), IFCMPSO (Improved Fuzzy C-Means with Particle Swarm Optimization), GAIFCM (Genetic Algorithm Improved Fuzzy C-Means) on noisy MRI images of an adult Brain.   When using a genetic algorithm or PSO (Particle Swarm Optimization) on a single machine for optimization we witnessed long execution times for practical clinical usage. Therefore, in the current paper our goal was to speed up the execution of 3DPIFCM by taking out parts of the algorithm and executing them as kernels on a GPU. The algorithm was implemented using the CUDA [13] framework from NVIDIA and experiments where performed on a server containing 64GB RAM , 8 cores and a TITAN X GPU with 3072 SP cores and 12GB of GPU memory.   Our results show that the parallel version of the algorithm performs up to 27x faster than the original sequential version and 68x faster than GAIFCM algorithm. We show that the speedup of the parallel version increases as we increase the size of the image due to better utilization of cores in the GPU. Also, we show a speedup of up to 5x in our Brainweb experiment compared to other generic variants such as IFCMPSO and GAIFCM.



### 3DPIFCM Novel Algorithm for Segmentation of Noisy Brain MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2002.01985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01985v2)
- **Published**: 2020-02-05 20:48:51+00:00
- **Updated**: 2020-02-10 19:22:59+00:00
- **Authors**: Arie Agranonik, Maya Herman, Mark Last
- **Comment**: 16 pages, 21 figures
- **Journal**: None
- **Summary**: We present a novel algorithm named 3DPIFCM, for automatic segmentation of noisy MRI Brain images. The algorithm is an extension of a well-known IFCM (Improved Fuzzy C-Means) algorithm. It performs fuzzy segmentation and introduces a fitness function that is affected by proximity of the voxels and by the color intensity in 3D images. The 3DPIFCM algorithm uses PSO (Particle Swarm Optimization) in order to optimize the fitness function. In addition, the 3DPIFCM uses 3D features of near voxels to better adjust the noisy artifacts. In our experiments, we evaluate 3DPIFCM on T1 Brainweb dataset with noise levels ranging from 1% to 20% and on a synthetic dataset with ground truth both in 3D. The analysis of the segmentation results shows a significant improvement in the segmentation quality of up to 28% compared to two generic variants in noisy images and up to 60% when compared to the original FCM (Fuzzy C-Means).



### Automatic image-based identification and biomass estimation of invertebrates
- **Arxiv ID**: http://arxiv.org/abs/2002.03807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03807v1)
- **Published**: 2020-02-05 21:38:57+00:00
- **Updated**: 2020-02-05 21:38:57+00:00
- **Authors**: Johanna Ärje, Claus Melvad, Mads Rosenhøj Jeppesen, Sigurd Agerskov Madsen, Jenni Raitoharju, Maria Strandgård Rasmussen, Alexandros Iosifidis, Ville Tirronen, Kristian Meissner, Moncef Gabbouj, Toke Thomas Høye
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding how biological communities respond to environmental changes is a key challenge in ecology and ecosystem management. The apparent decline of insect populations necessitates more biomonitoring but the time-consuming sorting and identification of taxa pose strong limitations on how many insect samples can be processed. In turn, this affects the scale of efforts to map invertebrate diversity altogether. Given recent advances in computer vision, we propose to replace the standard manual approach of human expert-based sorting and identification with an automatic image-based technology. We describe a robot-enabled image-based identification machine, which can automate the process of invertebrate identification, biomass estimation and sample sorting. We use the imaging device to generate a comprehensive image database of terrestrial arthropod species. We use this database to test the classification accuracy i.e. how well the species identity of a specimen can be predicted from images taken by the machine. We also test sensitivity of the classification accuracy to the camera settings (aperture and exposure time) in order to move forward with the best possible image quality. We use state-of-the-art Resnet-50 and InceptionV3 CNNs for the classification task. The results for the initial dataset are very promising ($\overline{ACC}=0.980$). The system is general and can easily be used for other groups of invertebrates as well. As such, our results pave the way for generating more data on spatial and temporal variation in invertebrate abundance, diversity and biomass.



### Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.02033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02033v1)
- **Published**: 2020-02-05 23:05:09+00:00
- **Updated**: 2020-02-05 23:05:09+00:00
- **Authors**: Deying Kong, Haoyu Ma, Yifei Chen, Xiaohui Xie
- **Comment**: 2020 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: In this paper, we propose a new architecture named Rotation-invariant Mixed Graphical Model Network (R-MGMN) to solve the problem of 2D hand pose estimation from a monocular RGB image. By integrating a rotation net, the R-MGMN is invariant to rotations of the hand in the image. It also has a pool of graphical models, from which a combination of graphical models could be selected, conditioning on the input image. Belief propagation is performed on each graphical model separately, generating a set of marginal distributions, which are taken as the confidence maps of hand keypoint positions. Final confidence maps are obtained by aggregating these confidence maps together. We evaluate the R-MGMN on two public hand pose datasets. Experiment results show our model outperforms the state-of-the-art algorithm which is widely used in 2D hand pose estimation by a noticeable margin.



### CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2002.03797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2002.03797v1)
- **Published**: 2020-02-05 23:55:45+00:00
- **Updated**: 2020-02-05 23:55:45+00:00
- **Authors**: Hannaneh Barahouei Pasandi, Tamer Nadeem
- **Comment**: 6 pages, 4 figures, 2020 IEEE International Conference on Pervasive
  Computing and Communications Workshops (PerCom Workshops)
- **Journal**: None
- **Summary**: Today, video cameras are deployed in dense for monitoring physical places e.g., city, industrial, or agricultural sites. In the current systems, each camera node sends its feed to a cloud server individually. However, this approach suffers from several hurdles including higher computation cost, large bandwidth requirement for analyzing the enormous data, and privacy concerns. In dense deployment, video nodes typically demonstrate a significant spatio-temporal correlation. To overcome these obstacles in current approaches, this paper introduces CONVINCE, a new approach to look at the network cameras as a collective entity that enables collaborative video analytics pipeline among cameras. CONVINCE aims at 1) reducing the computation cost and bandwidth requirements by leveraging spatio-temporal correlations among cameras in eliminating redundant frames intelligently, and ii) improving vision algorithms' accuracy by enabling collaborative knowledge sharing among relevant cameras. Our results demonstrate that CONVINCE achieves an object identification accuracy of $\sim$91\%, by transmitting only about $\sim$25\% of all the recorded frames.



