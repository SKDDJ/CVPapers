# Arxiv Papers in cs.CV on 2020-02-02
### Depth Map Estimation of Dynamic Scenes Using Prior Depth Information
- **Arxiv ID**: http://arxiv.org/abs/2002.00297v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00297v1)
- **Published**: 2020-02-02 01:04:27+00:00
- **Updated**: 2020-02-02 01:04:27+00:00
- **Authors**: James Noraky, Vivienne Sze
- **Comment**: None
- **Journal**: None
- **Summary**: Depth information is useful for many applications. Active depth sensors are appealing because they obtain dense and accurate depth maps. However, due to issues that range from power constraints to multi-sensor interference, these sensors cannot always be continuously used. To overcome this limitation, we propose an algorithm that estimates depth maps using concurrently collected images and a previously measured depth map for dynamic scenes, where both the camera and objects in the scene may be independently moving. To estimate depth in these scenarios, our algorithm models the dynamic scene motion using independent and rigid motions. It then uses the previous depth map to efficiently estimate these rigid motions and obtain a new depth map. Our goal is to balance the acquisition of depth between the active depth sensor and computation, without incurring a large computational cost. Thus, we leverage the prior depth information to avoid computationally expensive operations like dense optical flow estimation or segmentation used in similar approaches. Our approach can obtain dense depth maps at up to real-time (30 FPS) on a standard laptop computer, which is orders of magnitude faster than similar approaches. When evaluated using RGB-D datasets of various dynamic scenes, our approach estimates depth maps with a mean relative error of 2.5% while reducing the active depth sensor usage by over 90%.



### 3D Object Detection on Point Clouds using Local Ground-aware and Adaptive Representation of scenes' surface
- **Arxiv ID**: http://arxiv.org/abs/2002.00336v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00336v2)
- **Published**: 2020-02-02 05:42:23+00:00
- **Updated**: 2020-06-26 22:13:24+00:00
- **Authors**: Arun CS Kumar, Disha Ahuja, Ashwath Aithal
- **Comment**: None
- **Journal**: None
- **Summary**: A novel, adaptive ground-aware, and cost-effective 3D Object Detection pipeline is proposed. The ground surface representation introduced in this paper, in comparison to its uni-planar counterparts (methods that model the surface of a whole 3D scene using single plane), is far more accurate while being ~10x faster. The novelty of the ground representation lies both in the way in which the ground surface of the scene is represented in Lidar perception problems, as well as in the (cost-efficient) way in which it is computed. Furthermore, the proposed object detection pipeline builds on the traditional two-stage object detection models by incorporating the ability to dynamically reason the surface of the scene, ultimately achieving a new state-of-the-art 3D object detection performance among the two-stage Lidar Object Detection pipelines.



### Adversarial Generation of Continuous Implicit Shape Representations
- **Arxiv ID**: http://arxiv.org/abs/2002.00349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00349v2)
- **Published**: 2020-02-02 08:20:42+00:00
- **Updated**: 2020-03-26 07:45:33+00:00
- **Authors**: Marian Kleineberg, Matthias Fey, Frank Weichert
- **Comment**: Published in Eurographics 2020 - Short Papers
- **Journal**: None
- **Summary**: This work presents a generative adversarial architecture for generating three-dimensional shapes based on signed distance representations. While the deep generation of shapes has been mostly tackled by voxel and surface point cloud approaches, our generator learns to approximate the signed distance for any point in space given prior latent information. Although structurally similar to generative point cloud approaches, this formulation can be evaluated with arbitrary point density during inference, leading to fine-grained details in generated outputs. Furthermore, we study the effects of using either progressively growing voxel- or point-processing networks as discriminators, and propose a refinement scheme to strengthen the generator's capabilities in modeling the zero iso-surface decision boundary of shapes. We train our approach on the ShapeNet benchmark dataset and validate, both quantitatively and qualitatively, its performance in generating realistic 3D shapes.



### Interpreting video features: a comparison of 3D convolutional networks and convolutional LSTM networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00367v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00367v2)
- **Published**: 2020-02-02 11:27:07+00:00
- **Updated**: 2020-07-10 14:32:22+00:00
- **Authors**: Joonatan Mänttäri, Sofia Broomé, John Folkesson, Hedvig Kjellström
- **Comment**: None
- **Journal**: None
- **Summary**: A number of techniques for interpretability have been presented for deep learning in computer vision, typically with the goal of understanding what the networks have based their classification on. However, interpretability for deep video architectures is still in its infancy and we do not yet have a clear concept of how to decode spatiotemporal features. In this paper, we present a study comparing how 3D convolutional networks and convolutional LSTM networks learn features across temporally dependent frames. This is the first comparison of two video models that both convolve to learn spatial features but have principally different methods of modeling time. Additionally, we extend the concept of meaningful perturbation introduced by \cite{MeaningFulPert} to the temporal dimension, to identify the temporal part of a sequence most meaningful to the network for a classification decision. Our findings indicate that the 3D convolutional model concentrates on shorter events in the input sequence, and places its spatial focus on fewer, contiguous areas.



### A Novel Graph based Trajectory Predictor with Pseudo Oracle
- **Arxiv ID**: http://arxiv.org/abs/2002.00391v2
- **DOI**: 10.1109/TNNLS.2021.3084143
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00391v2)
- **Published**: 2020-02-02 13:40:47+00:00
- **Updated**: 2021-06-18 07:24:45+00:00
- **Authors**: Biao Yang, Guocheng Yan, Pin Wang, Chingyao Chan, Xiang Song, Yang Chen
- **Comment**: 17 apges, 8 figures
- **Journal**: already published by TNNLS 2021
- **Summary**: Pedestrian trajectory prediction in dynamic scenes remains a challenging and critical problem in numerous applications, such as self-driving cars and socially aware robots. Challenges concentrate on capturing pedestrians' motion patterns and social interactions, as well as handling the future uncertainties. Recent studies focus on modeling pedestrians' motion patterns with recurrent neural networks, capturing social interactions with pooling-based or graph-based methods, and handling future uncertainties by using random Gaussian noise as the latent variable. However, they do not integrate specific obstacle avoidance experience (OAE) that may improve prediction performance. For example, pedestrians' future trajectories are always influenced by others in front. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo Oracle), an encoder-decoder-based method conditioned on pedestrians' future behaviors. Pedestrians' motion patterns are encoded with a long short-term memory unit, which introduces the temporal attention to highlight specific time steps. Their interactions are captured by a graph-based attention mechanism, which draws OAE into the data-driven learning process of graph attention. Future uncertainties are handled by generating multi-modal outputs with an informative latent variable. Such a variable is generated by a novel pseudo oracle predictor, which minimizes the knowledge gap between historical and ground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and Stanford Drone datasets, and the results demonstrate state-of-the-art performance. Besides, the qualitative evaluations show successful cases of handling sudden motion changes in the future. Such findings indicate that GTPPO can peek into the future.



### Towards Deep Machine Reasoning: a Prototype-based Deep Neural Network with Decision Tree Inference
- **Arxiv ID**: http://arxiv.org/abs/2002.03776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03776v1)
- **Published**: 2020-02-02 14:11:07+00:00
- **Updated**: 2020-02-02 14:11:07+00:00
- **Authors**: Plamen Angelov, Eduardo Soares
- **Comment**: Submitted to the IEEE Joint Conference on Neural Networks (IJCNN -
  2020)
- **Journal**: None
- **Summary**: In this paper we introduce the DMR -- a prototype-based method and network architecture for deep learning which is using a decision tree (DT)-based inference and synthetic data to balance the classes. It builds upon the recently introduced xDNN method addressing more complex multi-class problems, specifically when classes are highly imbalanced. DMR moves away from a direct decision based on all classes towards a layered DT of pair-wise class comparisons. In addition, it forces the prototypes to be balanced between classes regardless of possible class imbalances of the training data. It has two novel mechanisms, namely i) using a DT to determine the winning class label, and ii) balancing the classes by synthesizing data around the prototypes determined from the available training data. As a result, we improved significantly the performance of the resulting fully explainable DNN as evidenced by the best reported result on the well know benchmark problem Caltech-101 surpassing our own recently published "world record". Furthermore, we also achieved another "world record" for another very hard benchmark problem, namely Caltech-256 as well as surpassed the results of other approaches on Faces-1999 problem. In summary, we propose a new approach specifically advantageous for imbalanced multi-class problems that achieved two world records on well known hard benchmark problems and the best result on another problem in terms of accuracy. Moreover, DMR offers full explainability, does not require GPUs and can continue to learn from new data by adding new prototypes preserving the previous ones but not requiring full retraining.



### 3D Shape Segmentation with Geometric Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.00397v1
- **DOI**: 10.1007/978-3-030-30642-7_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00397v1)
- **Published**: 2020-02-02 14:11:16+00:00
- **Updated**: 2020-02-02 14:11:16+00:00
- **Authors**: Davide Boscaini, Fabio Poiesi
- **Comment**: None
- **Journal**: None
- **Summary**: The semantic segmentation of 3D shapes with a high-density of vertices could be impractical due to large memory requirements. To make this problem computationally tractable, we propose a neural-network based approach that produces 3D augmented views of the 3D shape to solve the whole segmentation as sub-segmentation problems. 3D augmented views are obtained by projecting vertices and normals of a 3D shape onto 2D regular grids taken from different viewpoints around the shape. These 3D views are then processed by a Convolutional Neural Network to produce a probability distribution function (pdf) over the set of the semantic classes for each vertex. These pdfs are then re-projected on the original 3D shape and postprocessed using contextual information through Conditional Random Fields. We validate our approach using 3D shapes of publicly available datasets and of real objects that are reconstructed using photogrammetry techniques. We compare our approach against state-of-the-art alternatives.



### Music2Dance: DanceNet for Music-driven Dance Generation
- **Arxiv ID**: http://arxiv.org/abs/2002.03761v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.03761v2)
- **Published**: 2020-02-02 17:18:31+00:00
- **Updated**: 2020-03-10 18:32:15+00:00
- **Authors**: Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, Yangang Wang
- **Comment**: Our results are shown at https://youtu.be/bTHSrfEHcG8
- **Journal**: None
- **Summary**: Synthesize human motions from music, i.e., music to dance, is appealing and attracts lots of research interests in recent years. It is challenging due to not only the requirement of realistic and complex human motions for dance, but more importantly, the synthesized motions should be consistent with the style, rhythm and melody of the music. In this paper, we propose a novel autoregressive generative model, DanceNet, to take the style, rhythm and melody of music as the control signals to generate 3D dance motions with high realism and diversity. To boost the performance of our proposed model, we capture several synchronized music-dance pairs by professional dancers, and build a high-quality music-dance pair dataset. Experiments have demonstrated that the proposed method can achieve the state-of-the-art results.



### Simultaneous Left Atrium Anatomy and Scar Segmentations via Deep Learning in Multiview Information with Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.00440v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.00440v1)
- **Published**: 2020-02-02 18:03:44+00:00
- **Updated**: 2020-02-02 18:03:44+00:00
- **Authors**: Guang Yang, Jun Chen, Zhifan Gao, Shuo Li, Hao Ni, Elsa Angelini, Tom Wong, Raad Mohiaddin, Eva Nyktari, Ricardo Wage, Lei Xu, Yanping Zhang, Xiuquan Du, Heye Zhang, David Firmin, Jennifer Keegan
- **Comment**: 34 pages, 10 figures, 7 tables, accepted by Future Generation
  Computer Systems journal
- **Journal**: None
- **Summary**: Three-dimensional late gadolinium enhanced (LGE) cardiac MR (CMR) of left atrial scar in patients with atrial fibrillation (AF) has recently emerged as a promising technique to stratify patients, to guide ablation therapy and to predict treatment success. This requires a segmentation of the high intensity scar tissue and also a segmentation of the left atrium (LA) anatomy, the latter usually being derived from a separate bright-blood acquisition. Performing both segmentations automatically from a single 3D LGE CMR acquisition would eliminate the need for an additional acquisition and avoid subsequent registration issues. In this paper, we propose a joint segmentation method based on multiview two-task (MVTT) recursive attention model working directly on 3D LGE CMR images to segment the LA (and proximal pulmonary veins) and to delineate the scar on the same dataset. Using our MVTT recursive attention model, both the LA anatomy and scar can be segmented accurately (mean Dice score of 93% for the LA anatomy and 87% for the scar segmentations) and efficiently (~0.27 seconds to simultaneously segment the LA anatomy and scars directly from the 3D LGE CMR dataset with 60-68 2D slices). Compared to conventional unsupervised learning and other state-of-the-art deep learning based methods, the proposed MVTT model achieved excellent results, leading to an automatic generation of a patient-specific anatomical model combined with scar segmentation for patients in AF.



### Regularizing Reasons for Outfit Evaluation with Gradient Penalty
- **Arxiv ID**: http://arxiv.org/abs/2002.00460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00460v1)
- **Published**: 2020-02-02 18:59:55+00:00
- **Updated**: 2020-02-02 18:59:55+00:00
- **Authors**: Xingxing Zou, Zhizhong Li, Ke Bai, Dahua Lin, Waikeung Wong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we build an outfit evaluation system which provides feedbacks consisting of a judgment with a convincing explanation. The system is trained in a supervised manner which faithfully follows the domain knowledge in fashion. We create the EVALUATION3 dataset which is annotated with judgment, the decisive reason for the judgment, and all corresponding attributes (e.g. print, silhouette, and material \etc.). In the training process, features of all attributes in an outfit are first extracted and then concatenated as the input for the intra-factor compatibility net. Then, the inter-factor compatibility net is used to compute the loss for judgment. We penalize the gradient of judgment loss of so that our Grad-CAM-like reason is regularized to be consistent with the labeled reason. In inference, according to the obtained information of judgment, reason, and attributes, a user-friendly explanation sentence is generated by the pre-defined templates. The experimental results show that the obtained network combines the advantages of high precision and good interpretation.



### Effect of Analysis Window and Feature Selection on Classification of Hand Movements Using EMG Signal
- **Arxiv ID**: http://arxiv.org/abs/2002.00461v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2002.00461v4)
- **Published**: 2020-02-02 19:03:23+00:00
- **Updated**: 2020-08-11 18:21:46+00:00
- **Authors**: Asad Ullah, Sarwan Ali, Imdadullah Khan, Muhammad Asad Khan, Safiullah Faizullah
- **Comment**: Accepted to Intelligent Systems Conference (IntelliSys) 2020
- **Journal**: None
- **Summary**: Electromyography (EMG) signals have been successfully employed for driving prosthetic limbs of a single or double degree of freedom. This principle works by using the amplitude of the EMG signals to decide between one or two simpler movements. This method underperforms as compare to the contemporary advances done at the mechanical, electronics, and robotics end, and it lacks intuition. Recently, research on myoelectric control based on pattern recognition (PR) shows promising results with the aid of machine learning classifiers. Using the approach termed as, EMG-PR, EMG signals are divided into analysis windows, and features are extracted for each window. These features are then fed to the machine learning classifiers as input. By offering multiple class movements and intuitive control, this method has the potential to power an amputated subject to perform everyday life movements. In this paper, we investigate the effect of the analysis window and feature selection on classification accuracy of different hand and wrist movements using time-domain features. We show that effective data preprocessing and optimum feature selection helps to improve the classification accuracy of hand movements. We use publicly available hand and wrist gesture dataset of $40$ intact subjects for experimentation. Results computed using different classification algorithms show that the proposed preprocessing and features selection outperforms the baseline and achieve up to $98\%$ classification accuracy.



### Neural Sign Language Translation by Learning Tokenization
- **Arxiv ID**: http://arxiv.org/abs/2002.00479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00479v2)
- **Published**: 2020-02-02 19:59:30+00:00
- **Updated**: 2020-03-04 15:28:18+00:00
- **Authors**: Alptekin Orbay, Lale Akarun
- **Comment**: 8 pages, 2 figures, FG 2020
- **Journal**: None
- **Summary**: Sign Language Translation has attained considerable success recently, raising hopes for improved communication with the Deaf. A pre-processing step called tokenization improves the success of translations. Tokens can be learned from sign videos if supervised data is available. However, data annotation at the gloss level is costly, and annotated data is scarce. The paper utilizes Adversarial, Multitask, Transfer Learning to search for semi-supervised tokenization approaches without burden of additional labeling. It provides extensive experiments to compare all the methods in different settings to conduct a deeper analysis. In the case of no additional target annotation besides sentences, the proposed methodology attains 13.25 BLUE-4 and 36.28 ROUGE scores which improves the current state-of-the-art by 4 points in BLUE-4 and 5 points in ROUGE.



### Non-linear Neurons with Human-like Apical Dendrite Activations
- **Arxiv ID**: http://arxiv.org/abs/2003.03229v5
- **DOI**: 10.1007/s10489-023-04921-w
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.03229v5)
- **Published**: 2020-02-02 21:09:39+00:00
- **Updated**: 2023-08-10 21:19:32+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Nicolae-Catalin Ristea, Nicu Sebe
- **Comment**: Accepted for publication in Applied Intelligence
- **Journal**: None
- **Summary**: In order to classify linearly non-separable data, neurons are typically organized into multi-layer neural networks that are equipped with at least one hidden layer. Inspired by some recent discoveries in neuroscience, we propose a new model of artificial neuron along with a novel activation function enabling the learning of nonlinear decision boundaries using a single neuron. We show that a standard neuron followed by our novel apical dendrite activation (ADA) can learn the XOR logical function with 100% accuracy. Furthermore, we conduct experiments on six benchmark data sets from computer vision, signal processing and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST, Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and Swish, for various neural network architectures, e.g. one-hidden-layer or two-hidden-layer multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs) such as LeNet, VGG, ResNet and Character-level CNN. We obtain further performance improvements when we change the standard model of the neuron with our pyramidal neuron with apical dendrite activations (PyNADA). Our code is available at: https://github.com/raduionescu/pynada.



