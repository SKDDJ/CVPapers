# Arxiv Papers in cs.CV on 2020-02-26
### Deep Learning and Statistical Models for Time-Critical Pedestrian Behaviour Prediction
- **Arxiv ID**: http://arxiv.org/abs/2002.11226v1
- **DOI**: 10.1007/978-3-030-36808-1_50
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11226v1)
- **Published**: 2020-02-26 00:05:19+00:00
- **Updated**: 2020-02-26 00:05:19+00:00
- **Authors**: Joel Janek Dabrowski, Johan Pieter de Villiers, Ashfaqur Rahman, Conrad Beyers
- **Comment**: None
- **Journal**: In: Gedeon T., Wong K., Lee M. (eds) Neural Information
  Processing. ICONIP 2019. Communications in Computer and Information Science,
  vol 1142. Springer, Cham
- **Summary**: The time it takes for a classifier to make an accurate prediction can be crucial in many behaviour recognition problems. For example, an autonomous vehicle should detect hazardous pedestrian behaviour early enough for it to take appropriate measures. In this context, we compare the switching linear dynamical system (SLDS) and a three-layered bi-directional long short-term memory (LSTM) neural network, which are applied to infer pedestrian behaviour from motion tracks. We show that, though the neural network model achieves an accuracy of 80%, it requires long sequences to achieve this (100 samples or more). The SLDS, has a lower accuracy of 74%, but it achieves this result with short sequences (10 samples). To our knowledge, such a comparison on sequence length has not been considered in the literature before. The results provide a key intuition of the suitability of the models in time-critical problems.



### Max-Affine Spline Insights into Deep Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.11912v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CG, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.11912v1)
- **Published**: 2020-02-26 00:20:02+00:00
- **Updated**: 2020-02-26 00:20:02+00:00
- **Authors**: Randall Balestriero, Sebastien Paris, Richard Baraniuk
- **Comment**: None
- **Journal**: None
- **Summary**: We connect a large class of Generative Deep Networks (GDNs) with spline operators in order to derive their properties, limitations, and new opportunities. By characterizing the latent space partition, dimension and angularity of the generated manifold, we relate the manifold dimension and approximation error to the sample size. The manifold-per-region affine subspace defines a local coordinate basis; we provide necessary and sufficient conditions relating those basis vectors with disentanglement. We also derive the output probability density mapped onto the generated manifold in terms of the latent space density, which enables the computation of key statistics such as its Shannon entropy. This finding also enables the computation of the GDN likelihood, which provides a new mechanism for model comparison as well as providing a quality measure for (generated) samples under the learned distribution. We demonstrate how low entropy and/or multimodal distributions are not naturally modeled by DGNs and are a cause of training instabilities.



### Personalized Taste and Cuisine Preference Modeling via Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2003.08769v1)
- **Published**: 2020-02-26 01:07:56+00:00
- **Updated**: 2020-02-26 01:07:56+00:00
- **Authors**: Nitish Nag, Bindu Rajanna, Ramesh Jain
- **Comment**: None
- **Journal**: None
- **Summary**: With the exponential growth in the usage of social media to share live updates about life, taking pictures has become an unavoidable phenomenon. Individuals unknowingly create a unique knowledge base with these images. The food images, in particular, are of interest as they contain a plethora of information. From the image metadata and using computer vision tools, we can extract distinct insights for each user to build a personal profile. Using the underlying connection between cuisines and their inherent tastes, we attempt to develop such a profile for an individual based solely on the images of his food. Our study provides insights about an individual's inclination towards particular cuisines. Interpreting these insights can lead to the development of a more precise recommendation system. Such a system would avoid the generic approach in favor of a personalized recommendation system.



### Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/2002.11244v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11244v2)
- **Published**: 2020-02-26 01:08:42+00:00
- **Updated**: 2020-03-16 10:44:41+00:00
- **Authors**: Yoonsik Kim, Jae Woong Soh, Gu Yong Park, Nam Ik Cho
- **Comment**: CVPR accepted paper. The paper will be updated according to
  reviewers' comments
- **Journal**: None
- **Summary**: Real-noise denoising is a challenging task because the statistics of real-noise do not follow the normal distribution, and they are also spatially and temporally changing. In order to cope with various and complex real-noise, we propose a well-generalized denoising architecture and a transfer learning scheme. Specifically, we adopt an adaptive instance normalization to build a denoiser, which can regularize the feature map and prevent the network from overfitting to the training set. We also introduce a transfer learning scheme that transfers knowledge learned from synthetic-noise data to the real-noise denoiser. From the proposed transfer learning, the synthetic-noise denoiser can learn general features from various synthetic-noise data, and the real-noise denoiser can learn the real-noise characteristics from real data. From the experiments, we find that the proposed denoising method has great generalization ability, such that our network trained with synthetic-noise achieves the best performance for Darmstadt Noise Dataset (DND) among the methods from published papers. We can also see that the proposed transfer learning scheme robustly works for real-noise images through the learning with a very small number of labeled data.



### Super-Resolving Commercial Satellite Imagery Using Realistic Training Data
- **Arxiv ID**: http://arxiv.org/abs/2002.11248v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11248v1)
- **Published**: 2020-02-26 01:18:51+00:00
- **Updated**: 2020-02-26 01:18:51+00:00
- **Authors**: Xiang Zhu, Hossein Talebi, Xinwei Shi, Feng Yang, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning based single image super-resolution, the degradation model is embedded in training data generation. However, most existing satellite image super-resolution methods use a simple down-sampling model with a fixed kernel to create training images. These methods work fine on synthetic data, but do not perform well on real satellite images. We propose a realistic training data generation model for commercial satellite imagery products, which includes not only the imaging process on satellites but also the post-process on the ground. We also propose a convolutional neural network optimized for satellite images. Experiments show that the proposed training data generation model is able to improve super-resolution performance on real satellite images.



### Multi-Attribute Guided Painting Generation
- **Arxiv ID**: http://arxiv.org/abs/2002.11261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11261v1)
- **Published**: 2020-02-26 02:22:23+00:00
- **Updated**: 2020-02-26 02:22:23+00:00
- **Authors**: Minxuan Lin, Yingying Deng, Fan Tang, Weiming Dong, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Controllable painting generation plays a pivotal role in image stylization. Currently, the control way of style transfer is subject to exemplar-based reference or a random one-hot vector guidance. Few works focus on decoupling the intrinsic properties of painting as control conditions, e.g., artist, genre and period. Under this circumstance, we propose a novel framework adopting multiple attributes from the painting to control the stylized results. An asymmetrical cycle structure is equipped to preserve the fidelity, associating with style preserving and attribute regression loss to keep the unique distinction of colors and textures between domains. Several qualitative and quantitative results demonstrate the effect of the combinations of multiple attributes and achieve satisfactory performance.



### Learning Light Field Angular Super-Resolution via a Geometry-Aware Network
- **Arxiv ID**: http://arxiv.org/abs/2002.11263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11263v1)
- **Published**: 2020-02-26 02:36:57+00:00
- **Updated**: 2020-02-26 02:36:57+00:00
- **Authors**: Jing Jin, Junhui Hou, Hui Yuan, Sam Kwong
- **Comment**: This paper was accepted by AAAI 2020
- **Journal**: None
- **Summary**: The acquisition of light field images with high angular resolution is costly. Although many methods have been proposed to improve the angular resolution of a sparsely-sampled light field, they always focus on the light field with a small baseline, which is captured by a consumer light field camera. By making full use of the intrinsic \textit{geometry} information of light fields, in this paper we propose an end-to-end learning-based approach aiming at angularly super-resolving a sparsely-sampled light field with a large baseline. Our model consists of two learnable modules and a physically-based module. Specifically, it includes a depth estimation module for explicitly modeling the scene geometry, a physically-based warping for novel views synthesis, and a light field blending module specifically designed for light field reconstruction. Moreover, we introduce a novel loss function to promote the preservation of the light field parallax structure. Experimental results over various light field datasets including large baseline light field images demonstrate the significant superiority of our method when compared with state-of-the-art ones, i.e., our method improves the PSNR of the second best method up to 2 dB in average, while saves the execution time 48$\times$. In addition, our method preserves the light field parallax structure better.



### Generalized Product Quantization Network for Semi-supervised Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.11281v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11281v3)
- **Published**: 2020-02-26 03:36:32+00:00
- **Updated**: 2020-06-12 00:21:29+00:00
- **Authors**: Young Kyun Jang, Nam Ik Cho
- **Comment**: 10 pages, 10 figures, Computer Vision and Pattern Recognition (CVPR)
  2020 accpeted paper
- **Journal**: None
- **Summary**: Image retrieval methods that employ hashing or vector quantization have achieved great success by taking advantage of deep learning. However, these approaches do not meet expectations unless expensive label information is sufficient. To resolve this issue, we propose the first quantization-based semi-supervised image retrieval scheme: Generalized Product Quantization (GPQ) network. We design a novel metric learning strategy that preserves semantic similarity between labeled data, and employ entropy regularization term to fully exploit inherent potentials of unlabeled data. Our solution increases the generalization capacity of the quantization network, which allows overcoming previous limitations in the retrieval community. Extensive experimental results demonstrate that GPQ yields state-of-the-art performance on large-scale real image benchmark datasets.



### Adversarial Ranking Attack and Defense
- **Arxiv ID**: http://arxiv.org/abs/2002.11293v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.11293v3)
- **Published**: 2020-02-26 04:03:14+00:00
- **Updated**: 2020-07-06 08:49:00+00:00
- **Authors**: Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, Gang Hua
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously. Our adversarial ranking attacks and defense are evaluated on datasets including MNIST, Fashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Meanwhile, the system robustness can be moderately improved with our defense. Furthermore, the transferable and universal properties of our adversary illustrate the possibility of realistic black-box attack.



### Dam Burst: A region-merging-based image segmentation method
- **Arxiv ID**: http://arxiv.org/abs/2003.04797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.04797v1)
- **Published**: 2020-02-26 04:07:48+00:00
- **Updated**: 2020-02-26 04:07:48+00:00
- **Authors**: Rui Tang, Wenlong Song, Xiaoping Guan, Huibin Ge, Deke Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Until now, all single level segmentation algorithms except CNN-based ones lead to over segmentation. And CNN-based segmentation algorithms have their own problems. To avoid over segmentation, multiple thresholds of criteria are adopted in region merging process to produce hierarchical segmentation results. However, there still has extreme over segmentation in the low level of the hierarchy, and outstanding tiny objects are merged to their large adjacencies in the high level of the hierarchy. This paper proposes a region-merging-based image segmentation method that we call it Dam Burst. As a single level segmentation algorithm, this method avoids over segmentation and retains details by the same time. It is named because of that it simulates a flooding from underground destroys dams between water-pools. We treat edge detection results as strengthening structure of a dam if it is on the dam. To simulate a flooding from underground, regions are merged by ascending order of the average gra-dient inside the region.



### Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data
- **Arxiv ID**: http://arxiv.org/abs/2002.11297v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11297v2)
- **Published**: 2020-02-26 04:18:25+00:00
- **Updated**: 2020-03-31 18:13:34+00:00
- **Authors**: Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consisting of small image datasets. However, many recent methods based on neural networks rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define a-priori, and its selection can easily bias the learning. We base our work on a popular method ODIN, proposing two strategies for freeing it from the needs of tuning with OoD data, while improving its OoD detection performance. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help in detection performance. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when ODIN-like strategies do or do not work.



### Self-supervised Image Enhancement Network: Training with Low Light Images Only
- **Arxiv ID**: http://arxiv.org/abs/2002.11300v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68U10, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2002.11300v1)
- **Published**: 2020-02-26 04:39:07+00:00
- **Updated**: 2020-02-26 04:39:07+00:00
- **Authors**: Yu Zhang, Xiaoguang Di, Bin Zhang, Chunhui Wang
- **Comment**: 14 pages,13 figures
- **Journal**: None
- **Summary**: This paper proposes a self-supervised low light image enhancement method based on deep learning. Inspired by information entropy theory and Retinex model, we proposed a maximum entropy based Retinex model. With this model, a very simple network can separate the illumination and reflectance, and the network can be trained with low light images only. We introduce a constraint that the maximum channel of the reflectance conforms to the maximum channel of the low light image and its entropy should be largest in our model to achieve self-supervised learning. Our model is very simple and does not rely on any well-designed data set (even one low light image can complete the training). The network only needs minute-level training to achieve image enhancement. It can be proved through experiments that the proposed method has reached the state-of-the-art in terms of processing speed and effect.



### From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation (VIN)
- **Arxiv ID**: http://arxiv.org/abs/2002.11310v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11310v3)
- **Published**: 2020-02-26 05:27:30+00:00
- **Updated**: 2022-07-02 17:01:29+00:00
- **Authors**: Xin Ye, Yezhou Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Indoor Navigation (VIN) task has drawn increasing attention from the data-driven machine learning communities especially with the recently reported success from learning-based methods. Due to the innate complexity of this task, researchers have tried approaching the problem from a variety of different angles, the full scope of which has not yet been captured within an overarching report. This survey first summarizes the representative work of learning-based approaches for the VIN task and then identifies and discusses lingering issues impeding the VIN performance, as well as motivates future research in these key areas worth exploring for the community.



### Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.11318v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11318v5)
- **Published**: 2020-02-26 06:25:06+00:00
- **Updated**: 2021-11-10 18:26:37+00:00
- **Authors**: Sandesh Kamath, Amit Deshpande, K V Subrahmanyam, Vineeth N Balasubramanian
- **Comment**: Accepted NeurIPS 2021. Preliminary version consisting early
  experimental results was presented in ICML 2018 Workshop on "Towards learning
  with limited labels: Equivariance, Invariance,and Beyond" as "Understanding
  Adversarial Robustness of Symmetric Networks"
- **Journal**: None
- **Summary**: (Non-)robustness of neural networks to small, adversarial pixel-wise perturbations, and as more recently shown, to even random spatial transformations (e.g., translations, rotations) entreats both theoretical and empirical understanding. Spatial robustness to random translations and rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs) and training augmentation, whereas adversarial robustness is typically achieved by adversarial training. In this paper, we prove a quantitative trade-off between spatial and adversarial robustness in a simple statistical setting. We complement this empirically by showing that: (a) as the spatial robustness of equivariant models improves by training augmentation with progressively larger transformations, their adversarial robustness worsens progressively, and (b) as the state-of-the-art robust models are adversarially trained with progressively larger pixel-wise perturbations, their spatial robustness drops progressively. Towards achieving pareto-optimality in this trade-off, we propose a method based on curriculum learning that trains gradually on more difficult perturbations (both spatial and adversarial) to improve spatial and adversarial robustness simultaneously.



### ParasNet: Fast Parasites Detection with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.11327v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11327v2)
- **Published**: 2020-02-26 06:58:17+00:00
- **Updated**: 2020-03-24 04:58:58+00:00
- **Authors**: X. F. Xu, S. Talbot, T. Selvaraja
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has dramatically improved the performance in many application areas such as image classification, object detection, speech recognition, drug discovery and etc since 2012. Where deep learning algorithms promise to discover the intricate hidden information inside the data by leveraging the large dataset, advanced model and computing power. Although deep learning techniques show medical expert level performance in a lot of medical applications, but some of the applications are still not explored or under explored due to the variation of the species. In this work, we studied the bright field based cell level Cryptosporidium and Giardia detection in the drink water with deep learning. Our experimental demonstrates that the new developed deep learning-based algorithm surpassed the handcrafted SVM based algorithm with above 97 percentage in accuracy and 700+fps in speed on embedded Jetson TX2 platform. Our research will lead to real-time and high accuracy label-free cell level Cryptosporidium and Giardia detection system in the future.



### Refined Gate: A Simple and Effective Gating Mechanism for Recurrent Units
- **Arxiv ID**: http://arxiv.org/abs/2002.11338v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.11338v2)
- **Published**: 2020-02-26 07:51:38+00:00
- **Updated**: 2020-05-26 13:59:48+00:00
- **Authors**: Zhanzhan Cheng, Yunlu Xu, Mingjian Cheng, Yu Qiao, Shiliang Pu, Yi Niu, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent neural network (RNN) has been widely studied in sequence learning tasks, while the mainstream models (e.g., LSTM and GRU) rely on the gating mechanism (in control of how information flows between hidden states). However, the vanilla gates in RNN (e.g., the input gate in LSTM) suffer from the problem of gate undertraining, which can be caused by various factors, such as the saturating activation functions, the gate layouts (e.g., the gate number and gating functions), or even the suboptimal memory state etc.. Those may result in failures of learning gating switch roles and thus the weak performance. In this paper, we propose a new gating mechanism within general gated recurrent neural networks to handle this issue. Specifically, the proposed gates directly short connect the extracted input features to the outputs of vanilla gates, denoted as refined gates. The refining mechanism allows enhancing gradient back-propagation as well as extending the gating activation scope, which can guide RNN to reach possibly deeper minima. We verify the proposed gating mechanism on three popular types of gated RNNs including LSTM, GRU and MGU. Extensive experiments on 3 synthetic tasks, 3 language modeling tasks and 5 scene text recognition benchmarks demonstrate the effectiveness of our method.



### Multi-source Domain Adaptation in the Deep Learning Era: A Systematic Survey
- **Arxiv ID**: http://arxiv.org/abs/2002.12169v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12169v1)
- **Published**: 2020-02-26 08:07:58+00:00
- **Updated**: 2020-02-26 08:07:58+00:00
- **Authors**: Sicheng Zhao, Bo Li, Colorado Reed, Pengfei Xu, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: In many practical applications, it is often difficult and expensive to obtain enough large-scale labeled data to train deep neural networks to their full capability. Therefore, transferring the learned knowledge from a separate, labeled source domain to an unlabeled or sparsely labeled target domain becomes an appealing alternative. However, direct transfer often results in significant performance decay due to domain shift. Domain adaptation (DA) addresses this problem by minimizing the impact of domain shift between the source and target domains. Multi-source domain adaptation (MDA) is a powerful extension in which the labeled data may be collected from multiple sources with different distributions. Due to the success of DA methods and the prevalence of multi-source data, MDA has attracted increasing attention in both academia and industry. In this survey, we define various MDA strategies and summarize available datasets for evaluation. We also compare modern MDA methods in the deep learning era, including latent space transformation and intermediate domain generation. Finally, we discuss future research directions for MDA.



### Rethinking the Route Towards Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2002.11359v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11359v2)
- **Published**: 2020-02-26 08:54:20+00:00
- **Updated**: 2020-03-03 03:12:56+00:00
- **Authors**: Chen-Lin Zhang, Yun-Hao Cao, Jianxin Wu
- **Comment**: Accepted by CVPR 2020; Corrected some typo in the paper; The code
  repository is https://github.com/tzzcl/PSOL
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels. Previous methods often try to utilize feature maps and classification weights to localize objects using image level annotations indirectly. In this paper, we demonstrate that weakly supervised object localization should be divided into two parts: class-agnostic object localization and object classification. For class-agnostic object localization, we should use class-agnostic methods to generate noisy pseudo annotations and then perform bounding box regression on them without class labels. We propose the pseudo supervised object localization (PSOL) method as a new way to solve WSOL. Our PSOL models have good transferability across different datasets without fine-tuning. With generated pseudo bounding boxes, we achieve 58.00% localization accuracy on ImageNet and 74.97% localization accuracy on CUB-200, which have a large edge over previous models.



### Unsupervised Temporal Video Segmentation as an Auxiliary Task for Predicting the Remaining Surgery Duration
- **Arxiv ID**: http://arxiv.org/abs/2002.11367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11367v1)
- **Published**: 2020-02-26 09:13:39+00:00
- **Updated**: 2020-02-26 09:13:39+00:00
- **Authors**: Dominik Rivoir, Sebastian Bodenstedt, Felix von Bechtolsheim, Marius Distler, Jürgen Weitz, Stefanie Speidel
- **Comment**: None
- **Journal**: OR 2.0 Context-Aware Operating Theaters and Machine Learning in
  Clinical Neuroimaging (2019) 29-37
- **Summary**: Estimating the remaining surgery duration (RSD) during surgical procedures can be useful for OR planning and anesthesia dose estimation. With the recent success of deep learning-based methods in computer vision, several neural network approaches have been proposed for fully automatic RSD prediction based solely on visual data from the endoscopic camera. We investigate whether RSD prediction can be improved using unsupervised temporal video segmentation as an auxiliary learning task. As opposed to previous work, which presented supervised surgical phase recognition as auxiliary task, we avoid the need for manual annotations by proposing a similar but unsupervised learning objective which clusters video sequences into temporally coherent segments. In multiple experimental setups, results obtained by learning the auxiliary task are incorporated into a deep RSD model through feature extraction, pretraining or regularization. Further, we propose a novel loss function for RSD training which attempts to counteract unfavorable characteristics of the RSD ground truth. Using our unsupervised method as an auxiliary task for RSD training, we outperform other self-supervised methods and are comparable to the supervised state-of-the-art. Combined with the novel RSD loss, we slightly outperform the supervised approach.



### PuzzleNet: Scene Text Detection by Segment Context Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.11371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11371v1)
- **Published**: 2020-02-26 09:21:05+00:00
- **Updated**: 2020-02-26 09:21:05+00:00
- **Authors**: Hao Liu, Antai Guo, Deqiang Jiang, Yiqing Hu, Bo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a series of decomposition-based scene text detection methods has achieved impressive progress by decomposing challenging text regions into pieces and linking them in a bottom-up manner. However, most of them merely focus on linking independent text pieces while the context information is underestimated. In the puzzle game, the solver often put pieces together in a logical way according to the contextual information of each piece, in order to arrive at the correct solution. Inspired by it, we propose a novel decomposition-based method, termed Puzzle Networks (PuzzleNet), to address the challenging scene text detection task in this work. PuzzleNet consists of the Segment Proposal Network (SPN) that predicts the candidate text segments fitting arbitrary shape of text region, and the two-branch Multiple-Similarity Graph Convolutional Network (MSGCN) that models both appearance and geometry correlations between each segment to its contextual ones. By building segments as context graphs, MSGCN effectively employs segment context to predict combinations of segments. Final detections of polygon shape are produced by merging segments according to the predicted combinations. Evaluations on three benchmark datasets, ICDAR15, MSRA-TD500 and SCUT-CTW1500, have demonstrated that our method can achieve better or comparable performance than current state-of-the-arts, which is beneficial from the exploitation of segment context graph.



### Adversarial Attack on Deep Product Quantization Network for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.11374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11374v1)
- **Published**: 2020-02-26 09:25:58+00:00
- **Updated**: 2020-02-26 09:25:58+00:00
- **Authors**: Yan Feng, Bin Chen, Tao Dai, Shutao Xia
- **Comment**: Accepted at AAAI20
- **Journal**: None
- **Summary**: Deep product quantization network (DPQN) has recently received much attention in fast image retrieval tasks due to its efficiency of encoding high-dimensional visual features especially when dealing with large-scale datasets. Recent studies show that deep neural networks (DNNs) are vulnerable to input with small and maliciously designed perturbations (a.k.a., adversarial examples). This phenomenon raises the concern of security issues for DPQN in the testing/deploying stage as well. However, little effort has been devoted to investigating how adversarial examples affect DPQN. To this end, we propose product quantization adversarial generation (PQ-AG), a simple yet effective method to generate adversarial examples for product quantization based retrieval systems. PQ-AG aims to generate imperceptible adversarial perturbations for query images to form adversarial queries, whose nearest neighbors from a targeted product quantizaiton model are not semantically related to those from the original queries. Extensive experiments show that our PQ-AQ successfully creates adversarial examples to mislead targeted product quantization retrieval models. Besides, we found that our PQ-AG significantly degrades retrieval performance in both white-box and black-box settings.



### Controllable Descendant Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2002.11376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11376v1)
- **Published**: 2020-02-26 09:33:41+00:00
- **Updated**: 2020-02-26 09:33:41+00:00
- **Authors**: Yong Zhang, Le Li, Zhilei Liu, Baoyuan Wu, Yanbo Fan, Zhifeng Li
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Kinship face synthesis is an interesting topic raised to answer questions like "what will your future children look like?". Published approaches to this topic are limited. Most of the existing methods train models for one-versus-one kin relation, which only consider one parent face and one child face by directly using an auto-encoder without any explicit control over the resemblance of the synthesized face to the parent face. In this paper, we propose a novel method for controllable descendant face synthesis, which models two-versus-one kin relation between two parent faces and one child face. Our model consists of an inheritance module and an attribute enhancement module, where the former is designed for accurate control over the resemblance between the synthesized face and parent faces, and the latter is designed for control over age and gender. As there is no large scale database with father-mother-child kinship annotation, we propose an effective strategy to train the model without using the ground truth descendant faces. No carefully designed image pairs are required for learning except only age and gender labels of training faces. We conduct comprehensive experimental evaluations on three public benchmark databases, which demonstrates encouraging results.



### CheXpedition: Investigating Generalization Challenges for Translation of Chest X-Ray Algorithms to the Clinical Setting
- **Arxiv ID**: http://arxiv.org/abs/2002.11379v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.11379v2)
- **Published**: 2020-02-26 09:44:21+00:00
- **Updated**: 2020-03-11 07:15:57+00:00
- **Authors**: Pranav Rajpurkar, Anirudh Joshi, Anuj Pareek, Phil Chen, Amirhossein Kiani, Jeremy Irvin, Andrew Y. Ng, Matthew P. Lungren
- **Comment**: Accepted as workshop paper at ACM Conference on Health, Inference,
  and Learning (CHIL) 2020
- **Journal**: None
- **Summary**: Although there have been several recent advances in the application of deep learning algorithms to chest x-ray interpretation, we identify three major challenges for the translation of chest x-ray algorithms to the clinical setting. We examine the performance of the top 10 performing models on the CheXpert challenge leaderboard on three tasks: (1) TB detection, (2) pathology detection on photos of chest x-rays, and (3) pathology detection on data from an external institution. First, we find that the top 10 chest x-ray models on the CheXpert competition achieve an average AUC of 0.851 on the task of detecting TB on two public TB datasets without fine-tuning or including the TB labels in training data. Second, we find that the average performance of the models on photos of x-rays (AUC = 0.916) is similar to their performance on the original chest x-ray images (AUC = 0.924). Third, we find that the models tested on an external dataset either perform comparably to or exceed the average performance of radiologists. We believe that our investigation will inform rapid translation of deep learning algorithms to safe and effective clinical decision support tools that can be validated prospectively with large impact studies and clinical trials.



### Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning via Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/2002.12168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12168v1)
- **Published**: 2020-02-26 10:02:32+00:00
- **Updated**: 2020-02-26 10:02:32+00:00
- **Authors**: Jilin Hu, Jianbing Shen, Bin Yang, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional neural networks~(GCNs) have recently demonstrated promising results on graph-based semi-supervised classification, but little work has been done to explore their theoretical properties. Recently, several deep neural networks, e.g., fully connected and convolutional neural networks, with infinite hidden units have been proved to be equivalent to Gaussian processes~(GPs). To exploit both the powerful representational capacity of GCNs and the great expressive power of GPs, we investigate similar properties of infinitely wide GCNs. More specifically, we propose a GP regression model via GCNs~(GPGC) for graph-based semi-supervised learning. In the process, we formulate the kernel matrix computation of GPGC in an iterative analytical form. Finally, we derive a conditional distribution for the labels of unobserved nodes based on the graph structure, labels for the observed nodes, and the feature matrix of all the nodes. We conduct extensive experiments to evaluate the semi-supervised classification performance of GPGC and demonstrate that it outperforms other state-of-the-art methods by a clear margin on all the datasets while being efficient.



### Unpaired Image Super-Resolution using Pseudo-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2002.11397v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11397v1)
- **Published**: 2020-02-26 10:30:52+00:00
- **Updated**: 2020-02-26 10:30:52+00:00
- **Authors**: Shunta Maeda
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: In most studies on learning-based image super-resolution (SR), the paired training dataset is created by downscaling high-resolution (HR) images with a predetermined operation (e.g., bicubic). However, these methods fail to super-resolve real-world low-resolution (LR) images, for which the degradation process is much more complicated and unknown. In this paper, we propose an unpaired SR method using a generative adversarial network that does not require a paired/aligned training dataset. Our network consists of an unpaired kernel/noise correction network and a pseudo-paired SR network. The correction network removes noise and adjusts the kernel of the inputted LR image; then, the corrected clean LR image is upscaled by the SR network. In the training phase, the correction network also produces a pseudo-clean LR image from the inputted HR image, and then a mapping from the pseudo-clean LR image to the inputted HR image is learned by the SR network in a paired manner. Because our SR network is independent of the correction network, well-studied existing network architectures and pixel-wise loss functions can be integrated with the proposed framework. Experiments on diverse datasets show that the proposed method is superior to existing solutions to the unpaired SR problem.



### Force-Ultrasound Fusion: Bringing Spine Robotic-US to the Next "Level"
- **Arxiv ID**: http://arxiv.org/abs/2002.11404v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11404v1)
- **Published**: 2020-02-26 10:49:53+00:00
- **Updated**: 2020-02-26 10:49:53+00:00
- **Authors**: Maria Tirindelli, Maria Victorova, Javier Esteban, Seong Tae Kim, David Navarro-Alarcon, Yong Ping Zheng, Nassir Navab
- **Comment**: Under Review in IEEE Robotics and Automation Letters (RA-L) with IROS
  2020
- **Journal**: None
- **Summary**: Spine injections are commonly performed in several clinical procedures. The localization of the target vertebral level (i.e. the position of a vertebra in a spine) is typically done by back palpation or under X-ray guidance, yielding either higher chances of procedure failure or exposure to ionizing radiation. Preliminary studies have been conducted in the literature, suggesting that ultrasound imaging may be a precise and safe alternative to X-ray for spine level detection. However, ultrasound data are noisy and complicated to interpret. In this study, a robotic-ultrasound approach for automatic vertebral level detection is introduced. The method relies on the fusion of ultrasound and force data, thus providing both "tactile" and visual feedback during the procedure, which results in higher performances in presence of data corruption. A robotic arm automatically scans the volunteer's back along the spine by using force-ultrasound data to locate vertebral levels. The occurrences of vertebral levels are visible on the force trace as peaks, which are enhanced by properly controlling the force applied by the robot on the patient back. Ultrasound data are processed with a Deep Learning method to extract a 1D signal modelling the probabilities of having a vertebra at each location along the spine. Processed force and ultrasound data are fused using a 1D Convolutional Network to compute the location of the vertebral levels. The method is compared to pure image and pure force-based methods for vertebral level counting, showing improved performance. In particular, the fusion method is able to correctly classify 100% of the vertebral levels in the test set, while pure image and pure force-based method could only classify 80% and 90% vertebrae, respectively. The potential of the proposed method is evaluated in an exemplary simulated clinical application.



### Disentangling Image Distortions in Deep Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2002.11409v2
- **DOI**: 10.1016/j.patrec.2021.05.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11409v2)
- **Published**: 2020-02-26 11:02:13+00:00
- **Updated**: 2020-06-16 13:04:12+00:00
- **Authors**: Simone Bianco, Luigi Celona, Paolo Napoletano
- **Comment**: None
- **Journal**: Elsevier Pattern Recognition Letters, 148 (2021), 128-135
- **Summary**: Previous literature suggests that perceptual similarity is an emergent property shared across deep visual representations. Experiments conducted on a dataset of human-judged image distortions have proven that deep features outperform classic perceptual metrics. In this work we take a further step in the direction of a broader understanding of such property by analyzing the capability of deep visual representations to intrinsically characterize different types of image distortions. To this end, we firstly generate a number of synthetically distorted images and then we analyze the features extracted by different layers of different Deep Neural Networks. We observe that a dimension-reduced representation of the features extracted from a given layer permits to efficiently separate types of distortions in the feature space. Moreover, each network layer exhibits a different ability to separate between different types of distortions, and this ability varies according to the network architecture. Finally, we evaluate the exploitation of features taken from the layer that better separates image distortions for: i) reduced-reference image quality assessment, and ii) distortion types and severity levels characterization on both single and multiple distortion databases. Results achieved on both tasks suggest that deep visual representations can be unsupervisedly employed to efficiently characterize various image distortions.



### Performance Evaluation of Deep Generative Models for Generating Hand-Written Character Images
- **Arxiv ID**: http://arxiv.org/abs/2002.11424v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11424v1)
- **Published**: 2020-02-26 12:09:06+00:00
- **Updated**: 2020-02-26 12:09:06+00:00
- **Authors**: Tanmoy Mondal, LE Thi Thuy Trang, Mickaël Coustaty, Jean-Marc Ogier
- **Comment**: None
- **Journal**: None
- **Summary**: There have been many work in the literature on generation of various kinds of images such as Hand-Written characters (MNIST dataset), scene images (CIFAR-10 dataset), various objects images (ImageNet dataset), road signboard images (SVHN dataset) etc. Unfortunately, there have been very limited amount of work done in the domain of document image processing. Automatic image generation can lead to the enormous increase of labeled datasets with the help of only limited amount of labeled data. Various kinds of Deep generative models can be primarily divided into two categories. First category is auto-encoder (AE) and the second one is Generative Adversarial Networks (GANs). In this paper, we have evaluated various kinds of AE as well as GANs and have compared their performances on hand-written digits dataset (MNIST) and also on historical hand-written character dataset of Indonesian BALI language. Moreover, these generated characters are recognized by using character recognition tool for calculating the statistical performance of these generated characters with respect to original character images.



### Deform-GAN:An Unsupervised Learning Model for Deformable Registration
- **Arxiv ID**: http://arxiv.org/abs/2002.11430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11430v1)
- **Published**: 2020-02-26 12:20:46+00:00
- **Updated**: 2020-02-26 12:20:46+00:00
- **Authors**: Xiaoyue Zhang, Weijian Jian, Yu Chen, Shihting Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable registration is one of the most challenging task in the field of medical image analysis, especially for the alignment between different sequences and modalities. In this paper, a non-rigid registration method is proposed for 3D medical images leveraging unsupervised learning. To the best of our knowledge, this is the first attempt to introduce gradient loss into deep-learning-based registration. The proposed gradient loss is robust across sequences and modals for large deformation. Besides, adversarial learning approach is used to transfer multi-modal similarity to mono-modal similarity and improve the precision. Neither ground-truth nor manual labeling is required during training. We evaluated our network on a 3D brain registration task comprehensively. The experiments demonstrate that the proposed method can cope with the data which has non-functional intensity relations, noise and blur. Our approach outperforms other methods especially in accuracy and speed.



### Efficient Semantic Video Segmentation with Per-frame Inference
- **Arxiv ID**: http://arxiv.org/abs/2002.11433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11433v2)
- **Published**: 2020-02-26 12:24:32+00:00
- **Updated**: 2020-07-17 12:57:29+00:00
- **Authors**: Yifan Liu, Chunhua Shen, Changqian Yu, Jingdong Wang
- **Comment**: Accepted to Proc. Eur. Conf. Computer Vision (ECCV), 2020
- **Journal**: None
- **Summary**: For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results for a video sequence. Advanced methods take into considerations the correlations in the video sequence, e.g., by propagating the results to the neighboring frames using optical flow, or extracting the frame representations with other frames, which may lead to inaccurate results or unbalanced latency. In this work, we process efficient semantic video segmentation in a per-frame fashion during the inference process. Different from previous per-frame models, we explicitly consider the temporal consistency among frames as extra constraints during the training process and embed the temporal consistency into the segmentation network. Therefore, in the inference process, we can process each frame independently with no latency, and improve the temporal consistency with no extra computational cost and post-processing. We employ compact models for real-time execution. To narrow the performance gap between compact models and large models, new knowledge distillation methods are designed. Our results outperform previous keyframe based methods with a better trade-off between the accuracy and the inference speed on popular benchmarks, including the Cityscapes and Camvid. The temporal consistency is also improved compared with corresponding baselines which are trained with each frame independently. Code is available at: https://tinyurl.com/segment-video



### Towards Interpretable Semantic Segmentation via Gradient-weighted Class Activation Mapping
- **Arxiv ID**: http://arxiv.org/abs/2002.11434v1
- **DOI**: 10.1609/aaai.v34i10.7244
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11434v1)
- **Published**: 2020-02-26 12:32:40+00:00
- **Updated**: 2020-02-26 12:32:40+00:00
- **Authors**: Kira Vinogradova, Alexandr Dibrov, Gene Myers
- **Comment**: 2 pages, 2 figures. AAAI 2020 camera-ready
- **Journal**: Proceedings of the Thirty-Fourth AAAI Conference on Artificial
  Intelligence, New York, USA, Feb 2020
- **Summary**: Convolutional neural networks have become state-of-the-art in a wide range of image recognition tasks. The interpretation of their predictions, however, is an active area of research. Whereas various interpretation methods have been suggested for image classification, the interpretation of image segmentation still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a gradient-based method for interpreting semantic segmentation. Our method is an extension of the widely-used Grad-CAM method, applied locally to produce heatmaps showing the relevance of individual pixels for semantic segmentation.



### Recognizing Handwritten Mathematical Expressions as LaTex Sequences Using a Multiscale Robust Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2003.00817v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00817v1)
- **Published**: 2020-02-26 12:39:06+00:00
- **Updated**: 2020-02-26 12:39:06+00:00
- **Authors**: Hongyu Wang, Guangcun Shan
- **Comment**: 6 figures, 5 tables, 20 pages
- **Journal**: None
- **Summary**: In this paper, a robust multiscale neural network is proposed to recognize handwritten mathematical expressions and output LaTeX sequences, which can effectively and correctly focus on where each step of output should be concerned and has a positive effect on analyzing the two-dimensional structure of handwritten mathematical expressions and identifying different mathematical symbols in a long expression. With the addition of visualization, the model's recognition process is shown in detail. In addition, our model achieved 49.459% and 46.062% ExpRate on the public CROHME 2014 and CROHME 2016 datasets. The present model results suggest that the state-of-the-art model has better robustness, fewer errors, and higher accuracy.



### A Comprehensive Approach to Unsupervised Embedding Learning based on AND Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2002.12158v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12158v1)
- **Published**: 2020-02-26 13:22:04+00:00
- **Updated**: 2020-02-26 13:22:04+00:00
- **Authors**: Sungwon Han, Yizhan Xu, Sungwon Park, Meeyoung Cha, Cheng-Te Li
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised embedding learning aims to extract good representation from data without the need for any manual labels, which has been a critical challenge in many supervised learning tasks. This paper proposes a new unsupervised embedding approach, called Super-AND, which extends the current state-of-the-art model. Super-AND has its unique set of losses that can gather similar samples nearby within a low-density space while keeping invariant features intact against data augmentation. Super-AND outperforms all existing approaches and achieves an accuracy of 89.2% on the image classification task for CIFAR-10. We discuss the practical implications of this method in assisting semi-supervised tasks.



### Region of Interest Identification for Brain Tumors in Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2002.11509v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11509v1)
- **Published**: 2020-02-26 14:10:40+00:00
- **Updated**: 2020-02-26 14:10:40+00:00
- **Authors**: Fateme Mostafaie, Reihaneh Teimouri, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Glioma is a common type of brain tumor, and accurate detection of it plays a vital role in the diagnosis and treatment process. Despite advances in medical image analyzing, accurate tumor segmentation in brain magnetic resonance (MR) images remains a challenge due to variations in tumor texture, position, and shape. In this paper, we propose a fast, automated method, with light computational complexity, to find the smallest bounding box around the tumor region. This region-of-interest can be used as a preprocessing step in training networks for subregion tumor segmentation. By adopting the outputs of this algorithm, redundant information is removed; hence the network can focus on learning notable features related to subregions' classes. The proposed method has six main stages, in which the brain segmentation is the most vital step. Expectation-maximization (EM) and K-means algorithms are used for brain segmentation. The proposed method is evaluated on the BraTS 2015 dataset, and the average gained DICE score is 0.73, which is an acceptable result for this application.



### PointTrackNet: An End-to-End Network For 3-D Object Detection and Tracking From Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2002.11559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.11559v1)
- **Published**: 2020-02-26 15:19:28+00:00
- **Updated**: 2020-02-26 15:19:28+00:00
- **Authors**: Sukai Wang, Yuxiang Sun, Chengju Liu, Ming Liu
- **Comment**: 7 pages, ICRA-RAL2020 accepted
- **Journal**: None
- **Summary**: Recent machine learning-based multi-object tracking (MOT) frameworks are becoming popular for 3-D point clouds. Most traditional tracking approaches use filters (e.g., Kalman filter or particle filter) to predict object locations in a time sequence, however, they are vulnerable to extreme motion conditions, such as sudden braking and turning. In this letter, we propose PointTrackNet, an end-to-end 3-D object detection and tracking network, to generate foreground masks, 3-D bounding boxes, and point-wise tracking association displacements for each detected object. The network merely takes as input two adjacent point-cloud frames. Experimental results on the KITTI tracking dataset show competitive results over the state-of-the-arts, especially in the irregularly and rapidly changing scenarios.



### Object Relational Graph with Teacher-Recommended Learning for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2002.11566v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2002.11566v1)
- **Published**: 2020-02-26 15:34:52+00:00
- **Updated**: 2020-02-26 15:34:52+00:00
- **Authors**: Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, Zhengjun Zha
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Taking full advantage of the information from both vision and language is critical for the video captioning task. Existing models lack adequate visual representation due to the neglect of interaction between object, and sufficient training for content-related words due to long-tailed problems. In this paper, we propose a complete video captioning system including both a novel model and an effective training strategy. Specifically, we propose an object relational graph (ORG) based encoder, which captures more detailed interaction features to enrich visual representation. Meanwhile, we design a teacher-recommended learning (TRL) method to make full use of the successful external language model (ELM) to integrate the abundant linguistic knowledge into the caption model. The ELM generates more semantically similar word proposals which extend the ground-truth words used for training to deal with the long-tailed problem. Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the proposed ORG-TRL system achieves state-of-the-art performance. Extensive ablation studies and visualizations illustrate the effectiveness of our system.



### Revisiting Ensembles in an Adversarial Context: Improving Natural Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2002.11572v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.11572v1)
- **Published**: 2020-02-26 15:45:58+00:00
- **Updated**: 2020-02-26 15:45:58+00:00
- **Authors**: Aditya Saligrama, Guillaume Leclerc
- **Comment**: 5 pages, accepted to ICLR 2020 Workshop on Towards Trustworthy ML:
  Rethinking Security and Privacy for ML
- **Journal**: None
- **Summary**: A necessary characteristic for the deployment of deep learning models in real world applications is resistance to small adversarial perturbations while maintaining accuracy on non-malicious inputs. While robust training provides models that exhibit better adversarial accuracy than standard models, there is still a significant gap in natural accuracy between robust and non-robust models which we aim to bridge. We consider a number of ensemble methods designed to mitigate this performance difference. Our key insight is that model trained to withstand small attacks, when ensembled, can often withstand significantly larger attacks, and this concept can in turn be leveraged to optimize natural accuracy. We consider two schemes, one that combines predictions from several randomly initialized robust models, and the other that fuses features from robust and standard models.



### Automatically Searching for U-Net Image Translator Architecture
- **Arxiv ID**: http://arxiv.org/abs/2002.11581v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11581v1)
- **Published**: 2020-02-26 16:05:23+00:00
- **Updated**: 2020-02-26 16:05:23+00:00
- **Authors**: Han Shu, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image translators have been successfully applied to many important low level image processing tasks. However, classical network architecture of image translator like U-Net, is borrowed from other vision tasks like biomedical image segmentation. This straightforward adaptation may not be optimal and could cause redundancy in the network structure. In this paper, we propose an automatic architecture searching method for image translator. By utilizing evolutionary algorithm, we investigate a more efficient network architecture which costs less computation resources and achieves better performance than the original one. Extensive qualitative and quantitative experiments are conducted to demonstrate the effectiveness of the proposed method. Moreover, we transplant the searched network architecture to other datasets which are not involved in the architecture searching procedure. Efficiency of the searched architecture on these datasets further demonstrates the generalization of the method.



### Evolving Losses for Unsupervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.12177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12177v1)
- **Published**: 2020-02-26 16:56:07+00:00
- **Updated**: 2020-02-26 16:56:07+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo
- **Comment**: arXiv admin note: text overlap with arXiv:1906.03248
- **Journal**: CVPR 2020
- **Summary**: We present a new method to learn video representations from large-scale unlabeled video data. Ideally, this representation will be generic and transferable, directly usable for new tasks such as action recognition and zero or few-shot learning. We formulate unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are shared across different modalities via distillation. Further, we introduce the concept of loss function evolution by using an evolutionary search algorithm to automatically find optimal combination of loss functions capturing many (self-supervised) tasks and modalities. Thirdly, we propose an unsupervised representation evaluation metric using distribution matching to a large unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised constraint, which is not guided by any labeling, produces similar results to weakly-supervised, task-specific ones. The proposed unsupervised representation learning results in a single RGB network and outperforms previous methods. Notably, it is also more effective than several label-based methods (e.g., ImageNet), with the exception of large, fully labeled video datasets.



### Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2002.11616v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV, I.2; I.4.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2002.11616v1)
- **Published**: 2020-02-26 16:59:48+00:00
- **Updated**: 2020-02-26 16:59:48+00:00
- **Authors**: Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach, Chenliang Xu
- **Comment**: This work is accepted in CVPR 2020. The source code and pre-trained
  model are available on https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.
  12 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we explore the space-time video super-resolution task, which aims to generate a high-resolution (HR) slow-motion video from a low frame rate (LFR), low-resolution (LR) video. A simple solution is to split it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). However, temporal interpolation and spatial super-resolution are intra-related in this task. Two-stage methods cannot fully take advantage of the natural property. In addition, state-of-the-art VFI or VSR networks require a large frame-synthesis or reconstruction module for predicting high-quality video frames, which makes the two-stage methods have large model sizes and thus be time-consuming. To overcome the problems, we propose a one-stage space-time video super-resolution framework, which directly synthesizes an HR slow-motion video from an LFR, LR video. Rather than synthesizing missing LR video frames as VFI networks do, we firstly temporally interpolate LR frame features in missing LR video frames capturing local temporal contexts by the proposed feature temporal interpolation network. Then, we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously for better leveraging global temporal contexts. Finally, a deep reconstruction network is adopted to predict HR slow-motion video frames. Extensive experiments on benchmark datasets demonstrate that the proposed method not only achieves better quantitative and qualitative performance but also is more than three times faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR and DAIN+RBPN.



### Dynamic Graph Correlation Learning for Disease Diagnosis with Incomplete Labels
- **Arxiv ID**: http://arxiv.org/abs/2002.11629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11629v2)
- **Published**: 2020-02-26 17:10:48+00:00
- **Updated**: 2020-02-28 09:07:04+00:00
- **Authors**: Daizong Liu, Shuangjie Xu, Pan Zhou, Kun He, Wei Wei, Zichuan Xu
- **Comment**: Because of the novel coronavirus (2019-nCoV) in Wuhan, China, we can
  not get the codes in the locked lab. Some authors do not agree to submit. We
  will re-submit it once we get our codes
- **Journal**: None
- **Summary**: Disease diagnosis on chest X-ray images is a challenging multi-label classification task. Previous works generally classify the diseases independently on the input image without considering any correlation among diseases. However, such correlation actually exists, for example, Pleural Effusion is more likely to appear when Pneumothorax is present. In this work, we propose a Disease Diagnosis Graph Convolutional Network (DD-GCN) that presents a novel view of investigating the inter-dependency among different diseases by using a dynamic learnable adjacency matrix in graph structure to improve the diagnosis accuracy. To learn more natural and reliable correlation relationship, we feed each node with the image-level individual feature map corresponding to each type of disease. To our knowledge, our method is the first to build a graph over the feature maps with a dynamic adjacency matrix for correlation learning. To further deal with a practical issue of incomplete labels, DD-GCN also utilizes an adaptive loss and a curriculum learning strategy to train the model on incomplete labels. Experimental results on two popular chest X-ray (CXR) datasets show that our prediction accuracy outperforms state-of-the-arts, and the learned graph adjacency matrix establishes the correlation representations of different diseases, which is consistent with expert experience. In addition, we apply an ablation study to demonstrate the effectiveness of each component in DD-GCN.



### A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems
- **Arxiv ID**: http://arxiv.org/abs/2002.11644v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11644v3)
- **Published**: 2020-02-26 17:18:53+00:00
- **Updated**: 2020-03-20 16:53:14+00:00
- **Authors**: Hugo Proença, Ehsan Yaghoubi, Pendar Alirezazadeh
- **Comment**: 10 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: This paper describes one objective function for learning semantically coherent feature embeddings in multi-output classification problems, i.e., when the response variables have dimension higher than one. In particular, we consider the problems of identity retrieval and soft biometrics labelling in visual surveillance environments, which have been attracting growing interests. Inspired by the triplet loss [34] function, we propose a generalization that: 1) defines a metric that considers the number of agreeing labels between pairs of elements; and 2) disregards the notion of anchor, replacing d(A1, A2) < d(A1, B) by d(A, B) < d(C, D), for A, B, C, D distance constraints, according to the number of agreeing labels between pairs. As the triplet loss formulation, our proposal also privileges small distances between positive pairs, but at the same time explicitly enforces that the distance between other pairs corresponds directly to their similarity in terms of agreeing labels. This yields feature embeddings with a strong correspondence between the classes centroids and their semantic descriptions, i.e., where elements are closer to others that share some of their labels than to elements with fully disjoint labels membership. As practical effect, the proposed loss can be seen as particularly suitable for performing joint coarse (soft label) + fine (ID) inference, based on simple rules as k-neighbours, which is a novelty with respect to previous related loss functions. Also, in opposition to its triplet counterpart, the proposed loss is agnostic with regard to any demanding criteria for mining learning instances (such as the semi-hard pairs). Our experiments were carried out in five different datasets (BIODI, LFW, IJB-A, Megaface and PETA) and validate our assumptions, showing highly promising results.



### Inceptive Event Time-Surfaces for Object Classification Using Neuromorphic Cameras
- **Arxiv ID**: http://arxiv.org/abs/2002.11656v1
- **DOI**: 10.1007/978-3-030-27272-2_35
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11656v1)
- **Published**: 2020-02-26 17:36:27+00:00
- **Updated**: 2020-02-26 17:36:27+00:00
- **Authors**: R Wes Baldwin, Mohammed Almatrafi, Jason R Kaufman, Vijayan Asari, Keigo Hirakawa
- **Comment**: None
- **Journal**: Image Analysis and Recognition. ICIAR 2019. Lecture Notes in
  Computer Science, vol 11663. Springer, Cham
- **Summary**: This paper presents a novel fusion of low-level approaches for dimensionality reduction into an effective approach for high-level objects in neuromorphic camera data called Inceptive Event Time-Surfaces (IETS). IETSs overcome several limitations of conventional time-surfaces by increasing robustness to noise, promoting spatial consistency, and improving the temporal localization of (moving) edges. Combining IETS with transfer learning improves state-of-the-art performance on the challenging problem of object classification utilizing event camera data.



### Pedestrian Models for Autonomous Driving Part I: Low-Level Models, from Sensing to Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.11669v2
- **DOI**: 10.1109/TITS.2020.3006768
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.11669v2)
- **Published**: 2020-02-26 17:57:42+00:00
- **Updated**: 2020-07-20 14:42:27+00:00
- **Authors**: Fanta Camara, Nicola Bellotto, Serhan Cosar, Dimitris Nathanael, Matthias Althoff, Jingyuan Wu, Johannes Ruenz, André Dietrich, Charles W. Fox
- **Comment**: Accepted for publication in the IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, interactive motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behaviour as well as detecting and tracking them. This narrative review article is Part I of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychology models, from the perspective of an AV designer. This self-contained Part I covers the lower levels of this stack, from sensing, through detection and recognition, up to tracking of pedestrians. Technologies at these levels are found to be mature and available as foundations for use in high-level systems, such as behaviour modelling, prediction and interaction control.



### Graphcore C2 Card performance for image-based deep learning application: A Report
- **Arxiv ID**: http://arxiv.org/abs/2002.11670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.11670v2)
- **Published**: 2020-02-26 17:58:24+00:00
- **Updated**: 2020-02-27 22:17:19+00:00
- **Authors**: Ilyes Kacher, Maxime Portaz, Hicham Randrianarivo, Sylvain Peyronnet
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Graphcore has introduced an IPU Processor for accelerating machine learning applications. The architecture of the processor has been designed to achieve state of the art performance on current machine intelligence models for both training and inference.   In this paper, we report on a benchmark in which we have evaluated the performance of IPU processors on deep neural networks for inference. We focus on deep vision models such as ResNeXt. We report the observed latency, throughput and energy efficiency.



### CLARA: Clinical Report Auto-completion
- **Arxiv ID**: http://arxiv.org/abs/2002.11701v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11701v2)
- **Published**: 2020-02-26 18:45:00+00:00
- **Updated**: 2020-03-04 13:32:52+00:00
- **Authors**: Siddharth Biswal, Cao Xiao, Lucas M. Glass, M. Brandon Westover, Jimeng Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Generating clinical reports from raw recordings such as X-rays and electroencephalogram (EEG) is an essential and routine task for doctors. However, it is often time-consuming to write accurate and detailed reports. Most existing methods try to generate the whole reports from the raw input with limited success because 1) generated reports often contain errors that need manual review and correction, 2) it does not save time when doctors want to write additional information into the report, and 3) the generated reports are not customized based on individual doctors' preference. We propose {\it CL}inic{\it A}l {\it R}eport {\it A}uto-completion (CLARA), an interactive method that generates reports in a sentence by sentence fashion based on doctors' anchor words and partially completed sentences. CLARA searches for most relevant sentences from existing reports as the template for the current report. The retrieved sentences are sequentially modified by combining with the input feature representations to create the final report. In our experimental evaluation, CLARA achieved 0.393 CIDEr and 0.248 BLEU-4 on X-ray reports and 0.482 CIDEr and 0.491 BLEU-4 for EEG reports for sentence-level generation, which is up to 35% improvement over the best baseline. Also via our qualitative evaluation, CLARA is shown to produce reports which have a significantly higher level of approval by doctors in a user study (3.74 out of 5 for CLARA vs 2.52 out of 5 for the baseline).



### Sketch-to-Art: Synthesizing Stylized Art Images From Sketches
- **Arxiv ID**: http://arxiv.org/abs/2002.12888v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12888v3)
- **Published**: 2020-02-26 19:02:10+00:00
- **Updated**: 2020-10-02 17:20:25+00:00
- **Authors**: Bingchen Liu, Kunpeng Song, Ahmed Elgammal
- **Comment**: 24 pages
- **Journal**: ACCV 2020
- **Summary**: We propose a new approach for synthesizing fully detailed art-stylized images from sketches. Given a sketch, with no semantic tagging, and a reference image of a specific style, the model can synthesize meaningful details with colors and textures. The model consists of three modules designed explicitly for better artistic style capturing and generation. Based on a GAN framework, a dual-masked mechanism is introduced to enforce the content constraints (from the sketch), and a feature-map transformation technique is developed to strengthen the style consistency (to the reference image). Finally, an inverse procedure of instance-normalization is proposed to disentangle the style and content information, therefore yields better synthesis performance. Experiments demonstrate a significant qualitative and quantitative boost over baselines based on previous state-of-the-art techniques, adopted for the proposed process.



### Quantized Neural Network Inference with Precision Batching
- **Arxiv ID**: http://arxiv.org/abs/2003.00822v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2003.00822v1)
- **Published**: 2020-02-26 19:34:11+00:00
- **Updated**: 2020-02-26 19:34:11+00:00
- **Authors**: Maximilian Lam, Zachary Yedidia, Colby Banbury, Vijay Janapa Reddi
- **Comment**: None
- **Journal**: None
- **Summary**: We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.



### On Leveraging Pretrained GANs for Generation with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2002.11810v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11810v3)
- **Published**: 2020-02-26 21:53:36+00:00
- **Updated**: 2020-08-08 03:59:02+00:00
- **Authors**: Miaoyun Zhao, Yulai Cong, Lawrence Carin
- **Comment**: Accepted at ICML 2020
- **Journal**: None
- **Summary**: Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptually-distinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.



### Learning to Shadow Hand-drawn Sketches
- **Arxiv ID**: http://arxiv.org/abs/2002.11812v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.11812v2)
- **Published**: 2020-02-26 21:57:17+00:00
- **Updated**: 2020-04-02 23:12:21+00:00
- **Authors**: Qingyuan Zheng, Zhuoru Li, Adam Bargteil
- **Comment**: To appear in CVPR 2020 (Oral presentation)
- **Journal**: None
- **Summary**: We present a fully automatic method to generate detailed and accurate artistic shadows from pairs of line drawing sketches and lighting directions. We also contribute a new dataset of one thousand examples of pairs of line drawings and shadows that are tagged with lighting directions. Remarkably, the generated shadows quickly communicate the underlying 3D structure of the sketched scene. Consequently, the shadows generated by our approach can be used directly or as an excellent starting point for artists. We demonstrate that the deep learning network we propose takes a hand-drawn sketch, builds a 3D model in latent space, and renders the resulting shadows. The generated shadows respect the hand-drawn lines and underlying 3D space and contain sophisticated and accurate details, such as self-shadowing effects. Moreover, the generated shadows contain artistic effects, such as rim lighting or halos appearing from back lighting, that would be achievable with traditional 3D rendering methods.



### Improving Robustness of Deep-Learning-Based Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2002.11821v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11821v1)
- **Published**: 2020-02-26 22:12:36+00:00
- **Updated**: 2020-02-26 22:12:36+00:00
- **Authors**: Ankit Raj, Yoram Bresler, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different.



### Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level Optimization
- **Arxiv ID**: http://arxiv.org/abs/2002.11826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11826v1)
- **Published**: 2020-02-26 22:28:00+00:00
- **Updated**: 2020-02-26 22:28:00+00:00
- **Authors**: Shihao Jiang, Dylan Campbell, Miaomiao Liu, Stephen Gould, Richard Hartley
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of joint optical flow and camera motion estimation in rigid scenes by incorporating geometric constraints into an unsupervised deep learning framework. Unlike existing approaches which rely on brightness constancy and local smoothness for optical flow estimation, we exploit the global relationship between optical flow and camera motion using epipolar geometry. In particular, we formulate the prediction of optical flow and camera motion as a bi-level optimization problem, consisting of an upper-level problem to estimate the flow that conforms to the predicted camera motion, and a lower-level problem to estimate the camera motion given the predicted optical flow. We use implicit differentiation to enable back-propagation through the lower-level geometric optimization layer independent of its implementation, allowing end-to-end training of the network. With globally-enforced geometric constraints, we are able to improve the quality of the estimated optical flow in challenging scenarios and obtain better camera motion estimates compared to other unsupervised learning methods.



### Towards Universal Representation Learning for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.11841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11841v1)
- **Published**: 2020-02-26 23:29:57+00:00
- **Updated**: 2020-02-26 23:29:57+00:00
- **Authors**: Yichun Shi, Xiang Yu, Kihyuk Sohn, Manmohan Chandraker, Anil K. Jain
- **Comment**: to appear in CVPR 2020
- **Journal**: None
- **Summary**: Recognizing wild faces is extremely hard as they appear with all kinds of variations. Traditional methods either train with specifically annotated variation data from target domains, or by introducing unlabeled target variation data to adapt from the training data. Instead, we propose a universal representation learning framework that can deal with larger variation unseen in the given training data without leveraging target domain knowledge. We firstly synthesize training data alongside some semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly feeding the augmented data for training will not converge well as the newly introduced samples are mostly hard examples. We propose to split the feature embedding into multiple sub-embeddings, and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing variation classification loss and variation adversarial loss on different partitions of them. Experiments show that our method achieves top performance on general face recognition datasets such as LFW and MegaFace, while significantly better on extreme benchmarks such as TinyFace and IJB-S.



