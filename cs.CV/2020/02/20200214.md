# Arxiv Papers in cs.CV on 2020-02-14
### GSANet: Semantic Segmentation with Global and Selective Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.00830v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00830v1)
- **Published**: 2020-02-14 00:09:42+00:00
- **Updated**: 2020-02-14 00:09:42+00:00
- **Authors**: Qingfeng Liu, Mostafa El-Khamy, Dongwoon Bai, Jungwon Lee
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel deep learning architecture for semantic segmentation. The proposed Global and Selective Attention Network (GSANet) features Atrous Spatial Pyramid Pooling (ASPP) with a novel sparsemax global attention and a novel selective attention that deploys a condensation and diffusion mechanism to aggregate the multi-scale contextual information from the extracted deep features. A selective attention decoder is also proposed to process the GSA-ASPP outputs for optimizing the softmax volume. We are the first to benchmark the performance of semantic segmentation networks with the low-complexity feature extraction network (FXN) MobileNetEdge, that is optimized for low latency on edge devices. We show that GSANet can result in more accurate segmentation with MobileNetEdge, as well as with strong FXNs, such as Xception. GSANet improves the state-of-art semantic segmentation accuracy on both the ADE20k and the Cityscapes datasets.



### Remove Appearance Shift for Ultrasound Image Segmentation via Fast and Universal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2002.05844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05844v1)
- **Published**: 2020-02-14 02:00:57+00:00
- **Updated**: 2020-02-14 02:00:57+00:00
- **Authors**: Zhendong Liu, Xin Yang, Rui Gao, Shengfeng Liu, Haoran Dou, Shuangchi He, Yuhao Huang, Yankai Huang, Huanjia Luo, Yuanji Zhang, Yi Xiong, Dong Ni
- **Comment**: IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) suffer from the performance degradation when image appearance shift occurs, especially in ultrasound (US) image segmentation. In this paper, we propose a novel and intuitive framework to remove the appearance shift, and hence improve the generalization ability of DNNs. Our work has three highlights. First, we follow the spirit of universal style transfer to remove appearance shifts, which was not explored before for US images. Without sacrificing image structure details, it enables the arbitrary style-content transfer. Second, accelerated with Adaptive Instance Normalization block, our framework achieved real-time speed required in the clinical US scanning. Third, an efficient and effective style image selection strategy is proposed to ensure the target-style US image and testing content US image properly match each other. Experiments on two large US datasets demonstrate that our methods are superior to state-of-the-art methods on making DNNs robust against various appearance shifts.



### An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset
- **Arxiv ID**: http://arxiv.org/abs/2002.05878v2
- **DOI**: 10.3390/app10062046
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.05878v2)
- **Published**: 2020-02-14 05:28:15+00:00
- **Updated**: 2020-03-23 16:25:20+00:00
- **Authors**: Zhicheng Gu, Zhihao Li, Xuan Di, Rongye Shi
- **Comment**: None
- **Journal**: Applied Sciences 10(6) 2046, 2020
- **Summary**: The Waymo Open Dataset has been released recently, providing a platform to crowdsource some fundamental challenges for automated vehicles (AVs), such as 3D detection and tracking. While~the dataset provides a large amount of high-quality and multi-source driving information, people in academia are more interested in the underlying driving policy programmed in Waymo self-driving cars, which is inaccessible due to AV manufacturers' proprietary protection. Accordingly, academic researchers have to make various assumptions to implement AV components in their models or simulations, which may not represent the realistic interactions in real-world traffic. Thus, this paper introduces an approach to learn a long short-term memory (LSTM)-based model for imitating the behavior of Waymo's self-driving model. The proposed model has been evaluated based on Mean Absolute Error (MAE). The experimental results show that our model outperforms several baseline models in driving action prediction. In addition, a visualization tool is presented for verifying the performance of the model.



### Liver Segmentation in Abdominal CT Images via Auto-Context Neural Network and Self-Supervised Contour Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.05895v1
- **DOI**: 10.1016/j.artmed.2021.102023
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2002.05895v1)
- **Published**: 2020-02-14 07:32:45+00:00
- **Updated**: 2020-02-14 07:32:45+00:00
- **Authors**: Minyoung Chung, Jingyu Lee, Jeongjin Lee, Yeong-Gil Shin
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Accurate image segmentation of the liver is a challenging problem owing to its large shape variability and unclear boundaries. Although the applications of fully convolutional neural networks (CNNs) have shown groundbreaking results, limited studies have focused on the performance of generalization. In this study, we introduce a CNN for liver segmentation on abdominal computed tomography (CT) images that shows high generalization performance and accuracy. To improve the generalization performance, we initially propose an auto-context algorithm in a single CNN. The proposed auto-context neural network exploits an effective high-level residual estimation to obtain the shape prior. Identical dual paths are effectively trained to represent mutual complementary features for an accurate posterior analysis of a liver. Further, we extend our network by employing a self-supervised contour scheme. We trained sparse contour features by penalizing the ground-truth contour to focus more contour attentions on the failures. The experimental results show that the proposed network results in better accuracy when compared to the state-of-the-art networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT images for training and validation. Two-fold cross-validation is presented for a comparison with the state-of-the-art neural networks. Novel multiple N-fold cross-validations are conducted to verify the performance of generalization. The proposed network showed the best generalization performance among the networks. Additionally, we present a series of ablation experiments that comprehensively support the importance of the underlying concepts.



### A Survey on 3D Skeleton-Based Action Recognition Using Learning Method
- **Arxiv ID**: http://arxiv.org/abs/2002.05907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05907v1)
- **Published**: 2020-02-14 08:12:12+00:00
- **Updated**: 2020-02-14 08:12:12+00:00
- **Authors**: Bin Ren, Mengyuan Liu, Runwei Ding, Hong Liu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: 3D skeleton-based action recognition, owing to the latent advantages of skeleton, has been an active topic in computer vision. As a consequence, there are lots of impressive works including conventional handcraft feature based and learned feature based have been done over the years. However, previous surveys about action recognition mostly focus on the video or RGB data dominated methods, and the scanty existing reviews related to skeleton data mainly indicate the representation of skeleton data or performance of some classic techniques on a certain dataset. Besides, though deep learning methods has been applied to this field for years, there is no related reserach concern about an introduction or review from the perspective of deep learning architectures. To break those limitations, this survey firstly highlight the necessity of action recognition and the significance of 3D-skeleton data. Then a comprehensive introduction about Recurrent Neural Network(RNN)-based, Convolutional Neural Network(CNN)-based and Graph Convolutional Network(GCN)-based main stream action recognition techniques are illustrated in a data-driven manner. Finally, we give a brief talk about the biggest 3D skeleton dataset NTU-RGB+D and its new edition called NTU-RGB+D 120, accompanied with several existing top rank algorithms within those two datasets. To our best knowledge, this is the first research which give an overall discussion over deep learning-based action recognitin using 3D skeleton data.



### End-to-end Learning of Object Motion Estimation from Retinal Events for Event-based Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.05911v1
- **DOI**: 10.1609/aaai.v34i07.6625
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05911v1)
- **Published**: 2020-02-14 08:19:50+00:00
- **Updated**: 2020-02-14 08:19:50+00:00
- **Authors**: Haosheng Chen, David Suter, Qiangqiang Wu, Hanzi Wang
- **Comment**: 9 pages, 3 figures
- **Journal**: Proceedings of the Thirty-Fourth AAAI Conference on Artificial
  Intelligence (AAAI '20). 2020, New York, USA. AAAI, New York, NY, USA
- **Summary**: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in computer vision and artificial intelligence. However, the application of event cameras to object-level motion estimation or tracking is still in its infancy. The main idea behind this work is to propose a novel deep neural network to learn and regress a parametric object-level motion/transform model for event-based object tracking. To achieve this goal, we propose a synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which effectively encodes the spatio-temporal information of asynchronous retinal events into TSLTD frames with clear motion patterns. We feed the sequence of TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform an end-to-end 5-DoF object motion regression. Our method is compared with state-of-the-art object tracking methods, that are based on conventional cameras or event cameras. The experimental results show the superiority of our method in handling various challenging environments such as fast motion and low illumination conditions.



### SemI2I: Semantically Consistent Image-to-Image Translation for Domain Adaptation of Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2002.05925v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05925v2)
- **Published**: 2020-02-14 09:07:09+00:00
- **Updated**: 2020-02-21 09:21:35+00:00
- **Authors**: Onur Tasar, S L Happy, Yuliya Tarabalka, Pierre Alliez
- **Comment**: None
- **Journal**: None
- **Summary**: Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods.



### Counting dense objects in remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/2002.05928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05928v1)
- **Published**: 2020-02-14 09:13:54+00:00
- **Updated**: 2020-02-14 09:13:54+00:00
- **Authors**: Guangshuai Gao, Qingjie Liu, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating accurate number of interested objects from a given image is a challenging yet important task. Significant efforts have been made to address this problem and achieve great progress, yet counting number of ground objects from remote sensing images is barely studied. In this paper, we are interested in counting dense objects from remote sensing images. Compared with object counting in natural scene, this task is challenging in following factors: large scale variation, complex cluttered background and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting dataset based on remote sensing images, which contains four kinds of objects: buildings, crowded ships in harbor, large-vehicles and small-vehicles in parking lot. We then benchmark the dataset by designing a novel neural network which can generate density map of an input image. The proposed network consists of three parts namely convolution block attention module (CBAM), scale pyramid module (SPM) and deformable convolution module (DCM). Experiments on the proposed dataset and comparisons with state of the art methods demonstrate the challenging of the proposed dataset, and superiority and effectiveness of our method.



### Residual-Sparse Fuzzy $C$-Means Clustering Incorporating Morphological Reconstruction and Wavelet frames
- **Arxiv ID**: http://arxiv.org/abs/2002.08418v1
- **DOI**: 10.1109/TFUZZ.2020.3029296
- **Categories**: **cs.CV**, 62H30, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2002.08418v1)
- **Published**: 2020-02-14 10:00:03+00:00
- **Updated**: 2020-02-14 10:00:03+00:00
- **Authors**: Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou, Jun Zhao
- **Comment**: 12 pages, 11 figure
- **Journal**: IEEE Transactions on Fuzzy Systems, 2020
- **Summary**: Instead of directly utilizing an observed image including some outliers, noise or intensity inhomogeneity, the use of its ideal value (e.g. noise-free image) has a favorable impact on clustering. Hence, the accurate estimation of the residual (e.g. unknown noise) between the observed image and its ideal value is an important task. To do so, we propose an $\ell_0$ regularization-based Fuzzy $C$-Means (FCM) algorithm incorporating a morphological reconstruction operation and a tight wavelet frame transform. To achieve a sound trade-off between detail preservation and noise suppression, morphological reconstruction is used to filter an observed image. By combining the observed and filtered images, a weighted sum image is generated. Since a tight wavelet frame system has sparse representations of an image, it is employed to decompose the weighted sum image, thus forming its corresponding feature set. Taking it as data for clustering, we present an improved FCM algorithm by imposing an $\ell_0$ regularization term on the residual between the feature set and its ideal value, which implies that the favorable estimation of the residual is obtained and the ideal value participates in clustering. Spatial information is also introduced into clustering since it is naturally encountered in image segmentation. Furthermore, it makes the estimation of the residual more reliable. To further enhance the segmentation effects of the improved FCM algorithm, we also employ the morphological reconstruction to smoothen the labels generated by clustering. Finally, based on the prototypes and smoothed labels, the segmented image is reconstructed by using a tight wavelet frame reconstruction operation. Experimental results reported for synthetic, medical, and color images show that the proposed algorithm is effective and efficient, and outperforms other algorithms.



### Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2002.05962v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05962v1)
- **Published**: 2020-02-14 10:47:40+00:00
- **Updated**: 2020-02-14 10:47:40+00:00
- **Authors**: Jiawen Lyn
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural network (CNN) has been widely used in Single Image Super Resolution (SISR) so that SISR has been a great success recently. As the network deepens, the learning ability of network becomes more and more powerful. However, most SISR methods based on CNN do not make full use of hierarchical feature and the learning ability of network. These features cannot be extracted directly by subsequent layers, so the previous layer hierarchical information has little impact on the output and performance of subsequent layers relatively poor. To solve above problem, a novel Multi-Level Feature Fusion network (MLRN) is proposed, which can take full use of global intermediate features. We also introduce Feature Skip Fusion Block (FSFblock) as basic module. Each block can be extracted directly to the raw multiscale feature and fusion multi-level feature, then learn feature spatial correlation. The correlation among the features of the holistic approach leads to a continuous global memory of information mechanism. Extensive experiments on public datasets show that the method proposed by MLRN can be implemented, which is favorable performance for the most advanced methods.



### MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory Prediction in Mixed Traffic
- **Arxiv ID**: http://arxiv.org/abs/2002.05966v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2002.05966v5)
- **Published**: 2020-02-14 11:04:41+00:00
- **Updated**: 2020-06-23 13:06:17+00:00
- **Authors**: Hao Cheng, Wentong Liao, Michael Ying Yang, Monika Sester, Bodo Rosenhahn
- **Comment**: 8 pages, 5 figures, code is available on
  https://github.com/haohao11/MCENET
- **Journal**: None
- **Summary**: Trajectory prediction in urban mixed-traffic zones (a.k.a. shared spaces) is critical for many intelligent transportation systems, such as intent detection for autonomous driving. However, there are many challenges to predict the trajectories of heterogeneous road agents (pedestrians, cyclists and vehicles) at a microscopical level. For example, an agent might be able to choose multiple plausible paths in complex interactions with other agents in varying environments. To this end, we propose an approach named Multi-Context Encoder Network (MCENET) that is trained by encoding both past and future scene context, interaction context and motion information to capture the patterns and variations of the future trajectories using a set of stochastic latent variables. In inference time, we combine the past context and motion information of the target agent with samplings of the latent variables to predict multiple realistic trajectories in the future. Through experiments on several datasets of varying scenes, our method outperforms some of the recent state-of-the-art methods for mixed traffic trajectory prediction by a large margin and more robust in a very challenging environment. The impact of each context is justified via ablation studies.



### A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI data: An ABIDE Autism Classification study
- **Arxiv ID**: http://arxiv.org/abs/2002.05981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05981v1)
- **Published**: 2020-02-14 11:52:00+00:00
- **Updated**: 2020-02-14 11:52:00+00:00
- **Authors**: Ahmed El-Gazzar, Mirjam Quaak, Leonardo Cerliani, Peter Bloem, Guido van Wingen, Rajat Mani Thomas
- **Comment**: 8pages
- **Journal**: Second International Workshop, OR 2.0 2019, and Second
  International Workshop, MLCN 2019, Held in Conjunction with MICCAI 2019,
  Shenzhen, China, October 13 and 17, 2019, Proceedings
- **Summary**: Functional Magnetic Resonance Imaging (fMRI) captures the temporal dynamics of neural activity as a function of spatial location in the brain. Thus, fMRI scans are represented as 4-Dimensional (3-space + 1-time) tensors. And it is widely believed that the spatio-temporal patterns in fMRI manifests as behaviour and clinical symptoms. Because of the high dimensionality ($\sim$ 1 Million) of fMRI, and the added constraints of limited cardinality of data sets, extracting such patterns are challenging. A standard approach to overcome these hurdles is to reduce the dimensionality of the data by either summarizing activation over time or space at the expense of possible loss of useful information. Here, we introduce an end-to-end algorithm capable of extracting spatiotemporal features from the full 4-D data using 3-D CNNs and 3-D Convolutional LSTMs. We evaluate our proposed model on the publicly available ABIDE dataset to demonstrate the capability of our model to classify Autism Spectrum Disorder (ASD) from resting-state fMRI data. Our results show that the proposed model achieves state of the art results on single sites with F1-scores of 0.78 and 0.7 on NYU and UM sites, respectively.



### Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets
- **Arxiv ID**: http://arxiv.org/abs/2002.05990v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05990v1)
- **Published**: 2020-02-14 12:09:21+00:00
- **Updated**: 2020-02-14 12:09:21+00:00
- **Authors**: Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, Xingjun Ma
- **Comment**: ICLR 2020 conference paper (spotlight)
- **Journal**: None
- **Summary**: Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising security weakness of skip connections in this paper. Use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed Skip Gradient Method(SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures.



### Building Networks for Image Segmentation using Particle Competition and Cooperation
- **Arxiv ID**: http://arxiv.org/abs/2002.06001v1
- **DOI**: 10.1007/978-3-319-62392-4_16
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.06001v1)
- **Published**: 2020-02-14 12:45:12+00:00
- **Updated**: 2020-02-14 12:45:12+00:00
- **Authors**: Fabricio Breve
- **Comment**: None
- **Journal**: BREVE, FA. Building Networks for Image Segmentation Using Particle
  Competition and Cooperation. Lecture Notes in Computer Science. Cham:
  Springer International Publishing AG, 2017. v.10404. p.217 - 231
- **Summary**: Particle competition and cooperation (PCC) is a graph-based semi-supervised learning approach. When PCC is applied to interactive image segmentation tasks, pixels are converted into network nodes, and each node is connected to its k-nearest neighbors, according to the distance between a set of features extracted from the image. Building a proper network to feed PCC is crucial to achieve good segmentation results. However, some features may be more important than others to identify the segments, depending on the characteristics of the image to be segmented. In this paper, an index to evaluate candidate networks is proposed. Thus, building the network becomes a problem of optimizing some feature weights based on the proposed index. Computer simulations are performed on some real-world images from the Microsoft GrabCut database, and the segmentation results related in this paper show the effectiveness of the proposed method.



### AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.06048v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06048v3)
- **Published**: 2020-02-14 14:24:40+00:00
- **Updated**: 2021-01-04 01:41:13+00:00
- **Authors**: Youngmin Ro, Jin Young Choi
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: Existing fine-tuning methods use a single learning rate over all layers. In this paper, first, we discuss that trends of layer-wise weight variations by fine-tuning using a single learning rate do not match the well-known notion that lower-level layers extract general features and higher-level layers extract specific features. Based on our discussion, we propose an algorithm that improves fine-tuning performance and reduces network complexity through layer-wise pruning and auto-tuning of layer-wise learning rates. The proposed algorithm has verified the effectiveness by achieving state-of-the-art performance on the image retrieval benchmark datasets (CUB-200, Cars-196, Stanford online product, and Inshop). Code is available at https://github.com/youngminPIL/AutoLR.



### Verifying Deep Learning-based Decisions for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.00828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00828v1)
- **Published**: 2020-02-14 15:59:32+00:00
- **Updated**: 2020-02-14 15:59:32+00:00
- **Authors**: Ines Rieger, Rene Kollmann, Bettina Finzel, Dominik Seuss, Ute Schmid
- **Comment**: accepted at ESANN 2020
- **Journal**: Proceedings of the 28th European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning (ESANN 2020)
  139-145
- **Summary**: Neural networks with high performance can still be biased towards non-relevant features. However, reliability and robustness is especially important for high-risk fields such as clinical pain treatment. We therefore propose a verification pipeline, which consists of three steps. First, we classify facial expressions with a neural network. Next, we apply layer-wise relevance propagation to create pixel-based explanations. Finally, we quantify these visual explanations based on a bounding-box method with respect to facial regions. Although our results show that the neural network achieves state-of-the-art results, the evaluation of the visual explanations reveals that relevant facial regions may not be considered.



### Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers
- **Arxiv ID**: http://arxiv.org/abs/2002.06144v4
- **DOI**: 10.46298/jdmdh.6107
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06144v4)
- **Published**: 2020-02-14 17:56:18+00:00
- **Updated**: 2020-12-14 16:56:29+00:00
- **Authors**: Raphaël Barman, Maud Ehrmann, Simon Clematide, Sofia Ares Oliveira, Frédéric Kaplan
- **Comment**: None
- **Journal**: Journal of Data Mining & Digital Humanities, HistoInformatics,
  HistoInformatics (January 19, 2021) jdmdh:6107
- **Summary**: The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance.



### Spectrum Translation for Cross-Spectral Ocular Matching
- **Arxiv ID**: http://arxiv.org/abs/2002.06228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06228v1)
- **Published**: 2020-02-14 19:30:31+00:00
- **Updated**: 2020-02-14 19:30:31+00:00
- **Authors**: Kevin Hernandez Diaz, Fernando Alonso-Fernandez, Josef Bigun
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-spectral verification remains a big issue in biometrics, especially for the ocular area due to differences in the reflected features in the images depending on the region and spectrum used.   In this paper, we investigate the use of Conditional Adversarial Networks for spectrum translation between near infra-red and visual light images for ocular biometrics. We analyze the transformation based on the overall visual quality of the transformed images and the accuracy drop of the identification system when trained with opposing data.   We use the PolyU database and propose two different systems for biometric verification, the first one based on Siamese Networks trained with Softmax and Cross-Entropy loss, and the second one a Triplet Loss network. We achieved an EER of 1\% when using a Triplet Loss network trained for NIR and finding the Euclidean distance between the real NIR images and the fake ones translated from the visible spectrum. We also outperform previous results using baseline algorithms.



### Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2002.06241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.06241v1)
- **Published**: 2020-02-14 20:11:13+00:00
- **Updated**: 2020-02-14 20:11:13+00:00
- **Authors**: Jiachen Li, Hengbo Ma, Zhihao Zhang, Masayoshi Tomizuka
- **Comment**: None
- **Journal**: None
- **Summary**: Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are indispensable for intelligent mobile systems (like autonomous vehicles and social robots) to achieve safe and high-quality planning when they navigate in highly interactive and crowded scenarios. Due to the existence of frequent interactions and uncertainty in the scene evolution, it is desired for the prediction system to enable relational reasoning on different entities and provide a distribution of future trajectories for each agent. In this paper, we propose a generic generative neural system (called Social-WaGDAT) for multi-agent trajectory prediction, which makes a step forward to explicit interaction modeling by incorporating relational inductive biases with a dynamic graph representation and leverages both trajectory and scene context information. We also employ an efficient kinematic constraint layer applied to vehicle trajectory prediction which not only ensures physical feasibility but also enhances model performance. The proposed system is evaluated on three public benchmark datasets for trajectory prediction, where the agents cover pedestrians, cyclists and on-road vehicles. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy.



### Why Do Line Drawings Work? A Realism Hypothesis
- **Arxiv ID**: http://arxiv.org/abs/2002.06260v1
- **DOI**: 10.1177/0301006620908207
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2002.06260v1)
- **Published**: 2020-02-14 21:41:00+00:00
- **Updated**: 2020-02-14 21:41:00+00:00
- **Authors**: Aaron Hertzmann
- **Comment**: Accepted to Perception
- **Journal**: Perception. 49:4 (2020) 439-451
- **Summary**: Why is it that we can recognize object identity and 3D shape from line drawings, even though they do not exist in the natural world? This paper hypothesizes that the human visual system perceives line drawings as if they were approximately realistic images. Moreover, the techniques of line drawing are chosen to accurately convey shape to a human observer. Several implications and variants of this hypothesis are explored.



### Realistic River Image Synthesis using Deep Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.00826v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00826v3)
- **Published**: 2020-02-14 21:49:33+00:00
- **Updated**: 2021-07-27 21:02:06+00:00
- **Authors**: Akshat Gautam, Muhammed Sit, Ibrahim Demir
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we demonstrated a practical application of realistic river image generation using deep learning. Specifically, we explored a generative adversarial network (GAN) model capable of generating high-resolution and realistic river images that can be used to support modeling and analysis in surface water estimation, river meandering, wetland loss, and other hydrological research studies. First, we have created an extensive repository of overhead river images to be used in training. Second, we incorporated the Progressive Growing GAN (PGGAN), a network architecture that iteratively trains smaller-resolution GANs to gradually build up to a very high resolution to generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler GAN architectures, difficulties arose in terms of exponential increase of training time and vanishing/exploding gradient issues, which the PGGAN implementation seemed to significantly reduce. The results presented in this study show great promise in generating high-quality images and capturing the details of river structure and flow to support hydrological research, which often requires extensive imagery for model performance.



### Layered Embeddings for Amodal Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.06264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06264v1)
- **Published**: 2020-02-14 22:00:45+00:00
- **Updated**: 2020-02-14 22:00:45+00:00
- **Authors**: Yanfeng Liu, Eric Psota, Lance Pérez
- **Comment**: International Conference on Image Analysis and Recognition. Springer,
  Cham, 2019 (ICIAR 2019)
- **Journal**: None
- **Summary**: The proposed method extends upon the representational output of semantic instance segmentation by explicitly including both visible and occluded parts. A fully convolutional network is trained to produce consistent pixel-level embedding across two layers such that, when clustered, the results convey the full spatial extent and depth ordering of each instance. Results demonstrate that the network can accurately estimate complete masks in the presence of occlusion and outperform leading top-down bounding-box approaches. Source code available at https://github.com/yanfengliu/layered_embeddings



### CheXclusion: Fairness gaps in deep chest X-ray classifiers
- **Arxiv ID**: http://arxiv.org/abs/2003.00827v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00827v2)
- **Published**: 2020-02-14 22:08:12+00:00
- **Updated**: 2020-10-16 03:26:20+00:00
- **Authors**: Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene Y. Chen, Marzyeh Ghassemi
- **Comment**: Paper is accepted in Pacific Symposium on Biocomputing 2021
  (PSB2021). Code can be found at, https://github.com/LalehSeyyed/CheXclusion
- **Journal**: None
- **Summary**: Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in 3 prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation of all those datasets. We evaluate the TPR disparity -- the difference in true positive rates (TPR) -- among different protected attributes such as patient sex, age, race, and insurance type as a proxy for socioeconomic status. We demonstrate that TPR disparities exist in the state-of-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. A multi-source dataset corresponds to the smallest disparities, suggesting one way to reduce bias. We find that TPR disparities are not significantly correlated with a subgroup's proportional disease burden. As clinical models move from papers to products, we encourage clinical decision makers to carefully audit for algorithmic disparities prior to deployment. Our code can be found at, https://github.com/LalehSeyyed/CheXclusion



### Single Unit Status in Deep Convolutional Neural Network Codes for Face Identification: Sparseness Redefined
- **Arxiv ID**: http://arxiv.org/abs/2002.06274v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06274v2)
- **Published**: 2020-02-14 22:42:02+00:00
- **Updated**: 2020-03-01 09:58:01+00:00
- **Authors**: Connor J. Parde, Y. Ivette Colón, Matthew Q. Hill, Carlos D. Castillo, Prithviraj Dhar, Alice J. O'Toole
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) trained for face identification develop representations that generalize over variable images, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. Identity, gender, and viewpoint codes were studied at the "neural unit" and ensemble levels of a face-identification network. At the unit level, identification, gender classification, and viewpoint estimation were measured by deleting units to create variably-sized, randomly-sampled subspaces at the top network layer. Identification of 3,531 identities remained high (area under the ROC approximately 1.0) as dimensionality decreased from 512 units to 16 (0.95), 4 (0.80), and 2 (0.72) units. Individual identities separated statistically on every top-layer unit. Cross-unit responses were minimally correlated, indicating that units code non-redundant identity cues. This "distributed" code requires only a sparse, random sample of units to identify faces accurately. Gender classification declined gradually and viewpoint estimation fell steeply as dimensionality decreased. Individual units were weakly predictive of gender and viewpoint, but ensembles proved effective predictors. Therefore, distributed and sparse codes co-exist in the network units to represent different face attributes. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint information separated into high-dimensional subspaces, ordered by explained variance. Identity, gender, and viewpoint information contributed to all individual unit responses, undercutting a neural tuning analogy for face attributes. Interpretation of neural-like codes from DCNNs, and by analogy, high-level visual codes, cannot be inferred from single unit responses. Instead, "meaning" is encoded by directions in the high-dimensional space.



