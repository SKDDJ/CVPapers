# Arxiv Papers in cs.CV on 2020-02-04
### Multistage Model for Robust Face Alignment Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01075v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01075v1)
- **Published**: 2020-02-04 01:13:58+00:00
- **Updated**: 2020-02-04 01:13:58+00:00
- **Authors**: Huabin Wang, Rui Cheng, Jian Zhou, Liang Tao, Hon Keung Kwan
- **Comment**: None
- **Journal**: None
- **Summary**: An ability to generalize unconstrained conditions such as severe occlusions and large pose variations remains a challenging goal to achieve in face alignment. In this paper, a multistage model based on deep neural networks is proposed which takes advantage of spatial transformer networks, hourglass networks and exemplar-based shape constraints. First, a spatial transformer - generative adversarial network which consists of convolutional layers and residual units is utilized to solve the initialization issues caused by face detectors, such as rotation and scale variations, to obtain improved face bounding boxes for face alignment. Then, stacked hourglass network is employed to obtain preliminary locations of landmarks as well as their corresponding scores. In addition, an exemplar-based shape dictionary is designed to determine landmarks with low scores based on those with high scores. By incorporating face shape constraints, misaligned landmarks caused by occlusions or cluttered backgrounds can be considerably improved. Extensive experiments based on challenging benchmark datasets are performed to demonstrate the superior performance of the proposed method over other state-of-the-art methods.



### Object Instance Mining for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.01087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01087v1)
- **Published**: 2020-02-04 02:11:39+00:00
- **Updated**: 2020-02-04 02:11:39+00:00
- **Authors**: Chenhao Lin, Siwen Wang, Dongqi Xu, Yu Lu, Wayne Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) using only image-level annotations has attracted growing attention over the past few years. Existing approaches using multiple instance learning easily fall into local optima, because such mechanism tends to learn from the most discriminative object in an image for each category. Therefore, these methods suffer from missing object instances which degrade the performance of WSOD. To address this problem, this paper introduces an end-to-end object instance mining (OIM) framework for weakly supervised object detection. OIM attempts to detect all possible object instances existing in each image by introducing information propagation on the spatial and appearance graphs, without any additional annotations. During the iterative learning process, the less discriminative object instances from the same class can be gradually detected and utilized for training. In addition, we design an object instance reweighted loss to learn larger portion of each object instance to further improve the performance. The experimental results on two publicly available databases, VOC 2007 and 2012, demonstrate the efficacy of proposed approach.



### Aesthetic Quality Assessment for Group photograph
- **Arxiv ID**: http://arxiv.org/abs/2002.01096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01096v1)
- **Published**: 2020-02-04 02:52:52+00:00
- **Updated**: 2020-02-04 02:52:52+00:00
- **Authors**: Yaoting Wang, Yongzhen Ke, Kai Wang, Cuijiao Zhang, Fan Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Image aesthetic quality assessment has got much attention in recent years, but not many works have been done on a specific genre of photos: Group photograph. In this work, we designed a set of high-level features based on the experience and principles of group photography: Opened-eye, Gaze, Smile, Occluded faces, Face Orientation, Facial blur, Character center. Then we combined them and 83 generic aesthetic features to build two aesthetic assessment models. We also constructed a large dataset of group photographs - GPD- annotated with the aesthetic score. The experimental result shows that our features perform well for categorizing professional photos and snapshots and predicting the distinction of multiple group photographs of diverse human states under the same scene.



### Improved dual channel pulse coupled neural network and its application to multi-focus image fusion
- **Arxiv ID**: http://arxiv.org/abs/2002.01102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01102v2)
- **Published**: 2020-02-04 03:26:31+00:00
- **Updated**: 2022-01-29 05:48:43+00:00
- **Authors**: Huai-Shui Tong, Xiao-Jun Wu, Hui Li
- **Comment**: 16 pages, 7 figures, 9 tables
- **Journal**: None
- **Summary**: This paper presents an improved dual channel pulse coupled neural network (IDC-PCNN) model for image fusion. The model can overcome some defects of standard PCNN model. In this fusion scheme, the multiplication rule is replaced by addition rule in the information fusion pool of dual channel PCNN (DC-PCNN) model. Meanwhile the sum of modified Laplacian (SML) measure is adopted, which is better than other focus measures. This method not only inherits the good characteristics of the standard PCNN model but also enhances the computing efficiency and fusion quality. The performance of the proposed method is evaluated by using four criteria including average cross entropy, root mean square error, peak value signal to noise ratio and structure similarity index. Comparative studies show that the proposed fusion algorithm outperforms the standard PCNN method and the DC-PCNN method.



### TPPO: A Novel Trajectory Predictor with Pseudo Oracle
- **Arxiv ID**: http://arxiv.org/abs/2002.01852v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01852v3)
- **Published**: 2020-02-04 03:28:55+00:00
- **Updated**: 2021-12-29 06:28:52+00:00
- **Authors**: Biao Yang, Caizhen He, Pin Wang, Ching-yao Chan, Xiaofeng Liu, Yang Chen
- **Comment**: 14 pages, 7 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2002.00391. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice
- **Journal**: None
- **Summary**: Forecasting pedestrian trajectories in dynamic scenes remains a critical problem in various applications, such as autonomous driving and socially aware robots. Such forecasting is challenging due to human-human and human-object interactions and future uncertainties caused by human randomness. Generative model-based methods handle future uncertainties by sampling a latent variable. However, few studies explored the generation of the latent variable. In this work, we propose the Trajectory Predictor with Pseudo Oracle (TPPO), which is a generative model-based trajectory predictor. The first pseudo oracle is pedestrians' moving directions, and the second one is the latent variable estimated from ground truth trajectories. A social attention module is used to aggregate neighbors' interactions based on the correlation between pedestrians' moving directions and future trajectories. This correlation is inspired by the fact that pedestrians' future trajectories are often influenced by pedestrians in front. A latent variable predictor is proposed to estimate latent variable distributions from observed and ground-truth trajectories. Moreover, the gap between these two distributions is minimized during training. Therefore, the latent variable predictor can estimate the latent variable from observed trajectories to approximate that estimated from ground-truth trajectories. We compare the performance of TPPO with related methods on several public datasets. Results demonstrate that TPPO outperforms state-of-the-art methods with low average and final displacement errors. The ablation study shows that the prediction performance will not dramatically decrease as sampling times decline during tests.



### Multi-label Relation Modeling in Facial Action Units Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.01105v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01105v2)
- **Published**: 2020-02-04 03:33:00+00:00
- **Updated**: 2020-02-08 10:39:30+00:00
- **Authors**: Xianpeng Ji, Yu Ding, Lincheng Li, Yu Chen, Changjie Fan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an approach to the facial action units detections. The involved action units (AU) include AU1 (Inner Brow Raiser), AU2 (Outer Brow Raiser), AU4 (Brow Lowerer), AU6 (Cheek Raise), AU12 (Lip Corner Puller), AU15 (Lip Corner Depressor), AU20 (Lip Stretcher), and AU25 (Lip Part). Our work relies on the dataset released by the FG-2020 Competition: Affective Behavior Analysis In-the-Wild (ABAW). The proposed method consists of the data preprocessing, the feature extraction and the AU classification. The data preprocessing includes the detection of face texture and landmarks. The texture static and landmark dynamic features are extracted through neural networks and then fused as the feature latent representation. Finally, the fused feature is taken as the initial hidden state of a recurrent neural network with a trainable lookup AU table. The output of the RNN is the results of AU classification. The detected accuracy is evaluated with 0.5$\times$accuracy + 0.5$\times$F1. Our method achieve 0.56 with the validation data that is specified by the organization committee.



### Acoustic anomaly detection via latent regularized gaussian mixture generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01107v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2002.01107v2)
- **Published**: 2020-02-04 03:39:50+00:00
- **Updated**: 2020-02-05 02:27:12+00:00
- **Authors**: Chengwei Chen, Pan Chen, Lingyu Yang, Jinyuan Mo, Haichuan Song, Yuan Xie, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Acoustic anomaly detection aims at distinguishing abnormal acoustic signals from the normal ones. It suffers from the class imbalance issue and the lacking in the abnormal instances. In addition, collecting all kinds of abnormal or unknown samples for training purpose is impractical and timeconsuming. In this paper, a novel Gaussian Mixture Generative Adversarial Network (GMGAN) is proposed under semi-supervised learning framework, in which the underlying structure of training data is not only captured in spectrogram reconstruction space, but also can be further restricted in the space of latent representation in a discriminant manner. Experiments show that our model has clear superiority over previous methods, and achieves the state-of-the-art results on DCASE dataset.



### Selective Segmentation Networks Using Top-Down Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.01125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01125v1)
- **Published**: 2020-02-04 04:47:23+00:00
- **Updated**: 2020-02-04 04:47:23+00:00
- **Authors**: Mahdi Biparva, John Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks model the transformation of the input sensory data at the bottom of a network hierarchy to the semantic information at the top of the visual hierarchy. Feedforward processing is sufficient for some object recognition tasks. Top-Down selection is potentially required in addition to the Bottom-Up feedforward pass. It can, in part, address the shortcoming of the loss of location information imposed by the hierarchical feature pyramids. We propose a unified 2-pass framework for object segmentation that augments Bottom-Up \convnets with a Top-Down selection network. We utilize the top-down selection gating activities to modulate the bottom-up hidden activities for segmentation predictions. We develop an end-to-end multi-task framework with loss terms satisfying task requirements at the two ends of the network. We evaluate the proposed network on benchmark datasets for semantic segmentation, and show that networks with the Top-Down selection capability outperform the baseline model. Additionally, we shed light on the superior aspects of the new segmentation paradigm and qualitatively and quantitatively support the efficiency of the novel framework over the baseline model that relies purely on parametric skip connections.



### 3D ResNet with Ranking Loss Function for Abnormal Activity Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.01132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01132v1)
- **Published**: 2020-02-04 05:32:21+00:00
- **Updated**: 2020-02-04 05:32:21+00:00
- **Authors**: Shikha Dubey, Abhijeet Boragule, Moongu Jeon
- **Comment**: None
- **Journal**: None
- **Summary**: Abnormal activity detection is one of the most challenging tasks in the field of computer vision. This study is motivated by the recent state-of-art work of abnormal activity detection, which utilizes both abnormal and normal videos in learning abnormalities with the help of multiple instance learning by providing the data with video-level information. In the absence of temporal-annotations, such a model is prone to give a false alarm while detecting the abnormalities. For this reason, in this paper, we focus on the task of minimizing the false alarm rate while performing an abnormal activity detection task. The mitigation of these false alarms and recent advancement of 3D deep neural network in video action recognition task collectively give us motivation to exploit the 3D ResNet in our proposed method, which helps to extract spatial-temporal features from the videos. Afterwards, using these features and deep multiple instance learning along with the proposed ranking loss, our model learns to predict the abnormality score at the video segment level. Therefore, our proposed method 3D deep Multiple Instance Learning with ResNet (MILR) along with the new proposed ranking loss function achieves the best performance on the UCF-Crime benchmark dataset, as compared to other state-of-art methods. The effectiveness of our proposed method is demonstrated on the UCF-Crime dataset.



### Classification of Hyperspectral and LiDAR Data Using Coupled CNNs
- **Arxiv ID**: http://arxiv.org/abs/2002.01144v1
- **DOI**: 10.1109/TGRS.2020.2969024
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01144v1)
- **Published**: 2020-02-04 06:23:36+00:00
- **Updated**: 2020-02-04 06:23:36+00:00
- **Authors**: Renlong Hang, Zhu Li, Pedram Ghamisi, Danfeng Hong, Guiyu Xia, Qingshan Liu
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Summary**: In this paper, we propose an efficient and effective framework to fuse hyperspectral and Light Detection And Ranging (LiDAR) data using two coupled convolutional neural networks (CNNs). One CNN is designed to learn spectral-spatial features from hyperspectral data, and the other one is used to capture the elevation information from LiDAR data. Both of them consist of three convolutional layers, and the last two convolutional layers are coupled together via a parameter sharing strategy. In the fusion phase, feature-level and decision-level fusion methods are simultaneously used to integrate these heterogeneous features sufficiently. For the feature-level fusion, three different fusion strategies are evaluated, including the concatenation strategy, the maximization strategy, and the summation strategy. For the decision-level fusion, a weighted summation strategy is adopted, where the weights are determined by the classification accuracy of each output. The proposed model is evaluated on an urban data set acquired over Houston, USA, and a rural one captured over Trento, Italy. On the Houston data, our model can achieve a new record overall accuracy of 96.03%. On the Trento data, it achieves an overall accuracy of 99.12%. These results sufficiently certify the effectiveness of our proposed model.



### Adversarially Robust Frame Sampling with Bounded Irregularities
- **Arxiv ID**: http://arxiv.org/abs/2002.01147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01147v1)
- **Published**: 2020-02-04 06:33:43+00:00
- **Updated**: 2020-02-04 06:33:43+00:00
- **Authors**: Hanhan Li, Pin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, video analysis tools for automatically extracting meaningful information from videos are widely studied and deployed. Because most of them use deep neural networks which are computationally expensive, feeding only a subset of video frames into such algorithms is desired. Sampling the frames with fixed rate is always attractive for its simplicity, representativeness, and interpretability. For example, a popular cloud video API generated video and shot labels by processing only the first frame of every second in a video. However, one can easily attack such strategies by placing chosen frames at the sampled locations. In this paper, we present an elegant solution to this sampling problem that is provably robust against adversarial attacks and introduces bounded irregularities as well.



### Texture Classification using Block Intensity and Gradient Difference (BIGD) Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2002.01154v1
- **DOI**: 10.1016/j.image.2019.115770
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01154v1)
- **Published**: 2020-02-04 07:03:51+00:00
- **Updated**: 2020-02-04 07:03:51+00:00
- **Authors**: Yuting Hu, Zhen Wang, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an efficient and distinctive local descriptor, namely block intensity and gradient difference (BIGD). In an image patch, we randomly sample multi-scale block pairs and utilize the intensity and gradient differences of pairwise blocks to construct the local BIGD descriptor. The random sampling strategy and the multi-scale framework help BIGD descriptors capture the distinctive patterns of patches at different orientations and spatial granularity levels. We use vectors of locally aggregated descriptors (VLAD) or improved Fisher vector (IFV) to encode local BIGD descriptors into a full image descriptor, which is then fed into a linear support vector machine (SVM) classifier for texture classification. We compare the proposed descriptor with typical and state-of-the-art ones by evaluating their classification performance on five public texture data sets including Brodatz, CUReT, KTH-TIPS, and KTH-TIPS-2a and -2b. Experimental results show that the proposed BIGD descriptor with stronger discriminative power yields 0.12% ~ 6.43% higher classification accuracy than the state-of-the-art texture descriptor, dense microblock difference (DMD).



### Simultaneous Enhancement and Super-Resolution of Underwater Imagery for Improved Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2002.01155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01155v1)
- **Published**: 2020-02-04 07:07:08+00:00
- **Updated**: 2020-02-04 07:07:08+00:00
- **Authors**: Md Jahidul Islam, Peigen Luo, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce and tackle the simultaneous enhancement and super-resolution (SESR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present Deep SESR, a residual-in-residual network-based generative model that can learn to restore perceptual image qualities at 2x, 3x, or 4x higher spatial resolution. We supervise its training by formulating a multi-modal objective function that addresses the chrominance-specific underwater color degradation, lack of image sharpness, and loss in high-level feature representation. It is also supervised to learn salient foreground regions in the image, which in turn guides the network to learn global contrast enhancement. We design an end-to-end training pipeline to jointly learn the saliency prediction and SESR on a shared hierarchical feature space for fast inference. Moreover, we present UFO-120, the first dataset to facilitate large-scale SESR learning; it contains over 1500 training samples and a benchmark test set of 120 samples. By thorough experimental evaluation on the UFO-120 and other standard datasets, we demonstrate that Deep SESR outperforms the existing solutions for underwater image enhancement and super-resolution. We also validate its generalization performance on several test cases that include underwater images with diverse spectral and spatial degradation levels, and also terrestrial images with unseen natural objects. Lastly, we analyze its computational feasibility for single-board deployments and demonstrate its operational benefits for visually-guided underwater robots. The model and dataset information will be available at: https://github.com/xahidbuffon/Deep-SESR.



### Vanishing Point Detection with Direct and Transposed Fast Hough Transform inside the neural network
- **Arxiv ID**: http://arxiv.org/abs/2002.01176v3
- **DOI**: 10.18287/2412-6179-CO-676
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01176v3)
- **Published**: 2020-02-04 09:10:45+00:00
- **Updated**: 2020-07-07 13:08:55+00:00
- **Authors**: A. Sheshkus, A. Chirvonaya, D. Matveev, D. Nikolaev, V. L. Arlazarov
- **Comment**: 9 pages, 9 figures, submitted to "Computer Optics"; extra experiment
  added, new theorem proof added, references added; typos corrected
- **Journal**: Computer Optics 2020; 44(5): 737-745
- **Summary**: In this paper, we suggest a new neural network architecture for vanishing point detection in images. The key element is the use of the direct and transposed Fast Hough Transforms separated by convolutional layer blocks with standard activation functions. It allows us to get the answer in the coordinates of the input image at the output of the network and thus to calculate the coordinates of the vanishing point by simply selecting the maximum. Besides, it was proved that calculation of the transposed Fast Hough Transform can be performed using the direct one. The use of integral operators enables the neural network to rely on global rectilinear features in the image, and so it is ideal for detecting vanishing points. To demonstrate the effectiveness of the proposed architecture, we use a set of images from a DVR and show its superiority over existing methods. Note, in addition, that the proposed neural network architecture essentially repeats the process of direct and back projection used, for example, in computed tomography.



### Lane Detection in Low-light Conditions Using an Efficient Data Enhancement : Light Conditions Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2002.01177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01177v2)
- **Published**: 2020-02-04 09:14:13+00:00
- **Updated**: 2020-05-16 09:39:45+00:00
- **Authors**: Tong Liu, Zhaowei Chen, Yi Yang, Zehao Wu, Haowei Li
- **Comment**: Accepted by IV 2020
- **Journal**: None
- **Summary**: Nowadays, deep learning techniques are widely used for lane detection, but application in low-light conditions remains a challenge until this day. Although multi-task learning and contextual-information-based methods have been proposed to solve the problem, they either require additional manual annotations or introduce extra inference overhead respectively. In this paper, we propose a style-transfer-based data enhancement method, which uses Generative Adversarial Networks (GANs) to generate images in low-light conditions, that increases the environmental adaptability of the lane detector. Our solution consists of three parts: the proposed SIM-CycleGAN, light conditions style transfer and lane detection network. It does not require additional manual annotations nor extra inference overhead. We validated our methods on the lane detection benchmark CULane using ERFNet. Empirically, lane detection model trained using our method demonstrated adaptability in low-light conditions and robustness in complex scenarios. Our code for this paper will be publicly available.



### Robust Generative Restricted Kernel Machines using Weighted Conjugate Feature Duality
- **Arxiv ID**: http://arxiv.org/abs/2002.01180v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01180v3)
- **Published**: 2020-02-04 09:23:25+00:00
- **Updated**: 2020-06-23 14:35:30+00:00
- **Authors**: Arun Pandey, Joachim Schreurs, Johan A. K. Suykens
- **Comment**: None
- **Journal**: None
- **Summary**: Interest in generative models has grown tremendously in the past decade. However, their training performance can be adversely affected by contamination, where outliers are encoded in the representation of the model. This results in the generation of noisy data. In this paper, we introduce weighted conjugate feature duality in the framework of Restricted Kernel Machines (RKMs). The RKM formulation allows for an easy integration of methods from classical robust statistics. This formulation is used to fine-tune the latent space of generative RKMs using a weighting function based on the Minimum Covariance Determinant, which is a highly robust estimator of multivariate location and scatter. Experiments show that the weighted RKM is capable of generating clean images when contamination is present in the training data. We further show that the robust method also preserves uncorrelated feature learning through qualitative and quantitative experiments on standard datasets.



### Unsupervised Multiple Person Tracking using AutoEncoder-Based Lifted Multicuts
- **Arxiv ID**: http://arxiv.org/abs/2002.01192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01192v1)
- **Published**: 2020-02-04 09:42:34+00:00
- **Updated**: 2020-02-04 09:42:34+00:00
- **Authors**: Kalun Ho, Janis Keuper, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) is a long-standing task in computer vision. Current approaches based on the tracking by detection paradigm either require some sort of domain knowledge or supervision to associate data correctly into tracks. In this work, we present an unsupervised multiple object tracking approach based on visual features and minimum cost lifted multicuts. Our method is based on straight-forward spatio-temporal cues that can be extracted from neighboring frames in an image sequences without superivison. Clustering based on these cues enables us to learn the required appearance invariances for the tracking task at hand and train an autoencoder to generate suitable latent representation. Thus, the resulting latent representations can serve as robust appearance cues for tracking even over large temporal distances where no reliable spatio-temporal features could be extracted. We show that, despite being trained without using the provided annotations, our model provides competitive results on the challenging MOT Benchmark for pedestrian tracking.



### Selective Convolutional Network: An Efficient Object Detector with Ignoring Background
- **Arxiv ID**: http://arxiv.org/abs/2002.01205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01205v1)
- **Published**: 2020-02-04 10:07:01+00:00
- **Updated**: 2020-02-04 10:07:01+00:00
- **Authors**: Hefei Ling, Yangyang Qin, Li Zhang, Yuxuan Shi, Ping Li
- **Comment**: Accepted at ICASSP 2020
- **Journal**: None
- **Summary**: It is well known that attention mechanisms can effectively improve the performance of many CNNs including object detectors. Instead of refining feature maps prevalently, we reduce the prohibitive computational complexity by a novel attempt at attention. Therefore, we introduce an efficient object detector called Selective Convolutional Network (SCN), which selectively calculates only on the locations that contain meaningful and conducive information. The basic idea is to exclude the insignificant background areas, which effectively reduces the computational cost especially during the feature extraction. To solve it, we design an elaborate structure with negligible overheads to guide the network where to look next. It's end-to-end trainable and easy-embedding. Without additional segmentation datasets, we explores two different train strategies including direct supervision and indirect supervision. Extensive experiments assess the performance on PASCAL VOC2007 and MS COCO detection datasets. Results show that SSD and Pelee integrated with our method averagely reduce the calculations in a range of 1/5 and 1/3 with slight loss of accuracy, demonstrating the feasibility of SCN.



### Deep-Geometric 6 DoF Localization from a Single Image in Topo-metric Maps
- **Arxiv ID**: http://arxiv.org/abs/2002.01210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01210v1)
- **Published**: 2020-02-04 10:11:46+00:00
- **Updated**: 2020-02-04 10:11:46+00:00
- **Authors**: Tom Roussel, Punarjay Chakravarty, Gaurav Pandey, Tinne Tuytelaars, Luc Van Eycken
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a Deep-Geometric Localizer that is able to estimate the full 6 Degree of Freedom (DoF) global pose of the camera from a single image in a previously mapped environment. Our map is a topo-metric one, with discrete topological nodes whose 6 DoF poses are known. Each topo-node in our map also comprises of a set of points, whose 2D features and 3D locations are stored as part of the mapping process. For the mapping phase, we utilise a stereo camera and a regular stereo visual SLAM pipeline. During the localization phase, we take a single camera image, localize it to a topological node using Deep Learning, and use a geometric algorithm (PnP) on the matched 2D features (and their 3D positions in the topo map) to determine the full 6 DoF globally consistent pose of the camera. Our method divorces the mapping and the localization algorithms and sensors (stereo and mono), and allows accurate 6 DoF pose estimation in a previously mapped environment using a single camera. With potential VR/AR and localization applications in single camera devices such as mobile phones and drones, our hybrid algorithm compares favourably with the fully Deep-Learning based Pose-Net that regresses pose from a single image in simulated as well as real environments.



### Fast reconstruction of atomic-scale STEM-EELS images from sparse sampling
- **Arxiv ID**: http://arxiv.org/abs/2002.01225v1
- **DOI**: None
- **Categories**: **eess.IV**, cond-mat.mtrl-sci, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01225v1)
- **Published**: 2020-02-04 11:07:56+00:00
- **Updated**: 2020-02-04 11:07:56+00:00
- **Authors**: Etienne Monier, Thomas Oberlin, Nathalie Brun, Xiaoyan Li, Marcel Tencé, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper discusses the reconstruction of partially sampled spectrum-images to accelerate the acquisition in scanning transmission electron microscopy (STEM). The problem of image reconstruction has been widely considered in the literature for many imaging modalities, but only a few attempts handled 3D data such as spectral images acquired by STEM electron energy loss spectroscopy (EELS). Besides, among the methods proposed in the microscopy literature, some are fast but inaccurate while others provide accurate reconstruction but at the price of a high computation burden. Thus none of the proposed reconstruction methods fulfills our expectations in terms of accuracy and computation complexity. In this paper, we propose a fast and accurate reconstruction method suited for atomic-scale EELS. This method is compared to popular solutions such as beta process factor analysis (BPFA) which is used for the first time on STEM-EELS images. Experiments based on real as synthetic data will be conducted.



### Determination of the relative inclination and the viewing angle of an interacting pair of galaxies using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01238v2
- **DOI**: 10.1093/mnras/staa2109
- **Categories**: **astro-ph.GA**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01238v2)
- **Published**: 2020-02-04 11:54:07+00:00
- **Updated**: 2020-07-30 06:42:42+00:00
- **Authors**: Prem Prakash, Arunima Banerjee, Pavan Kumar Perepu
- **Comment**: 13 pages, 11 Figures, 15 tables (Accepted for publication in the
  MNRAS)
- **Journal**: None
- **Summary**: Constructing dynamical models for interacting pair of galaxies as constrained by their observed structure and kinematics crucially depends on the correct choice of the values of the relative inclination ($i$) between their galactic planes as well as the viewing angle ($\theta$), the angle between the line of sight and the normal to the plane of their orbital motion. We construct Deep Convolutional Neural Network (DCNN) models to determine the relative inclination ($i$) and the viewing angle ($\theta$) of interacting galaxy pairs, using N-body $+$ Smoothed Particle Hydrodynamics (SPH) simulation data from the GALMER database for training the same. In order to classify galaxy pairs based on their $i$ values only, we first construct DCNN models for a (a) 2-class ( $i$ = 0 $^{\circ}$, 45$^{\circ}$ ) and (b) 3-class ($i = 0^{\circ}, 45^{\circ} \text{ and } 90^{\circ}$) classification, obtaining $F_1$ scores of 99% and 98% respectively. Further, for a classification based on both $i$ and $\theta$ values, we develop a DCNN model for a 9-class classification ($(i,\theta) \sim (0^{\circ},15^{\circ}) ,(0^{\circ},45^{\circ}), (0^{\circ},90^{\circ}), (45^{\circ},15^{\circ}), (45^{\circ}, 45^{\circ}), (45^{\circ}, 90^{\circ}), (90^{\circ}, 15^{\circ}), (90^{\circ}, 45^{\circ}), (90^{\circ},90^{\circ})$), and the $F_1$ score was 97$\%$. Finally, we tested our 2-class model on real data of interacting galaxy pairs from the Sloan Digital Sky Survey (SDSS) DR15, and achieve an $F_1$ score of 78%. Our DCNN models could be further extended to determine additional parameters needed to model dynamics of interacting galaxy pairs, which is currently accomplished by trial and error method.



### GTC: Guided Training of CTC Towards Efficient and Accurate Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.01276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01276v1)
- **Published**: 2020-02-04 13:26:14+00:00
- **Updated**: 2020-02-04 13:26:14+00:00
- **Authors**: Wenyang Hu, Xiaocong Cai, Jun Hou, Shuai Yi, Zhiping Lin
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance. With the benefit of guided training, CTC model achieves robust and accurate prediction for both regular and irregular scene text while maintaining a fast inference speed. Moreover, to further leverage the potential of CTC decoder, a graph convolutional network (GCN) is proposed to learn the local correlations of extracted features. Extensive experiments on standard benchmarks demonstrate that our end-to-end model achieves a new state-of-the-art for regular and irregular scene text recognition and needs 6 times shorter inference time than attentionbased methods.



### Pixel-wise Conditioned Generative Adversarial Networks for Image Synthesis and Completion
- **Arxiv ID**: http://arxiv.org/abs/2002.01281v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01281v1)
- **Published**: 2020-02-04 13:49:15+00:00
- **Updated**: 2020-02-04 13:49:15+00:00
- **Authors**: Cyprien Ruffino, Romain Hérault, Eric Laloy, Gilles Gasso
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works have extended GANs to image inpainting by conditioning the generation with parts of the image to be reconstructed. Despite their success, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper we investigate the effectiveness of conditioning GANs when very few pixel values are provided. We propose a modelling framework which results in adding an explicit cost term to the GAN objective function to enforce pixel-wise conditioning. We investigate the influence of this regularization term on the quality of the generated images and the fulfillment of the given pixel constraints. Using the recent PacGAN technique, we ensure that we keep diversity in the generated samples. Conducted experiments on FashionMNIST show that the regularization term effectively controls the trade-off between quality of the generated images and the conditioning. Experimental evaluation on the CIFAR-10 and CelebA datasets evidences that our method achieves accurate results both visually and quantitatively in term of Fr\'echet Inception Distance, while still enforcing the pixel conditioning. We also evaluate our method on a texture image generation task using fully-convolutional networks. As a final contribution, we apply the method to a classical geological simulation application.



### Obstruction level detection of sewer videos using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01284v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01284v1)
- **Published**: 2020-02-04 13:52:02+00:00
- **Updated**: 2020-02-04 13:52:02+00:00
- **Authors**: Mario A. Gutierrez-Mondragon, Dario Garcia-Gasulla, Sergio Alvarez-Napagao, Jaume Brossa-Ordoñez, Rafael Gimenez-Esteban
- **Comment**: None
- **Journal**: None
- **Summary**: Worldwide, sewer networks are designed to transport wastewater to a centralized treatment plant to be treated and returned to the environment. This process is critical for the current society, preventing waterborne illnesses, providing safe drinking water and enhancing general sanitation. To keep a sewer network perfectly operational, sampling inspections are performed constantly to identify obstructions. Typically, a Closed-Circuit Television system is used to record the inside of pipes and report the obstruction level, which may trigger a cleaning operative. Currently, the obstruction level assessment is done manually, which is time-consuming and inconsistent. In this work, we design a methodology to train a Convolutional Neural Network for identifying the level of obstruction in pipes, thus reducing the human effort required on such a frequent and repetitive task. We gathered a database of videos that are explored and adapted to generate useful frames to fed into the model. Our resulting classifier obtains deployment ready performances. To validate the consistency of the approach and its industrial applicability, we integrate the Layer-wise Relevance Propagation explainability technique, which enables us to further understand the behavior of the neural network for this task. In the end, the proposed system can provide higher speed, accuracy, and consistency in the process of sewer examination. Our analysis also uncovers some guidelines on how to further improve the quality of the data gathering methodology.



### A Two-Stream Symmetric Network with Bidirectional Ensemble for Aerial Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2002.01325v1
- **DOI**: 10.3390/rs12030465
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01325v1)
- **Published**: 2020-02-04 14:38:18+00:00
- **Updated**: 2020-02-04 14:38:18+00:00
- **Authors**: Jae-Hyun Park, Woo-Jeoung Nam, Seong-Whan Lee
- **Comment**: 20pages
- **Journal**: Remote Sens. 12(3) (2020) 465-484
- **Summary**: In this paper, we propose a novel method to precisely match two aerial images that were obtained in different environments via a two-stream deep network. By internally augmenting the target image, the network considers the two-stream with the three input images and reflects the additional augmented pair in the training. As a result, the training process of the deep network is regularized and the network becomes robust for the variance of aerial images. Furthermore, we introduce an ensemble method that is based on the bidirectional network, which is motivated by the isomorphic nature of the geometric transformation. We obtain two global transformation parameters without any additional network or parameters, which alleviate asymmetric matching results and enable significant improvement in performance by fusing two outcomes. For the experiment, we adopt aerial images from Google Earth and the International Society for Photogrammetry and Remote Sensing (ISPRS). To quantitatively assess our result, we apply the probability of correct keypoints (PCK) metric, which measures the degree of matching. The qualitative and quantitative results show the sizable gap of performance compared to the conventional methods for matching the aerial images. All code and our trained model, as well as the dataset are available online.



### Unified machine learning: Classification with simultaneous observed and unobserved novelty detection
- **Arxiv ID**: http://arxiv.org/abs/2002.01368v7
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.01368v7)
- **Published**: 2020-02-04 15:32:23+00:00
- **Updated**: 2023-06-28 12:00:14+00:00
- **Authors**: Emile R. Engelbrecht, Johan A. du Preez
- **Comment**: 7 Pages
- **Journal**: None
- **Summary**: A unified approach of Positive and Unlabelled (PU)-learning, Semi-Supervised Learning (SSL), and Open-Set Recognition (OSR) would significantly enhance the development of cost-efficient application-grade classifiers. However, previous attempts have conflated the definitions of \mbox{\textit{observed}} and \mbox{\textit{unobserved}} novel categories. Observed novel categories are defined in PU-learning as those in unlabelled training data and exist due to an incomplete set of category labels for the training set. In contrast, unobserved novel categories are defined in OSR as those that only exist in the testing data and represent new and interesting patterns that emerge over time. To maintain safe and practical classifier development, models must generalise the difference between these novel category types. In this letter, we thoroughly review the relevant machine learning research fields to propose a new unified machine learning policy called Open-set Learning with Augmented Categories by exploiting Unlabelled data or Open-LACU. Specifically, Open-LACU requires models to accurately classify $K > 1$ number of labelled categories while simultaneously detecting and separating observed novel categories into the augmented background category ($K + 1$) and further detecting and separating unobserved novel categories into the augmented unknown category ($K + 2$). Open-LACU is the first machine learning policy to generalise observed and unobserved novel categories. The significance of Open-LACU is also highlighted by discussing its application in semantic segmentation of remote sensing images, object detection within medical radiology images and disease identification through cough sound analysis.



### Combining 3D Model Contour Energy and Keypoints for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.01379v1
- **DOI**: 10.1007/978-3-030-01258-8_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01379v1)
- **Published**: 2020-02-04 15:53:26+00:00
- **Updated**: 2020-02-04 15:53:26+00:00
- **Authors**: Bogdan Bugaev, Anton Kryshchenko, Roman Belov
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: We present a new combined approach for monocular model-based 3D tracking. A preliminary object pose is estimated by using a keypoint-based technique. The pose is then refined by optimizing the contour energy function. The energy determines the degree of correspondence between the contour of the model projection and the image edges. It is calculated based on both the intensity and orientation of the raw image gradient. For optimization, we propose a technique and search area constraints that allow overcoming the local optima and taking into account information obtained through keypoint-based pose estimation. Owing to its combined nature, our method eliminates numerous issues of keypoint-based and edge-based approaches. We demonstrate the efficiency of our method by comparing it with state-of-the-art methods on a public benchmark dataset that includes videos with various lighting conditions, movement patterns, and speed.



### Action Graphs: Weakly-supervised Action Localization with Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.01449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01449v1)
- **Published**: 2020-02-04 18:21:10+00:00
- **Updated**: 2020-02-04 18:21:10+00:00
- **Authors**: Maheen Rashid, Hedvig Kjellström, Yong Jae Lee
- **Comment**: Accepted at WACV 2020
- **Journal**: None
- **Summary**: We present a method for weakly-supervised action localization based on graph convolutions. In order to find and classify video time segments that correspond to relevant action classes, a system must be able to both identify discriminative time segments in each video, and identify the full extent of each action. Achieving this with weak video level labels requires the system to use similarity and dissimilarity between moments across videos in the training data to understand both how an action appears, as well as the sub-actions that comprise the action's full extent. However, current methods do not make explicit use of similarity between video moments to inform the localization and classification predictions. We present a novel method that uses graph convolutions to explicitly model similarity between video moments. Our method utilizes similarity graphs that encode appearance and motion, and pushes the state of the art on THUMOS '14, ActivityNet 1.2, and Charades for weakly supervised action localization.



### Measuring the Utilization of Public Open Spaces by Deep Learning: a Benchmark Study at the Detroit Riverfront
- **Arxiv ID**: http://arxiv.org/abs/2002.01461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.01461v1)
- **Published**: 2020-02-04 18:38:19+00:00
- **Updated**: 2020-02-04 18:38:19+00:00
- **Authors**: Peng Sun, Rui Hou, Jerome Lynch
- **Comment**: None
- **Journal**: None
- **Summary**: Physical activities and social interactions are essential activities that ensure a healthy lifestyle. Public open spaces (POS), such as parks, plazas and greenways, are key environments that encourage those activities. To evaluate a POS, there is a need to study how humans use the facilities within it. However, traditional approaches to studying use of POS are manual and therefore time and labor intensive. They also may only provide qualitative insights. It is appealing to make use of surveillance cameras and to extract user-related information through computer vision. This paper proposes a proof-of-concept deep learning computer vision framework for measuring human activities quantitatively in POS and demonstrates a case study of the proposed framework using the Detroit Riverfront Conservancy (DRFC) surveillance camera network. A custom image dataset is presented to train the framework; the dataset includes 7826 fully annotated images collected from 18 cameras across the DRFC park space under various illumination conditions. Dataset analysis is also provided as well as a baseline model for one-step user localization and activity recognition. The mAP results are 77.5\% for {\it pedestrian} detection and 81.6\% for {\it cyclist} detection. Behavioral maps are autonomously generated by the framework to locate different POS users and the average error for behavioral localization is within 10 cm.



### Visual Concept-Metaconcept Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.01464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.01464v1)
- **Published**: 2020-02-04 18:42:30+00:00
- **Updated**: 2020-02-04 18:42:30+00:00
- **Authors**: Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: NeurIPS 2019. First two authors contributed equally. Project page:
  http://vcml.csail.mit.edu/
- **Journal**: None
- **Summary**: Humans reason with concepts and metaconcepts: we recognize red and green from visual input; we also understand that they describe the same property of objects (i.e., the color). In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and green describe the same property of objects, we generalize to the fact that cube and sphere also describe the same property of objects, since they both categorize the shape of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data. From just a few examples of purple cubes we can understand a new color purple, which resembles the hue of the cubes instead of the shape of them. Evaluation on both synthetic and real-world datasets validates our claims.



### Privacy-Preserving Image Sharing via Sparsifying Layers on Convolutional Groups
- **Arxiv ID**: http://arxiv.org/abs/2002.01469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.01469v1)
- **Published**: 2020-02-04 18:54:52+00:00
- **Updated**: 2020-02-04 18:54:52+00:00
- **Authors**: Sohrab Ferdowsi, Behrooz Razeghi, Taras Holotyak, Flavio P. Calmon, Slava Voloshynovskiy
- **Comment**: Accepted as an oral presentation for ICASSP 2020
- **Journal**: None
- **Summary**: We propose a practical framework to address the problem of privacy-aware image sharing in large-scale setups. We argue that, while compactness is always desired at scale, this need is more severe when trying to furthermore protect the privacy-sensitive content. We therefore encode images, such that, from one hand, representations are stored in the public domain without paying the huge cost of privacy protection, but ambiguated and hence leaking no discernible content from the images, unless a combinatorially-expensive guessing mechanism is available for the attacker. From the other hand, authorized users are provided with very compact keys that can easily be kept secure. This can be used to disambiguate and reconstruct faithfully the corresponding access-granted images. We achieve this with a convolutional autoencoder of our design, where feature maps are passed independently through sparsifying transformations, providing multiple compact codes, each responsible for reconstructing different attributes of the image. The framework is tested on a large-scale database of images with public implementation available.



