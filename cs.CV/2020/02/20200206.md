# Arxiv Papers in cs.CV on 2020-02-06
### Residual-Recursion Autoencoder for Shape Illustration Images
- **Arxiv ID**: http://arxiv.org/abs/2002.02063v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02063v1)
- **Published**: 2020-02-06 01:51:13+00:00
- **Updated**: 2020-02-06 01:51:13+00:00
- **Authors**: Qianwei Zhou, Peng Tao, Xiaoxin Li, Shengyong Chen, Fan Zhang, Haigen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Shape illustration images (SIIs) are common and important in describing the cross-sections of industrial products. Same as MNIST, the handwritten digit images, SIIs are gray or binary and containing shapes that are surrounded by large areas of blanks. In this work, Residual-Recursion Autoencoder (RRAE) has been proposed to extract low-dimensional features from SIIs while maintaining reconstruction accuracy as high as possible. RRAE will try to reconstruct the original image several times and recursively fill the latest residual image to the reserved channel of the encoder's input before the next trial of reconstruction. As a kind of neural network training framework, RRAE can wrap over other autoencoders and increase their performance. From experiment results, the reconstruction loss is decreased by 86.47% for convolutional autoencoder with high-resolution SIIs, 10.77% for variational autoencoder and 8.06% for conditional variational autoencoder with MNIST.



### Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.02077v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02077v6)
- **Published**: 2020-02-06 02:45:25+00:00
- **Updated**: 2021-06-15 21:41:54+00:00
- **Authors**: Akshay Rangesh, Bowen Zhang, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: A driver's gaze is critical for determining their attention, state, situational awareness, and readiness to take over control from partially automated vehicles. Estimating the gaze direction is the most obvious way to gauge a driver's state under ideal conditions when limited to using non-intrusive imaging sensors. Unfortunately, the vehicular environment introduces a variety of challenges that are usually unaccounted for - harsh illumination, nighttime conditions, and reflective eyeglasses. Relying on head pose alone under such conditions can prove to be unreliable and erroneous. In this study, we offer solutions to address these problems encountered in the real world. To solve issues with lighting, we demonstrate that using an infrared camera with suitable equalization and normalization suffices. To handle eyeglasses and their corresponding artifacts, we adopt image-to-image translation using generative adversarial networks to pre-process images prior to gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is trained to preserve the driver's gaze while removing potential eyeglasses from face images. GPCycleGAN is based on the well-known CycleGAN approach - with the addition of a gaze classifier and a gaze consistency loss for additional supervision. Our approach exhibits improved performance, interpretability, robustness and superior qualitative results on challenging real-world datasets.



### Forensic Scanner Identification Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.02079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02079v1)
- **Published**: 2020-02-06 02:48:09+00:00
- **Updated**: 2020-02-06 02:48:09+00:00
- **Authors**: Ruiting Shao, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the increasing availability and functionality of image editing tools, many forensic techniques such as digital image authentication, source identification and tamper detection are important for forensic image analysis. In this paper, we describe a machine learning based system to address the forensic analysis of scanner devices. The proposed system uses deep-learning to automatically learn the intrinsic features from various scanned images. Our experimental results show that high accuracy can be achieved for source scanner identification. The proposed system can also generate a reliability map that indicates the manipulated regions in an scanned image.



### An Information-rich Sampling Technique over Spatio-Temporal CNN for Classification of Human Actions in Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.02100v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02100v2)
- **Published**: 2020-02-06 05:07:41+00:00
- **Updated**: 2020-02-07 06:42:20+00:00
- **Authors**: S. H. Shabbeer Basha, Viswanath Pulabaigari, Snehasis Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel scheme for human action recognition in videos, using a 3-dimensional Convolutional Neural Network (3D CNN) based classifier. Traditionally in deep learning based human activity recognition approaches, either a few random frames or every $k^{th}$ frame of the video is considered for training the 3D CNN, where $k$ is a small positive integer, like 4, 5, or 6. This kind of sampling reduces the volume of the input data, which speeds-up training of the network and also avoids over-fitting to some extent, thus enhancing the performance of the 3D CNN model. In the proposed video sampling technique, consecutive $k$ frames of a video are aggregated into a single frame by computing a Gaussian-weighted summation of the $k$ frames. The resulting frame (aggregated frame) preserves the information in a better way than the conventional approaches and experimentally shown to perform better. In this paper, a 3D CNN architecture is proposed to extract the spatio-temporal features and follows Long Short-Term Memory (LSTM) to recognize human actions. The proposed 3D CNN architecture is capable of handling the videos where the camera is placed at a distance from the performer. Experiments are performed with KTH and WEIZMANN human actions datasets, whereby it is shown to produce comparable results with the state-of-the-art techniques.



### Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2002.02112v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02112v1)
- **Published**: 2020-02-06 06:03:04+00:00
- **Updated**: 2020-02-06 06:03:04+00:00
- **Authors**: Hyungrok Ham, Tae Joon Jun, Daeyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Unbalanced GANs, which pre-trains the generator of the generative adversarial network (GAN) using variational autoencoder (VAE). We guarantee the stable training of the generator by preventing the faster convergence of the discriminator at early epochs. Furthermore, we balance between the generator and the discriminator at early epochs and thus maintain the stabilized training of GANs. We apply Unbalanced GANs to well known public datasets and find that Unbalanced GANs reduce mode collapses. We also show that Unbalanced GANs outperform ordinary GANs in terms of stabilized learning, faster convergence and better image quality at early epochs.



### Pose-Aware Instance Segmentation Framework from Cone Beam CT Images for Tooth Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.02143v1
- **DOI**: 10.1016/j.compbiomed.2020.103720
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2002.02143v1)
- **Published**: 2020-02-06 07:57:34+00:00
- **Updated**: 2020-02-06 07:57:34+00:00
- **Authors**: Minyoung Chung, Minkyung Lee, Jioh Hong, Sanguk Park, Jusang Lee, Jingyu Lee, Jeongjin Lee, Yeong-Gil Shin
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Individual tooth segmentation from cone beam computed tomography (CBCT) images is an essential prerequisite for an anatomical understanding of orthodontic structures in several applications, such as tooth reformation planning and implant guide simulations. However, the presence of severe metal artifacts in CBCT images hinders the accurate segmentation of each individual tooth. In this study, we propose a neural network for pixel-wise labeling to exploit an instance segmentation framework that is robust to metal artifacts. Our method comprises of three steps: 1) image cropping and realignment by pose regressions, 2) metal-robust individual tooth detection, and 3) segmentation. We first extract the alignment information of the patient by pose regression neural networks to attain a volume-of-interest (VOI) region and realign the input image, which reduces the inter-overlapping area between tooth bounding boxes. Then, individual tooth regions are localized within a VOI realigned image using a convolutional detector. We improved the accuracy of the detector by employing non-maximum suppression and multiclass classification metrics in the region proposal network. Finally, we apply a convolutional neural network (CNN) to perform individual tooth segmentation by converting the pixel-wise labeling task to a distance regression task. Metal-intensive image augmentation is also employed for a robust segmentation of metal artifacts. The result shows that our proposed method outperforms other state-of-the-art methods, especially for teeth with metal artifacts. The primary significance of the proposed method is two-fold: 1) an introduction of pose-aware VOI realignment followed by a robust tooth detection and 2) a metal-robust CNN framework for accurate tooth segmentation.



### FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping
- **Arxiv ID**: http://arxiv.org/abs/2002.02159v1
- **DOI**: 10.1109/TVCG.2020.2973444
- **Categories**: **cs.GR**, cs.CV, cs.HC, H.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2002.02159v1)
- **Published**: 2020-02-06 08:56:46+00:00
- **Updated**: 2020-02-06 08:56:46+00:00
- **Authors**: Daiki Tone, Daisuke Iwai, Shinsaku Hiura, Kosuke Sato
- **Comment**: 11 pages, 14 figures
- **Journal**: None
- **Summary**: This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.



### Joint Deep Learning of Facial Expression Synthesis and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.02194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02194v1)
- **Published**: 2020-02-06 10:56:00+00:00
- **Updated**: 2020-02-06 10:56:00+00:00
- **Authors**: Yan Yan, Ying Huang, Si Chen, Chunhua Shen, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning based facial expression recognition (FER) methods have attracted considerable attention and they usually require large-scale labelled training data. Nonetheless, the publicly available facial expression databases typically contain a small amount of labelled data. In this paper, to overcome the above issue, we propose a novel joint deep learning of facial expression synthesis and recognition method for effective FER. More specifically, the proposed method involves a two-stage learning procedure. Firstly, a facial expression synthesis generative adversarial network (FESGAN) is pre-trained to generate facial images with different facial expressions. To increase the diversity of the training images, FESGAN is elaborately designed to generate images with new identities from a prior distribution. Secondly, an expression recognition network is jointly learned with the pre-trained FESGAN in a unified framework. In particular, the classification loss computed from the recognition network is used to simultaneously optimize the performance of both the recognition network and the generator of FESGAN. Moreover, in order to alleviate the problem of data bias between the real images and the synthetic images, we propose an intra-class loss with a novel real data-guided back-propagation (RDBP) algorithm to reduce the intra-class variations of images from the same class, which can significantly improve the final performance. Extensive experimental results on public facial expression databases demonstrate the superiority of the proposed method compared with several state-of-the-art FER methods.



### AnimePose: Multi-person 3D pose estimation and animation
- **Arxiv ID**: http://arxiv.org/abs/2002.02792v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02792v1)
- **Published**: 2020-02-06 11:11:56+00:00
- **Updated**: 2020-02-06 11:11:56+00:00
- **Authors**: Laxman Kumarapu, Prerana Mukherjee
- **Comment**: arXiv admin note: text overlap with arXiv:1907.11346 by other authors
- **Journal**: None
- **Summary**: 3D animation of humans in action is quite challenging as it involves using a huge setup with several motion trackers all over the person's body to track the movements of every limb. This is time-consuming and may cause the person discomfort in wearing exoskeleton body suits with motion sensors. In this work, we present a trivial yet effective solution to generate 3D animation of multiple persons from a 2D video using deep learning. Although significant improvement has been achieved recently in 3D human pose estimation, most of the prior works work well in case of single person pose estimation and multi-person pose estimation is still a challenging problem. In this work, we firstly propose a supervised multi-person 3D pose estimation and animation framework namely AnimePose for a given input RGB video sequence. The pipeline of the proposed system consists of various modules: i) Person detection and segmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for person localization iv) Person trajectory prediction and human pose tracking. Our proposed system produces comparable results on previous state-of-the-art 3D multi-person pose estimation methods on publicly available datasets MuCo-3DHP and MuPoTS-3D datasets and it also outperforms previous state-of-the-art human pose tracking methods by a significant margin of 11.7% performance gain on MOTA score on Posetrack 2018 dataset.



### RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2002.02200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02200v1)
- **Published**: 2020-02-06 11:16:24+00:00
- **Updated**: 2020-02-06 11:16:24+00:00
- **Authors**: Jean Lahoud, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Although well-known large-scale datasets, such as ImageNet, have driven image understanding forward, most of these datasets require extensive manual annotation and are thus not easily scalable. This limits the advancement of image understanding techniques. The impact of these large-scale datasets can be observed in almost every vision task and technique in the form of pre-training for initialization. In this work, we propose an easily scalable and self-supervised technique that can be used to pre-train any semantic RGB segmentation method. In particular, our pre-training approach makes use of automatically generated labels that can be obtained using depth sensors. These labels, denoted by HN-labels, represent different height and normal patches, which allow mining of local semantic information that is useful in the task of semantic RGB segmentation. We show how our proposed self-supervised pre-training with HN-labels can be used to replace ImageNet pre-training, while using 25x less images and without requiring any manual labeling. We pre-train a semantic segmentation network with our HN-labels, which resembles our final task more than pre-training on a less related task, e.g. classification with ImageNet. We evaluate on two datasets (NYUv2 and CamVid), and we show how the similarity in tasks is advantageous not only in speeding up the pre-training process, but also in achieving better final semantic segmentation accuracy than ImageNet pre-training



### From Data to Actions in Intelligent Transportation Systems: a Prescription of Functional Requirements for Model Actionability
- **Arxiv ID**: http://arxiv.org/abs/2002.02210v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.02210v3)
- **Published**: 2020-02-06 12:02:30+00:00
- **Updated**: 2021-02-08 15:17:27+00:00
- **Authors**: Ibai Lana, Javier J. Sanchez-Medina, Eleni I. Vlahogianni, Javier Del Ser
- **Comment**: 40 pages, 3 figures
- **Journal**: None
- **Summary**: Advances in Data Science permeate every field of Transportation Science and Engineering, resulting in developments in the transportation sector that {are} data-driven. Nowadays, Intelligent Transportation Systems (ITS) could be arguably approached as a ``story'' intensively producing and consuming large amounts of data. A~diversity of sensing devices densely spread over the infrastructure, vehicles or the travelers' personal devices act as sources of data flows that are eventually fed {into} software running on automatic devices, actuators or control systems producing, in~turn, complex information flows {among} users, traffic managers, data analysts, traffic modeling scientists, etc. These~information flows provide enormous opportunities to improve model development and decision-making. This work aims to describe how data, coming from diverse ITS sources, can be used to learn and adapt data-driven models for efficiently operating ITS assets, systems and processes; in~other words, for data-based models to fully become \emph{actionable}. Grounded in this described data modeling pipeline for ITS, we~define the characteristics, engineering requisites and challenges intrinsic to its three compounding stages, namely, data fusion, adaptive learning and model evaluation. We~deliberately generalize model learning to be adaptive, since, in~the core of our paper is the firm conviction that most learners will have to adapt to the ever-changing phenomenon scenario underlying the majority of ITS applications. Finally, we~provide a prospect of current research lines within Data Science that can bring notable advances to data-based ITS modeling, which will eventually bridge the gap towards the practicality and actionability of such models.



### Deep Learning for Classifying Food Waste
- **Arxiv ID**: http://arxiv.org/abs/2002.03786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2002.03786v1)
- **Published**: 2020-02-06 12:40:16+00:00
- **Updated**: 2020-02-06 12:40:16+00:00
- **Authors**: Amin Mazloumian, Matthias Rosenthal, Hans Gelke
- **Comment**: None
- **Journal**: None
- **Summary**: One third of food produced in the world for human consumption -- approximately 1.3 billion tons -- is lost or wasted every year. By classifying food waste of individual consumers and raising awareness of the measures, avoidable food waste can be significantly reduced. In this research, we use deep learning to classify food waste in half a million images captured by cameras installed on top of food waste bins. We specifically designed a deep neural network that classifies food waste for every time food waste is thrown in the waste bins. Our method presents how deep learning networks can be tailored to best learn from available training data.



### Unsupervised Bidirectional Cross-Modality Adaptation via Deeply Synergistic Image and Feature Alignment for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.02255v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02255v1)
- **Published**: 2020-02-06 13:49:47+00:00
- **Updated**: 2020-02-06 13:49:47+00:00
- **Authors**: Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng Ann Heng
- **Comment**: IEEE TMI
- **Journal**: None
- **Summary**: Unsupervised domain adaptation has increasingly gained interest in medical image computing, aiming to tackle the performance degradation of deep neural networks when being deployed to unseen data with heterogeneous characteristics. In this work, we present a novel unsupervised domain adaptation framework, named as Synergistic Image and Feature Alignment (SIFA), to effectively adapt a segmentation network to an unlabeled target domain. Our proposed SIFA conducts synergistic alignment of domains from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features by leveraging adversarial learning in multiple aspects and with a deeply supervised mechanism. The feature encoder is shared between both adaptive perspectives to leverage their mutual benefits via end-to-end learning. We have extensively evaluated our method with cardiac substructure segmentation and abdominal multi-organ segmentation for bidirectional cross-modality adaptation between MRI and CT images. Experimental results on two different tasks demonstrate that our SIFA method is effective in improving segmentation performance on unlabeled target images, and outperforms the state-of-the-art domain adaptation approaches by a large margin.



### Looking GLAMORous: Vehicle Re-Id in Heterogeneous Cameras Networks with Global and Local Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.02256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02256v1)
- **Published**: 2020-02-06 13:51:00+00:00
- **Updated**: 2020-02-06 13:51:00+00:00
- **Authors**: Abhijit Suprem, Calton Pu
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (re-id) is a fundamental problem for modern surveillance camera networks. Existing approaches for vehicle re-id utilize global features and local features for re-id by combining multiple subnetworks and losses. In this paper, we propose GLAMOR, or Global and Local Attention MOdules for Re-id. GLAMOR performs global and local feature extraction simultaneously in a unified model to achieve state-of-the-art performance in vehicle re-id across a variety of adversarial conditions and datasets (mAPs 80.34, 76.48, 77.15 on VeRi-776, VRIC, and VeRi-Wild, respectively). GLAMOR introduces several contributions: a better backbone construction method that outperforms recent approaches, group and layer normalization to address conflicting loss targets for re-id, a novel global attention module for global feature extraction, and a novel local attention module for self-guided part-based local feature extraction that does not require supervision. Additionally, GLAMOR is a compact and fast model that is 10x smaller while delivering 25% better performance.



### Person Re-identification by Contour Sketch under Moderate Clothing Change
- **Arxiv ID**: http://arxiv.org/abs/2002.02295v1
- **DOI**: 10.1109/TPAMI.2019.2960509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02295v1)
- **Published**: 2020-02-06 15:13:55+00:00
- **Updated**: 2020-02-06 15:13:55+00:00
- **Authors**: Qize Yang, Ancong Wu, Wei-Shi Zheng
- **Comment**: To appear in TPAMI
- **Journal**: None
- **Summary**: Person re-identification (re-id), the process of matching pedestrian images across different camera views, is an important task in visual surveillance. Substantial development of re-id has recently been observed, and the majority of existing models are largely dependent on color appearance and assume that pedestrians do not change their clothes across camera views. This limitation, however, can be an issue for re-id when tracking a person at different places and at different time if that person (e.g., a criminal suspect) changes his/her clothes, causing most existing methods to fail, since they are heavily relying on color appearance and thus they are inclined to match a person to another person wearing similar clothes. In this work, we call the person re-id under clothing change the "cross-clothes person re-id". In particular, we consider the case when a person only changes his clothes moderately as a first attempt at solving this problem based on visible light images; that is we assume that a person wears clothes of a similar thickness, and thus the shape of a person would not change significantly when the weather does not change substantially within a short period of time. We perform cross-clothes person re-id based on a contour sketch of person image to take advantage of the shape of the human body instead of color information for extracting features that are robust to moderate clothing change. Due to the lack of a large-scale dataset for cross-clothes person re-id, we contribute a new dataset that consists of 33698 images from 221 identities. Our experiments illustrate the challenges of cross-clothes person re-id and demonstrate the effectiveness of our proposed method.



### VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for Robot Swarms
- **Arxiv ID**: http://arxiv.org/abs/2002.02308v2
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2002.02308v2)
- **Published**: 2020-02-06 15:25:23+00:00
- **Updated**: 2020-12-10 14:10:23+00:00
- **Authors**: Ting-Kuei Hu, Fernando Gama, Tianlong Chen, Zhangyang Wang, Alejandro Ribeiro, Brian M. Sadler
- **Comment**: None
- **Journal**: None
- **Summary**: Decentralized coordination of a robot swarm requires addressing the tension between local perceptions and actions, and the accomplishment of a global objective. In this work, we propose to learn decentralized controllers based on solely raw visual inputs. For the first time, that integrates the learning of two key components: communication and visual perception, in one end-to-end framework. More specifically, we consider that each robot has access to a visual perception of the immediate surroundings, and communication capabilities to transmit and receive messages from other neighboring robots. Our proposed learning framework combines a convolutional neural network (CNN) for each robot to extract messages from the visual inputs, and a graph neural network (GNN) over the entire swarm to transmit, receive and process these messages in order to decide on actions. The use of a GNN and locally-run CNNs results naturally in a decentralized controller. We jointly train the CNNs and the GNN so that each robot learns to extract messages from the images that are adequate for the team as a whole. Our experiments demonstrate the proposed architecture in the problem of drone flocking and show its promising performance and scalability, e.g., achieving successful decentralized flocking for large-sized swarms consisting of up to 75 drones.



### Random VLAD based Deep Hashing for Efficient Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.02333v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, eess.IV, H.3; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2002.02333v1)
- **Published**: 2020-02-06 16:22:44+00:00
- **Updated**: 2020-02-06 16:22:44+00:00
- **Authors**: Li Weng, Lingzhi Ye, Jiangmin Tian, Jiuwen Cao, Jianzhong Wang
- **Comment**: 10 pages, 17 figures, submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Image hash algorithms generate compact binary representations that can be quickly matched by Hamming distance, thus become an efficient solution for large-scale image retrieval. This paper proposes RV-SSDH, a deep image hash algorithm that incorporates the classical VLAD (vector of locally aggregated descriptors) architecture into neural networks. Specifically, a novel neural network component is formed by coupling a random VLAD layer with a latent hash layer through a transform layer. This component can be combined with convolutional layers to realize a hash algorithm. We implement RV-SSDH as a point-wise algorithm that can be efficiently trained by minimizing classification error and quantization loss. Comprehensive experiments show this new architecture significantly outperforms baselines such as NetVLAD and SSDH, and offers a cost-effective trade-off in the state-of-the-art. In addition, the proposed random VLAD layer leads to satisfactory accuracy with low complexity, thus shows promising potentials as an alternative to NetVLAD.



### Lane Boundary Geometry Extraction from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2002.02362v1
- **DOI**: 10.1145/3149092.3149093
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02362v1)
- **Published**: 2020-02-06 17:10:35+00:00
- **Updated**: 2020-02-06 17:10:35+00:00
- **Authors**: Andi Zang, Runsheng Xu, Zichen Li, David Doria
- **Comment**: None
- **Journal**: In Proceedings of the 1st ACM SIGSPATIAL Workshop on
  High-Precision Maps and Intelligent Applications for Autonomous Vehicles,
  page 1. ACM, 2017
- **Summary**: Autonomous driving car is becoming more of a reality, as a key component,high-definition(HD) maps shows its value in both market place and industry. Even though HD maps generation from LiDAR or stereo/perspective imagery has achieved impressive success, its inherent defects cannot be ignored. In this paper, we proposal a novel method for Highway HD maps modeling using pixel-wise segmentation on satellite imagery and formalized hypotheses linking, which is cheaper and faster than current HD maps modeling approaches from LiDAR point cloud and perspective view imagery, and let it becomes an ideal complementary of state of the art. We also manual code/label an HD road model dataset as ground truth, aligned with Bing tile image server, to train, test and evaluate our methodology. This dataset will be publish at same time to contribute research in HD maps modeling from aerial imagery.



### StegColNet: Steganalysis based on an ensemble colorspace approach
- **Arxiv ID**: http://arxiv.org/abs/2002.02413v2
- **DOI**: 10.1007/978-3-030-73973-7_30
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02413v2)
- **Published**: 2020-02-06 17:44:25+00:00
- **Updated**: 2020-10-16 16:30:15+00:00
- **Authors**: Shreyank N Gowda, Chun Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Image steganography refers to the process of hiding information inside images. Steganalysis is the process of detecting a steganographic image. We introduce a steganalysis approach that uses an ensemble color space model to obtain a weighted concatenated feature activation map. The concatenated map helps to obtain certain features explicit to each color space. We use a levy-flight grey wolf optimization strategy to reduce the number of features selected in the map. We then use these features to classify the image into one of two classes: whether the given image has secret information stored or not. Extensive experiments have been done on a large scale dataset extracted from the Bossbase dataset. Also, we show that the model can be transferred to different datasets and perform extensive experiments on a mixture of datasets. Our results show that the proposed approach outperforms the recent state of the art deep learning steganalytical approaches by 2.32 percent on average for 0.2 bits per channel (bpc) and 1.87 percent on average for 0.4 bpc.



### Reliability Validation of Learning Enabled Vehicle Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.02424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.02424v1)
- **Published**: 2020-02-06 18:07:54+00:00
- **Updated**: 2020-02-06 18:07:54+00:00
- **Authors**: Youcheng Sun, Yifan Zhou, Simon Maskell, James Sharp, Xiaowei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components -- to process the imagery inputs -- and multiple symbolic (Kalman filter) components -- to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which cannot be determined by analysing the deep learning components only. This research suggests the need for novel verification and validation methods for learning-enabled systems.



### Continuous Geodesic Convolutions for Learning on 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2002.02506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02506v1)
- **Published**: 2020-02-06 20:37:31+00:00
- **Updated**: 2020-02-06 20:37:31+00:00
- **Authors**: Zhangsihao Yang, Or Litany, Tolga Birdal, Srinath Sridhar, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of descriptor-based methods for geometric processing of non-rigid shape rely on hand-crafted descriptors. Recently, learning-based techniques have been shown effective, achieving state-of-the-art results in a variety of tasks. Yet, even though these methods can in principle work directly on raw data, most methods still rely on hand-crafted descriptors at the input layer. In this work, we wish to challenge this practice and use a neural network to learn descriptors directly from the raw mesh. To this end, we introduce two modules into our neural architecture. The first is a local reference frame (LRF) used to explicitly make the features invariant to rigid transformations. The second is continuous convolution kernels that provide robustness to sampling. We show the efficacy of our proposed network in learning on raw meshes using two cornerstone tasks: shape matching, and human body parts segmentation. Our results show superior results over baseline methods that use hand-crafted descriptors.



### Contradictory Structure Learning for Semi-supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.02545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02545v2)
- **Published**: 2020-02-06 22:58:20+00:00
- **Updated**: 2021-02-14 19:58:09+00:00
- **Authors**: Can Qin, Lichen Wang, Qianqian Ma, Yu Yin, Huan Wang, Yun Fu
- **Comment**: 8 pages without citations
- **Journal**: None
- **Summary**: Current adversarial adaptation methods attempt to align the cross-domain features, whereas two challenges remain unsolved: 1) the conditional distribution mismatch and 2) the bias of the decision boundary towards the source domain. To solve these challenges, we propose a novel framework for semi-supervised domain adaptation by unifying the learning of opposite structures (UODA). UODA consists of a generator and two classifiers (i.e., the source-scattering classifier and the target-clustering classifier), which are trained for contradictory purposes. The target-clustering classifier attempts to cluster the target features to improve intra-class density and enlarge inter-class divergence. Meanwhile, the source-scattering classifier is designed to scatter the source features to enhance the decision boundary's smoothness. Through the alternation of source-feature expansion and target-feature clustering procedures, the target features are well-enclosed within the dilated boundary of the corresponding source features. This strategy can make the cross-domain features to be precisely aligned against the source bias simultaneously. Moreover, to overcome the model collapse through training, we progressively update the measurement of feature's distance and their representation via an adversarial training paradigm. Extensive experiments on the benchmarks of DomainNet and Office-home datasets demonstrate the superiority of our approach over the state-of-the-art methods.



### Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow
- **Arxiv ID**: http://arxiv.org/abs/2002.02547v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02547v3)
- **Published**: 2020-02-06 22:58:51+00:00
- **Updated**: 2020-10-30 17:05:37+00:00
- **Authors**: Didrik Nielsen, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: Flow models have recently made great progress at modeling ordinal discrete data such as images and audio. Due to the continuous nature of flow models, dequantization is typically applied when using them for such discrete data, resulting in lower bound estimates of the likelihood. In this paper, we introduce subset flows, a class of flows that can tractably transform finite volumes and thus allow exact computation of likelihoods for discrete data. Based on subset flows, we identify ordinal discrete autoregressive models, including WaveNets, PixelCNNs and Transformers, as single-layer flows. We use the flow formulation to compare models trained and evaluated with either the exact likelihood or its dequantization lower bound. Finally, we study multilayer flows composed of PixelCNNs and non-autoregressive coupling layers and demonstrate state-of-the-art results on CIFAR-10 for flow models trained with dequantization.



### Impact of ImageNet Model Selection on Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.02559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02559v1)
- **Published**: 2020-02-06 23:58:23+00:00
- **Updated**: 2020-02-06 23:58:23+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: In 2020 IEEE Winter Applications of Computer Vision Workshops
  (WACVW)
- **Summary**: Deep neural networks are widely used in image classification problems. However, little work addresses how features from different deep neural networks affect the domain adaptation problem. Existing methods often extract deep features from one ImageNet model, without exploring other neural networks. In this paper, we investigate how different ImageNet models affect transfer accuracy on domain adaptation problems. We extract features from sixteen distinct pre-trained ImageNet models and examine the performance of twelve benchmarking methods when using the features. Extensive experimental results show that a higher accuracy ImageNet model produces better features, and leads to higher accuracy on domain adaptation problems (with a correlation coefficient of up to 0.95). We also examine the architecture of each neural network to find the best layer for feature extraction. Together, performance from our features exceeds that of the state-of-the-art in three benchmark datasets.



