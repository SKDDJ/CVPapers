# Arxiv Papers in cs.CV on 2020-02-21
### Face Phylogeny Tree Using Basis Functions
- **Arxiv ID**: http://arxiv.org/abs/2002.09068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09068v2)
- **Published**: 2020-02-21 00:13:21+00:00
- **Updated**: 2020-03-13 20:35:06+00:00
- **Authors**: Sudipta Banerjee, Arun Ross
- **Comment**: Updated paper particulalrly Section 4.2.7
- **Journal**: None
- **Summary**: Photometric transformations, such as brightness and contrast adjustment, can be applied to a face image repeatedly creating a set of near-duplicate images. Identifying the original image from a set of such near-duplicates and deducing the relationship between them are important in the context of digital image forensics. This is commonly done by generating an image phylogeny tree \textemdash \hspace{0.08cm} a hierarchical structure depicting the relationship between a set of near-duplicate images. In this work, we utilize three different families of basis functions to model pairwise relationships between near-duplicate images. The basis functions used in this work are orthogonal polynomials, wavelet basis functions and radial basis functions. We perform extensive experiments to assess the performance of the proposed method across three different modalities, namely, face, fingerprint and iris images; across different image phylogeny tree configurations; and across different types of photometric transformations. We also utilize the same basis functions to model geometric transformations and deep-learning based transformations. We also perform extensive analysis of each basis function with respect to its ability to model arbitrary transformations and to distinguish between the original and the transformed images. Finally, we utilize the concept of approximate von Neumann graph entropy to explain the success and failure cases of the proposed IPT generation algorithm. Experiments indicate that the proposed algorithm generalizes well across different scenarios thereby suggesting the merits of using basis functions to model the relationship between photometrically and geometrically modified images.



### Leveraging Photogrammetric Mesh Models for Aerial-Ground Feature Point Matching Toward Integrated 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2002.09085v2
- **DOI**: 10.1016/j.isprsjprs.2020.05.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09085v2)
- **Published**: 2020-02-21 01:47:59+00:00
- **Updated**: 2020-05-28 07:41:15+00:00
- **Authors**: Qing Zhu, Zhendong Wang, Han Hu, Linfu Xie, Xuming Ge, Yeting Zhang
- **Comment**: Accepted for publication in ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: Integration of aerial and ground images has been proved as an efficient approach to enhance the surface reconstruction in urban environments. However, as the first step, the feature point matching between aerial and ground images is remarkably difficult, due to the large differences in viewpoint and illumination conditions. Previous studies based on geometry-aware image rectification have alleviated this problem, but the performance and convenience of this strategy is limited by several flaws, e.g. quadratic image pairs, segregated extraction of descriptors and occlusions. To address these problems, we propose a novel approach: leveraging photogrammetric mesh models for aerial-ground image matching. The methods of this proposed approach have linear time complexity with regard to the number of images, can explicitly handle low overlap using multi-view images and can be directly injected into off-the-shelf structure-from-motion (SfM) and multi-view stereo (MVS) solutions. First, aerial and ground images are reconstructed separately and initially co-registered through weak georeferencing data. Second, aerial models are rendered to the initial ground views, in which the color, depth and normal images are obtained. Then, the synthesized color images and the corresponding ground images are matched by comparing the descriptors, filtered by local geometrical information, and then propagated to the aerial views using depth images and patch-based matching. Experimental evaluations using various datasets confirm the superior performance of the proposed methods in aerial-ground image matching. In addition, incorporation of the existing SfM and MVS solutions into these methods enables more complete and accurate models to be directly obtained.



### Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.09103v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.09103v2)
- **Published**: 2020-02-21 02:57:13+00:00
- **Updated**: 2020-06-20 13:10:23+00:00
- **Authors**: Dmitry Molchanov, Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Vetrov
- **Comment**: None
- **Journal**: None
- **Summary**: Test-time data augmentation$-$averaging the predictions of a machine learning model across multiple augmented samples of data$-$is a widely used technique that improves the predictive performance. While many advanced learnable data augmentation techniques have emerged in recent years, they are focused on the training phase. Such techniques are not necessarily optimal for test-time augmentation and can be outperformed by a policy consisting of simple crops and flips. The primary goal of this paper is to demonstrate that test-time augmentation policies can be successfully learned too. We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation. We demonstrate that augmentation policies learned with GPS achieve superior predictive performance on image classification problems, provide better in-domain uncertainty estimation, and improve the robustness to domain shift.



### Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras
- **Arxiv ID**: http://arxiv.org/abs/2002.09107v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.09107v2)
- **Published**: 2020-02-21 03:28:42+00:00
- **Updated**: 2021-03-31 18:48:24+00:00
- **Authors**: Iretiayo Akinola, Jacob Varley, Dmitry Kalashnikov
- **Comment**: Accepted at International Conference on Robotics and Automation (ICRA
  2020)
- **Journal**: None
- **Summary**: In this work, we present an effective multi-view approach to closed-loop end-to-end learning of precise manipulation tasks that are 3D in nature. Our method learns to accomplish these tasks using multiple statically placed but uncalibrated RGB camera views without building an explicit 3D representation such as a pointcloud or voxel grid. This multi-camera approach achieves superior task performance on difficult stacking and insertion tasks compared to single-view baselines. Single view robotic agents struggle from occlusion and challenges in estimating relative poses between points of interest. While full 3D scene representations (voxels or pointclouds) are obtainable from registered output of multiple depth sensors, several challenges complicate operating off such explicit 3D representations. These challenges include imperfect camera calibration, poor depth maps due to object properties such as reflective surfaces, and slower inference speeds over 3D representations compared to 2D images. Our use of static but uncalibrated cameras does not require camera-robot or camera-camera calibration making the proposed approach easy to setup and our use of \textit{sensor dropout} during training makes it resilient to the loss of camera-views after deployment.



### Affective Expression Analysis in-the-wild using Multi-Task Temporal Statistical Deep Learning Model
- **Arxiv ID**: http://arxiv.org/abs/2002.09120v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09120v3)
- **Published**: 2020-02-21 04:06:03+00:00
- **Updated**: 2020-03-05 08:23:46+00:00
- **Authors**: Nhu-Tai Do, Tram-Tran Nguyen-Quynh, Soo-Hyung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Affective behavior analysis plays an important role in human-computer interaction, customer marketing, health monitoring. ABAW Challenge and Aff-Wild2 dataset raise the new challenge for classifying basic emotions and regression valence-arousal value under in-the-wild environments. In this paper, we present an affective expression analysis model that deals with the above challenges. Our approach includes STAT and Temporal Module for fine-tuning again face feature model. We experimented on Aff-Wild2 dataset, a large-scale dataset for ABAW Challenge with the annotations for both the categorical and valence-arousal emotion. We achieved the expression score 0.543 and valence-arousal score 0.534 on the validation set.



### Convolutional Tensor-Train LSTM for Spatio-temporal Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.09131v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09131v5)
- **Published**: 2020-02-21 05:00:01+00:00
- **Updated**: 2020-10-04 23:14:31+00:00
- **Authors**: Jiahao Su, Wonmin Byeon, Jean Kossaifi, Furong Huang, Jan Kautz, Animashree Anandkumar
- **Comment**: Jiahao Su and Wonmin Byeon contributed equally to this work. 22
  pages, 14 figures, NeurIPS 2020
- **Journal**: None
- **Summary**: Learning from spatio-temporal data has numerous applications such as human-behavior analysis, object tracking, video compression, and physics simulation.However, existing methods still perform poorly on challenging video tasks such as long-term forecasting. This is because these kinds of challenging tasks require learning long-term spatio-temporal correlations in the video sequence. In this paper, we propose a higher-order convolutional LSTM model that can efficiently learn these correlations, along with a succinct representations of the history. This is accomplished through a novel tensor train module that performs prediction by combining convolutional features across time. To make this feasible in terms of computation and memory requirements, we propose a novel convolutional tensor-train decomposition of the higher-order model. This decomposition reduces the model complexity by jointly approximating a sequence of convolutional kernels asa low-rank tensor-train factorization. As a result, our model outperforms existing approaches, but uses only a fraction of parameters, including the baseline models.Our results achieve state-of-the-art performance in a wide range of applications and datasets, including the multi-steps video prediction on the Moving-MNIST-2and KTH action datasets as well as early activity recognition on the Something-Something V2 dataset.



### Kullback-Leibler Divergence-Based Fuzzy $C$-Means Clustering Incorporating Morphological Reconstruction and Wavelet Frames for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.09479v2
- **DOI**: None
- **Categories**: **cs.CV**, 62H30, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2002.09479v2)
- **Published**: 2020-02-21 05:19:10+00:00
- **Updated**: 2020-07-01 02:58:29+00:00
- **Authors**: Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou
- **Comment**: This paper has been withdrawn by the author due to a crucial
  definition error of objective function
- **Journal**: None
- **Summary**: Although spatial information of images usually enhance the robustness of the Fuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for image segmentation. To achieve a sound trade-off between the segmentation performance and the speed of clustering, we come up with a Kullback-Leibler (KL) divergence-based FCM algorithm by incorporating a tight wavelet frame transform and a morphological reconstruction operation. To enhance FCM's robustness, an observed image is first filtered by using the morphological reconstruction. A tight wavelet frame system is employed to decompose the observed and filtered images so as to form their feature sets. Considering these feature sets as data of clustering, an modified FCM algorithm is proposed, which introduces a KL divergence term in the partition matrix into its objective function. The KL divergence term aims to make membership degrees of each image pixel closer to those of its neighbors, which brings that the membership partition becomes more suitable and the parameter setting of FCM becomes simplified. On the basis of the obtained partition matrix and prototypes, the segmented feature set is reconstructed by minimizing the inverse process of the modified objective function. To modify abnormal features produced in the reconstruction process, each reconstructed feature is reassigned to the closest prototype. As a result, the segmentation accuracy of KL divergence-based FCM is further improved. What's more, the segmented image is reconstructed by using a tight wavelet frame reconstruction operation. Finally, supporting experiments coping with synthetic, medical and color images are reported. Experimental results exhibit that the proposed algorithm works well and comes with better segmentation performance than other comparative algorithms. Moreover, the proposed algorithm requires less time than most of the FCM-related algorithms.



### Disentangling Controllable Object through Video Prediction Improves Visual Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.09136v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09136v1)
- **Published**: 2020-02-21 05:43:34+00:00
- **Updated**: 2020-02-21 05:43:34+00:00
- **Authors**: Yuanyi Zhong, Alexander Schwing, Jian Peng
- **Comment**: Accepted to ICASSP 2020
- **Journal**: None
- **Summary**: In many vision-based reinforcement learning (RL) problems, the agent controls a movable object in its visual field, e.g., the player's avatar in video games and the robotic arm in visual grasping and manipulation. Leveraging action-conditioned video prediction, we propose an end-to-end learning framework to disentangle the controllable object from the observation signal. The disentangled representation is shown to be useful for RL as additional observation channels to the agent. Experiments on a set of Atari games with the popular Double DQN algorithm demonstrate improved sample efficiency and game performance (from 222.8% to 261.4% measured in normalized game scores, with prediction bonus reward).



### Robust Iris Presentation Attack Detection Fusing 2D and 3D Information
- **Arxiv ID**: http://arxiv.org/abs/2002.09137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09137v2)
- **Published**: 2020-02-21 05:44:38+00:00
- **Updated**: 2020-08-05 17:38:41+00:00
- **Authors**: Zhaoyuan Fang, Adam Czajka, Kevin W. Bowyer
- **Comment**: Accepted to IEEE T-IFS
- **Journal**: None
- **Summary**: Diversity and unpredictability of artifacts potentially presented to an iris sensor calls for presentation attack detection methods that are agnostic to specificity of presentation attack instruments. This paper proposes a method that combines two-dimensional and three-dimensional properties of the observed iris to address the problem of spoof detection in case when some properties of artifacts are unknown. The 2D (textural) iris features are extracted by a state-of-the-art method employing Binary Statistical Image Features (BSIF) and an ensemble of classifiers is used to deliver 2D modality-related decision. The 3D (shape) iris features are reconstructed by a photometric stereo method from only two images captured under near-infrared illumination placed at two different angles, as in many current commercial iris recognition sensors. The map of normal vectors is used to assess the convexity of the observed iris surface. The combination of these two approaches has been applied to detect whether a subject is wearing a textured contact lens to disguise their identity. Extensive experiments with NDCLD'15 dataset, and a newly collected NDIris3D dataset show that the proposed method is highly robust under various open-set testing scenarios, and that it outperforms all available open-source iris PAD methods tested in identical scenarios. The source code and the newly prepared benchmark are made available along with this paper.



### SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances
- **Arxiv ID**: http://arxiv.org/abs/2002.09147v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09147v1)
- **Published**: 2020-02-21 06:10:34+00:00
- **Updated**: 2020-02-21 06:10:34+00:00
- **Authors**: Yancheng Pan, Biao Gao, Jilin Mei, Sibo Geng, Chengkun Li, Huijing Zhao
- **Comment**: submited to IEEE Intelligent Vehicles Symposium(2020)
- **Journal**: None
- **Summary**: 3D semantic segmentation is one of the key tasks for autonomous driving system. Recently, deep learning models for 3D semantic segmentation task have been widely researched, but they usually require large amounts of training data. However, the present datasets for 3D semantic segmentation are lack of point-wise annotation, diversiform scenes and dynamic objects.   In this paper, we propose the SemanticPOSS dataset, which contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI. In addition, we evaluate several typical 3D semantic segmentation models on our SemanticPOSS dataset. Experimental results show that SemanticPOSS can help to improve the prediction accuracy of dynamic objects as people, car in some degree. SemanticPOSS will be published at \url{www.poss.pku.edu.cn}.



### Bidirectional Generative Modeling Using Adversarial Gradient Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.09161v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09161v3)
- **Published**: 2020-02-21 07:28:56+00:00
- **Updated**: 2020-06-30 03:59:02+00:00
- **Authors**: Xinwei Shen, Tong Zhang, Kani Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the general $f$-divergence formulation of bidirectional generative modeling, which includes VAE and BiGAN as special cases. We present a new optimization method for this formulation, where the gradient is computed using an adversarially learned discriminator. In our framework, we show that different divergences induce similar algorithms in terms of gradient evaluation, except with different scaling. Therefore this paper gives a general recipe for a class of principled $f$-divergence based generative modeling methods. Theoretical justifications and extensive empirical studies are provided to demonstrate the advantage of our approach over existing methods.



### Residual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2002.09168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09168v1)
- **Published**: 2020-02-21 07:49:26+00:00
- **Updated**: 2020-02-21 07:49:26+00:00
- **Authors**: Mengya Gao, Yujun Shen, Quanquan Li, Chen Change Loy
- **Comment**: 9 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is one of the most potent ways for model compression. The key idea is to transfer the knowledge from a deep teacher model (T) to a shallower student (S). However, existing methods suffer from performance degradation due to the substantial gap between the learning capacities of S and T. To remedy this problem, this work proposes Residual Knowledge Distillation (RKD), which further distills the knowledge by introducing an assistant (A). Specifically, S is trained to mimic the feature maps of T, and A aids this process by learning the residual error between them. In this way, S and A complement with each other to get better knowledge from T. Furthermore, we devise an effective method to derive S and A from a given model without increasing the total computational cost. Extensive experiments show that our approach achieves appealing results on popular classification datasets, CIFAR-100 and ImageNet, surpassing state-of-the-art methods.



### Unsupervised Enhancement of Soft-biometric Privacy with Negative Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.09181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09181v1)
- **Published**: 2020-02-21 08:37:16+00:00
- **Updated**: 2020-02-21 08:37:16+00:00
- **Authors**: Philipp Terhörst, Marco Huber, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Currently under review
- **Journal**: None
- **Summary**: Current research on soft-biometrics showed that privacy-sensitive information can be deduced from biometric templates of an individual. Since for many applications, these templates are expected to be used for recognition purposes only, this raises major privacy issues. Previous works focused on supervised privacy-enhancing solutions that require privacy-sensitive information about individuals and limit their application to the suppression of single and pre-defined attributes. Consequently, they do not take into account attributes that are not considered in the training. In this work, we present Negative Face Recognition (NFR), a novel face recognition approach that enhances the soft-biometric privacy on the template-level by representing face templates in a complementary (negative) domain. While ordinary templates characterize facial properties of an individual, negative templates describe facial properties that does not exist for this individual. This suppresses privacy-sensitive information from stored templates. Experiments are conducted on two publicly available datasets captured under controlled and uncontrolled scenarios on three privacy-sensitive attributes. The experiments demonstrate that our proposed approach reaches higher suppression rates than previous work, while maintaining higher recognition performances as well. Unlike previous works, our approach does not require privacy-sensitive labels and offers a more comprehensive privacy-protection not limited to pre-defined attributes.



### AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent Videos with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.10981v1
- **DOI**: 10.1109/TMM.2020.3005033
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.10981v1)
- **Published**: 2020-02-21 09:08:28+00:00
- **Updated**: 2020-02-21 09:08:28+00:00
- **Authors**: Sanchita Ghose, John J. Prevost
- **Comment**: 14 pages, 14 figures
- **Journal**: IEEE TRANSACTIONS ON MULTIMEDIA, 2020
- **Summary**: In movie productions, the Foley Artist is responsible for creating an overlay soundtrack that helps the movie come alive for the audience. This requires the artist to first identify the sounds that will enhance the experience for the listener thereby reinforcing the Directors's intention for a given scene. In this paper, we present AutoFoley, a fully-automated deep learning tool that can be used to synthesize a representative audio track for videos. AutoFoley can be used in the applications where there is either no corresponding audio file associated with the video or in cases where there is a need to identify critical scenarios and provide a synthesized, reinforced soundtrack. An important performance criterion of the synthesized soundtrack is to be time-synchronized with the input video, which provides for a realistic and believable portrayal of the synthesized sound. Unlike existing sound prediction and generation architectures, our algorithm is capable of precise recognition of actions as well as inter-frame relations in fast moving video clips by incorporating an interpolation technique and Temporal Relationship Networks (TRN). We employ a robust multi-scale Recurrent Neural Network (RNN) associated with a Convolutional Neural Network (CNN) for a better understanding of the intricate input-to-output associations over time. To evaluate AutoFoley, we create and introduce a large scale audio-video dataset containing a variety of sounds frequently used as Foley effects in movies. Our experiments show that the synthesized sounds are realistically portrayed with accurate temporal synchronization of the associated visual inputs. Human qualitative testing of AutoFoley show over 73% of the test subjects considered the generated soundtrack as original, which is a noteworthy improvement in cross-modal research in sound synthesis.



### Curating Social Media Data
- **Arxiv ID**: http://arxiv.org/abs/2002.09202v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09202v1)
- **Published**: 2020-02-21 10:07:15+00:00
- **Updated**: 2020-02-21 10:07:15+00:00
- **Authors**: Kushal Vaghani
- **Comment**: Masters by Research Thesis
- **Journal**: None
- **Summary**: Social media platforms have empowered the democratization of the pulse of people in the modern era. Due to its immense popularity and high usage, data published on social media sites (e.g., Twitter, Facebook and Tumblr) is a rich ocean of information. Therefore data-driven analytics of social imprints has become a vital asset for organisations and governments to further improve their products and services. However, due to the dynamic and noisy nature of social media data, performing accurate analysis on raw data is a challenging task. A key requirement is to curate the raw data before fed into analytics pipelines. This curation process transforms the raw data into contextualized data and knowledge. We propose a data curation pipeline, namely CrowdCorrect, to enable analysts cleansing and curating social data and preparing it for reliable analytics. Our pipeline provides an automatic feature extraction from a corpus of social media data using existing in-house tools. Further, we offer a dual-correction mechanism using both automated and crowd-sourced approaches. The implementation of this pipeline also includes a set of tools for automatically creating micro-tasks to facilitate the contribution of crowd users in curating the raw data. For the purposes of this research, we use Twitter as our motivational social media data platform due to its popularity.



### Detection and Classification of Astronomical Targets with Deep Neural Networks in Wide Field Small Aperture Telescopes
- **Arxiv ID**: http://arxiv.org/abs/2002.09211v2
- **DOI**: 10.3847/1538-3881/ab800a
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09211v2)
- **Published**: 2020-02-21 10:35:31+00:00
- **Updated**: 2020-03-14 13:21:56+00:00
- **Authors**: Peng Jia, Qiang Liu, Yongyang Sun
- **Comment**: Accepted by Astronomical Journal. The complete code can be downloaded
  from https://doi.org/10.12149/101016. This code can be directly used to
  process images obtained by WFSATs. Images obtained by ordinary sky survey
  telescopes can also be processed with this code, however more annotated
  images are required to train the neural network
- **Journal**: None
- **Summary**: Wide field small aperture telescopes are widely used for optical transient observations. Detection and classification of astronomical targets in observed images are the most important and basic step. In this paper, we propose an astronomical targets detection and classification framework based on deep neural networks. Our framework adopts the concept of the Faster R-CNN and uses a modified Resnet-50 as backbone network and a Feature Pyramid Network to extract features from images of different astronomical targets. To increase the generalization ability of our framework, we use both simulated and real observation images to train the neural network. After training, the neural network could detect and classify astronomical targets automatically. We test the performance of our framework with simulated data and find that our framework has almost the same detection ability as that of the traditional method for bright and isolated sources and our framework has 2 times better detection ability for dim targets, albeit all celestial objects detected by the traditional method can be classified correctly. We also use our framework to process real observation data and find that our framework can improve 25 % detection ability than that of the traditional method when the threshold of our framework is 0.6. Rapid discovery of transient targets is quite important and we further propose to install our framework in embedded devices such as the Nvidia Jetson Xavier to achieve real-time astronomical targets detection and classification abilities.



### Stochastic Latent Residual Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2002.09219v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09219v4)
- **Published**: 2020-02-21 10:44:01+00:00
- **Updated**: 2020-08-07 14:37:21+00:00
- **Authors**: Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen, Sylvain Lamprier, Patrick Gallinari
- **Comment**: None
- **Journal**: Thirty-seventh International Conference on Machine Learning,
  International Machine Learning Society, Jul 2020, Vienne, Austria. pp.89--102
- **Summary**: Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent networks, which raises several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and temporal dynamics. However, no such model for stochastic video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model whose dynamics are governed in a latent space by a residual update rule. This first-order scheme is motivated by discretization schemes of differential equations. It naturally models video dynamics as it allows our simpler, more interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.



### Exploiting the Full Capacity of Deep Neural Networks while Avoiding Overfitting by Targeted Sparsity Regularization
- **Arxiv ID**: http://arxiv.org/abs/2002.09237v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2002.09237v1)
- **Published**: 2020-02-21 11:38:17+00:00
- **Updated**: 2020-02-21 11:38:17+00:00
- **Authors**: Karim Huesmann, Soeren Klemm, Lars Linsen, Benjamin Risse
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Overfitting is one of the most common problems when training deep neural networks on comparatively small datasets. Here, we demonstrate that neural network activation sparsity is a reliable indicator for overfitting which we utilize to propose novel targeted sparsity visualization and regularization strategies. Based on these strategies we are able to understand and counteract overfitting caused by activation sparsity and filter correlation in a targeted layer-by-layer manner. Our results demonstrate that targeted sparsity regularization can efficiently be used to regularize well-known datasets and architectures with a significant increase in image classification performance while outperforming both dropout and batch normalization. Ultimately, our study reveals novel insights into the contradicting concepts of activation sparsity and network capacity by demonstrating that targeted sparsity regularization enables salient and discriminative feature learning while exploiting the full capacity of deep models without suffering from overfitting, even when trained excessively.



### Efficient Learning of Model Weights via Changing Features During Training
- **Arxiv ID**: http://arxiv.org/abs/2002.09249v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09249v1)
- **Published**: 2020-02-21 12:38:14+00:00
- **Updated**: 2020-02-21 12:38:14+00:00
- **Authors**: Marcell Beregi-Kovács, Ágnes Baran, András Hajdu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a machine learning model, which dynamically changes the features during training. Our main motivation is to update the model in a small content during the training process with replacing less descriptive features to new ones from a large pool. The main benefit is coming from the fact that opposite to the common practice we do not start training a new model from the scratch, but can keep the already learned weights. This procedure allows the scan of a large feature pool which together with keeping the complexity of the model leads to an increase of the model accuracy within the same training time. The efficiency of our approach is demonstrated in several classic machine learning scenarios including linear regression and neural network-based training. As a specific analysis towards signal processing, we have successfully tested our approach on the database MNIST for digit classification considering single pixel and pixel-pairs intensities as possible features.



### Learning to Inpaint by Progressively Growing the Mask Regions
- **Arxiv ID**: http://arxiv.org/abs/2002.09280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09280v1)
- **Published**: 2020-02-21 13:33:05+00:00
- **Updated**: 2020-02-21 13:33:05+00:00
- **Authors**: Mohamed Abbas Hedjazi, Yakup Genc
- **Comment**: ICCV Workshop on Should we preregister experiments in computer
  vision?, Seoul, South Korea, 2019
- **Journal**: None
- **Summary**: Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models.



### 3D U-Net for Segmentation of Plant Root MRI Images in Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2002.09317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09317v1)
- **Published**: 2020-02-21 14:12:57+00:00
- **Updated**: 2020-02-21 14:12:57+00:00
- **Authors**: Yi Zhao, Nils Wandel, Magdalena Landl, Andrea Schnepf, Sven Behnke
- **Comment**: 6 pages, 5 figures, in the 28th European Symposium on Artificial
  Neural Networks
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) enables plant scientists to non-invasively study root system development and root-soil interaction. Challenging recording conditions, such as low resolution and a high level of noise hamper the performance of traditional root extraction algorithms, though. We propose to increase signal-to-noise ratio and resolution by segmenting the scanned volumes into root and soil in super-resolution using a 3D U-Net. Tests on real data show that the trained network is capable to detect most roots successfully and even finds roots that were missed by human annotators. Our experiments show that the segmentation performance can be further improved with modifications of the loss function.



### Human Activity Recognition using Multi-Head CNN followed by LSTM
- **Arxiv ID**: http://arxiv.org/abs/2003.06327v1
- **DOI**: 10.1109/ICET48972.2019.8994412
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06327v1)
- **Published**: 2020-02-21 14:29:59+00:00
- **Updated**: 2020-02-21 14:29:59+00:00
- **Authors**: Waqar Ahmad, Misbah Kazmi, Hazrat Ali
- **Comment**: IEEE ICET 2019
- **Journal**: None
- **Summary**: This study presents a novel method to recognize human physical activities using CNN followed by LSTM. Achieving high accuracy by traditional machine learning algorithms, (such as SVM, KNN and random forest method) is a challenging task because the data acquired from the wearable sensors like accelerometer and gyroscope is a time-series data. So, to achieve high accuracy, we propose a multi-head CNN model comprising of three CNNs to extract features for the data acquired from different sensors and all three CNNs are then merged, which are followed by an LSTM layer and a dense layer. The configuration of all three CNNs is kept the same so that the same number of features are obtained for every input to CNN. By using the proposed method, we achieve state-of-the-art accuracy, which is comparable to traditional machine learning algorithms and other deep neural network algorithms.



### The Automated Inspection of Opaque Liquid Vaccines
- **Arxiv ID**: http://arxiv.org/abs/2002.09406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09406v1)
- **Published**: 2020-02-21 16:45:29+00:00
- **Updated**: 2020-02-21 16:45:29+00:00
- **Authors**: Gregory Palmer, Benjamin Schnieders, Rahul Savani, Karl Tuyls, Joscha-David Fossel, Harry Flore
- **Comment**: 8 pages, 5 Figures, 3 Tables, ECAI 2020 Conference Proceedings
- **Journal**: None
- **Summary**: In the pharmaceutical industry the screening of opaque vaccines containing suspensions is currently a manual task carried out by trained human visual inspectors. We show that deep learning can be used to effectively automate this process. A moving contrast is required to distinguish anomalies from other particles, reflections and dust resting on a vial's surface. We train 3D-ConvNets to predict the likelihood of 20-frame video samples containing anomalies. Our unaugmented dataset consists of hand-labelled samples, recorded using vials provided by the HAL Allergy Group, a pharmaceutical company. We trained ten randomly initialized 3D-ConvNets to provide a benchmark, observing mean AUROC scores of 0.94 and 0.93 for positive samples (containing anomalies) and negative (anomaly-free) samples, respectively. Using Frame-Completion Generative Adversarial Networks we: (i) introduce an algorithm for computing saliency maps, which we use to verify that the 3D-ConvNets are indeed identifying anomalies; (ii) propose a novel self-training approach using the saliency maps to determine if multiple networks agree on the location of anomalies. Our self-training approach allows us to augment our data set by labelling 217,888 additional samples. 3D-ConvNets trained with our augmented dataset improve on the results we get when we train only on the unaugmented dataset.



### Calibrating Deep Neural Networks using Focal Loss
- **Arxiv ID**: http://arxiv.org/abs/2002.09437v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09437v2)
- **Published**: 2020-02-21 17:35:50+00:00
- **Updated**: 2020-10-26 14:22:17+00:00
- **Authors**: Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H. S. Torr, Puneet K. Dokania
- **Comment**: This paper was accepted at NeurIPS 2020
- **Journal**: None
- **Summary**: Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at https://github.com/torrvision/focal_calibration.



### Fine-Grained Instance-Level Sketch-Based Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.09461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.09461v1)
- **Published**: 2020-02-21 18:28:35+00:00
- **Updated**: 2020-02-21 18:28:35+00:00
- **Authors**: Peng Xu, Kun Liu, Tao Xiang, Timothy M. Hospedales, Zhanyu Ma, Jun Guo, Yi-Zhe Song
- **Comment**: None
- **Journal**: None
- **Summary**: Existing sketch-analysis work studies sketches depicting static objects or scenes. In this work, we propose a novel cross-modal retrieval problem of fine-grained instance-level sketch-based video retrieval (FG-SBVR), where a sketch sequence is used as a query to retrieve a specific target video instance. Compared with sketch-based still image retrieval, and coarse-grained category-level video retrieval, this is more challenging as both visual appearance and motion need to be simultaneously matched at a fine-grained level. We contribute the first FG-SBVR dataset with rich annotations. We then introduce a novel multi-stream multi-modality deep network to perform FG-SBVR under both strong and weakly supervised settings. The key component of the network is a relation module, designed to prevent model over-fitting given scarce training data. We show that this model significantly outperforms a number of existing state-of-the-art models designed for video analysis.



### Introducing Fuzzy Layers for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00880v1
- **DOI**: 10.1109/FUZZ-IEEE.2019.8858790
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00880v1)
- **Published**: 2020-02-21 19:33:30+00:00
- **Updated**: 2020-02-21 19:33:30+00:00
- **Authors**: Stanton R. Price, Steven R. Price, Derek T. Anderson
- **Comment**: 6 pages, 4 figures, published in 2019 IEEE International Conference
  on Fuzzy Systems (FUZZ-IEEE)
- **Journal**: IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), New
  Orleans, LA, USA, 2019, pp. 1-6
- **Summary**: Many state-of-the-art technologies developed in recent years have been influenced by machine learning to some extent. Most popular at the time of this writing are artificial intelligence methodologies that fall under the umbrella of deep learning. Deep learning has been shown across many applications to be extremely powerful and capable of handling problems that possess great complexity and difficulty. In this work, we introduce a new layer to deep learning: the fuzzy layer. Traditionally, the network architecture of neural networks is composed of an input layer, some combination of hidden layers, and an output layer. We propose the introduction of fuzzy layers into the deep learning architecture to exploit the powerful aggregation properties expressed through fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals. To date, fuzzy approaches taken to deep learning have been through the application of various fusion strategies at the decision level to aggregate outputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16, GoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown to improve accuracy performance for image classification tasks, none have explored the use of fuzzified intermediate, or hidden, layers. Herein, we present a new deep learning strategy that incorporates fuzzy strategies into the deep learning architecture focused on the application of semantic segmentation using per-pixel classification. Experiments are conducted on a benchmark data set as well as a data set collected via an unmanned aerial system at a U.S. Army test site for the task of automatic road segmentation, and preliminary results are promising.



### Image to Language Understanding: Captioning approach
- **Arxiv ID**: http://arxiv.org/abs/2002.09536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09536v1)
- **Published**: 2020-02-21 20:15:33+00:00
- **Updated**: 2020-02-21 20:15:33+00:00
- **Authors**: Madhavan Seshadri, Malavika Srikanth, Mikhail Belov
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Extracting context from visual representations is of utmost importance in the advancement of Computer Science. Representation of such a format in Natural Language has a huge variety of applications such as helping the visually impaired etc. Such an approach is a combination of Computer Vision and Natural Language techniques which is a hard problem to solve. This project aims to compare different approaches for solving the image captioning problem. In specific, the focus was on comparing two different types of models: Encoder-Decoder approach and a Multi-model approach. In the encoder-decoder approach, inject and merge architectures were compared against a multi-modal image captioning approach based primarily on object detection. These approaches have been compared on the basis on state of the art sentence comparison metrics such as BLEU, GLEU, Meteor, and Rouge on a subset of the Google Conceptual captions dataset which contains 100k images. On the basis of this comparison, we observed that the best model was the Inception injected encoder model. This best approach has been deployed as a web-based system. On uploading an image, such a system will output the best caption associated with the image.



### Applying Rule-Based Context Knowledge to Build Abstract Semantic Maps of Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2002.10938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10938v1)
- **Published**: 2020-02-21 20:56:02+00:00
- **Updated**: 2020-02-21 20:56:02+00:00
- **Authors**: Ziyuan Liu, Georg von Wichert
- **Comment**: arXiv admin note: text overlap with arXiv:2002.08402
- **Journal**: None
- **Summary**: In this paper, we propose a generalizable method that systematically combines data driven MCMC samplingand inference using rule-based context knowledge for data abstraction. In particular, we demonstrate the usefulness of our method in the scenario of building abstract semantic maps for indoor environments. The product of our system is a parametric abstract model of the perceived environment that not only accurately represents the geometry of the environment but also provides valuable abstract information which benefits high-level robotic applications. Based on predefined abstract terms,such as type and relation, we define task-specific context knowledge as descriptive rules in Markov Logic Networks. The corresponding inference results are used to construct a priordistribution that aims to add reasonable constraints to the solution space of semantic maps. In addition, by applying a semantically annotated sensor model, we explicitly use context information to interpret the sensor data. Experiments on real world data show promising results and thus confirm the usefulness of our system.



### Online Semantic Exploration of Indoor Maps
- **Arxiv ID**: http://arxiv.org/abs/2002.10939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.10939v1)
- **Published**: 2020-02-21 21:07:28+00:00
- **Updated**: 2020-02-21 21:07:28+00:00
- **Authors**: Ziyuan Liu, Dong Chen, Georg von Wichert
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2002.08348
- **Journal**: None
- **Summary**: In this paper we propose a method to extract an abstracted floor plan from typical grid maps using Bayesian reasoning. The result of this procedure is a probabilistic generative model of the environment defined over abstract concepts. It is well suited for higher-level reasoning and communication purposes. We demonstrate the effectiveness of the approach through real-world experiments.



### Particle Filter Based Monocular Human Tracking with a 3D Cardbox Model and a Novel Deterministic Resampling Strategy
- **Arxiv ID**: http://arxiv.org/abs/2002.09554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.09554v1)
- **Published**: 2020-02-21 21:21:58+00:00
- **Updated**: 2020-02-21 21:21:58+00:00
- **Authors**: Ziyuan Liu, Dongheui Lee, Wolfgang Sepp
- **Comment**: None
- **Journal**: None
- **Summary**: The challenge of markerless human motion tracking is the high dimensionality of the search space. Thus, efficient exploration in the search space is of great significance. In this paper, a motion capturing algorithm is proposed for upper body motion tracking. The proposed system tracks human motion based on monocular silhouette-matching, and it is built on the top of a hierarchical particle filter, within which a novel deterministic resampling strategy (DRS) is applied. The proposed system is evaluated quantitatively with the ground truth data measured by an inertial sensor system. In addition, we compare the DRS with the stratified resampling strategy (SRS). It is shown in experiments that DRS outperforms SRS with the same amount of particles. Moreover, a new 3D articulated human upper body model with the name 3D cardbox model is created and is proven to work successfully for motion tracking. Experiments show that the proposed system can robustly track upper body motion without self-occlusion. Motions towards the camera can also be well tracked.



### Self-Supervised Poisson-Gaussian Denoising
- **Arxiv ID**: http://arxiv.org/abs/2002.09558v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09558v2)
- **Published**: 2020-02-21 21:34:33+00:00
- **Updated**: 2020-11-19 01:13:33+00:00
- **Authors**: Wesley Khademi, Sonia Rao, Clare Minnerath, Guy Hagen, Jonathan Ventura
- **Comment**: to appear in IEEE WACV 2021
- **Journal**: None
- **Summary**: We extend the blindspot model for self-supervised denoising to handle Poisson-Gaussian noise and introduce an improved training scheme that avoids hyperparameters and adapts the denoiser to the test data. Self-supervised models for denoising learn to denoise from only noisy data and do not require corresponding clean images, which are difficult or impossible to acquire in some application areas of interest such as low-light microscopy. We introduce a new training strategy to handle Poisson-Gaussian noise which is the standard noise model for microscope images. Our new strategy eliminates hyperparameters from the loss function, which is important in a self-supervised regime where no ground truth data is available to guide hyperparameter tuning. We show how our denoiser can be adapted to the test data to improve performance. Our evaluations on microscope image denoising benchmarks validate our approach.



### Towards Robust and Reproducible Active Learning Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.09564v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09564v3)
- **Published**: 2020-02-21 22:01:47+00:00
- **Updated**: 2022-06-15 18:50:54+00:00
- **Authors**: Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati, Shadab Khan
- **Comment**: Accepted at CVPR 2022; Improved figures and plots for better
  readability
- **Journal**: None
- **Summary**: Active learning (AL) is a promising ML paradigm that has the potential to parse through large unlabeled data and help reduce annotation cost in domains where labeling data can be prohibitive. Recently proposed neural network based AL methods use different heuristics to accomplish this goal. In this study, we demonstrate that under identical experimental settings, different types of AL algorithms (uncertainty based, diversity based, and committee based) produce an inconsistent gain over random sampling baseline. Through a variety of experiments, controlling for sources of stochasticity, we show that variance in performance metrics achieved by AL algorithms can lead to results that are not consistent with the previously reported results. We also found that under strong regularization, AL methods show marginal or no advantage over the random sampling baseline under a variety of experimental conditions. Finally, we conclude with a set of recommendations on how to assess the results using a new AL algorithm to ensure results are reproducible and robust under changes in experimental conditions. We share our codes to facilitate AL evaluations. We believe our findings and recommendations will help advance reproducible research in AL using neural networks. We open source our code at https://github.com/PrateekMunjal/TorchAL



### Learning to Continually Learn
- **Arxiv ID**: http://arxiv.org/abs/2002.09571v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09571v2)
- **Published**: 2020-02-21 22:52:00+00:00
- **Updated**: 2020-03-04 03:22:48+00:00
- **Authors**: Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, Nick Cheney
- **Comment**: None
- **Journal**: None
- **Summary**: Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).



### UnMask: Adversarial Detection and Defense Through Robust Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2002.09576v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.09576v2)
- **Published**: 2020-02-21 23:20:23+00:00
- **Updated**: 2020-11-14 20:21:11+00:00
- **Authors**: Scott Freitas, Shang-Tse Chen, Zijie J. Wang, Duen Horng Chau
- **Comment**: Accepted into IEEE Big Data 2020
- **Journal**: None
- **Summary**: Deep learning models are being integrated into a wide range of high-impact, security-critical systems, from self-driving cars to medical diagnosis. However, recent research has demonstrated that many of these deep learning architectures are vulnerable to adversarial attacks--highlighting the vital need for defensive techniques to detect and mitigate these attacks before they occur. To combat these adversarial attacks, we developed UnMask, an adversarial detection and defense framework based on robust feature alignment. The core idea behind UnMask is to protect these models by verifying that an image's predicted class ("bird") contains the expected robust features (e.g., beak, wings, eyes). For example, if an image is classified as "bird", but the extracted features are wheel, saddle and frame, the model may be under attack. UnMask detects such attacks and defends the model by rectifying the misclassification, re-classifying the image based on its robust features. Our extensive evaluation shows that UnMask (1) detects up to 96.75% of attacks, and (2) defends the model by correctly classifying up to 93% of adversarial images produced by the current strongest attack, Projected Gradient Descent, in the gray-box setting. UnMask provides significantly better protection than adversarial training across 8 attack vectors, averaging 31.18% higher accuracy. We open source the code repository and data with this paper: https://github.com/safreita1/unmask.



