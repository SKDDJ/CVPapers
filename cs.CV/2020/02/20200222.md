# Arxiv Papers in cs.CV on 2020-02-22
### Convex Shape Representation with Binary Labels for Image Segmentation: Models and Fast Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2002.09600v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09600v1)
- **Published**: 2020-02-22 01:55:20+00:00
- **Updated**: 2020-02-22 01:55:20+00:00
- **Authors**: Shousheng Luo, Xue-Cheng Tai, Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel and effective binary representation for convex shapes. We show the equivalence between the shape convexity and some properties of the associated indicator function. The proposed method has two advantages. Firstly, the representation is based on a simple inequality constraint on the binary function rather than the definition of convex shapes, which allows us to obtain efficient algorithms for various applications with convexity prior. Secondly, this method is independent of the dimension of the concerned shape. In order to show the effectiveness of the proposed representation approach, we incorporate it with a probability based model for object segmentation with convexity prior. Efficient algorithms are given to solve the proposed models using Lagrange multiplier methods and linear approximations. Various experiments are given to show the superiority of the proposed methods.



### Estimating a Null Model of Scientific Image Reuse to Support Research Integrity Investigations
- **Arxiv ID**: http://arxiv.org/abs/2003.00878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00878v1)
- **Published**: 2020-02-22 02:41:13+00:00
- **Updated**: 2020-02-22 02:41:13+00:00
- **Authors**: Daniel E. Acuna, Ziyue Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: When there is a suspicious figure reuse case in science, research integrity investigators often find it difficult to rebut authors claiming that "it happened by chance". In other words, when there is a "collision" of image features, it is difficult to justify whether it appears rarely or not. In this article, we provide a method to predict the rarity of an image feature by statistically estimating the chance of it randomly occurring across all scientific imagery. Our method is based on high-dimensional density estimation of ORB features using 7+ million images in the PubMed Open Access Subset dataset. We show that this method can lead to meaningful feedback during research integrity investigations by providing a null hypothesis for scientific image reuse and thus a p-value during deliberations. We apply the model to a sample of increasingly complex imagery and confirm that it produces decreasingly smaller p-values as expected. We discuss applications to research integrity investigations as well as future work.



### Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/2002.09611v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09611v2)
- **Published**: 2020-02-22 03:09:48+00:00
- **Updated**: 2020-11-18 13:33:47+00:00
- **Authors**: Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane SchÃ¶nlieb, Hua Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Plug-and-play (PnP) is a non-convex framework that combines ADMM or other proximal algorithms with advanced denoiser priors. Recently, PnP has achieved great empirical success, especially with the integration of deep learning-based denoisers. However, a key problem of PnP based approaches is that they require manual parameter tweaking. It is necessary to obtain high-quality results across the high discrepancy in terms of imaging conditions and varying scene content. In this work, we present a tuning-free PnP proximal algorithm, which can automatically determine the internal parameters including the penalty parameter, the denoising strength and the terminal time. A key part of our approach is to develop a policy network for automatic search of parameters, which can be effectively learned via mixed model-free and model-based deep reinforcement learning. We demonstrate, through numerical and visual experiments, that the learned policy can customize different parameters for different states, and often more efficient and effective than existing handcrafted criteria. Moreover, we discuss the practical considerations of the plugged denoisers, which together with our learned policy yield state-of-the-art results. This is prevalent on both linear and nonlinear exemplary inverse imaging problems, and in particular, we show promising results on Compressed Sensing MRI and phase retrieval.



### Neural Architecture Search for Compressed Sensing Magnetic Resonance Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2002.09625v6
- **DOI**: 10.1016/j.compmedimag.2020.101784
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09625v6)
- **Published**: 2020-02-22 04:40:16+00:00
- **Updated**: 2020-08-23 11:35:24+00:00
- **Authors**: Jiangpeng Yan, Shuo Chen, Yongbing Zhang, Xiu Li
- **Comment**: To be appear in Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Recent works have demonstrated that deep learning (DL) based compressed sensing (CS) implementation can accelerate Magnetic Resonance (MR) Imaging by reconstructing MR images from sub-sampled k-space data. However, network architectures adopted in previous methods are all designed by handcraft. Neural Architecture Search (NAS) algorithms can automatically build neural network architectures which have outperformed human designed ones in several vision tasks. Inspired by this, here we proposed a novel and efficient network for the MR image reconstruction problem via NAS instead of manual attempts. Particularly, a specific cell structure, which was integrated into the model-driven MR reconstruction pipeline, was automatically searched from a flexible pre-defined operation search space in a differentiable manner. Experimental results show that our searched network can produce better reconstruction results compared to previous state-of-the-art methods in terms of PSNR and SSIM with 4-6 times fewer computation resources. Extensive experiments were conducted to analyze how hyper-parameters affect reconstruction performance and the searched structures. The generalizability of the searched architecture was also evaluated on different organ MR datasets. Our proposed method can reach a better trade-off between computation cost and reconstruction performance for MR reconstruction problem with good generalizability and offer insights to design neural networks for other medical image applications. The evaluation code will be available at https://github.com/yjump/NAS-for-CSMRI.



### Towards Label-Free 3D Segmentation of Optical Coherence Tomography Images of the Optic Nerve Head Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.09635v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.09635v1)
- **Published**: 2020-02-22 05:41:45+00:00
- **Updated**: 2020-02-22 05:41:45+00:00
- **Authors**: Sripad Krishna Devalla, Tan Hung Pham, Satish Kumar Panda, Liang Zhang, Giridhar Subramanian, Anirudh Swaminathan, Chin Zhi Yun, Mohan Rajan, Sujatha Mohan, Ramaswami Krishnadas, Vijayalakshmi Senthil, John Mark S. de Leon, Tin A. Tun, Ching-Yu Cheng, Leopold Schmetterer, Shamira Perera, Tin Aung, Alexandre H. Thiery, Michael J. A. Girard
- **Comment**: None
- **Journal**: None
- **Summary**: Since the introduction of optical coherence tomography (OCT), it has been possible to study the complex 3D morphological changes of the optic nerve head (ONH) tissues that occur along with the progression of glaucoma. Although several deep learning (DL) techniques have been recently proposed for the automated extraction (segmentation) and quantification of these morphological changes, the device specific nature and the difficulty in preparing manual segmentations (training data) limit their clinical adoption. With several new manufacturers and next-generation OCT devices entering the market, the complexity in deploying DL algorithms clinically is only increasing. To address this, we propose a DL based 3D segmentation framework that is easily translatable across OCT devices in a label-free manner (i.e. without the need to manually re-segment data for each device). Specifically, we developed 2 sets of DL networks. The first (referred to as the enhancer) was able to enhance OCT image quality from 3 OCT devices, and harmonized image-characteristics across these devices. The second performed 3D segmentation of 6 important ONH tissue layers. We found that the use of the enhancer was critical for our segmentation network to achieve device independency. In other words, our 3D segmentation network trained on any of 3 devices successfully segmented ONH tissue layers from the other two devices with high performance (Dice coefficients > 0.92). With such an approach, we could automatically segment images from new OCT devices without ever needing manual segmentation data from such devices.



### Image Stylization: From Predefined to Personalized
- **Arxiv ID**: http://arxiv.org/abs/2002.10945v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10945v1)
- **Published**: 2020-02-22 06:48:28+00:00
- **Updated**: 2020-02-22 06:48:28+00:00
- **Authors**: Ignacio Garcia-Dorado, Pascal Getreuer, Bartlomiej Wronski, Peyman Milanfar
- **Comment**: 14 pages, 22 figures. arXiv admin note: text overlap with
  arXiv:1712.06654
- **Journal**: None
- **Summary**: We present a framework for interactive design of new image stylizations using a wide range of predefined filter blocks. Both novel and off-the-shelf image filtering and rendering techniques are extended and combined to allow the user to unleash their creativity to intuitively invent, modify, and tune new styles from a given set of filters. In parallel to this manual design, we propose a novel procedural approach that automatically assembles sequences of filters, leading to unique and novel styles. An important aim of our framework is to allow for interactive exploration and design, as well as to enable videos and camera streams to be stylized on the fly. In order to achieve this real-time performance, we use the \textit{Best Linear Adaptive Enhancement} (BLADE) framework -- an interpretable shallow machine learning method that simulates complex filter blocks in real time. Our representative results include over a dozen styles designed using our interactive tool, a set of styles created procedurally, and new filters trained with our BLADE approach.



### Active Lighting Recurrence by Parallel Lighting Analogy for Fine-Grained Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.09663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09663v1)
- **Published**: 2020-02-22 08:51:31+00:00
- **Updated**: 2020-02-22 08:51:31+00:00
- **Authors**: Qian Zhang, Wei Feng, Liang Wan, Fei-Peng Tian, Xiaowei Wang, Ping Tan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies a new problem, namely active lighting recurrence (ALR) that physically relocalizes a light source to reproduce the lighting condition from single reference image for a same scene, which may suffer from fine-grained changes during twice observations. ALR is of great importance for fine-grained visual inspection and change detection, because some phenomena or minute changes can only be clearly observed under particular lighting conditions. Therefore, effective ALR should be able to online navigate a light source toward the target pose, which is challenging due to the complexity and diversity of real-world lighting and imaging processes. To this end, we propose to use the simple parallel lighting as an analogy model and based on Lambertian law to compose an instant navigation ball for this purpose. We theoretically prove the feasibility, i.e., equivalence and convergence, of this ALR approach for realistic near point light source and small near surface light source. Besides, we also theoretically prove the invariance of our ALR approach to the ambiguity of normal and lighting decomposition. The effectiveness and superiority of the proposed approach have been verified by both extensive quantitative experiments and challenging real-world tasks on fine-grained change detection of cultural heritages. We also validate the generality of our approach to non-Lambertian scenes.



### The perceptual boost of visual attention is task-dependent in naturalistic settings
- **Arxiv ID**: http://arxiv.org/abs/2003.00882v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00882v2)
- **Published**: 2020-02-22 09:10:24+00:00
- **Updated**: 2020-04-06 14:30:14+00:00
- **Authors**: Freddie Bickford Smith, Xiaoliang Luo, Brett D. Roads, Bradley C. Love
- **Comment**: Published as a workshop paper at "Bridging AI and Cognitive Science"
  (ICLR 2020)
- **Journal**: None
- **Summary**: Top-down attention allows people to focus on task-relevant visual information. Is the resulting perceptual boost task-dependent in naturalistic settings? We aim to answer this with a large-scale computational experiment. First, we design a collection of visual tasks, each consisting of classifying images from a chosen task set (subset of ImageNet categories). The nature of a task is determined by which categories are included in the task set. Second, on each task we train an attention-augmented neural network and then compare its accuracy to that of a baseline network. We show that the perceptual boost of attention is stronger with increasing task-set difficulty, weaker with increasing task-set size and weaker with increasing perceptual similarity within a task set.



### Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.09674v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09674v3)
- **Published**: 2020-02-22 10:08:42+00:00
- **Updated**: 2021-08-10 11:48:25+00:00
- **Authors**: Ziwen He, Wei Wang, Jing Dong, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition is widely used in social security applications due to its advantages in long-distance human identification. Recently, sequence-based methods have achieved high accuracy by learning abundant temporal and spatial information. However, their robustness under adversarial attacks has not been clearly explored. In this paper, we demonstrate that the state-of-the-art gait recognition model is vulnerable to such attacks. To this end, we propose a novel temporal sparse adversarial attack method. Different from previous additive noise models which add perturbations on original samples, we employ a generative adversarial network based architecture to semantically generate adversarial high-quality gait silhouettes or video frames. Moreover, by sparsely substituting or inserting a few adversarial gait silhouettes, the proposed method ensures its imperceptibility and achieves a high attack success rate. The experimental results show that if only one-fortieth of the frames are attacked, the accuracy of the target model drops dramatically.



### Back to the Future: Joint Aware Temporal Deep Learning 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.11251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.11251v1)
- **Published**: 2020-02-22 10:11:13+00:00
- **Updated**: 2020-02-22 10:11:13+00:00
- **Authors**: Vikas Gupta
- **Comment**: Our model and code are available at https://vnmr.github.io/
- **Journal**: None
- **Summary**: We propose a new deep learning network that introduces a deeper CNN channel filter and constraints as losses to reduce joint position and motion errors for 3D video human body pose estimation. Our model outperforms the previous best result from the literature based on mean per-joint position error, velocity error, and acceleration errors on the Human 3.6M benchmark corresponding to a new state-of-the-art mean error reduction in all protocols and motion metrics. Mean per joint error is reduced by 1%, velocity error by 7% and acceleration by 13% compared to the best results from the literature. Our contribution increasing positional accuracy and motion smoothness in video can be integrated with future end to end networks without increasing network complexity. Our model and code are available at https://vnmr.github.io/   Keywords: 3D, human, image, pose, action, detection, object, video, visual, supervised, joint, kinematic



### A Multi-view Perspective of Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00877v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00877v2)
- **Published**: 2020-02-22 13:26:00+00:00
- **Updated**: 2020-05-15 04:20:00+00:00
- **Authors**: Chuanxing Geng, Zhenghao Tan, Songcan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: As a newly emerging unsupervised learning paradigm, self-supervised learning (SSL) recently gained widespread attention, which usually introduces a pretext task without manual annotation of data. With its help, SSL effectively learns the feature representation beneficial for downstream tasks. Thus the pretext task plays a key role. However, the study of its design, especially its essence currently is still open. In this paper, we borrow a multi-view perspective to decouple a class of popular pretext tasks into a combination of view data augmentation (VDA) and view label classification (VLC), where we attempt to explore the essence of such pretext task while providing some insights into its design. Specifically, a simple multi-view learning framework is specially designed (SSL-MV), which assists the feature learning of downstream tasks (original view) through the same tasks on the augmented views. SSL-MV focuses on VDA while abandons VLC, empirically uncovering that it is VDA rather than generally considered VLC that dominates the performance of such SSL. Additionally, thanks to replacing VLC with VDA tasks, SSL-MV also enables an integrated inference combining the predictions from the augmented views, further improving the performance. Experiments on several benchmark datasets demonstrate its advantages.



### Automatic Data Augmentation via Deep Reinforcement Learning for Effective Kidney Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.09703v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09703v1)
- **Published**: 2020-02-22 14:10:13+00:00
- **Updated**: 2020-02-22 14:10:13+00:00
- **Authors**: Tiexin Qin, Ziyuan Wang, Kelei He, Yinghuan Shi, Yang Gao, Dinggang Shen
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Conventional data augmentation realized by performing simple pre-processing operations (\eg, rotation, crop, \etc) has been validated for its advantage in enhancing the performance for medical image segmentation. However, the data generated by these conventional augmentation methods are random and sometimes harmful to the subsequent segmentation. In this paper, we developed a novel automatic learning-based data augmentation method for medical image segmentation which models the augmentation task as a trial-and-error procedure using deep reinforcement learning (DRL). In our method, we innovatively combine the data augmentation module and the subsequent segmentation module in an end-to-end training manner with a consistent loss. Specifically, the best sequential combination of different basic operations is automatically learned by directly maximizing the performance improvement (\ie, Dice ratio) on the available validation set. We extensively evaluated our method on CT kidney tumor segmentation which validated the promising results of our method.



### Robust Multimodal Brain Tumor Segmentation via Feature Disentanglement and Gated Fusion
- **Arxiv ID**: http://arxiv.org/abs/2002.09708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09708v1)
- **Published**: 2020-02-22 14:32:04+00:00
- **Updated**: 2020-02-22 14:32:04+00:00
- **Authors**: Cheng Chen, Qi Dou, Yueming Jin, Hao Chen, Jing Qin, Pheng-Ann Heng
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Accurate medical image segmentation commonly requires effective learning of the complementary information from multimodal data. However, in clinical practice, we often encounter the problem of missing imaging modalities. We tackle this challenge and propose a novel multimodal segmentation framework which is robust to the absence of imaging modalities. Our network uses feature disentanglement to decompose the input modalities into the modality-specific appearance code, which uniquely sticks to each modality, and the modality-invariant content code, which absorbs multimodal information for the segmentation task. With enhanced modality-invariance, the disentangled content code from each modality is fused into a shared representation which gains robustness to missing data. The fusion is achieved via a learning-based strategy to gate the contribution of different modalities at different locations. We validate our method on the important yet challenging multimodal brain tumor segmentation task with the BRATS challenge dataset. With competitive performance to the state-of-the-art approaches for full modality, our method achieves outstanding robustness under various missing modality(ies) situations, significantly exceeding the state-of-the-art method by over 16% in average for Dice on whole tumor segmentation.



### HarDNN: Feature Map Vulnerability Evaluation in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2002.09786v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09786v2)
- **Published**: 2020-02-22 23:05:03+00:00
- **Updated**: 2020-02-25 11:07:36+00:00
- **Authors**: Abdulrahman Mahmoud, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, Stephen W. Keckler
- **Comment**: 14 pages, 5 figures, a short version accepted for publication in
  First Workshop on Secure and Resilient Autonomy (SARA) co-located with
  MLSys2020
- **Journal**: None
- **Summary**: As Convolutional Neural Networks (CNNs) are increasingly being employed in safety-critical applications, it is important that they behave reliably in the face of hardware errors. Transient hardware errors may percolate undesirable state during execution, resulting in software-manifested errors which can adversely affect high-level decision making. This paper presents HarDNN, a software-directed approach to identify vulnerable computations during a CNN inference and selectively protect them based on their propensity towards corrupting the inference output in the presence of a hardware error. We show that HarDNN can accurately estimate relative vulnerability of a feature map (fmap) in CNNs using a statistical error injection campaign, and explore heuristics for fast vulnerability assessment. Based on these results, we analyze the tradeoff between error coverage and computational overhead that the system designers can use to employ selective protection. Results show that the improvement in resilience for the added computation is superlinear with HarDNN. For example, HarDNN improves SqueezeNet's resilience by 10x with just 30% additional computations.



### Shallow2Deep: Indoor Scene Modeling by Single Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/2002.09790v1
- **DOI**: 10.1016/j.patcog.2020.107271
- **Categories**: **cs.CV**, 68T45 (primary), 65D19 (Secondary), I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2002.09790v1)
- **Published**: 2020-02-22 23:27:22+00:00
- **Updated**: 2020-02-22 23:27:22+00:00
- **Authors**: Yinyu Nie, Shihui Guo, Jian Chang, Xiaoguang Han, Jiahui Huang, Shi-Min Hu, Jian Jun Zhang
- **Comment**: Accepted by Pattern Recognition
- **Journal**: Pattern Recognition. 2020 Feb 12:107271
- **Summary**: Dense indoor scene modeling from 2D images has been bottlenecked due to the absence of depth information and cluttered occlusions. We present an automatic indoor scene modeling approach using deep features from neural networks. Given a single RGB image, our method simultaneously recovers semantic contents, 3D geometry and object relationship by reasoning indoor environment context. Particularly, we design a shallow-to-deep architecture on the basis of convolutional networks for semantic scene understanding and modeling. It involves multi-level convolutional networks to parse indoor semantics/geometry into non-relational and relational knowledge. Non-relational knowledge extracted from shallow-end networks (e.g. room layout, object geometry) is fed forward into deeper levels to parse relational semantics (e.g. support relationship). A Relation Network is proposed to infer the support relationship between objects. All the structured semantics and geometry above are assembled to guide a global optimization for 3D scene modeling. Qualitative and quantitative analysis demonstrates the feasibility of our method in understanding and modeling semantics-enriched indoor scenes by evaluating the performance of reconstruction accuracy, computation performance and scene complexity.



