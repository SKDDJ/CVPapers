# Arxiv Papers in cs.CV on 2020-02-18
### Variational Bayesian Quantization
- **Arxiv ID**: http://arxiv.org/abs/2002.08158v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08158v2)
- **Published**: 2020-02-18 00:15:37+00:00
- **Updated**: 2020-09-07 22:25:12+00:00
- **Authors**: Yibo Yang, Robert Bamler, Stephan Mandt
- **Comment**: 9 pages + detailed supplement with additional full resolution
  reconstructed images; ICML 2020 final camera-ready version, title changed to
  "Variational Bayesian Quantization" following reviewer feedback
- **Journal**: None
- **Summary**: We propose a novel algorithm for quantizing continuous latent representations in trained models. Our approach applies to deep probabilistic models, such as variational autoencoders (VAEs), and enables both data and model compression. Unlike current end-to-end neural compression methods that cater the model to a fixed quantization scheme, our algorithm separates model design and training from quantization. Consequently, our algorithm enables "plug-and-play" compression with variable rate-distortion trade-off, using a single trained model. Our algorithm can be seen as a novel extension of arithmetic coding to the continuous domain, and uses adaptive quantization accuracy based on estimates of posterior uncertainty. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single standard VAE. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.



### TensorShield: Tensor-based Defense Against Adversarial Attacks on Images
- **Arxiv ID**: http://arxiv.org/abs/2002.10252v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10252v1)
- **Published**: 2020-02-18 00:39:49+00:00
- **Updated**: 2020-02-18 00:39:49+00:00
- **Authors**: Negin Entezari, Evangelos E. Papalexakis
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have demonstrated that machine learning approaches like deep neural networks (DNNs) are easily fooled by adversarial attacks. Subtle and imperceptible perturbations of the data are able to change the result of deep neural networks. Leveraging vulnerable machine learning methods raises many concerns especially in domains where security is an important factor. Therefore, it is crucial to design defense mechanisms against adversarial attacks. For the task of image classification, unnoticeable perturbations mostly occur in the high-frequency spectrum of the image. In this paper, we utilize tensor decomposition techniques as a preprocessing step to find a low-rank approximation of images which can significantly discard high-frequency perturbations. Recently a defense framework called Shield could "vaccinate" Convolutional Neural Networks (CNN) against adversarial examples by performing random-quality JPEG compressions on local patches of images on the ImageNet dataset. Our tensor-based defense mechanism outperforms the SLQ method from Shield by 14% against FastGradient Descent (FGSM) adversarial attacks, while maintaining comparable speed.



### Bottom-Up Temporal Action Localization with Mutual Regularization
- **Arxiv ID**: http://arxiv.org/abs/2002.07358v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07358v3)
- **Published**: 2020-02-18 03:59:13+00:00
- **Updated**: 2021-02-26 02:26:46+00:00
- **Authors**: Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Recently, temporal action localization (TAL), i.e., finding specific action segments in untrimmed videos, has attracted increasing attentions of the computer vision community. State-of-the-art solutions for TAL involves evaluating the frame-level probabilities of three action-indicating phases, i.e. starting, continuing, and ending; and then post-processing these predictions for the final localization. This paper delves deep into this mechanism, and argues that existing methods, by modeling these phases as individual classification tasks, ignored the potential temporal constraints between them. This can lead to incorrect and/or inconsistent predictions when some frames of the video input lack sufficient discriminative information. To alleviate this problem, we introduce two regularization terms to mutually regularize the learning procedure: the Intra-phase Consistency (IntraC) regularization is proposed to make the predictions verified inside each phase; and the Inter-phase Consistency (InterC) regularization is proposed to keep consistency between these phases. Jointly optimizing these two terms, the entire framework is aware of these potential constraints during an end-to-end optimization process. Experiments are performed on two popular TAL datasets, THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both quantitatively and qualitatively. The proposed regularization also generalizes to other TAL methods (e.g., TSA-Net and PGCN). code: https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR



### MILA: Multi-Task Learning from Videos via Efficient Inter-Frame Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.07362v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07362v3)
- **Published**: 2020-02-18 04:25:58+00:00
- **Updated**: 2021-10-10 23:18:15+00:00
- **Authors**: Donghyun Kim, Tian Lan, Chuhang Zou, Ning Xu, Bryan A. Plummer, Stan Sclaroff, Jayan Eledath, Gerard Medioni
- **Comment**: Accepted in ICCV 2021 MTL Workshop
- **Journal**: None
- **Summary**: Prior work in multi-task learning has mainly focused on predictions on a single image. In this work, we present a new approach for multi-task learning from videos via efficient inter-frame local attention (MILA). Our approach contains a novel inter-frame attention module which allows learning of task-specific attention across frames. We embed the attention module in a ``slow-fast'' architecture, where the slower network runs on sparsely sampled keyframes and the light-weight shallow network runs on non-keyframes at a high frame rate. We also propose an effective adversarial learning strategy to encourage the slow and fast network to learn similar features. Our approach ensures low-latency multi-task learning while maintaining high quality predictions. Experiments show competitive accuracy compared to state-of-the-art on two multi-task learning benchmarks while reducing the number of floating point operations (FLOPs) by up to 70\%. In addition, our attention based feature propagation method (ILA) outperforms prior work in terms of task accuracy while also reducing up to 90\% of FLOPs.



### High-Order Paired-ASPP Networks for Semantic Segmenation
- **Arxiv ID**: http://arxiv.org/abs/2002.07371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07371v1)
- **Published**: 2020-02-18 04:50:06+00:00
- **Updated**: 2020-02-18 04:50:06+00:00
- **Authors**: Yu Zhang, Xin Sun, Junyu Dong, Changrui Chen, Yue Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Current semantic segmentation models only exploit first-order statistics, while rarely exploring high-order statistics. However, common first-order statistics are insufficient to support a solid unanimous representation. In this paper, we propose High-Order Paired-ASPP Network to exploit high-order statistics from various feature levels. The network first introduces a High-Order Representation module to extract the contextual high-order information from all stages of the backbone. They can provide more semantic clues and discriminative information than the first-order ones. Besides, a Paired-ASPP module is proposed to embed high-order statistics of the early stages into the last stage. It can further preserve the boundary-related and spatial context in the low-level features for final prediction. Our experiments show that the high-order statistics significantly boost the performance on confusing objects. Our method achieves competitive performance without bells and whistles on three benchmarks, i.e, Cityscapes, ADE20K and Pascal-Context with the mIoU of 81.6%, 45.3% and 52.9%.



### Picking Winning Tickets Before Training by Preserving Gradient Flow
- **Arxiv ID**: http://arxiv.org/abs/2002.07376v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07376v2)
- **Published**: 2020-02-18 05:14:47+00:00
- **Updated**: 2020-08-07 00:02:33+00:00
- **Authors**: Chaoqi Wang, Guodong Zhang, Roger Grosse
- **Comment**: Fix several typos
- **Journal**: In Proceedings of the 8th International Conference on Learning
  Representations (ICLR), 2020
- **Summary**: Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.



### DivideMix: Learning with Noisy Labels as Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.07394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07394v1)
- **Published**: 2020-02-18 06:20:06+00:00
- **Updated**: 2020-02-18 06:20:06+00:00
- **Authors**: Junnan Li, Richard Socher, Steven C. H. Hoi
- **Comment**: None
- **Journal**: International Conference on Learning Representations, 2020
- **Summary**: Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .



### Deflecting Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2002.07405v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07405v1)
- **Published**: 2020-02-18 06:59:13+00:00
- **Updated**: 2020-02-18 06:59:13+00:00
- **Authors**: Yao Qin, Nicholas Frosst, Colin Raffel, Garrison Cottrell, Geoffrey Hinton
- **Comment**: None
- **Journal**: None
- **Summary**: There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we "deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called "adversarial'' because our network classifies them the same way as humans do.



### Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2002.07417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07417v1)
- **Published**: 2020-02-18 07:57:45+00:00
- **Updated**: 2020-02-18 07:57:45+00:00
- **Authors**: Hang Xu, Linpu Fang, Xiaodan Liang, Wenxiong Kang, Zhenguo Li
- **Comment**: Accepted by AAAI20
- **Journal**: None
- **Summary**: The dominant object detection approaches treat each dataset separately and fit towards a specific domain, which cannot adapt to other domains without extensive retraining. In this paper, we address the problem of designing a universal object detection model that exploits diverse category granularity from multiple domains and predict all kinds of categories in one system. Existing works treat this problem by integrating multiple detection branches upon one shared backbone network. However, this paradigm overlooks the crucial semantic correlations between multiple domains, such as categories hierarchy, visual similarity, and linguistic relationship. To address these drawbacks, we present a novel universal object detector called Universal-RCNN that incorporates graph transfer learning for propagating relevant semantic information across multiple datasets to reach semantic coherency. Specifically, we first generate a global semantic pool by integrating all high-level semantic representation of all the categories. Then an Intra-Domain Reasoning Module learns and propagates the sparse graph representation within one dataset guided by a spatial-aware GCN. Finally, an InterDomain Transfer Module is proposed to exploit diverse transfer dependencies across all domains and enhance the regional feature representation by attending and transferring semantic contexts globally. Extensive experiments demonstrate that the proposed method significantly outperforms multiple-branch models and achieves the state-of-the-art results on multiple object detection benchmarks (mAP: 49.1% on COCO).



### EHSOD: CAM-Guided End-to-end Hybrid-Supervised Object Detection with Cascade Refinement
- **Arxiv ID**: http://arxiv.org/abs/2002.07421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07421v1)
- **Published**: 2020-02-18 08:04:58+00:00
- **Updated**: 2020-02-18 08:04:58+00:00
- **Authors**: Linpu Fang, Hang Xu, Zhili Liu, Sarah Parisot, Zhenguo Li
- **Comment**: Accepted by AAAI20
- **Journal**: None
- **Summary**: Object detectors trained on fully-annotated data currently yield state of the art performance but require expensive manual annotations. On the other hand, weakly-supervised detectors have much lower performance and cannot be used reliably in a realistic setting. In this paper, we study the hybrid-supervised object detection problem, aiming to train a high quality detector with only a limited amount of fullyannotated data and fully exploiting cheap data with imagelevel labels. State of the art methods typically propose an iterative approach, alternating between generating pseudo-labels and updating a detector. This paradigm requires careful manual hyper-parameter tuning for mining good pseudo labels at each round and is quite time-consuming. To address these issues, we present EHSOD, an end-to-end hybrid-supervised object detection system which can be trained in one shot on both fully and weakly-annotated data. Specifically, based on a two-stage detector, we proposed two modules to fully utilize the information from both kinds of labels: 1) CAMRPN module aims at finding foreground proposals guided by a class activation heat-map; 2) hybrid-supervised cascade module further refines the bounding-box position and classification with the help of an auxiliary head compatible with image-level data. Extensive experiments demonstrate the effectiveness of the proposed method and it achieves comparable results on multiple object detection benchmarks with only 30% fully-annotated data, e.g. 37.5% mAP on COCO. We will release the code and the trained models.



### V4D:4D Convolutional Neural Networks for Video-level Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.07442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07442v1)
- **Published**: 2020-02-18 09:27:41+00:00
- **Updated**: 2020-02-18 09:27:41+00:00
- **Authors**: Shiwen Zhang, Sheng Guo, Weilin Huang, Matthew R. Scott, Limin Wang
- **Comment**: To appear in ICLR2020
- **Journal**: None
- **Summary**: Most existing 3D CNNs for video representation learning are clip-based methods, and thus do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, referred as V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, and at the same time, to preserve strong 3D spatio-temporal representation with residual connections. Specifically, we design a new 4D residual block able to capture inter-clip interactions, which could enhance the representation power of the original clip-level 3D CNNs. The 4D residual blocks can be easily integrated into the existing 3D CNNs to perform long-range modeling hierarchically. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.



### Registration of multi-view point sets under the perspective of expectation-maximization
- **Arxiv ID**: http://arxiv.org/abs/2002.07464v2
- **DOI**: 10.1109/TIP.2020.3024096
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07464v2)
- **Published**: 2020-02-18 10:04:51+00:00
- **Updated**: 2020-03-09 08:55:53+00:00
- **Authors**: Jihua Zhu, Jing Zhang, Huimin Lu, Zhongyu Li
- **Comment**: None
- **Journal**: IEEE TIP 2020
- **Summary**: Registration of multi-view point sets is a prerequisite for 3D model reconstruction. To solve this problem, most of previous approaches either partially explore available information or blindly utilize unnecessary information to align each point set, which may lead to the undesired results or introduce extra computation complexity. To this end, this paper consider the multi-view registration problem as a maximum likelihood estimation problem and proposes a novel multi-view registration approach under the perspective of Expectation-Maximization (EM). The basic idea of our approach is that different data points are generated by the same number of Gaussian mixture models (GMMs). For each data point in one point set, its nearest neighbors can be searched from other well-aligned point sets. Then, we can suppose this data point is generated by the special GMM, which is composed of each nearest neighbor adhered with one Gaussian distribution. Based on this assumption, it is reasonable to define the likelihood function including all rigid transformations, which requires to be estimated for multi-view registration. Subsequently, the EM algorithm is utilized to maximize the likelihood function so as to estimate all rigid transformations. Finally, the proposed approach is tested on several bench mark data sets and compared with some state-of-the-art algorithms. Experimental results illustrate its super performance on accuracy, robustness and efficiency for the registration of multi-view point sets.



### Automated Cardiothoracic Ratio Calculation and Cardiomegaly Detection using Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2002.07468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07468v1)
- **Published**: 2020-02-18 10:10:28+00:00
- **Updated**: 2020-02-18 10:10:28+00:00
- **Authors**: Isarun Chamveha, Treethep Promwiset, Trongtum Tongdee, Pairash Saiviroonporn, Warasinee Chaisangmongkon
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: We propose an algorithm for calculating the cardiothoracic ratio (CTR) from chest X-ray films. Our approach applies a deep learning model based on U-Net with VGG16 encoder to extract lung and heart masks from chest X-ray images and calculate CTR from the extents of obtained masks. Human radiologists evaluated our CTR measurements, and $76.5\%$ were accepted to be included in medical reports without any need for adjustment. This result translates to a large amount of time and labor saved for radiologists using our automated tools.



### Knowledge Integration Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.07471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07471v1)
- **Published**: 2020-02-18 10:20:30+00:00
- **Updated**: 2020-02-18 10:20:30+00:00
- **Authors**: Shiwen Zhang, Sheng Guo, Limin Wang, Weilin Huang, Matthew R. Scott
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: In this work, we propose Knowledge Integration Networks (referred as KINet) for video action recognition. KINet is capable of aggregating meaningful context features which are of great importance to identifying an action, such as human information and scene context. We design a three-branch architecture consisting of a main branch for action recognition, and two auxiliary branches for human parsing and scene recognition which allow the model to encode the knowledge of human and scene for action recognition. We explore two pre-trained models as teacher networks to distill the knowledge of human and scene for training the auxiliary tasks of KINet. Furthermore, we propose a two-level knowledge encoding mechanism which contains a Cross Branch Integration (CBI) module for encoding the auxiliary knowledge into medium-level convolutional features, and an Action Knowledge Graph (AKG) for effectively fusing high-level context information. This results in an end-to-end trainable framework where the three tasks can be trained collaboratively, allowing the model to compute strong context knowledge efficiently. The proposed KINet achieves the state-of-the-art performance on a large-scale action recognition benchmark Kinetics-400, with a top-1 accuracy of 77.8%. We further demonstrate that our KINet has strong capability by transferring the Kinetics-trained model to UCF-101, where it obtains 97.8% top-1 accuracy.



### Motion Deblurring using Spatiotemporal Phase Aperture Coding
- **Arxiv ID**: http://arxiv.org/abs/2002.07483v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07483v1)
- **Published**: 2020-02-18 10:46:14+00:00
- **Updated**: 2020-02-18 10:46:14+00:00
- **Authors**: Shay Elmalem, Raja Giryes, Emanuel Marom
- **Comment**: 10 pages, 23 figures
- **Journal**: None
- **Summary**: Motion blur is a known issue in photography, as it limits the exposure time while capturing moving objects. Extensive research has been carried to compensate for it. In this work, a computational imaging approach for motion deblurring is proposed and demonstrated. Using dynamic phase-coding in the lens aperture during the image acquisition, the trajectory of the motion is encoded in an intermediate optical image. This encoding embeds both the motion direction and extent by coloring the spatial blur of each object. The color cues serve as prior information for a blind deblurring process, implemented using a convolutional neural network (CNN) trained to utilize such coding for image restoration. We demonstrate the advantage of the proposed approach over blind-deblurring with no coding and other solutions that use coded acquisition, both in simulation and real-world experiments.



### NoiseBreaker: Gradual Image Denoising Guided by Noise Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.07487v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07487v2)
- **Published**: 2020-02-18 11:09:03+00:00
- **Updated**: 2020-07-31 13:46:09+00:00
- **Authors**: Florian Lemarchand, Erwan Nogues, Maxime Pelcat
- **Comment**: ACCEPTED by MMSP20
- **Journal**: None
- **Summary**: Fully supervised deep-learning based denoisers are currently the most performing image denoising solutions. However, they require clean reference images. When the target noise is complex, e.g. composed of an unknown mixture of primary noises with unknown intensity, fully supervised solutions are limited by the difficulty to build a suited training set for the problem. This paper proposes a gradual denoising strategy that iteratively detects the dominating noise in an image, and removes it using a tailored denoiser. The method is shown to keep up with state of the art blind denoisers on mixture noises. Moreover, noise analysis is demonstrated to guide denoisers efficiently not only on noise type, but also on noise intensity. The method provides an insight on the nature of the encountered noise, and it makes it possible to extend an existing denoiser with new noise nature. This feature makes the method adaptive to varied denoising cases.



### MapLUR: Exploring a new Paradigm for Estimating Air Pollution using Deep Learning on Map Images
- **Arxiv ID**: http://arxiv.org/abs/2002.07493v1
- **DOI**: 10.1145/3380973
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07493v1)
- **Published**: 2020-02-18 11:21:55+00:00
- **Updated**: 2020-02-18 11:21:55+00:00
- **Authors**: Michael Steininger, Konstantin Kobs, Albin Zehe, Florian Lautenschlager, Martin Becker, Andreas Hotho
- **Comment**: Accepted for publication in ACM TSAS - Special Issue on Deep Learning
- **Journal**: None
- **Summary**: Land-use regression (LUR) models are important for the assessment of air pollution concentrations in areas without measurement stations. While many such models exist, they often use manually constructed features based on restricted, locally available data. Thus, they are typically hard to reproduce and challenging to adapt to areas beyond those they have been developed for. In this paper, we advocate a paradigm shift for LUR models: We propose the Data-driven, Open, Global (DOG) paradigm that entails models based on purely data-driven approaches using only openly and globally available data. Progress within this paradigm will alleviate the need for experts to adapt models to the local characteristics of the available data sources and thus facilitate the generalizability of air pollution models to new areas on a global scale. In order to illustrate the feasibility of the DOG paradigm for LUR, we introduce a deep learning model called MapLUR. It is based on a convolutional neural network architecture and is trained exclusively on globally and openly available map data without requiring manual feature engineering. We compare our model to state-of-the-art baselines like linear regression, random forests and multi-layer perceptrons using a large data set of modeled $\text{NO}_2$ concentrations in Central London. Our results show that MapLUR significantly outperforms these approaches even though they are provided with manually tailored features. Furthermore, we illustrate that the automatic feature extraction inherent to models based on the DOG paradigm can learn features that are readily interpretable and closely resemble those commonly used in traditional LUR approaches.



### Few-Shot Few-Shot Learning and the role of Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.07522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07522v1)
- **Published**: 2020-02-18 12:32:01+00:00
- **Updated**: 2020-02-18 12:32:01+00:00
- **Authors**: Yann Lifchitz, Yannis Avrithis, Sylvaine Picard
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is often motivated by the ability of humans to learn new tasks from few examples. However, standard few-shot classification benchmarks assume that the representation is learned on a limited amount of base class data, ignoring the amount of prior knowledge that a human may have accumulated before learning new tasks. At the same time, even if a powerful representation is available, it may happen in some domain that base class data are limited or non-existent. This motivates us to study a problem where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. We adapt the representation in two stages, namely on the few base class data if available and on the even fewer data of new tasks. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class data are few, the network cannot learn where to focus implicitly. We also show that a pre-trained network may be easily adapted to novel classes, without meta-learning.



### FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2002.07662v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07662v2)
- **Published**: 2020-02-18 15:50:37+00:00
- **Updated**: 2020-10-12 15:54:06+00:00
- **Authors**: Niels Ole Salscheider
- **Comment**: None
- **Journal**: None
- **Summary**: Most state of the art object detectors output multiple detections per object. The duplicates are removed in a post-processing step called Non-Maximum Suppression. Classical Non-Maximum Suppression has shortcomings in scenes that contain objects with high overlap: This heuristic assumes that a high overlap between two bounding boxes corresponds to a high probability of one being a duplicate. We propose FeatureNMS to solve this problem. FeatureNMS recognizes duplicates not only based on the intersection over union between the bounding boxes, but also based on the difference of feature vectors. These feature vectors can encode more information like visual appearance. Our approach outperforms classical NMS and derived approaches and achieves state of the art performance.



### Robust Quantization: One Model to Rule Them All
- **Arxiv ID**: http://arxiv.org/abs/2002.07686v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07686v3)
- **Published**: 2020-02-18 16:14:36+00:00
- **Updated**: 2020-10-22 08:46:01+00:00
- **Authors**: Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization methods often involve simulating the quantization process during training, making the trained model highly dependent on the target bit-width and precise way quantization is performed. Robust quantization offers an alternative approach with improved tolerance to different classes of data-types and quantization policies. It opens up new exciting applications where the quantization process is not static and can vary to meet different circumstances and implementations. To address this issue, we propose a method that provides intrinsic robustness to the model against a broad range of quantization processes. Our method is motivated by theoretical arguments and enables us to store a single generic model capable of operating at various bit-widths and quantization policies. We validate our method's effectiveness on different ImageNet models.



### Voxel-Based Indoor Reconstruction From HoloLens Triangle Meshes
- **Arxiv ID**: http://arxiv.org/abs/2002.07689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07689v1)
- **Published**: 2020-02-18 16:15:17+00:00
- **Updated**: 2020-02-18 16:15:17+00:00
- **Authors**: P. Hübner, M. Weinmann, S. Wursthorn
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Current mobile augmented reality devices are often equipped with range sensors. The Microsoft HoloLens for instance is equipped with a Time-Of-Flight (ToF) range camera providing coarse triangle meshes that can be used in custom applications. We suggest to use the triangle meshes for the automatic generation of indoor models that can serve as basis for augmenting their physical counterpart with location-dependent information. In this paper, we present a novel voxel-based approach for automated indoor reconstruction from unstructured three-dimensional geometries like triangle meshes. After an initial voxelization of the input data, rooms are detected in the resulting voxel grid by segmenting connected voxel components of ceiling candidates and extruding them downwards to find floor candidates. Semantic class labels like 'Wall', 'Wall Opening', 'Interior Object' and 'Empty Interior' are then assigned to the room voxels in-between ceiling and floor by a rule-based voxel sweep algorithm. Finally, the geometry of the detected walls and their openings is refined in voxel representation. The proposed approach is not restricted to Manhattan World scenarios and does not rely on room surfaces being planar.



### Deep Learning in Medical Ultrasound Image Segmentation: a Review
- **Arxiv ID**: http://arxiv.org/abs/2002.07703v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07703v3)
- **Published**: 2020-02-18 16:33:22+00:00
- **Updated**: 2021-03-05 00:05:17+00:00
- **Authors**: Ziyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Applying machine learning technologies, especially deep learning, into medical image segmentation is being widely studied because of its state-of-the-art performance and results. It can be a key step to provide a reliable basis for clinical diagnosis, such as 3D reconstruction of human tissues, image-guided interventions, image analyzing and visualization. In this review article, deep-learning-based methods for ultrasound image segmentation are categorized into six main groups according to their architectures and training at first. Secondly, for each group, several current representative algorithms are selected, introduced, analyzed and summarized in detail. In addition, common evaluation methods for image segmentation and ultrasound image segmentation datasets are summarized. Further, the performance of the current methods and their evaluations are reviewed. In the end, the challenges and potential research directions for medical ultrasound image segmentation are discussed.



### Towards Bounding-Box Free Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.07705v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.07705v3)
- **Published**: 2020-02-18 16:34:01+00:00
- **Updated**: 2020-07-27 17:48:08+00:00
- **Authors**: Ujwal Bonde, Pablo F. Alcantarilla, Stefan Leutenegger
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: In this work we introduce a new Bounding-Box Free Network (BBFNet) for panoptic segmentation. Panoptic segmentation is an ideal problem for proposal-free methods as it already requires per-pixel semantic class labels. We use this observation to exploit class boundaries from off-the-shelf semantic segmentation networks and refine them to predict instance labels. Towards this goal BBFNet predicts coarse watershed levels and uses them to detect large instance candidates where boundaries are well defined. For smaller instances, whose boundaries are less reliable, BBFNet also predicts instance centers by means of Hough voting followed by mean-shift to reliably detect small objects. A novel triplet loss network helps merging fragmented instances while refining boundary pixels. Our approach is distinct from previous works in panoptic segmentation that rely on a combination of a semantic segmentation network with a computationally costly instance segmentation network based on bounding box proposals, such as Mask R-CNN, to guide the prediction of instance labels using a Mixture-of-Expert (MoE) approach. We benchmark our proposal-free method on Cityscapes and Microsoft COCO datasets and show competitive performance with other MoE based approaches while outperforming existing non-proposal based methods on the COCO dataset. We show the flexibility of our method using different semantic segmentation backbones.



### Computational optimization of convolutional neural networks using separated filters architecture
- **Arxiv ID**: http://arxiv.org/abs/2002.07754v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2002.07754v1)
- **Published**: 2020-02-18 17:42:13+00:00
- **Updated**: 2020-02-18 17:42:13+00:00
- **Authors**: Elena Limonova, Alexander Sheshkus, Dmitry Nikolaev
- **Comment**: 4 pages, 3 figures
- **Journal**: International Journal of Applied Engineering Research (ISSN
  0973-4562), Volume 11, Number 11 (2016), pp 7491-7494
- **Summary**: This paper considers a convolutional neural network transformation that reduces computation complexity and thus speedups neural network processing. Usage of convolutional neural networks (CNN) is the standard approach to image recognition despite the fact they can be too computationally demanding, for example for recognition on mobile platforms or in embedded systems. In this paper we propose CNN structure transformation which expresses 2D convolution filters as a linear combination of separable filters. It allows to obtain separated convolutional filters by standard training algorithms. We study the computation efficiency of this structure transformation and suggest fast implementation easily handled by CPU or GPU. We demonstrate that CNNs designed for letter and digit recognition of proposed structure show 15% speedup without accuracy loss in industrial image recognition system. In conclusion, we discuss the question of possible accuracy decrease and the application of proposed transformation to different recognition problems. convolutional neural networks, computational optimization, separable filters, complexity reduction.



### Learning Bijective Feature Maps for Linear ICA
- **Arxiv ID**: http://arxiv.org/abs/2002.07766v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07766v5)
- **Published**: 2020-02-18 17:58:07+00:00
- **Updated**: 2021-01-29 18:08:28+00:00
- **Authors**: Alexander Camuto, Matthew Willetts, Brooks Paige, Chris Holmes, Stephen Roberts
- **Comment**: 8 pages
- **Journal**: AISTATS 2021
- **Summary**: Separating high-dimensional data like images into independent latent factors, i.e independent component analysis (ICA), remains an open research problem. As we show, existing probabilistic deep generative models (DGMs), which are tailor-made for image data, underperform on non-linear ICA tasks. To address this, we propose a DGM which combines bijective feature maps with a linear ICA model to learn interpretable latent structures for high-dimensional data. Given the complexities of jointly training such a hybrid model, we introduce novel theory that constrains linear ICA to lie close to the manifold of orthogonal rectangular matrices, the Stiefel manifold. By doing so we create models that converge quickly, are easy to train, and achieve better unsupervised latent factor discovery than flow-based models, linear ICA, and Variational Autoencoders on images.



### The Tree Ensemble Layer: Differentiability meets Conditional Computation
- **Arxiv ID**: http://arxiv.org/abs/2002.07772v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07772v2)
- **Published**: 2020-02-18 18:05:31+00:00
- **Updated**: 2020-07-11 00:40:16+00:00
- **Authors**: Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul Mazumder
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: Neural networks and tree ensembles are state-of-the-art learners, each with its unique statistical and computational advantages. We aim to combine these advantages by introducing a new layer for neural networks, composed of an ensemble of differentiable decision trees (a.k.a. soft trees). While differentiable trees demonstrate promising results in the literature, they are typically slow in training and inference as they do not support conditional computation. We mitigate this issue by introducing a new sparse activation function for sample routing, and implement true conditional computation by developing specialized forward and backward propagation algorithms that exploit sparsity. Our efficient algorithms pave the way for jointly training over deep and wide tree ensembles using first-order methods (e.g., SGD). Experiments on 23 classification datasets indicate over 10x speed-ups compared to the differentiable trees used in the literature and over 20x reduction in the number of parameters compared to gradient boosted trees, while maintaining competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion MNIST indicate that replacing dense layers in CNNs with our tree layer reduces the test loss by 7-53% and the number of parameters by 8x. We provide an open-source TensorFlow implementation with a Keras API.



### MAST: A Memory-Augmented Self-supervised Tracker
- **Arxiv ID**: http://arxiv.org/abs/2002.07793v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.07793v2)
- **Published**: 2020-02-18 18:43:28+00:00
- **Updated**: 2020-02-26 00:58:21+00:00
- **Authors**: Zihang Lai, Erika Lu, Weidi Xie
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recent interest in self-supervised dense tracking has yielded rapid progress, but performance still remains far from supervised methods. We propose a dense tracking model trained on videos without any annotations that surpasses previous self-supervised methods on existing benchmarks by a significant margin (+15%), and achieves performance comparable to supervised methods. In this paper, we first reassess the traditional choices used for self-supervised training and reconstruction loss by conducting thorough experiments that finally elucidate the optimal choices. Second, we further improve on existing methods by augmenting our architecture with a crucial memory component. Third, we benchmark on large-scale semi-supervised video object segmentation(aka. dense tracking), and propose a new metric: generalizability. Our first two contributions yield a self-supervised network that for the first time is competitive with supervised methods on standard evaluation metrics of dense tracking. When measuring generalizability, we show self-supervised approaches are actually superior to the majority of supervised methods. We believe this new generalizability metric can better capture the real-world use-cases for dense tracking, and will spur new interest in this research direction.



### Conditional Adversarial Camera Model Anonymization
- **Arxiv ID**: http://arxiv.org/abs/2002.07798v3
- **DOI**: 10.1007/978-3-030-66823-5_13
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07798v3)
- **Published**: 2020-02-18 18:53:21+00:00
- **Updated**: 2020-12-03 13:42:36+00:00
- **Authors**: Jerone T. A. Andrews, Yidan Zhang, Lewis D. Griffin
- **Comment**: ECCV 2020 - Advances in Image Manipulation workshop (AIM 2020)
- **Journal**: None
- **Summary**: The model of camera that was used to capture a particular photographic image (model attribution) is typically inferred from high-frequency model-specific artifacts present within the image. Model anonymization is the process of transforming these artifacts such that the apparent capture model is changed. We propose a conditional adversarial approach for learning such transformations. In contrast to previous works, we cast model anonymization as the process of transforming both high and low spatial frequency information. We augment the objective with the loss from a pre-trained dual-stream model attribution classifier, which constrains the generative network to transform the full range of artifacts. Quantitative comparisons demonstrate the efficacy of our framework in a restrictive non-interactive black-box setting.



### Lake Ice Monitoring with Webcams and Crowd-Sourced Images
- **Arxiv ID**: http://arxiv.org/abs/2002.07875v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07875v2)
- **Published**: 2020-02-18 20:46:46+00:00
- **Updated**: 2020-05-08 03:02:35+00:00
- **Authors**: Rajanie Prabha, Manu Tom, Mathias Rothermel, Emmanuel Baltsavias, Laura Leal-Taixe, Konrad Schindler
- **Comment**: Accepted for ISPRS Congress 2020, Nice, France
- **Journal**: None
- **Summary**: Lake ice is a strong climate indicator and has been recognised as part of the Essential Climate Variables (ECV) by the Global Climate Observing System (GCOS). The dynamics of freezing and thawing, and possible shifts of freezing patterns over time, can help in understanding the local and global climate systems. One way to acquire the spatio-temporal information about lake ice formation, independent of clouds, is to analyse webcam images. This paper intends to move towards a universal model for monitoring lake ice with freely available webcam data. We demonstrate good performance, including the ability to generalise across different winters and different lakes, with a state-of-the-art Convolutional Neural Network (CNN) model for semantic image segmentation, Deeplab v3+. Moreover, we design a variant of that model, termed Deep-U-Lab, which predicts sharper, more correct segmentation boundaries. We have tested the model's ability to generalise with data from multiple camera views and two different winters. On average, it achieves intersection-over-union (IoU) values of ~71% across different cameras and ~69% across different winters, greatly outperforming prior work. Going even further, we show that the model even achieves 60% IoU on arbitrary images scraped from photo-sharing web sites. As part of the work, we introduce a new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations.



### Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/2002.07891v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07891v1)
- **Published**: 2020-02-18 21:48:54+00:00
- **Updated**: 2020-02-18 21:48:54+00:00
- **Authors**: Pu Zhao, Pin-Yu Chen, Siyue Wang, Xue Lin
- **Comment**: accepted by AAAI 2020
- **Journal**: None
- **Summary**: Despite the great achievements of the modern deep neural networks (DNNs), the vulnerability/robustness of state-of-the-art DNNs raises security concerns in many application domains requiring high reliability. Various adversarial attacks are proposed to sabotage the learning performance of DNN models. Among those, the black-box adversarial attack methods have received special attentions owing to their practicality and simplicity. Black-box attacks usually prefer less queries in order to maintain stealthy and low costs. However, most of the current black-box attack methods adopt the first-order gradient descent method, which may come with certain deficiencies such as relatively slow convergence and high sensitivity to hyper-parameter settings. In this paper, we propose a zeroth-order natural gradient descent (ZO-NGD) method to design the adversarial attacks, which incorporates the zeroth-order gradient estimation technique catering to the black-box attack scenario and the second-order natural gradient descent to achieve higher query efficiency. The empirical evaluations on image classification datasets demonstrate that ZO-NGD can obtain significantly lower model query complexities compared with state-of-the-art attack methods.



### LocoGAN -- Locally Convolutional GAN
- **Arxiv ID**: http://arxiv.org/abs/2002.07897v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.07897v1)
- **Published**: 2020-02-18 22:03:27+00:00
- **Updated**: 2020-02-18 22:03:27+00:00
- **Authors**: Łukasz Struski, Szymon Knop, Jacek Tabor, Wiktor Daniec, Przemysław Spurek
- **Comment**: None
- **Journal**: None
- **Summary**: In the paper we construct a fully convolutional GAN model: LocoGAN, which latent space is given by noise-like images of possibly different resolutions. The learning is local, i.e. we process not the whole noise-like image, but the sub-images of a fixed size. As a consequence LocoGAN can produce images of arbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our approach comes from the fact that we use the position channels, which allows the generation of fully periodic (e.g. cylindrical panoramic images) or almost periodic ,,infinitely long" images (e.g. wall-papers).



### Dataset of Segmented Nuclei in Hematoxylin and Eosin Stained Histopathology Images of 10 Cancer Types
- **Arxiv ID**: http://arxiv.org/abs/2002.07913v2
- **DOI**: 10.1038/s41597-020-0528-1
- **Categories**: **eess.IV**, cs.CV, I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2002.07913v2)
- **Published**: 2020-02-18 22:45:59+00:00
- **Updated**: 2020-11-30 20:07:00+00:00
- **Authors**: Le Hou, Rajarsi Gupta, John S. Van Arnam, Yuwei Zhang, Kaustubh Sivalenka, Dimitris Samaras, Tahsin M. Kurc, Joel H. Saltz
- **Comment**: None
- **Journal**: Sci Data 7, 185 (2020)
- **Summary**: The distribution and appearance of nuclei are essential markers for the diagnosis and study of cancer. Despite the importance of nuclear morphology, there is a lack of large scale, accurate, publicly accessible nucleus segmentation data. To address this, we developed an analysis pipeline that segments nuclei in whole slide tissue images from multiple cancer types with a quality control process. We have generated nucleus segmentation results in 5,060 Whole Slide Tissue images from 10 cancer types in The Cancer Genome Atlas. One key component of our work is that we carried out a multi-level quality control process (WSI-level and image patch-level), to evaluate the quality of our segmentation results. The image patch-level quality control used manual segmentation ground truth data from 1,356 sampled image patches. The datasets we publish in this work consist of roughly 5 billion quality controlled nuclei from more than 5,060 TCGA WSIs from 10 different TCGA cancer types and 1,356 manually segmented TCGA image patches from the same 10 cancer types plus additional 4 cancer types. Data is available at https://doi.org/10.7937/tcia.2019.4a4dkp9u



### Block Switching: A Stochastic Approach for Deep Learning Security
- **Arxiv ID**: http://arxiv.org/abs/2002.07920v1
- **DOI**: 10.47852/bonviewJCCE2202320
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07920v1)
- **Published**: 2020-02-18 23:14:25+00:00
- **Updated**: 2020-02-18 23:14:25+00:00
- **Authors**: Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin
- **Comment**: Accepted by AdvML19: Workshop on Adversarial Learning Methods for
  Machine Learning and Data Mining at KDD, Anchorage, Alaska, USA, August 5th,
  2019, 5 pages
- **Journal**: Journal of Computational and Cognitive Engineering. Volume 1,
  Issue 4, 2022
- **Summary**: Recent study of adversarial attacks has revealed the vulnerability of modern deep learning models. That is, subtly crafted perturbations of the input can make a trained network with high accuracy produce arbitrary incorrect predictions, while maintain imperceptible to human vision system. In this paper, we introduce Block Switching (BS), a defense strategy against adversarial attacks based on stochasticity. BS replaces a block of model layers with multiple parallel channels, and the active channel is randomly assigned in the run time hence unpredictable to the adversary. We show empirically that BS leads to a more dispersed input gradient distribution and superior defense effectiveness compared with other stochastic defenses such as stochastic activation pruning (SAP). Compared to other defenses, BS is also characterized by the following features: (i) BS causes less test accuracy drop; (ii) BS is attack-independent and (iii) BS is compatible with other defenses and can be used jointly with others.



