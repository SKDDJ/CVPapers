# Arxiv Papers in cs.CV on 2020-02-13
### Geom-GCN: Geometric Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.05287v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05287v2)
- **Published**: 2020-02-13 00:03:09+00:00
- **Updated**: 2020-02-14 01:47:35+00:00
- **Authors**: Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.



### Improving Efficiency in Neural Network Accelerator Using Operands Hamming Distance optimization
- **Arxiv ID**: http://arxiv.org/abs/2002.05293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05293v1)
- **Published**: 2020-02-13 00:36:36+00:00
- **Updated**: 2020-02-13 00:36:36+00:00
- **Authors**: Meng Li, Yilei Li, Pierce Chuang, Liangzhen Lai, Vikas Chandra
- **Comment**: 12 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Neural network accelerator is a key enabler for the on-device AI inference, for which energy efficiency is an important metric. The data-path energy, including the computation energy and the data movement energy among the arithmetic units, claims a significant part of the total accelerator energy. By revisiting the basic physics of the arithmetic logic circuits, we show that the data-path energy is highly correlated with the bit flips when streaming the input operands into the arithmetic units, defined as the hamming distance of the input operand matrices. Based on the insight, we propose a post-training optimization algorithm and a hamming-distance-aware training algorithm to co-design and co-optimize the accelerator and the network synergistically. The experimental results based on post-layout simulation with MobileNetV2 demonstrate on average 2.85X data-path energy reduction and up to 8.51X data-path energy reduction for certain layers.



### Depth Descent Synchronization in $\mathrm{SO}(D)$
- **Arxiv ID**: http://arxiv.org/abs/2002.05299v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05299v3)
- **Published**: 2020-02-13 01:01:17+00:00
- **Updated**: 2022-03-17 12:10:54+00:00
- **Authors**: Tyler Maunu, Gilad Lerman
- **Comment**: 22 pages, 3 figures
- **Journal**: None
- **Summary**: We give robust recovery results for synchronization on the rotation group, $\mathrm{SO}(D)$. In particular, we consider an adversarial corruption setting, where a limited percentage of the observations are arbitrarily corrupted. We give a novel algorithm that exploits Tukey depth in the tangent space, which exactly recovers the underlying rotations up to an outlier percentage of $1/(D(D-1)+2)$. This corresponds to an outlier fraction of $1/4$ for $\mathrm{SO}(2)$ and $1/8$ for $\mathrm{SO}(3)$. In the case of $D=2$, we demonstrate that a variant of this algorithm converges linearly to the ground truth rotations. We finish by discussing this result in relation to a simpler nonconvex energy minimization framework based on least absolute deviations, which exhibits spurious fixed points.



### SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2002.05316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05316v1)
- **Published**: 2020-02-13 02:42:31+00:00
- **Updated**: 2020-02-13 02:42:31+00:00
- **Authors**: Hongwei Yi, Shaoshuai Shi, Mingyu Ding, Jiankai Sun, Kui Xu, Hui Zhou, Zhe Wang, Sheng Li, Guoping Wang
- **Comment**: Accepted by ICRA2020
- **Journal**: None
- **Summary**: 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only.



### Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral Segmentation of Rock micro-CT Images
- **Arxiv ID**: http://arxiv.org/abs/2002.05322v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05322v2)
- **Published**: 2020-02-13 03:14:17+00:00
- **Updated**: 2020-02-15 11:14:47+00:00
- **Authors**: Ying Da Wang, Mehdi Shabaninejad, Ryan T. Armstrong, Peyman Mostaghimi
- **Comment**: 16 figures
- **Journal**: None
- **Summary**: Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is essential for further Digital Rock Physics (DRP) analysis, however, conventional methods such as thresholding, watershed segmentation, and converging active contours are susceptible to user-bias. Deep Convolutional Neural Networks (CNNs) have produced accurate pixelwise semantic segmentation results with natural images and $\mu$CT rock images, however, physical accuracy is not well documented. The performance of 4 CNN architectures is tested for 2D and 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon Sandstone are treated as ground truth and used as training and validation data, with a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then used to validate physical accuracy. The topology of each segmented phase is calculated, and the absolute permeability and multiphase flow is modelled with direct simulation in single and mixed wetting cases. These physical measures of connectivity, and flow characteristics show high variance and uncertainty, with models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and connectivities orders of magnitude off. A new network architecture is also introduced as a hybrid fusion of U-net and ResNet, combining short and long skip connections in a Network-in-Network configuration. The 3D implementation outperforms all other tested models in voxelwise and physical accuracy measures. The network architecture and the volume fraction in the dataset (and associated weighting), are factors that not only influence the accuracy trade-off in the voxelwise case, but is especially important in training a physically accurate model for segmentation.



### MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater Image Color Correction
- **Arxiv ID**: http://arxiv.org/abs/2002.05333v1
- **DOI**: 10.1109/LGRS.2019.2950056
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05333v1)
- **Published**: 2020-02-13 04:15:10+00:00
- **Updated**: 2020-02-13 04:15:10+00:00
- **Authors**: Xiaodong Liu, Zhi Gao, Ben M. Chen
- **Comment**: This paper has already been accepted to journal IEEE geoscience and
  remote sensing letters
- **Journal**: None
- **Summary**: Color correction for underwater images has received increasing interests, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural networks (DCNNs) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multi-scale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multi-scale features are extracted first, followed by augmenting local features on each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared with the state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements.



### Multi-Task Incremental Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.05347v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05347v3)
- **Published**: 2020-02-13 04:58:37+00:00
- **Updated**: 2020-11-18 20:31:18+00:00
- **Authors**: Xialei Liu, Hao Yang, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learns multiple tasks, while sharing knowledge and computation among them. However, it suffers from catastrophic forgetting of previous knowledge when learned incrementally without access to the old data. Most existing object detectors are domain-specific and static, while some are learned incrementally but only within a single domain. Training an object detector incrementally across various domains has rarely been explored. In this work, we propose three incremental learning scenarios across various domains and categories for object detection. To mitigate catastrophic forgetting, attentive feature distillation is proposed to leverages both bottom-up and top-down attentions to extract important information for distillation. We then systematically analyze the proposed distillation method in different scenarios. We find out that, contrary to common understanding, domain gaps have smaller negative impact on incremental detection, while category differences are problematic. For the difficult cases, where the domain gaps and especially category differences are large, we explore three different exemplar sampling methods and show the proposed adaptive sampling method is effective to select diverse and informative samples from entire datasets, to further prevent forgetting. Experimental results show that we achieve the significant improvement in three different scenarios across seven object detection benchmark datasets.



### Object Detection on Single Monocular Images through Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.05349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05349v1)
- **Published**: 2020-02-13 05:03:42+00:00
- **Updated**: 2020-02-13 05:03:42+00:00
- **Authors**: Zifan Yu, Suya You
- **Comment**: 17 pages; Research report
- **Journal**: None
- **Summary**: Without using extra 3-D data like points cloud or depth images for providing 3-D information, we retrieve the 3-D object information from single monocular images. The high-quality predicted depth images are recovered from single monocular images, and it is fed into the 2-D object proposal network with corresponding monocular images. Most existing deep learning frameworks with two-streams input data always fuse separate data by concatenating or adding, which views every part of a feature map can contribute equally to the whole task. However, when data are noisy, and too much information is redundant, these methods no longer produce predictions or classifications efficiently. In this report, we propose a two-dimensional CCA(canonical correlation analysis) framework to fuse monocular images and corresponding predicted depth images for basic computer vision tasks like image classification and object detection. Firstly, we implemented different structures with one-dimensional CCA and Alexnet to test the performance on the image classification task. And then, we applied one of these structures with 2D-CCA for object detection. During these experiments, we found that our proposed framework behaves better when taking predicted depth images as inputs with the model trained from ground truth depth.



### Hypergraph Optimization for Multi-structural Geometric Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/2002.05350v1
- **DOI**: 10.1609/aaai.v33i01.33018730
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05350v1)
- **Published**: 2020-02-13 05:07:11+00:00
- **Updated**: 2020-02-13 05:07:11+00:00
- **Authors**: Shuyuan Lin, Guobao Xiao, Yan Yan, David Suter, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, some hypergraph-based methods have been proposed to deal with the problem of model fitting in computer vision, mainly due to the superior capability of hypergraph to represent the complex relationship between data points. However, a hypergraph becomes extremely complicated when the input data include a large number of data points (usually contaminated with noises and outliers), which will significantly increase the computational burden. In order to overcome the above problem, we propose a novel hypergraph optimization based model fitting (HOMF) method to construct a simple but effective hypergraph. Specifically, HOMF includes two main parts: an adaptive inlier estimation algorithm for vertex optimization and an iterative hyperedge optimization algorithm for hyperedge optimization. The proposed method is highly efficient, and it can obtain accurate model fitting results within a few iterations. Moreover, HOMF can then directly apply spectral clustering, to achieve good fitting performance. Extensive experimental results show that HOMF outperforms several state-of-the-art model fitting methods on both synthetic data and real images, especially in sampling efficiency and in handling data with severe outliers.



### Recurrent Attention Model with Log-Polar Mapping is Robust against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2002.05388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05388v1)
- **Published**: 2020-02-13 08:40:48+00:00
- **Updated**: 2020-02-13 08:40:48+00:00
- **Authors**: Taro Kiritani, Koji Ono
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Convolutional neural networks are vulnerable to small $\ell^p$ adversarial attacks, while the human visual system is not. Inspired by neural networks in the eye and the brain, we developed a novel artificial neural network model that recurrently collects data with a log-polar field of view that is controlled by attention. We demonstrate the effectiveness of this design as a defense against SPSA and PGD adversarial attacks. It also has beneficial properties observed in the animal visual system, such as reflex-like pathways for low-latency inference, fixed amount of computation independent of image size, and rotation and scale invariance. The code for experiments is available at https://gitlab.com/exwzd-public/kiritani_ono_2020.



### Character Segmentation in Asian Collector's Seal Imprints: An Attempt to Retrieval Based on Ancient Character Typeface
- **Arxiv ID**: http://arxiv.org/abs/2003.00831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00831v1)
- **Published**: 2020-02-13 09:55:20+00:00
- **Updated**: 2020-02-13 09:55:20+00:00
- **Authors**: Kangying Li, Biligsaikhan Batjargal, Akira Maeda
- **Comment**: None
- **Journal**: None
- **Summary**: Collector's seals provide important clues about the ownership of a book. They contain much information pertaining to the essential elements of ancient materials and also show the details of possession, its relation to the book, the identity of the collectors and their social status and wealth, amongst others. Asian collectors have typically used artistic ancient characters rather than modern ones to make their seals. In addition to the owner's name, several other words are used to express more profound meanings. A system that automatically recognizes these characters can help enthusiasts and professionals better understand the background information of these seals. However, there is a lack of training data and labelled images, as samples of some seals are scarce and most of them are degraded images. It is necessary to find new ways to make full use of such scarce data. While these data are available online, they do not contain information on the characters'position. The goal of this research is to provide retrieval tools assist in obtaining more information from Asian collector's seals imprints without consuming a lot of computational resources. In this paper, a character segmentation method is proposed to predict the candidate characters'area without any labelled training data that contain character coordinate information. A retrieval-based recognition system that focuses on a single character is also proposed to support seal retrieval and matching. The experimental results demonstrate that the proposed character segmentation method performs well on Asian collector's seals, with 92% of the test data being correctly segmented.



### Emotion Recognition for In-the-wild Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.05447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05447v1)
- **Published**: 2020-02-13 11:29:46+00:00
- **Updated**: 2020-02-13 11:29:46+00:00
- **Authors**: Hanyu Liu, Jiabei Zeng, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.



### EndoL2H: Deep Super-Resolution for Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2002.05459v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05459v2)
- **Published**: 2020-02-13 11:52:47+00:00
- **Updated**: 2020-06-22 19:18:33+00:00
- **Authors**: Yasin Almalioglu, Kutsev Bengisu Ozyoruk, Abdulkadir Gokce, Kagan Incetan, Guliz Irem Gokceler, Muhammed Ali Simsek, Kivanc Ararat, Richard J. Chen, Nicholas J. Durr, Faisal Mahmood, Mehmet Turan
- **Comment**: 23 pages, submitted to IEEE Transactions on Medical Imaging,
  corresponding Author: Mehmet Turan
- **Journal**: None
- **Summary**: Although wireless capsule endoscopy is the preferred modality for diagnosis and assessment of small bowel diseases, the poor camera resolution is a substantial limitation for both subjective and automated diagnostics. Enhanced-resolution endoscopy has shown to improve adenoma detection rate for conventional endoscopy and is likely to do the same for capsule endoscopy. In this work, we propose and quantitatively validate a novel framework to learn a mapping from low-to-high resolution endoscopic images. We combine conditional adversarial networks with a spatial attention block to improve the resolution by up to factors of 8x, 10x, 12x, respectively. Quantitative and qualitative studies performed demonstrate the superiority of EndoL2H over state-of-the-art deep super-resolution methods DBPN, RCAN and SRGAN. MOS tests performed by 30 gastroenterologists qualitatively assess and confirm the clinical relevance of the approach. EndoL2H is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization. Our code and trained models are available at https://github.com/CapsuleEndoscope/EndoL2H.



### PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks for Thermal and NIR to Visible Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/2002.07082v2
- **DOI**: 10.1016/j.neucom.2020.06.104
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.07082v2)
- **Published**: 2020-02-13 11:55:03+00:00
- **Updated**: 2020-08-06 11:50:33+00:00
- **Authors**: Kancharagunta Kishan Babu, Shiv Ram Dubey
- **Comment**: Published in Neurocomputing Journal, Elsevier
- **Journal**: Neurocomputing, 413:41-50, Nov 2020
- **Summary**: In many real world scenarios, it is difficult to capture the images in the visible light spectrum (VIS) due to bad lighting conditions. However, the images can be captured in such scenarios using Near-Infrared (NIR) and Thermal (THM) cameras. The NIR and THM images contain the limited details. Thus, there is a need to transform the images from THM/NIR to VIS for better understanding. However, it is non-trivial task due to the large domain discrepancies and lack of abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to transform the images from one domain to another domain. Most of the available GAN based methods use the combination of the adversarial and the pixel-wise losses (like $L_1$ or $L_2$) as the objective function for training. The quality of transformed images in case of THM/NIR to VIS transformation is still not up to the mark using such objective function. Thus, better objective functions are needed to improve the quality, fine details and realism of the transformed images. A new model for THM/NIR to VIS image transformation called Perceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is introduced to address these issues. The PCSGAN uses the combination of the perceptual (i.e., feature based) losses along with the pixel-wise and the adversarial losses. Both the quantitative and qualitative measures are used to judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR scene datasets. The proposed PCSGAN outperforms the state-of-the-art image transformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is available at https://github.com/KishanKancharagunta/PCSGAN.



### Chaotic Phase Synchronization and Desynchronization in an Oscillator Network for Object Selection
- **Arxiv ID**: http://arxiv.org/abs/2002.05493v1
- **DOI**: 10.1016/j.neunet.2009.06.027
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05493v1)
- **Published**: 2020-02-13 13:39:25+00:00
- **Updated**: 2020-02-13 13:39:25+00:00
- **Authors**: Fabricio A Breve, Marcos G Quiles, Liang Zhao, Elbert E. N. Macau
- **Comment**: None
- **Journal**: BREVE, FA; ZHAO, L; QUILES, MG; MACAU, EEN. Chaotic Phase
  Synchronization and Desynchronization in an Oscillator Network for Object
  Selection. Neural Networks, v. 22, p. 728-737, 2009
- **Summary**: Object selection refers to the mechanism of extracting objects of interest while ignoring other objects and background in a given visual scene. It is a fundamental issue for many computer vision and image analysis techniques and it is still a challenging task to artificial visual systems. Chaotic phase synchronization takes place in cases involving almost identical dynamical systems and it means that the phase difference between the systems is kept bounded over the time, while their amplitudes remain chaotic and may be uncorrelated. Instead of complete synchronization, phase synchronization is believed to be a mechanism for neural integration in brain. In this paper, an object selection model is proposed. Oscillators in the network representing the salient object in a given scene are phase synchronized, while no phase synchronization occurs for background objects. In this way, the salient object can be extracted. In this model, a shift mechanism is also introduced to change attention from one object to another. Computer simulations show that the model produces some results similar to those observed in natural vision systems.



### Replacing Mobile Camera ISP with a Single Deep Learning Model
- **Arxiv ID**: http://arxiv.org/abs/2002.05509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05509v1)
- **Published**: 2020-02-13 14:22:39+00:00
- **Updated**: 2020-02-13 14:22:39+00:00
- **Authors**: Andrey Ignatov, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: As the popularity of mobile photography is growing constantly, lots of efforts are being invested now into building complex hand-crafted camera ISP solutions. In this work, we demonstrate that even the most sophisticated ISP pipelines can be replaced with a single end-to-end deep learning model trained without any prior knowledge about the sensor and optics used in a particular device. For this, we present PyNET, a novel pyramidal CNN architecture designed for fine-grained image restoration that implicitly learns to perform all ISP steps such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The model is trained to convert RAW Bayer data obtained directly from mobile camera sensor into photos captured with a professional high-end DSLR camera, making the solution independent of any particular mobile ISP implementation. To validate the proposed approach on the real data, we collected a large-scale dataset consisting of 10 thousand full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20 cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The experiments demonstrate that the proposed solution can easily get to the level of the embedded P20's ISP pipeline that, unlike our approach, is combining the data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and codes used in this paper are available on the project website.



### SpotNet: Self-Attention Multi-Task Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.05540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05540v2)
- **Published**: 2020-02-13 14:43:24+00:00
- **Updated**: 2020-06-11 14:22:49+00:00
- **Authors**: Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Maguelonne Héritier
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.



### Superpixel Image Classification with Graph Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.05544v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05544v2)
- **Published**: 2020-02-13 14:52:32+00:00
- **Updated**: 2020-11-15 17:04:53+00:00
- **Authors**: Pedro H. C. Avelar, Anderson R. Tavares, Thiago L. T. da Silveira, Cláudio R. Jung, Luís C. Lamb
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with self-attention mechanisms, outperforms other GNN models. Although raw image classifiers perform better than GATs due to information loss during the RAG generation, our methodology opens an interesting avenue of research on deep learning beyond rectangular-gridded images, such as 360-degree field of view panoramas. Traditional convolutional kernels of current state-of-the-art methods cannot handle panoramas, whereas the adapted superpixel algorithms and the resulting region adjacency graphs can naturally feed a GNN, without topology issues.



### Sparse and Structured Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.05556v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05556v2)
- **Published**: 2020-02-13 15:08:12+00:00
- **Updated**: 2021-07-08 12:39:43+00:00
- **Authors**: Pedro Henrique Martins, Vlad Niculae, Zita Marinho, André Martins
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention mechanisms are widely used in multimodal tasks, as visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign some probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select only the relevant regions (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in VQA show gains in accuracy as well as higher similarity to human attention, which suggests better interpretability.



### An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization
- **Arxiv ID**: http://arxiv.org/abs/2002.07613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07613v1)
- **Published**: 2020-02-13 15:28:42+00:00
- **Updated**: 2020-02-13 15:28:42+00:00
- **Authors**: Yiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, Kangning Liu, Sudarshini Tyagi, Laura Heacock, S. Gene Kim, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: Medical images differ from natural images in significantly higher resolutions and smaller regions of interest. Because of these differences, neural network architectures that work well for natural images might not be applicable to medical image analysis. In this work, we extend the globally-aware multiple instance classifier, a framework we proposed to address these unique properties of medical images. This model first uses a low-capacity, yet memory-efficient, network on the whole image to identify the most informative regions. It then applies another higher-capacity network to collect details from chosen regions. Finally, it employs a fusion module that aggregates global and local information to make a final prediction. While existing methods often require lesion segmentation during training, our model is trained with only image-level labels and can generate pixel-level saliency maps indicating possible malignant findings. We apply the model to screening mammography interpretation: predicting the presence or absence of benign and malignant lesions. On the NYU Breast Cancer Screening Dataset, consisting of more than one million images, our model achieves an AUC of 0.93 in classifying breasts with malignant findings, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, our model is 4.1x faster for inference while using 78.4% less GPU memory. Furthermore, we demonstrate, in a reader study, that our model surpasses radiologist-level AUC by a margin of 0.11. The proposed model is available online: https://github.com/nyukat/GMIC.



### Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for Event-based Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.05583v1
- **DOI**: 10.1145/3343031.3350975
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05583v1)
- **Published**: 2020-02-13 15:58:31+00:00
- **Updated**: 2020-02-13 15:58:31+00:00
- **Authors**: Haosheng Chen, Qiangqiang Wu, Yanjie Liang, Xinbo Gao, Hanzi Wang
- **Comment**: 9 pages, 5 figures
- **Journal**: Proceedings of the 27th ACM International Conference on Multimedia
  (MM '19). 2019, Nice, France. ACM, New York, NY, USA
- **Summary**: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in a variety of situations, such as fast motion and low illumination scenes. However, most of the event-based object tracking methods are designed for scenarios with untextured objects and uncluttered backgrounds. There are few event-based object tracking methods that support bounding box-based object tracking. The main idea behind this work is to propose an asynchronous Event-based Tracking-by-Detection (ETD) method for generic bounding box-based object tracking. To achieve this goal, we present an Adaptive Time-Surface with Linear Time Decay (ATSLTD) event-to-frame conversion algorithm, which asynchronously and effectively warps the spatio-temporal information of asynchronous retinal events to a sequence of ATSLTD frames with clear object contours. We feed the sequence of ATSLTD frames to the proposed ETD method to perform accurate and efficient object tracking, which leverages the high temporal resolution property of event cameras. We compare the proposed ETD method with seven popular object tracking methods, that are based on conventional cameras or event cameras, and two variants of ETD. The experimental results show the superiority of the proposed ETD method in handling various challenging environments.



### A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of Appearance Bias in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2002.05636v3
- **DOI**: 10.1007/s43681-020-00035-y
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05636v3)
- **Published**: 2020-02-13 17:09:27+00:00
- **Updated**: 2021-01-13 17:15:05+00:00
- **Authors**: Ryan Steed, Aylin Caliskan
- **Comment**: 11 pages, 7 figures. Revision for AI Ethics
- **Journal**: AI Ethics (2021)
- **Summary**: Research in social psychology has shown that people's biased, subjective judgments about another's personality based solely on their appearance are not predictive of their actual personality traits. But researchers and companies often utilize computer vision models to predict similarly subjective personality attributes such as "employability." We seek to determine whether state-of-the-art, black box face processing technology can learn human-like appearance biases. With features extracted with FaceNet, a widely used face recognition framework, we train a transfer learning model on human subjects' first impressions of personality traits in other faces as measured by social psychologists. We find that features extracted with FaceNet can be used to predict human appearance bias scores for deliberately manipulated faces but not for randomly generated faces scored by humans. Additionally, in contrast to work with human biases in social psychology, the model does not find a significant signal correlating politicians' vote shares with perceived competence bias. With Local Interpretable Model-Agnostic Explanations (LIME), we provide several explanations for this discrepancy. Our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate. We shed light on the ways in which appearance bias could be embedded in face processing technology and cast further doubt on the practice of predicting subjective traits based on appearances.



### GANILLA: Generative Adversarial Networks for Image to Illustration Translation
- **Arxiv ID**: http://arxiv.org/abs/2002.05638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05638v2)
- **Published**: 2020-02-13 17:12:09+00:00
- **Updated**: 2020-02-14 09:46:35+00:00
- **Authors**: Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu
- **Comment**: to be published in Image and Vision Computing
- **Journal**: None
- **Summary**: In this paper, we explore illustrations in children's books as a new domain in unpaired image-to-image translation. We show that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the same time. We propose a new generator network to address this issue and show that the resulting network strikes a better balance between style and content.   There are no well-defined or agreed-upon evaluation metrics for unpaired image-to-image translation. So far, the success of image translation models has been based on subjective, qualitative visual comparison on a limited number of images. To address this problem, we propose a new framework for the quantitative evaluation of image-to-illustration models, where both content and style are taken into account using separate classifiers. In this new evaluation framework, our proposed model performs better than the current state-of-the-art models on the illustrations dataset. Our code and pretrained models can be found at https://github.com/giddyyupp/ganilla.



### Summarizing the performances of a background subtraction algorithm measured on several videos
- **Arxiv ID**: http://arxiv.org/abs/2002.05654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05654v2)
- **Published**: 2020-02-13 17:35:34+00:00
- **Updated**: 2020-05-28 15:55:54+00:00
- **Authors**: Sébastien Piérard, Marc Van Droogenbroeck
- **Comment**: Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: ICIP 2020
- **Summary**: There exist many background subtraction algorithms to detect motion in videos. To help comparing them, datasets with ground-truth data such as CDNET or LASIESTA have been proposed. These datasets organize videos in categories that represent typical challenges for background subtraction. The evaluation procedure promoted by their authors consists in measuring performance indicators for each video separately and to average them hierarchically, within a category first, then between categories, a procedure which we name "summarization". While the summarization by averaging performance indicators is a valuable effort to standardize the evaluation procedure, it has no theoretical justification and it breaks the intrinsic relationships between summarized indicators. This leads to interpretation inconsistencies. In this paper, we present a theoretical approach to summarize the performances for multiple videos that preserves the relationships between performance indicators. In addition, we give formulas and an algorithm to calculate summarized performances. Finally, we showcase our observations on CDNET 2014.



### Classifying the classifier: dissecting the weight space of neural networks
- **Arxiv ID**: http://arxiv.org/abs/2002.05688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.05688v1)
- **Published**: 2020-02-13 18:12:02+00:00
- **Updated**: 2020-02-13 18:12:02+00:00
- **Authors**: Gabriel Eilertsen, Daniel Jönsson, Timo Ropinski, Jonas Unger, Anders Ynnerman
- **Comment**: ECAI 2020
- **Journal**: None
- **Summary**: This paper presents an empirical study on the weights of neural networks, where we interpret each model as a point in a high-dimensional space -- the neural weight space. To explore the complex structure of this space, we sample from a diverse selection of training variations (dataset, optimization procedure, architecture, etc.) of neural network classifiers, and train a large number of models to represent the weight space. Then, we use a machine learning approach for analyzing and extracting information from this space. Most centrally, we train a number of novel deep meta-classifiers with the objective of classifying different properties of the training setup by identifying their footprints in the weight space. Thus, the meta-classifiers probe for patterns induced by hyper-parameters, so that we can quantify how much, where, and when these are encoded through the optimization process. This provides a novel and complementary view for explainable AI, and we show how meta-classifiers can reveal a great deal of information about the training setup and optimization, by only considering a small subset of randomly selected consecutive weights. To promote further research on the weight space, we release the neural weight space (NWS) dataset -- a collection of 320K weight snapshots from 16K individually trained deep neural networks.



### Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE
- **Arxiv ID**: http://arxiv.org/abs/2002.05692v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2002.05692v1)
- **Published**: 2020-02-13 18:18:51+00:00
- **Updated**: 2020-02-13 18:18:51+00:00
- **Authors**: Petru-Daniel Tudosiu, Thomas Varsavsky, Richard Shaw, Mark Graham, Parashkev Nachev, Sebastien Ourselin, Carole H. Sudre, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing efficiency and compactness of deep learning architectures, together with hardware improvements, have enabled the complex and high-dimensional modelling of medical volumetric data at higher resolutions. Recently, Vector-Quantised Variational Autoencoders (VQ-VAE) have been proposed as an efficient generative unsupervised learning approach that can encode images to a small percentage of their initial size, while preserving their decoded fidelity. Here, we show a VQ-VAE inspired network can efficiently encode a full-resolution 3D brain volume, compressing the data to $0.825\%$ of the original size while maintaining image fidelity, and significantly outperforming the previous state-of-the-art. We then demonstrate that VQ-VAE decoded images preserve the morphological characteristics of the original data through voxel-based morphology and segmentation experiments. Lastly, we show that such models can be pre-trained and then fine-tuned on different datasets without the introduction of bias.



### Generative-based Airway and Vessel Morphology Quantification on Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2002.05702v2
- **DOI**: 10.1016/j.media.2020.101691
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML, 68T20, I.2.1; I.4.7; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2002.05702v2)
- **Published**: 2020-02-13 18:45:31+00:00
- **Updated**: 2020-03-13 16:32:10+00:00
- **Authors**: Pietro Nardelli, James C. Ross, Raúl San José Estépar
- **Comment**: 19 pages, 13 figures
- **Journal**: None
- **Summary**: Accurately and precisely characterizing the morphology of small pulmonary structures from Computed Tomography (CT) images, such as airways and vessels, is becoming of great importance for diagnosis of pulmonary diseases. The smaller conducting airways are the major site of increased airflow resistance in chronic obstructive pulmonary disease (COPD), while accurately sizing vessels can help identify arterial and venous changes in lung regions that may determine future disorders. However, traditional methods are often limited due to image resolution and artifacts.   We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius. CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.   For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods. Then, in-vivo validation is performed by analyzing the association between the percentage of the predicted forced expiratory volume in one second (FEV1\%) and the value of the Pi10 parameter, two well-known measures of lung function and airway disease, for airways. For vessels, we assess the correlation between our estimate of the small-vessel blood volume and the lungs' diffusing capacity for carbon monoxide (DLCO).   The results demonstrate that Convolutional Neural Networks (CNNs) provide a promising direction for accurately measuring vessels and airways on chest CT images with physiological correlates.



### A Simple Framework for Contrastive Learning of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2002.05709v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05709v3)
- **Published**: 2020-02-13 18:50:45+00:00
- **Updated**: 2020-07-01 00:09:08+00:00
- **Authors**: Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton
- **Comment**: ICML'2020. Code and pretrained models at
  https://github.com/google-research/simclr
- **Journal**: None
- **Summary**: This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.



### Cross-Iteration Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2002.05712v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05712v3)
- **Published**: 2020-02-13 18:52:57+00:00
- **Updated**: 2021-03-25 06:57:36+00:00
- **Authors**: Zhuliang Yao, Yue Cao, Shuxin Zheng, Gao Huang, Stephen Lin
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: A well-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, we present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. Code is available at https://github.com/Howal/Cross-iterationBatchNorm .



### Automatically Discovering and Learning New Visual Categories with Ranking Statistics
- **Arxiv ID**: http://arxiv.org/abs/2002.05714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.05714v1)
- **Published**: 2020-02-13 18:53:32+00:00
- **Updated**: 2020-02-13 18:53:32+00:00
- **Authors**: Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, Andrew Zisserman
- **Comment**: ICLR 2020, code: http://www.robots.ox.ac.uk/~vgg/research/auto_novel
- **Journal**: None
- **Summary**: We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. We evaluate our approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin.



### ACEnet: Anatomical Context-Encoding Network for Neuroanatomy Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.05773v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.05773v3)
- **Published**: 2020-02-13 20:48:46+00:00
- **Updated**: 2021-01-02 22:30:59+00:00
- **Authors**: Yuemeng Li, Hongming Li, Yong Fan
- **Comment**: None
- **Journal**: Medical Image Analysis, 2021
- **Summary**: Segmentation of brain structures from magnetic resonance (MR) scans plays an important role in the quantification of brain morphology. Since 3D deep learning models suffer from high computational cost, 2D deep learning methods are favored for their computational efficiency. However, existing 2D deep learning methods are not equipped to effectively capture 3D spatial contextual information that is needed to achieve accurate brain structure segmentation. In order to overcome this limitation, we develop an Anatomical Context-Encoding Network (ACEnet) to incorporate 3D spatial and anatomical contexts in 2D convolutional neural networks (CNNs) for efficient and accurate segmentation of brain structures from MR scans, consisting of 1) an anatomical context encoding module to incorporate anatomical information in 2D CNNs and 2) a spatial context encoding module to integrate 3D image information in 2D CNNs. In addition, a skull stripping module is adopted to guide the 2D CNNs to attend to the brain. Extensive experiments on three benchmark datasets have demonstrated that our method achieves promising performance compared with state-of-the-art alternative methods for brain structure segmentation in terms of both computational efficiency and segmentation accuracy.



### CBIR using features derived by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.07877v1
- **DOI**: 10.1145/3470568
- **Categories**: **cs.IR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07877v1)
- **Published**: 2020-02-13 21:26:32+00:00
- **Updated**: 2020-02-13 21:26:32+00:00
- **Authors**: Subhadip Maji, Smarajit Bose
- **Comment**: 18 pages, 31 figures
- **Journal**: None
- **Summary**: In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the semantic gap.   In this paper, we propose to use features derived from pre-trained network models from a deep-learning convolution network trained for a large image classification problem. This approach appears to produce vastly superior results for a variety of databases, and it outperforms many contemporary CBIR systems. We analyse the retrieval time of the method, and also propose a pre-clustering of the database based on the above-mentioned features which yields comparable results in a much shorter time in most of the cases.



### Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.05809v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.05809v2)
- **Published**: 2020-02-13 23:18:52+00:00
- **Updated**: 2021-09-09 21:34:45+00:00
- **Authors**: Konstantinos P. Panousis, Sotirios Chatzis, Sergios Theodoridis
- **Comment**: International Symposium on Visual Computing (ISVC) 2021
- **Journal**: None
- **Summary**: Hidden Markov Models (HMMs) comprise a powerful generative approach for modeling sequential data and time-series in general. However, the commonly employed assumption of the dependence of the current time frame to a single or multiple immediately preceding frames is unrealistic; more complicated dynamics potentially exist in real world scenarios. This paper revisits conventional sequential modeling approaches, aiming to address the problem of capturing time-varying temporal dependency patterns. To this end, we propose a different formulation of HMMs, whereby the dependence on past frames is dynamically inferred from the data. Specifically, we introduce a hierarchical extension by postulating an additional latent variable layer; therein, the (time-varying) temporal dependence patterns are treated as latent variables over which inference is performed. We leverage solid arguments from the Variational Bayes framework and derive a tractable inference algorithm based on the forward-backward algorithm. As we experimentally show, our approach can model highly complex sequential data and can effectively handle data with missing values.



