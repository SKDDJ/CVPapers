# Arxiv Papers in cs.CV on 2020-02-08
### Predictive online optimisation with applications to optical flow
- **Arxiv ID**: http://arxiv.org/abs/2002.03053v2
- **DOI**: 10.1007/s10851-020-01000-4
- **Categories**: **math.OC**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2002.03053v2)
- **Published**: 2020-02-08 00:21:01+00:00
- **Updated**: 2020-08-03 17:50:32+00:00
- **Authors**: Tuomo Valkonen
- **Comment**: None
- **Journal**: Journal of Mathematical Imaging and Vision (2021)
- **Summary**: Online optimisation revolves around new data being introduced into a problem while it is still being solved; think of deep learning as more training samples become available. We adapt the idea to dynamic inverse problems such as video processing with optical flow. We introduce a corresponding predictive online primal-dual proximal splitting method. The video frames now exactly correspond to the algorithm iterations. A user-prescribed predictor describes the evolution of the primal variable. To prove convergence we need a predictor for the dual variable based on (proximal) gradient flow. This affects the model that the method asymptotically minimises. We show that for inverse problems the effect is, essentially, to construct a new dynamic regulariser based on infimal convolution of the static regularisers with the temporal coupling. We finish by demonstrating excellent real-time performance of our method in computational image stabilisation and convergence in terms of regularisation theory.



### Bone Suppression on Chest Radiographs With Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.03073v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03073v1)
- **Published**: 2020-02-08 02:53:16+00:00
- **Updated**: 2020-02-08 02:53:16+00:00
- **Authors**: Jia Liang, Yuxing Tang, Youbao Tang, Jing Xiao, Ronald M. Summers
- **Comment**: Accepted by SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: Dual-energy (DE) chest radiography provides the capability of selectively imaging two clinically relevant materials, namely soft tissues, and osseous structures, to better characterize a wide variety of thoracic pathology and potentially improve diagnosis in posteroanterior (PA) chest radiographs. However, DE imaging requires specialized hardware and a higher radiation dose than conventional radiography, and motion artifacts sometimes happen due to involuntary patient motion. In this work, we learn the mapping between conventional radiographs and bone suppressed radiographs. Specifically, we propose to utilize two variations of generative adversarial networks (GANs) for image-to-image translation between conventional and bone suppressed radiographs obtained by DE imaging technique. We compare the effectiveness of training with patient-wisely paired and unpaired radiographs. Experiments show both training strategies yield "radio-realistic'' radiographs with suppressed bony structures and few motion artifacts on a hold-out test set. While training with paired images yields slightly better performance than that of unpaired images when measuring with two objective image quality metrics, namely Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), training with unpaired images demonstrates better generalization ability on unseen anteroposterior (AP) radiographs than paired training.



### Analysis of Random Perturbations for Robust Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.03080v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03080v4)
- **Published**: 2020-02-08 03:46:07+00:00
- **Updated**: 2020-06-07 19:25:31+00:00
- **Authors**: Adam Dziedzic, Sanjay Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has extensively shown that randomized perturbations of neural networks can improve robustness to adversarial attacks. The literature is, however, lacking a detailed compare-and-contrast of the latest proposals to understand what classes of perturbations work, when they work, and why they work. We contribute a detailed evaluation that elucidates these questions and benchmarks perturbation based defenses consistently. In particular, we show five main results: (1) all input perturbation defenses, whether random or deterministic, are equivalent in their efficacy, (2) attacks transfer between perturbation defenses so the attackers need not know the specific type of defense -- only that it involves perturbations, (3) a tuned sequence of noise layers across a network provides the best empirical robustness, (4) perturbation based defenses offer almost no robustness to adaptive attacks unless these perturbations are observed during training, and (5) adversarial examples in a close neighborhood of original inputs show an elevated sensitivity to perturbations in first and second-order analyses.



### Attacking Optical Character Recognition (OCR) Systems with Adversarial Watermarks
- **Arxiv ID**: http://arxiv.org/abs/2002.03095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03095v1)
- **Published**: 2020-02-08 05:53:21+00:00
- **Updated**: 2020-02-08 05:53:21+00:00
- **Authors**: Lu Chen, Wei Xu
- **Comment**: 9 pages, this http url http://aics.site/AICS2020/AICS20_paper_18.pdf
- **Journal**: None
- **Summary**: Optical character recognition (OCR) is widely applied in real applications serving as a key preprocessing tool. The adoption of deep neural network (DNN) in OCR results in the vulnerability against adversarial examples which are crafted to mislead the output of the threat model. Different from vanilla colorful images, images of printed text have clear backgrounds usually. However, adversarial examples generated by most of the existing adversarial attacks are unnatural and pollute the background severely. To address this issue, we propose a watermark attack method to produce natural distortion that is in the disguise of watermarks and evade human eyes' detection. Experimental results show that watermark attacks can yield a set of natural adversarial examples attached with watermarks and attain similar attack performance to the state-of-the-art methods in different attack scenarios.



### Task Augmentation by Rotating for Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00804v1)
- **Published**: 2020-02-08 07:57:24+00:00
- **Updated**: 2020-02-08 07:57:24+00:00
- **Authors**: Jialin Liu, Fei Chao, Chih-Min Lin
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. In this paper, we introduce a task augmentation method by rotating, which increases the number of classes by rotating the original images 90, 180 and 270 degrees, different from traditional augmentation methods which increase the number of images. With a larger amount of classes, we can sample more diverse task instances during training. Therefore, task augmentation by rotating allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach is better than the rotation for increasing the number of images and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. The code is available on \url{www.github.com/AceChuse/TaskLevelAug}.



### Ramifications and Diminution of Image Noise in Iris Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2002.03125v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2002.03125v1)
- **Published**: 2020-02-08 09:06:20+00:00
- **Updated**: 2020-02-08 09:06:20+00:00
- **Authors**: Prajoy Podder, A. H. M Shahariar Parvez, Md. Mizanur Rahman, Tanvir Zaman Khan
- **Comment**: Proceeding of 2018 IEEE International Conference on Current Trends
  toward Converging Technologies, Coimbatore, India
- **Journal**: Published in 2018 IEEE International Conference on Current Trends
  toward Converging Technologies, Coimbatore, India
- **Summary**: Human Identity verification has always been an eye-catching goal in digital based security system. Authentication or identification systems developed using human characteristics such as face, finger print, hand geometry, iris, and voice are denoted as biometric systems. Among the various characteristics, Iris recognition trusts on the idiosyncratic human iris patterns to find out and corroborate the identity of a person. The image is normally contemplated as a gathering of information. Existence of noises in the input or processed image effects degradation in the image superiority. It should be paramount to restore original image from noises for attaining maximum amount of information from corrupted images. Noisy images in biometric identification system cannot give accurate identity. So Image related data or information tends to loss or damage. Images are affected by various sorts of noises. This paper mainly focuses on Salt and Pepper noise, Gaussian noise, Uniform noise, Speckle noise. Different filtering techniques can be adapted for noise diminution to develop the visual quality as well as understandability of images. In this paper, four types of noises have been undertaken and applied on some images. The filtering of these noises uses different types of filters like Mean, Median, Weiner, Gaussian filter etc. A relative interpretation is performed using four different categories of filter with finding the value of quality determined parameters like mean square error (MSE), peak signal to noise ratio (PSNR), average difference value (AD) and maximum difference value (MD).



### Variable-Viewpoint Representations for 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03131v1)
- **Published**: 2020-02-08 10:06:30+00:00
- **Updated**: 2020-02-08 10:06:30+00:00
- **Authors**: Tengyu Ma, Joel Michelson, James Ainooson, Deepayan Sanyal, Xiaohan Wang, Maithilee Kunda
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: For the problem of 3D object recognition, researchers using deep learning methods have developed several very different input representations, including "multi-view" snapshots taken from discrete viewpoints around an object, as well as "spherical" representations consisting of a dense map of essentially ray-traced samples of the object from all directions. These representations offer trade-offs in terms of what object information is captured and to what degree of detail it is captured, but it is not clear how to measure these information trade-offs since the two types of representations are so different. We demonstrate that both types of representations in fact exist at two extremes of a common representational continuum, essentially choosing to prioritize either the number of views of an object or the pixels (i.e., field of view) allotted per view. We identify interesting intermediate representations that lie at points in between these two extremes, and we show, through systematic empirical experiments, how accuracy varies along this continuum as a function of input information as well as the particular deep learning architecture that is used.



### Symbiotic Attention with Privileged Information for Egocentric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03137v1)
- **Published**: 2020-02-08 10:48:43+00:00
- **Updated**: 2020-02-08 10:48:43+00:00
- **Authors**: Xiaohan Wang, Yu Wu, Linchao Zhu, Yi Yang
- **Comment**: AAAI-2020(Oral)
- **Journal**: None
- **Summary**: Egocentric video recognition is a natural testbed for diverse interaction reasoning. Due to the large action vocabulary in egocentric video datasets, recent studies usually utilize a two-branch structure for action recognition, ie, one branch for verb classification and the other branch for noun classification. However, correlation studies between the verb and the noun branches have been largely ignored. Besides, the two branches fail to exploit local features due to the absence of a position-aware attention mechanism. In this paper, we propose a novel Symbiotic Attention framework leveraging Privileged information (SAP) for egocentric video recognition. Finer position-aware object detection features can facilitate the understanding of actor's interaction with the object. We introduce these features in action recognition and regard them as privileged information. Our framework enables mutual communication among the verb branch, the noun branch, and the privileged information. This communication process not only injects local details into global features but also exploits implicit guidance about the spatio-temporal position of an on-going action. We introduce novel symbiotic attention (SA) to enable effective communication. It first normalizes the detection guided features on one branch to underline the action-relevant information from the other branch. SA adaptively enhances the interactions among the three sources. To further catalyze this communication, spatial relations are uncovered for the selection of most action-relevant information. It identifies the most valuable and discriminative feature for classification. We validate the effectiveness of our SAP quantitatively and qualitatively. Notably, it achieves the state-of-the-art on two large-scale egocentric video datasets.



### Multi-Modality Cascaded Fusion Technology for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2002.03138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03138v1)
- **Published**: 2020-02-08 10:59:18+00:00
- **Updated**: 2020-02-08 10:59:18+00:00
- **Authors**: Hongwu Kuang, Xiaodong Liu, Jingwei Zhang, Zicheng Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modality fusion is the guarantee of the stability of autonomous driving systems. In this paper, we propose a general multi-modality cascaded fusion framework, exploiting the advantages of decision-level and feature-level fusion, utilizing target position, size, velocity, appearance and confidence to achieve accurate fusion results. In the fusion process, dynamic coordinate alignment(DCA) is conducted to reduce the error between sensors from different modalities. In addition, the calculation of affinity matrix is the core module of sensor fusion, we propose an affinity loss that improves the performance of deep affinity network(DAN). Last, the proposed step-by-step cascaded fusion framework is more interpretable and flexible compared to the end-toend fusion methods. Extensive experiments on Nuscenes [2] dataset show that our approach achieves the state-of-theart performance.dataset show that our approach achieves the state-of-the-art performance.



### CTM: Collaborative Temporal Modeling for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03152v1)
- **Published**: 2020-02-08 12:14:02+00:00
- **Updated**: 2020-02-08 12:14:02+00:00
- **Authors**: Qian Liu, Tao Wang, Jie Liu, Yang Guan, Qi Bu, Longfei Yang
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of digital multimedia, video understanding has become an important field. For action recognition, temporal dimension plays an important role, and this is quite different from image recognition. In order to learn powerful feature of videos, we propose a Collaborative Temporal Modeling (CTM) block (Figure 1) to learn temporal information for action recognition. Besides a parameter-free identity shortcut, as a separate temporal modeling block, CTM includes two collaborative paths: a spatial-aware temporal modeling path, which we propose the Temporal-Channel Convolution Module (TCCM) with unshared parameters for each spatial position (H*W) to build, and a spatial-unaware temporal modeling path. CTM blocks can seamlessly be inserted into many popular networks to generate CTM Networks and bring the capability of learning temporal information to 2D CNN backbone networks, which only capture spatial information. Experiments on several popular action recognition datasets demonstrate that CTM blocks bring the performance improvements on 2D CNN baselines, and our method achieves the competitive results against the state-of-the-art methods. Code will be made publicly available.



### Towards Reading Beyond Faces for Sparsity-Aware 4D Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03157v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03157v4)
- **Published**: 2020-02-08 13:09:11+00:00
- **Updated**: 2020-08-19 11:02:47+00:00
- **Authors**: Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a sparsity-aware deep network for automatic 4D facial expression recognition (FER). Given 4D data, we first propose a novel augmentation method to combat the data limitation problem for deep learning. This is achieved by projecting the input data into RGB and depth map images and then iteratively performing randomized channel concatenation. Encoded in the given 3D landmarks, we also introduce an effective way to capture the facial muscle movements from three orthogonal plans (TOP), the TOP-landmarks over multi-views. Importantly, we then present a sparsity-aware deep network to compute the sparse representations of convolutional features over multi-views. This is not only effective for a higher recognition accuracy but is also computationally convenient. For training, the TOP-landmarks and sparse representations are used to train a long short-term memory (LSTM) network. The refined predictions are achieved when the learned features collaborate over multi-views. Extensive experimental results achieved on the BU-4DFE dataset show the significance of our method over the state-of-the-art methods by reaching a promising accuracy of 99.69% for 4D FER.



### Deep No-reference Tone Mapped Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2002.03165v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03165v1)
- **Published**: 2020-02-08 13:41:18+00:00
- **Updated**: 2020-02-08 13:41:18+00:00
- **Authors**: Chandra Sekhar Ravuri, Rajesh Sureddi, Sathya Veera Reddy Dendi, Shanmuganathan Raman, Sumohana S. Channappayya
- **Comment**: 5 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: The process of rendering high dynamic range (HDR) images to be viewed on conventional displays is called tone mapping. However, tone mapping introduces distortions in the final image which may lead to visual displeasure. To quantify these distortions, we introduce a novel no-reference quality assessment technique for these tone mapped images. This technique is composed of two stages. In the first stage, we employ a convolutional neural network (CNN) to generate quality aware maps (also known as distortion maps) from tone mapped images by training it with the ground truth distortion maps. In the second stage, we model the normalized image and distortion maps using an Asymmetric Generalized Gaussian Distribution (AGGD). The parameters of the AGGD model are then used to estimate the quality score using support vector regression (SVR). We show that the proposed technique delivers competitive performance relative to the state-of-the-art techniques. The novelty of this work is its ability to visualize various distortions as quality maps (distortion maps), especially in the no-reference setting, and to use these maps as features to estimate the quality score of tone mapped images.



### Spatial-Temporal Multi-Cue Network for Continuous Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03187v1)
- **Published**: 2020-02-08 15:38:44+00:00
- **Updated**: 2020-02-08 15:38:44+00:00
- **Authors**: Hao Zhou, Wengang Zhou, Yun Zhou, Houqiang Li
- **Comment**: Accepted as an oral presentation paper at AAAI 2020. (To appear in
  Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence)
- **Journal**: None
- **Summary**: Despite the recent success of deep learning in continuous sign language recognition (CSLR), deep models typically focus on the most discriminative features, ignoring other potentially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars behind the collaboration of different visual cues (i,e., hand shape, facial expression and body posture). By injecting multi-cue learning into neural network design, we propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module is dedicated to spatial representation and explicitly decomposes visual features of different cues with the aid of a self-contained pose estimation branch. The TMC module models temporal correlations along two parallel paths, i.e., intra-cue and inter-cue, which aims to preserve the uniqueness and explore the collaboration of multiple cues. Finally, we design a joint optimization strategy to achieve the end-to-end sequence learning of the STMC network. To validate the effectiveness, we perform experiments on three large-scale CSLR benchmarks: PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.



### Correction of Chromatic Aberration from a Single Image Using Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2002.03196v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03196v1)
- **Published**: 2020-02-08 16:36:30+00:00
- **Updated**: 2020-02-08 16:36:30+00:00
- **Authors**: Benjamin T. Cecchetto
- **Comment**: Originally this paper was a project for a course in 2009 and has not
  been published. It has been cited multiple times since then. The LaTeX code
  was lost, so it has been revised in February 2020 to post on ArXiV
- **Journal**: None
- **Summary**: In this paper, we propose a method to correct for chromatic aberration in a single photograph. Our method replicates what a user would do in a photo editing program to account for this defect. We find matching keypoints in each colour channel then align them as a user would.



### Exocentric to Egocentric Image Generation via Parallel Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2002.03219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03219v1)
- **Published**: 2020-02-08 19:10:36+00:00
- **Updated**: 2020-02-08 19:10:36+00:00
- **Authors**: Gaowen Liu, Hao Tang, Hugo Latapie, Yan Yan
- **Comment**: It has been accepted by ICASSP 2020
- **Journal**: None
- **Summary**: Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate exocentric (third-person) view to egocentric (first-person) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a non-trivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets show that our model outperforms the state-of-the-art approaches.



### Free-breathing Cardiovascular MRI Using a Plug-and-Play Method with Learned Denoiser
- **Arxiv ID**: http://arxiv.org/abs/2002.03226v1
- **DOI**: 10.1109/ISBI45749.2020.9098453
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03226v1)
- **Published**: 2020-02-08 20:27:07+00:00
- **Updated**: 2020-02-08 20:27:07+00:00
- **Authors**: Sizhuo Liu, Edward Reehorst, Philip Schniter, Rizwan Ahmad
- **Comment**: IEEE ISBI 2020, International Symposium on Biomedical Imaging
- **Journal**: None
- **Summary**: Cardiac magnetic resonance imaging (CMR) is a noninvasive imaging modality that provides a comprehensive evaluation of the cardiovascular system. The clinical utility of CMR is hampered by long acquisition times, however. In this work, we propose and validate a plug-and-play (PnP) method for CMR reconstruction from undersampled multi-coil data. To fully exploit the rich image structure inherent in CMR, we pair the PnP framework with a deep learning (DL)-based denoiser that is trained using spatiotemporal patches from high-quality, breath-held cardiac cine images. The resulting "PnP-DL" method iterates over data consistency and denoising subroutines. We compare the reconstruction performance of PnP-DL to that of compressed sensing (CS) using eight breath-held and ten real-time (RT) free-breathing cardiac cine datasets. We find that, for breath-held datasets, PnP-DL offers more than one dB advantage over commonly used CS methods. For RT free-breathing datasets, where ground truth is not available, PnP-DL receives higher scores in qualitative evaluation. The results highlight the potential of PnP-DL to accelerate RT CMR.



### Intrinsic Dimension Estimation via Nearest Constrained Subspace Classifier
- **Arxiv ID**: http://arxiv.org/abs/2002.03228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03228v1)
- **Published**: 2020-02-08 20:54:42+00:00
- **Updated**: 2020-02-08 20:54:42+00:00
- **Authors**: Liang Liao, Stephen John Maybank
- **Comment**: 19 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: We consider the problems of classification and intrinsic dimension estimation on image data. A new subspace based classifier is proposed for supervised classification or intrinsic dimension estimation. The distribution of the data in each class is modeled by a union of of a finite number ofaffine subspaces of the feature space. The affine subspaces have a common dimension, which is assumed to be much less than the dimension of the feature space. The subspaces are found using regression based on the L0-norm. The proposed method is a generalisation of classical NN (Nearest Neighbor), NFL (Nearest Feature Line) classifiers and has a close relationship to NS (Nearest Subspace) classifier. The proposed classifier with an accurately estimated dimension parameter generally outperforms its competitors in terms of classification accuracy. We also propose a fast version of the classifier using a neighborhood representation to reduce its computational complexity. Experiments on publicly available datasets corroborate these claims.



### Soft Threshold Weight Reparameterization for Learnable Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2002.03231v9
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03231v9)
- **Published**: 2020-02-08 21:31:25+00:00
- **Updated**: 2020-06-22 23:37:12+00:00
- **Authors**: Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi
- **Comment**: 19 pages, 10 figures, Published at International Conference on
  Machine Learning (ICML) 2020
- **Journal**: None
- **Summary**: Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github.com/RAIVNLab/STR.



### Multi-Label Class Balancing Algorithm for Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.03238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03238v1)
- **Published**: 2020-02-08 21:56:28+00:00
- **Updated**: 2020-02-08 21:56:28+00:00
- **Authors**: Jaspar Pahl, Ines Rieger, Dominik Seuss
- **Comment**: This submission is subject to the Action Unit detection task of the
  Affective Behavior Analysis in-the-wild (ABAW) challenge at the IEEE
  Conference on Face and Gesture Recognition 2020
- **Journal**: None
- **Summary**: Isolated facial movements, so-called Action Units, can describe combined emotions or physical states such as pain. As datasets are limited and mostly imbalanced, we present an approach incorporating a multi-label class balancing algorithm. This submission is subject to the Action Unit detection task of the Affective Behavior Analysis in-the-wild (ABAW) challenge at the IEEE Conference on Face and Gesture Recognition 2020.



### Ensemble of Deep Convolutional Neural Networks for Automatic Pavement Crack Detection and Measurement
- **Arxiv ID**: http://arxiv.org/abs/2002.03241v1
- **DOI**: 10.3390/coatings10020152
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03241v1)
- **Published**: 2020-02-08 22:15:11+00:00
- **Updated**: 2020-02-08 22:15:11+00:00
- **Authors**: Zhun Fan, Chong Li, Ying Chen, Paola Di Mascio, Xiaopeng Chen, Guijie Zhu, Giuseppe Loprencipe
- **Comment**: None
- **Journal**: None
- **Summary**: Automated pavement crack detection and measurement are important road issues. Agencies have to guarantee the improvement of road safety. Conventional crack detection and measurement algorithms can be extremely time-consuming and low efficiency. Therefore, recently, innovative algorithms have received increased attention from researchers. In this paper, we propose an ensemble of convolutional neural networks (without a pooling layer) based on probability fusion for automated pavement crack detection and measurement. Specifically, an ensemble of convolutional neural networks was employed to identify the structure of small cracks with raw images. Secondly, outputs of the individual convolutional neural network model for the ensemble were averaged to produce the final crack probability value of each pixel, which can obtain a predicted probability map. Finally, the predicted morphological features of the cracks were measured by using the skeleton extraction algorithm. To validate the proposed method, some experiments were performed on two public crack databases (CFD and AigleRN) and the results of the different state-of-the-art methods were compared. The experimental results show that the proposed method outperforms the other methods. For crack measurement, the crack length and width can be measure based on different crack types (complex, common, thin, and intersecting cracks.). The results show that the proposed algorithm can be effectively applied for crack measurement.



