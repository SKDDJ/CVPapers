# Arxiv Papers in cs.CV on 2020-02-17
### Learning a Directional Soft Lane Affordance Model for Road Scenes Using Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2002.11477v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2002.11477v2)
- **Published**: 2020-02-17 00:57:34+00:00
- **Updated**: 2020-04-15 13:19:45+00:00
- **Authors**: Robin Karlsson, Erik Sjoberg
- **Comment**: Accepted for IEEE IV 2020
- **Journal**: None
- **Summary**: Humans navigate complex environments in an organized yet flexible manner, adapting to the context and implicit social rules. Understanding these naturally learned patterns of behavior is essential for applications such as autonomous vehicles. However, algorithmically defining these implicit rules of human behavior remains difficult. This work proposes a novel self-supervised method for training a probabilistic network model to estimate the regions humans are most likely to drive in as well as a multimodal representation of the inferred direction of travel at each point. The model is trained on individual human trajectories conditioned on a representation of the driving environment. The model is shown to successfully generalize to new road scenes, demonstrating potential for real-world application as a prior for socially acceptable driving behavior in challenging or ambiguous scenarios which are poorly handled by explicit traffic rules.



### SpotTheFake: An Initial Report on a New CNN-Enhanced Platform for Counterfeit Goods Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.06735v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV, I.2.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2002.06735v2)
- **Published**: 2020-02-17 01:51:22+00:00
- **Updated**: 2020-02-19 16:36:31+00:00
- **Authors**: Alexandru Şerban, George Ilaş, George-Cosmin Poruşniuc
- **Comment**: 7 pages, 13 figures
- **Journal**: None
- **Summary**: The counterfeit goods trade represents nowadays more than 3.3% of the whole world trade and thus it's a problem that needs now more than ever a lot of attention and a reliable solution that would reduce the negative impact it has over the modern society. This paper presents the design and early stage development of a novel counterfeit goods detection platform that makes use of the outstsanding learning capabilities of the classical VGG16 convolutional model trained through the process of "transfer learning" and a multi-stage fake detection procedure that proved to be not only reliable but also very robust in the experiments we have conducted so far using an image dataset of various goods which we gathered ourselves.



### Directional Deep Embedding and Appearance Learning for Fast Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.06736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06736v1)
- **Published**: 2020-02-17 01:51:57+00:00
- **Updated**: 2020-02-17 01:51:57+00:00
- **Authors**: Yingjie Yin, De Xu, Xingang Wang, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent semi-supervised video object segmentation (VOS) methods rely on fine-tuning deep convolutional neural networks online using the given mask of the first frame or predicted masks of subsequent frames. However, the online fine-tuning process is usually time-consuming, limiting the practical use of such methods. We propose a directional deep embedding and appearance learning (DDEAL) method, which is free of the online fine-tuning process, for fast VOS. First, a global directional matching module, which can be efficiently implemented by parallel convolutional operations, is proposed to learn a semantic pixel-wise embedding as an internal guidance. Second, an effective directional appearance model based statistics is proposed to represent the target and background on a spherical embedding space for VOS. Equipped with the global directional matching module and the directional appearance model learning module, DDEAL learns static cues from the labeled first frame and dynamically updates cues of the subsequent frames for object segmentation. Our method exhibits state-of-the-art VOS performance without using online fine-tuning. Specifically, it achieves a J & F mean score of 74.8% on DAVIS 2017 dataset and an overall score G of 71.3% on the large-scale YouTube-VOS dataset, while retaining a speed of 25 fps with a single NVIDIA TITAN Xp GPU. Furthermore, our faster version runs 31 fps with only a little accuracy loss. Our code and trained networks are available at https://github.com/YingjieYin/Directional-Deep-Embedding-and-Appearance-Learning-for-Fast-Video-Object-Segmentation.



### Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks
- **Arxiv ID**: http://arxiv.org/abs/2002.06753v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.06753v3)
- **Published**: 2020-02-17 03:18:45+00:00
- **Updated**: 2020-07-01 13:59:50+00:00
- **Authors**: Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, Tom Goldstein
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.



### Superpixel Segmentation via Convolutional Neural Networks with Regularized Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2002.06765v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06765v3)
- **Published**: 2020-02-17 04:32:03+00:00
- **Updated**: 2020-06-26 14:02:13+00:00
- **Authors**: Teppei Suzuki
- **Comment**: To appear in ICASSP 2020
- **Journal**: None
- **Summary**: We propose an unsupervised superpixel segmentation method by optimizing a randomly-initialized convolutional neural network (CNN) in inference time. Our method generates superpixels via CNN from a single image without any labels by minimizing a proposed objective function for superpixel segmentation in inference time. There are three advantages to our method compared with many of existing methods: (i) leverages an image prior of CNN for superpixel segmentation, (ii) adaptively changes the number of superpixels according to the given images, and (iii) controls the property of superpixels by adding an auxiliary cost to the objective function. We verify the advantages of our method quantitatively and qualitatively on BSDS500 and SBD datasets.



### Dimensionality Reduction and Motion Clustering during Activities of Daily Living: 3, 4, and 7 Degree-of-Freedom Arm Movements
- **Arxiv ID**: http://arxiv.org/abs/2003.02641v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.02641v1)
- **Published**: 2020-02-17 04:32:36+00:00
- **Updated**: 2020-02-17 04:32:36+00:00
- **Authors**: Yuri Gloumakov, Adam J. Spiers, Aaron M. Dollar
- **Comment**: 11 pages, 10 figures, 1 table
- **Journal**: None
- **Summary**: The wide variety of motions performed by the human arm during daily tasks makes it desirable to find representative subsets to reduce the dimensionality of these movements for a variety of applications, including the design and control of robotic and prosthetic devices. This paper presents a novel method and the results of an extensive human subjects study to obtain representative arm joint angle trajectories that span naturalistic motions during Activities of Daily Living (ADLs). In particular, we seek to identify sets of useful motion trajectories of the upper limb that are functions of a single variable, allowing, for instance, an entire prosthetic or robotic arm to be controlled with a single input from a user, along with a means to select between motions for different tasks. Data driven approaches are used to obtain clusters as well as representative motion averages for the full-arm 7 degree of freedom (DOF), elbow-wrist 4 DOF, and wrist-only 3 DOF motions. The proposed method makes use of well-known techniques such as dynamic time warping (DTW) to obtain a divergence measure between motion segments, DTW barycenter averaging (DBA) to obtain averages, Ward's distance criterion to build hierarchical trees, batch-DTW to simultaneously align multiple motion data, and functional principal component analysis (fPCA) to evaluate cluster variability. The clusters that emerge associate various recorded motions into primarily hand start and end location for the full-arm system, motion direction for the wrist-only system, and an intermediate between the two qualities for the elbow-wrist system. The proposed clustering methodology is justified by comparing results against alternative approaches.



### Unsupervised Image-generation Enhanced Adaptation for Object Detection in Thermal images
- **Arxiv ID**: http://arxiv.org/abs/2002.06770v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06770v3)
- **Published**: 2020-02-17 04:53:30+00:00
- **Updated**: 2021-11-01 00:49:45+00:00
- **Authors**: Peng Liu, Fuyu Li, Wanyi Li
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Object detection in thermal images is an important computer vision task and has many applications such as unmanned vehicles, robotics, surveillance and night vision. Deep learning based detectors have achieved major progress, which usually need large amount of labelled training data. However, labelled data for object detection in thermal images is scarce and expensive to collect. How to take advantage of the large number labelled visible images and adapt them into thermal image domain, is expected to solve. This paper proposes an unsupervised image-generation enhanced adaptation method for object detection in thermal images. To reduce the gap between visible domain and thermal domain, the proposed method manages to generate simulated fake thermal images that are similar to the target images, and preserves the annotation information of the visible source domain. The image generation includes a CycleGAN based image-to-image translation and an intensity inversion transformation. Generated fake thermal images are used as renewed source domain. And then the off-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between generated intermediate domain and the thermal target domain. Experiments demonstrate the effectiveness and superiority of the proposed method.



### Deep Domain Adaptive Object Detection: a Survey
- **Arxiv ID**: http://arxiv.org/abs/2002.06797v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06797v3)
- **Published**: 2020-02-17 06:40:19+00:00
- **Updated**: 2020-11-11 06:29:01+00:00
- **Authors**: Wanyi Li, Fuyu Li, Yongkang Luo, Peng Wang, Jia sun
- **Comment**: Accepted by IEEE SSCI 2020, 6 pages
- **Journal**: None
- **Summary**: Deep learning (DL) based object detection has achieved great progress. These methods typically assume that large amount of labeled training data is available, and training and test data are drawn from an identical distribution. However, the two assumptions are not always hold in practice. Deep domain adaptive object detection (DDAOD) has emerged as a new learning paradigm to address the above mentioned challenges. This paper aims to review the state-of-the-art progress on deep domain adaptive object detection approaches. Firstly, we introduce briefly the basic concepts of deep domain adaptation. Secondly, the deep domain adaptive detectors are classified into five categories and detailed descriptions of representative methods in each category are provided. Finally, insights for future research trend are presented.



### CQ-VQA: Visual Question Answering on Categorized Questions
- **Arxiv ID**: http://arxiv.org/abs/2002.06800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06800v1)
- **Published**: 2020-02-17 06:45:29+00:00
- **Updated**: 2020-02-17 06:45:29+00:00
- **Authors**: Aakansha Mishra, Ashish Anand, Prithwijit Guha
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes CQ-VQA, a novel 2-level hierarchical but end-to-end model to solve the task of visual question answering (VQA). The first level of CQ-VQA, referred to as question categorizer (QC), classifies questions to reduce the potential answer search space. The QC uses attended and fused features of the input question and image. The second level, referred to as answer predictor (AP), comprises of a set of distinct classifiers corresponding to each question category. Depending on the question category predicted by QC, only one of the classifiers of AP remains active. The loss functions of QC and AP are aggregated together to make it an end-to-end model. The proposed model (CQ-VQA) is evaluated on the TDIUC dataset and is benchmarked against state-of-the-art approaches. Results indicate competitive or better performance of CQ-VQA.



### Fully Convolutional Neural Networks for Raw Eye Tracking Data Segmentation, Generation, and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2002.10905v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10905v3)
- **Published**: 2020-02-17 06:57:09+00:00
- **Updated**: 2021-01-17 12:22:08+00:00
- **Authors**: Wolfgang Fuhl, Yao Rong, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we use fully convolutional neural networks for the semantic segmentation of eye tracking data. We also use these networks for reconstruction, and in conjunction with a variational auto-encoder to generate eye movement data. The first improvement of our approach is that no input window is necessary, due to the use of fully convolutional networks and therefore any input size can be processed directly. The second improvement is that the used and generated data is raw eye tracking data (position X, Y and time) without preprocessing. This is achieved by pre-initializing the filters in the first layer and by building the input tensor along the z axis. We evaluated our approach on three publicly available datasets and compare the results to the state of the art.



### Reinforcement learning for the privacy preservation and manipulation of eye tracking data
- **Arxiv ID**: http://arxiv.org/abs/2002.06806v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06806v2)
- **Published**: 2020-02-17 07:02:19+00:00
- **Updated**: 2020-10-02 06:41:49+00:00
- **Authors**: Wolfgang Fuhl, Efe Bozkir, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an approach based on reinforcement learning for eye tracking data manipulation. It is based on two opposing agents, where one tries to classify the data correctly and the second agent looks for patterns in the data, which get manipulated to hide specific information. We show that our approach is successfully applicable to preserve the privacy of the subjects. For this purpose, we evaluate our approach iteratively to showcase the behavior of the reinforcement learning based approach. In addition, we evaluate the importance of temporal, as well as spatial, information of eye tracking data for specific classification goals. In the last part of our evaluation, we apply the procedure to further public data sets without re-training the autoencoder or the data manipulator. The results show that the learned manipulation is generalized and applicable to unseen data as well.



### AIBench: An Agile Domain-specific Benchmarking Methodology and an AI Benchmark Suite
- **Arxiv ID**: http://arxiv.org/abs/2002.07162v1
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07162v1)
- **Published**: 2020-02-17 07:29:05+00:00
- **Updated**: 2020-02-17 07:29:05+00:00
- **Authors**: Wanling Gao, Fei Tang, Jianfeng Zhan, Chuanxin Lan, Chunjie Luo, Lei Wang, Jiahui Dai, Zheng Cao, Xiongwang Xiong, Zihan Jiang, Tianshu Hao, Fanda Fan, Xu Wen, Fan Zhang, Yunyou Huang, Jianan Chen, Mengjia Du, Rui Ren, Chen Zheng, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Gang Lu, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, Hainan Ye
- **Comment**: 25 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1908.08998
- **Journal**: None
- **Summary**: Domain-specific software and hardware co-design is encouraging as it is much easier to achieve efficiency for fewer tasks. Agile domain-specific benchmarking speeds up the process as it provides not only relevant design inputs but also relevant metrics, and tools. Unfortunately, modern workloads like Big data, AI, and Internet services dwarf the traditional one in terms of code size, deployment scale, and execution path, and hence raise serious benchmarking challenges.   This paper proposes an agile domain-specific benchmarking methodology. Together with seventeen industry partners, we identify ten important end-to-end application scenarios, among which sixteen representative AI tasks are distilled as the AI component benchmarks. We propose the permutations of essential AI and non-AI component benchmarks as end-to-end benchmarks. An end-to-end benchmark is a distillation of the essential attributes of an industry-scale application. We design and implement a highly extensible, configurable, and flexible benchmark framework, on the basis of which, we propose the guideline for building end-to-end benchmarks, and present the first end-to-end Internet service AI benchmark.   The preliminary evaluation shows the value of our benchmark suite---AIBench against MLPerf and TailBench for hardware and software designers, micro-architectural researchers, and code developers. The specifications, source code, testbed, and results are publicly available from the web site \url{http://www.benchcouncil.org/AIBench/index.html}.



### Discernible Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2002.06810v3
- **DOI**: 10.1145/3394171.3413968
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06810v3)
- **Published**: 2020-02-17 07:35:08+00:00
- **Updated**: 2020-09-08 00:44:12+00:00
- **Authors**: Zhaohui Yang, Yunhe Wang, Chang Xu, Peng Du, Chao Xu, Chunjing Xu, Qi Tian
- **Comment**: Accepted by ACMMM 2020
- **Journal**: None
- **Summary**: Image compression, as one of the fundamental low-level image processing tasks, is very essential for computer vision. Tremendous computing and storage resources can be preserved with a trivial amount of visual information. Conventional image compression methods tend to obtain compressed images by minimizing their appearance discrepancy with the corresponding original images, but pay little attention to their efficacy in downstream perception tasks, e.g., image recognition and object detection. Thus, some of compressed images could be recognized with bias. In contrast, this paper aims to produce compressed images by pursuing both appearance and perceptual consistency. Based on the encoder-decoder framework, we propose using a pre-trained CNN to extract features of the original and compressed images, and making them similar. Thus the compressed images are discernible to subsequent tasks, and we name our method as Discernible Image Compression (DIC). In addition, the maximum mean discrepancy (MMD) is employed to minimize the difference between feature distributions. The resulting compression network can generate images with high image quality and preserve the consistent perception in the feature domain, so that these images can be well recognized by pre-trained machine learning models. Experiments on benchmarks demonstrate that images compressed by using the proposed method can also be well recognized by subsequent visual recognition and detection models. For instance, the mAP value of compressed images by DIC is about 0.6% higher than that of using compressed images by conventional methods.



### Class-Imbalanced Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.06815v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.06815v1)
- **Published**: 2020-02-17 07:48:47+00:00
- **Updated**: 2020-02-17 07:48:47+00:00
- **Authors**: Minsung Hyun, Jisoo Jeong, Nojun Kwak
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) has achieved great success in overcoming the difficulties of labeling and making full use of unlabeled data. However, SSL has a limited assumption that the numbers of samples in different classes are balanced, and many SSL algorithms show lower performance for the datasets with the imbalanced class distribution. In this paper, we introduce a task of class-imbalanced semi-supervised learning (CISSL), which refers to semi-supervised learning with class-imbalanced data. In doing so, we consider class imbalance in both labeled and unlabeled sets. First, we analyze existing SSL methods in imbalanced environments and examine how the class imbalance affects SSL methods. Then we propose Suppressed Consistency Loss (SCL), a regularization method robust to class imbalance. Our method shows better performance than the conventional methods in the CISSL environment. In particular, the more severe the class imbalance and the smaller the size of the labeled data, the better our method performs.



### On the Similarity of Deep Learning Representations Across Didactic and Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2002.06816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC, I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2002.06816v1)
- **Published**: 2020-02-17 07:49:20+00:00
- **Updated**: 2020-02-17 07:49:20+00:00
- **Authors**: Pamela K. Douglas, Farzad Vasheghani Farahani
- **Comment**: 2 figures
- **Journal**: Med NeurIPS 2019
- **Summary**: The increasing use of deep neural networks (DNNs) has motivated a parallel endeavor: the design of adversaries that profit from successful misclassifications. However, not all adversarial examples are crafted for malicious purposes. For example, real world systems often contain physical, temporal, and sampling variability across instrumentation. Adversarial examples in the wild may inadvertently prove deleterious for accurate predictive modeling. Conversely, naturally occurring covariance of image features may serve didactic purposes. Here, we studied the stability of deep learning representations for neuroimaging classification across didactic and adversarial conditions characteristic of MRI acquisition variability. We show that representational similarity and performance vary according to the frequency of adversarial examples in the input space.



### Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2002.06820v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06820v2)
- **Published**: 2020-02-17 08:07:19+00:00
- **Updated**: 2021-10-25 09:34:22+00:00
- **Authors**: Liang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, Fei Wu
- **Comment**: Accepted by AAAI2020. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR
- **Journal**: None
- **Summary**: Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text.



### DeepDualMapper: A Gated Fusion Network for Automatic Map Extraction using Aerial Images and Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2002.06832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06832v1)
- **Published**: 2020-02-17 08:33:46+00:00
- **Updated**: 2020-02-17 08:33:46+00:00
- **Authors**: Hao Wu, Hanyuan Zhang, Xinyu Zhang, Weiwei Sun, Baihua Zheng, Yuning Jiang
- **Comment**: 7 pages, AAAI 2020 accepted paper
- **Journal**: None
- **Summary**: Automatic map extraction is of great importance to urban computing and location-based services. Aerial image and GPS trajectory data refer to two different data sources that could be leveraged to generate the map, although they carry different types of information. Most previous works on data fusion between aerial images and data from auxiliary sensors do not fully utilize the information of both modalities and hence suffer from the issue of information loss. We propose a deep convolutional neural network called DeepDualMapper which fuses the aerial image and trajectory data in a more seamless manner to extract the digital map. We design a gated fusion module to explicitly control the information flows from both modalities in a complementary-aware manner. Moreover, we propose a novel densely supervised refinement decoder to generate the prediction in a coarse-to-fine way. Our comprehensive experiments demonstrate that DeepDualMapper can fuse the information of images and trajectories much more effectively than existing approaches, and is able to generate maps with higher accuracy.



### Stratified Rule-Aware Network for Abstract Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2002.06838v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06838v3)
- **Published**: 2020-02-17 08:44:05+00:00
- **Updated**: 2022-06-07 11:49:44+00:00
- **Authors**: Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, Shihao Bai
- **Comment**: AAAI 2021 paper. Code: https://github.com/husheng12345/SRAN
- **Journal**: None
- **Summary**: Abstract reasoning refers to the ability to analyze information, discover rules at an intangible level, and solve problems in innovative ways. Raven's Progressive Matrices (RPM) test is typically used to examine the capability of abstract reasoning. The subject is asked to identify the correct choice from the answer set to fill the missing panel at the bottom right of RPM (e.g., a 3$\times$3 matrix), following the underlying rules inside the matrix. Recent studies, taking advantage of Convolutional Neural Networks (CNNs), have achieved encouraging progress to accomplish the RPM test. However, they partly ignore necessary inductive biases of RPM solver, such as order sensitivity within each row/column and incremental rule induction. To address this problem, in this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the rule embeddings for two input sequences. Our SRAN learns multiple granularity rule embeddings at different levels, and incrementally integrates the stratified embedding flows through a gated fusion module. With the help of embeddings, a rule similarity metric is applied to guarantee that SRAN can not only be trained using a tuplet loss but also infer the best answer efficiently. We further point out the severe defects existing in the popular RAVEN dataset for RPM test, which prevent from the fair evaluation of the abstract reasoning ability. To fix the defects, we propose an answer set generation algorithm called Attribute Bisection Tree (ABT), forming an improved dataset named Impartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on both PGM and I-RAVEN datasets, showing that our SRAN outperforms the state-of-the-art models by a considerable margin.



### Large-scale biometry with interpretable neural network regression on UK Biobank body MRI
- **Arxiv ID**: http://arxiv.org/abs/2002.06862v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06862v3)
- **Published**: 2020-02-17 09:47:58+00:00
- **Updated**: 2020-10-09 08:59:16+00:00
- **Authors**: Taro Langner, Robin Strand, Håkan Ahlström, Joel Kullberg
- **Comment**: None
- **Journal**: None
- **Summary**: In a large-scale medical examination, the UK Biobank study has successfully imaged more than 32,000 volunteer participants with magnetic resonance imaging (MRI). Each scan is linked to extensive metadata, providing a comprehensive medical survey of imaged anatomy and related health states. Despite its potential for research, this vast amount of data presents a challenge to established methods of evaluation, which often rely on manual input. To date, the range of reference values for cardiovascular and metabolic risk factors is therefore incomplete. In this work, neural networks were trained for image-based regression to infer various biological metrics from the neck-to-knee body MRI automatically. The approach requires no manual intervention or direct access to reference segmentations for training. The examined fields span 64 variables derived from anthropometric measurements, dual-energy X-ray absorptiometry (DXA), atlas-based segmentations, and dedicated liver scans. With the ResNet50, the standardized framework achieves a close fit to the target values (median R^2 > 0.97) in cross-validation. Interpretation of aggregated saliency maps suggests that the network correctly targets specific body regions and limbs, and learned to emulate different modalities. On several body composition metrics, the quality of the predictions is within the range of variability observed between established gold standard techniques.



### Amplifying The Uncanny
- **Arxiv ID**: http://arxiv.org/abs/2002.06890v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06890v3)
- **Published**: 2020-02-17 11:12:39+00:00
- **Updated**: 2020-11-13 13:18:10+00:00
- **Authors**: Terence Broad, Frederic Fol Leymarie, Mick Grierson
- **Comment**: None
- **Journal**: Proceedings of the Eighth Conference on Proceedings of the Eighth
  Conference on Computation, Communication, Aesthetics & X, 2020
- **Summary**: Deep neural networks have become remarkably good at producing realistic deepfakes, images of people that (to the untrained eye) are indistinguishable from real images. Deepfakes are produced by algorithms that learn to distinguish between real and fake images and are optimised to generate samples that the system deems realistic. This paper, and the resulting series of artworks Being Foiled explore the aesthetic outcome of inverting this process, instead optimising the system to generate images that it predicts as being fake. This maximises the unlikelihood of the data and in turn, amplifies the uncanny nature of these machine hallucinations.



### Patient-Specific Finetuning of Deep Learning Models for Adaptive Radiotherapy in Prostate CT
- **Arxiv ID**: http://arxiv.org/abs/2002.06927v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06927v1)
- **Published**: 2020-02-17 12:53:37+00:00
- **Updated**: 2020-02-17 12:53:37+00:00
- **Authors**: Mohamed S. Elmahdy, Tanuj Ahuja, U. A. van der Heide, Marius Staring
- **Comment**: IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)
- **Journal**: None
- **Summary**: Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step in radiotherapy treatment planning. In an adaptive radiotherapy setting, updated contours need to be generated based on daily imaging. In this work, we leverage personalized anatomical knowledge accumulated over the treatment sessions, to improve the segmentation accuracy of a pre-trained Convolution Neural Network (CNN), for a specific patient. We investigate a transfer learning approach, fine-tuning the baseline CNN model to a specific patient, based on imaging acquired in earlier treatment fractions. The baseline CNN model is trained on a prostate CT dataset from one hospital of 379 patients. This model is then fine-tuned and tested on an independent dataset of another hospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate, seminal vesicles, bladder and rectum, the model fine-tuned on each specific patient achieved a Mean Surface Distance (MSD) of $1.64 \pm 0.43$ mm, $2.38 \pm 2.76$ mm, $2.30 \pm 0.96$ mm, and $1.24 \pm 0.89$ mm, respectively, which was significantly better than the baseline model. The proposed personalized model adaptation is therefore very promising for clinical implementation in the context of adaptive radiotherapy of prostate cancer.



### Neural arbitrary style transfer for portrait images using the attention mechanism
- **Arxiv ID**: http://arxiv.org/abs/2002.07643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07643v1)
- **Published**: 2020-02-17 13:59:58+00:00
- **Updated**: 2020-02-17 13:59:58+00:00
- **Authors**: S. A. Berezin, V. M. Volkova
- **Comment**: in Russian
- **Journal**: None
- **Summary**: Arbitrary style transfer is the task of synthesis of an image that has never been seen before, using two given images: content image and style image. The content image forms the structure, the basic geometric lines and shapes of the resulting image, while the style image sets the color and texture of the result. The word "arbitrary" in this context means the absence of any one pre-learned style. So, for example, convolutional neural networks capable of transferring a new style only after training or retraining on a new amount of data are not con-sidered to solve such a problem, while networks based on the attention mech-anism that are capable of performing such a transformation without retraining - yes. An original image can be, for example, a photograph, and a style image can be a painting of a famous artist. The resulting image in this case will be the scene depicted in the original photograph, made in the stylie of this picture. Recent arbitrary style transfer algorithms make it possible to achieve good re-sults in this task, however, in processing portrait images of people, the result of such algorithms is either unacceptable due to excessive distortion of facial features, or weakly expressed, not bearing the characteristic features of a style image. In this paper, we consider an approach to solving this problem using the combined architecture of deep neural networks with a attention mechanism that transfers style based on the contents of a particular image segment: with a clear predominance of style over the form for the background part of the im-age, and with the prevalence of content over the form in the image part con-taining directly the image of a person.



### Learning Architectures for Binary Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.06963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06963v2)
- **Published**: 2020-02-17 14:06:45+00:00
- **Updated**: 2020-04-10 09:08:41+00:00
- **Authors**: Dahyun Kim, Kunal Pratap Singh, Jonghyun Choi
- **Comment**: The manuscript was changed to a one-column format along with minor
  modifications to the content
- **Journal**: None
- **Summary**: Backbone architectures of most binary networks are well-known floating point architectures such as the ResNet family. Questioning that the architectures designed for floating point networks would not be the best for binary networks, we propose to search architectures for binary networks (BNAS) by defining a new search space for binary architectures and a novel search objective. Specifically, based on the cell based search method, we define the new search space of binary layer types, design a new cell template, and rediscover the utility of and propose to use the Zeroise layer instead of using it as a placeholder. The novel search objective diversifies early search to learn better performing binary architectures. We show that our proposed method searches architectures with stable training curves despite the quantization error inherent in binary networks. Quantitative analyses demonstrate that our searched architectures outperform the architectures used in state-of-the-art binary networks and outperform or perform on par with state-of-the-art binary networks that employ various techniques other than architectural changes.



### Lake Ice Detection from Sentinel-1 SAR with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.07040v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2002.07040v2)
- **Published**: 2020-02-17 16:31:41+00:00
- **Updated**: 2020-05-06 23:01:15+00:00
- **Authors**: Manu Tom, Roberto Aguilar, Pascal Imhof, Silvan Leinss, Emmanuel Baltsavias, Konrad Schindler
- **Comment**: Accepted for ISPRS Congress 2020, Nice, France
- **Journal**: None
- **Summary**: Lake ice, as part of the Essential Climate Variable (ECV) lakes, is an important indicator to monitor climate change and global warming. The spatio-temporal extent of lake ice cover, along with the timings of key phenological events such as freeze-up and break-up, provide important cues about the local and global climate. We present a lake ice monitoring system based on the automatic analysis of Sentinel-1 Synthetic Aperture Radar (SAR) data with a deep neural network. In previous studies that used optical satellite imagery for lake ice monitoring, frequent cloud cover was a main limiting factor, which we overcome thanks to the ability of microwave sensors to penetrate clouds and observe the lakes regardless of the weather and illumination conditions. We cast ice detection as a two class (frozen, non-frozen) semantic segmentation problem and solve it using a state-of-the-art deep convolutional network (CNN). We report results on two winters ( 2016 - 17 and 2017 - 18 ) and three alpine lakes in Switzerland. The proposed model reaches mean Intersection-over-Union (mIoU) scores >90% on average, and >84% even for the most difficult lake. Additionally, we perform cross-validation tests and show that our algorithm generalises well across unseen lakes and winters.



### GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/2002.07088v6
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.07088v6)
- **Published**: 2020-02-17 17:24:14+00:00
- **Updated**: 2022-02-28 08:24:00+00:00
- **Authors**: Ryan Feng, Neal Mangaokar, Jiefeng Chen, Earlence Fernandes, Somesh Jha, Atul Prakash
- **Comment**: IEEE European Symposium on Security and Privacy 2022 (EuroS&P 2022)
- **Journal**: None
- **Summary**: This paper investigates an adversary's ease of attack in generating adversarial examples for real-world scenarios. We address three key requirements for practical attacks for the real-world: 1) automatically constraining the size and shape of the attack so it can be applied with stickers, 2) transform-robustness, i.e., robustness of a attack to environmental physical variations such as viewpoint and lighting changes, and 3) supporting attacks in not only white-box, but also black-box hard-label scenarios, so that the adversary can attack proprietary models. In this work, we propose GRAPHITE, an efficient and general framework for generating attacks that satisfy the above three key requirements. GRAPHITE takes advantage of transform-robustness, a metric based on expectation over transforms (EoT), to automatically generate small masks and optimize with gradient-free optimization. GRAPHITE is also flexible as it can easily trade-off transform-robustness, perturbation size, and query count in black-box settings. On a GTSRB model in a hard-label black-box setting, we are able to find attacks on all possible 1,806 victim-target class pairs with averages of 77.8% transform-robustness, perturbation size of 16.63% of the victim images, and 126K queries per pair. For digital-only attacks where achieving transform-robustness is not a requirement, GRAPHITE is able to find successful small-patch attacks with an average of only 566 queries for 92.2% of victim-target pairs. GRAPHITE is also able to find successful attacks using perturbations that modify small areas of the input image against PatchGuard, a recently proposed defense against patch-based attacks.



### 4D Semantic Cardiac Magnetic Resonance Image Synthesis on XCAT Anatomical Model
- **Arxiv ID**: http://arxiv.org/abs/2002.07089v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07089v3)
- **Published**: 2020-02-17 17:25:07+00:00
- **Updated**: 2020-05-20 14:01:13+00:00
- **Authors**: Samaneh Abbasi-Sureshjani, Sina Amirrajab, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer
- **Comment**: Accepted to MIDL 2020
- **Journal**: None
- **Summary**: We propose a hybrid controllable image generation method to synthesize anatomically meaningful 3D+t labeled Cardiac Magnetic Resonance (CMR) images. Our hybrid method takes the mechanistic 4D eXtended CArdiac Torso (XCAT) heart model as the anatomical ground truth and synthesizes CMR images via a data-driven Generative Adversarial Network (GAN). We employ the state-of-the-art SPatially Adaptive De-normalization (SPADE) technique for conditional image synthesis to preserve the semantic spatial information of ground truth anatomy. Using the parameterized motion model of the XCAT heart, we generate labels for 25 time frames of the heart for one cardiac cycle at 18 locations for the short axis view. Subsequently, realistic images are generated from these labels, with modality-specific features that are learned from real CMR image data. We demonstrate that style transfer from another cardiac image can be accomplished by using a style encoder network. Due to the flexibility of XCAT in creating new heart models, this approach can result in a realistic virtual population to address different challenges the medical image analysis research community is facing such as expensive data collection. Our proposed method has a great potential to synthesize 4D controllable CMR images with annotations and adaptable styles to be used in various supervised multi-site, multi-vendor applications in medical image analysis.



### Pores for thought: The use of generative adversarial networks for the stochastic reconstruction of 3D multi-phase electrode microstructures with periodic boundaries
- **Arxiv ID**: http://arxiv.org/abs/2003.11632v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, 68T10, 68T45, 92E99, 82D30, J.2; I.4.5; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2003.11632v2)
- **Published**: 2020-02-17 17:38:27+00:00
- **Updated**: 2020-05-04 21:37:20+00:00
- **Authors**: Andrea Gayon-Lombardo, Lukas Mosser, Nigel P. Brandon, Samuel J. Cooper
- **Comment**: 37 pages, 10 figures
- **Journal**: None
- **Summary**: The generation of multiphase porous electrode microstructures is a critical step in the optimisation of electrochemical energy storage devices. This work implements a deep convolutional generative adversarial network (DC-GAN) for generating realistic n-phase microstructural data. The same network architecture is successfully applied to two very different three-phase microstructures: A lithium-ion battery cathode and a solid oxide fuel cell anode. A comparison between the real and synthetic data is performed in terms of the morphological properties (volume fraction, specific surface area, triple-phase boundary) and transport properties (relative diffusivity), as well as the two-point correlation function. The results show excellent agreement between for datasets and they are also visually indistinguishable. By modifying the input to the generator, we show that it is possible to generate microstructure with periodic boundaries in all three directions. This has the potential to significantly reduce the simulated volume required to be considered representative and therefore massively reduce the computational cost of the electrochemical simulations necessary to predict the performance of a particular microstructure during optimisation.



### Seeing Around Corners with Edge-Resolved Transient Imaging
- **Arxiv ID**: http://arxiv.org/abs/2002.07118v1
- **DOI**: 10.1038/s41467-020-19727-4
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2002.07118v1)
- **Published**: 2020-02-17 18:33:48+00:00
- **Updated**: 2020-02-17 18:33:48+00:00
- **Authors**: Joshua Rapp, Charles Saunders, Julián Tachella, John Murray-Bruce, Yoann Altmann, Jean-Yves Tourneret, Stephen McLaughlin, Robin M. A. Dawson, Franco N. C. Wong, Vivek K Goyal
- **Comment**: Includes manuscript (14 pages) and supplement (24 pages)
- **Journal**: None
- **Summary**: Non-line-of-sight (NLOS) imaging is a rapidly growing field seeking to form images of objects outside the field of view, with potential applications in search and rescue, reconnaissance, and even medical imaging. The critical challenge of NLOS imaging is that diffuse reflections scatter light in all directions, resulting in weak signals and a loss of directional information. To address this problem, we propose a method for seeing around corners that derives angular resolution from vertical edges and longitudinal resolution from the temporal response to a pulsed light source. We introduce an acquisition strategy, scene response model, and reconstruction algorithm that enable the formation of 2.5-dimensional representations -- a plan view plus heights -- and a 180$^{\circ}$ field of view (FOV) for large-scale scenes. Our experiments demonstrate accurate reconstructions of hidden rooms up to 3 meters in each dimension.



### Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations
- **Arxiv ID**: http://arxiv.org/abs/2002.07136v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.07136v2)
- **Published**: 2020-02-17 18:54:37+00:00
- **Updated**: 2020-05-29 03:10:27+00:00
- **Authors**: Yichi Zhang, Ritchie Zhao, Weizhe Hua, Nayun Xu, G. Edward Suh, Zhiru Zhang
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Our experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4$\times$ less compute on ImageNet. PG furthermore applies to RNNs. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7$\times$ computational cost reduction on LSTM on the Penn Tree Bank dataset. Code is available at: https://github.com/cornell-zhang/dnn-gating



### Multilinear Compressive Learning with Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2002.07203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07203v1)
- **Published**: 2020-02-17 19:06:05+00:00
- **Updated**: 2020-02-17 19:06:05+00:00
- **Authors**: Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: 15 pages, 1 figure, 7 tables
- **Journal**: None
- **Summary**: The recently proposed Multilinear Compressive Learning (MCL) framework combines Multilinear Compressive Sensing and Machine Learning into an end-to-end system that takes into account the multidimensional structure of the signals when designing the sensing and feature synthesis components. The key idea behind MCL is the assumption of the existence of a tensor subspace which can capture the essential features from the signal for the downstream learning task. Thus, the ability to find such a discriminative tensor subspace and optimize the system to project the signals onto that data manifold plays an important role in Multilinear Compressive Learning. In this paper, we propose a novel solution to address both of the aforementioned requirements, i.e., How to find those tensor subspaces in which the signals of interest are highly separable? and How to optimize the sensing and feature synthesis components to transform the original signals to the data manifold found in the first question? In our proposal, the discovery of a high-quality data manifold is conducted by training a nonlinear compressive learning system on the inference task. Its knowledge of the data manifold of interest is then progressively transferred to the MCL components via multi-stage supervised training with the supervisory information encoding how the compressed measurements, the synthesized features, and the predictions should be like. The proposed knowledge transfer algorithm also comes with a semi-supervised adaption that enables compressive learning models to utilize unlabeled data effectively. Extensive experiments demonstrate that the proposed knowledge transfer method can effectively train MCL models to compressively sense and synthesize better features for the learning tasks with improved performances, especially when the complexity of the learning task increases.



### Evolutionary Optimization of Deep Learning Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/2002.07224v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.07224v2)
- **Published**: 2020-02-17 19:54:26+00:00
- **Updated**: 2020-04-11 15:53:12+00:00
- **Authors**: Garrett Bingham, William Macke, Risto Miikkulainen
- **Comment**: 8 pages; 9 figures/tables; GECCO 2020
- **Journal**: None
- **Summary**: The choice of activation function can have a large effect on the performance of a neural network. While there have been some attempts to hand-engineer novel activation functions, the Rectified Linear Unit (ReLU) remains the most commonly-used in practice. This paper shows that evolutionary algorithms can discover novel activation functions that outperform ReLU. A tree-based search space of candidate activation functions is defined and explored with mutation, crossover, and exhaustive search. Experiments on training wide residual networks on the CIFAR-10 and CIFAR-100 image datasets show that this approach is effective. Replacing ReLU with evolved activation functions results in statistically significant increases in network accuracy. Optimal performance is achieved when evolution is allowed to customize activation functions to a particular task; however, these novel activation functions are shown to generalize, achieving high performance across tasks. Evolutionary optimization of activation functions is therefore a promising new dimension of metalearning in neural networks.



### Dual-Attention GAN for Large-Pose Face Frontalization
- **Arxiv ID**: http://arxiv.org/abs/2002.07227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07227v1)
- **Published**: 2020-02-17 20:00:56+00:00
- **Updated**: 2020-02-17 20:00:56+00:00
- **Authors**: Yu Yin, Songyao Jiang, Joseph P. Robinson, Yun Fu
- **Comment**: The 15th IEEE International Conference on Automatic Face and Gesture
  Recognition (FG 2020)
- **Journal**: None
- **Summary**: Face frontalization provides an effective and efficient way for face data augmentation and further improves the face recognition performance in extreme pose scenario. Despite recent advances in deep learning-based face synthesis approaches, this problem is still challenging due to significant pose and illumination discrepancy. In this paper, we present a novel Dual-Attention Generative Adversarial Network (DA-GAN) for photo-realistic face frontalization by capturing both contextual dependencies and local consistency during GAN training. Specifically, a self-attention-based generator is introduced to integrate local features with their long-range dependencies yielding better feature representations, and hence generate faces that preserve identities better, especially for larger pose angles. Moreover, a novel face-attention-based discriminator is applied to emphasize local features of face regions, and hence reinforce the realism of synthetic frontal faces. Guided by semantic segmentation, four independent discriminators are used to distinguish between different aspects of a face (\ie skin, keypoints, hairline, and frontalized face). By introducing these two complementary attention mechanisms in generator and discriminator separately, we can learn a richer feature representation and generate identity preserving inference of frontal views with much finer details (i.e., more accurate facial appearance and textures) comparing to the state-of-the-art. Quantitative and qualitative experimental results demonstrate the effectiveness and efficiency of our DA-GAN approach.



### 3D Gated Recurrent Fusion for Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2002.07269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07269v1)
- **Published**: 2020-02-17 21:45:43+00:00
- **Updated**: 2020-02-17 21:45:43+00:00
- **Authors**: Yu Liu, Jie Li, Qingsen Yan, Xia Yuan, Chunxia Zhao, Ian Reid, Cesar Cadena
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: This paper tackles the problem of data fusion in the semantic scene completion (SSC) task, which can simultaneously deal with semantic labeling and scene completion. RGB images contain texture details of the object(s) which are vital for semantic scene understanding. Meanwhile, depth images capture geometric clues of high relevance for shape completion. Using both RGB and depth images can further boost the accuracy of SSC over employing one modality in isolation. We propose a 3D gated recurrent fusion network (GRFNet), which learns to adaptively select and fuse the relevant information from depth and RGB by making use of the gate and memory modules. Based on the single-stage fusion, we further propose a multi-stage fusion strategy, which could model the correlations among different stages within the network. Extensive experiments on two benchmark datasets demonstrate the superior performance and the effectiveness of the proposed GRFNet for data fusion in SSC. Code will be made available.



