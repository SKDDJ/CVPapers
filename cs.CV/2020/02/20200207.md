# Arxiv Papers in cs.CV on 2020-02-07
### Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2002.02582v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02582v1)
- **Published**: 2020-02-07 01:48:13+00:00
- **Updated**: 2020-02-07 01:48:13+00:00
- **Authors**: Mohammad Hashir, Hadrien Bertrand, Joseph Paul Cohen
- **Comment**: Under review at MIDL 2020
- **Journal**: None
- **Summary**: Most deep learning models in chest X-ray prediction utilize the posteroanterior (PA) view due to the lack of other views available. PadChest is a large-scale chest X-ray dataset that has almost 200 labels and multiple views available. In this work, we use PadChest to explore multiple approaches to merging the PA and lateral views for predicting the radiological labels associated with the X-ray image. We find that different methods of merging the model utilize the lateral view differently. We also find that including the lateral view increases performance for 32 labels in the dataset, while being neutral for the others. The increase in overall performance is comparable to the one obtained by using only the PA view with twice the amount of patients in the training set.



### CIFAR-10 Image Classification Using Feature Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2002.03846v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03846v2)
- **Published**: 2020-02-07 01:53:46+00:00
- **Updated**: 2020-02-27 22:33:53+00:00
- **Authors**: Felipe O. Giuste, Juan C. Vizcarra
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification requires the generation of features capable of detecting image patterns informative of group identity. The objective of this study was to classify images from the public CIFAR-10 image dataset by leveraging combinations of disparate image feature sources from both manual and deep learning approaches. Histogram of oriented gradients (HOG) and pixel intensities successfully inform classification (53% and 59% classification accuracy, respectively), yet there is much room for improvement. VGG16 with ImageNet trained weights and a CIFAR-10 optimized model (CIFAR-VGG) further improve upon image classification (60% and 93.43% accuracy, respectively). We further improved classification by utilizing transfer learning to re-establish optimal network weights for VGG16 (TL-VGG) and Inception ResNet v2 (TL-Inception) resulting in significant performance increases (85% and 90.74%, respectively), yet fail to surpass CIFAR-VGG. We hypothesized that if each generated feature set obtained some unique insight into the classification problem, then combining these features would result in greater classification accuracy, surpassing that of CIFAR-VGG. Upon selection of the top 1000 principal components from TL-VGG, TL-Inception, HOG, pixel intensities, and CIFAR-VGG, we achieved testing accuracy of 94.6%, lending support to our hypothesis.



### Learning Hyperspectral Feature Extraction and Classification with ResNeXt Network
- **Arxiv ID**: http://arxiv.org/abs/2002.02585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02585v1)
- **Published**: 2020-02-07 01:54:15+00:00
- **Updated**: 2020-02-07 01:54:15+00:00
- **Authors**: Divinah Nyasaka, Jing Wang, Haron Tinega
- **Comment**: None
- **Journal**: None
- **Summary**: The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset.



### Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.02589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02589v2)
- **Published**: 2020-02-07 02:25:11+00:00
- **Updated**: 2020-02-25 10:03:50+00:00
- **Authors**: Ziqing Yang, Shoudong Han, Jun Zhao
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Graph convolutional network (GCN) is now an effective tool to deal with non-Euclidean data, such as social networks in social behavior analysis, molecular structure analysis in the field of chemistry, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in GCN to extract nodes' feature, and some improvements of it have reached promising performance theoretically and experimentally. However, there is limited research about how exactly different data types and graph structures influence the performance of these kernels. Most existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not reveals the internal reasons. In this paper, we started from theoretical analysis of the spectral graph and studied the properties of existing graph convolutional kernels. While taking some designed datasets with specific parameters into consideration, we revealed the self-smoothing phenomenon of convolutional kernels. After that, we proposed the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark dataset where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets.



### Object-Adaptive LSTM Network for Real-time Visual Tracking with Adversarial Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.02598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02598v1)
- **Published**: 2020-02-07 03:06:07+00:00
- **Updated**: 2020-02-07 03:06:07+00:00
- **Authors**: Yihan Du, Yan Yan, Si Chen, Yang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking.



### Adaptive Deep Metric Embeddings for Person Re-Identification under Occlusions
- **Arxiv ID**: http://arxiv.org/abs/2002.02603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02603v1)
- **Published**: 2020-02-07 03:18:10+00:00
- **Updated**: 2020-02-07 03:18:10+00:00
- **Authors**: Wanxiang Yang, Yan Yan, Si Chen
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Person re-identification (ReID) under occlusions is a challenging problem in video surveillance. Most of existing person ReID methods take advantage of local features to deal with occlusions. However, these methods usually independently extract features from the local regions of an image without considering the relationship among different local regions. In this paper, we propose a novel person ReID method, which learns the spatial dependencies between the local regions and extracts the discriminative feature representation of the pedestrian image based on Long Short-Term Memory (LSTM), dealing with the problem of occlusions. In particular, we propose a novel loss (termed the adaptive nearest neighbor loss) based on the classification uncertainty to effectively reduce intra-class variations while enlarging inter-class differences within the adaptive neighborhood of the sample. The proposed loss enables the deep neural network to adaptively learn discriminative metric embeddings, which significantly improve the generalization capability of recognizing unseen person identities. Extensive comparative evaluations on challenging person ReID datasets demonstrate the significantly improved performance of the proposed method compared with several state-of-the-art methods.



### Image Fine-grained Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2002.02609v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.02609v2)
- **Published**: 2020-02-07 03:45:25+00:00
- **Updated**: 2020-10-04 03:52:51+00:00
- **Authors**: Zheng Hui, Jie Li, Xiumei Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting techniques have shown promising improvement with the assistance of generative adversarial networks (GANs) recently. However, most of them often suffered from completed results with unreasonable structure or blurriness. To mitigate this problem, in this paper, we present a one-stage model that utilizes dense combinations of dilated convolutions to obtain larger and more effective receptive fields. Benefited from the property of this network, we can more easily recover large regions in an incomplete image. To better train this efficient generator, except for frequently-used VGG feature matching loss, we design a novel self-guided regression loss for concentrating on uncertain areas and enhancing the semantic details. Besides, we devise a geometrical alignment constraint item to compensate for the pixel-based distance between prediction features and ground-truth ones. We also employ a discriminator with local and global branches to ensure local-global contents consistency. To further improve the quality of generated images, discriminator feature matching on the local branch is introduced, which dynamically minimizes the similarity of intermediate features between synthetic and ground-truth patches. Extensive experiments on several public datasets demonstrate that our approach outperforms current state-of-the-art methods. Code is available at https://github.com/Zheng222/DMFN.



### Visual search over billions of aerial and satellite images
- **Arxiv ID**: http://arxiv.org/abs/2002.02624v1
- **DOI**: 10.1016/j.cviu.2019.07.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02624v1)
- **Published**: 2020-02-07 04:59:50+00:00
- **Updated**: 2020-02-07 04:59:50+00:00
- **Authors**: Ryan Keisler, Samuel W. Skillman, Sunny Gonnabathula, Justin Poehnelt, Xander Rudelis, Michael S. Warren
- **Comment**: None
- **Journal**: None
- **Summary**: We present a system for performing visual search over billions of aerial and satellite images. The purpose of visual search is to find images that are visually similar to a query image. We define visual similarity using 512 abstract visual features generated by a convolutional neural network that has been trained on aerial and satellite imagery. The features are converted to binary values to reduce data and compute requirements. We employ a hash-based search using Bigtable, a scalable database service from Google Cloud. Searching the continental United States at 1-meter pixel resolution, corresponding to approximately 2 billion images, takes approximately 0.1 seconds. This system enables real-time visual search over the surface of the earth, and an interactive demo is available at https://search.descarteslabs.com.



### SideInfNet: A Deep Neural Network for Semi-Automatic Semantic Segmentation with Side Information
- **Arxiv ID**: http://arxiv.org/abs/2002.02634v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02634v4)
- **Published**: 2020-02-07 06:10:54+00:00
- **Updated**: 2020-07-17 12:59:56+00:00
- **Authors**: Jing Yu Koh, Duc Thanh Nguyen, Quang-Trung Truong, Sai-Kit Yeung, Alexander Binder
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Fully-automatic execution is the ultimate goal for many Computer Vision applications. However, this objective is not always realistic in tasks associated with high failure costs, such as medical applications. For these tasks, semi-automatic methods allowing minimal effort from users to guide computer algorithms are often preferred due to desirable accuracy and performance. Inspired by the practicality and applicability of the semi-automatic approach, this paper proposes a novel deep neural network architecture, namely SideInfNet that effectively integrates features learnt from images with side information extracted from user annotations. To evaluate our method, we applied the proposed network to three semantic segmentation tasks and conducted extensive experiments on benchmark datasets. Experimental results and comparison with prior work have verified the superiority of our model, suggesting the generality and effectiveness of the model in semi-automatic semantic segmentation.



### Statistical Outlier Identification in Multi-robot Visual SLAM using Expectation Maximization
- **Arxiv ID**: http://arxiv.org/abs/2002.02638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02638v1)
- **Published**: 2020-02-07 06:34:44+00:00
- **Updated**: 2020-02-07 06:34:44+00:00
- **Authors**: Arman Karimian, Ziqi Yang, Roberto Tron
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel and distributed method for detecting inter-map loop closure outliers in simultaneous localization and mapping (SLAM). The proposed algorithm does not rely on a good initialization and can handle more than two maps at a time. In multi-robot SLAM applications, maps made by different agents have nonidentical spatial frames of reference which makes initialization very difficult in the presence of outliers. This paper presents a probabilistic approach for detecting incorrect orientation measurements prior to pose graph optimization by checking the geometric consistency of rotation measurements. Expectation-Maximization is used to fine-tune the model parameters. As ancillary contributions, a new approximate discrete inference procedure is presented which uses evidence on loops in a graph and is based on optimization (Alternate Direction Method of Multipliers). This method yields superior results compared to Belief Propagation and has convergence guarantees. Simulation and experimental results are presented that evaluate the performance of the outlier detection method and the inference algorithm on synthetic and real-world data.



### Exploiting Temporal Coherence for Multi-modal Video Categorization
- **Arxiv ID**: http://arxiv.org/abs/2002.03844v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03844v2)
- **Published**: 2020-02-07 06:42:12+00:00
- **Updated**: 2020-06-06 00:17:11+00:00
- **Authors**: Palash Goyal, Saurabh Sahu, Shalini Ghosh, Chul Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal ML models can process data in multiple modalities (e.g., video, images, audio, text) and are useful for video content analysis in a variety of problems (e.g., object detection, scene understanding). In this paper, we focus on the problem of video categorization by using a multimodal approach. We have developed a novel temporal coherence-based regularization approach, which applies to different types of models (e.g., RNN, NetVLAD, Transformer). We demonstrate through experiments how our proposed multimodal video categorization models with temporal coherence out-perform strong state-of-the-art baseline models.



### Learning Class Regularized Features for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.02651v1
- **DOI**: 10.3390/app10186241
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02651v1)
- **Published**: 2020-02-07 07:27:49+00:00
- **Updated**: 2020-02-07 07:27:49+00:00
- **Authors**: Alexandros Stergiou, Ronald Poppe, Remco C. Veltkamp
- **Comment**: None
- **Journal**: None
- **Summary**: Training Deep Convolutional Neural Networks (CNNs) is based on the notion of using multiple kernels and non-linearities in their subsequent activations to extract useful features. The kernels are used as general feature extractors without specific correspondence to the target class. As a result, the extracted features do not correspond to specific classes. Subtle differences between similar classes are modeled in the same way as large differences between dissimilar classes. To overcome the class-agnostic use of kernels in CNNs, we introduce a novel method named Class Regularization that performs class-based regularization of layer activations. We demonstrate that this not only improves feature search during training, but also allows an explicit assignment of features per class during each stage of the feature extraction process. We show that using Class Regularization blocks in state-of-the-art CNN architectures for action recognition leads to systematic improvement gains of 1.8%, 1.2% and 1.4% on the Kinetics, UCF-101 and HMDB-51 datasets, respectively.



### Optimization of Structural Similarity in Mathematical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2002.02657v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2002.02657v1)
- **Published**: 2020-02-07 07:46:31+00:00
- **Updated**: 2020-02-07 07:46:31+00:00
- **Authors**: D. Otero, D. La Torre, O. Michailovich, E. R. Vrscay
- **Comment**: None
- **Journal**: None
- **Summary**: It is now generally accepted that Euclidean-based metrics may not always adequately represent the subjective judgement of a human observer. As a result, many image processing methodologies have been recently extended to take advantage of alternative visual quality measures, the most prominent of which is the Structural Similarity Index Measure (SSIM). The superiority of the latter over Euclidean-based metrics have been demonstrated in several studies. However, being focused on specific applications, the findings of such studies often lack generality which, if otherwise acknowledged, could have provided a useful guidance for further development of SSIM-based image processing algorithms. Accordingly, instead of focusing on a particular image processing task, in this paper, we introduce a general framework that encompasses a wide range of imaging applications in which the SSIM can be employed as a fidelity measure. Subsequently, we show how the framework can be used to cast some standard as well as original imaging tasks into optimization problems, followed by a discussion of a number of novel numerical strategies for their solution.



### Deep Robust Multilevel Semantic Cross-Modal Hashing
- **Arxiv ID**: http://arxiv.org/abs/2002.02698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02698v2)
- **Published**: 2020-02-07 10:08:21+00:00
- **Updated**: 2020-10-06 09:32:31+00:00
- **Authors**: Ge Song, Jun Zhao, Xiaoyang Tan
- **Comment**: 11 pages, 9 figures, submitted to a journal
- **Journal**: None
- **Summary**: Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing (RMSH) for more accurate cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance.



### Iterative Label Improvement: Robust Training by Confidence Based Filtering and Dataset Partitioning
- **Arxiv ID**: http://arxiv.org/abs/2002.02705v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02705v3)
- **Published**: 2020-02-07 10:42:26+00:00
- **Updated**: 2020-07-17 10:13:54+00:00
- **Authors**: Christian Haase-Schütz, Rainer Stal, Heinz Hertlein, Bernhard Sick
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art, high capacity deep neural networks not only require large amounts of labelled training data, they are also highly susceptible to label errors in this data, typically resulting in large efforts and costs and therefore limiting the applicability of deep learning. To alleviate this issue, we propose a novel meta training and labelling scheme that is able to use inexpensive unlabelled data by taking advantage of the generalization power of deep neural networks. We show experimentally that by solely relying on one network architecture and our proposed scheme of iterative training and prediction steps, both label quality and resulting model accuracy can be improved significantly. Our method achieves state-of-the-art results, while being architecture agnostic and therefore broadly applicable. Compared to other methods dealing with erroneous labels, our approach does neither require another network to be trained, nor does it necessarily need an additional, highly accurate reference label set. Instead of removing samples from a labelled set, our technique uses additional sensor data without the need for manual labelling. Furthermore, our approach can be used for semi-supervised learning.



### FourierNet: Compact mask representation for instance segmentation using differentiable shape decoders
- **Arxiv ID**: http://arxiv.org/abs/2002.02709v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02709v2)
- **Published**: 2020-02-07 10:54:54+00:00
- **Updated**: 2020-10-19 11:46:58+00:00
- **Authors**: Hamd ul Moqeet Riaz, Nuri Benbarka, Andreas Zell
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: We present FourierNet, a single shot, anchor-free, fully convolutional instance segmentation method that predicts a shape vector. Consequently, this shape vector is converted into the masks' contour points using a fast numerical transform. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which manages the automatic weight balancing of the shape vector's coefficients. We used the Fourier series as a shape encoder because of its coefficient interpretability and fast implementation. FourierNet shows promising results compared to polygon representation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At lower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It reaches 23.3 mAP using just eight parameters to represent the mask (note that at least four parameters are needed for bounding box prediction only). Qualitative analysis shows that suppressing a reasonable proportion of higher frequencies of Fourier series, still generates meaningful masks. These results validate our understanding that lower frequency components hold higher information for the segmentation task, and therefore, we can achieve a compressed representation. Code is available at: github.com/cogsys-tuebingen/FourierNet.



### Attentive Group Equivariant Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.03830v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03830v3)
- **Published**: 2020-02-07 14:06:24+00:00
- **Updated**: 2020-06-30 07:41:35+00:00
- **Authors**: David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn
- **Comment**: Proceedings of the 37th International Conference on Machine Learning
  (ICML), 2020
- **Journal**: None
- **Summary**: Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.



### Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding Network
- **Arxiv ID**: http://arxiv.org/abs/2002.02814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02814v1)
- **Published**: 2020-02-07 14:42:26+00:00
- **Updated**: 2020-02-07 14:42:26+00:00
- **Authors**: Zhe Ma, Jianfeng Dong, Yao Zhang, Zhongzi Long, Yuan He, Hui Xue, Shouling Ji
- **Comment**: 16 pages, 13 figutes. Accepted by AAAI 2020. Code and data are
  available at https://github.com/Maryeon/asen
- **Journal**: None
- **Summary**: This paper strives to learn fine-grained fashion similarity. In this similarity paradigm, one should pay more attention to the similarity in terms of a specific design/attribute among fashion items, which has potential values in many fashion related applications such as fashion copyright protection. To this end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn multiple attribute-specific embeddings in an end-to-end manner, thus measure the fine-grained similarity in the corresponding space. With two attention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel Attention, ASEN is able to locate the related regions and capture the essential patterns under the guidance of the specified attribute, thus make the learned attribute-specific embeddings better reflect the fine-grained similarity. Extensive experiments on four fashion-related datasets show the effectiveness of ASEN for fine-grained fashion similarity learning and its potential for fashion reranking.



### Switchable Precision Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.02815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02815v1)
- **Published**: 2020-02-07 14:43:44+00:00
- **Updated**: 2020-02-07 14:43:44+00:00
- **Authors**: Luis Guerra, Bohan Zhuang, Ian Reid, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Instantaneous and on demand accuracy-efficiency trade-off has been recently explored in the context of neural networks slimming. In this paper, we propose a flexible quantization strategy, termed Switchable Precision neural Networks (SP-Nets), to train a shared network capable of operating at multiple quantization levels. At runtime, the network can adjust its precision on the fly according to instant memory, latency, power consumption and accuracy demands. For example, by constraining the network weights to 1-bit with switchable precision activations, our shared network spans from BinaryConnect to Binarized Neural Network, allowing to perform dot-products using only summations or bit operations. In addition, a self-distillation scheme is proposed to increase the performance of the quantized switches. We tested our approach with three different quantizers and demonstrate the performance of SP-Nets against independently trained quantized models in classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures.



### Input Dropout for Spatially Aligned Modalities
- **Arxiv ID**: http://arxiv.org/abs/2002.02852v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02852v2)
- **Published**: 2020-02-07 15:39:34+00:00
- **Updated**: 2020-05-21 11:35:17+00:00
- **Authors**: Sébastien de Blois, Mathieu Garon, Christian Gagné, Jean-François Lalonde
- **Comment**: Accepted in ICIP 2020. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Computer vision datasets containing multiple modalities such as color, depth, and thermal properties are now commonly accessible and useful for solving a wide array of challenging tasks. However, deploying multi-sensor heads is not possible in many scenarios. As such many practical solutions tend to be based on simpler sensors, mostly for cost, simplicity and robustness considerations. In this work, we propose a training methodology to take advantage of these additional modalities available in datasets, even if they are not available at test time. By assuming that the modalities have a strong spatial correlation, we propose Input Dropout, a simple technique that consists in stochastic hiding of one or many input modalities at training time, while using only the canonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout trivially combines with existing deep convolutional architectures, and improves their performance on a wide range of computer vision tasks such as dehazing, 6-DOF object tracking, pedestrian detection and object classification.



### An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2002.02857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02857v1)
- **Published**: 2020-02-07 15:47:55+00:00
- **Updated**: 2020-02-07 15:47:55+00:00
- **Authors**: Peter Hirsch, Dagmar Kainmueller
- **Comment**: code available at: https://github.com/Kainmueller-Lab/aux_cpv_loss
- **Journal**: None
- **Summary**: Segmentation of cell nuclei in microscopy images is a prevalent necessity in cell biology. Especially for three-dimensional datasets, manual segmentation is prohibitively time-consuming, motivating the need for automated methods. Learning-based methods trained on pixel-wise ground-truth segmentations have been shown to yield state-of-the-art results on 2d benchmark image data of nuclei, yet a respective benchmark is missing for 3d image data. In this work, we perform a comparative evaluation of nuclei segmentation algorithms on a database of manually segmented 3d light microscopy volumes. We propose a novel learning strategy that boosts segmentation accuracy by means of a simple auxiliary task, thereby robustly outperforming each of our baselines. Furthermore, we show that one of our baselines, the popular three-label model, when trained with our proposed auxiliary task, outperforms the recent StarDist-3D. As an additional, practical contribution, we benchmark nuclei segmentation against nuclei detection, i.e. the task of merely pinpointing individual nuclei without generating respective pixel-accurate segmentations. For learning nuclei detection, large 3d training datasets of manually annotated nuclei center points are available. However, the impact on detection accuracy caused by training on such sparse ground truth as opposed to dense pixel-wise ground truth has not yet been quantified. To this end, we compare nuclei detection accuracy yielded by training on dense vs. sparse ground truth. Our results suggest that training on sparse ground truth yields competitive nuclei detection rates.



### Data augmentation with Mobius transformations
- **Arxiv ID**: http://arxiv.org/abs/2002.02917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.02917v2)
- **Published**: 2020-02-07 17:45:39+00:00
- **Updated**: 2020-06-07 08:00:04+00:00
- **Authors**: Sharon Zhou, Jiequan Zhang, Hang Jiang, Torbjorn Lundh, Andrew Y. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation has led to substantial improvements in the performance and generalization of deep models, and remain a highly adaptable method to evolving model architectures and varying amounts of data---in particular, extremely scarce amounts of available training data. In this paper, we present a novel method of applying Mobius transformations to augment input images during training. Mobius transformations are bijective conformal maps that generalize image translation to operate over complex inversion in pixel space. As a result, Mobius transformations can operate on the sample level and preserve data labels. We show that the inclusion of Mobius transformations during training enables improved generalization over prior sample-level data augmentation techniques such as cutout and standard crop-and-flip transformations, most notably in low data regimes.



### iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge: Hierarchical Group-wise Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.02918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02918v1)
- **Published**: 2020-02-07 17:46:38+00:00
- **Updated**: 2020-02-07 17:46:38+00:00
- **Authors**: Qian Liu, Dongyang Cai, Jie Liu, Nan Ding, Tao Wang
- **Comment**: Tech report
- **Journal**: None
- **Summary**: In this report, the method for the iqiyi submission to the task of ActivityNet 2019 Kinetics-700 challenge is described. Three models are involved in the model ensemble stage: TSN, HG-NL and StNet. We propose the hierarchical group-wise non-local (HG-NL) module for frame-level features aggregation for video classification. The standard non-local (NL) module is effective in aggregating frame-level features on the task of video classification but presents low parameters efficiency and high computational cost. The HG-NL method involves a hierarchical group-wise structure and generates multiple attention maps to enhance performance. Basing on this hierarchical group-wise structure, the proposed method has competitive accuracy, fewer parameters and smaller computational cost than the standard NL. For the task of ActivityNet 2019 Kinetics-700 challenge, after model ensemble, we finally obtain an averaged top-1 and top-5 error percentage 28.444% on the test set.



### Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources
- **Arxiv ID**: http://arxiv.org/abs/2002.02921v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02921v1)
- **Published**: 2020-02-07 17:49:08+00:00
- **Updated**: 2020-02-07 17:49:08+00:00
- **Authors**: Yidan Qin, Sahba Aghajani Pedram, Seyedshams Feyzabadi, Max Allan, A. Jonathan McLeod, Joel W. Burdick, Mahdi Azizian
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset.



### Subspace Capsule Network
- **Arxiv ID**: http://arxiv.org/abs/2002.02924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02924v1)
- **Published**: 2020-02-07 17:51:56+00:00
- **Updated**: 2020-02-07 17:51:56+00:00
- **Authors**: Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have become a key asset to most of fields in AI. Despite their successful performance, CNNs suffer from a major drawback. They fail to capture the hierarchy of spatial relation among different parts of an entity. As a remedy to this problem, the idea of capsules was proposed by Hinton. In this paper, we propose the SubSpace Capsule Network (SCN) that exploits the idea of capsule networks to model possible variations in the appearance or implicitly defined properties of an entity through a group of capsule subspaces instead of simply grouping neurons to create capsules. A capsule is created by projecting an input feature vector from a lower layer onto the capsule subspace using a learnable transformation. This transformation finds the degree of alignment of the input with the properties modeled by the capsule subspace. We show that SCN is a general capsule network that can successfully be applied to both discriminative and generative models without incurring computational overhead compared to CNN during test time. Effectiveness of SCN is evaluated through a comprehensive set of experiments on supervised image classification, semi-supervised image classification and high-resolution image generation tasks using the generative adversarial network (GAN) framework. SCN significantly improves the performance of the baseline models in all 3 tasks.



### SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.02927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02927v1)
- **Published**: 2020-02-07 17:55:28+00:00
- **Updated**: 2020-02-07 17:55:28+00:00
- **Authors**: Matthias Kirchner, Cameron Johnson
- **Comment**: Presented at the IEEE International Workshop on Information Forensics
  and Security (WIFS) 2019
- **Journal**: None
- **Summary**: We explore means to advance source camera identification based on sensor noise in a data-driven framework. Our focus is on improving the sensor pattern noise (SPN) extraction from a single image at test time. Where existing works suppress nuisance content with denoising filters that are largely agnostic to the specific SPN signal of interest, we demonstrate that a~deep learning approach can yield a more suitable extractor that leads to improved source attribution. A series of extensive experiments on various public datasets confirms the feasibility of our approach and its applicability to image manipulation localization and video source attribution. A critical discussion of potential pitfalls completes the text.



### How Does Gender Balance In Training Data Affect Face Recognition Accuracy?
- **Arxiv ID**: http://arxiv.org/abs/2002.02934v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02934v2)
- **Published**: 2020-02-07 18:11:01+00:00
- **Updated**: 2020-04-06 21:30:35+00:00
- **Authors**: Vítor Albiero, Kai Zhang, Kevin W. Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have greatly increased the accuracy of face recognition, but an old problem still persists: accuracy is usually higher for men than women. It is often speculated that lower accuracy for women is caused by under-representation in the training data. This work investigates female under-representation in the training data is truly the cause of lower accuracy for females on test data. Using a state-of-the-art deep CNN, three different loss functions, and two training datasets, we train each on seven subsets with different male/female ratios, totaling forty two trainings, that are tested on three different datasets. Results show that (1) gender balance in the training data does not translate into gender balance in the test accuracy, (2) the "gender gap" in test accuracy is not minimized by a gender-balanced training set, but by a training set with more male images than female images, and (3) training to minimize the accuracy gap does not result in highest female, male or average accuracy



### On the Robustness of Face Recognition Algorithms Against Attacks and Bias
- **Arxiv ID**: http://arxiv.org/abs/2002.02942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.02942v1)
- **Published**: 2020-02-07 18:21:59+00:00
- **Updated**: 2020-02-07 18:21:59+00:00
- **Authors**: Richa Singh, Akshay Agarwal, Maneet Singh, Shruti Nagpal, Mayank Vatsa
- **Comment**: Accepted in Senior Member Track, AAAI2020
- **Journal**: None
- **Summary**: Face recognition algorithms have demonstrated very high recognition performance, suggesting suitability for real world applications. Despite the enhanced accuracies, robustness of these algorithms against attacks and bias has been challenged. This paper summarizes different ways in which the robustness of a face recognition algorithm is challenged, which can severely affect its intended working. Different types of attacks such as physical presentation attacks, disguise/makeup, digital adversarial attacks, and morphing/tampering using GANs have been discussed. We also present a discussion on the effect of bias on face recognition models and showcase that factors such as age and gender variations affect the performance of modern algorithms. The paper also presents the potential reasons for these challenges and some of the future research directions for increasing the robustness of face recognition models.



### Activation Density driven Energy-Efficient Pruning in Training
- **Arxiv ID**: http://arxiv.org/abs/2002.02949v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02949v2)
- **Published**: 2020-02-07 18:34:31+00:00
- **Updated**: 2020-10-12 12:16:25+00:00
- **Authors**: Timothy Foldy-Porto, Yeshwanth Venkatesha, Priyadarshini Panda
- **Comment**: 8 pages, 5 figures, 4 tables (Accepted in ICPR 2020)
- **Journal**: None
- **Summary**: Neural network pruning with suitable retraining can yield networks with considerably fewer parameters than the original with comparable degrees of accuracy. Typical pruning methods require large, fully trained networks as a starting point from which they perform a time-intensive iterative pruning and retraining procedure to regain the original accuracy. We propose a novel pruning method that prunes a network real-time during training, reducing the overall training time to achieve an efficient compressed network. We introduce an activation density based analysis to identify the optimal relative sizing or compression for each layer of the network. Our method is architecture agnostic, allowing it to be employed on a wide variety of systems. For VGG-19 and ResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse networks (up to $200 \times$ reduction in parameters and over $60 \times$ reduction in inference compute operations in the best case) with accuracy comparable to the baseline network. By reducing the network size periodically during training, we achieve total training times that are shorter than those of previously proposed pruning methods. Furthermore, training compressed networks at different epochs with our proposed method yields considerable reduction in training compute complexity ($1.6\times$ to $3.2\times$ lower) at near iso-accuracy as compared to a baseline network trained entirely from scratch.



### $M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2002.02957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.02957v1)
- **Published**: 2020-02-07 18:53:13+00:00
- **Updated**: 2020-02-07 18:53:13+00:00
- **Authors**: Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, Shiguang Shan, Xilin Chen
- **Comment**: 6 pages, technical report; submission to ABAW Challenge at FG 2020
- **Journal**: None
- **Summary**: This report describes a multi-modal multi-task ($M^3$T) approach underlying our submission to the valence-arousal estimation track of the Affective Behavior Analysis in-the-wild (ABAW) Challenge, held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. In the proposed $M^3$T framework, we fuse both visual features from videos and acoustic features from the audio tracks to estimate the valence and arousal. The spatio-temporal visual features are extracted with a 3D convolutional network and a bidirectional recurrent neural network. Considering the correlations between valence / arousal, emotions, and facial actions, we also explores mechanisms to benefit from other tasks. We evaluated the $M^3$T framework on the validation set provided by ABAW and it significantly outperforms the baseline method.



### Revisiting Spatial Invariance with Low-Rank Local Connectivity
- **Arxiv ID**: http://arxiv.org/abs/2002.02959v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02959v2)
- **Published**: 2020-02-07 18:56:37+00:00
- **Updated**: 2020-08-14 20:45:09+00:00
- **Authors**: Gamaleldin F. Elsayed, Prajit Ramachandran, Jonathon Shlens, Simon Kornblith
- **Comment**: None
- **Journal**: International Conference on Machine Learning, 2020
- **Summary**: Convolutional neural networks are among the most successful architectures in deep learning with this success at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers only in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner; we create a \textit{low-rank} locally connected layer, where the filter bank applied at each position is constructed as a linear combination of basis set of filter banks with spatially varying combining weights. By varying the number of basis filter banks, we can control the degree of relaxation of spatial invariance. In experiments with small convolutional networks, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial invariance may be an overly restrictive prior.



### DropCluster: A structured dropout for convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/2002.02997v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02997v1)
- **Published**: 2020-02-07 20:02:47+00:00
- **Updated**: 2020-02-07 20:02:47+00:00
- **Authors**: Liyan Chen, Philip Gautier, Sergul Aydore
- **Comment**: 11 pages, 10 figures, under review
- **Journal**: None
- **Summary**: Dropout as a regularizer in deep neural networks has been less effective in convolutional layers than in fully connected layers. This is due to the fact that dropout drops features randomly. When features are spatially correlated as in the case of convolutional layers, information about the dropped pixels can still propagate to the next layers via neighboring pixels. In order to address this problem, more structured forms of dropout have been proposed. A drawback of these methods is that they do not adapt to the data. In this work, we introduce a novel structured regularization for convolutional layers, which we call DropCluster. Our regularizer relies on data-driven structure. It finds clusters of correlated features in convolutional layer outputs and drops the clusters randomly at each iteration. The clusters are learned and updated during model training so that they adapt both to the data and to the model weights. Our experiments on the ResNet-50 architecture demonstrate that our approach achieves better performance than DropBlock or other existing structured dropout variants. We also demonstrate the robustness of our approach when the size of training data is limited and when there is corruption in the data at test time.



### Renofeation: A Simple Transfer Learning Method for Improved Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2002.02998v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02998v2)
- **Published**: 2020-02-07 20:07:22+00:00
- **Updated**: 2021-04-28 14:46:56+00:00
- **Authors**: Ting-Wu Chin, Cha Zhang, Diana Marculescu
- **Comment**: 2021 IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer
  Vision
- **Journal**: None
- **Summary**: Fine-tuning through knowledge transfer from a pre-trained model on a large-scale dataset is a widely spread approach to effectively build models on small-scale datasets. In this work, we show that a recent adversarial attack designed for transfer learning via re-training the last linear layer can successfully deceive models trained with transfer learning via end-to-end fine-tuning. This raises security concerns for many industrial applications. In contrast, models trained with random initialization without transfer are much more robust to such attacks, although these models often exhibit much lower accuracy. To this end, we propose noisy feature distillation, a new transfer learning method that trains a network from random initialization while achieving clean-data performance competitive with fine-tuning. Code available at https://github.com/cmu-enyac/Renofeation.



### Cognitive Anthropomorphism of AI: How Humans and Computers Classify Images
- **Arxiv ID**: http://arxiv.org/abs/2002.03024v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2002.03024v1)
- **Published**: 2020-02-07 21:49:58+00:00
- **Updated**: 2020-02-07 21:49:58+00:00
- **Authors**: Shane T. Mueller
- **Comment**: None
- **Journal**: None
- **Summary**: Modern AI image classifiers have made impressive advances in recent years, but their performance often appears strange or violates expectations of users. This suggests humans engage in cognitive anthropomorphism: expecting AI to have the same nature as human intelligence. This mismatch presents an obstacle to appropriate human-AI interaction. To delineate this mismatch, I examine known properties of human classification, in comparison to image classifier systems. Based on this examination, I offer three strategies for system design that can address the mismatch between human and AI classification: explainable AI, novel methods for training users, and new algorithms that match human cognition.



### Deepfakes for Medical Video De-Identification: Privacy Protection and Diagnostic Information Preservation
- **Arxiv ID**: http://arxiv.org/abs/2003.00813v1
- **DOI**: 10.1145/3375627.3375849
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.00813v1)
- **Published**: 2020-02-07 22:36:48+00:00
- **Updated**: 2020-02-07 22:36:48+00:00
- **Authors**: Bingquan Zhu, Hao Fang, Yanan Sui, Luming Li
- **Comment**: Accepted for publication at the AAAI/ACM Conference on Artificial
  Intelligence, Ethics, and Society (AIES) 2020
- **Journal**: None
- **Summary**: Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.



### Local Facial Attribute Transfer through Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2002.03040v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03040v2)
- **Published**: 2020-02-07 22:57:01+00:00
- **Updated**: 2020-10-12 09:07:54+00:00
- **Authors**: Ricard Durall, Franz-Josef Pfreundt, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: The term attribute transfer refers to the tasks of altering images in such a way, that the semantic interpretation of a given input image is shifted towards an intended direction, which is quantified by semantic attributes. Prominent example applications are photo realistic changes of facial features and expressions, like changing the hair color, adding a smile, enlarging the nose or altering the entire context of a scene, like transforming a summer landscape into a winter panorama. Recent advances in attribute transfer are mostly based on generative deep neural networks, using various techniques to manipulate images in the latent space of the generator.   In this paper, we present a novel method for the common sub-task of local attribute transfers, where only parts of a face have to be altered in order to achieve semantic changes (e.g. removing a mustache). In contrast to previous methods, where such local changes have been implemented by generating new (global) images, we propose to formulate local attribute transfers as an inpainting problem. Removing and regenerating only parts of images, our Attribute Transfer Inpainting Generative Adversarial Network (ATI-GAN) is able to utilize local context information to focus on the attributes while keeping the background unmodified resulting in visually sound results.



