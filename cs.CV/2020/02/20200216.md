# Arxiv Papers in cs.CV on 2020-02-16
### Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories
- **Arxiv ID**: http://arxiv.org/abs/2002.06478v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06478v4)
- **Published**: 2020-02-16 00:23:43+00:00
- **Updated**: 2021-09-18 03:30:24+00:00
- **Authors**: Tiange Luo, Kaichun Mo, Zhiao Huang, Jiarui Xu, Siyu Hu, Liwei Wang, Hao Su
- **Comment**: ICLR2020
- **Journal**: None
- **Summary**: We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance.



### Face Recognition: Too Bias, or Not Too Bias?
- **Arxiv ID**: http://arxiv.org/abs/2002.06483v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06483v4)
- **Published**: 2020-02-16 01:08:12+00:00
- **Updated**: 2020-04-21 01:34:11+00:00
- **Authors**: Joseph P Robinson, Gennady Livitz, Yann Henon, Can Qin, Yun Fu, Samson Timoner
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR)
  Workshops, 2020
- **Journal**: None
- **Summary**: We reveal critical insights into problems of bias in state-of-the-art facial recognition (FR) systems using a novel Balanced Faces In the Wild (BFW) dataset: data balanced for gender and ethnic groups. We show variations in the optimal scoring threshold for face-pairs across different subgroups. Thus, the conventional approach of learning a global threshold for all pairs resulting in performance gaps among subgroups. By learning subgroup-specific thresholds, we not only mitigate problems in performance gaps but also show a notable boost in the overall performance. Furthermore, we do a human evaluation to measure the bias in humans, which supports the hypothesis that such a bias exists in human perception. For the BFW database, source code, and more, visit github.com/visionjo/facerec-bias-bfw.



### Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells
- **Arxiv ID**: http://arxiv.org/abs/2003.00824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML, I.2.0; I.2.6; I.5.1; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2003.00824v1)
- **Published**: 2020-02-16 04:22:18+00:00
- **Updated**: 2020-02-16 04:22:18+00:00
- **Authors**: Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, Ni Lao
- **Comment**: 15 pages; Accepted to ICLR 2020 as a spotlight paper
- **Journal**: ICLR 2020, Apr. 26 - 30, 2020, Addis Ababa, ETHIOPIA
- **Summary**: Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales.



### A Real-Time Deep Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2002.06515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06515v1)
- **Published**: 2020-02-16 06:09:22+00:00
- **Updated**: 2020-02-16 06:09:22+00:00
- **Authors**: Xiaowen Shi, Xin Li, Caili Wu, Shuchen Kong, Jing Yang, Liang He
- **Comment**: None
- **Journal**: IEEE ICASSP 2020
- **Summary**: Automatic analysis of highly crowded people has attracted extensive attention from computer vision research. Previous approaches for crowd counting have already achieved promising performance across various benchmarks. However, to deal with the real situation, we hope the model run as fast as possible while keeping accuracy. In this paper, we propose a compact convolutional neural network for crowd counting which learns a more efficient model with a small number of parameters. With three parallel filters executing the convolutional operation on the input image simultaneously at the front of the network, our model could achieve nearly real-time speed and save more computing resources. Experiments on two benchmarks show that our proposed method not only takes a balance between performance and efficiency which is more suitable for actual scenes but also is superior to existing light-weight models in speed.



### Facial Attribute Capsules for Noise Face Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2002.06518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06518v1)
- **Published**: 2020-02-16 06:22:28+00:00
- **Updated**: 2020-02-16 06:22:28+00:00
- **Authors**: Jingwei Xin, Nannan Wang, Xinrui Jiang, Jie Li, Xinbo Gao, Zhifeng Li
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: Existing face super-resolution (SR) methods mainly assume the input image to be noise-free. Their performance degrades drastically when applied to real-world scenarios where the input image is always contaminated by noise. In this paper, we propose a Facial Attribute Capsules Network (FACN) to deal with the problem of high-scale super-resolution of noisy face image. Capsule is a group of neurons whose activity vector models different properties of the same entity. Inspired by the concept of capsule, we propose an integrated representation model of facial information, which named Facial Attribute Capsule (FAC). In the SR processing, we first generated a group of FACs from the input LR face, and then reconstructed the HR face from this group of FACs. Aiming to effectively improve the robustness of FAC to noise, we generate FAC in semantic, probabilistic and facial attributes manners by means of integrated learning strategy. Each FAC can be divided into two sub-capsules: Semantic Capsule (SC) and Probabilistic Capsule (PC). Them describe an explicit facial attribute in detail from two aspects of semantic representation and probability distribution. The group of FACs model an image as a combination of facial attribute information in the semantic space and probabilistic space by an attribute-disentangling way. The diverse FACs could better combine the face prior information to generate the face images with fine-grained semantic attributes. Extensive benchmark experiments show that our method achieves superior hallucination results and outperforms state-of-the-art for very low resolution (LR) noise face image super resolution.



### SynFi: Automatic Synthetic Fingerprint Generation
- **Arxiv ID**: http://arxiv.org/abs/2002.08900v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.08900v1)
- **Published**: 2020-02-16 07:45:29+00:00
- **Updated**: 2020-02-16 07:45:29+00:00
- **Authors**: M. Sadegh Riazi, Seyed M. Chavoshian, Farinaz Koushanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Authentication and identification methods based on human fingerprints are ubiquitous in several systems ranging from government organizations to consumer products. The performance and reliability of such systems directly rely on the volume of data on which they have been verified. Unfortunately, a large volume of fingerprint databases is not publicly available due to many privacy and security concerns.   In this paper, we introduce a new approach to automatically generate high-fidelity synthetic fingerprints at scale. Our approach relies on (i) Generative Adversarial Networks to estimate the probability distribution of human fingerprints and (ii) Super-Resolution methods to synthesize fine-grained textures. We rigorously test our system and show that our methodology is the first to generate fingerprints that are computationally indistinguishable from real ones, a task that prior art could not accomplish.



### Breast Cancer Histopathology Image Classification and Localization using Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00823v1)
- **Published**: 2020-02-16 10:29:16+00:00
- **Updated**: 2020-02-16 10:29:16+00:00
- **Authors**: Abhijeet Patil, Dipesh Tamboli, Swati Meena, Deepak Anand, Amit Sethi
- **Comment**: Accepted in 2019 5th IEEE International WIE Conference on Electrical
  and Computer Engineering (WIECON-ECE) and Awarded as best paper
- **Journal**: None
- **Summary**: Breast cancer has the highest mortality among cancers in women. Computer-aided pathology to analyze microscopic histopathology images for diagnosis with an increasing number of breast cancer patients can bring the cost and delays of diagnosis down. Deep learning in histopathology has attracted attention over the last decade of achieving state-of-the-art performance in classification and localization tasks. The convolutional neural network, a deep learning framework, provides remarkable results in tissue images analysis, but lacks in providing interpretation and reasoning behind the decisions. We aim to provide a better interpretation of classification results by providing localization on microscopic histopathology images. We frame the image classification problem as weakly supervised multiple instance learning problem where an image is collection of patches i.e. instances. Attention-based multiple instance learning (A-MIL) learns attention on the patches from the image to localize the malignant and normal regions in an image and use them to classify the image. We present classification and localization results on two publicly available BreakHIS and BACH dataset. The classification and visualization results are compared with other recent techniques. The proposed method achieves better localization results without compromising classification accuracy.



### Topological Mapping for Manhattan-like Repetitive Environments
- **Arxiv ID**: http://arxiv.org/abs/2002.06575v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.06575v3)
- **Published**: 2020-02-16 13:20:28+00:00
- **Updated**: 2020-03-10 16:11:21+00:00
- **Authors**: Sai Shubodh Puligilla, Satyajit Tourani, Tushar Vaidya, Udit Singh Parihar, Ravi Kiran Sarvadevabhatla, K. Madhava Krishna
- **Comment**: Accepted to ICRA 2020. Project Page:
  https://github.com/Shubodh/ICRA2020
- **Journal**: None
- **Summary**: We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.



### Reinforced active learning for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.06583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06583v1)
- **Published**: 2020-02-16 14:03:06+00:00
- **Updated**: 2020-02-16 14:03:06+00:00
- **Authors**: Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J. Pal
- **Comment**: Accepted to ICLR2020
- **Journal**: None
- **Summary**: Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.



### Automated Labelling using an Attention model for Radiology reports of MRI scans (ALARM)
- **Arxiv ID**: http://arxiv.org/abs/2002.06588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06588v1)
- **Published**: 2020-02-16 15:04:52+00:00
- **Updated**: 2020-02-16 15:04:52+00:00
- **Authors**: David A. Wood, Jeremy Lynch, Sina Kafiabadi, Emily Guilhem, Aisha Al Busaidi, Antanas Montvila, Thomas Varsavsky, Juveria Siddiqui, Naveen Gadapa, Matthew Townend, Martin Kiik, Keena Patel, Gareth Barker, Sebastian Ourselin, James H. Cole, Thomas C. Booth
- **Comment**: None
- **Journal**: None
- **Summary**: Labelling large datasets for training high-capacity neural networks is a major obstacle to the development of deep learning-based medical imaging applications. Here we present a transformer-based network for magnetic resonance imaging (MRI) radiology report classification which automates this task by assigning image labels on the basis of free-text expert radiology reports. Our model's performance is comparable to that of an expert radiologist, and better than that of an expert physician, demonstrating the feasibility of this approach. We make code available online for researchers to label their own MRI datasets for medical imaging applications.



### Analytic Marching: An Analytic Meshing Solution from Deep Implicit Surface Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.06597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2002.06597v1)
- **Published**: 2020-02-16 15:36:19+00:00
- **Updated**: 2020-02-16 15:36:19+00:00
- **Authors**: Jiabao Lei, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies a problem of learning surface mesh via implicit functions in an emerging field of deep learning surface reconstruction, where implicit functions are popularly implemented as multi-layer perceptrons (MLPs) with rectified linear units (ReLU). To achieve meshing from learned implicit functions, existing methods adopt the de-facto standard algorithm of marching cubes; while promising, they suffer from loss of precision learned in the MLPs, due to the discretization nature of marching cubes. Motivated by the knowledge that a ReLU based MLP partitions its input space into a number of linear regions, we identify from these regions analytic cells and analytic faces that are associated with zero-level isosurface of the implicit function, and characterize the theoretical conditions under which the identified analytic faces are guaranteed to connect and form a closed, piecewise planar surface. Based on our theorem, we propose a naturally parallelizable algorithm of analytic marching, which marches among analytic cells to exactly recover the mesh captured by a learned MLP. Experiments on deep learning mesh reconstruction verify the advantages of our algorithm over existing ones.



### Two-dimensional Multi-fiber Spectrum Image Correction Based on Machine Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2002.06600v1
- **DOI**: 10.1093/mnras/staa2883
- **Categories**: **astro-ph.IM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06600v1)
- **Published**: 2020-02-16 15:39:09+00:00
- **Updated**: 2020-02-16 15:39:09+00:00
- **Authors**: Jiali Xu, Qian Yin, Ping Guo, Xin Zheng
- **Comment**: 10 pages, 14 figures
- **Journal**: None
- **Summary**: Due to limited size and imperfect of the optical components in a spectrometer, aberration has inevitably been brought into two-dimensional multi-fiber spectrum image in LAMOST, which leads to obvious spacial variation of the point spread functions (PSFs). Consequently, if spatial variant PSFs are estimated directly , the huge storage and intensive computation requirements result in deconvolutional spectral extraction method become intractable. In this paper, we proposed a novel method to solve the problem of spatial variation PSF through image aberration correction. When CCD image aberration is corrected, PSF, the convolution kernel, can be approximated by one spatial invariant PSF only. Specifically, machine learning techniques are adopted to calibrate distorted spectral image, including Total Least Squares (TLS) algorithm, intelligent sampling method, multi-layer feed-forward neural networks. The calibration experiments on the LAMOST CCD images show that the calibration effect of proposed method is effectible. At the same time, the spectrum extraction results before and after calibration are compared, results show the characteristics of the extracted one-dimensional waveform are more close to an ideal optics system, and the PSF of the corrected object spectrum image estimated by the blind deconvolution method is nearly central symmetry, which indicates that our proposed method can significantly reduce the complexity of spectrum extraction and improve extraction accuracy.



### Key Points Estimation and Point Instance Segmentation Approach for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.06604v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06604v4)
- **Published**: 2020-02-16 15:51:30+00:00
- **Updated**: 2020-09-14 03:22:44+00:00
- **Authors**: Yeongmin Ko, Younkwan Lee, Shoaib Azam, Farzeen Munir, Moongu Jeon, Witold Pedrycz
- **Comment**: Submitted to "IEEE Transactions on Intelligent Transportation
  Systems"
- **Journal**: None
- **Summary**: Perception techniques for autonomous driving should be adaptive to various environments. In the case of traffic line detection, an essential perception module, many condition should be considered, such as number of traffic lines and computing power of the target system. To address these problems, in this paper, we propose a traffic line detection method called Point Instance Network (PINet); the method is based on the key points estimation and instance segmentation approach. The PINet includes several stacked hourglass networks that are trained simultaneously. Therefore the size of the trained models can be chosen according to the computing power of the target environment. We cast a clustering problem of the predicted key points as an instance segmentation problem; the PINet can be trained regardless of the number of the traffic lines. The PINet achieves competitive accuracy and false positive on the TuSimple and Culane datasets, popular public datasets for lane detection. Our code is available at https://github.com/koyeongmin/PINet_new



### CRL: Class Representative Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.06619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06619v1)
- **Published**: 2020-02-16 17:02:59+00:00
- **Updated**: 2020-02-16 17:02:59+00:00
- **Authors**: Mayanka Chandrashekar, Yugyung Lee
- **Comment**: 15 pages, Table 8, Figure 6
- **Journal**: None
- **Summary**: Building robust and real-time classifiers with diverse datasets are one of the most significant challenges to deep learning researchers. It is because there is a considerable gap between a model built with training (seen) data and real (unseen) data in applications. Recent works including Zero-Shot Learning (ZSL), have attempted to deal with this problem of overcoming the apparent gap through transfer learning. In this paper, we propose a novel model, called Class Representative Learning Model (CRL), that can be especially effective in image classification influenced by ZSL. In the CRL model, first, the learning step is to build class representatives to represent classes in datasets by aggregating prominent features extracted from a Convolutional Neural Network (CNN). Second, the inferencing step in CRL is to match between the class representatives and new data. The proposed CRL model demonstrated superior performance compared to the current state-of-the-art research in ZSL and mobile deep learning. The proposed CRL model has been implemented and evaluated in a parallel environment, using Apache Spark, for both distributed learning and recognition. An extensive experimental study on the benchmark datasets, ImageNet-1K, CalTech-101, CalTech-256, CIFAR-100, shows that CRL can build a class distribution model with drastic improvement in learning and recognition performance without sacrificing accuracy compared to the state-of-the-art performances in image classification.



### Block Annotation: Better Image Annotation for Semantic Segmentation with Sub-Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2002.06626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06626v1)
- **Published**: 2020-02-16 17:42:37+00:00
- **Updated**: 2020-02-16 17:42:37+00:00
- **Authors**: Hubert Lin, Paul Upchurch, Kavita Bala
- **Comment**: ICCV 2019; http://www.cs.cornell.edu/~hubert/block_annotation/
- **Journal**: None
- **Summary**: Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation.



### Resource-Frugal Classification and Analysis of Pathology Slides Using Image Entropy
- **Arxiv ID**: http://arxiv.org/abs/2002.07621v3
- **DOI**: 10.1016/j.bspc.2020.102388
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07621v3)
- **Published**: 2020-02-16 18:42:36+00:00
- **Updated**: 2020-12-02 19:26:41+00:00
- **Authors**: Steven J. Frank
- **Comment**: None
- **Journal**: Biomedical Signal Processing and Control, vol. 66, April 2021,
  102388
- **Summary**: Pathology slides of lung malignancies are classified using resource-frugal convolution neural networks (CNNs) that may be deployed on mobile devices. In particular, the challenging task of distinguishing adenocarcinoma (LUAD) and squamous-cell carcinoma (LUSC) lung cancer subtypes is approached in two stages. First, whole-slide histopathology images are downsampled to a size too large for CNN analysis but large enough to retain key anatomic detail. The downsampled images are decomposed into smaller square tiles, which are sifted based on their image entropies. A lightweight CNN produces tile-level classifications that are aggregated to classify the slide. The resulting accuracies are comparable to those obtained with much more complex CNNs and larger training sets. To allow clinicians to visually assess the basis for the classification -- that is, to see the image regions that underlie it -- color-coded probability maps are created by overlapping tiles and averaging the tile-level probabilities at a pixel level.



### Coresets for the Nearest-Neighbor Rule
- **Arxiv ID**: http://arxiv.org/abs/2002.06650v3
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.DS, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.06650v3)
- **Published**: 2020-02-16 19:00:48+00:00
- **Updated**: 2020-07-22 19:14:41+00:00
- **Authors**: Alejandro Flores-Velazco, David M. Mount
- **Comment**: None
- **Journal**: None
- **Summary**: Given a training set $P$ of labeled points, the nearest-neighbor rule predicts the class of an unlabeled query point as the label of its closest point in the set. To improve the time and space complexity of classification, a natural question is how to reduce the training set without significantly affecting the accuracy of the nearest-neighbor rule. Nearest-neighbor condensation deals with finding a subset $R \subseteq P$ such that for every point $p \in P$, $p$'s nearest-neighbor in $R$ has the same label as $p$. This relates to the concept of coresets, which can be broadly defined as subsets of the set, such that an exact result on the coreset corresponds to an approximate result on the original set. However, the guarantees of a coreset hold for any query point, and not only for the points of the training set.   This paper introduces the concept of coresets for nearest-neighbor classification. We extend existing criteria used for condensation, and prove sufficient conditions to correctly classify any query point when using these subsets. Additionally, we prove that finding such subsets of minimum cardinality is NP-hard, and propose quadratic-time approximation algorithms with provable upper-bounds on the size of their selected subsets. Moreover, we show how to improve one of these algorithms to have subquadratic runtime, being the first of this kind for condensation.



### Cortical surface parcellation based on intra-subject white matter fiber clustering
- **Arxiv ID**: http://arxiv.org/abs/2002.09034v1
- **DOI**: 10.1109/CHILECON47746.2019.8988066
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09034v1)
- **Published**: 2020-02-16 19:14:39+00:00
- **Updated**: 2020-02-16 19:14:39+00:00
- **Authors**: Narciso López-López, Andrea Vázquez, Cyril Poupon, Jean-François Mangin, Pamela Guevara
- **Comment**: This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO
  NACIONAL/2016-21160342, CONICYT FONDECYT 1190701, CONICYT PIA/Anillo de
  Investigaci\'on en Ciencia y Tecnolog\'ia ACT172121 and CONICYT Basal Center
  FB0008
- **Journal**: None
- **Summary**: We present a hybrid method that performs the complete parcellation of the cerebral cortex of an individual, based on the connectivity information of the white matter fibers from a whole-brain tractography dataset. The method consists of five steps, first intra-subject clustering is performed on the brain tractography. The fibers that make up each cluster are then intersected with the cortical mesh and then filtered to discard outliers. In addition, the method resolves the overlapping between the different intersection regions (sub-parcels) throughout the cortex efficiently. Finally, a post-processing is done to achieve more uniform sub-parcels. The output is the complete labeling of cortical mesh vertices, representing the different cortex sub-parcels, with strong connections to other sub-parcels. We evaluated our method with measures of brain connectivity such as functional segregation (clustering coefficient), functional integration (characteristic path length) and small-world. Results in five subjects from ARCHI database show a good individual cortical parcellation for each one, composed of about 200 subparcels per hemisphere and complying with these connectivity measures.



### Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings
- **Arxiv ID**: http://arxiv.org/abs/2002.06661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06661v1)
- **Published**: 2020-02-16 19:49:30+00:00
- **Updated**: 2020-02-16 19:49:30+00:00
- **Authors**: Shweta Mahajan, Iryna Gurevych, Stefan Roth
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: Learned joint representations of images and text form the backbone of several important cross-domain tasks such as image captioning. Prior work mostly maps both domains into a common latent representation in a purely supervised fashion. This is rather restrictive, however, as the two domains follow distinct generative processes. Therefore, we propose a novel semi-supervised framework, which models shared information between domains and domain-specific information separately. The information shared between the domains is aligned with an invertible neural network. Our model integrates normalizing flow-based priors for the domain-specific information, which allows us to learn diverse many-to-many mappings between the two domains. We demonstrate the effectiveness of our model on diverse tasks, including image captioning and text-to-image synthesis.



### PeeledHuman: Robust Shape Representation for Textured 3D Human Body Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2002.06664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06664v2)
- **Published**: 2020-02-16 20:03:24+00:00
- **Updated**: 2020-11-02 10:06:42+00:00
- **Authors**: Sai Sagar Jinka, Rohan Chacko, Avinash Sharma, P. J. Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce PeeledHuman - a novel shape representation of the human body that is robust to self-occlusions. PeeledHuman encodes the human body as a set of Peeled Depth and RGB maps in 2D, obtained by performing ray-tracing on the 3D body model and extending each ray beyond its first intersection. This formulation allows us to handle self-occlusions efficiently compared to other representations. Given a monocular RGB image, we learn these Peeled maps in an end-to-end generative adversarial fashion using our novel framework - PeelGAN. We train PeelGAN using a 3D Chamfer loss and other 2D losses to generate multiple depth values per-pixel and a corresponding RGB field per-vertex in a dual-branch setup. In our simple non-parametric solution, the generated Peeled Depth maps are back-projected to 3D space to obtain a complete textured 3D shape. The corresponding RGB maps provide vertex-level texture details. We compare our method with current parametric and non-parametric methods in 3D reconstruction and find that we achieve state-of-the-art-results. We demonstrate the effectiveness of our representation on publicly available BUFF and MonoPerfCap datasets as well as loose clothing data collected by our calibrated multi-Kinect setup.



### AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic Video Scenes
- **Arxiv ID**: http://arxiv.org/abs/2002.06666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.06666v2)
- **Published**: 2020-02-16 20:11:32+00:00
- **Updated**: 2020-08-09 21:20:34+00:00
- **Authors**: Manh Huynh, Gita Alaghband
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: We present a novel adaptive online learning (AOL) framework to predict human movement trajectories in dynamic video scenes. Our framework learns and adapts to changes in the scene environment and generates best network weights for different scenarios. The framework can be applied to prediction models and improve their performance as it dynamically adjusts when it encounters changes in the scene and can apply the best training weights for predicting the next locations. We demonstrate this by integrating our framework with two existing prediction models: LSTM [3] and Future Person Location (FPL) [1]. Furthermore, we analyze the number of network weights for optimal performance and show that we can achieve real-time with a fixed number of networks using the least recently used (LRU) strategy for maintaining the most recently trained network weights. With extensive experiments, we show that our framework increases prediction accuracies of LSTM and FPL by ~17% and 28% on average, and up to ~50% for FPL on the worst case while achieving real-time (20fps).



### Generator From Edges: Reconstruction of Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2002.06682v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.06682v3)
- **Published**: 2020-02-16 21:18:04+00:00
- **Updated**: 2020-05-14 04:44:05+00:00
- **Authors**: Nao Takano, Gita Alaghband
- **Comment**: None
- **Journal**: None
- **Summary**: Applications that involve supervised training require paired images. Researchers of single image super-resolution (SISR) create such images by artificially generating blurry input images from the corresponding ground truth. Similarly we can create paired images with the canny edge. We propose Generator From Edges (GFE) [Figure 2]. Our aim is to determine the best architecture for GFE, along with reviews of perceptual loss [1, 2]. To this end, we conducted three experiments. First, we explored the effects of the adversarial loss often used in SISR. In particular, we uncovered that it is not an essential component to form a perceptual loss. Eliminating adversarial loss will lead to a more effective architecture from the perspective of hardware resource. It also means that considerations for the problems pertaining to generative adversarial network (GAN) [3], such as mode collapse, are not necessary. Second, we reexamined VGG loss and found that the mid-layers yield the best results. By extracting the full potential of VGG loss, the overall performance of perceptual loss improves significantly. Third, based on the findings of the first two experiments, we reevaluated the dense network to construct GFE. Using GFE as an intermediate process, reconstructing a facial image from a pencil sketch can become an easy task.



### Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework
- **Arxiv ID**: http://arxiv.org/abs/2002.06701v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.06701v1)
- **Published**: 2020-02-16 23:03:32+00:00
- **Updated**: 2020-02-16 23:03:32+00:00
- **Authors**: Chiranjib Sur
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF) for Better Semantic Selection for Indian regional language-based image captioning and introduced a procedure where we used the existing translation and English crowd-sourced sentences for training. We have shown that this architecture is a promising alternative source, where there is a crunch in resources. Our main contribution of this work is the development of deep learning architectures for the Bengali language (is the fifth widely spoken language in the world) with a completely different grammar and language attributes. We have shown that these are working well for complex applications like language generation from image contexts and can diversify the representation through introducing constraints, more extensive features, and unique feature spaces. We also established that we could achieve absolute precision and diversity when we use smoothened semantic tensor with the traditional LSTM and feature decomposition networks. With better learning architecture, we succeeded in establishing an automated algorithm and assessment procedure that can help in the evaluation of competent applications without the requirement for expertise and human intervention.



