# Arxiv Papers in cs.CV on 2020-02-25
### CookGAN: Meal Image Synthesis from Ingredients
- **Arxiv ID**: http://arxiv.org/abs/2002.11493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11493v1)
- **Published**: 2020-02-25 00:54:10+00:00
- **Updated**: 2020-02-25 00:54:10+00:00
- **Authors**: Fangda Han, Ricardo Guerrero, Vladimir Pavlovic
- **Comment**: 10 pages, 5 figures, accepted by WACV 2020. arXiv admin note:
  substantial text overlap with arXiv:1905.13149
- **Journal**: None
- **Summary**: In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual list of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by generative neural networks (GAN) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers, but meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. To generate real-like meal images from ingredients, we propose Cook Generative Adversarial Networks (CookGAN), CookGAN first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Experiments show our model is able to generate meal images corresponding to the ingredients.



### Fast Loop Closure Detection via Binary Content
- **Arxiv ID**: http://arxiv.org/abs/2002.10622v2
- **DOI**: 10.1109/ICCA.2019.8899937
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.10622v2)
- **Published**: 2020-02-25 01:59:54+00:00
- **Updated**: 2021-02-08 13:54:37+00:00
- **Authors**: Han Wang, Juncheng Li, Maopeng Ran, Lihua Xie
- **Comment**: IEEE International Conference on Control and Automation (ICCA) 2019
- **Journal**: 2019 IEEE International Conference on Control and Automation
  (ICCA)
- **Summary**: Loop closure detection plays an important role in reducing localization drift in Simultaneous Localization And Mapping (SLAM). It aims to find repetitive scenes from historical data to reset localization. To tackle the loop closure problem, existing methods often leverage on the matching of visual features, which achieve good accuracy but require high computational resources. However, feature point based methods ignore the patterns of image, i.e., the shape of the objects as well as the distribution of objects in an image. It is believed that this information is usually unique for a scene and can be utilized to improve the performance of traditional loop closure detection methods. In this paper we leverage and compress the information into a binary image to accelerate an existing fast loop closure detection method via binary content. The proposed method can greatly reduce the computational cost without sacrificing recall rate. It consists of three parts: binary content construction, fast image retrieval and precise loop closure detection. No offline training is required. Our method is compared with the state-of-the-art loop closure detection methods and the results show that it outperforms the traditional methods at both recall rate and speed.



### Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space
- **Arxiv ID**: http://arxiv.org/abs/2003.00883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00883v1)
- **Published**: 2020-02-25 02:41:42+00:00
- **Updated**: 2020-02-25 02:41:42+00:00
- **Authors**: Camilo Pestana, Naveed Akhtar, Wei Liu, David Glance, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning offers state of the art solutions for image recognition. However, deep models are vulnerable to adversarial perturbations in images that are subtle but significantly change the model's prediction. In a white-box attack, these perturbations are generally learned for deep models that operate on RGB images and, hence, the perturbations are equally distributed in the RGB color space. In this paper, we show that the adversarial perturbations prevail in the Y-channel of the YCbCr space. Our finding is motivated from the fact that the human vision and deep models are more responsive to shape and texture rather than color. Based on our finding, we propose a defense against adversarial images. Our defence, coined ResUpNet, removes perturbations only from the Y-channel by exploiting ResNet features in an upsampling framework without the need for a bottleneck. At the final stage, the untouched CbCr-channels are combined with the refined Y-channel to restore the clean image. Note that ResUpNet is model agnostic as it does not modify the DNN structure. ResUpNet is trained end-to-end in Pytorch and the results are compared to existing defence techniques in the input transformation category. Our results show that our approach achieves the best balance between defence against adversarial attacks such as FGSM, PGD and DDN and maintaining the original accuracies of VGG-16, ResNet50 and DenseNet121 on clean images. We perform another experiment to show that learning adversarial perturbations only for the Y-channel results in higher fooling rates for the same perturbation magnitude.



### Batch norm with entropic regularization turns deterministic autoencoders into generative models
- **Arxiv ID**: http://arxiv.org/abs/2002.10631v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10631v2)
- **Published**: 2020-02-25 02:42:18+00:00
- **Updated**: 2021-09-22 00:51:29+00:00
- **Authors**: Amur Ghose, Abdullah Rashwan, Pascal Poupart
- **Comment**: None
- **Journal**: Published in the Proceedings of the International Conference on
  Uncertainty in Artificial Intelligence (UAI), 2020
- **Summary**: The variational autoencoder is a well defined deep generative model that utilizes an encoder-decoder framework where an encoding neural network outputs a non-deterministic code for reconstructing an input. The encoder achieves this by sampling from a distribution for every input, instead of outputting a deterministic code per input. The great advantage of this process is that it allows the use of the network as a generative model for sampling from the data distribution beyond provided samples for training. We show in this work that utilizing batch normalization as a source for non-determinism suffices to turn deterministic autoencoders into generative models on par with variational ones, so long as we add a suitable entropic regularization to the training objective.



### Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2002.10638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.10638v2)
- **Published**: 2020-02-25 03:08:12+00:00
- **Updated**: 2020-04-05 03:20:31+00:00
- **Authors**: Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao
- **Comment**: To appear at CVPR 2020. The first two authors contributed equally to
  this manuscript. Code: https://github.com/weituo12321/PREVALENT
- **Journal**: None
- **Summary**: Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. In this paper, we present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent called Prevalent. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room benchmark, our model improves the state-of-the-art from 47% to 51% on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation and "Help, Anna!" the proposed Prevalent leads to significant improvement over existing methods, achieving a new state of the art.



### I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively
- **Arxiv ID**: http://arxiv.org/abs/2002.10648v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10648v1)
- **Published**: 2020-02-25 03:32:29+00:00
- **Updated**: 2020-02-25 03:32:29+00:00
- **Authors**: Haotao Wang, Tianlong Chen, Zhangyang Wang, Kede Ma
- **Comment**: Accepted to ICLR 2020
- **Journal**: None
- **Summary**: The learning of hierarchical representations for image classification has experienced an impressive series of successes due in part to the availability of large-scale labeled data for training. On the other hand, the trained classifiers have traditionally been evaluated on small and fixed sets of test images, which are deemed to be extremely sparsely distributed in the space of all natural images. It is thus questionable whether recent performance improvements on the excessively re-used test sets generalize to real-world natural images with much richer content variations. Inspired by efficient stimulus selection for testing perceptual models in psychophysical and physiological studies, we present an alternative framework for comparing image classifiers, which we name the MAximum Discrepancy (MAD) competition. Rather than comparing image classifiers using fixed test images, we adaptively sample a small test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy. Human labeling on the resulting model-dependent image sets reveals the relative performance of the competing classifiers, and provides useful insights on potential ways to improve them. We report the MAD competition results of eleven ImageNet classifiers while noting that the framework is readily extensible and cost-effective to add future classifiers into the competition. Codes can be found at https://github.com/TAMU-VITA/MAD.



### Copy and Paste GAN: Face Hallucination from Shaded Thumbnails
- **Arxiv ID**: http://arxiv.org/abs/2002.10650v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10650v3)
- **Published**: 2020-02-25 03:34:58+00:00
- **Updated**: 2020-03-19 07:31:34+00:00
- **Authors**: Yang Zhang, Ivor Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, Xin Yu
- **Comment**: CVPR2020 (oral) preprint
- **Journal**: None
- **Summary**: Existing face hallucination methods based on convolutional neural networks (CNN) have achieved impressive performance on low-resolution (LR) faces in a normal illumination condition. However, their performance degrades dramatically when LR faces are captured in low or non-uniform illumination conditions. This paper proposes a Copy and Paste Generative Adversarial Network (CPGAN) to recover authentic high-resolution (HR) face images while compensating for low and non-uniform illumination. To this end, we develop two key components in our CPGAN: internal and external Copy and Paste nets (CPnets). Specifically, our internal CPnet exploits facial information residing in the input image to enhance facial details; while our external CPnet leverages an external HR face for illumination compensation. A new illumination compensation loss is thus developed to capture illumination from the external guided face image effectively. Furthermore, our method offsets illumination and upsamples facial details alternately in a coarse-to-fine fashion, thus alleviating the correspondence ambiguity between LR inputs and external HR inputs. Extensive experiments demonstrate that our method manifests authentic HR face images in a uniform illumination condition and outperforms state-of-the-art methods qualitatively and quantitatively.



### A Comparative Evaluation of Temporal Pooling Methods for Blind Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2002.10651v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10651v1)
- **Published**: 2020-02-25 03:35:06+00:00
- **Updated**: 2020-02-25 03:35:06+00:00
- **Authors**: Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Neil Birkbeck, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Many objective video quality assessment (VQA) algorithms include a key step of temporal pooling of frame-level quality scores. However, less attention has been paid to studying the relative efficiencies of different pooling methods on no-reference (blind) VQA. Here we conduct a large-scale comparative evaluation to assess the capabilities and limitations of multiple temporal pooling strategies on blind VQA of user-generated videos. The study yields insights and general guidance regarding the application and selection of temporal pooling models. In addition, we also propose an ensemble pooling model built on top of high-performing temporal pooling models. Our experimental results demonstrate the relative efficacies of the evaluated temporal pooling models, using several popular VQA algorithms, and evaluated on two recent large-scale natural video quality databases. In addition to the new ensemble model, we provide a general recipe for applying temporal pooling of frame-based quality predictions.



### Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization
- **Arxiv ID**: http://arxiv.org/abs/2002.10657v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10657v1)
- **Published**: 2020-02-25 03:59:31+00:00
- **Updated**: 2020-02-25 03:59:31+00:00
- **Authors**: Satrajit Chatterjee
- **Comment**: To appear in ICLR 2020
- **Journal**: None
- **Summary**: An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.



### Towards Backdoor Attacks and Defense in Robust Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2003.00865v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00865v4)
- **Published**: 2020-02-25 04:45:26+00:00
- **Updated**: 2023-01-11 05:00:29+00:00
- **Authors**: Ezekiel Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay
- **Comment**: Accepted in Computers & Security, 2023
- **Journal**: None
- **Summary**: The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. Notably, the state-of-the-art projected gradient descent (PGD)-based training method has been shown to be universally and reliably effective in defending against adversarial inputs. This robustness approach uses PGD as a reliable and universal "first-order adversary". However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we study how to inject and defend against backdoor attacks for robust models trained using PGD-based robust optimisation. We demonstrate that these models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect such backdoor-infected models via a detection technique called AEGIS. Specifically, given a robust Deep Neural Network (DNN) that is trained using PGD-based first-order adversarial training approach, AEGIS uses feature clustering to effectively detect whether such DNNs are backdoor-infected or clean.   In our evaluation of several visible and hidden backdoor triggers on major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects PGD-trained robust DNNs infected with backdoors. AEGIS detects such backdoor-infected models with 91.6% accuracy (11 out of 12 tested models), without any false positives. Furthermore, AEGIS detects the targeted class in the backdoor-infected model with a reasonably low (11.1%) false positive rate. Our investigation reveals that salient features of adversarially robust DNNs could be promising to break the stealthy nature of backdoor attacks.



### Towards Better Surgical Instrument Segmentation in Endoscopic Vision: Multi-Angle Feature Aggregation and Contour Supervision
- **Arxiv ID**: http://arxiv.org/abs/2002.10675v2
- **DOI**: 10.1109/LRA.2020.3009073
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10675v2)
- **Published**: 2020-02-25 05:28:46+00:00
- **Updated**: 2020-08-11 03:20:35+00:00
- **Authors**: Fangbo Qin, Shan Lin, Yangming Li, Randall A. Bly, Kris S. Moe, Blake Hannaford
- **Comment**: Accepted by IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Accurate and real-time surgical instrument segmentation is important in the endoscopic vision of robot-assisted surgery, and significant challenges are posed by frequent instrument-tissue contacts and continuous change of observation perspective. For these challenging tasks more and more deep neural networks (DNN) models are designed in recent years. We are motivated to propose a general embeddable approach to improve these current DNN segmentation models without increasing the model parameter number. Firstly, observing the limited rotation-invariance performance of DNN, we proposed the Multi-Angle Feature Aggregation (MAFA) method, leveraging active image rotation to gain richer visual cues and make the prediction more robust to instrument orientation changes. Secondly, in the end-to-end training stage, the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness, so that the contour shape of segmentation mask is more precise. The proposed method is validated with ablation experiments on the novel Sinus-Surgery datasets collected from surgeons' operations, and is compared to the existing methods on a public dataset collected with a da Vinci Xi Robot.



### Globally Optimal Contrast Maximisation for Event-based Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2002.10686v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10686v3)
- **Published**: 2020-02-25 05:54:29+00:00
- **Updated**: 2020-03-16 01:06:05+00:00
- **Authors**: Daqi Liu, Álvaro Parra, Tat-Jun Chin
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: Contrast maximisation estimates the motion captured in an event stream by maximising the sharpness of the motion compensated event image. To carry out contrast maximisation, many previous works employ iterative optimisation algorithms, such as conjugate gradient, which require good initialisation to avoid converging to bad local minima. To alleviate this weakness, we propose a new globally optimal event-based motion estimation algorithm. Based on branch-and-bound (BnB), our method solves rotational (3DoF) motion estimation on event streams, which supports practical applications such as video stabilisation and attitude estimation. Underpinning our method are novel bounding functions for contrast maximisation, whose theoretical validity is rigorously established. We show concrete examples from public datasets where globally optimal solutions are vital to the success of contrast maximisation. Despite its exact nature, our algorithm is currently able to process a 50,000 event input in 300 seconds (a locally optimal solver takes 30 seconds on the same input), and has the potential to be further speeded-up using GPUs.



### Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge
- **Arxiv ID**: http://arxiv.org/abs/2002.10695v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.10695v1)
- **Published**: 2020-02-25 06:41:07+00:00
- **Updated**: 2020-02-25 06:41:07+00:00
- **Authors**: Hung Le, Nancy F. Chen
- **Comment**: Accepted at DSTC Workshop at AAAI 2020
- **Journal**: None
- **Summary**: Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question Answering (QA) whereby the dialogue agent is required to generate natural language responses to address user queries and carry on conversations. This is a challenging task as it consists of video features of multiple modalities, including text, visual, and audio features. The agent also needs to learn semantic dependencies among user utterances and system responses to make coherent conversations with humans. In this work, we describe our submission to the AVSD track of the 8th Dialogue System Technology Challenge. We adopt dot-product attention to combine text and non-text features of input video. We further enhance the generation capability of the dialogue agent by adopting pointer networks to point to tokens from multiple source sequences in each generation step. Our systems achieve high performance in automatic metrics and obtain 5th and 6th place in human evaluation among all submissions.



### Hierarchical Conditional Relation Networks for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2002.10698v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10698v3)
- **Published**: 2020-02-25 07:00:48+00:00
- **Updated**: 2020-03-17 08:32:45+00:00
- **Authors**: Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran
- **Comment**: Check out our code on GitHub at
  https://github.com/thaolmk54/hcrn-videoqa
- **Journal**: CVPR 2020, Oral
- **Summary**: Video question answering (VideoQA) is challenging as it requires modeling capacity to distill dynamic visual artifacts and distant relations and to associate them with linguistic concepts. We introduce a general-purpose reusable neural unit called Conditional Relation Network (CRN) that serves as a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning. The resulting architecture for VideoQA is a CRN hierarchy whose branches represent sub-videos or clips, all sharing the same question as the contextual condition. Our evaluations on well-known datasets achieved new SoTA results, demonstrating the impact of building a general-purpose reasoning unit on complex domains such as VideoQA.



### FPConv: Learning Local Flattening for Point Convolution
- **Arxiv ID**: http://arxiv.org/abs/2002.10701v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10701v3)
- **Published**: 2020-02-25 07:15:08+00:00
- **Updated**: 2020-03-14 04:13:13+00:00
- **Authors**: Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang Liu, Shuguang Cui, Xiaoguang Han
- **Comment**: Camera-ready, accepted to CVPR 2020
- **Journal**: None
- **Summary**: We introduce FPConv, a novel surface-style convolution operator designed for 3D point cloud analysis. Unlike previous methods, FPConv doesn't require transforming to intermediate representation like 3D grid or graph and directly works on surface geometry of point cloud. To be more specific, for each point, FPConv performs a local flattening by automatically learning a weight map to softly project surrounding points onto a 2D grid. Regular 2D convolution can thus be applied for efficient feature learning. FPConv can be easily integrated into various network architectures for tasks like 3D object classification and 3D scene segmentation, and achieve comparable performance with existing volumetric-type convolutions. More importantly, our experiments also show that FPConv can be a complementary of volumetric convolutions and jointly training them can further boost overall performance into state-of-the-art results.



### Gödel's Sentence Is An Adversarial Example But Unsolvable
- **Arxiv ID**: http://arxiv.org/abs/2002.10703v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10703v1)
- **Published**: 2020-02-25 07:20:17+00:00
- **Updated**: 2020-02-25 07:20:17+00:00
- **Authors**: Xiaodong Qi, Lansheng Han
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, different types of adversarial examples from different fields have emerged endlessly, including purely natural ones without perturbations. A variety of defenses are proposed and then broken quickly. Two fundamental questions need to be asked: What's the reason for the existence of adversarial examples and are adversarial examples unsolvable? In this paper, we will show the reason for the existence of adversarial examples is there are non-isomorphic natural explanations that can all explain data set. Specifically, for two natural explanations of being true and provable, G\"odel's sentence is an adversarial example but ineliminable. It can't be solved by the re-accumulation of data set or the re-improvement of learning algorithm. Finally, from the perspective of computability, we will prove the incomputability for adversarial examples, which are unrecognizable.



### Searching for Winograd-aware Quantized Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.10711v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10711v1)
- **Published**: 2020-02-25 07:53:53+00:00
- **Updated**: 2020-02-25 07:53:53+00:00
- **Authors**: Javier Fernandez-Marques, Paul N. Whatmough, Andrew Mundy, Matthew Mattina
- **Comment**: Published as a conference paper at MLSys 2020
- **Journal**: Proceedings of Machine Learning and Systems (2020), 14-29
- **Summary**: Lightweight architectural designs of Convolutional Neural Networks (CNNs) together with quantization have paved the way for the deployment of demanding computer vision applications on mobile devices. Parallel to this, alternative formulations to the convolution operation such as FFT, Strassen and Winograd, have been adapted for use in CNNs offering further speedups. Winograd convolutions are the fastest known algorithm for spatially small convolutions, but exploiting their full potential comes with the burden of numerical error, rendering them unusable in quantized contexts. In this work we propose a Winograd-aware formulation of convolution layers which exposes the numerical inaccuracies introduced by the Winograd transformations to the learning of the model parameters, enabling the design of competitive quantized models without impacting model size. We also address the source of the numerical error and propose a relaxation on the form of the transformation matrices, resulting in up to 10% higher classification accuracy on CIFAR-10. Finally, we propose wiNAS, a neural architecture search (NAS) framework that jointly optimizes a given macro-architecture for accuracy and latency leveraging Winograd-aware layers. A Winograd-aware ResNet-18 optimized with wiNAS for CIFAR-10 results in 2.66x speedup compared to im2row, one of the most widely used optimized convolution implementations, with no loss in accuracy.



### Technical report: Kidney tumor segmentation using a 2D U-Net followed by a statistical post-processing filter
- **Arxiv ID**: http://arxiv.org/abs/2002.10727v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10727v1)
- **Published**: 2020-02-25 08:25:33+00:00
- **Updated**: 2020-02-25 08:25:33+00:00
- **Authors**: Iwan Paolucci
- **Comment**: KiTS 2019 challenge
- **Journal**: None
- **Summary**: Each year, there are about 400'000 new cases of kidney cancer worldwide causing around 175'000 deaths. For clinical decision making it is important to understand the morphometry of the tumor, which involves the time-consuming task of delineating tumor and kidney in 3D CT images. Automatic segmentation could be an important tool for clinicians and researchers to also study the correlations between tumor morphometry and clinical outcomes. We present a segmentation method which combines the popular U-Net convolutional neural network architecture with post-processing based on statistical constraints of the available training data. The full implementation, based on PyTorch, and the trained weights can be found on GitHub.



### (De)Randomized Smoothing for Certifiable Defense against Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2002.10733v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10733v3)
- **Published**: 2020-02-25 08:39:46+00:00
- **Updated**: 2021-01-08 06:36:56+00:00
- **Authors**: Alexander Levine, Soheil Feizi
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Patch adversarial attacks on images, in which the attacker can distort pixels within a region of bounded size, are an important threat model since they provide a quantitative model for physical adversarial attacks. In this paper, we introduce a certifiable defense against patch attacks that guarantees for a given image and patch attack size, no patch adversarial examples exist. Our method is related to the broad class of randomized smoothing robustness schemes which provide high-confidence probabilistic robustness certificates. By exploiting the fact that patch attacks are more constrained than general sparse attacks, we derive meaningfully large robustness certificates against them. Additionally, in contrast to smoothing-based defenses against L_p and sparse attacks, our defense method against patch attacks is de-randomized, yielding improved, deterministic certificates. Compared to the existing patch certification method proposed by Chiang et al. (2020), which relies on interval bound propagation, our method can be trained significantly faster, achieves high clean and certified robust accuracy on CIFAR-10, and provides certificates at ImageNet scale. For example, for a 5-by-5 patch attack on CIFAR-10, our method achieves up to around 57.6% certified accuracy (with a classifier with around 83.8% clean accuracy), compared to at most 30.3% certified accuracy for the existing method (with a classifier with around 47.8% clean accuracy). Our results effectively establish a new state-of-the-art of certifiable defense against patch attacks on CIFAR-10 and ImageNet. Code is available at https://github.com/alevine0/patchSmoothing.



### MPM: Joint Representation of Motion and Position Map for Cell Tracking
- **Arxiv ID**: http://arxiv.org/abs/2002.10749v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10749v2)
- **Published**: 2020-02-25 09:06:55+00:00
- **Updated**: 2020-02-26 12:41:58+00:00
- **Authors**: Junya Hayashida, Kazuya Nishimura, Ryoma Bise
- **Comment**: 8 pages, 11 figures, Accepted in CVPR 2020
- **Journal**: None
- **Summary**: Conventional cell tracking methods detect multiple cells in each frame (detection) and then associate the detection results in successive time-frames (association). Most cell tracking methods perform the association task independently from the detection task. However, there is no guarantee of preserving coherence between these tasks, and lack of coherence may adversely affect tracking performance. In this paper, we propose the Motion and Position Map (MPM) that jointly represents both detection and association for not only migration but also cell division. It guarantees coherence such that if a cell is detected, the corresponding motion flow can always be obtained. It is a simple but powerful method for multi-object tracking in dense environments. We compared the proposed method with current tracking methods under various conditions in real biological images and found that it outperformed the state-of-the-art (+5.2\% improvement compared to the second-best).



### ScopeFlow: Dynamic Scene Scoping for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2002.10770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10770v2)
- **Published**: 2020-02-25 09:58:49+00:00
- **Updated**: 2020-05-04 08:19:29+00:00
- **Authors**: Aviram Bar-Haim, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol.   Using an existing low parameters architecture, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10%. The method also surpasses all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.



### Fully-automated Body Composition Analysis in Routine CT Imaging Using 3D Semantic Segmentation Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.10776v1
- **DOI**: 10.1007/s00330-020-07147-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10776v1)
- **Published**: 2020-02-25 10:17:19+00:00
- **Updated**: 2020-02-25 10:17:19+00:00
- **Authors**: Sven Koitka, Lennard Kroll, Eugen Malamutmann, Arzu Oezcelik, Felix Nensa
- **Comment**: None
- **Journal**: None
- **Summary**: Body tissue composition is a long-known biomarker with high diagnostic and prognostic value in cardiovascular, oncological and orthopaedic diseases, but also in rehabilitation medicine or drug dosage. In this study, the aim was to develop a fully automated, reproducible and quantitative 3D volumetry of body tissue composition from standard CT examinations of the abdomen in order to be able to offer such valuable biomarkers as part of routine clinical imaging. Therefore an in-house dataset of 40 CTs for training and 10 CTs for testing were fully annotated on every fifth axial slice with five different semantic body regions: abdominal cavity, bones, muscle, subcutaneous tissue, and thoracic cavity. Multi-resolution U-Net 3D neural networks were employed for segmenting these body regions, followed by subclassifying adipose tissue and muscle using known hounsfield unit limits. The S{\o}rensen Dice scores averaged over all semantic regions was 0.9553 and the intra-class correlation coefficients for subclassified tissues were above 0.99. Our results show that fully-automated body composition analysis on routine CT imaging can provide stable biomarkers across the whole abdomen and not just on L3 slices, which is historically the reference location for analysing body composition in the clinical routine.



### Layer-wise Conditioning Analysis in Exploring the Learning Dynamics of DNNs
- **Arxiv ID**: http://arxiv.org/abs/2002.10801v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.10801v3)
- **Published**: 2020-02-25 11:40:27+00:00
- **Updated**: 2020-07-29 13:30:54+00:00
- **Authors**: Lei Huang, Jie Qin, Li Liu, Fan Zhu, Ling Shao
- **Comment**: Accepted to ECCV 2020. The code is available at:
  https://github.com/huangleiBuaa/LayerwiseCA
- **Journal**: None
- **Summary**: Conditioning analysis uncovers the landscape of an optimization objective by exploring the spectrum of its curvature matrix. This has been well explored theoretically for linear models. We extend this analysis to deep neural networks (DNNs) in order to investigate their learning dynamics. To this end, we propose layer-wise conditioning analysis, which explores the optimization landscape with respect to each layer independently. Such an analysis is theoretically supported under mild assumptions that approximately hold in practice. Based on our analysis, we show that batch normalization (BN) can stabilize the training, but sometimes result in the false impression of a local minimum, which has detrimental effects on the learning. Besides, we experimentally observe that BN can improve the layer-wise conditioning of the optimization problem. Finally, we find that the last linear layer of a very deep residual network displays ill-conditioned behavior. We solve this problem by only adding one BN layer before the last linear layer, which achieves improved performance over the original and pre-activation residual networks.



### Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective
- **Arxiv ID**: http://arxiv.org/abs/2002.10826v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10826v3)
- **Published**: 2020-02-25 12:38:32+00:00
- **Updated**: 2020-04-12 16:32:33+00:00
- **Authors**: Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, Wenhui Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. Intuitively, we seek to expand the distribution of the tail classes by transferring from the head classes, so as to alleviate the distortion of the feature space. To this end, we propose to construct each feature into a "feature cloud". If a sample belongs to a tail class, the corresponding feature cloud will have relatively large distribution range, in compensation to its lack of diversity. It allows each tail sample to push the samples from other classes far away, recovering the intra-class diversity of tail classes. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.



### What BERT Sees: Cross-Modal Transfer for Visual Question Generation
- **Arxiv ID**: http://arxiv.org/abs/2002.10832v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.10832v3)
- **Published**: 2020-02-25 12:44:36+00:00
- **Updated**: 2020-12-16 15:48:35+00:00
- **Authors**: Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, Patrick Gallinari
- **Comment**: INLG 2020
- **Journal**: None
- **Summary**: Pre-trained language models have recently contributed to significant advances in NLP tasks. Recently, multi-modal versions of BERT have been developed, using heavy pre-training relying on vast corpora of aligned textual and image data, primarily applied to classification tasks such as VQA. In this paper, we are interested in evaluating the visual capabilities of BERT out-of-the-box, by avoiding pre-training made on supplementary data. We choose to study Visual Question Generation, a task of great interest for grounded dialog, that enables to study the impact of each modality (as input can be visual and/or textual). Moreover, the generation aspect of the task requires an adaptation since BERT is primarily designed as an encoder. We introduce BERT-gen, a BERT-based architecture for text generation, able to leverage on either mono- or multi- modal representations. The results reported under different configurations indicate an innate capacity for BERT-gen to adapt to multi-modal data and text generation, even with few data available, avoiding expensive pre-training. The proposed model obtains substantial improvements over the state-of-the-art on two established VQG datasets.



### Gesture recognition with 60GHz 802.11 waveforms
- **Arxiv ID**: http://arxiv.org/abs/2002.10836v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10836v1)
- **Published**: 2020-02-25 12:49:45+00:00
- **Updated**: 2020-02-25 12:49:45+00:00
- **Authors**: Eran Hof, Amichai Sanderovich, Evyatar Hemo
- **Comment**: None
- **Journal**: None
- **Summary**: Gesture recognition application over 802.11 ad/y waveforms is developed. Simultaneous gestures of slider-control and two-finger gesture for switching are detected based on Golay sequences of channel estimation fields of the packets.



### Optimal least-squares solution to the hand-eye calibration problem
- **Arxiv ID**: http://arxiv.org/abs/2002.10838v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.10838v2)
- **Published**: 2020-02-25 12:59:06+00:00
- **Updated**: 2020-05-19 15:14:39+00:00
- **Authors**: Amit Dekel, Linus Härenstam-Nielsen, Sergio Caccamo
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We propose a least-squares formulation to the noisy hand-eye calibration problem using dual-quaternions, and introduce efficient algorithms to find the exact optimal solution, based on analytic properties of the problem, avoiding non-linear optimization. We further present simple analytic approximate solutions which provide remarkably good estimations compared to the exact solution. In addition, we show how to generalize our solution to account for a given extrinsic prior in the cost function. To the best of our knowledge our algorithm is the most efficient approach to optimally solve the hand-eye calibration problem.



### Circle Loss: A Unified Perspective of Pair Similarity Optimization
- **Arxiv ID**: http://arxiv.org/abs/2002.10857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10857v2)
- **Published**: 2020-02-25 13:56:40+00:00
- **Updated**: 2020-06-15 08:15:47+00:00
- **Authors**: Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.



### Cross-layer Feature Pyramid Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.10864v1
- **DOI**: 10.1109/TIP.2021.3072811
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10864v1)
- **Published**: 2020-02-25 14:06:27+00:00
- **Updated**: 2020-02-25 14:06:27+00:00
- **Authors**: Zun Li, Congyan Lang, Junhao Liew, Qibin Hou, Yidong Li, Jiashi Feng
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Feature pyramid network (FPN) based models, which fuse the semantics and salient details in a progressive manner, have been proven highly effective in salient object detection. However, it is observed that these models often generate saliency maps with incomplete object structures or unclear object boundaries, due to the \emph{indirect} information propagation among distant layers that makes such fusion structure less effective. In this work, we propose a novel Cross-layer Feature Pyramid Network (CFPN), in which direct cross-layer communication is enabled to improve the progressive fusion in salient object detection. Specifically, the proposed network first aggregates multi-scale features from different layers into feature maps that have access to both the high- and low-level information. Then, it distributes the aggregated features to all the involved layers to gain access to richer context. In this way, the distributed features per layer own both semantics and salient details from all other layers simultaneously, and suffer reduced loss of important information. Extensive experimental results over six widely used salient object detection benchmarks and with three popular backbones clearly demonstrate that CFPN can accurately locate fairly complete salient regions and effectively segment the object boundaries.



### PointAugment: an Auto-Augmentation Framework for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.10876v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10876v2)
- **Published**: 2020-02-25 14:25:01+00:00
- **Updated**: 2020-03-20 02:56:33+00:00
- **Authors**: Ruihui Li, Xianzhi Li, Pheng-Ann Heng, Chi-Wing Fu
- **Comment**: Camera-Ready Version for CVPR 2020 (Oral); code is
  https://github.com/liruihui/PointAugment/
- **Journal**: None
- **Summary**: We present PointAugment, a new auto-augmentation framework that automatically optimizes and augments point cloud samples to enrich the data diversity when we train a classification network. Different from existing auto-augmentation methods for 2D images, PointAugment is sample-aware and takes an adversarial learning strategy to jointly optimize an augmentor network and a classifier network, such that the augmentor can learn to produce augmented samples that best fit the classifier. Moreover, we formulate a learnable point augmentation function with a shape-wise transformation and a point-wise displacement, and carefully design loss functions to adopt the augmented samples based on the learning progress of the classifier. Extensive experiments also confirm PointAugment's effectiveness and robustness to improve the performance of various networks on shape classification and retrieval.



### 3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.10893v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10893v5)
- **Published**: 2020-02-25 14:33:50+00:00
- **Updated**: 2021-04-27 15:31:54+00:00
- **Authors**: Iñigo Alonso, Luis Riazuelo, Luis Montesano, Ana C. Murillo
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: LIDAR semantic segmentation, which assigns a semantic label to each 3D point measured by the LIDAR, is becoming an essential task for many robotic applications such as autonomous driving. Fast and efficient semantic segmentation methods are needed to match the strong computational and temporal restrictions of many of these real-world applications.   This work presents 3D-MiniNet, a novel approach for LIDAR semantic segmentation that combines 3D and 2D learning layers. It first learns a 2D representation from the raw points through a novel projection which extracts local and global information from the 3D data. This representation is fed to an efficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D semantic segmentation. These 2D semantic labels are re-projected back to the 3D space and enhanced through a post-processing module. The main novelty in our strategy relies on the projection learning module. Our detailed ablation study shows how each component contributes to the final performance of 3D-MiniNet. We validate our approach on well known public benchmarks (SemanticKITTI and KITTI), where 3D-MiniNet gets state-of-the-art results while being faster and more parameter-efficient than previous methods.



### Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs
- **Arxiv ID**: http://arxiv.org/abs/2002.10964v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10964v2)
- **Published**: 2020-02-25 15:30:17+00:00
- **Updated**: 2020-02-28 10:53:50+00:00
- **Authors**: Sangwoo Mo, Minsu Cho, Jinwoo Shin
- **Comment**: Tech report; High resolution images are in
  https://github.com/sangwoomo/FreezeD
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have shown outstanding performance on a wide range of problems in computer vision, graphics, and machine learning, but often require numerous training data and heavy computational resources. To tackle this issue, several methods introduce a transfer learning technique in GAN training. They, however, are either prone to overfitting or limited to learning small distribution shifts. In this paper, we show that simple fine-tuning of GANs with frozen lower layers of the discriminator performs surprisingly well. This simple baseline, FreezeD, significantly outperforms previous techniques used in both unconditional and conditional GANs. We demonstrate the consistent effect using StyleGAN and SNGAN-projection architectures on several datasets of Animal Face, Anime Face, Oxford Flower, CUB-200-2011, and Caltech-256 datasets. The code and results are available at https://github.com/sangwoomo/FreezeD.



### Fault Diagnosis in Microelectronics Attachment via Deep Learning Analysis of 3D Laser Scans
- **Arxiv ID**: http://arxiv.org/abs/2002.10974v1
- **DOI**: 10.1109/TIE.2019.2931220
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10974v1)
- **Published**: 2020-02-25 15:38:11+00:00
- **Updated**: 2020-02-25 15:38:11+00:00
- **Authors**: Nikolaos Dimitriou, Lampros Leontaris, Thanasis Vafeiadis, Dimosthenis Ioannidis, Tracy Wotherspoon, Gregory Tinker, Dimitrios Tzovaras
- **Comment**: 10 pages, 12 figures. in IEEE Transactions on Industrial Electronics,
  2019 (early access)
- **Journal**: None
- **Summary**: A common source of defects in manufacturing miniature Printed Circuits Boards (PCB) is the attachment of silicon die or other wire bondable components on a Liquid Crystal Polymer (LCP) substrate. Typically, a conductive glue is dispensed prior to attachment with defects caused either by insufficient or excessive glue. The current practice in electronics industry is to examine the deposited glue by a human operator a process that is both time consuming and inefficient especially in preproduction runs where the error rate is high. In this paper we propose a system that automates fault diagnosis by accurately estimating the volume of glue deposits before and even after die attachment. To this end a modular scanning system is deployed that produces high resolution point clouds whereas the actual estimation of glue volume is performed by (R)egression-Net (RNet), a 3D Convolutional Neural Network (3DCNN). RNet outperforms other deep architectures and is able to estimate the volume either directly from the point cloud of a glue deposit or more interestingly after die attachment when only a small part of glue is visible around each die. The entire methodology is evaluated under operational conditions where the proposed system achieves accurate results without delaying the manufacturing process.



### MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2002.10979v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.10979v4)
- **Published**: 2020-02-25 15:43:46+00:00
- **Updated**: 2020-05-05 02:22:42+00:00
- **Authors**: Yushi Lan, Yuan Liu, Maoqing Tian, Xinchi Zhou, Xuesen Zhang, Shuai Yi, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the "Semantic Adversarial Branch" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce "Semantic Fusion Branch" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function "Semantic Diversity Loss" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks.



### A Deep Learning Framework for Simulation and Defect Prediction Applied in Microelectronics
- **Arxiv ID**: http://arxiv.org/abs/2002.10986v1
- **DOI**: 10.1016/j.simpat.2019.102063
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.10986v1)
- **Published**: 2020-02-25 15:54:33+00:00
- **Updated**: 2020-02-25 15:54:33+00:00
- **Authors**: Nikolaos Dimitriou, Lampros Leontaris, Thanasis Vafeiadis, Dimosthenis Ioannidis, Tracy Wotherspoon, Gregory Tinker, Dimitrios Tzovaras
- **Comment**: 21 pages, 5 figures
- **Journal**: Simulation Modelling Practice and Theory, Volume 100, 2020
- **Summary**: The prediction of upcoming events in industrial processes has been a long-standing research goal since it enables optimization of manufacturing parameters, planning of equipment maintenance and more importantly prediction and eventually prevention of defects. While existing approaches have accomplished substantial progress, they are mostly limited to processing of one dimensional signals or require parameter tuning to model environmental parameters. In this paper, we propose an alternative approach based on deep neural networks that simulates changes in the 3D structure of a monitored object in a batch based on previous 3D measurements. In particular, we propose an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to model the geometric variations in manufacturing parameters and predict upcoming events related to sub-optimal performance. We validate our framework on a microelectronics use-case using the recently published PCB scans dataset where we simulate changes on the shape and volume of glue deposited on an Liquid Crystal Polymer (LCP) substrate before the attachment of integrated circuits (IC). Experimental evaluation examines the impact of different choices in the cost function during training and shows that the proposed method can be efficiently used for defect prediction.



### Relevant-features based Auxiliary Cells for Energy Efficient Detection of Natural Errors
- **Arxiv ID**: http://arxiv.org/abs/2002.11052v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11052v2)
- **Published**: 2020-02-25 17:22:10+00:00
- **Updated**: 2020-02-26 01:30:08+00:00
- **Authors**: Sai Aparna Aketi, Priyadarshini Panda, Kaushik Roy
- **Comment**: 16 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated state-of-the-art performance on many classification tasks. However, they have no inherent capability to recognize when their predictions are wrong. There have been several efforts in the recent past to detect natural errors but the suggested mechanisms pose additional energy requirements. To address this issue, we propose an ensemble of classifiers at hidden layers to enable energy efficient detection of natural errors. In particular, we append Relevant-features based Auxiliary Cells (RACs) which are class specific binary linear classifiers trained on relevant features. The consensus of RACs is used to detect natural errors. Based on combined confidence of RACs, classification can be terminated early, thereby resulting in energy efficient detection. We demonstrate the effectiveness of our technique on various image classification datasets such as CIFAR-10, CIFAR-100 and Tiny-ImageNet.



### Ground Texture Based Localization Using Compact Binary Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2002.11061v2
- **DOI**: 10.1109/ICRA40945.2020.9197221
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.11061v2)
- **Published**: 2020-02-25 17:31:41+00:00
- **Updated**: 2020-12-18 12:32:22+00:00
- **Authors**: Jan Fabian Schmid, Stephan F. Simon, Rudolf Mester
- **Comment**: Published at 2020 IEEE International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: Ground texture based localization is a promising approach to achieve high-accuracy positioning of vehicles. We present a self-contained method that can be used for global localization as well as for subsequent local localization updates, i.e. it allows a robot to localize without any knowledge of its current whereabouts, but it can also take advantage of a prior pose estimate to reduce computation time significantly. Our method is based on a novel matching strategy, which we call identity matching, that is based on compact binary feature descriptors. Identity matching treats pairs of features as matches only if their descriptors are identical. While other methods for global localization are faster to compute, our method reaches higher localization success rates, and can switch to local localization after the initial localization.



### RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2003.01811v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01811v2)
- **Published**: 2020-02-25 18:19:12+00:00
- **Updated**: 2020-04-01 17:27:05+00:00
- **Authors**: Bing Han, Gopalakrishnan Srinivasan, Kaushik Roy
- **Comment**: to be published in CVPR'20
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have recently attracted significant research interest as the third generation of artificial neural networks that can enable low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained Analog Neural Network (ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of integrate-and-fire neurons with "proper" firing thresholds. The converted SNNs typically incur loss in accuracy compared to that provided by the original ANN and require sizable number of inference time-steps to achieve the best accuracy. We find that performance degradation in the converted SNN stems from using "hard reset" spiking neuron that is driven to fixed reset potential once its membrane potential exceeds the firing threshold, leading to information loss during SNN inference. We propose ANN-SNN conversion using "soft reset" spiking neuron model, referred to as Residual Membrane Potential (RMP) spiking neuron, which retains the "residual" membrane potential above threshold at the firing instants. We demonstrate near loss-less ANN-SNN conversion using RMP neurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets including CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet (73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best inference accuracy provided by the converted SNN with "hard reset" spiking neurons using 2-8 times fewer inference time-steps across network architectures and datasets.



### DDet: Dual-path Dynamic Enhancement Network for Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2002.11079v1
- **DOI**: 10.1109/LSP.2020.2978410
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11079v1)
- **Published**: 2020-02-25 18:24:51+00:00
- **Updated**: 2020-02-25 18:24:51+00:00
- **Authors**: Yukai Shi, Haoyu Zhong, Zhijing Yang, Xiaojun Yang, Liang Lin
- **Comment**: Code address: https://github.com/ykshi/DDet
- **Journal**: None
- **Summary**: Different from traditional image super-resolution task, real image super-resolution(Real-SR) focus on the relationship between real-world high-resolution(HR) and low-resolution(LR) image. Most of the traditional image SR obtains the LR sample by applying a fixed down-sampling operator. Real-SR obtains the LR and HR image pair by incorporating different quality optical sensors. Generally, Real-SR has more challenges as well as broader application scenarios. Previous image SR methods fail to exhibit similar performance on Real-SR as the image data is not aligned inherently. In this article, we propose a Dual-path Dynamic Enhancement Network(DDet) for Real-SR, which addresses the cross-camera image mapping by realizing a dual-way dynamic sub-pixel weighted aggregation and refinement. Unlike conventional methods which stack up massive convolutional blocks for feature representation, we introduce a content-aware framework to study non-inherently aligned image pair in image SR issue. First, we use a content-adaptive component to exhibit the Multi-scale Dynamic Attention(MDA). Second, we incorporate a long-term skip connection with a Coupled Detail Manipulation(CDM) to perform collaborative compensation and manipulation. The above dual-path model is joint into a unified model and works collaboratively. Extensive experiments on the challenging benchmarks demonstrate the superiority of our model.



### Model Watermarking for Image Processing Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.11088v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.11088v1)
- **Published**: 2020-02-25 18:36:18+00:00
- **Updated**: 2020-02-25 18:36:18+00:00
- **Authors**: Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo Zhou, Hao Cui, Nenghai Yu
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Deep learning has achieved tremendous success in numerous industrial applications. As training a good model often needs massive high-quality data and computation resources, the learned models often have significant business values. However, these valuable deep models are exposed to a huge risk of infringements. For example, if the attacker has the full information of one target model including the network structure and weights, the model can be easily finetuned on new datasets. Even if the attacker can only access the output of the target model, he/she can still train another similar surrogate model by generating a large scale of input-output training pairs. How to protect the intellectual property of deep models is a very important but seriously under-researched problem. There are a few recent attempts at classification network protection only. In this paper, we propose the first model watermarking framework for protecting image processing models. To achieve this goal, we leverage the spatial invisible watermarking mechanism. Specifically, given a black-box target model, a unified and invisible watermark is hidden into its outputs, which can be regarded as a special task-agnostic barrier. In this way, when the attacker trains one surrogate model by using the input-output pairs of the target model, the hidden watermark will be learned and extracted afterward. To enable watermarks from binary bits to high-resolution images, both traditional and deep spatial invisible watermarking mechanism are considered. Experiments demonstrate the robustness of the proposed watermarking mechanism, which can resist surrogate models learned with different network structures and objective functions. Besides deep models, the proposed method is also easy to be extended to protect data and traditional image processing algorithms.



### Toward fast and accurate human pose estimation via soft-gated skip connections
- **Arxiv ID**: http://arxiv.org/abs/2002.11098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11098v1)
- **Published**: 2020-02-25 18:51:51+00:00
- **Updated**: 2020-02-25 18:51:51+00:00
- **Authors**: Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: Accepted to FG 2020 (oral)
- **Journal**: None
- **Summary**: This paper is on highly accurate and highly efficient human pose estimation. Recent works based on Fully Convolutional Networks (FCNs) have demonstrated excellent results for this difficult problem. While residual connections within FCNs have proved to be quintessential for achieving high accuracy, we re-analyze this design choice in the context of improving both the accuracy and the efficiency over the state-of-the-art. In particular, we make the following contributions: (a) We propose gated skip connections with per-channel learnable parameters to control the data flow for each channel within the module within the macro-module. (b) We introduce a hybrid network that combines the HourGlass and U-Net architectures which minimizes the number of identity connections within the network and increases the performance for the same parameter budget. Our model achieves state-of-the-art results on the MPII and LSP datasets. In addition, with a reduction of 3x in model size and complexity, we show no decrease in performance when compared to the original HourGlass network.



### On Feature Normalization and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.11102v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11102v3)
- **Published**: 2020-02-25 18:59:05+00:00
- **Updated**: 2021-03-30 18:00:00+00:00
- **Authors**: Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, Kilian Q. Weinberger
- **Comment**: CVPR 2021. Code is available at https://github.com/Boyiliee/MoEx
- **Journal**: None
- **Summary**: The moments (a.k.a., mean and standard deviation) of latent features are often removed as noise when training image recognition models, to increase stability and reduce training time. However, in the field of image generation, the moments play a much more central role. Studies have shown that the moments extracted from instance normalization and positional normalization can roughly capture style and shape information of an image. Instead of being discarded, these moments are instrumental to the generation process. In this paper we propose Moment Exchange, an implicit data augmentation method that encourages the model to utilize the moment information also for recognition models. Specifically, we replace the moments of the learned features of one training image by those of another, and also interpolate the target labels -- forcing the model to extract training signal from the moments in addition to the normalized features. As our approach is fast, operates entirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing augmentation approaches. We demonstrate its efficacy across several recognition benchmark data sets where it improves the generalization capability of highly competitive baseline networks with remarkable consistency.



### Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes with Applications to Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.11169v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11169v4)
- **Published**: 2020-02-25 20:50:47+00:00
- **Updated**: 2021-06-07 15:50:10+00:00
- **Authors**: William Paul, I-Jeng Wang, Fady Alajaji, Philippe Burlina
- **Comment**: MIT Neural Computation 2021, Vol 33(3), pp. 802--826
- **Journal**: None
- **Summary**: Our work focuses on unsupervised and generative methods that address the following goals: (a) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (b) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (c) developing anomaly detection methods that leverage representations learned in (a). For (a), we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For (b), we derive an analytical result (Lemma 1) that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in our loss function, we then show empirically that, for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement trade space, when compared to other state of the art methods, with quality assessed via the Frechet Inception Distance (FID), and disentanglement via mutual information gap. For (c), we design several systems for anomaly detection exploiting representations learned in (a), and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. The above contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.



### Geometric Fusion via Joint Delay Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2002.11201v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2002.11201v1)
- **Published**: 2020-02-25 22:20:12+00:00
- **Updated**: 2020-02-25 22:20:12+00:00
- **Authors**: Elchanan Solomon, Paul Bendich
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce geometric and topological methods to develop a new framework for fusing multi-sensor time series. This framework consists of two steps: (1) a joint delay embedding, which reconstructs a high-dimensional state space in which our sensors correspond to observation functions, and (2) a simple orthogonalization scheme, which accounts for tangencies between such observation functions, and produces a more diversified geometry on the embedding space. We conclude with some synthetic and real-world experiments demonstrating that our framework outperforms traditional metric fusion methods.



### Style Transfer for Light Field Photography
- **Arxiv ID**: http://arxiv.org/abs/2002.11220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.11220v1)
- **Published**: 2020-02-25 23:21:47+00:00
- **Updated**: 2020-02-25 23:21:47+00:00
- **Authors**: David Hart, Jessica Greenland, Bryan Morse
- **Comment**: To be presented at WACV 2020
- **Journal**: None
- **Summary**: As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. Instead, the proposed method backpropagates the loss through the network, and the process is iterated to optimize (essentially overfit) the resulting stylization for a single light field image alone. The network architecture allows for the incorporation of pre-trained fast monocular stylization networks while avoiding the need for a large light field training set.



