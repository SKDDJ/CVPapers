# Arxiv Papers in cs.CV on 2020-02-28
### Cross-modality Person re-identification with Shared-Specific Feature Transfer
- **Arxiv ID**: http://arxiv.org/abs/2002.12489v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12489v3)
- **Published**: 2020-02-28 00:18:45+00:00
- **Updated**: 2020-03-12 08:52:17+00:00
- **Authors**: Yan Lu, Yue Wu, Bin Liu, Tianzhu Zhang, Baopu Li, Qi Chu, Nenghai Yu
- **Comment**: To appear at CVPR2020
- **Journal**: None
- **Summary**: Cross-modality person re-identification (cm-ReID) is a challenging but key technology for intelligent video analysis. Existing works mainly focus on learning common representation by embedding different modalities into a same feature space. However, only learning the common characteristics means great information loss, lowering the upper bound of feature distinctiveness. In this paper, we tackle the above limitation by proposing a novel cross-modality shared-specific feature transfer algorithm (termed cm-SSFT) to explore the potential of both the modality-shared information and the modality-specific characteristics to boost the re-identification performance. We model the affinities of different modality samples according to the shared features and then transfer both shared and specific features among and across modalities. We also propose a complementary feature learning strategy including modality adaption, project adversarial learning and reconstruction enhancement to learn discriminative and complementary shared and specific features of each modality, respectively. The entire cm-SSFT algorithm can be trained in an end-to-end manner. We conducted comprehensive experiments to validate the superiority of the overall algorithm and the effectiveness of each component. The proposed algorithm significantly outperforms state-of-the-arts by 22.5% and 19.3% mAP on the two mainstream benchmark datasets SYSU-MM01 and RegDB, respectively.



### Road Curb Detection and Localization with Monocular Forward-view Vehicle Camera
- **Arxiv ID**: http://arxiv.org/abs/2002.12492v1
- **DOI**: 10.1109/TITS.2018.2878652
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12492v1)
- **Published**: 2020-02-28 00:24:18+00:00
- **Updated**: 2020-02-28 00:24:18+00:00
- **Authors**: Stanislav Panev, Francisco Vicente, Fernando De la Torre, VÃ©ronique Prinet
- **Comment**: 17 pages, 21 figures, IEEE Transactions on Intelligent Transportation
  Systems
- **Journal**: IEEE Transactions on Intelligent Transportation Systems (Volume:
  20, Issue: 9, Sept. 2019)
- **Summary**: We propose a robust method for estimating road curb 3D parameters (size, location, orientation) using a calibrated monocular camera equipped with a fisheye lens. Automatic curb detection and localization is particularly important in the context of Advanced Driver Assistance System (ADAS), i.e. to prevent possible collision and damage of the vehicle's bumper during perpendicular and diagonal parking maneuvers. Combining 3D geometric reasoning with advanced vision-based detection methods, our approach is able to estimate the vehicle to curb distance in real time with mean accuracy of more than 90%, as well as its orientation, height and depth.   Our approach consists of two distinct components - curb detection in each individual video frame and temporal analysis. The first part comprises of sophisticated curb edges extraction and parametrized 3D curb template fitting. Using a few assumptions regarding the real world geometry, we can thus retrieve the curb's height and its relative position w.r.t. the moving vehicle on which the camera is mounted. Support Vector Machine (SVM) classifier fed with Histograms of Oriented Gradients (HOG) is used for appearance-based filtering out outliers. In the second part, the detected curb regions are tracked in the temporal domain, so as to perform a second pass of false positives rejection.   We have validated our approach on a newly collected database of 11 videos under different conditions. We have used point-wise LIDAR measurements and manual exhaustive labels as a ground truth.



### Detecting Patch Adversarial Attacks with Image Residuals
- **Arxiv ID**: http://arxiv.org/abs/2002.12504v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12504v2)
- **Published**: 2020-02-28 01:28:22+00:00
- **Updated**: 2020-03-02 16:19:17+00:00
- **Authors**: Marius Arvinte, Ahmed Tewfik, Sriram Vishwanath
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an adversarial sample detection algorithm based on image residuals, specifically designed to guard against patch-based attacks. The image residual is obtained as the difference between an input image and a denoised version of it, and a discriminator is trained to distinguish between clean and adversarial samples. More precisely, we use a wavelet domain algorithm for denoising images and demonstrate that the obtained residuals act as a digital fingerprint for adversarial attacks. To emulate the limitations of a physical adversary, we evaluate the performance of our approach against localized (patch-based) adversarial attacks, including in settings where the adversary has complete knowledge about the detection scheme. Our results show that the proposed detection method generalizes to previously unseen, stronger attacks and that it is able to reduce the success rate (conversely, increase the computational effort) of an adaptive attacker.



### DGST : Discriminator Guided Scene Text detector
- **Arxiv ID**: http://arxiv.org/abs/2002.12509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12509v1)
- **Published**: 2020-02-28 01:47:36+00:00
- **Updated**: 2020-02-28 01:47:36+00:00
- **Authors**: Jinyuan Zhao, Yanna Wang, Baihua Xiao, Cunzhao Shi, Fuxi Jia, Chunheng Wang
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Scene text detection task has attracted considerable attention in computer vision because of its wide application. In recent years, many researchers have introduced methods of semantic segmentation into the task of scene text detection, and achieved promising results. This paper proposes a detector framework based on the conditional generative adversarial networks to improve the segmentation effect of scene text detection, called DGST (Discriminator Guided Scene Text detector). Instead of binary text score maps generated by some existing semantic segmentation based methods, we generate a multi-scale soft text score map with more information to represent the text position more reasonably, and solve the problem of text pixel adhesion in the process of text extraction. Experiments on standard datasets demonstrate that the proposed DGST brings noticeable gain and outperforms state-of-the-art methods. Specifically, it achieves an F-measure of 87% on ICDAR 2015 dataset.



### Utilizing Network Properties to Detect Erroneous Inputs
- **Arxiv ID**: http://arxiv.org/abs/2002.12520v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12520v3)
- **Published**: 2020-02-28 03:20:55+00:00
- **Updated**: 2023-03-24 19:01:41+00:00
- **Authors**: Matt Gorbett, Nathaniel Blanchard
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are vulnerable to a wide range of erroneous inputs such as adversarial, corrupted, out-of-distribution, and misclassified examples. In this work, we train a linear SVM classifier to detect these four types of erroneous data using hidden and softmax feature vectors of pre-trained neural networks. Our results indicate that these faulty data types generally exhibit linearly separable activation properties from correct examples, giving us the ability to reject bad inputs with no extra training or overhead. We experimentally validate our findings across a diverse range of datasets, domains, pre-trained models, and adversarial attacks.



### Are L2 adversarial examples intrinsically different?
- **Arxiv ID**: http://arxiv.org/abs/2002.12527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12527v2)
- **Published**: 2020-02-28 03:42:52+00:00
- **Updated**: 2020-09-07 04:37:21+00:00
- **Authors**: Mingxuan Li, Jingyuan Wang, Yufan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network (DDN) has achieved notable success in various tasks, including many security concerning scenarios. However, a considerable amount of work has proved its vulnerability to adversaries. We unravel the properties that can intrinsically differentiate adversarial examples and normal inputs through theoretical analysis. That is, adversarial examples generated by $L_2$ attacks usually have larger input sensitivity which can be used to identify them efficiently. We also found that those generated by $L_\infty$ attacks will be different enough in the pixel domain to be detected empirically. To verify our analysis, we proposed a \textbf{G}uided \textbf{C}omplementary \textbf{D}efense module (\textbf{GCD}) integrating detection and recovery processes. When compared with adversarial detection methods, our detector achieves a detection AUC of over 0.98 against most of the attacks. When comparing our guided rectifier with commonly used adversarial training methods and other rectification methods, our rectifier outperforms them by a large margin. We achieve a recovered classification accuracy of up to 99\% on MNIST, 89\% on CIFAR-10, and 87\% on ImageNet subsets against $L_2$ attacks. Furthermore, under the white-box setting, our holistic defensive module shows a promising degree of robustness. Thus, we confirm that at least $L_2$ adversarial examples are intrinsically different enough from normal inputs both theoretically and empirically. And we shed light upon designing simple yet effective defensive methods with these properties.



### A Video Analysis Method on Wanfang Dataset via Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2002.12535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12535v1)
- **Published**: 2020-02-28 04:09:53+00:00
- **Updated**: 2020-02-28 04:09:53+00:00
- **Authors**: Jinlong Kang, Jiaxiang Zheng, Heng Bai, Xiaoting Xue, Yang Zhou, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The topic of object detection has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as small object, compact and dense or highly overlapping object. Existing methods can detect multiple objects wonderfully, but because of the slight changes between frames, the detection effect of the model will become unstable, the detection results may result in dropping or increasing the object. In the pedestrian flow detection task, such phenomenon can not accurately calculate the flow. To solve this problem, in this paper, we describe the new function for real-time multi-object detection in sports competition and pedestrians flow detection in public based on deep learning. Our work is to extract a video clip and solve this frame of clips efficiently. More specfically, our algorithm includes two stages: judge method and optimization method. The judge can set a maximum threshold for better results under the model, the threshold value corresponds to the upper limit of the algorithm with better detection results. The optimization method to solve detection jitter problem. Because of the occurrence of frame hopping in the video, and it will result in the generation of video fragments discontinuity. We use optimization algorithm to get the key value, and then the detection result value of index is replaced by key value to stabilize the change of detection result sequence. Based on the proposed algorithm, we adopt wanfang sports competition dataset as the main test dataset and our own test dataset for YOLOv3-Abnormal Number Version(YOLOv3-ANV), which is 5.4% average improvement compared with existing methods. Also, video above the threshold value can be obtained for further analysis. Spontaneously, our work also can used for pedestrians flow detection and pedestrian alarm tasks.



### Automated classification of stems and leaves of potted plants based on point cloud data
- **Arxiv ID**: http://arxiv.org/abs/2002.12536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12536v1)
- **Published**: 2020-02-28 04:15:38+00:00
- **Updated**: 2020-02-28 04:15:38+00:00
- **Authors**: Zichu Liu, Qing Zhang, Pei Wang, Zhen Li, Huiru Wang
- **Comment**: 31 pages,15 figures, 8 tables
- **Journal**: None
- **Summary**: The accurate classification of plant organs is a key step in monitoring the growing status and physiology of plants. A classification method was proposed to classify the leaves and stems of potted plants automatically based on the point cloud data of the plants, which is a nondestructive acquisition. The leaf point training samples were automatically extracted by using the three-dimensional convex hull algorithm, while stem point training samples were extracted by using the point density of a two-dimensional projection. The two training sets were used to classify all the points into leaf points and stem points by utilizing the support vector machine (SVM) algorithm. The proposed method was tested by using the point cloud data of three potted plants and compared with two other methods, which showed that the proposed method can classify leaf and stem points accurately and efficiently.



### Hand-Priming in Object Localization for Assistive Egocentric Vision
- **Arxiv ID**: http://arxiv.org/abs/2002.12557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2002.12557v1)
- **Published**: 2020-02-28 05:32:36+00:00
- **Updated**: 2020-02-28 05:32:36+00:00
- **Authors**: Kyungjun Lee, Abhinav Shrivastava, Hernisa Kacorri
- **Comment**: the 2020 Winter Conference on Applications of Computer Vision (WACV
  2020)
- **Journal**: None
- **Summary**: Egocentric vision holds great promises for increasing access to visual information and improving the quality of life for people with visual impairments, with object recognition being one of the daily challenges for this population. While we strive to improve recognition performance, it remains difficult to identify which object is of interest to the user; the object may not even be included in the frame due to challenges in camera aiming without visual feedback. Also, gaze information, commonly used to infer the area of interest in egocentric vision, is often not dependable. However, blind users often tend to include their hand either interacting with the object that they wish to recognize or simply placing it in proximity for better camera aiming. We propose localization models that leverage the presence of the hand as the contextual information for priming the center area of the object of interest. In our approach, hand segmentation is fed to either the entire localization network or its last convolutional layers. Using egocentric datasets from sighted and blind individuals, we show that the hand-priming achieves higher precision than other approaches, such as fine-tuning, multi-class, and multi-task learning, which also encode hand-object interactions in localization.



### MANet: Multimodal Attention Network based Point- View fusion for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.12573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12573v1)
- **Published**: 2020-02-28 07:00:14+00:00
- **Updated**: 2020-02-28 07:00:14+00:00
- **Authors**: Yaxin Zhao, Jichao Jiao, Tangkun Zhang
- **Comment**: 8 pages,6 figures
- **Journal**: None
- **Summary**: 3D shape recognition has attracted more and more attention as a task of 3D vision research. The proliferation of 3D data encourages various deep learning methods based on 3D data. Now there have been many deep learning models based on point-cloud data or multi-view data alone. However, in the era of big data, integrating data of two different modals to obtain a unified 3D shape descriptor is bound to improve the recognition accuracy. Therefore, this paper proposes a fusion network based on multimodal attention mechanism for 3D shape recognition. Considering the limitations of multi-view data, we introduce a soft attention scheme, which can use the global point-cloud features to filter the multi-view features, and then realize the effective fusion of the two features. More specifically, we obtain the enhanced multi-view features by mining the contribution of each multi-view image to the overall shape recognition, and then fuse the point-cloud features and the enhanced multi-view features to obtain a more discriminative 3D shape descriptor. We have performed relevant experiments on the ModelNet40 dataset, and experimental results verify the effectiveness of our method.



### Learning to See: You Are What You See
- **Arxiv ID**: http://arxiv.org/abs/2003.00902v1
- **DOI**: 10.1145/3306211.3320143
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.00902v1)
- **Published**: 2020-02-28 07:12:52+00:00
- **Updated**: 2020-02-28 07:12:52+00:00
- **Authors**: Memo Akten, Rebecca Fiebrink, Mick Grierson
- **Comment**: Presented as an Art Paper at SIGGRAPH 2019
- **Journal**: ACM SIGGRAPH 2019 Art Gallery July 2019 Article No 13 Pages 1 to 6
- **Summary**: The authors present a visual instrument developed as part of the creation of the artwork Learning to See. The artwork explores bias in artificial neural networks and provides mechanisms for the manipulation of specifically trained for real-world representations. The exploration of these representations acts as a metaphor for the process of developing a visual understanding and/or visual vocabulary of the world. These representations can be explored and manipulated in real time, and have been produced in such a way so as to reflect specific creative perspectives that call into question the relationship between how both artificial neural networks and humans may construct meaning.



### Class-Specific Blind Deconvolutional Phase Retrieval Under a Generative Prior
- **Arxiv ID**: http://arxiv.org/abs/2002.12578v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12578v1)
- **Published**: 2020-02-28 07:36:28+00:00
- **Updated**: 2020-02-28 07:36:28+00:00
- **Authors**: Fahad Shamshad, Ali Ahmed
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we consider the highly ill-posed problem of jointly recovering two real-valued signals from the phaseless measurements of their circular convolution. The problem arises in various imaging modalities such as Fourier ptychography, X-ray crystallography, and in visible light communication. We propose to solve this inverse problem using alternating gradient descent algorithm under two pretrained deep generative networks as priors; one is trained on sharp images and the other on blur kernels. The proposed recovery algorithm strives to find a sharp image and a blur kernel in the range of the respective pre-generators that \textit{best} explain the forward measurement model. In doing so, we are able to reconstruct quality image estimates. Moreover, the numerics show that the proposed approach performs well on the challenging measurement models that reflect the physically realizable imaging systems and is also robust to noise



### Neural Inheritance Relation Guided One-Shot Layer Assignment Search
- **Arxiv ID**: http://arxiv.org/abs/2002.12580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12580v1)
- **Published**: 2020-02-28 07:40:48+00:00
- **Updated**: 2020-02-28 07:40:48+00:00
- **Authors**: Rang Meng, Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu
- **Comment**: AAAI2020
- **Journal**: None
- **Summary**: Layer assignment is seldom picked out as an independent research topic in neural architecture search. In this paper, for the first time, we systematically investigate the impact of different layer assignments to the network performance by building an architecture dataset of layer assignment on CIFAR-100. Through analyzing this dataset, we discover a neural inheritance relation among the networks with different layer assignments, that is, the optimal layer assignments for deeper networks always inherit from those for shallow networks. Inspired by this neural inheritance relation, we propose an efficient one-shot layer assignment search approach via inherited sampling. Specifically, the optimal layer assignment searched in the shallow network can be provided as a strong sampling priori to train and search the deeper ones in supernet, which extremely reduces the network search space. Comprehensive experiments carried out on CIFAR-100 illustrate the efficiency of our proposed method. Our search results are strongly consistent with the optimal ones directly selected from the architecture dataset. To further confirm the generalization of our proposed method, we also conduct experiments on Tiny-ImageNet and ImageNet. Our searched results are remarkably superior to the handcrafted ones under the unchanged computational budgets. The neural inheritance relation discovered in this paper can provide insights to the universal neural architecture search.



### Exploring and Distilling Cross-Modal Information for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2002.12585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12585v2)
- **Published**: 2020-02-28 07:46:48+00:00
- **Updated**: 2020-03-15 11:53:51+00:00
- **Authors**: Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Kai Lei, Xu Sun
- **Comment**: Accepted by IJCAI 2019
- **Journal**: None
- **Summary**: Recently, attention-based encoder-decoder models have been used extensively in image captioning. Yet there is still great difficulty for the current methods to achieve deep image understanding. In this work, we argue that such understanding requires visual attention to correlated image regions and semantic attention to coherent attributes of interest. Based on the Transformer, to perform effective attention, we explore image captioning from a cross-modal perspective and propose the Global-and-Local Information Exploring-and-Distilling approach that explores and distills the source information in vision and language. It globally provides the aspect vector, a spatial and relational representation of images based on caption contexts, through the extraction of salient region groupings and attribute collocations, and locally extracts the fine-grained regions and attributes in reference to the aspect vector for word selection. Our Transformer-based model achieves a CIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with remarkable efficiency in terms of accuracy, speed, and parameter budget.



### Regional Registration of Whole Slide Image Stacks Containing Highly Deformed Artefacts
- **Arxiv ID**: http://arxiv.org/abs/2002.12588v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12588v1)
- **Published**: 2020-02-28 07:57:56+00:00
- **Updated**: 2020-02-28 07:57:56+00:00
- **Authors**: Mahsa Paknezhad, Sheng Yang Michael Loh, Yukti Choudhury, Valerie Koh Cui Koh, TimothyTay Kwang Yong, Hui Shan Tan, Ravindran Kanesvaran, Puay Hoon Tan, John Yuen Shyi Peng, Weimiao Yu, Yongcheng Benjamin Tan, Yong Zhen Loy, Min-Han Tan, Hwee Kuan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Motivation: High resolution 2D whole slide imaging provides rich information about the tissue structure. This information can be a lot richer if these 2D images can be stacked into a 3D tissue volume. A 3D analysis, however, requires accurate reconstruction of the tissue volume from the 2D image stack. This task is not trivial due to the distortions that each individual tissue slice experiences while cutting and mounting the tissue on the glass slide. Performing registration for the whole tissue slices may be adversely affected by the deformed tissue regions. Consequently, regional registration is found to be more effective. In this paper, we propose an accurate and robust regional registration algorithm for whole slide images which incrementally focuses registration on the area around the region of interest. Results: Using mean similarity index as the metric, the proposed algorithm (mean $\pm$ std: $0.84 \pm 0.11$) followed by a fine registration algorithm ($0.86 \pm 0.08$) outperformed the state-of-the-art linear whole tissue registration algorithm ($0.74 \pm 0.19$) and the regional version of this algorithm ($0.81 \pm 0.15$). The proposed algorithm also outperforms the state-of-the-art nonlinear registration algorithm (original : $0.82 \pm 0.12$, regional : $0.77 \pm 0.22$) for whole slide images and a recently proposed patch-based registration algorithm (patch size 256: $0.79 \pm 0.16$ , patch size 512: $0.77 \pm 0.16$) for medical images. Availability: The C++ implementation code is available online at the github repository: https://github.com/MahsaPaknezhad/WSIRegistration



### An Efficient Method of Training Small Models for Regression Problems with Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2002.12597v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12597v1)
- **Published**: 2020-02-28 08:46:12+00:00
- **Updated**: 2020-02-28 08:46:12+00:00
- **Authors**: Makoto Takamoto, Yusuke Morishita, Hitoshi Imaoka
- **Comment**: 7 pages, 2 figures, draft version of a paper accepted for IEEE 3rd
  International Conference on Multimedia Information Processing and Retrieval
  (MIPR2020)
- **Journal**: None
- **Summary**: Compressing deep neural network (DNN) models becomes a very important and necessary technique for real-world applications, such as deploying those models on mobile devices. Knowledge distillation is one of the most popular methods for model compression, and many studies have been made on developing this technique. However, those studies mainly focused on classification problems, and very few attempts have been made on regression problems, although there are many application of DNNs on regression problems. In this paper, we propose a new formalism of knowledge distillation for regression problems. First, we propose a new loss function, teacher outlier rejection loss, which rejects outliers in training samples using teacher model predictions. Second, we consider a multi-task network with two outputs: one estimates training labels which is in general contaminated by noisy labels; And the other estimates teacher model's output which is expected to modify the noise labels following the memorization effects. By considering the multi-task network, training of the feature extraction of student models becomes more effective, and it allows us to obtain a better student model than one trained from scratch. We performed comprehensive evaluation with one simple toy model: sinusoidal function, and two open datasets: MPIIGaze, and Multi-PIE. Our results show consistent improvement in accuracy regardless of the annotation error level in the datasets.



### SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2002.12609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.12609v1)
- **Published**: 2020-02-28 09:25:01+00:00
- **Updated**: 2020-02-28 09:25:01+00:00
- **Authors**: Hyeongseok Jeon, Junwon Choi, Dongsuk Kum
- **Comment**: 8 pages, 9 figures, and 2 tables, the paper is submitted to the
  conference IROS2020 and is under review
- **Journal**: None
- **Summary**: Predicting the future trajectory of surrounding vehicles in a randomly varying traffic level is one of the most challenging problems in developing an autonomous vehicle. Since there is no pre-defined number of interacting vehicles participate in, the prediction network has to be scalable with respect to the vehicle number in order to guarantee the consistency in terms of both accuracy and computational load. In this paper, the first fully scalable trajectory prediction network, SCALE-Net, is proposed that can ensure both higher prediction performance and consistent computational load regardless of the number of surrounding vehicles. The SCALE-Net employs the Edge-enhance Graph Convolutional Neural Network (EGCN) for the inter-vehicular interaction embedding network. Since the proposed EGCN is inherently scalable with respect to the graph node (an agent in this study), the model can be operated independently from the total number of vehicles considered. We evaluated the scalability of the SCALE-Net on the publically available NGSIM datasets by comparing variations on computation time and prediction accuracy per single driving scene with respect to the varying vehicle number. The experimental test shows that both computation time and prediction performance of the SCALE-Net consistently outperform those of previous models regardless of the level of traffic complexities.



### MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment
- **Arxiv ID**: http://arxiv.org/abs/2002.12623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2002.12623v1)
- **Published**: 2020-02-28 09:54:06+00:00
- **Updated**: 2020-02-28 09:54:06+00:00
- **Authors**: Florian Bernard, Zeeshan Khan Suri, Christian Theobalt
- **Comment**: to appear at CVPR
- **Journal**: None
- **Summary**: We present a convex mixed-integer programming formulation for non-rigid shape matching. To this end, we propose a novel shape deformation model based on an efficient low-dimensional discrete model, so that finding a globally optimal solution is tractable in (most) practical cases. Our approach combines several favourable properties: it is independent of the initialisation, it is much more efficient to solve to global optimality compared to analogous quadratic assignment problem formulations, and it is highly flexible in terms of the variants of matching problems it can handle. Experimentally we demonstrate that our approach outperforms existing methods for sparse shape matching, that it can be used for initialising dense shape matching methods, and we showcase its flexibility on several examples.



### 4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras
- **Arxiv ID**: http://arxiv.org/abs/2002.12625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12625v1)
- **Published**: 2020-02-28 09:57:05+00:00
- **Updated**: 2020-02-28 09:57:05+00:00
- **Authors**: Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, Yebin Liu
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation.



### A U-Net Based Discriminator for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.12655v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12655v2)
- **Published**: 2020-02-28 11:16:54+00:00
- **Updated**: 2021-03-19 23:22:06+00:00
- **Authors**: Edgar SchÃ¶nfeld, Bernt Schiele, Anna Khoreva
- **Comment**: CVPR 2020 (Main Conference). Code repository:
  https://github.com/boschresearch/unetgan
- **Journal**: None
- **Summary**: Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals dataset. The code is available at https://github.com/boschresearch/unetgan.



### HOTCAKE: Higher Order Tucker Articulated Kernels for Deeper CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/2002.12663v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.12663v1)
- **Published**: 2020-02-28 11:37:09+00:00
- **Updated**: 2020-02-28 11:37:09+00:00
- **Authors**: Rui Lin, Ching-Yun Ko, Zhuolun He, Cong Chen, Yuan Cheng, Hao Yu, Graziano Chesi, Ngai Wong
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: The emerging edge computing has promoted immense interests in compacting a neural network without sacrificing much accuracy. In this regard, low-rank tensor decomposition constitutes a powerful tool to compress convolutional neural networks (CNNs) by decomposing the 4-way kernel tensor into multi-stage smaller ones. Building on top of Tucker-2 decomposition, we propose a generalized Higher Order Tucker Articulated Kernels (HOTCAKE) scheme comprising four steps: input channel decomposition, guided Tucker rank selection, higher order Tucker decomposition and fine-tuning. By subjecting each CONV layer to HOTCAKE, a highly compressed CNN model with graceful accuracy trade-off is obtained. Experiments show HOTCAKE can compress even pre-compressed models and produce state-of-the-art lightweight networks.



### Revisiting Convolutional Neural Networks for Citywide Crowd Flow Analytics
- **Arxiv ID**: http://arxiv.org/abs/2003.00895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00895v2)
- **Published**: 2020-02-28 12:21:31+00:00
- **Updated**: 2020-06-20 06:34:48+00:00
- **Authors**: Yuxuan Liang, Kun Ouyang, Yiwei Wang, Ye Liu, Junbo Zhang, Yu Zheng, David S. Rosenblum
- **Comment**: to appear at ECML-PKDD 2020
- **Journal**: None
- **Summary**: Citywide crowd flow analytics is of great importance to smart city efforts. It aims to model the crowd flow (e.g., inflow and outflow) of each region in a city based on historical observations. Nowadays, Convolutional Neural Networks (CNNs) have been widely adopted in raster-based crowd flow analytics by virtue of their capability in capturing spatial dependencies. After revisiting CNN-based methods for different analytics tasks, we expose two common critical drawbacks in the existing uses: 1) inefficiency in learning global spatial dependencies, and 2) overlooking latent region functions. To tackle these challenges, in this paper we present a novel framework entitled DeepLGR that can be easily generalized to address various citywide crowd flow analytics problems. This framework consists of three parts: 1) a local feature extraction module to learn representations for each region; 2) a global context module to extract global contextual priors and upsample them to generate the global features; and 3) a region-specific predictor based on tensor decomposition to provide customized predictions for each region, which is very parameter-efficient compared to previous methods. Extensive experiments on two typical crowd flow analytics tasks demonstrate the effectiveness, stability, and generality of our framework.



### Inverse Graphics GAN: Learning to Generate 3D Shapes from Unstructured 2D Data
- **Arxiv ID**: http://arxiv.org/abs/2002.12674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12674v1)
- **Published**: 2020-02-28 12:28:12+00:00
- **Updated**: 2020-02-28 12:28:12+00:00
- **Authors**: Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, Nate Kushman
- **Comment**: 8 pages paper, 3 pages references, 18 pages appendix
- **Journal**: None
- **Summary**: Recent work has shown the ability to learn generative models for 3D shapes from only unstructured 2D images. However, training such models requires differentiating through the rasterization step of the rendering process, therefore past work has focused on developing bespoke rendering models which smooth over this non-differentiable process in various ways. Such models are thus unable to take advantage of the photo-realistic, fully featured, industrial renderers built by the gaming and graphics industry. In this paper we introduce the first scalable training technique for 3D generative models from 2D data which utilizes an off-the-shelf non-differentiable renderer. To account for the non-differentiability, we introduce a proxy neural renderer to match the output of the non-differentiable renderer. We further propose discriminator output matching to ensure that the neural renderer learns to smooth over the rasterization appropriately. We evaluate our model on images rendered from our generated 3D shapes, and show that our model can consistently learn to generate better shapes than existing models when trained with exclusively unstructured 2D images.



### A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical Image
- **Arxiv ID**: http://arxiv.org/abs/2002.12680v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12680v2)
- **Published**: 2020-02-28 12:40:34+00:00
- **Updated**: 2020-04-25 02:18:37+00:00
- **Authors**: Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, Jinman Kim
- **Comment**: 10 pages, 8 figures, Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: Dynamic medical imaging is usually limited in application due to the large radiation doses and longer image scanning and reconstruction times. Existing methods attempt to reduce the dynamic sequence by interpolating the volumes between the acquired image volumes. However, these methods are limited to either 2D images and/or are unable to support large variations in the motion between the image volume sequences. In this paper, we present a spatiotemporal volumetric interpolation network (SVIN) designed for 4D dynamic medical images. SVIN introduces dual networks: first is the spatiotemporal motion network that leverages the 3D convolutional neural network (CNN) for unsupervised parametric volumetric registration to derive spatiotemporal motion field from two-image volumes; the second is the sequential volumetric interpolation network, which uses the derived motion field to interpolate image volumes, together with a new regression-based module to characterize the periodic motion cycles in functional organ structures. We also introduce an adaptive multi-scale architecture to capture the volumetric large anatomy motions. Experimental results demonstrated that our SVIN outperformed state-of-the-art temporal medical interpolation methods and natural video interpolation methods that have been extended to support volumetric images. Our ablation study further exemplified that our motion network was able to better represent the large functional motion compared with the state-of-the-art unsupervised medical registration methods.



### KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous Human Annotations
- **Arxiv ID**: http://arxiv.org/abs/2002.12687v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12687v6)
- **Published**: 2020-02-28 12:58:56+00:00
- **Updated**: 2020-08-07 02:07:56+00:00
- **Authors**: Yang You, Yujing Lou, Chengkun Li, Zhoujun Cheng, Liangwei Li, Lizhuang Ma, Weiming Wang, Cewu Lu
- **Comment**: 8 pages; to appear in CVPR 2020
- **Journal**: None
- **Summary**: Detecting 3D objects keypoints is of great interest to the areas of both graphics and computer vision. There have been several 2D and 3D keypoint datasets aiming to address this problem in a data-driven way. These datasets, however, either lack scalability or bring ambiguity to the definition of keypoints. Therefore, we present KeypointNet: the first large-scale and diverse 3D keypoint dataset that contains 103,450 keypoints and 8,234 3D models from 16 object categories, by leveraging numerous human annotations. To handle the inconsistency between annotations from different people, we propose a novel method to aggregate these keypoints automatically, through minimization of a fidelity loss. Finally, ten state-of-the-art methods are benchmarked on our proposed dataset. Our code and data are available on https://github.com/qq456cvb/KeypointNet.



### Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth Estimation Using Displacement Fields
- **Arxiv ID**: http://arxiv.org/abs/2002.12730v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12730v3)
- **Published**: 2020-02-28 14:15:07+00:00
- **Updated**: 2020-05-10 23:12:00+00:00
- **Authors**: Michael Ramamonjisoa, Yuming Du, Vincent Lepetit
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Current methods for depth map prediction from monocular images tend to predict smooth, poorly localized contours for the occlusion boundaries in the input image. This is unfortunate as occlusion boundaries are important cues to recognize objects, and as we show, may lead to a way to discover new objects from scene reconstruction. To improve predicted depth maps, recent methods rely on various forms of filtering or predict an additive residual depth map to refine a first estimate. We instead learn to predict, given a depth map predicted by some reconstruction method, a 2D displacement field able to re-sample pixels around the occlusion boundaries into sharper reconstructions. Our method can be applied to the output of any depth estimation method, in an end-to-end trainable fashion. For evaluation, we manually annotated the occlusion boundaries in all the images in the test split of popular NYUv2-Depth dataset. We show that our approach improves the localization of occlusion boundaries for all state-of-the-art monocular depth estimation methods that we could evaluate, without degrading the depth accuracy for the rest of the images.



### Indoor Scene Recognition in 3D
- **Arxiv ID**: http://arxiv.org/abs/2002.12819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.12819v2)
- **Published**: 2020-02-28 15:47:09+00:00
- **Updated**: 2020-07-02 21:25:18+00:00
- **Authors**: Shengyu Huang, Mikhail Usvyatsov, Konrad Schindler
- **Comment**: IROS 2020 - Camera Ready
- **Journal**: None
- **Summary**: Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.



### Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy, and Glomerulosclerosis in Renal Biopsies
- **Arxiv ID**: http://arxiv.org/abs/2002.12868v1
- **DOI**: 10.1681/ASN.2020050652
- **Categories**: **q-bio.TO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12868v1)
- **Published**: 2020-02-28 17:05:59+00:00
- **Updated**: 2020-02-28 17:05:59+00:00
- **Authors**: Brandon Ginley, Kuang-Yu Jen, Avi Rosenberg, Felicia Yen, Sanjay Jain, Agnes Fogo, Pinaki Sarder
- **Comment**: None
- **Journal**: None
- **Summary**: Glomerulosclerosis, interstitial fibrosis, and tubular atrophy (IFTA) are histologic indicators of irrecoverable kidney injury. In standard clinical practice, the renal pathologist visually assesses, under the microscope, the percentage of sclerotic glomeruli and the percentage of renal cortical involvement by IFTA. Estimation of IFTA is a subjective process due to a varied spectrum and definition of morphological manifestations. Modern artificial intelligence and computer vision algorithms have the ability to reduce inter-observer variability through rigorous quantitation. In this work, we apply convolutional neural networks for the segmentation of glomerulosclerosis and IFTA in periodic acid-Schiff stained renal biopsies. The convolutional network approach achieves high performance in intra-institutional holdout data, and achieves moderate performance in inter-intuitional holdout data, which the network had never seen in training. The convolutional approach demonstrated interesting properties, such as learning to predict regions better than the provided ground truth as well as developing its own conceptualization of segmental sclerosis. Subsequent estimations of IFTA and glomerulosclerosis percentages showed high correlation with ground truth.



### Infrared and 3D skeleton feature fusion for RGB-D action recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.12886v1
- **DOI**: 10.1109/ACCESS.2020.3023599
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.12886v1)
- **Published**: 2020-02-28 17:42:53+00:00
- **Updated**: 2020-02-28 17:42:53+00:00
- **Authors**: Alban Main de Boissiere, Rita Noumeir
- **Comment**: 11 pages, 5 figures, submitted to IEEE Access
- **Journal**: IEEE Access, vol. 8, pp. 168297-168308, 2020
- **Summary**: A challenge of skeleton-based action recognition is the difficulty to classify actions with similar motions and object-related actions. Visual clues from other streams help in that regard. RGB data are sensible to illumination conditions, thus unusable in the dark. To alleviate this issue and still benefit from a visual stream, we propose a modular network (FUSION) combining skeleton and infrared data. A 2D convolutional neural network (CNN) is used as a pose module to extract features from skeleton data. A 3D CNN is used as an infrared module to extract visual cues from videos. Both feature vectors are then concatenated and exploited conjointly using a multilayer perceptron (MLP). Skeleton data also condition the infrared videos, providing a crop around the performing subjects and thus virtually focusing the attention of the infrared module. Ablation studies show that using pre-trained networks on other large scale datasets as our modules and data augmentation yield considerable improvements on the action classification accuracy. The strong contribution of our cropping strategy is also demonstrated. We evaluate our method on the NTU RGB+D dataset, the largest dataset for human action recognition from depth cameras, and report state-of-the-art performances.



### Review: Noise and artifact reduction for MRI using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2002.12889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2002.12889v1)
- **Published**: 2020-02-28 17:50:21+00:00
- **Updated**: 2020-02-28 17:50:21+00:00
- **Authors**: Daiki Tamada
- **Comment**: Submitted to Magnetic Resonance in Medical Sciences on 2/27/2020
- **Journal**: None
- **Summary**: For several years, numerous attempts have been made to reduce noise and artifacts in MRI. Although there have been many successful methods to address these problems, practical implementation for clinical images is still challenging because of its complicated mechanism. Recently, deep learning received considerable attention, emerging as a machine learning approach in delivering robust MR image processing. The purpose here is therefore to explore further and review noise and artifact reduction using deep learning for MRI.



### A Multi-Hypothesis Approach to Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/2002.12896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12896v2)
- **Published**: 2020-02-28 18:05:16+00:00
- **Updated**: 2020-03-02 15:07:43+00:00
- **Authors**: Daniel Hernandez-Juarez, Sarah Parisot, Benjamin Busam, Ales Leonardis, Gregory Slabaugh, Steven McDonagh
- **Comment**: Accepted for publication at CVPR2020
- **Journal**: None
- **Summary**: Contemporary approaches frame the color constancy problem as learning camera specific illuminant mappings. While high accuracy can be achieved on camera specific data, these models depend on camera spectral sensitivity and typically exhibit poor generalisation to new devices. Additionally, regression methods produce point estimates that do not explicitly account for potential ambiguities among plausible illuminant solutions, due to the ill-posed nature of the problem. We propose a Bayesian framework that naturally handles color constancy ambiguity via a multi-hypothesis strategy. Firstly, we select a set of candidate scene illuminants in a data-driven fashion and apply them to a target image to generate of set of corrected images. Secondly, we estimate, for each corrected image, the likelihood of the light source being achromatic using a camera-agnostic CNN. Finally, our method explicitly learns a final illumination estimate from the generated posterior probability distribution. Our likelihood estimator learns to answer a camera-agnostic question and thus enables effective multi-camera training by disentangling illuminant estimation from the supervised learning task. We extensively evaluate our proposed approach and additionally set a benchmark for novel sensor generalisation without re-training. Our method provides state-of-the-art accuracy on multiple public datasets (up to 11% median angular error improvement) while maintaining real-time execution.



### Instance Separation Emerges from Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2003.00891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00891v1)
- **Published**: 2020-02-28 18:05:39+00:00
- **Updated**: 2020-02-28 18:05:39+00:00
- **Authors**: Steffen Wolf, Fred A. Hamprecht, Jan Funke
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks trained to inpaint partially occluded images show a deep understanding of image composition and have even been shown to remove objects from images convincingly. In this work, we investigate how this implicit knowledge of image composition can be leveraged for fully self-supervised instance separation. We propose a measure for the independence of two image regions given a fully self-supervised inpainting network and separate objects by maximizing this independence. We evaluate our method on two microscopy image datasets and show that it reaches similar segmentation performance to fully supervised methods.



### Applying Tensor Decomposition to image for Robustness against Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2002.12913v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.12913v2)
- **Published**: 2020-02-28 18:30:22+00:00
- **Updated**: 2020-03-05 14:28:41+00:00
- **Authors**: Seungju Cho, Tae Joon Jun, Mingu Kang, Daeyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays the deep learning technology is growing faster and shows dramatic performance in computer vision areas. However, it turns out a deep learning based model is highly vulnerable to some small perturbation called an adversarial attack. It can easily fool the deep learning model by adding small perturbations. On the other hand, tensor decomposition method widely uses for compressing the tensor data, including data matrix, image, etc. In this paper, we suggest combining tensor decomposition for defending the model against adversarial example. We verify this idea is simple and effective to resist adversarial attack. In addition, this method rarely degrades the original performance of clean data. We experiment on MNIST, CIFAR10 and ImageNet data and show our method robust on state-of-the-art attack methods.



### Learning Nonparametric Human Mesh Reconstruction from a Single Image without Ground Truth Meshes
- **Arxiv ID**: http://arxiv.org/abs/2003.00052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00052v1)
- **Published**: 2020-02-28 20:30:07+00:00
- **Updated**: 2020-02-28 20:30:07+00:00
- **Authors**: Kevin Lin, Lijuan Wang, Ying Jin, Zicheng Liu, Ming-Ting Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Nonparametric approaches have shown promising results on reconstructing 3D human mesh from a single monocular image. Unlike previous approaches that use a parametric human model like skinned multi-person linear model (SMPL), and attempt to regress the model parameters, nonparametric approaches relax the heavy reliance on the parametric space. However, existing nonparametric methods require ground truth meshes as their regression target for each vertex, and obtaining ground truth mesh labels is very expensive. In this paper, we propose a novel approach to learn human mesh reconstruction without any ground truth meshes. This is made possible by introducing two new terms into the loss function of a graph convolutional neural network (Graph CNN). The first term is the Laplacian prior that acts as a regularizer on the reconstructed mesh. The second term is the part segmentation loss that forces the projected region of the reconstructed mesh to match the part segmentation. Experimental results on multiple public datasets show that without using 3D ground truth meshes, the proposed approach outperforms the previous state-of-the-art approaches that require ground truth meshes for training.



### Bio-Inspired Modality Fusion for Active Speaker Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.00063v2
- **DOI**: 10.3390/app11083397
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00063v2)
- **Published**: 2020-02-28 20:56:24+00:00
- **Updated**: 2021-04-13 11:05:06+00:00
- **Authors**: Gustavo AssunÃ§Ã£o, Nuno GonÃ§alves, Paulo Menezes
- **Comment**: None
- **Journal**: Appl. Sci. 2021, 11(8), 3397
- **Summary**: Human beings have developed fantastic abilities to integrate information from various sensory sources exploring their inherent complementarity. Perceptual capabilities are therefore heightened, enabling, for instance, the well-known "cocktail party" and McGurk effects, i.e., speech disambiguation from a panoply of sound signals. This fusion ability is also key in refining the perception of sound source location, as in distinguishing whose voice is being heard in a group conversation. Furthermore, neuroscience has successfully identified the superior colliculus region in the brain as the one responsible for this modality fusion, with a handful of biological models having been proposed to approach its underlying neurophysiological process. Deriving inspiration from one of these models, this paper presents a methodology for effectively fusing correlated auditory and visual information for active speaker detection. Such an ability can have a wide range of applications, from teleconferencing systems to social robotics. The detection approach initially routes auditory and visual information through two specialized neural network structures. The resulting embeddings are fused via a novel layer based on the superior colliculus, whose topological structure emulates spatial neuron cross-mapping of unimodal perceptual fields. The validation process employed two publicly available datasets, with achieved results confirming and greatly surpassing initial expectations.



### Inexpensive surface electromyography sleeve with consistent electrode placement enables dexterous and stable prosthetic control through deep learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00070v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2003.00070v1)
- **Published**: 2020-02-28 21:24:19+00:00
- **Updated**: 2020-02-28 21:24:19+00:00
- **Authors**: Jacob A. George, Anna Neibling, Michael D. Paskett, Gregory A. Clark
- **Comment**: MEC2020
- **Journal**: None
- **Summary**: The dexterity of conventional myoelectric prostheses is limited in part by the small datasets used to train the control algorithms. Variations in surface electrode positioning make it difficult to collect consistent data and to estimate motor intent reliably over time. To address these challenges, we developed an inexpensive, easy-to-don sleeve that can record robust and repeatable surface electromyography from 32 embedded monopolar electrodes. Embedded grommets are used to consistently align the sleeve with natural skin markings (e.g., moles, freckles, scars). The sleeve can be manufactured in a few hours for less than $60. Data from seven intact participants show the sleeve provides a signal-to-noise ratio of 14, a don-time under 11 seconds, and sub-centimeter precision for electrode placement. Furthermore, in a case study with one intact participant, we use the sleeve to demonstrate that neural networks can provide simultaneous and proportional control of six degrees of freedom, even 263 days after initial algorithm training. We also highlight that consistent recordings, accumulated over time to establish a large dataset, significantly improve dexterity. These results suggest that deep learning with a 74-layer neural network can substantially improve the dexterity and stability of myoelectric prosthetic control, and that deep-learning techniques can be readily instantiated and further validated through inexpensive sleeves/sockets with consistent recording locations.



### Transferring Dense Pose to Proximal Animal Classes
- **Arxiv ID**: http://arxiv.org/abs/2003.00080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00080v1)
- **Published**: 2020-02-28 21:43:53+00:00
- **Updated**: 2020-02-28 21:43:53+00:00
- **Authors**: Artsiom Sanakoyeu, Vasil Khalidov, Maureen S. McCarthy, Andrea Vedaldi, Natalia Neverova
- **Comment**: Accepted at CVPR 2020; Project page:
  https://asanakoy.github.io/densepose-evolution
- **Journal**: None
- **Summary**: Recent contributions have demonstrated that it is possible to recognize the pose of humans densely and accurately given a large dataset of poses annotated in detail. In principle, the same approach could be extended to any animal class, but the effort required for collecting new annotations for each case makes this strategy impractical, despite important applications in natural conservation, science and business. We show that, at least for proximal animal classes such as chimpanzees, it is possible to transfer the knowledge existing in dense pose recognition for humans, as well as in more general object detectors and segmenters, to the problem of dense pose recognition in other classes. We do this by (1) establishing a DensePose model for the new animal which is also geometrically aligned to humans (2) introducing a multi-head R-CNN architecture that facilitates transfer of multiple recognition tasks between classes, (3) finding which combination of known classes can be transferred most effectively to the new animal and (4) using self-calibrated uncertainty heads to generate pseudo-labels graded by quality for training a model for this class. We also introduce two benchmark datasets labelled in the manner of DensePose for the class chimpanzee and use them to evaluate our approach, showing excellent transfer learning performance.



### Self-supervised Representation Learning for Ultrasound Video
- **Arxiv ID**: http://arxiv.org/abs/2003.00105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.00105v1)
- **Published**: 2020-02-28 23:00:26+00:00
- **Updated**: 2020-02-28 23:00:26+00:00
- **Authors**: Jianbo Jiao, Richard Droste, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: ISBI 2020
- **Journal**: None
- **Summary**: Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model. In practice, such annotations are expensive to collect and can be scarce for medical imaging applications. Therefore, there is significant interest in learning representations from unlabelled raw data. In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation. We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data. Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip. Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction.



