# Arxiv Papers in cs.CV on 2020-08-16
### Cascaded channel pruning using hierarchical self-distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.06814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06814v1)
- **Published**: 2020-08-16 00:19:35+00:00
- **Updated**: 2020-08-16 00:19:35+00:00
- **Authors**: Roy Miles, Krystian Mikolajczyk
- **Comment**: BMVC 2020
- **Journal**: None
- **Summary**: In this paper, we propose an approach for filter-level pruning with hierarchical knowledge distillation based on the teacher, teaching-assistant, and student framework. Our method makes use of teaching assistants at intermediate pruning levels that share the same architecture and weights as the target student. We propose to prune each model independently using the gradient information from its corresponding teacher. By considering the relative sizes of each student-teacher pair, this formulation provides a natural trade-off between the capacity gap for knowledge distillation and the bias of the filter saliency updates. Our results show improvements in the attainable accuracy and model compression across the CIFAR10 and ImageNet classification tasks using the VGG16and ResNet50 architectures. We provide an extensive evaluation that demonstrates the benefits of using a varying number of teaching assistant models at different sizes.



### Relevance Attack on Detectors
- **Arxiv ID**: http://arxiv.org/abs/2008.06822v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06822v4)
- **Published**: 2020-08-16 02:44:25+00:00
- **Updated**: 2021-11-23 14:15:34+00:00
- **Authors**: Sizhe Chen, Fan He, Xiaolin Huang, Kun Zhang
- **Comment**: accepted by Pattern Recognition
- **Journal**: None
- **Summary**: This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation, i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.



### Faster Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.06826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06826v1)
- **Published**: 2020-08-16 03:02:49+00:00
- **Updated**: 2020-08-16 03:02:49+00:00
- **Authors**: Guan'an Wang, Shaogang Gong, Jian Cheng, Zengguang Hou
- **Comment**: accepted by ECCV2020, https://github.com/wangguanan/light-reid
- **Journal**: None
- **Summary**: Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a $F_{\beta}$ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is $50\times$ faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.



### A novel approach to remove foreign objects from chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2008.06828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06828v1)
- **Published**: 2020-08-16 03:06:28+00:00
- **Updated**: 2020-08-16 03:06:28+00:00
- **Authors**: Hieu X. Le, Phuong D. Nguyen, Thang H. Nguyen, Khanh N. Q. Le, Thanh T. Nguyen
- **Comment**: 9 pages, 7 figures, 7 tables
- **Journal**: None
- **Summary**: We initially proposed a deep learning approach for foreign objects inpainting in smartphone-camera captured chest radiographs utilizing the cheXphoto dataset. Foreign objects which can significantly affect the quality of a computer-aided diagnostic prediction are captured under various settings. In this paper, we used multi-method to tackle both removal and inpainting chest radiographs. Firstly, an object detection model is trained to separate the foreign objects from the given image. Subsequently, the binary mask of each object is extracted utilizing a segmentation model. Each pair of the binary mask and the extracted object are then used for inpainting purposes. Finally, the in-painted regions are now merged back to the original image, resulting in a clean and non-foreign-object-existing output. To conclude, we achieved state-of-the-art accuracy. The experimental results showed a new approach to the possible applications of this method for chest X-ray images detection.



### Open source tools for management and archiving of digital microscopy data to allow integration with patient pathology and treatment information
- **Arxiv ID**: http://arxiv.org/abs/2008.06837v1
- **DOI**: 10.1186/1746-1596-8-22
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06837v1)
- **Published**: 2020-08-16 04:28:53+00:00
- **Updated**: 2020-08-16 04:28:53+00:00
- **Authors**: Matloob Khushi, Georgina Edwards, Diego Alonso de Marcos, Jane E Carpenter, J Dinny Graham, Christine L Clarke
- **Comment**: None
- **Journal**: Diagnostic Pathology volume 8, Article number: 22 (2013)
- **Summary**: Virtual microscopy includes digitisation of histology slides and the use of computer technologies for complex investigation of diseases such as cancer. However, automated image analysis, or website publishing of such digital images, is hampered by their large file sizes. We have developed two Java based open source tools: Snapshot Creator and NDPI-Splitter. Snapshot Creator converts a portion of a large digital slide into a desired quality JPEG image. The image is linked to the patients clinical and treatment information in a customised open source cancer data management software (Caisis) in use at the Australian Breast Cancer Tissue Bank (ABCTB) and then published on the ABCTB website www.abctb.org.au using Deep Zoom open source technology. Using the ABCTB online search engine, digital images can be searched by defining various criteria such as cancer type, or biomarkers expressed. NDPI-Splitter splits a large image file into smaller sections of TIFF images so that they can be easily analysed by image analysis software such as Metamorph or Matlab. NDPI-Splitter also has the capacity to filter out empty images. Snapshot Creator and NDPI-Splitter are novel open source Java tools. They convert digital slides into files of smaller size for further processing. In conjunction with other open source tools such as Deep Zoom and Caisis, this suite of tools is used for the management and archiving of digital microscopy images, enabling digitised images to be explored and zoomed online. Our online image repository also has the capacity to be used as a teaching resource. These tools also enable large files to be sectioned for image analysis.



### We Learn Better Road Pothole Detection: from Attention Aggregation to Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.06840v2
- **DOI**: 10.1007/978-3-030-66823-5_17
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.06840v2)
- **Published**: 2020-08-16 05:17:42+00:00
- **Updated**: 2020-12-11 10:32:39+00:00
- **Authors**: Rui Fan, Hengli Wang, Mohammud J. Bocus, Ming Liu
- **Comment**: 16 pages, 7 figures and 2 tables. This paper is accepted by ECCV
  Workshops 2020
- **Journal**: None
- **Summary**: Manual visual inspection performed by certified inspectors is still the main form of road pothole detection. This process is, however, not only tedious, time-consuming and costly, but also dangerous for the inspectors. Furthermore, the road pothole detection results are always subjective, because they depend entirely on the individual experience. Our recently introduced disparity (or inverse depth) transformation algorithm allows better discrimination between damaged and undamaged road areas, and it can be easily deployed to any semantic segmentation network for better road pothole detection results. To boost the performance, we propose a novel attention aggregation (AA) framework, which takes the advantages of different types of attention modules. In addition, we develop an effective training set augmentation technique based on adversarial domain adaptation, where the synthetic road RGB images and transformed road disparity (or inverse depth) images are generated to enhance the training of semantic segmentation networks. The experimental results demonstrate that, firstly, the transformed disparity (or inverse depth) images become more informative; secondly, AA-UNet and AA-RTFNet, our best performing implementations, respectively outperform all other state-of-the-art single-modal and data-fusion networks for road pothole detection; and finally, the training set augmentation technique based on adversarial domain adaptation not only improves the accuracy of the state-of-the-art semantic segmentation networks, but also accelerates their convergence.



### Wavelet Denoising and Attention-based RNN-ARIMA Model to Predict Forex Price
- **Arxiv ID**: http://arxiv.org/abs/2008.06841v1
- **DOI**: None
- **Categories**: **cs.CE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06841v1)
- **Published**: 2020-08-16 05:32:40+00:00
- **Updated**: 2020-08-16 05:32:40+00:00
- **Authors**: Zhiwen Zeng, Matloob Khushi
- **Comment**: None
- **Journal**: IJCNN 2020
- **Summary**: Every change of trend in the forex market presents a great opportunity as well as a risk for investors. Accurate forecasting of forex prices is a crucial element in any effective hedging or speculation strategy. However, the complex nature of the forex market makes the predicting problem challenging, which has prompted extensive research from various academic disciplines. In this paper, a novel approach that integrates the wavelet denoising, Attention-based Recurrent Neural Network (ARNN), and Autoregressive Integrated Moving Average (ARIMA) are proposed. Wavelet transform removes the noise from the time series to stabilize the data structure. ARNN model captures the robust and non-linear relationships in the sequence and ARIMA can well fit the linear correlation of the sequential information. By hybridization of the three models, the methodology is capable of modelling dynamic systems such as the forex market. Our experiments on USD/JPY five-minute data outperforms the baseline methods. Root-Mean-Squared-Error (RMSE) of the hybrid approach was found to be 1.65 with a directional accuracy of ~76%.



### GA-MSSR: Genetic Algorithm Maximizing Sharpe and Sterling Ratio Method for RoboTrading
- **Arxiv ID**: http://arxiv.org/abs/2008.09471v1
- **DOI**: None
- **Categories**: **q-fin.ST**, cs.CE, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09471v1)
- **Published**: 2020-08-16 05:33:35+00:00
- **Updated**: 2020-08-16 05:33:35+00:00
- **Authors**: Zezheng Zhang, Matloob Khushi
- **Comment**: None
- **Journal**: IJCNN 2020
- **Summary**: Foreign exchange is the largest financial market in the world, and it is also one of the most volatile markets. Technical analysis plays an important role in the forex market and trading algorithms are designed utilizing machine learning techniques. Most literature used historical price information and technical indicators for training. However, the noisy nature of the market affects the consistency and profitability of the algorithms. To address this problem, we designed trading rule features that are derived from technical indicators and trading rules. The parameters of technical indicators are optimized to maximize trading performance. We also proposed a novel cost function that computes the risk-adjusted return, Sharpe and Sterling Ratio (SSR), in an effort to reduce the variance and the magnitude of drawdowns. An automatic robotic trading (RoboTrading) strategy is designed with the proposed Genetic Algorithm Maximizing Sharpe and Sterling Ratio model (GA-MSSR) model. The experiment was conducted on intraday data of 6 major currency pairs from 2018 to 2019. The results consistently showed significant positive returns and the performance of the trading system is superior using the optimized rule-based features. The highest return obtained was 320% annually using 5-minute AUDUSD currency pair. Besides, the proposed model achieves the best performance on risk factors, including maximum drawdowns and variance in return, comparing to benchmark models. The code can be accessed at https://github.com/zzzac/rule-based-forextrading-system



### Learning Flow-based Feature Warping for Face Frontalization with Illumination Inconsistent Supervision
- **Arxiv ID**: http://arxiv.org/abs/2008.06843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06843v2)
- **Published**: 2020-08-16 06:07:00+00:00
- **Updated**: 2020-09-09 12:51:26+00:00
- **Authors**: Yuxiang Wei, Ming Liu, Haolin Wang, Ruifeng Zhu, Guosheng Hu, Wangmeng Zuo
- **Comment**: ECCV 2020. Code is available at: https://github.com/csyxwei/FFWM
- **Journal**: None
- **Summary**: Despite recent advances in deep learning-based face frontalization methods, photo-realistic and illumination preserving frontal face synthesis is still challenging due to large pose and illumination discrepancy during training. We propose a novel Flow-based Feature Warping Model (FFWM) which can learn to synthesize photo-realistic and illumination preserving frontal images with illumination inconsistent supervision. Specifically, an Illumination Preserving Module (IPM) is proposed to learn illumination preserving image synthesis from illumination inconsistent image pairs. IPM includes two pathways which collaborate to ensure the synthesized frontal images are illumination preserving and with fine details. Moreover, a Warp Attention Module (WAM) is introduced to reduce the pose discrepancy in the feature level, and hence to synthesize frontal images more effectively and preserve more details of profile images. The attention mechanism in WAM helps reduce the artifacts caused by the displacements between the profile and the frontal images. Quantitative and qualitative experimental results show that our FFWM can synthesize photo-realistic and illumination preserving frontal images and performs favorably against the state-of-the-art results.



### Detection of Gait Abnormalities caused by Neurological Disorders
- **Arxiv ID**: http://arxiv.org/abs/2008.06861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06861v1)
- **Published**: 2020-08-16 09:00:36+00:00
- **Updated**: 2020-08-16 09:00:36+00:00
- **Authors**: Daksh Goyal, Koteswar Rao Jerripothula, Ankush Mittal
- **Comment**: 6 pages, 5 figures, Accepted by IEEE Workshop on Multimedia Signal
  Processing (MMSP), 2020
- **Journal**: None
- **Summary**: In this paper, we leverage gait to potentially detect some of the important neurological disorders, namely Parkinson's disease, Diplegia, Hemiplegia, and Huntington's Chorea. Persons with these neurological disorders often have a very abnormal gait, which motivates us to target gait for their potential detection. Some of the abnormalities involve the circumduction of legs, forward-bending, involuntary movements, etc. To detect such abnormalities in gait, we develop gait features from the key-points of the human pose, namely shoulders, elbows, hips, knees, ankles, etc. To evaluate the effectiveness of our gait features in detecting the abnormalities related to these diseases, we build a synthetic video dataset of persons mimicking the gait of persons with such disorders, considering the difficulty in finding a sufficient number of people with these disorders. We name it \textit{NeuroSynGait} video dataset. Experiments demonstrated that our gait features were indeed successful in detecting these abnormalities.



### KutralNet: A Portable Deep Learning Model for Fire Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.06866v1
- **DOI**: 10.1109/IJCNN48605.2020.9207202
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06866v1)
- **Published**: 2020-08-16 09:35:25+00:00
- **Updated**: 2020-08-16 09:35:25+00:00
- **Authors**: Angel Ayala, Bruno Fernandes, Francisco Cruz, David Macêdo, Adriano L. I. Oliveira, Cleber Zanchettin
- **Comment**: Accepted in the IEEE International Joint Conference on Neural
  Networks (IJCNN), 2020
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Most of the automatic fire alarm systems detect the fire presence through sensors like thermal, smoke, or flame. One of the new approaches to the problem is the use of images to perform the detection. The image approach is promising since it does not need specific sensors and can be easily embedded in different devices. However, besides the high performance, the computational cost of the used deep learning methods is a challenge to their deployment in portable devices. In this work, we propose a new deep learning architecture that requires fewer floating-point operations (flops) for fire recognition. Additionally, we propose a portable approach for fire recognition and the use of modern techniques such as inverted residual block, convolutions like depth-wise, and octave, to reduce the model's computational cost. The experiments show that our model keeps high accuracy while substantially reducing the number of parameters and flops. One of our models presents 71\% fewer parameters than FireNet, while still presenting competitive accuracy and AUROC performance. The proposed methods are evaluated on FireNet and FiSmo datasets. The obtained results are promising for the implementation of the model in a mobile device, considering the reduced number of flops and parameters acquired.



### SMPLpix: Neural Avatars from 3D Human Models
- **Arxiv ID**: http://arxiv.org/abs/2008.06872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06872v2)
- **Published**: 2020-08-16 10:22:00+00:00
- **Updated**: 2020-11-09 11:08:09+00:00
- **Authors**: Sergey Prokudin, Michael J. Black, Javier Romero
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep generative models have led to an unprecedented level of realism for synthetically generated images of humans. However, one of the remaining fundamental limitations of these models is the ability to flexibly control the generative process, e.g.~change the camera and human pose while retaining the subject identity. At the same time, deformable human body models like SMPL and its successors provide full control over pose and shape but rely on classic computer graphics pipelines for rendering. Such rendering pipelines require explicit mesh rasterization that (a) does not have the potential to fix artifacts or lack of realism in the original 3D geometry and (b) until recently, were not fully incorporated into deep learning frameworks. In this work, we propose to bridge the gap between classic geometry-based rendering and the latest generative networks operating in pixel space. We train a network that directly converts a sparse set of 3D mesh vertices into photorealistic images, alleviating the need for traditional rasterization mechanism. We train our model on a large corpus of human 3D models and corresponding real photos, and show the advantage over conventional differentiable renderers both in terms of the level of photorealism and rendering efficiency.



### Poet: Product-oriented Video Captioner for E-commerce
- **Arxiv ID**: http://arxiv.org/abs/2008.06880v1
- **DOI**: 10.1145/3394171.3413880
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06880v1)
- **Published**: 2020-08-16 10:53:46+00:00
- **Updated**: 2020-08-16 10:53:46+00:00
- **Authors**: Shengyu Zhang, Ziqi Tan, Jin Yu, Zhou Zhao, Kun Kuang, Jie Liu, Jingren Zhou, Hongxia Yang, Fei Wu
- **Comment**: 10 pages, 3 figures, to appear in ACM MM 2020 proceedings
- **Journal**: None
- **Summary**: In e-commerce, a growing number of user-generated videos are used for product promotion. How to generate video descriptions that narrate the user-preferred product characteristics depicted in the video is vital for successful promoting. Traditional video captioning methods, which focus on routinely describing what exists and happens in a video, are not amenable for product-oriented video captioning. To address this problem, we propose a product-oriented video captioner framework, abbreviated as Poet. Poet firstly represents the videos as product-oriented spatial-temporal graphs. Then, based on the aspects of the video-associated product, we perform knowledge-enhanced spatial-temporal inference on those graphs for capturing the dynamic change of fine-grained product-part characteristics. The knowledge leveraging module in Poet differs from the traditional design by performing knowledge filtering and dynamic memory modeling. We show that Poet achieves consistent performance improvement over previous methods concerning generation quality, product aspects capturing, and lexical diversity. Experiments are performed on two product-oriented video captioning datasets, buyer-generated fashion video dataset (BFVD) and fan-generated fashion video dataset (FFVD), collected from Mobile Taobao. We will release the desensitized datasets to promote further investigations on both video captioning and general video analysis problems.



### Cross-Modality 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.10436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10436v1)
- **Published**: 2020-08-16 11:01:20+00:00
- **Updated**: 2020-08-16 11:01:20+00:00
- **Authors**: Ming Zhu, Chao Ma, Pan Ji, Xiaokang Yang
- **Comment**: Accepted by WACV 2021
- **Journal**: None
- **Summary**: In this paper, we focus on exploring the fusion of images and point clouds for 3D object detection in view of the complementary nature of the two modalities, i.e., images possess more semantic information while point clouds specialize in distance sensing. To this end, we present a novel two-stage multi-modal fusion network for 3D object detection, taking both binocular images and raw point clouds as input. The whole architecture facilitates two-stage fusion. The first stage aims at producing 3D proposals through sparse point-wise feature fusion. Within the first stage, we further exploit a joint anchor mechanism that enables the network to utilize 2D-3D classification and regression simultaneously for better proposal generation.   The second stage works on the 2D and 3D proposal regions and fuses their dense features. In addition, we propose to use pseudo LiDAR points from stereo matching as a data augmentation method to densify the LiDAR points, as we observe that objects missed by the detection network mostly have too few points especially for far-away objects. Our experiments on the KITTI dataset show that the proposed multi-stage fusion helps the network to learn better representations.



### SPL-MLL: Selecting Predictable Landmarks for Multi-Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.06883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06883v1)
- **Published**: 2020-08-16 11:07:44+00:00
- **Updated**: 2020-08-16 11:07:44+00:00
- **Authors**: Junbing Li, Changqing Zhang, Pengfei Zhu, Baoyuan Wu, Lei Chen, Qinghua Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.



### DeVLBert: Learning Deconfounded Visio-Linguistic Representations
- **Arxiv ID**: http://arxiv.org/abs/2008.06884v2
- **DOI**: 10.1145/3394171.3413518
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2008.06884v2)
- **Published**: 2020-08-16 11:09:22+00:00
- **Updated**: 2020-10-02 12:00:56+00:00
- **Authors**: Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu
- **Comment**: 10 pages, 4 figures, to appear in ACM MM 2020 proceedings
- **Journal**: None
- **Summary**: In this paper, we propose to investigate the problem of out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token (object or word) given another one can be high (due to the dataset biases) without robust (causal) relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability.



### AutoSimulate: (Quickly) Learning Synthetic Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.08424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.08424v1)
- **Published**: 2020-08-16 11:36:11+00:00
- **Updated**: 2020-08-16 11:36:11+00:00
- **Authors**: Harkirat Singh Behl, Atılım Güneş Baydin, Ran Gal, Philip H. S. Torr, Vibhav Vineet
- **Comment**: ECCV 2020
- **Journal**: European Conference on Computer Vision (ECCV) 2020
- **Summary**: Simulation is increasingly being used for generating large labelled datasets in many machine learning problems. Recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task, usually relying on REINFORCE-like gradient estimators. However these approaches are very expensive as they treat the entire data generation, model training, and validation pipeline as a black-box and require multiple costly objective evaluations at each iteration. We propose an efficient alternative for optimal synthetic data generation, based on a novel differentiable approximation of the objective. This allows us to optimize the simulator, which may be non-differentiable, requiring only one objective evaluation at each iteration with a little overhead. We demonstrate on a state-of-the-art photorealistic renderer that the proposed method finds the optimal data distribution faster (up to $50\times$), with significantly reduced training data generation (up to $30\times$) and better accuracy ($+8.7\%$) on real-world test datasets than previous methods.



### Context-aware Feature Generation for Zero-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06893v1
- **DOI**: 10.1145/3394171.3413593
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06893v1)
- **Published**: 2020-08-16 12:20:49+00:00
- **Updated**: 2020-08-16 12:20:49+00:00
- **Authors**: Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, Liqing Zhang
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Existing semantic segmentation models heavily rely on dense pixel-wise annotations. To reduce the annotation pressure, we focus on a challenging task named zero-shot semantic segmentation, which aims to segment unseen objects with zero annotations. This task can be accomplished by transferring knowledge across categories via semantic word embeddings. In this paper, we propose a novel context-aware feature generation method for zero-shot segmentation named CaGNet. In particular, with the observation that a pixel-wise feature highly depends on its contextual information, we insert a contextual module in a segmentation network to capture the pixel-wise contextual information, which guides the process of generating more diverse and context-aware features from semantic word embeddings. Our method achieves state-of-the-art results on three benchmark datasets for zero-shot segmentation. Codes are available at: https://github.com/bcmi/CaGNet-Zero-Shot-Semantic-Segmentation.



### Geodesic Paths for Image Segmentation with Implicit Region-based Homogeneity Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2008.06909v4
- **DOI**: 10.1109/TIP.2021.3078106
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06909v4)
- **Published**: 2020-08-16 13:29:11+00:00
- **Updated**: 2021-05-06 14:07:42+00:00
- **Authors**: Da Chen, Jian Zhu, Xinxin Zhang, Minglei Shu, Laurent D. Cohen
- **Comment**: Published in IEEE Trans. Image Processing
- **Journal**: None
- **Summary**: Minimal paths are regarded as a powerful and efficient tool for boundary detection and image segmentation due to its global optimality and the well-established numerical solutions such as fast marching method. In this paper, we introduce a flexible interactive image segmentation model based on the Eikonal partial differential equation (PDE) framework in conjunction with region-based homogeneity enhancement. A key ingredient in the introduced model is the construction of local geodesic metrics, which are capable of integrating anisotropic and asymmetric edge features, implicit region-based homogeneity features and/or curvature regularization. The incorporation of the region-based homogeneity features into the metrics considered relies on an implicit representation of these features, which is one of the contributions of this work. Moreover, we also introduce a way to build simple closed contours as the concatenation of two disjoint open curves. Experimental results prove that the proposed model indeed outperforms state-of-the-art minimal paths-based image segmentation approaches.



### Neural Descent for Visual 3D Human Pose and Shape
- **Arxiv ID**: http://arxiv.org/abs/2008.06910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06910v2)
- **Published**: 2020-08-16 13:38:41+00:00
- **Updated**: 2021-06-14 09:10:36+00:00
- **Authors**: Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present deep neural network methodology to reconstruct the 3d pose and shape of people, given an input RGB image. We rely on a recently introduced, expressivefull body statistical 3d human model, GHUM, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as HUmanNeural Descent (HUND), which avoids both second-order differentiation when training the model parameters,and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape parameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure end-progress. HUND's symmetry between training and testing makes it the first 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that HUND achieves very competitive results in datasets like H3.6M and 3DPW, aswell as good quality 3d reconstructions for complex imagery collected in-the-wild.



### Visual stream connectivity predicts assessments of image quality
- **Arxiv ID**: http://arxiv.org/abs/2008.06939v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2008.06939v1)
- **Published**: 2020-08-16 15:38:17+00:00
- **Updated**: 2020-08-16 15:38:17+00:00
- **Authors**: Elijah Bowen, Antonio Rodriguez, Damian Sowinski, Richard Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Some biological mechanisms of early vision are comparatively well understood, but they have yet to be evaluated for their ability to accurately predict and explain human judgments of image similarity. From well-studied simple connectivity patterns in early vision, we derive a novel formalization of the psychophysics of similarity, showing the differential geometry that provides accurate and explanatory accounts of perceptual similarity judgments. These predictions then are further improved via simple regression on human behavioral reports, which in turn are used to construct more elaborate hypothesized neural connectivity patterns. Both approaches outperform standard successful measures of perceived image fidelity from the literature, as well as providing explanatory principles of similarity perception.



### Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2008.06941v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.06941v2)
- **Published**: 2020-08-16 15:39:56+00:00
- **Updated**: 2020-08-22 11:11:32+00:00
- **Authors**: Zhu Zhang, Zhou Zhao, Zhijie Lin, Baoxing Huai, Nicholas Jing Yuan
- **Comment**: Accepted by IJCAI 2020
- **Journal**: None
- **Summary**: Spatio-temporal video grounding aims to retrieve the spatio-temporal tube of a queried object according to the given sentence. Currently, most existing grounding methods are restricted to well-aligned segment-sentence pairs. In this paper, we explore spatio-temporal video grounding on unaligned data and multi-form sentences. This challenging task requires to capture critical object relations to identify the queried target. However, existing approaches cannot distinguish notable objects and remain in ineffective relation modeling between unnecessary objects. Thus, we propose a novel object-aware multi-branch relation network for object-aware relation discovery. Concretely, we first devise multiple branches to develop object-aware region modeling, where each branch focuses on a crucial object mentioned in the sentence. We then propose multi-branch relation reasoning to capture critical object relationships between the main branch and auxiliary branches. Moreover, we apply a diversity loss to make each branch only pay attention to its corresponding object and boost multi-branch learning. The extensive experiments show the effectiveness of our proposed method.



### Image Stylization for Robust Features
- **Arxiv ID**: http://arxiv.org/abs/2008.06959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06959v1)
- **Published**: 2020-08-16 17:03:03+00:00
- **Updated**: 2020-08-16 17:03:03+00:00
- **Authors**: Iaroslav Melekhov, Gabriel J. Brostow, Juho Kannala, Daniyar Turmukhambetov
- **Comment**: v1.1
- **Journal**: None
- **Summary**: Local features that are robust to both viewpoint and appearance changes are crucial for many computer vision tasks. In this work we investigate if photorealistic image stylization improves robustness of local features to not only day-night, but also weather and season variations. We show that image stylization in addition to color augmentation is a powerful method of learning robust features. We evaluate learned features on visual localization benchmarks, outperforming state of the art baseline models despite training without ground-truth 3D correspondences using synthetic homographies only.   We use trained feature networks to compete in Long-Term Visual Localization and Map-based Localization for Autonomous Driving challenges achieving competitive scores.



### Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians
- **Arxiv ID**: http://arxiv.org/abs/2008.06963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06963v1)
- **Published**: 2020-08-16 17:45:14+00:00
- **Updated**: 2020-08-16 17:45:14+00:00
- **Authors**: Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang Jiang, Xiaowei Guo, Wei-Shi Zheng, Nong Sang, Xing Sun
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: In the conventional person Re-ID setting, it is widely assumed that cropped person images are for each individual. However, in a crowded scene, off-shelf-detectors may generate bounding boxes involving multiple people, where the large proportion of background pedestrians or human occlusion exists. The representation extracted from such cropped images, which contain both the target and the interference pedestrians, might include distractive information. This will lead to wrong retrieval results. To address this problem, this paper presents a novel deep network termed Pedestrian-Interference Suppression Network (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to enhance the feature of the target in the gallery, under the guidance of the query. Furthermore, the involving Guidance Reversed Attention Module and the Multi-Person Separation Loss promote QGAB to suppress the interference of other pedestrians. Our method is evaluated on two new pedestrian-interference datasets and the results show that the proposed method performs favorably against existing Re-ID methods.



### Automated Detection of Congenital Heart Disease in Fetal Ultrasound Screening
- **Arxiv ID**: http://arxiv.org/abs/2008.06966v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06966v2)
- **Published**: 2020-08-16 17:53:41+00:00
- **Updated**: 2020-08-18 01:29:45+00:00
- **Authors**: Jeremy Tan, Anselm Au, Qingjie Meng, Sandy FinesilverSmith, John Simpson, Daniel Rueckert, Reza Razavi, Thomas Day, David Lloyd, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Prenatal screening with ultrasound can lower neonatal mortality significantly for selected cardiac abnormalities. However, the need for human expertise, coupled with the high volume of screening cases, limits the practically achievable detection rates. In this paper we discuss the potential for deep learning techniques to aid in the detection of congenital heart disease (CHD) in fetal ultrasound. We propose a pipeline for automated data curation and classification. During both training and inference, we exploit an auxiliary view classification task to bias features toward relevant cardiac structures. This bias helps to improve in F1-scores from 0.72 and 0.77 to 0.87 and 0.85 for healthy and CHD classes respectively.



### Mesorasi: Architecture Support for Point Cloud Analytics via Delayed-Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2008.06967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2008.06967v1)
- **Published**: 2020-08-16 18:11:19+00:00
- **Updated**: 2020-08-16 18:11:19+00:00
- **Authors**: Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, Yuhao Zhu
- **Comment**: None
- **Journal**: Proceedings of the 53nd (2020) Annual IEEE/ACM International
  Symposium on Microarchitecture
- **Summary**: Point cloud analytics is poised to become a key workload on battery-powered embedded and mobile platforms in a wide range of emerging application domains, such as autonomous driving, robotics, and augmented reality, where efficiency is paramount. This paper proposes Mesorasi, an algorithm-architecture co-designed system that simultaneously improves the performance and energy efficiency of point cloud analytics while retaining its accuracy. Our extensive characterizations of state-of-the-art point cloud algorithms show that, while structurally reminiscent of convolutional neural networks (CNNs), point cloud algorithms exhibit inherent compute and memory inefficiencies due to the unique characteristics of point cloud data. We propose delayed-aggregation, a new algorithmic primitive for building efficient point cloud algorithms. Delayed-aggregation hides the performance bottlenecks and reduces the compute and memory redundancies by exploiting the approximately distributive property of key operations in point cloud algorithms. Delayed-aggregation let point cloud algorithms achieve 1.6x speedup and 51.1% energy reduction on a mobile GPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the algorithmic benefits, we propose minor extensions to contemporary CNN accelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC) without modifying other SoC components. With additional hardware support, Mesorasi achieves up to 3.6x speedup.



### Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and Novel-View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.06981v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06981v2)
- **Published**: 2020-08-16 19:40:56+00:00
- **Updated**: 2021-04-06 19:18:51+00:00
- **Authors**: Zhipeng Bao, Yu-Xiong Wang, Martial Hebert
- **Comment**: Accepted as a Poster paper at ICLR 2021
- **Journal**: None
- **Summary**: We propose a novel task of joint few-shot recognition and novel-view synthesis: given only one or few images of a novel object from arbitrary views with only category annotation, we aim to simultaneously learn an object classifier and generate images of that type of object from new viewpoints. While existing work copes with two or more tasks mainly by multi-task learning of shareable feature representations, we take a different perspective. We focus on the interaction and cooperation between a generative model and a discriminative model, in a way that facilitates knowledge to flow across tasks in complementary directions. To this end, we propose bowtie networks that jointly learn 3D geometric and semantic representations with a feedback loop. Experimental evaluation on challenging fine-grained recognition datasets demonstrates that our synthesized images are realistic from multiple viewpoints and significantly improve recognition performance as ways of data augmentation, especially in the low-data regime. Code and pre-trained models are released at https://github.com/zpbao/bowtie_networks.



### A Self-supervised GAN for Unsupervised Few-shot Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.06982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06982v2)
- **Published**: 2020-08-16 19:47:26+00:00
- **Updated**: 2020-10-19 18:05:25+00:00
- **Authors**: Khoi Nguyen, Sinisa Todorovic
- **Comment**: To be appeared in ICPR 2020
- **Journal**: None
- **Summary**: This paper addresses unsupervised few-shot object recognition, where all training images are unlabeled, and test images are divided into queries and a few labeled support images per object class of interest. The training and test images do not share object classes. We extend the vanilla GAN with two loss functions, both aimed at self-supervised learning. The first is a reconstruction loss that enforces the discriminator to reconstruct the probabilistically sampled latent code which has been used for generating the "fake" image. The second is a triplet loss that enforces the discriminator to output image encodings that are closer for more similar images. Evaluation, comparisons, and detailed ablation studies are done in the context of few-shot classification. Our approach significantly outperforms the state of the art on the Mini-Imagenet and Tiered-Imagenet datasets.



### False Detection (Positives and Negatives) in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.06986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.06986v1)
- **Published**: 2020-08-16 20:09:05+00:00
- **Updated**: 2020-08-16 20:09:05+00:00
- **Authors**: Subrata Goswami
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a very important function of visual perception systems. Since the early days of classical object detection based on HOG to modern deep learning based detectors, object detection has improved in accuracy. Two stage detectors usually have higher accuracy than single stage ones. Both types of detectors use some form of quantization of the search space of rectangular regions of image. There are far more of the quantized elements than true objects. The way these bounding boxes are filtered out possibly results in the false positive and false negatives. This empirical experimental study explores ways of reducing false positives and negatives with labelled data.. In the process also discovered insufficient labelling in Openimage 2019 Object Detection dataset.



### Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are
- **Arxiv ID**: http://arxiv.org/abs/2008.06989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06989v1)
- **Published**: 2020-08-16 20:29:05+00:00
- **Updated**: 2020-08-16 20:29:05+00:00
- **Authors**: Vítor Albiero, Kevin W. Bowyer
- **Comment**: Paper will appear on BMVC 2020
- **Journal**: None
- **Summary**: Recent news articles have accused face recognition of being "biased", "sexist" or "racist". There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher false match rate and a higher false non-match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists "Analyze cause and effect" under the heading "What we did not do". We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images reverses the apparent higher false non-match rate for females. Also, principal component analysis indicates that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in false match rates.



### Deep Learning Predicts Cardiovascular Disease Risks from Lung Cancer Screening Low Dose Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2008.06997v2
- **DOI**: 10.1038/s41467-021-23235-4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06997v2)
- **Published**: 2020-08-16 21:07:01+00:00
- **Updated**: 2021-03-29 15:15:03+00:00
- **Authors**: Hanqing Chao, Hongming Shan, Fatemeh Homayounieh, Ramandeep Singh, Ruhani Doda Khera, Hengtao Guo, Timothy Su, Ge Wang, Mannudeep K. Kalra, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer patients have a higher risk of cardiovascular disease (CVD) mortality than the general population. Low dose computed tomography (LDCT) for lung cancer screening offers an opportunity for simultaneous CVD risk estimation in at-risk patients. Our deep learning CVD risk prediction model, trained with 30,286 LDCTs from the National Lung Cancer Screening Trial, achieved an area under the curve (AUC) of 0.871 on a separate test set of 2,085 subjects and identified patients with high CVD mortality risks (AUC of 0.768). We validated our model against ECG-gated cardiac CT based markers, including coronary artery calcification (CAC) score, CAD-RADS score, and MESA 10-year risk score from an independent dataset of 335 subjects. Our work shows that, in high-risk patients, deep learning can convert LDCT for lung cancer screening into a dual-screening quantitative tool for CVD risk estimation.



### RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty Quantification in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06999v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06999v2)
- **Published**: 2020-08-16 21:16:19+00:00
- **Updated**: 2020-08-18 08:18:44+00:00
- **Authors**: Marc Gantenbein, Ertunc Erdil, Ender Konukoglu
- **Comment**: Accepted to UNSURE by MICCAI 2020
- **Journal**: None
- **Summary**: Quantifying segmentation uncertainty has become an important issue in medical image analysis due to the inherent ambiguity of anatomical structures and its pathologies. Recently, neural network-based uncertainty quantification methods have been successfully applied to various problems. One of the main limitations of the existing techniques is the high memory requirement during training; which limits their application to processing smaller field-of-views (FOVs) and/or using shallower architectures. In this paper, we investigate the effect of using reversible blocks for building memory-efficient neural network architectures for quantification of segmentation uncertainty. The reversible architecture achieves memory saving by exactly computing the activations from the outputs of the subsequent layers during backpropagation instead of storing the activations for each layer. We incorporate the reversible blocks into a recently proposed architecture called PHiSeg that is developed for uncertainty quantification in medical image segmentation. The reversible architecture, RevPHiSeg, allows training neural networks for quantifying segmentation uncertainty on GPUs with limited memory and processing larger FOVs. We perform experiments on the LIDC-IDRI dataset and an in-house prostate dataset, and present comparisons with PHiSeg. The results demonstrate that RevPHiSeg consumes ~30% less memory compared to PHiSeg while achieving very similar segmentation accuracy.



### Spontaneous preterm birth prediction using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07000v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07000v2)
- **Published**: 2020-08-16 21:21:33+00:00
- **Updated**: 2020-08-21 19:35:33+00:00
- **Authors**: Tomasz Włodarczyk, Szymon Płotka, Przemysław Rokita, Nicole Sochacki-Wójcicka, Jakub Wójcicki, Michał Lipa, Tomasz Trzciński
- **Comment**: Accepted at MICCAI Workshop on Perinatal, Preterm and Paediatric
  Image analysis (PIPPI) 2020, Lima, Peru
- **Journal**: None
- **Summary**: An estimated 15 million babies are born too early every year. Approximately 1 million children die each year due to complications of preterm birth (PTB). Many survivors face a lifetime of disability, including learning disabilities and visual and hearing problems. Although manual analysis of ultrasound images (US) is still prevalent, it is prone to errors due to its subjective component and complex variations in the shape and position of organs across patients. In this work, we introduce a conceptually simple convolutional neural network (CNN) trained for segmenting prenatal ultrasound images and classifying task for the purpose of preterm birth detection. Our method efficiently segments different types of cervixes in transvaginal ultrasound images while simultaneously predicting a preterm birth based on extracted image features without human oversight. We employed three popular network models: U-Net, Fully Convolutional Network, and Deeplabv3 for the cervix segmentation task. Based on the conducted results and model efficiency, we decided to extend U-Net by adding a parallel branch for classification task. The proposed model is trained and evaluated on a dataset consisting of 354 2D transvaginal ultrasound images and achieved a segmentation accuracy with a mean Jaccard coefficient index of 0.923 $\pm$ 0.081 and a classification sensitivity of 0.677 $\pm$ 0.042 with a 3.49\% false positive rate. Our method obtained better results in the prediction of preterm birth based on transvaginal ultrasound images compared to state-of-the-art methods.



### Learning Disentangled Expression Representations from Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2008.07001v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07001v2)
- **Published**: 2020-08-16 21:23:32+00:00
- **Updated**: 2020-08-18 06:58:13+00:00
- **Authors**: Marah Halawa, Manuel Wöllhaf, Eduardo Vellasques, Urko Sánchez Sanz, Olaf Hellwich
- **Comment**: Accepted at ECCV2020 workshops
- **Journal**: None
- **Summary**: Face images are subject to many different factors of variation, especially in unconstrained in-the-wild scenarios. For most tasks involving such images, e.g. expression recognition from video streams, having enough labeled data is prohibitively expensive. One common strategy to tackle such a problem is to learn disentangled representations for the different factors of variation of the observed data using adversarial learning. In this paper, we use a formulation of the adversarial loss to learn disentangled representations for face images. The used model facilitates learning on single-task datasets and improves the state-of-the-art in expression recognition with an accuracy of60.53%on the AffectNetdataset, without using any additional data.



### Monocular Instance Motion Segmentation for Autonomous Driving: KITTI InstanceMotSeg Dataset and Multi-task Baseline
- **Arxiv ID**: http://arxiv.org/abs/2008.07008v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.07008v4)
- **Published**: 2020-08-16 21:47:09+00:00
- **Updated**: 2021-05-26 15:12:49+00:00
- **Authors**: Eslam Mohamed, Mahmoud Ewaisha, Mennatullah Siam, Hazem Rashed, Senthil Yogamani, Waleed Hamdy, Muhammad Helmi, Ahmad El-Sallab
- **Comment**: Accepted for presentation at IEEE IV 2021 (Intelligent Vehicles
  Symposium) conference
- **Journal**: None
- **Summary**: Moving object segmentation is a crucial task for autonomous vehicles as it can be used to segment objects in a class agnostic manner based on their motion cues. It enables the detection of unseen objects during training (e.g., moose or a construction truck) based on their motion and independent of their appearance. Although pixel-wise motion segmentation has been studied in autonomous driving literature, it has been rarely addressed at the instance level, which would help separate connected segments of moving objects leading to better trajectory planning. As the main issue is the lack of large public datasets, we create a new InstanceMotSeg dataset comprising of 12.9K samples improving upon our KITTIMoSeg dataset. In addition to providing instance level annotations, we have added 4 additional classes which is crucial for studying class agnostic motion segmentation. We adapt YOLACT and implement a motion-based class agnostic instance segmentation model which would act as a baseline for the dataset. We also extend it to an efficient multi-task model which additionally provides semantic instance segmentation sharing the encoder. The model then learns separate prototype coefficients within the class agnostic and semantic heads providing two independent paths of object detection for redundant safety. To obtain real-time performance, we study different efficient encoders and obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an improvement of 10% mAP relative to the baseline. Our model improves the previous state of the art motion segmentation method by 3.3%. The dataset and qualitative results video are shared in our website at https://sites.google.com/view/instancemotseg/.



### DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/2008.07012v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07012v2)
- **Published**: 2020-08-16 22:05:13+00:00
- **Updated**: 2021-04-03 06:25:46+00:00
- **Authors**: Yanchao Yang, Brian Lai, Stefano Soatto
- **Comment**: camera-ready version for CVPR 2021
- **Journal**: None
- **Summary**: We describe an unsupervised method to detect and segment portions of images of live scenes that, at some point in time, are seen moving as a coherent whole, which we refer to as objects. Our method first partitions the motion field by minimizing the mutual information between segments. Then, it uses the segments to learn object models that can be used for detection in a static image. Static and dynamic models are represented by deep neural networks trained jointly in a bootstrapping strategy, which enables extrapolation to previously unseen objects. While the training process requires motion, the resulting object segmentation network can be used on either static images or videos at inference time. As the volume of seen videos grows, more and more objects are seen moving, priming their detection, which then serves as a regularizer for new objects, turning our method into unsupervised continual learning to segment objects. Our models are compared to the state of the art in both video object segmentation and salient object detection. In the six benchmark datasets tested, our models compare favorably even to those using pixel-level supervision, despite requiring no manual annotation.



### Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07015v2)
- **Published**: 2020-08-16 22:14:48+00:00
- **Updated**: 2020-08-18 18:31:40+00:00
- **Authors**: Elahe Arani, Fahad Sarfraz, Bahram Zonooz
- **Comment**: Accepted at 31st British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: Adversarial training has been proven to be an effective technique for improving the adversarial robustness of models. However, there seems to be an inherent trade-off between optimizing the model for accuracy and robustness. To this end, we propose Adversarial Concurrent Training (ACT), which employs adversarial training in a collaborative learning framework whereby we train a robust model in conjunction with a natural model in a minimax game. ACT encourages the two models to align their feature space by using the task-specific decision boundaries and explore the input space more broadly. Furthermore, the natural model acts as a regularizer, enforcing priors on features that the robust model should learn. Our analyses on the behavior of the models show that ACT leads to a robust model with lower model complexity, higher information compression in the learned representations, and high posterior entropy solutions indicative of convergence to a flatter minima. We demonstrate the effectiveness of the proposed approach across different datasets and network architectures. On ImageNet, ACT achieves 68.20% standard accuracy and 44.29% robustness accuracy under a 100-iteration untargeted attack, improving upon the standard adversarial training method's 65.70% standard accuracy and 42.36% robustness.



### AutoPose: Searching Multi-Scale Branch Aggregation for Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.07018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07018v1)
- **Published**: 2020-08-16 22:27:43+00:00
- **Updated**: 2020-08-16 22:27:43+00:00
- **Authors**: Xinyu Gong, Wuyang Chen, Yifan Jiang, Ye Yuan, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present AutoPose, a novel neural architecture search(NAS) framework that is capable of automatically discovering multiple parallel branches of cross-scale connections towards accurate and high-resolution 2D human pose estimation. Recently, high-performance hand-crafted convolutional networks for pose estimation show growing demands on multi-scale fusion and high-resolution representations. However, current NAS works exhibit limited flexibility on scale searching, they dominantly adopt simplified search spaces of single-branch architectures. Such simplification limits the fusion of information at different scales and fails to maintain high-resolution representations. The presentedAutoPose framework is able to search for multi-branch scales and network depth, in addition to the cell-level microstructure. Motivated by the search space, a novel bi-level optimization method is presented, where the network-level architecture is searched via reinforcement learning, and the cell-level search is conducted by the gradient-based method. Within 2.5 GPU days, AutoPose is able to find very competitive architectures on the MS COCO dataset, that are also transferable to the MPII dataset. Our code is available at https://github.com/VITA-Group/AutoPose.



### Training CNN Classifiers for Semantic Segmentation using Partially Annotated Images: with Application on Human Thigh and Calf MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.07030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07030v1)
- **Published**: 2020-08-16 23:38:02+00:00
- **Updated**: 2020-08-16 23:38:02+00:00
- **Authors**: Chun Kit Wong, Stephanie Marchesseau, Maria Kalimeri, Tiang Siew Yap, Serena S. H. Teo, Lingaraj Krishna, Alfredo Franco-Obregón, Stacey K. H. Tay, Chin Meng Khoo, Philip T. H. Lee, Melvin K. S. Leow, John J. Totman, Mary C. Stephenson
- **Comment**: Submitted to IEEE Transactions on Medical Imaging (Special Issue on
  Annotation-Efficient Deep Learning for Medical Imaging)
- **Journal**: None
- **Summary**: Objective: Medical image datasets with pixel-level labels tend to have a limited number of organ or tissue label classes annotated, even when the images have wide anatomical coverage. With supervised learning, multiple classifiers are usually needed given these partially annotated datasets. In this work, we propose a set of strategies to train one single classifier in segmenting all label classes that are heterogeneously annotated across multiple datasets without moving into semi-supervised learning. Methods: Masks were first created from each label image through a process we termed presence masking. Three presence masking modes were evaluated, differing mainly in weightage assigned to the annotated and unannotated classes. These masks were then applied to the loss function during training to remove the influence of unannotated classes. Results: Evaluation against publicly available CT datasets shows that presence masking is a viable method for training class-generic classifiers. Our class-generic classifier can perform as well as multiple class-specific classifiers combined, while the training duration is similar to that required for one class-specific classifier. Furthermore, the class-generic classifier can outperform the class-specific classifiers when trained on smaller datasets. Finally, consistent results are observed from evaluations against human thigh and calf MRI datasets collected in-house. Conclusion: The evaluation outcomes show that presence masking is capable of significantly improving both training and inference efficiency across imaging modalities and anatomical regions. Improved performance may even be observed on small datasets. Significance: Presence masking strategies can reduce the computational resources and costs involved in manual medical image annotations. All codes are publicly available at https://github.com/wong-ck/DeepSegment.



