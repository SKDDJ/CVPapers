# Arxiv Papers in cs.CV on 2020-08-06
### StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2008.02401v2
- **DOI**: 10.1145/3447648
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.02401v2)
- **Published**: 2020-08-06 00:10:03+00:00
- **Updated**: 2020-09-20 15:39:46+00:00
- **Authors**: Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka
- **Comment**: "Project Page https://rameenabdal.github.io/StyleFlow Video:
  https://youtu.be/LRAUJUn3EqQ "
- **Journal**: ACM Transactions on Graphics (TOG), 2021
- **Summary**: High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.



### Cross-Model Image Annotation Platform with Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02421v1)
- **Published**: 2020-08-06 01:42:25+00:00
- **Updated**: 2020-08-06 01:42:25+00:00
- **Authors**: Ng Hui Xian Lynnette, Henry Ng Siong Hock, Nguwi Yok Yen
- **Comment**: 8 pages.2 figures. 1 table
- **Journal**: None
- **Summary**: We have seen significant leapfrog advancement in machine learning in recent decades. The central idea of machine learnability lies on constructing learning algorithms that learn from good data. The availability of more data being made publicly available also accelerates the growth of AI in recent years. In the domain of computer vision, the quality of image data arises from the accuracy of image annotation. Labeling large volume of image data is a daunting and tedious task. This work presents an End-to-End pipeline tool for object annotation and recognition aims at enabling quick image labeling. We have developed a modular image annotation platform which seamlessly incorporates assisted image annotation (annotation assistance), active learning and model training and evaluation. Our approach provides a number of advantages over current image annotation tools. Firstly, the annotation assistance utilizes reference hierarchy and reference images to locate the objects in the images, thus reducing the need for annotating the whole object. Secondly, images can be annotated using polygon points allowing for objects of any shape to be annotated. Thirdly, it is also interoperable across several image models, and the tool provides an interface for object model training and evaluation across a series of pre-trained models. We have tested the model and embeds several benchmarking deep learning models. The highest accuracy achieved is 74%.



### Salvage Reusable Samples from Noisy Data for Robust Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02427v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.02427v1)
- **Published**: 2020-08-06 02:07:21+00:00
- **Updated**: 2020-08-06 02:07:21+00:00
- **Authors**: Zeren Sun, Xian-Sheng Hua, Yazhou Yao, Xiu-Shen Wei, Guosheng Hu, Jian Zhang
- **Comment**: accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Due to the existence of label noise in web images and the high memorization capacity of deep neural networks, training deep fine-grained (FG) models directly through web images tends to have an inferior recognition ability. In the literature, to alleviate this issue, loss correction methods try to estimate the noise transition matrix, but the inevitable false correction would cause severe accumulated errors. Sample selection methods identify clean ("easy") samples based on the fact that small losses can alleviate the accumulated errors. However, "hard" and mislabeled examples that can both boost the robustness of FG models are also dropped. To this end, we propose a certainty-based reusable sample selection and correction approach, termed as CRSSC, for coping with label noise in training deep FG models with web images. Our key idea is to additionally identify and correct reusable samples, and then leverage them together with clean examples to update the networks. We demonstrate the superiority of the proposed approach from both theoretical and experimental perspectives.



### GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.02436v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02436v1)
- **Published**: 2020-08-06 03:00:09+00:00
- **Updated**: 2020-08-06 03:00:09+00:00
- **Authors**: Ying Liu, Wenhong Cai, Xiaohui Yuan, Jinhai Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Although Generative Adversarial Networks have shown remarkable performance in image generation, there are some challenges in image realism and convergence speed. The results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model(GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. With a simple network structure, GL-GAN is allowed to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, by using feature map cues from discriminator output, we propose the adaptive local and global optimization method(Ada-OP) for specific implementation and find that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, CelebA-HQ and LSUN datasets.



### Data-driven Meta-set Based Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.02438v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.02438v1)
- **Published**: 2020-08-06 03:04:16+00:00
- **Updated**: 2020-08-06 03:04:16+00:00
- **Authors**: Chuanyi Zhang, Yazhou Yao, Xiangbo Shu, Zechao Li, Zhenmin Tang, Qi Wu
- **Comment**: accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Constructing fine-grained image datasets typically requires domain-specific expert knowledge, which is not always available for crowd-sourcing platform annotators. Accordingly, learning directly from web images becomes an alternative method for fine-grained visual recognition. However, label noise in the web training set can severely degrade the model performance. To this end, we propose a data-driven meta-set based approach to deal with noisy web images for fine-grained recognition. Specifically, guided by a small amount of clean meta-set, we train a selection net in a meta-learning manner to distinguish in- and out-of-distribution noisy images. To further boost the robustness of model, we also learn a labeling net to correct the labels of in-distribution noisy data. In this way, our proposed method can alleviate the harmful effects caused by out-of-distribution noise and properly exploit the in-distribution noisy samples for training. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is much superior to state-of-the-art noise-robust methods.



### Group Activity Prediction with Sequential Relational Anticipation Model
- **Arxiv ID**: http://arxiv.org/abs/2008.02441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02441v1)
- **Published**: 2020-08-06 03:17:14+00:00
- **Updated**: 2020-08-06 03:17:14+00:00
- **Authors**: Junwen Chen, Wentao Bao, Yu Kong
- **Comment**: This paper is accepted to ECCV2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach to predict group activities given the beginning frames with incomplete activity executions. Existing action prediction approaches learn to enhance the representation power of the partial observation. However, for group activity prediction, the relation evolution of people's activity and their positions over time is an important cue for predicting group activity. To this end, we propose a sequential relational anticipation model (SRAM) that summarizes the relational dynamics in the partial observation and progressively anticipates the group representations with rich discriminative information. Our model explicitly anticipates both activity features and positions by two graph auto-encoders, aiming to learn a discriminative group representation for group activity prediction. Experimental results on two popularly used datasets demonstrate that our approach significantly outperforms the state-of-the-art activity prediction methods.



### Fine-grained Iterative Attention Network for TemporalLanguage Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.02448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02448v1)
- **Published**: 2020-08-06 04:09:03+00:00
- **Updated**: 2020-08-06 04:09:03+00:00
- **Authors**: Xiaoye Qu, Pengwei Tang, Zhikang Zhou, Yu Cheng, Jianfeng Dong, Pan Zhou
- **Comment**: ACM MM 2020
- **Journal**: None
- **Summary**: Temporal language localization in videos aims to ground one video segment in an untrimmed video based on a given sentence query. To tackle this task, designing an effective model to extract ground-ing information from both visual and textual modalities is crucial. However, most previous attempts in this field only focus on unidirectional interactions from video to query, which emphasizes which words to listen and attends to sentence information via vanilla soft attention, but clues from query-by-video interactions implying where to look are not taken into consideration. In this paper, we propose a Fine-grained Iterative Attention Network (FIAN) that consists of an iterative attention module for bilateral query-video in-formation extraction. Specifically, in the iterative attention module, each word in the query is first enhanced by attending to each frame in the video through fine-grained attention, then video iteratively attends to the integrated query. Finally, both video and query information is utilized to provide robust cross-modal representation for further moment localization. In addition, to better predict the target segment, we propose a content-oriented localization strategy instead of applying recent anchor-based localization. We evaluate the proposed method on three challenging public benchmarks: Ac-tivityNet Captions, TACoS, and Charades-STA. FIAN significantly outperforms the state-of-the-art approaches.



### Structured Convolutions for Efficient Neural Network Design
- **Arxiv ID**: http://arxiv.org/abs/2008.02454v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.02454v2)
- **Published**: 2020-08-06 04:38:38+00:00
- **Updated**: 2020-10-31 04:41:55+00:00
- **Authors**: Yash Bhalgat, Yizhe Zhang, Jamie Lin, Fatih Porikli
- **Comment**: Camera-ready for NeurIPS 2020
- **Journal**: None
- **Summary**: In this work, we tackle model efficiency by exploiting redundancy in the \textit{implicit structure} of the building blocks of convolutional neural networks. We start our analysis by introducing a general definition of Composite Kernel structures that enable the execution of convolution operations in the form of efficient, scaled, sum-pooling components. As its special case, we propose \textit{Structured Convolutions} and show that these allow decomposition of the convolution operation into a sum-pooling operation followed by a convolution with significantly lower complexity and fewer weights. We show how this decomposition can be applied to 2D and 3D kernels as well as the fully-connected layers. Furthermore, we present a Structural Regularization loss that promotes neural network layers to leverage on this desired structure in a way that, after training, they can be decomposed with negligible performance loss. By applying our method to a wide range of CNN architectures, we demonstrate "structured" versions of the ResNets that are up to 2$\times$ smaller and a new Structured-MobileNetV2 that is more efficient while staying within an accuracy loss of 1% on ImageNet and CIFAR-10 datasets. We also show similar structured versions of EfficientNet on ImageNet and HRNet architecture for semantic segmentation on the Cityscapes dataset. Our method performs equally well or superior in terms of the complexity reduction in comparison to the existing tensor decomposition and channel pruning methods.



### Graph Convolutional Networks for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.02457v2
- **DOI**: 10.1109/TGRS.2020.3015157
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02457v2)
- **Published**: 2020-08-06 05:01:25+00:00
- **Updated**: 2021-01-15 00:06:09+00:00
- **Authors**: Danfeng Hong, Lianru Gao, Jing Yao, Bing Zhang, Antonio Plaza, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: To read the final version please go to IEEE TGRS on IEEE Xplore. Convolutional neural networks (CNNs) have been attracting increasing attention in hyperspectral (HS) image classification, owing to their ability to capture spatial-spectral feature representations. Nevertheless, their ability in modeling relations between samples remains limited. Beyond the limitations of grid sampling, graph convolutional networks (GCNs) have been recently proposed and successfully applied in irregular (or non-grid) data representation and analysis. In this paper, we thoroughly investigate CNNs and GCNs (qualitatively and quantitatively) in terms of HS image classification. Due to the construction of the adjacency matrix on all the data, traditional GCNs usually suffer from a huge computational cost, particularly in large-scale remote sensing (RS) problems. To this end, we develop a new mini-batch GCN (called miniGCN hereinafter) which allows to train large-scale GCNs in a mini-batch fashion. More significantly, our miniGCN is capable of inferring out-of-sample data without re-training networks and improving classification performance. Furthermore, as CNNs and GCNs can extract different types of HS features, an intuitive solution to break the performance bottleneck of a single model is to fuse them. Since miniGCNs can perform batch-wise network training (enabling the combination of CNNs and GCNs) we explore three fusion strategies: additive fusion, element-wise multiplicative fusion, and concatenation fusion to measure the obtained performance gain. Extensive experiments, conducted on three HS datasets, demonstrate the advantages of miniGCNs over GCNs and the superiority of the tested fusion strategies with regards to the single CNN or GCN models. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_GCN for the sake of reproducibility.



### Decomposition of Longitudinal Deformations via Beltrami Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2008.03154v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03154v2)
- **Published**: 2020-08-06 05:47:36+00:00
- **Updated**: 2021-03-31 03:54:43+00:00
- **Authors**: Ho Law, Lok Ming Lui, Chun Yin Siu
- **Comment**: arXiv admin note: text overlap with arXiv:1402.6908 by other authors
- **Journal**: None
- **Summary**: We present a mathematical model to decompose a longitudinal deformation into normal and abnormal components. The goal is to detect and extract subtle quivers from periodic motions in a video sequence. It has important applications in medical image analysis. To achieve this goal, we consider a representation of the longitudinal deformation, called the Beltrami descriptor, based on quasiconformal theories. The Beltrami descriptor is a complex-valued matrix. Each longitudinal deformation is associated to a Beltrami descriptor and vice versa. To decompose the longitudinal deformation, we propose to carry out the low rank and sparse decomposition of the Beltrami descriptor. The low rank component corresponds to the periodic motions, whereas the sparse part corresponds to the abnormal motions of a longitudinal deformation. Experiments have been carried out on both synthetic and real video sequences. Results demonstrate the efficacy of our proposed model to decompose a longitudinal deformation into regular and irregular components.



### Few-shot Classification via Adaptive Attention
- **Arxiv ID**: http://arxiv.org/abs/2008.02465v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02465v2)
- **Published**: 2020-08-06 05:52:59+00:00
- **Updated**: 2020-11-21 15:15:20+00:00
- **Authors**: Zihang Jiang, Bingyi Kang, Kuangqi Zhou, Jiashi Feng
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Training a neural network model that can quickly adapt to a new task is highly desirable yet challenging for few-shot learning problems. Recent few-shot learning methods mostly concentrate on developing various meta-learning strategies from two aspects, namely optimizing an initial model or learning a distance metric. In this work, we propose a novel few-shot learning method via optimizing and fast adapting the query sample representation based on very few reference samples. To be specific, we devise a simple and efficient meta-reweighting strategy to adapt the sample representations and generate soft attention to refine the representation such that the relevant features from the query and support samples can be extracted for a better few-shot classification. Such an adaptive attention model is also able to explain what the classification model is looking for as the evidence for classification to some extent. As demonstrated experimentally, the proposed model achieves state-of-the-art classification results on various benchmark few-shot classification and fine-grained recognition datasets.



### Zero-Shot Multi-View Indoor Localization via Graph Location Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.02492v1
- **DOI**: 10.1145/3394171.3413856
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.02492v1)
- **Published**: 2020-08-06 07:36:55+00:00
- **Updated**: 2020-08-06 07:36:55+00:00
- **Authors**: Meng-Jiun Chiou, Zhenguang Liu, Yifang Yin, Anan Liu, Roger Zimmermann
- **Comment**: Accepted at ACM MM 2020. 10 pages, 7 figures. Code and datasets
  available at
  https://github.com/coldmanck/zero-shot-indoor-localization-release
- **Journal**: Proceedings of the 28th ACM International Conference on
  Multimedia, 2020
- **Summary**: Indoor localization is a fundamental problem in location-based applications. Current approaches to this problem typically rely on Radio Frequency technology, which requires not only supporting infrastructures but human efforts to measure and calibrate the signal. Moreover, data collection for all locations is indispensable in existing methods, which in turn hinders their large-scale deployment. In this paper, we propose a novel neural network based architecture Graph Location Networks (GLN) to perform infrastructure-free, multi-view image based indoor localization. GLN makes location predictions based on robust location representations extracted from images through message-passing networks. Furthermore, we introduce a novel zero-shot indoor localization setting and tackle it by extending the proposed GLN to a dedicated zero-shot version, which exploits a novel mechanism Map2Vec to train location-aware embeddings and make predictions on novel unseen locations. Our extensive experiments show that the proposed approach outperforms state-of-the-art methods in the standard setting, and achieves promising accuracy even in the zero-shot setting where data for half of the locations are not available. The source code and datasets are publicly available at https://github.com/coldmanck/zero-shot-indoor-localization-release.



### Gender and Ethnicity Classification based on Palmprint and Palmar Hand Images from Uncontrolled Environment
- **Arxiv ID**: http://arxiv.org/abs/2008.02500v1
- **DOI**: 10.1109/IJCB48548.2020.9304907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02500v1)
- **Published**: 2020-08-06 07:50:06+00:00
- **Updated**: 2020-08-06 07:50:06+00:00
- **Authors**: Wojciech Michal Matkowski, Adams Wai Kin Kong
- **Comment**: Accepted in the International Joint Conference on Biometrics (IJCB
  2020), scheduled for Sep 28-Oct 1, 2020
- **Journal**: None
- **Summary**: Soft biometric attributes such as gender, ethnicity or age may provide useful information for biometrics and forensics applications. Researchers used, e.g., face, gait, iris, and hand, etc. to classify such attributes. Even though hand has been widely studied for biometric recognition, relatively less attention has been given to soft biometrics from hand. Previous studies of soft biometrics based on hand images focused on gender and well-controlled imaging environment. In this paper, the gender and ethnicity classification in uncontrolled environment are considered. Gender and ethnicity labels are collected and provided for subjects in a publicly available database, which contains hand images from the Internet. Five deep learning models are fine-tuned and evaluated in gender and ethnicity classification scenarios based on palmar 1) full hand, 2) segmented hand and 3) palmprint images. The experimental results indicate that for gender and ethnicity classification in uncontrolled environment, full and segmented hand images are more suitable than palmprint images.



### Object-based Illumination Estimation with Rendering-aware Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.02514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02514v1)
- **Published**: 2020-08-06 08:23:19+00:00
- **Updated**: 2020-08-06 08:23:19+00:00
- **Authors**: Xin Wei, Guojun Chen, Yue Dong, Stephen Lin, Xin Tong
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present a scheme for fast environment light estimation from the RGBD appearance of individual objects and their local image areas. Conventional inverse rendering is too computationally demanding for real-time applications, and the performance of purely learning-based techniques may be limited by the meager input data available from individual objects. To address these issues, we propose an approach that takes advantage of physical principles from inverse rendering to constrain the solution, while also utilizing neural networks to expedite the more computationally expensive portions of its processing, to increase robustness to noisy input data as well as to improve temporal and spatial stability. This results in a rendering-aware system that estimates the local illumination distribution at an object with high accuracy and in real time. With the estimated lighting, virtual objects can be rendered in AR scenarios with shading that is consistent to the real scene, leading to improved realism.



### FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire
- **Arxiv ID**: http://arxiv.org/abs/2008.02516v4
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2008.02516v4)
- **Published**: 2020-08-06 08:28:56+00:00
- **Updated**: 2021-03-15 07:23:19+00:00
- **Authors**: Jinglin Liu, Yi Ren, Zhou Zhao, Chen Zhang, Baoxing Huai, Nicholas Jing Yuan
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Lipreading is an impressive technique and there has been a definite improvement of accuracy in recent years. However, existing methods for lipreading mainly build on autoregressive (AR) model, which generate target tokens one by one and suffer from high inference latency. To breakthrough this constraint, we propose FastLR, a non-autoregressive (NAR) lipreading model which generates all target tokens simultaneously. NAR lipreading is a challenging task that has many difficulties: 1) the discrepancy of sequence lengths between source and target makes it difficult to estimate the length of the output sequence; 2) the conditionally independent behavior of NAR generation lacks the correlation across time which leads to a poor approximation of target distribution; 3) the feature representation ability of encoder can be weak due to lack of effective alignment mechanism; and 4) the removal of AR language model exacerbates the inherent ambiguity problem of lipreading. Thus, in this paper, we introduce three methods to reduce the gap between FastLR and AR model: 1) to address challenges 1 and 2, we leverage integrate-and-fire (I\&F) module to model the correspondence between source video frames and output text sequence. 2) To tackle challenge 3, we add an auxiliary connectionist temporal classification (CTC) decoder to the top of the encoder and optimize it with extra CTC loss. We also add an auxiliary autoregressive decoder to help the feature extraction of encoder. 3) To overcome challenge 4, we propose a novel Noisy Parallel Decoding (NPD) for I\&F and bring Byte-Pair Encoding (BPE) into lipreading. Our experiments exhibit that FastLR achieves the speedup up to 10.97$\times$ comparing with state-of-the-art lipreading model with slight WER absolute increase of 1.5\% and 5.5\% on GRID and LRS2 lipreading datasets respectively, which demonstrates the effectiveness of our proposed method.



### Dual Gaussian-based Variational Subspace Disentanglement for Visible-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.02520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02520v1)
- **Published**: 2020-08-06 08:43:35+00:00
- **Updated**: 2020-08-06 08:43:35+00:00
- **Authors**: Nan Pu, Wei Chen, Yu Liu, Erwin M. Bakker, Michael S. Lew
- **Comment**: Accepted by ACM MM 2020 poster. 12 pages, 10 appendixes
- **Journal**: None
- **Summary**: Visible-infrared person re-identification (VI-ReID) is a challenging and essential task in night-time intelligent surveillance systems. Except for the intra-modality variance that RGB-RGB person re-identification mainly overcomes, VI-ReID suffers from additional inter-modality variance caused by the inherent heterogeneous gap. To solve the problem, we present a carefully designed dual Gaussian-based variational auto-encoder (DG-VAE), which disentangles an identity-discriminable and an identity-ambiguous cross-modality feature subspace, following a mixture-of-Gaussians (MoG) prior and a standard Gaussian distribution prior, respectively. Disentangling cross-modality identity-discriminable features leads to more robust retrieval for VI-ReID. To achieve efficient optimization like conventional VAE, we theoretically derive two variational inference terms for the MoG prior under the supervised setting, which not only restricts the identity-discriminable subspace so that the model explicitly handles the cross-modality intra-identity variance, but also enables the MoG distribution to avoid posterior collapse. Furthermore, we propose a triplet swap reconstruction (TSR) strategy to promote the above disentangling process. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two VI-ReID datasets.



### Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.02531v2
- **DOI**: 10.1145/3394171.3413694
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02531v2)
- **Published**: 2020-08-06 09:08:14+00:00
- **Updated**: 2020-08-12 07:28:38+00:00
- **Authors**: Li Tao, Xueting Wang, Toshihiko Yamasaki
- **Comment**: Accepted by ACMMM 2020. Our project page is at
  https://bestjuly.github.io/Inter-intra-video-contrastive-learning/
- **Journal**: None
- **Summary**: We propose a self-supervised method to learn feature representations from videos. A standard approach in traditional self-supervised methods uses positive-negative data pairs to train with contrastive learning strategy. In such a case, different modalities of the same video are treated as positives and video clips from a different video are treated as negatives. Because the spatio-temporal information is important for video representation, we extend the negative samples by introducing intra-negative samples, which are transformed from the same anchor video by breaking temporal relations in video clips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn video representations. There are many flexible options in our IIC framework and we conduct experiments by using several different configurations. Evaluations are conducted on video retrieval and video recognition tasks using the learned video representation. Our proposed IIC outperforms current state-of-the-art results by a large margin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, improvements can also be obtained on these two benchmark datasets. Code is available at https://github.com/BestJuly/Inter-intra-video-contrastive-learning.



### Handwritten Character Recognition from Wearable Passive RFID
- **Arxiv ID**: http://arxiv.org/abs/2008.02543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2008.02543v1)
- **Published**: 2020-08-06 09:45:29+00:00
- **Updated**: 2020-08-06 09:45:29+00:00
- **Authors**: Leevi Raivio, Han He, Johanna Virkki, Heikki Huttunen
- **Comment**: Submitted to ICPR2020
- **Journal**: None
- **Summary**: In this paper we study the recognition of handwritten characters from data captured by a novel wearable electro-textile sensor panel. The data is collected sequentially, such that we record both the stroke order and the resulting bitmap. We propose a preprocessing pipeline that fuses the sequence and bitmap representations together. The data is collected from ten subjects containing altogether 7500 characters. We also propose a convolutional neural network architecture, whose novel upsampling structure enables successful use of conventional ImageNet pretrained networks, despite the small input size of only 10x10 pixels. The proposed model reaches 72\% accuracy in experimental tests, which can be considered good accuracy for this challenging dataset. Both the data and the model are released to the public.



### Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into Cognizance
- **Arxiv ID**: http://arxiv.org/abs/2008.02565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, C.0; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.02565v1)
- **Published**: 2020-08-06 10:38:07+00:00
- **Updated**: 2020-08-06 10:38:07+00:00
- **Authors**: Nandan Kumar Jha, Sparsh Mittal
- **Comment**: Accepted at IEEE Transactions on Computers (Special Issue on
  Machine-Learning Architectures and Accelerators) 2020
- **Journal**: None
- **Summary**: In recent years, researchers have focused on reducing the model size and number of computations (measured as "multiply-accumulate" or MAC operations) of DNNs. The energy consumption of a DNN depends on both the number of MAC operations and the energy efficiency of each MAC operation. The former can be estimated at design time; however, the latter depends on the intricate data reuse patterns and underlying hardware architecture. Hence, estimating it at design time is challenging. This work shows that the conventional approach to estimate the data reuse, viz. arithmetic intensity, does not always correctly estimate the degree of data reuse in DNNs since it gives equal importance to all the data types. We propose a novel model, termed "data type aware weighted arithmetic intensity" ($DI$), which accounts for the unequal importance of different data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs on two GPUs. We show that our model accurately models data-reuse for all possible data reuse patterns for different types of convolution and different types of layers. We show that our model is a better indicator of the energy efficiency of DNNs. We also show its generality using the central limit theorem.



### Fast Approximate Modelling of the Next Combination Result for Stopping the Text Recognition in a Video
- **Arxiv ID**: http://arxiv.org/abs/2008.02566v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2008.02566v1)
- **Published**: 2020-08-06 10:42:30+00:00
- **Updated**: 2020-08-06 10:42:30+00:00
- **Authors**: Konstantin Bulatov, Nadezhda Fedotova, Vladimir V. Arlazarov
- **Comment**: 8 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we consider a task of stopping the video stream recognition process of a text field, in which each frame is recognized independently and the individual results are combined together. The video stream recognition stopping problem is an under-researched topic with regards to computer vision, but its relevance for building high-performance video recognition systems is clear.   Firstly, we describe an existing method of optimally stopping such a process based on a modelling of the next combined result. Then, we describe approximations and assumptions which allowed us to build an optimized computation scheme and thus obtain a method with reduced computational complexity.   The methods were evaluated for the tasks of document text field recognition and arbitrary text recognition in a video. The experimental comparison shows that the introduced approximations do not diminish the quality of the stopping method in terms of the achieved combined result precision, while dramatically reducing the time required to make the stopping decision. The results were consistent for both text recognition tasks.



### IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents
- **Arxiv ID**: http://arxiv.org/abs/2008.02569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02569v1)
- **Published**: 2020-08-06 10:59:20+00:00
- **Updated**: 2020-08-06 10:59:20+00:00
- **Authors**: Ajoy Mondal, Peter Lipps, C. V. Jawahar
- **Comment**: 15 pages, DAS 2020
- **Journal**: Published in DAS 2020
- **Summary**: We introduce a new dataset for graphical object detection in business documents, more specifically annual reports. This dataset, IIIT-AR-13k, is created by manually annotating the bounding boxes of graphical or page objects in publicly available annual reports. This dataset contains a total of 13k annotated page images with objects in five different popular categories - table, figure, natural image, logo, and signature. It is the largest manually annotated dataset for graphical object detection. Annual reports created in multiple languages for several years from various companies bring high diversity into this dataset. We benchmark IIIT-AR-13K dataset with two state of the art graphical object detection techniques using Faster R-CNN [20] and Mask R-CNN [11] and establish high baselines for further research. Our dataset is highly effective as training data for developing practical solutions for graphical object detection in both business documents and technical articles. By training with IIIT-AR-13K, we demonstrate the feasibility of a single solution that can report superior performance compared to the equivalent ones trained with a much larger amount of data, for table detection. We hope that our dataset helps in advancing the research for detecting various types of graphical objects in business documents.



### Optical Flow and Mode Selection for Learning-based Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2008.02580v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.02580v1)
- **Published**: 2020-08-06 11:21:22+00:00
- **Updated**: 2020-08-06 11:21:22+00:00
- **Authors**: Théo Ladune, Pierrick Philippe, Wassim Hamidouche, Lu Zhang, Olivier Déforges
- **Comment**: MMSP 2020, IEEE 22nd International Workshop on Multimedia Signal
  Processing, Sep 2020, Tampere, Finland
- **Journal**: None
- **Summary**: This paper introduces a new method for inter-frame coding based on two complementary autoencoders: MOFNet and CodecNet. MOFNet aims at computing and conveying the Optical Flow and a pixel-wise coding Mode selection. The optical flow is used to perform a prediction of the frame to code. The coding mode selection enables competition between direct copy of the prediction or transmission through CodecNet. The proposed coding scheme is assessed under the Challenge on Learned Image Compression 2020 (CLIC20) P-frame coding conditions, where it is shown to perform on par with the state-of-the-art video codec ITU/MPEG HEVC. Moreover, the possibility of copying the prediction enables to learn the optical flow in an end-to-end fashion i.e. without relying on pre-training and/or a dedicated loss term.



### MED-TEX: Transferring and Explaining Knowledge with Less Data from Pretrained Medical Imaging Models
- **Arxiv ID**: http://arxiv.org/abs/2008.02593v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02593v3)
- **Published**: 2020-08-06 11:50:32+00:00
- **Updated**: 2022-01-12 11:01:28+00:00
- **Authors**: Thanh Nguyen-Duc, He Zhao, Jianfei Cai, Dinh Phung
- **Comment**: None
- **Journal**: International Symposium on Biomedical Imaging (ISBI, 2022)
- **Summary**: Deep learning methods usually require a large amount of training data and lack interpretability. In this paper, we propose a novel knowledge distillation and model interpretation framework for medical image classification that jointly solves the above two issues. Specifically, to address the data-hungry issue, a small student model is learned with less data by distilling knowledge from a cumbersome pretrained teacher model. To interpret the teacher model and assist the learning of the student, an explainer module is introduced to highlight the regions of an input that are important for the predictions of the teacher model. Furthermore, the joint framework is trained by a principled way derived from the information-theoretic perspective. Our framework outperforms on the knowledge distillation and model interpretation tasks compared to state-of-the-art methods on a fundus dataset.



### Gibbs Sampling with People
- **Arxiv ID**: http://arxiv.org/abs/2008.02595v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2008.02595v2)
- **Published**: 2020-08-06 11:57:07+00:00
- **Updated**: 2020-11-02 16:55:40+00:00
- **Authors**: Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn, Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori Jacoby
- **Comment**: Accepted for oral presentation at NeurIPS 2020
- **Journal**: None
- **Summary**: A core problem in cognitive science and machine learning is to understand how humans derive semantic representations from perceptual objects, such as color from an apple, pleasantness from a musical chord, or seriousness from a face. Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying such representations, in which participants are presented with binary choice trials constructed such that the decisions follow a Markov Chain Monte Carlo acceptance rule. However, while MCMCP has strong asymptotic properties, its binary choice paradigm generates relatively little information per trial, and its local proposal function makes it slow to explore the parameter space and find the modes of the distribution. Here we therefore generalize MCMCP to a continuous-sampling paradigm, where in each iteration the participant uses a slider to continuously manipulate a single stimulus dimension to optimize a given criterion such as 'pleasantness'. We formulate both methods from a utility-theory perspective, and show that the new method can be interpreted as 'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation parameter to the transition step, and show that this parameter can be manipulated to flexibly shift between Gibbs sampling and deterministic optimization. In an initial study, we show GSP clearly outperforming MCMCP; we then show that GSP provides novel and interpretable results in three other domains, namely musical chords, vocal emotions, and faces. We validate these results through large-scale perceptual rating experiments. The final experiments use GSP to navigate the latent space of a state-of-the-art image synthesis network (StyleGAN), a promising approach for applying GSP to high-dimensional perceptual spaces. We conclude by discussing future cognitive applications and ethical implications.



### Image Generation for Efficient Neural Network Training in Autonomous Drone Racing
- **Arxiv ID**: http://arxiv.org/abs/2008.02596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.02596v1)
- **Published**: 2020-08-06 12:07:36+00:00
- **Updated**: 2020-08-06 12:07:36+00:00
- **Authors**: Theo Morales, Andriy Sarabakha, Erdal Kayacan
- **Comment**: 2020 International Joint Conference on Neural Networks (IJCNN 2020)
- **Journal**: None
- **Summary**: Drone racing is a recreational sport in which the goal is to pass through a sequence of gates in a minimum amount of time while avoiding collisions. In autonomous drone racing, one must accomplish this task by flying fully autonomously in an unknown environment by relying only on computer vision methods for detecting the target gates. Due to the challenges such as background objects and varying lighting conditions, traditional object detection algorithms based on colour or geometry tend to fail. Convolutional neural networks offer impressive advances in computer vision but require an immense amount of data to learn. Collecting this data is a tedious process because the drone has to be flown manually, and the data collected can suffer from sensor failures. In this work, a semi-synthetic dataset generation method is proposed, using a combination of real background images and randomised 3D renders of the gates, to provide a limitless amount of training samples that do not suffer from those drawbacks. Using the detection results, a line-of-sight guidance algorithm is used to cross the gates. In several experimental real-time tests, the proposed framework successfully demonstrates fast and reliable detection and navigation.



### Deep Learning Based Defect Detection for Solder Joints on Industrial X-Ray Circuit Board Images
- **Arxiv ID**: http://arxiv.org/abs/2008.02604v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02604v2)
- **Published**: 2020-08-06 12:25:18+00:00
- **Updated**: 2021-03-25 05:28:18+00:00
- **Authors**: Qianru Zhang, Meng Zhang, Chinthaka Gamanayake, Chau Yuen, Zehao Geng, Hirunima Jayasekara, Xuewen Zhang, Chia-wei Woo, Jenny Low, Xiang Liu
- **Comment**: Accepted by conference INDIN 2020
- **Journal**: None
- **Summary**: Quality control is of vital importance during electronics production. As the methods of producing electronic circuits improve, there is an increasing chance of solder defects during assembling the printed circuit board (PCB). Many technologies have been incorporated for inspecting failed soldering, such as X-ray imaging, optical imaging, and thermal imaging. With some advanced algorithms, the new technologies are expected to control the production quality based on the digital images. However, current algorithms sometimes are not accurate enough to meet the quality control. Specialists are needed to do a follow-up checking. For automated X-ray inspection, joint of interest on the X-ray image is located by region of interest (ROI) and inspected by some algorithms. Some incorrect ROIs deteriorate the inspection algorithm. The high dimension of X-ray images and the varying sizes of image dimensions also challenge the inspection algorithms. On the other hand, recent advances on deep learning shed light on image-based tasks and are competitive to human levels. In this paper, deep learning is incorporated in X-ray imaging based quality control during PCB quality inspection. Two artificial intelligence (AI) based models are proposed and compared for joint defect detection. The noised ROI problem and the varying sizes of imaging dimension problem are addressed. The efficacy of the proposed methods are verified through experimenting on a real-world 3D X-ray dataset. By incorporating the proposed methods, specialist inspection workload is largely saved.



### Approach for Document Detection by Contours and Contrasts
- **Arxiv ID**: http://arxiv.org/abs/2008.02615v2
- **DOI**: 10.1109/ICPR48806.2021.9413271
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02615v2)
- **Published**: 2020-08-06 12:44:40+00:00
- **Updated**: 2020-10-19 14:03:44+00:00
- **Authors**: Daniil V. Tropin, Sergey A. Ilyuhin, Dmitry P. Nikolaev, Vladimir V. Arlazarov
- **Comment**: This paper has been accepted to the ICPR 2020 conference in Milan
  which will be held on the 10-15 January 2021. Therefore this work has not yet
  been presented
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR),
  (2021) 9689-9695
- **Summary**: This paper considers arbitrary document detection performed on a mobile device. The classical contour-based approach often fails in cases featuring occlusion, complex background, or blur. The region-based approach, which relies on the contrast between object and background, does not have application limitations, however, its known implementations are highly resource-consuming. We propose a modification of the contour-based method, in which the competing contour location hypotheses are ranked according to the contrast between the areas inside and outside the border. In the experiments, such modification allows for the decrease of alternatives ordering errors by 40% and the decrease of the overall detection errors by 10%. The proposed method provides unmatched state-of-the-art performance on the open MIDV-500 dataset, and it demonstrates results comparable with state-of-the-art performance on the SmartDoc dataset.



### Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.02655v2
- **DOI**: 10.1007/978-3-030-66415-2_53
- **Categories**: **cs.CV**, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2008.02655v2)
- **Published**: 2020-08-06 13:45:52+00:00
- **Updated**: 2021-02-24 19:11:20+00:00
- **Authors**: Vikas Kumar, Shivansh Rao, Li Yu
- **Comment**: Accepted in ECCV workshop BEEU (Bodily Expressed Emotion
  Understanding) 2020
- **Journal**: None
- **Summary**: Facial expression recognition from videos in the wild is a challenging task due to the lack of abundant labelled training data. Large DNN (deep neural network) architectures and ensemble methods have resulted in better performance, but soon reach saturation at some point due to data inadequacy. In this paper, we use a self-training method that utilizes a combination of a labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD). Experimental analysis shows that training a noisy student network iteratively helps in achieving significantly better results. Additionally, our model isolates different regions of the face and processes them independently using a multi-level attention mechanism which further boosts the performance. Our results show that the proposed method achieves state-of-the-art performance on benchmark datasets CK+ and AFEW 8.0 when compared to other single models.



### Dynamic Emotion Modeling with Learnable Graphs and Graph Inception Network
- **Arxiv ID**: http://arxiv.org/abs/2008.02661v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.02661v2)
- **Published**: 2020-08-06 13:51:31+00:00
- **Updated**: 2021-02-08 12:21:00+00:00
- **Authors**: A. Shirian, S. Tripathi, T. Guha
- **Comment**: None
- **Journal**: 10.1109/TMM.2021.3059169
- **Summary**: Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion sensors (body gestures). We propose a generalized approach to emotion recognition that can adapt across modalities by modeling dynamic data as structured graphs. The motivation behind the graph approach is to build compact models without compromising on performance. To alleviate the problem of optimal graph construction, we cast this as a joint graph learning and classification task. To this end, we present the Learnable Graph Inception Network (L-GrIN) that jointly learns to recognize emotion and to identify the underlying graph structure in the dynamic data. Our architecture comprises multiple novel components: a new graph convolution operation, a graph inception layer, learnable adjacency, and a learnable pooling function that yields a graph-level embedding. We evaluate the proposed architecture on five benchmark emotion recognition databases spanning three different modalities (video, audio, motion capture), where each database captures one of the following emotional cues: facial expressions, speech and body gestures. We achieve state-of-the-art performance on all five databases outperforming several competitive baselines and relevant existing methods. Our graph architecture shows superior performance with significantly fewer parameters (compared to convolutional or recurrent neural networks) promising its applicability to resource-constrained devices.



### Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards
- **Arxiv ID**: http://arxiv.org/abs/2008.02693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02693v2)
- **Published**: 2020-08-06 14:52:13+00:00
- **Updated**: 2022-04-23 18:07:49+00:00
- **Authors**: Xuewen Yang, Heming Zhang, Di Jin, Yingru Liu, Chi-Hao Wu, Jianchao Tan, Dongliang Xie, Jue Wang, Xin Wang
- **Comment**: In proceedings of ECCV 2020
- **Journal**: None
- **Summary**: Generating accurate descriptions for online fashion items is important not only for enhancing customers' shopping experiences, but also for the increase of online sales. Besides the need of correctly presenting the attributes of items, the expressions in an enchanting style could better attract customer interests. The goal of this work is to develop a novel learning framework for accurate and expressive fashion captioning. Different from popular work on image captioning, it is hard to identify and describe the rich attributes of fashion items. We seed the description of an item by first identifying its attributes, and introduce attribute-level semantic (ALS) reward and sentence-level semantic (SLS) reward as metrics to improve the quality of text descriptions. We further integrate the training of our model with maximum likelihood estimation (MLE), attribute embedding, and Reinforcement Learning (RL). To facilitate the learning, we build a new FAshion CAptioning Dataset (FACAD), which contains 993K images and 130K corresponding enchanting and diverse descriptions. Experiments on FACAD demonstrate the effectiveness of our model.



### Pairwise Relation Learning for Semi-supervised Gland Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.02699v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02699v1)
- **Published**: 2020-08-06 15:02:38+00:00
- **Updated**: 2020-08-06 15:02:38+00:00
- **Authors**: Yutong Xie, Jianpeng Zhang, Zhibin Liao, Chunhua Shen, Johan Verjans, Yong Xia
- **Comment**: Accepted by MICCAI2020
- **Journal**: None
- **Summary**: Accurate and automated gland segmentation on histology tissue images is an essential but challenging task in the computer-aided diagnosis of adenocarcinoma. Despite their prevalence, deep learning models always require a myriad number of densely annotated training images, which are difficult to obtain due to extensive labor and associated expert costs related to histology image annotations. In this paper, we propose the pairwise relation-based semi-supervised (PRS^2) model for gland segmentation on histology images. This model consists of a segmentation network (S-Net) and a pairwise relation network (PR-Net). The S-Net is trained on labeled data for segmentation, and PR-Net is trained on both labeled and unlabeled data in an unsupervised way to enhance its image representation ability via exploiting the semantic consistency between each pair of images in the feature space. Since both networks share their encoders, the image representation ability learned by PR-Net can be transferred to S-Net to improve its segmentation performance. We also design the object-level Dice loss to address the issues caused by touching glands and combine it with other two loss functions for S-Net. We evaluated our model against five recent methods on the GlaS dataset and three recent methods on the CRAG dataset. Our results not only demonstrate the effectiveness of the proposed PR-Net and object-level Dice loss, but also indicate that our PRS^2 model achieves the state-of-the-art gland segmentation performance on both benchmarks.



### Exploring Relations in Untrimmed Videos for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02711v1)
- **Published**: 2020-08-06 15:29:25+00:00
- **Updated**: 2020-08-06 15:29:25+00:00
- **Authors**: Dezhao Luo, Bo Fang, Yu Zhou, Yucan Zhou, Dayan Wu, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing video self-supervised learning methods mainly rely on trimmed videos for model training. However, trimmed datasets are manually annotated from untrimmed videos. In this sense, these methods are not really self-supervised. In this paper, we propose a novel self-supervised method, referred to as Exploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly applied to untrimmed videos (real unlabeled) to learn spatio-temporal features. ERUV first generates single-shot videos by shot change detection. Then a designed sampling strategy is used to model relations for video clips. The strategy is saved as our self-supervision signals. Finally, the network learns representations by predicting the category of relations between the video clips. ERUV is able to compare the differences and similarities of videos, which is also an essential procedure for action and video related tasks. We validate our learned models with action recognition and video retrieval tasks with three kinds of 3D CNNs. Experimental results show that ERUV is able to learn richer representations and it outperforms state-of-the-art self-supervised methods with significant margins.



### A Sensitivity Analysis Approach for Evaluating a Radar Simulation for Virtual Testing of Autonomous Driving Functions
- **Arxiv ID**: http://arxiv.org/abs/2008.02725v4
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.02725v4)
- **Published**: 2020-08-06 15:51:52+00:00
- **Updated**: 2020-10-12 09:02:46+00:00
- **Authors**: Anthony Ngo, Max Paul Bauer, Michael Resch
- **Comment**: IEEE 2020 Asia-Pacific Conference on Intelligent Robot Systems (ACIRS
  2020)
- **Journal**: None
- **Summary**: Simulation-based testing is a promising approach to significantly reduce the validation effort of automated driving functions. Realistic models of environment perception sensors such as camera, radar and lidar play a key role in this testing strategy. A generally accepted method to validate these sensor models does not yet exist. Particularly radar has traditionally been one of the most difficult sensors to model. Although promising as an alternative to real test drives, virtual tests are time-consuming due to the fact that they simulate the entire radar system in detail, using computation-intensive simulation techniques to approximate the propagation of electromagnetic waves. In this paper, we introduce a sensitivity analysis approach for developing and evaluating a radar simulation, with the objective to identify the parameters with the greatest impact regarding the system under test. A modular radar system simulation is presented and parameterized to conduct a sensitivity analysis in order to evaluate a spatial clustering algorithm as the system under test, while comparing the output from the radar model to real driving measurements to ensure a realistic model behavior. The presented approach is evaluated and it is demonstrated that with this approach results from different situations can be traced back to the contribution of the individual sub-modules of the radar simulation.



### Shonan Rotation Averaging: Global Optimality by Surfing $SO(p)^n$
- **Arxiv ID**: http://arxiv.org/abs/2008.02737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02737v1)
- **Published**: 2020-08-06 16:08:23+00:00
- **Updated**: 2020-08-06 16:08:23+00:00
- **Authors**: Frank Dellaert, David M. Rosen, Jing Wu, Robert Mahony, Luca Carlone
- **Comment**: 30 pages (paper + supplementary material). To appear at the European
  Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Shonan Rotation Averaging is a fast, simple, and elegant rotation averaging algorithm that is guaranteed to recover globally optimal solutions under mild assumptions on the measurement noise. Our method employs semidefinite relaxation in order to recover provably globally optimal solutions of the rotation averaging problem. In contrast to prior work, we show how to solve large-scale instances of these relaxations using manifold minimization on (only slightly) higher-dimensional rotation manifolds, re-using existing high-performance (but local) structure-from-motion pipelines. Our method thus preserves the speed and scalability of current SFM methods, while recovering globally optimal solutions.



### Towards Accurate Pixel-wise Object Tracking by Attention Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.02745v3
- **DOI**: 10.1109/TIP.2021.3117077
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02745v3)
- **Published**: 2020-08-06 16:25:23+00:00
- **Updated**: 2020-09-08 02:06:33+00:00
- **Authors**: Zhipeng Zhang, Bing Li, Weiming Hu, Houwen Peng
- **Comment**: Some technical errors. We would provide new versions later
- **Journal**: None
- **Summary**: The encoding of the target in object tracking moves from the coarse bounding-box to fine-grained segmentation map recently. Revisiting de facto real-time approaches that are capable of predicting mask during tracking, we observed that they usually fork a light branch from the backbone network for segmentation. Although efficient, directly fusing backbone features without considering the negative influence of background clutter tends to introduce false-negative predictions, lagging the segmentation accuracy. To mitigate this problem, we propose an attention retrieval network (ARN) to perform soft spatial constraints on backbone features. We first build a look-up-table (LUT) with the ground-truth mask in the starting frame, and then retrieves the LUT to obtain an attention map for spatial constraints. Moreover, we introduce a multi-resolution multi-stage segmentation network (MMS) to further weaken the influence of background clutter by reusing the predicted mask to filter backbone features. Our approach set a new state-of-the-art on recent pixel-wise object tracking benchmark VOT2020 while running at 40 fps. Notably, the proposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016, and DAVIS2017, respectively. We will release our code at https://github.com/researchmm/TracKit.



### The VISIONE Video Search System: Exploiting Off-the-Shelf Text Search Engines for Large-Scale Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.02749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.02749v2)
- **Published**: 2020-08-06 16:32:17+00:00
- **Updated**: 2021-03-18 14:37:27+00:00
- **Authors**: Giuseppe Amato, Paolo Bolettieri, Fabio Carrara, Franca Debole, Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo, Claudio Vairo
- **Comment**: 22 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we describe in details VISIONE, a video search system that allows users to search for videos using textual keywords, occurrence of objects and their spatial relationships, occurrence of colors and their spatial relationships, and image similarity. These modalities can be combined together to express complex queries and satisfy user needs. The peculiarity of our approach is that we encode all the information extracted from the keyframes, such as visual deep features, tags, color and object locations, using a convenient textual encoding indexed in a single text retrieval engine. This offers great flexibility when results corresponding to various parts of the query (visual, text and locations) have to be merged. In addition, we report an extensive analysis of the system retrieval performance, using the query logs generated during the Video Browser Showdown (VBS) 2019 competition. This allowed us to fine-tune the system by choosing the optimal parameters and strategies among the ones that we tested.



### Unsupervised Learning for Identifying Events in Active Target Experiments
- **Arxiv ID**: http://arxiv.org/abs/2008.02757v3
- **DOI**: 10.1016/j.nima.2021.165461
- **Categories**: **cs.CV**, nucl-ex
- **Links**: [PDF](http://arxiv.org/pdf/2008.02757v3)
- **Published**: 2020-08-06 16:49:39+00:00
- **Updated**: 2021-03-13 17:48:41+00:00
- **Authors**: Robert Solli, Daniel Bazin, Michelle P. Kuchera, Ryan R. Strauss, Morten Hjorth-Jensen
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents novel applications of unsupervised machine learning methods to the problem of event separation in an active target detector, the Active-Target Time Projection Chamber (AT-TPC). The overarching goal is to group similar events in the early stages of the data analysis, thereby improving efficiency by limiting the computationally expensive processing of unnecessary events. The application of unsupervised clustering algorithms to the analysis of two-dimensional projections of particle tracks from a resonant proton scattering experiment on $^{46}$Ar is introduced. We explore the performance of autoencoder neural networks and a pre-trained VGG16 convolutional neural network. We study clustering performance on both data from a simulated $^{46}$Ar experiment, and real events from the AT-TPC detector. We find that a $k$-means algorithm applied to simulated data in the VGG16 latent space forms almost perfect clusters. Additionally, the VGG16+$k$-means approach finds high purity clusters of proton events for real experimental data. We also explore the application of clustering the latent space of autoencoder neural networks for event separation. While these networks show strong performance, they suffer from high variability in their results.



### IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/2008.02760v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.9; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2008.02760v2)
- **Published**: 2020-08-06 17:01:39+00:00
- **Updated**: 2020-11-18 23:19:31+00:00
- **Authors**: Sadegh Rabiee, Joydeep Biswas
- **Comment**: To be published in CoRL 2020 (Conference on Robot Learning)
- **Journal**: None
- **Summary**: Existing solutions to visual simultaneous localization and mapping (V-SLAM) assume that errors in feature extraction and matching are independent and identically distributed (i.i.d), but this assumption is known to not be true -- features extracted from low-contrast regions of images exhibit wider error distributions than features from sharp corners. Furthermore, V-SLAM algorithms are prone to catastrophic tracking failures when sensed images include challenging conditions such as specular reflections, lens flare, or shadows of dynamic objects. To address such failures, previous work has focused on building more robust visual frontends, to filter out challenging features. In this paper, we present introspective vision for SLAM (IV-SLAM), a fundamentally different approach for addressing these challenges. IV-SLAM explicitly models the noise process of reprojection errors from visual features to be context-dependent, and hence non-i.i.d. We introduce an autonomously supervised approach for IV-SLAM to collect training data to learn such a context-aware noise model. Using this learned noise model, IV-SLAM guides feature extraction to select more features from parts of the image that are likely to result in lower noise, and further incorporate the learned noise model into the joint maximum likelihood estimation, thus making it robust to the aforementioned types of errors. We present empirical results to demonstrate that IV-SLAM 1) is able to accurately predict sources of error in input images, 2) reduces tracking error compared to V-SLAM, and 3) increases the mean distance between tracking failures by more than 70% on challenging real robot data compared to V-SLAM.



### Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining Network
- **Arxiv ID**: http://arxiv.org/abs/2008.02763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02763v1)
- **Published**: 2020-08-06 17:04:34+00:00
- **Updated**: 2020-08-06 17:04:34+00:00
- **Authors**: Cong Wang, Yutong Wu, Zhixun Su, Junyang Chen
- **Comment**: Accepted to ACM International Conference on Multimedia (MM'20)
- **Journal**: None
- **Summary**: In the field of multimedia, single image deraining is a basic pre-processing work, which can greatly improve the visual effect of subsequent high-level tasks in rainy conditions. In this paper, we propose an effective algorithm, called JDNet, to solve the single image deraining problem and conduct the segmentation and detection task for applications. Specifically, considering the important information on multi-scale features, we propose a Scale-Aggregation module to learn the features with different scales. Simultaneously, Self-Attention module is introduced to match or outperform their convolutional counterparts, which allows the feature aggregation to adapt to each channel. Furthermore, to improve the basic convolutional feature transformation process of Convolutional Neural Networks (CNNs), Self-Calibrated convolution is applied to build long-range spatial and inter-channel dependencies around each spatial location that explicitly expand fields-of-view of each convolutional layer through internal communications and hence enriches the output features. By designing the Scale-Aggregation and Self-Attention modules with Self-Calibrated convolution skillfully, the proposed model has better deraining results both on real-world and synthetic datasets. Extensive experiments are conducted to demonstrate the superiority of our method compared with state-of-the-art methods. The source code will be available at \url{https://supercong94.wixsite.com/supercong94}.



### Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2008.02766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02766v2)
- **Published**: 2020-08-06 17:11:19+00:00
- **Updated**: 2021-07-15 02:39:22+00:00
- **Authors**: Nishanth Arun, Nathan Gaw, Praveer Singh, Ken Chang, Mehak Aggarwal, Bryan Chen, Katharina Hoebel, Sharut Gupta, Jay Patel, Mishka Gidwani, Julius Adebayo, Matthew D. Li, Jayashree Kalpathy-Cramer
- **Comment**: Submitted to Radiology AI journal
- **Journal**: None
- **Summary**: Saliency maps have become a widely used method to make deep learning models more interpretable by providing post-hoc explanations of classifiers through identification of the most pertinent areas of the input medical image. They are increasingly being used in medical imaging to provide clinically plausible explanations for the decisions the neural network makes. However, the utility and robustness of these visualization maps has not yet been rigorously examined in the context of medical imaging. We posit that trustworthiness in this context requires 1) localization utility, 2) sensitivity to model weight randomization, 3) repeatability, and 4) reproducibility. Using the localization information available in two large public radiology datasets, we quantify the performance of eight commonly used saliency map approaches for the above criteria using area under the precision-recall curves (AUPRC) and structural similarity index (SSIM), comparing their performance to various baseline measures. Using our framework to quantify the trustworthiness of saliency maps, we show that all eight saliency map techniques fail at least one of the criteria and are, in most cases, less trustworthy when compared to the baselines. We suggest that their usage in the high-risk domain of medical imaging warrants additional scrutiny and recommend that detection or segmentation models be used if localization is the desired output of the network. Additionally, to promote reproducibility of our findings, we provide the code we used for all tests performed in this work at this link: https://github.com/QTIM-Lab/Assessing-Saliency-Maps.



### On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2008.02777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02777v1)
- **Published**: 2020-08-06 17:20:56+00:00
- **Updated**: 2020-08-06 17:20:56+00:00
- **Authors**: Bernhard Liebl, Manuel Burghardt
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate how to train a high quality optical character recognition (OCR) model for difficult historical typefaces on degraded paper. Through extensive grid searches, we obtain a neural network architecture and a set of optimal data augmentation settings. We discuss the influence of factors such as binarization, input line height, network width, network depth, and other network training parameters such as dropout. Implementing these findings into a practical model, we are able to obtain a 0.44% character error rate (CER) model from only 10,000 lines of training data, outperforming currently available pretrained models that were trained on more than 20 times the amount of data. We show ablations for all components of our training pipeline, which relies on the open source framework Calamari.



### Efficient Non-Line-of-Sight Imaging from Transient Sinograms
- **Arxiv ID**: http://arxiv.org/abs/2008.02787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV, eess.SP, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2008.02787v1)
- **Published**: 2020-08-06 17:50:50+00:00
- **Updated**: 2020-08-06 17:50:50+00:00
- **Authors**: Mariko Isogawa, Dorian Chan, Ye Yuan, Kris Kitani, Matthew O'Toole
- **Comment**: ECCV 2020. Project page:
  https://marikoisogawa.github.io/project/c2nlos
- **Journal**: None
- **Summary**: Non-line-of-sight (NLOS) imaging techniques use light that diffusely reflects off of visible surfaces (e.g., walls) to see around corners. One approach involves using pulsed lasers and ultrafast sensors to measure the travel time of multiply scattered light. Unlike existing NLOS techniques that generally require densely raster scanning points across the entirety of a relay wall, we explore a more efficient form of NLOS scanning that reduces both acquisition times and computational requirements. We propose a circular and confocal non-line-of-sight (C2NLOS) scan that involves illuminating and imaging a common point, and scanning this point in a circular path along a wall. We observe that (1) these C2NLOS measurements consist of a superposition of sinusoids, which we refer to as a transient sinogram, (2) there exists computationally efficient reconstruction procedures that transform these sinusoidal measurements into 3D positions of hidden scatterers or NLOS images of hidden objects, and (3) despite operating on an order of magnitude fewer measurements than previous approaches, these C2NLOS scans provide sufficient information about the hidden scene to solve these different NLOS imaging tasks. We show results from both simulated and real C2NLOS scans.



### CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations
- **Arxiv ID**: http://arxiv.org/abs/2008.02792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02792v2)
- **Published**: 2020-08-06 17:58:48+00:00
- **Updated**: 2020-11-11 19:00:31+00:00
- **Authors**: Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar, Leonidas J. Guibas
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We propose CaSPR, a method to learn object-centric Canonical Spatiotemporal Point Cloud Representations of dynamically moving or evolving objects. Our goal is to enable information aggregation over time and the interrogation of object state at any spatiotemporal neighborhood in the past, observed or not. Different from previous work, CaSPR learns representations that support spacetime continuity, are robust to variable and irregularly spacetime-sampled point clouds, and generalize to unseen object instances. Our approach divides the problem into two subtasks. First, we explicitly encode time by mapping an input point cloud sequence to a spatiotemporally-canonicalized object space. We then leverage this canonicalization to learn a spatiotemporal latent representation using neural ordinary differential equations and a generative model of dynamically evolving shapes using continuous normalizing flows. We demonstrate the effectiveness of our method on several applications including shape reconstruction, camera pose estimation, continuous spatiotemporal sequence reconstruction, and correspondence estimation from irregularly or intermittently sampled observations.



### Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications
- **Arxiv ID**: http://arxiv.org/abs/2008.02793v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02793v2)
- **Published**: 2020-08-06 17:59:04+00:00
- **Updated**: 2020-11-30 18:18:55+00:00
- **Authors**: Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, Arun Mallya
- **Comment**: None
- **Journal**: None
- **Summary**: The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this paper, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.



### Learning to Factorize and Relight a City
- **Arxiv ID**: http://arxiv.org/abs/2008.02796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.02796v1)
- **Published**: 2020-08-06 17:59:54+00:00
- **Updated**: 2020-08-06 17:59:54+00:00
- **Authors**: Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, Noah Snavely
- **Comment**: ECCV 2020 (Spotlight). Supplemental Material attached
- **Journal**: None
- **Summary**: We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit factorize-a-city.github.io for animated results.



### Integration of the 3D Environment for UAV Onboard Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.02834v3
- **DOI**: 10.3390/app10217622
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02834v3)
- **Published**: 2020-08-06 18:37:29+00:00
- **Updated**: 2020-10-29 10:20:21+00:00
- **Authors**: Stéphane Vujasinović, Stefan Becker, Timo Breuer, Sebastian Bullinger, Norbert Scherer-Negenborn, Michael Arens
- **Comment**: Accepted in MDPI Journal of Applied Sciences
- **Journal**: Vujasinovi\'c, S.; Becker, S.; Breuer, T.; Bullinger, S.;
  Scherer-Negenborn, N.; Arens, M. Integration of the 3D Environment for UAV
  Onboard Visual Object Tracking. Appl. Sci. 2020, 10, 7622
- **Summary**: Single visual object tracking from an unmanned aerial vehicle (UAV) poses fundamental challenges such as object occlusion, small-scale objects, background clutter, and abrupt camera motion. To tackle these difficulties, we propose to integrate the 3D structure of the observed scene into a detection-by-tracking algorithm. We introduce a pipeline that combines a model-free visual object tracker, a sparse 3D reconstruction, and a state estimator. The 3D reconstruction of the scene is computed with an image-based Structure-from-Motion (SfM) component that enables us to leverage a state estimator in the corresponding 3D scene during tracking. By representing the position of the target in 3D space rather than in image space, we stabilize the tracking during ego-motion and improve the handling of occlusions, background clutter, and small-scale objects. We evaluated our approach on prototypical image sequences, captured from a UAV with low-altitude oblique views. For this purpose, we adapted an existing dataset for visual object tracking and reconstructed the observed scene in 3D. The experimental results demonstrate that the proposed approach outperforms methods using plain visual cues as well as approaches leveraging image-space-based state estimations. We believe that our approach can be beneficial for traffic monitoring, video surveillance, and navigation.



### Confidence-guided Lesion Mask-based Simultaneous Synthesis of Anatomic and Molecular MR Images in Patients with Post-treatment Malignant Gliomas
- **Arxiv ID**: http://arxiv.org/abs/2008.02859v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02859v1)
- **Published**: 2020-08-06 20:20:22+00:00
- **Updated**: 2020-08-06 20:20:22+00:00
- **Authors**: Pengfei Guo, Puyang Wang, Rajeev Yasarla, Jinyuan Zhou, Vishal M. Patel, Shanshan Jiang
- **Comment**: Submit to IEEE TMI. arXiv admin note: text overlap with
  arXiv:2006.14761
- **Journal**: None
- **Summary**: Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas in neuro-oncology, especially with the help of standard anatomic and advanced molecular MR images. However, data quantity and quality remain a key determinant of, and a significant limit on, the potential of such applications. In our previous work, we explored synthesis of anatomic and molecular MR image network (SAMR) in patients with post-treatment malignant glioms. Now, we extend it and propose Confidence Guided SAMR (CG-SAMR) that synthesizes data from lesion information to multi-modal anatomic sequences, including T1-weighted (T1w), gadolinium enhanced T1w (Gd-T1w), T2-weighted (T2w), and fluid-attenuated inversion recovery (FLAIR), and the molecular amide proton transfer-weighted (APTw) sequence. We introduce a module which guides the synthesis based on confidence measure about the intermediate results. Furthermore, we extend the proposed architecture for unsupervised synthesis so that unpaired data can be used for training the network. Extensive experiments on real clinical data demonstrate that the proposed model can perform better than the state-of-theart synthesis methods.



### Improving Explainability of Image Classification in Scenarios with Class Overlap: Application to COVID-19 and Pneumonia
- **Arxiv ID**: http://arxiv.org/abs/2008.02866v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02866v3)
- **Published**: 2020-08-06 20:47:36+00:00
- **Updated**: 2020-08-16 01:33:25+00:00
- **Authors**: Edward Verenich, Alvaro Velasquez, Nazar Khan, Faraz Hussain
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Trust in predictions made by machine learning models is increased if the model generalizes well on previously unseen samples and when inference is accompanied by cogent explanations of the reasoning behind predictions. In the image classification domain, generalization can be assessed through accuracy, sensitivity, and specificity. Explainability can be assessed by how well the model localizes the object of interest within an image. However, both generalization and explainability through localization are degraded in scenarios with significant overlap between classes. We propose a method based on binary expert networks that enhances the explainability of image classifications through better localization by mitigating the model uncertainty induced by class overlap. Our technique performs discriminative localization on images that contain features with significant class overlap, without explicitly training for localization. Our method is particularly promising in real-world class overlap scenarios, such as COVID-19 and pneumonia, where expertly labeled data for localization is not readily available. This can be useful for early, rapid, and trustworthy screening for COVID-19.



### Fatigue Assessment using ECG and Actigraphy Sensors
- **Arxiv ID**: http://arxiv.org/abs/2008.02871v2
- **DOI**: 10.1145/3410531.3414308
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.02871v2)
- **Published**: 2020-08-06 21:04:33+00:00
- **Updated**: 2020-08-18 15:15:56+00:00
- **Authors**: Yang Bai, Yu Guan, Wan-Fai Ng
- **Comment**: accepted by ISWC 2020
- **Journal**: None
- **Summary**: Fatigue is one of the key factors in the loss of work efficiency and health-related quality of life, and most fatigue assessment methods were based on self-reporting, which may suffer from many factors such as recall bias. To address this issue, we developed an automated system using wearable sensing and machine learning techniques for objective fatigue assessment. ECG/Actigraphy data were collected from subjects in free-living environments. Preprocessing and feature engineering methods were applied, before interpretable solution and deep learning solution were introduced. Specifically, for interpretable solution, we proposed a feature selection approach which can select less correlated and high informative features for better understanding system's decision-making process. For deep learning solution, we used state-of-the-art self-attention model, based on which we further proposed a consistency self-attention (CSA) mechanism for fatigue assessment. Extensive experiments were conducted, and very promising results were achieved.



### Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02880v1)
- **Published**: 2020-08-06 21:33:44+00:00
- **Updated**: 2020-08-06 21:33:44+00:00
- **Authors**: Yannick Le Cacheux, Adrian Popescu, Hervé Le Borgne
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) makes object recognition in images possible in absence of visual training data for a part of the classes from a dataset. When the number of classes is large, classes are usually represented by semantic class prototypes learned automatically from unannotated text collections. This typically leads to much lower performances than with manually designed semantic prototypes such as attributes. While most ZSL works focus on the visual aspect and reuse standard semantic prototypes learned from generic text collections, we focus on the problem of semantic class prototype design for large scale ZSL. More specifically, we investigate the use of noisy textual metadata associated to photos as text collections, as we hypothesize they are likely to provide more plausible semantic embeddings for visual classes if exploited appropriately. We thus make use of a source-based voting strategy to improve the robustness of semantic prototypes. Evaluation on the large scale ImageNet dataset shows a significant improvement in ZSL performances over two strong baselines, and over usual semantic embeddings used in previous works. We show that this improvement is obtained for several embedding methods, leading to state of the art results when one uses automatically created visual and text features.



### Parts-Based Articulated Object Localization in Clutter Using Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/2008.02881v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02881v1)
- **Published**: 2020-08-06 21:34:52+00:00
- **Updated**: 2020-08-06 21:34:52+00:00
- **Authors**: Jana Pavlasek, Stanley Lewis, Karthik Desingh, Odest Chadwicke Jenkins
- **Comment**: Accepted to the 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Contact: Jana Pavlasek, pavlasek@umich.edu
- **Journal**: None
- **Summary**: Robots working in human environments must be able to perceive and act on challenging objects with articulations, such as a pile of tools. Articulated objects increase the dimensionality of the pose estimation problem, and partial observations under clutter create additional challenges. To address this problem, we present a generative-discriminative parts-based recognition and localization method for articulated objects in clutter. We formulate the problem of articulated object pose estimation as a Markov Random Field (MRF). Hidden nodes in this MRF express the pose of the object parts, and edges express the articulation constraints between parts. Localization is performed within the MRF using an efficient belief propagation method. The method is informed by both part segmentation heatmaps over the observation, generated by a neural network, and the articulation constraints between object parts. Our generative-discriminative approach allows the proposed method to function in cluttered environments by inferring the pose of occluded parts using hypotheses from the visible parts. We demonstrate the efficacy of our methods in a tabletop environment for recognizing and localizing hand tools in uncluttered and cluttered configurations.



### Diagnosis of Autism in Children using Facial Analysis and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02890v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02890v1)
- **Published**: 2020-08-06 22:15:20+00:00
- **Updated**: 2020-08-06 22:15:20+00:00
- **Authors**: Madison Beary, Alex Hadsell, Ryan Messersmith, Mohammad-Parsa Hosseini
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a deep learning model to classify children as either healthy or potentially autistic with 94.6% accuracy using Deep Learning. Autistic patients struggle with social skills, repetitive behaviors, and communication, both verbal and nonverbal. Although the disease is considered to be genetic, the highest rates of accurate diagnosis occur when the child is tested on behavioral characteristics and facial features. Patients have a common pattern of distinct facial deformities, allowing researchers to analyze only an image of the child to determine if the child has the disease. While there are other techniques and models used for facial analysis and autism classification on their own, our proposal bridges these two ideas allowing classification in a cheaper, more efficient method. Our deep learning model uses MobileNet and two dense layers in order to perform feature extraction and image classification. The model is trained and tested using 3,014 images, evenly split between children with autism and children without it. 90% of the data is used for training, and 10% is used for testing. Based on our accuracy, we propose that the diagnosis of autism can be done effectively using only a picture. Additionally, there may be other diseases that are similarly diagnosable.



