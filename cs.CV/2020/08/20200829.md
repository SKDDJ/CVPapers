# Arxiv Papers in cs.CV on 2020-08-29
### Self-Organized Operational Neural Networks for Severe Image Restoration Problems
- **Arxiv ID**: http://arxiv.org/abs/2008.12894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12894v1)
- **Published**: 2020-08-29 02:19:41+00:00
- **Updated**: 2020-08-29 02:19:41+00:00
- **Authors**: Junaid Malik, Serkan Kiranyaz, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative learning based on convolutional neural networks (CNNs) aims to perform image restoration by learning from training examples of noisy-clean image pairs. It has become the go-to methodology for tackling image restoration and has outperformed the traditional non-local class of methods. However, the top-performing networks are generally composed of many convolutional layers and hundreds of neurons, with trainable parameters in excess of several millions. We claim that this is due to the inherent linear nature of convolution-based transformation, which is inadequate for handling severe restoration problems. Recently, a non-linear generalization of CNNs, called the operational neural networks (ONN), has been shown to outperform CNN on AWGN denoising. However, its formulation is burdened by a fixed collection of well-known nonlinear operators and an exhaustive search to find the best possible configuration for a given architecture, whose efficacy is further limited by a fixed output layer operator assignment. In this study, we leverage the Taylor series-based function approximation to propose a self-organizing variant of ONNs, Self-ONNs, for image restoration, which synthesizes novel nodal transformations onthe-fly as part of the learning process, thus eliminating the need for redundant training runs for operator search. In addition, it enables a finer level of operator heterogeneity by diversifying individual connections of the receptive fields and weights. We perform a series of extensive ablation experiments across three severe image restoration tasks. Even when a strict equivalence of learnable parameters is imposed, Self-ONNs surpass CNNs by a considerable margin across all problems, improving the generalization performance by up to 3 dB in terms of PSNR.



### On segmentation of pectoralis muscle in digital mammograms by means of deep learning
- **Arxiv ID**: http://arxiv.org/abs/2008.12904v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12904v1)
- **Published**: 2020-08-29 03:38:11+00:00
- **Updated**: 2020-08-29 03:38:11+00:00
- **Authors**: Hossein Soleimani, Oleg V. Michailovich
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Computer-aided diagnosis (CAD) has long become an integral part of radiological management of breast disease, facilitating a number of important clinical applications, including quantitative assessment of breast density and early detection of malignancies based on X-ray mammography. Common to such applications is the need to automatically discriminate between breast tissue and adjacent anatomy, with the latter being predominantly represented by pectoralis major (or pectoral muscle). Especially in the case of mammograms acquired in the mediolateral oblique (MLO) view, the muscle is easily confusable with some elements of breast anatomy due to their morphological and photometric similarity. As a result, the problem of automatic detection and segmentation of pectoral muscle in MLO mammograms remains a challenging task, innovative approaches to which are still required and constantly searched for. To address this problem, the present paper introduces a two-step segmentation strategy based on a combined use of data-driven prediction (deep learning) and graph-based image processing. In particular, the proposed method employs a convolutional neural network (CNN) which is designed to predict the location of breast-pectoral boundary at different levels of spatial resolution. Subsequently, the predictions are used by the second stage of the algorithm, in which the desired boundary is recovered as a solution to the shortest path problem on a specially designed graph. The proposed algorithm has been tested on three different datasets (i.e., MIAS, CBIS-DDSm and InBreast) using a range of quantitative metrics. The results of comparative analysis show considerable improvement over state-of-the-art, while offering the possibility of model-free and fully automatic processing.



### Multi-Attention Based Ultra Lightweight Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.12912v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12912v2)
- **Published**: 2020-08-29 05:19:32+00:00
- **Updated**: 2020-09-21 06:07:14+00:00
- **Authors**: Abdul Muqeet, Jiwon Hwang, Subin Yang, Jung Heum Kang, Yongwoo Kim, Sung-Ho Bae
- **Comment**: ECCVW AIM2020
- **Journal**: None
- **Summary**: Lightweight image super-resolution (SR) networks have the utmost significance for real-world applications. There are several deep learning based SR methods with remarkable performance, but their memory and computational cost are hindrances in practical usage. To tackle this problem, we propose a Multi-Attentive Feature Fusion Super-Resolution Network (MAFFSRN). MAFFSRN consists of proposed feature fusion groups (FFGs) that serve as a feature extraction block. Each FFG contains a stack of proposed multi-attention blocks (MAB) that are combined in a novel feature fusion structure. Further, the MAB with a cost-efficient attention mechanism (CEA) helps us to refine and extract the features using multiple attention mechanisms. The comprehensive experiments show the superiority of our model over the existing state-of-the-art. We participated in AIM 2020 efficient SR challenge with our MAFFSRN model and won 1st, 3rd, and 4th places in memory usage, floating-point operations (FLOPs) and number of parameters, respectively.



### VR-Caps: A Virtual Environment for Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2008.12949v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12949v2)
- **Published**: 2020-08-29 09:54:05+00:00
- **Updated**: 2021-01-14 12:55:11+00:00
- **Authors**: Kagan Incetan, Ibrahim Omer Celik, Abdulhamid Obeid, Guliz Irem Gokceler, Kutsev Bengisu Ozyoruk, Yasin Almalioglu, Richard J. Chen, Faisal Mahmood, Hunter Gilbert, Nicholas J. Durr, Mehmet Turan
- **Comment**: 18 pages, 14 figures
- **Journal**: None
- **Summary**: Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation, automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes, but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of normal and abnormal tissue conditions (e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope designs (e.g., mono, stereo, dual and 360{\deg}camera), and the type, number, strength, and placement of internal and external magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop, optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems. To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results demonstrate the usefulness and effectiveness of the proposed virtual platform in developing algorithms that quantify fractional coverage, camera trajectory, 3D map reconstruction, and disease classification.



### Path Planning Followed by Kinodynamic Smoothing for Multirotor Aerial Vehicles (MAVs)
- **Arxiv ID**: http://arxiv.org/abs/2008.12950v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2008.12950v1)
- **Published**: 2020-08-29 09:55:49+00:00
- **Updated**: 2020-08-29 09:55:49+00:00
- **Authors**: Geesara Kulathunga, Dmitry Devitt, Roman Fedorenko, Sergei Savin, Alexandr Klimchik
- **Comment**: None
- **Journal**: None
- **Summary**: We explore path planning followed by kinodynamic smoothing while ensuring the vehicle dynamics feasibility for MAVs. We have chosen a geometrically based motion planning technique \textquotedblleft RRT*\textquotedblright\; for this purpose. In the proposed technique, we modified original RRT* introducing an adaptive search space and a steering function which help to increase the consistency of the planner. Moreover, we propose multiple RRT* which generates a set of desired paths, provided that the optimal path is selected among them. Then, apply kinodynamic smoothing, which will result in dynamically feasible as well as obstacle-free path. Thereafter, a b spline-based trajectory is generated to maneuver vehicle autonomously in unknown environments. Finally, we have tested the proposed technique in various simulated environments.



### Adaptive Local Structure Consistency based Heterogeneous Remote Sensing Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.12958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12958v2)
- **Published**: 2020-08-29 10:33:43+00:00
- **Updated**: 2020-11-11 02:59:30+00:00
- **Authors**: Lin Lei, Yuli Sun, Gangyao Kuang
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Change detection of heterogeneous remote sensing images is an important and challenging topic in remote sensing for emergency situation resulting from nature disaster. Due to the different imaging mechanisms of heterogeneous sensors, it is difficult to directly compare the images. To address this challenge, we explore an unsupervised change detection method based on adaptive local structure consistency (ALSC) between heterogeneous images in this letter, which constructs an adaptive graph representing the local structure for each patch in one image domain and then projects this graph to the other image domain to measure the change level. This local structure consistency exploits the fact that the heterogeneous images share the same structure information for the same ground object, which is imaging modality-invariant. To avoid the leakage of heterogeneous data, the pixelwise change image is calculated in the same image domain by graph projection. Experiment results demonstrate the effectiveness of the proposed ALSC based change detection method by comparing with some state-of-the-art methods.



### Puzzle-AE: Novelty Detection in Images through Solving Puzzles
- **Arxiv ID**: http://arxiv.org/abs/2008.12959v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12959v5)
- **Published**: 2020-08-29 10:53:55+00:00
- **Updated**: 2022-02-10 21:06:05+00:00
- **Authors**: Mohammadreza Salehi, Ainaz Eftekhar, Niousha Sadjadi, Mohammad Hossein Rohban, Hamid R. Rabiee
- **Comment**: The paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Autoencoder, as an essential part of many anomaly detection methods, is lacking flexibility on normal data in complex datasets. U-Net is proved to be effective for this purpose but overfits on the training data if trained by just using reconstruction error similar to other AE-based frameworks. Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods, has earlier proved its ability in learning semantically meaningful features. We show that training U-Nets based on this task is an effective remedy that prevents overfitting and facilitates learning beyond pixel-level features. Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw puzzles. We propose adversarial robust training as an effective automatic shortcut removal. We achieve competitive or superior results compared to the State of the Art (SOTA) anomaly detection methods on various toy and real-world datasets. Unlike many competitors, the proposed framework is stable, fast, data-efficient, and does not require unprincipled early stopping.



### Zero-Shot Learning from Adversarial Feature Residual to Compact Visual Feature
- **Arxiv ID**: http://arxiv.org/abs/2008.12962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12962v1)
- **Published**: 2020-08-29 11:16:11+00:00
- **Updated**: 2020-08-29 11:16:11+00:00
- **Authors**: Bo Liu, Qiulei Dong, Zhanyi Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many zero-shot learning (ZSL) methods focused on learning discriminative object features in an embedding feature space, however, the distributions of the unseen-class features learned by these methods are prone to be partly overlapped, resulting in inaccurate object recognition. Addressing this problem, we propose a novel adversarial network to synthesize compact semantic visual features for ZSL, consisting of a residual generator, a prototype predictor, and a discriminator. The residual generator is to generate the visual feature residual, which is integrated with a visual prototype predicted via the prototype predictor for synthesizing the visual feature. The discriminator is to distinguish the synthetic visual features from the real ones extracted from an existing categorization CNN. Since the generated residuals are generally numerically much smaller than the distances among all the prototypes, the distributions of the unseen-class features synthesized by the proposed network are less overlapped. In addition, considering that the visual features from categorization CNNs are generally inconsistent with their semantic features, a simple feature selection strategy is introduced for extracting more compact semantic visual features. Extensive experimental results on six benchmark datasets demonstrate that our method could achieve a significantly better performance than existing state-of-the-art methods by 1.2-13.2% in most cases.



### Patch-based Brain Age Estimation from MR Images
- **Arxiv ID**: http://arxiv.org/abs/2008.12965v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12965v2)
- **Published**: 2020-08-29 11:50:37+00:00
- **Updated**: 2020-10-01 13:26:02+00:00
- **Authors**: Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Arinbjörn Kolbeinsson, Alexander Hammers, Daniel Rueckert
- **Comment**: Accepted (oral) at the MLCN workshop, MICCAI 2020
- **Journal**: None
- **Summary**: Brain age estimation from Magnetic Resonance Images (MRI) derives the difference between a subject's biological brain age and their chronological age. This is a potential biomarker for neurodegeneration, e.g. as part of Alzheimer's disease. Early detection of neurodegeneration manifesting as a higher brain age can potentially facilitate better medical care and planning for affected individuals. Many studies have been proposed for the prediction of chronological age from brain MRI using machine learning and specifically deep learning techniques. Contrary to most studies, which use the whole brain volume, in this study, we develop a new deep learning approach that uses 3D patches of the brain as well as convolutional neural networks (CNNs) to develop a localised brain age estimator. In this way, we can obtain a visualization of the regions that play the most important role for estimating brain age, leading to more anatomically driven and interpretable results, and thus confirming relevant literature which suggests that the ventricles and the hippocampus are the areas that are most informative. In addition, we leverage this knowledge in order to improve the overall performance on the task of age estimation by combining the results of different patches using an ensemble method, such as averaging or linear regression. The network is trained on the UK Biobank dataset and the method achieves state-of-the-art results with a Mean Absolute Error of 2.46 years for purely regional estimates, and 2.13 years for an ensemble of patches before bias correction, while 1.96 years after bias correction.



### Unpaired Deep Learning for Accelerated MRI using Optimal Transport Driven CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2008.12967v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.12967v1)
- **Published**: 2020-08-29 12:02:49+00:00
- **Updated**: 2020-08-29 12:02:49+00:00
- **Authors**: Gyutaek Oh, Byeongsu Sim, Hyungjin Chung, Leonard Sunwoo, Jong Chul Ye
- **Comment**: Accepted for IEEE Transactions on Computational Imaging
- **Journal**: None
- **Summary**: Recently, deep learning approaches for accelerated MRI have been extensively studied thanks to their high performance reconstruction in spite of significantly reduced runtime complexity. These neural networks are usually trained in a supervised manner, so matched pairs of subsampled and fully sampled k-space data are required. Unfortunately, it is often difficult to acquire matched fully sampled k-space data, since the acquisition of fully sampled k-space data requires long scan time and often leads to the change of the acquisition protocol. Therefore, unpaired deep learning without matched label data has become a very important research topic. In this paper, we propose an unpaired deep learning approach using a optimal transport driven cycle-consistent generative adversarial network (OT-cycleGAN) that employs a single pair of generator and discriminator. The proposed OT-cycleGAN architecture is rigorously derived from a dual formulation of the optimal transport formulation using a specially designed penalized least squares cost. The experimental results show that our method can reconstruct high resolution MR images from accelerated k- space data from both single and multiple coil acquisition, without requiring matched reference data.



### Driving Through Ghosts: Behavioral Cloning with False Positives
- **Arxiv ID**: http://arxiv.org/abs/2008.12969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.12969v1)
- **Published**: 2020-08-29 12:10:23+00:00
- **Updated**: 2020-08-29 12:10:23+00:00
- **Authors**: Andreas Bühler, Adrien Gaidon, Andrei Cramariuc, Rares Ambrus, Guy Rosman, Wolfram Burgard
- **Comment**: 7 pages, 5 figures, 4 tables, accepted at 2020 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2020)
- **Journal**: None
- **Summary**: Safe autonomous driving requires robust detection of other traffic participants. However, robust does not mean perfect, and safe systems typically minimize missed detections at the expense of a higher false positive rate. This results in conservative and yet potentially dangerous behavior such as avoiding imaginary obstacles. In the context of behavioral cloning, perceptual errors at training time can lead to learning difficulties or wrong policies, as expert demonstrations might be inconsistent with the perceived world state. In this work, we propose a behavioral cloning approach that can safely leverage imperfect perception without being conservative. Our core contribution is a novel representation of perceptual uncertainty for learning to plan. We propose a new probabilistic birds-eye-view semantic grid to encode the noisy output of object perception systems. We then leverage expert demonstrations to learn an imitative driving policy using this probabilistic representation. Using the CARLA simulator, we show that our approach can safely overcome critical false positives that would otherwise lead to catastrophic failures or conservative behavior.



### Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise
- **Arxiv ID**: http://arxiv.org/abs/2008.12977v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12977v2)
- **Published**: 2020-08-29 13:50:49+00:00
- **Updated**: 2020-11-04 17:09:53+00:00
- **Authors**: Anne-Sophie Collin, Christophe De Vleeschouwer
- **Comment**: 8 pages, 7 figures, accepted at ICPR2020 -- Supplementary explanation
  about the stain noise model in section III (point B)
- **Journal**: None
- **Summary**: In industrial vision, the anomaly detection problem can be addressed with an autoencoder trained to map an arbitrary image, i.e. with or without any defect, to a clean image, i.e. without any defect. In this approach, anomaly detection relies conventionally on the reconstruction residual or, alternatively, on the reconstruction uncertainty. To improve the sharpness of the reconstruction, we consider an autoencoder architecture with skip connections. In the common scenario where only clean images are available for training, we propose to corrupt them with a synthetic noise model to prevent the convergence of the network towards the identity mapping, and introduce an original Stain noise model for that purpose. We show that this model favors the reconstruction of clean images from arbitrary real-world images, regardless of the actual defects appearance. In addition to demonstrating the relevance of our approach, our validation provides the first consistent assessment of reconstruction-based methods, by comparing their performance over the MVTec AD dataset, both for pixel- and image-wise anomaly detection.



### AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.12995v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12995v3)
- **Published**: 2020-08-29 15:22:00+00:00
- **Updated**: 2020-09-20 22:48:57+00:00
- **Authors**: Akash Roy
- **Comment**: language ambiguity cleared, typos corrected
- **Journal**: None
- **Summary**: I propose a state of the art deep neural architectural solution for handwritten character recognition for Bengali alphabets, compound characters as well as numerical digits that achieves state-of-the-art accuracy 96.8% in just 11 epochs. Similar work has been done before by Chatterjee, Swagato, et al. but they achieved 96.12% accuracy in about 47 epochs. The deep neural architecture used in that paper was fairly large considering the inclusion of the weights of the ResNet 50 model which is a 50 layer Residual Network. This proposed model achieves higher accuracy as compared to any previous work & in a little number of epochs. ResNet50 is a good model trained on the ImageNet dataset, but I propose an HCR network that is trained from the scratch on Bengali characters without the "Ensemble Learning" that can outperform previous architectures.



### Longitudinal Image Registration with Temporal-order and Subject-specificity Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2008.13002v1
- **DOI**: 10.1007/978-3-030-59716-0_24
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.13002v1)
- **Published**: 2020-08-29 16:06:58+00:00
- **Updated**: 2020-08-29 16:06:58+00:00
- **Authors**: Qianye Yang, Yunguan Fu, Francesco Giganti, Nooshin Ghavami, Qingchao Chen, J. Alison Noble, Tom Vercauteren, Dean Barratt, Yipeng Hu
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Morphological analysis of longitudinal MR images plays a key role in monitoring disease progression for prostate cancer patients, who are placed under an active surveillance program. In this paper, we describe a learning-based image registration algorithm to quantify changes on regions of interest between a pair of images from the same patient, acquired at two different time points. Combining intensity-based similarity and gland segmentation as weak supervision, the population-data-trained registration networks significantly lowered the target registration errors (TREs) on holdout patient data, compared with those before registration and those from an iterative registration algorithm. Furthermore, this work provides a quantitative analysis on several longitudinal-data-sampling strategies and, in turn, we propose a novel regularisation method based on maximum mean discrepancy, between differently-sampled training image pairs. Based on 216 3D MR images from 86 patients, we report a mean TRE of 5.6 mm and show statistically significant differences between the different training data sampling strategies.



### Lymph Node Gross Tumor Volume Detection in Oncology Imaging via Relationship Learning Using Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.13013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.13013v1)
- **Published**: 2020-08-29 16:59:23+00:00
- **Updated**: 2020-08-29 16:59:23+00:00
- **Authors**: Chun-Hung Chao, Zhuotun Zhu, Dazhou Guo, Ke Yan, Tsung-Ying Ho, Jinzheng Cai, Adam P. Harrison, Xianghua Ye, Jing Xiao, Alan Yuille, Min Sun, Le Lu, Dakai Jin
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Determining the spread of GTV$_{LN}$ is essential in defining the respective resection or irradiating regions for the downstream workflows of surgical resection and radiotherapy for many cancers. Different from the more common enlarged lymph node (LN), GTV$_{LN}$ also includes smaller ones if associated with high positron emission tomography signals and/or any metastasis signs in CT. This is a daunting task. In this work, we propose a unified LN appearance and inter-LN relationship learning framework to detect the true GTV$_{LN}$. This is motivated by the prior clinical knowledge that LNs form a connected lymphatic system, and the spread of cancer cells among LNs often follows certain pathways. Specifically, we first utilize a 3D convolutional neural network with ROI-pooling to extract the GTV$_{LN}$'s instance-wise appearance features. Next, we introduce a graph neural network to further model the inter-LN relationships where the global LN-tumor spatial priors are included in the learning process. This leads to an end-to-end trainable network to detect by classifying GTV$_{LN}$. We operate our model on a set of GTV$_{LN}$ candidates generated by a preliminary 1st-stage method, which has a sensitivity of $>85\%$ at the cost of high false positive (FP) ($>15$ FPs per patient). We validate our approach on a radiotherapy dataset with 142 paired PET/RTCT scans containing the chest and upper abdominal body parts. The proposed method significantly improves over the state-of-the-art (SOTA) LN classification method by $5.5\%$ and $13.1\%$ in F1 score and the averaged sensitivity value at $2, 3, 4, 6$ FPs per patient, respectively.



### Adaptive Exploitation of Pre-trained Deep Convolutional Neural Networks for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.13015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.13015v2)
- **Published**: 2020-08-29 17:09:43+00:00
- **Updated**: 2020-12-22 06:57:23+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei
- **Comment**: Accepted Manuscript in Multimedia Tools and Applications (MTAP),
  Springer
- **Journal**: None
- **Summary**: Due to the automatic feature extraction procedure via multi-layer nonlinear transformations, the deep learning-based visual trackers have recently achieved great success in challenging scenarios for visual tracking purposes. Although many of those trackers utilize the feature maps from pre-trained convolutional neural networks (CNNs), the effects of selecting different models and exploiting various combinations of their feature maps are still not compared completely. To the best of our knowledge, all those methods use a fixed number of convolutional feature maps without considering the scene attributes (e.g., occlusion, deformation, and fast motion) that might occur during tracking. As a pre-requisition, this paper proposes adaptive discriminative correlation filters (DCF) based on the methods that can exploit CNN models with different topologies. First, the paper provides a comprehensive analysis of four commonly used CNN models to determine the best feature maps of each model. Second, with the aid of analysis results as attribute dictionaries, adaptive exploitation of deep features is proposed to improve the accuracy and robustness of visual trackers regarding video characteristics. Third, the generalization of the proposed method is validated on various tracking datasets as well as CNN models with similar architectures. Finally, extensive experimental results demonstrate the effectiveness of the proposed adaptive method compared with state-of-the-art visual tracking methods.



### Dual Attention GANs for Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.13024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.13024v1)
- **Published**: 2020-08-29 17:49:01+00:00
- **Updated**: 2020-08-29 17:49:01+00:00
- **Authors**: Hao Tang, Song Bai, Nicu Sebe
- **Comment**: Accepted to ACM MM 2020, camera ready (9 pages) + supplementary (10
  pages)
- **Journal**: None
- **Summary**: In this paper, we focus on the semantic image synthesis task that aims at transferring semantic label maps to photo-realistic images. Existing methods lack effective semantic constraints to preserve the semantic information and ignore the structural correlations in both spatial and channel dimensions, leading to unsatisfactory blurry and artifact-prone results. To address these limitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize photo-realistic and semantically-consistent images with fine details from the input layouts without imposing extra training overhead or modifying the network architectures of existing methods. We also propose two novel modules, i.e., position-wise Spatial Attention Module (SAM) and scale-wise Channel Attention Module (CAM), to capture semantic structure attention in spatial and channel dimensions, respectively. Specifically, SAM selectively correlates the pixels at each position by a spatial attention map, leading to pixels with the same semantic label being related to each other regardless of their spatial distances. Meanwhile, CAM selectively emphasizes the scale-wise features at each channel by a channel attention map, which integrates associated features among all channel maps regardless of their scales. We finally sum the outputs of SAM and CAM to further improve feature representation. Extensive experiments on four challenging datasets show that DAGAN achieves remarkably better results than state-of-the-art methods, while using fewer model parameters. The source code and trained models are available at https://github.com/Ha0Tang/DAGAN.



### Introduction to Medical Image Registration with DeepReg, Between Old and New
- **Arxiv ID**: http://arxiv.org/abs/2009.01924v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MS
- **Links**: [PDF](http://arxiv.org/pdf/2009.01924v2)
- **Published**: 2020-08-29 19:44:23+00:00
- **Updated**: 2020-09-07 06:45:40+00:00
- **Authors**: N. Montana Brown, Y. Fu, S. U. Saeed, A. Casamitjana, Z. M. C. Baum, R. Delaunay, Q. Yang, A. Grimwood, Z. Min, E. Bonmati, T. Vercauteren, M. J. Clarkson, Y. Hu
- **Comment**: Submitted to MICCAI Educational Challenge 2020
- **Journal**: None
- **Summary**: This document outlines a tutorial to get started with medical image registration using the open-source package DeepReg. The basic concepts of medical image registration are discussed, linking classical methods to newer methods using deep learning. Two iterative, classical algorithms using optimisation and one learning-based algorithm using deep learning are coded step-by-step using DeepReg utilities, all with real, open-accessible, medical data.



### An automatic framework for fusing information from differently stained consecutive digital whole slide images: A case study in renal histology
- **Arxiv ID**: http://arxiv.org/abs/2008.13050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.13050v2)
- **Published**: 2020-08-29 20:30:46+00:00
- **Updated**: 2021-05-02 19:12:07+00:00
- **Authors**: Odyssee Merveille, Thomas Lampert, Jessica Schmitz, Germain Forestier, Friedrich Feuerhake, Cédric Wemmert
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: This article presents an automatic image processing framework to extract quantitative high-level information describing the micro-environment of glomeruli in consecutive whole slide images (WSIs) processed with different staining modalities of patients with chronic kidney rejection after kidney transplantation. Methods: This four-step framework consists of: 1) approximate rigid registration, 2) cell and anatomical structure segmentation 3) fusion of information from different stainings using a newly developed registration algorithm 4) feature extraction. Results: Each step of the framework is validated independently both quantitatively and qualitatively by pathologists. An illustration of the different types of features that can be extracted is presented. Conclusion: The proposed generic framework allows for the analysis of the micro-environment surrounding large structures that can be segmented (either manually or automatically). It is independent of the segmentation approach and is therefore applicable to a variety of biomedical research questions. Significance: Chronic tissue remodelling processes after kidney transplantation can result in interstitial fibrosis and tubular atrophy (IFTA) and glomerulosclerosis. This pipeline provides tools to quantitatively analyse, in the same spatial context, information from different consecutive WSIs and help researchers understand the complex underlying mechanisms leading to IFTA and glomerulosclerosis.



