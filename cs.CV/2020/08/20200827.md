# Arxiv Papers in cs.CV on 2020-08-27
### Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra
- **Arxiv ID**: http://arxiv.org/abs/2008.11865v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.11865v1)
- **Published**: 2020-08-27 00:08:49+00:00
- **Updated**: 2020-08-27 00:08:49+00:00
- **Authors**: Vardan Papyan
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, "spikes", and small but distinct continuous distributions, "bumps", often seen beyond the edge of a "main bulk".   The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets.



### Lymph Node Gross Tumor Volume Detection and Segmentation via Distance-based Gating using 3D CT/PET Imaging in Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2008.11870v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11870v1)
- **Published**: 2020-08-27 00:37:50+00:00
- **Updated**: 2020-08-27 00:37:50+00:00
- **Authors**: Zhuotun Zhu, Dakai Jin, Ke Yan, Tsung-Ying Ho, Xianghua Ye, Dazhou Guo, Chun-Hung Chao, Jing Xiao, Alan Yuille, Le Lu
- **Comment**: MICCAI2020
- **Journal**: None
- **Summary**: Finding, identifying and segmenting suspicious cancer metastasized lymph nodes from 3D multi-modality imaging is a clinical task of paramount importance. In radiotherapy, they are referred to as Lymph Node Gross Tumor Volume (GTVLN). Determining and delineating the spread of GTVLN is essential in defining the corresponding resection and irradiating regions for the downstream workflows of surgical resection and radiotherapy of various cancers. In this work, we propose an effective distance-based gating approach to simulate and simplify the high-level reasoning protocols conducted by radiation oncologists, in a divide-and-conquer manner. GTVLN is divided into two subgroups of tumor-proximal and tumor-distal, respectively, by means of binary or soft distance gating. This is motivated by the observation that each category can have distinct though overlapping distributions of appearance, size and other LN characteristics. A novel multi-branch detection-by-segmentation network is trained with each branch specializing on learning one GTVLN category features, and outputs from multi-branch are fused in inference. The proposed method is evaluated on an in-house dataset of $141$ esophageal cancer patients with both PET and CT imaging modalities. Our results validate significant improvements on the mean recall from $72.5\%$ to $78.2\%$, as compared to previous state-of-the-art work. The highest achieved GTVLN recall of $82.5\%$ at $20\%$ precision is clinically relevant and valuable since human observers tend to have low sensitivity (around $80\%$ for the most experienced radiation oncologists, as reported by literature).



### Towards Practical 2D Grapevine Bud Detection with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.11872v2
- **DOI**: 10.1016/j.compag.2020.105947
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11872v2)
- **Published**: 2020-08-27 00:46:03+00:00
- **Updated**: 2021-02-05 03:26:16+00:00
- **Authors**: Wenceslao Villegas Marset, Diego Sebastián Pérez, Carlos Ariel Díaz, Facundo Bromberg
- **Comment**: Revised version submitted to Journal of Computers and Electronics in
  Agriculture November 13, 2020
- **Journal**: Computers and Electronics in Agriculture, Volume 182, 2021,105947,
  ISSN 0168-1699
- **Summary**: In Viticulture, visual inspection of the plant is a necessary task for measuring relevant variables. In many cases, these visual inspections are susceptible to automation through computer vision methods. Bud detection is one such visual task, central for the measurement of important variables such as: measurement of bud sunlight exposure, autonomous pruning, bud counting, type-of-bud classification, bud geometric characterization, internode length, bud area, and bud development stage, among others. This paper presents a computer method for grapevine bud detection based on a Fully Convolutional Networks MobileNet architecture (FCN-MN). To validate its performance, this architecture was compared in the detection task with a strong method for bud detection, Scanning Windows (SW) based on a patch classifier, showing improvements over three aspects of detection: segmentation, correspondence identification and localization. The best version of FCN-MN showed a detection F1-measure of $88.6\%$ (for true positives defined as detected components whose intersection-over-union with the true bud is above $0.5$), and false positives that are small and near the true bud. Splits -- false positives overlapping the true bud -- showed a mean segmentation precision of $89.3\% (21.7)$, while false alarms -- false positives not overlapping the true bud -- showed a mean pixel area of only $8\%$ the area of a true bud, and a distance (between mass centers) of $1.1$ true bud diameters. The paper concludes by discussing how these results for FCN-MN would produce sufficiently accurate measurements of bud variables such as bud number, bud area, and internode length, suggesting a good performance in a practical setup.



### Adaptively-Accumulated Knowledge Transfer for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.11873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11873v1)
- **Published**: 2020-08-27 00:53:43+00:00
- **Updated**: 2020-08-27 00:53:43+00:00
- **Authors**: Taotao Jing, Haifeng Xia, Zhengming Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Partial domain adaptation (PDA) attracts appealing attention as it deals with a realistic and challenging problem when the source domain label space substitutes the target domain. Most conventional domain adaptation (DA) efforts concentrate on learning domain-invariant features to mitigate the distribution disparity across domains. However, it is crucial to alleviate the negative influence caused by the irrelevant source domain categories explicitly for PDA. In this work, we propose an Adaptively-Accumulated Knowledge Transfer framework (A$^2$KT) to align the relevant categories across two domains for effective domain adaptation. Specifically, an adaptively-accumulated mechanism is explored to gradually filter out the most confident target samples and their corresponding source categories, promoting positive transfer with more knowledge across two domains. Moreover, a dual distinct classifier architecture consisting of a prototype classifier and a multilayer perceptron classifier is built to capture intrinsic data distribution knowledge across domains from various perspectives. By maximizing the inter-class center-wise discrepancy and minimizing the intra-class sample-wise compactness, the proposed model is able to obtain more domain-invariant and task-specific discriminative representations of the shared categories data. Comprehensive experiments on several partial domain adaptation benchmarks demonstrate the effectiveness of our proposed model, compared with the state-of-the-art PDA methods.



### Adversarial Dual Distinct Classifiers for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.11878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11878v1)
- **Published**: 2020-08-27 01:29:10+00:00
- **Updated**: 2020-08-27 01:29:10+00:00
- **Authors**: Taotao Jing, Zhengming Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain adaptation (UDA) attempts to recognize the unlabeled target samples by building a learning model from a differently-distributed labeled source domain. Conventional UDA concentrates on extracting domain-invariant features through deep adversarial networks. However, most of them seek to match the different domain feature distributions, without considering the task-specific decision boundaries across various classes. In this paper, we propose a novel Adversarial Dual Distinct Classifiers Network (AD$^2$CN) to align the source and target domain data distribution simultaneously with matching task-specific category boundaries. To be specific, a domain-invariant feature generator is exploited to embed the source and target data into a latent common space with the guidance of discriminative cross-domain alignment. Moreover, we naturally design two different structure classifiers to identify the unlabeled target samples over the supervision of the labeled source domain data. Such dual distinct classifiers with various architectures can capture diverse knowledge of the target data structure from different perspectives. Extensive experimental results on several cross-domain visual benchmarks prove the model's effectiveness by comparing it with other state-of-the-art UDA.



### Crossing-Domain Generative Adversarial Networks for Unsupervised Multi-Domain Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2008.11882v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11882v1)
- **Published**: 2020-08-27 01:54:07+00:00
- **Updated**: 2020-08-27 01:54:07+00:00
- **Authors**: Xuewen Yang, Dongliang Xie, Xin Wang
- **Comment**: accepted in proceedings of ACM Multimedia 2018
- **Journal**: None
- **Summary**: State-of-the-art techniques in Generative Adversarial Networks (GANs) have shown remarkable success in image-to-image translation from peer domain X to domain Y using paired image data. However, obtaining abundant paired data is a non-trivial and expensive process in the majority of applications. When there is a need to translate images across n domains, if the training is performed between every two domains, the complexity of the training will increase quadratically. Moreover, training with data from two domains only at a time cannot benefit from data of other domains, which prevents the extraction of more useful features and hinders the progress of this research area. In this work, we propose a general framework for unsupervised image-to-image translation across multiple domains, which can translate images from domain X to any a domain without requiring direct training between the two domains involved in image translation. A byproduct of the framework is the reduction of computing time and computing resources, since it needs less time than training the domains in pairs as is done in state-of-the-art works. Our proposed framework consists of a pair of encoders along with a pair of GANs which learns high-level features across different domains to generate diverse and realistic samples from. Our framework shows competing results on many image-to-image tasks compared with state-of-the-art techniques.



### A Self-Reasoning Framework for Anomaly Detection Using Video-Level Labels
- **Arxiv ID**: http://arxiv.org/abs/2008.11887v1
- **DOI**: 10.1109/LSP.2020.3025688
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11887v1)
- **Published**: 2020-08-27 02:14:15+00:00
- **Updated**: 2020-08-27 02:14:15+00:00
- **Authors**: Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, Seung-Ik Lee
- **Comment**: Accepted to the IEEE Signal Processing Letters Journal
- **Journal**: None
- **Summary**: Anomalous event detection in surveillance videos is a challenging and practical research problem among image and video processing community. Compared to the frame-level annotations of anomalous events, obtaining video-level annotations is quite fast and cheap though such high-level labels may contain significant noise. More specifically, an anomalous labeled video may actually contain anomaly only in a short duration while the rest of the video frames may be normal. In the current work, we propose a weakly supervised anomaly detection framework based on deep neural networks which is trained in a self-reasoning fashion using only video-level labels. To carry out the self-reasoning based training, we generate pseudo labels by using binary clustering of spatio-temporal video features which helps in mitigating the noise present in the labels of anomalous videos. Our proposed formulation encourages both the main network and the clustering to complement each other in achieving the goal of more accurate anomaly detection. The proposed framework has been evaluated on publicly available real-world anomaly detection datasets including UCF-crime, ShanghaiTech and UCSD Ped2. The experiments demonstrate superiority of our proposed framework over the current state-of-the-art methods.



### Webly Supervised Image Classification with Self-Contained Confidence
- **Arxiv ID**: http://arxiv.org/abs/2008.11894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11894v1)
- **Published**: 2020-08-27 02:49:51+00:00
- **Updated**: 2020-08-27 02:49:51+00:00
- **Authors**: Jingkang Yang, Litong Feng, Weirong Chen, Xiaopeng Yan, Huabin Zheng, Ping Luo, Wayne Zhang
- **Comment**: 16 pages, 4 figures, Accepted to ECCV 2020
- **Journal**: None
- **Summary**: This paper focuses on webly supervised learning (WSL), where datasets are built by crawling samples from the Internet and directly using search queries as web labels. Although WSL benefits from fast and low-cost data collection, noises in web labels hinder better performance of the image classification model. To alleviate this problem, in recent works, self-label supervised loss $\mathcal{L}_s$ is utilized together with webly supervised loss $\mathcal{L}_w$. $\mathcal{L}_s$ relies on pseudo labels predicted by the model itself. Since the correctness of the web label or pseudo label is usually on a case-by-case basis for each web sample, it is desirable to adjust the balance between $\mathcal{L}_s$ and $\mathcal{L}_w$ on sample level. Inspired by the ability of Deep Neural Networks (DNNs) in confidence prediction, we introduce Self-Contained Confidence (SCC) by adapting model uncertainty for WSL setting, and use it to sample-wisely balance $\mathcal{L}_s$ and $\mathcal{L}_w$. Therefore, a simple yet effective WSL framework is proposed. A series of SCC-friendly regularization approaches are investigated, among which the proposed graph-enhanced mixup is the most effective method to provide high-quality confidence to enhance our framework. The proposed WSL framework has achieved the state-of-the-art results on two large-scale WSL datasets, WebVision-1000 and Food101-N. Code is available at https://github.com/bigvideoresearch/SCC.



### Pose-Guided High-Resolution Appearance Transfer via Progressive Training
- **Arxiv ID**: http://arxiv.org/abs/2008.11898v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11898v2)
- **Published**: 2020-08-27 03:18:44+00:00
- **Updated**: 2022-11-25 00:54:45+00:00
- **Authors**: Ji Liu, Heshan Liu, Mang-Tik Chiu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: 10 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: We propose a novel pose-guided appearance transfer network for transferring a given reference appearance to a target pose in unprecedented image resolution (1024 * 1024), given respectively an image of the reference and target person. No 3D model is used. Instead, our network utilizes dense local descriptors including local perceptual loss and local discriminators to refine details, which is trained progressively in a coarse-to-fine manner to produce the high-resolution output to faithfully preserve complex appearance of garment textures and geometry, while hallucinating seamlessly the transferred appearances including those with dis-occlusion. Our progressive encoder-decoder architecture can learn the reference appearance inherent in the input image at multiple scales. Extensive experimental results on the Human3.6M dataset, the DeepFashion dataset, and our dataset collected from YouTube show that our model produces high-quality images, which can be further utilized in useful applications such as garment transfer between people and pose-guided human video generation.



### Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.11901v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11901v2)
- **Published**: 2020-08-27 03:32:25+00:00
- **Updated**: 2021-10-19 00:36:07+00:00
- **Authors**: Sudeep Fadadu, Shreyash Pandey, Darshan Hegde, Yi Shi, Fang-Chieh Chou, Nemanja Djuric, Carlos Vallespi-Gonzalez
- **Comment**: Accepted for publication at IEEE Winter Conference on Applications of
  Computer Vision (WACV) 2022
- **Journal**: None
- **Summary**: We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns and camera images. In this work, we recognize the strengths and weaknesses of different view representations, and we propose an efficient and generic fusing method that aggregates benefits from all views. Our model builds on a state-of-the-art Bird's-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend this model with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, non-quantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, the RV fusion of LiDAR and camera is performed in a straightforward and computationally efficient manner using this framework. The proposed multi-view fusion approach improves the state-of-the-art on proprietary large-scale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set with minimal increases on the computational cost.



### Domain Adaptation Through Task Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.11911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.11911v1)
- **Published**: 2020-08-27 04:44:49+00:00
- **Updated**: 2020-08-27 04:44:49+00:00
- **Authors**: Brady Zhou, Nimit Kalra, Philipp Krähenbühl
- **Comment**: Published in European Conference on Computer Vision (ECCV 2020)
- **Journal**: None
- **Summary**: Deep networks devour millions of precisely annotated images to build their complex and powerful representations. Unfortunately, tasks like autonomous driving have virtually no real-world training data. Repeatedly crashing a car into a tree is simply too expensive. The commonly prescribed solution is simple: learn a representation in simulation and transfer it to the real world. However, this transfer is challenging since simulated and real-world visual experiences vary dramatically. Our core observation is that for certain tasks, such as image recognition, datasets are plentiful. They exist in any interesting domain, simulated or real, and are easy to label and extend. We use these recognition datasets to link up a source and target domain to transfer models between them in a task distillation framework. Our method can successfully transfer navigation policies between drastically different simulators: ViZDoom, SuperTuxKart, and CARLA. Furthermore, it shows promising results on standard domain adaptation benchmarks.



### Fingerprint Feature Extraction by Combining Texture, Minutiae, and Frequency Spectrum Using Multi-Task CNN
- **Arxiv ID**: http://arxiv.org/abs/2008.11917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11917v1)
- **Published**: 2020-08-27 05:15:39+00:00
- **Updated**: 2020-08-27 05:15:39+00:00
- **Authors**: Ai Takahashi, Yoshinori Koda, Koichi Ito, Takafumi Aoki
- **Comment**: IJCB2020
- **Journal**: None
- **Summary**: Although most fingerprint matching methods utilize minutia points and/or texture of fingerprint images as fingerprint features, the frequency spectrum is also a useful feature since a fingerprint is composed of ridge patterns with its inherent frequency band. We propose a novel CNN-based method for extracting fingerprint features from texture, minutiae, and frequency spectrum. In order to extract effective texture features from local regions around the minutiae, the minutia attention module is introduced to the proposed method. We also propose new data augmentation methods, which takes into account the characteristics of fingerprint images to increase the number of images during training since we use only a public dataset in training, which includes a few fingerprint classes. Through a set of experiments using FVC2004 DB1 and DB2, we demonstrated that the proposed method exhibits the efficient performance on fingerprint verification compared with a commercial fingerprint matching software and the conventional method.



### Unsupervised MRI Super-Resolution Using Deep External Learning and Guided Residual Dense Network with Multimodal Image Priors
- **Arxiv ID**: http://arxiv.org/abs/2008.11921v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11921v3)
- **Published**: 2020-08-27 05:46:31+00:00
- **Updated**: 2022-10-13 05:21:32+00:00
- **Authors**: Yutaro Iwamoto, Kyohei Takeda, Yinhao Li, Akihiko Shiino, Yen-Wei Chen
- **Comment**: 10 pages, 3 figures, Accepted by IEEE Transactions on Emerging Topics
  in Computational Intelligence (TETCI)
- **Journal**: None
- **Summary**: Deep learning techniques have led to state-of-the-art image super resolution with natural images. Normally, pairs of high-resolution and low-resolution images are used to train the deep learning models. These techniques have also been applied to medical image super-resolution. The characteristics of medical images differ significantly from natural images in several ways. First, it is difficult to obtain high-resolution images for training in real clinical applications due to the limitations of imaging systems and clinical requirements. Second, other modal high-resolution images are available (e.g., high-resolution T1-weighted images are available for enhancing low-resolution T2-weighted images). In this paper, we propose an unsupervised image super-resolution technique based on simple prior knowledge of the human anatomy. This technique does not require target T2WI high-resolution images for training. Furthermore, we present a guided residual dense network, which incorporates a residual dense network with a guided deep convolutional neural network for enhancing the resolution of low-resolution images by referring to different modal high-resolution images of the same subject. Experiments on a publicly available brain MRI database showed that our proposed method achieves better performance than the state-of-the-art methods.



### Machine Learning and Computer Vision Techniques to Predict Thermal Properties of Particulate Composites
- **Arxiv ID**: http://arxiv.org/abs/2010.01968v1
- **DOI**: None
- **Categories**: **physics.app-ph**, cs.CV, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2010.01968v1)
- **Published**: 2020-08-27 06:21:40+00:00
- **Updated**: 2020-08-27 06:21:40+00:00
- **Authors**: Fazlolah Mohaghegh, Jayathi Murthy
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate thermal analysis of composites and porous media requires detailed characterization of local thermal properties in small scale. For some important applications such as lithium-ion batteries, changes in the properties during the operation makes the analysis even more challenging, necessitating a rapid characterization. We propose a new method to characterize the thermal properties of particulate composites based on actual micro-images. Our computer-vision-based approach constructs 3D images from stacks of 2D SEM images and then extracts several representative elemental volumes (REVs) from the reconstructed images at random places, which leads to having a range of geometrical features for different REVs. A deep learning algorithm is designed based on convolutional neural nets to take the shape of the geometry and result in the effective conductivity of the REV. The training of the network is performed in two methods: First, based on implementing a coarser grid that uses the average values of conductivities from the fine grid and the resulted effective conductivity from the DNS solution of the fine grid. The other method uses conductivity values on cross sections from each REV in different directions. The results of training based on averaging show that using a coarser grid in the network does not have a meaningful effect on the network error; however, it decreases the training time up to three orders of magnitude. We showed that one general network can make accurate predictions using different types of electrode images, representing the difference in the geometry and constituents. Moreover, training based on averaging is more accurate than training based on cross sections. The study of the robustness of implementing a machine learning technique in predicting the thermal percolation shows the prediction error is almost half of the error from predictions based on the volume fraction.



### Attribute-guided image generation from layout
- **Arxiv ID**: http://arxiv.org/abs/2008.11932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11932v1)
- **Published**: 2020-08-27 06:22:14+00:00
- **Updated**: 2020-08-27 06:22:14+00:00
- **Authors**: Ke Ma, Bo Zhao, Leonid Sigal
- **Comment**: None
- **Journal**: BMVC 2020
- **Summary**: Recent approaches have achieved great success in image generation from structured inputs, e.g., semantic segmentation, scene graph or layout. Although these methods allow specification of objects and their locations at image-level, they lack the fidelity and semantic control to specify visual appearance of these objects at an instance-level. To address this limitation, we propose a new image generation method that enables instance-level attribute control. Specifically, the input to our attribute-guided generative model is a tuple that contains: (1) object bounding boxes, (2) object categories and (3) an (optional) set of attributes for each object. The output is a generated image where the requested objects are in the desired locations and have prescribed attributes. Several losses work collaboratively to encourage accurate, consistent and diverse image generation. Experiments on Visual Genome dataset demonstrate our model's capacity to control object-level attributes in generated images, and validate plausibility of disentangled object-attribute representation in the image generation from layout task. Also, the generated images from our model have higher resolution, object classification accuracy and consistency, as compared to the previous state-of-the-art.



### Mixed Noise Removal with Pareto Prior
- **Arxiv ID**: http://arxiv.org/abs/2008.11935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11935v1)
- **Published**: 2020-08-27 06:35:15+00:00
- **Updated**: 2020-08-27 06:35:15+00:00
- **Authors**: Zhou Liu, Lei Yu, Gui-Song Xia, Hong Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Denoising images contaminated by the mixture of additive white Gaussian noise (AWGN) and impulse noise (IN) is an essential but challenging problem. The presence of impulsive disturbances inevitably affects the distribution of noises and thus largely degrades the performance of traditional AWGN denoisers. Existing methods target to compensate the effects of IN by introducing a weighting matrix, which, however, is lack of proper priori and thus hard to be accurately estimated. To address this problem, we exploit the Pareto distribution as the priori of the weighting matrix, based on which an accurate and robust weight estimator is proposed for mixed noise removal. Particularly, a relatively small portion of pixels are assumed to be contaminated with IN, which should have weights with small values and then be penalized out. This phenomenon can be properly described by the Pareto distribution of type 1. Therefore, armed with the Pareto distribution, we formulate the problem of mixed noise removal in the Bayesian framework, where nonlocal self-similarity priori is further exploited by adopting nonlocal low rank approximation. Compared to existing methods, the proposed method can estimate the weighting matrix adaptively, accurately, and robust for different level of noises, thus can boost the denoising performance. Experimental results on widely used image datasets demonstrate the superiority of our proposed method to the state-of-the-arts.



### Moderately Supervised Learning: Definition, Framework and Generality
- **Arxiv ID**: http://arxiv.org/abs/2008.11945v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T20, A.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.11945v5)
- **Published**: 2020-08-27 06:53:53+00:00
- **Updated**: 2023-07-24 01:23:36+00:00
- **Authors**: Yongquan Yang
- **Comment**: 33 pages,14 figures
- **Journal**: None
- **Summary**: Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the appropriate solutions for specific SL tasks. Thus, for engineers in the AI application field, it is desirable to reveal these details systematically. This article attempts to achieve this goal by expanding the categorization of SL and investigating the sub-type moderately supervised learning (MSL) that concerns the situation where the given labels are ideal, but due to the simplicity in annotation, careful designs are required to transform the given labels into easy-to-learn targets. From the perspectives of the definition, framework and generality, we conceptualize MSL to present a complete fundamental basis to systematically analyse MSL tasks. At meantime, revealing the relation between the conceptualization of MSL and the mathematicians' vision, this paper as well establishes a tutorial for AI application engineers to refer to viewing a problem to be solved from the mathematicians' vision.



### Unsupervised Surgical Instrument Segmentation via Anchor Generation and Semantic Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2008.11946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11946v1)
- **Published**: 2020-08-27 06:54:27+00:00
- **Updated**: 2020-08-27 06:54:27+00:00
- **Authors**: Daochang Liu, Yuhui Wei, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu Li
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Surgical instrument segmentation is a key component in developing context-aware operating rooms. Existing works on this task heavily rely on the supervision of a large amount of labeled data, which involve laborious and expensive human efforts. In contrast, a more affordable unsupervised approach is developed in this paper. To train our model, we first generate anchors as pseudo labels for instruments and background tissues respectively by fusing coarse handcrafted cues. Then a semantic diffusion loss is proposed to resolve the ambiguity in the generated anchors via the feature correlation between adjacent video frames. In the experiments on the binary instrument segmentation task of the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset, the proposed method achieves 0.71 IoU and 0.81 Dice score without using a single manual annotation, which is promising to show the potential of unsupervised learning for surgical tool segmentation.



### Surgical Skill Assessment on In-Vivo Clinical Data via the Clearness of Operating Field
- **Arxiv ID**: http://arxiv.org/abs/2008.11954v1
- **DOI**: 10.1007/978-3-030-32254-0_53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11954v1)
- **Published**: 2020-08-27 07:12:16+00:00
- **Updated**: 2020-08-27 07:12:16+00:00
- **Authors**: Daochang Liu, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu Li
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Surgical skill assessment is important for surgery training and quality control. Prior works on this task largely focus on basic surgical tasks such as suturing and knot tying performed in simulation settings. In contrast, surgical skill assessment is studied in this paper on a real clinical dataset, which consists of fifty-seven in-vivo laparoscopic surgeries and corresponding skill scores annotated by six surgeons. From analyses on this dataset, the clearness of operating field (COF) is identified as a good proxy for overall surgical skills, given its strong correlation with overall skills and high inter-annotator consistency. Then an objective and automated framework based on neural network is proposed to predict surgical skills through the proxy of COF. The neural network is jointly trained with a supervised regression loss and an unsupervised rank loss. In experiments, the proposed method achieves 0.55 Spearman's correlation with the ground truth of overall technical skill, which is even comparable with the human performance of junior surgeons.



### Multi-task deep CNN model for no-reference image quality assessment on smartphone camera photos
- **Arxiv ID**: http://arxiv.org/abs/2008.11961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.11961v1)
- **Published**: 2020-08-27 07:33:05+00:00
- **Updated**: 2020-08-27 07:33:05+00:00
- **Authors**: Chen-Hsiu Huang, Ja-Ling Wu
- **Comment**: Proceedings of Computer Vision & Graphic Image Processing (CVGIP),
  Hsinchu, Taiwan, Aug. 16-18, 2020
- **Journal**: None
- **Summary**: Smartphone is the most successful consumer electronic product in today's mobile social network era. The smartphone camera quality and its image post-processing capability is the dominant factor that impacts consumer's buying decision. However, the quality evaluation of photos taken from smartphones remains a labor-intensive work and relies on professional photographers and experts. As an extension of the prior CNN-based NR-IQA approach, we propose a multi-task deep CNN model with scene type detection as an auxiliary task. With the shared model parameters in the convolution layer, the learned feature maps could become more scene-relevant and enhance the performance. The evaluation result shows improved SROCC performance compared to traditional NR-IQA methods and single task CNN-based models.



### Adversarial Eigen Attack on Black-Box Models
- **Arxiv ID**: http://arxiv.org/abs/2009.00097v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.00097v1)
- **Published**: 2020-08-27 07:37:43+00:00
- **Updated**: 2020-08-27 07:37:43+00:00
- **Authors**: Linjun Zhou, Peng Cui, Yinan Jiang, Shiqiang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Black-box adversarial attack has attracted a lot of research interests for its practical use in AI safety. Compared with the white-box attack, a black-box setting is more difficult for less available information related to the attacked model and the additional constraint on the query budget. A general way to improve the attack efficiency is to draw support from a pre-trained transferable white-box model. In this paper, we propose a novel setting of transferable black-box attack: attackers may use external information from a pre-trained model with available network parameters, however, different from previous studies, no additional training data is permitted to further change or tune the pre-trained model. To this end, we further propose a new algorithm, EigenBA to tackle this problem. Our method aims to explore more gradient information of the black-box model, and promote the attack efficiency, while keeping the perturbation to the original attacked image small, by leveraging the Jacobian matrix of the pre-trained white-box model. We show the optimal perturbations are closely related to the right singular vectors of the Jacobian matrix. Further experiments on ImageNet and CIFAR-10 show that even the unlearnable pre-trained white-box model could also significantly boost the efficiency of the black-box attack and our proposed method could further improve the attack efficiency.



### Visual Question Answering on Image Sets
- **Arxiv ID**: http://arxiv.org/abs/2008.11976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11976v1)
- **Published**: 2020-08-27 08:03:32+00:00
- **Updated**: 2020-08-27 08:03:32+00:00
- **Authors**: Ankan Bansal, Yuting Zhang, Rama Chellappa
- **Comment**: Conference paper at ECCV 2020
- **Journal**: None
- **Summary**: We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.



### Edge and Identity Preserving Network for Face Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.11977v2
- **DOI**: 10.1016/j.neucom.2021.03.048
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11977v2)
- **Published**: 2020-08-27 08:04:57+00:00
- **Updated**: 2021-03-31 02:17:03+00:00
- **Authors**: Jonghyun Kim, Gen Li, Inyong Yun, Cheolkon Jung, Joongkyu Kim
- **Comment**: Neurocomputing'2021
- **Journal**: None
- **Summary**: Face super-resolution (SR) has become an indispensable function in security solutions such as video surveillance and identification system, but the distortion in facial components is a great challenge in it. Most state-of-the-art methods have utilized facial priors with deep neural networks. These methods require extra labels, longer training time, and larger computation memory. In this paper, we propose a novel Edge and Identity Preserving Network for Face SR Network, named as EIPNet, to minimize the distortion by utilizing a lightweight edge block and identity information. We present an edge block to extract perceptual edge information, and concatenate it to the original feature maps in multiple scales. This structure progressively provides edge information in reconstruction to aggregate local and global structural information. Moreover, we define an identity loss function to preserve identification of SR images. The identity loss function compares feature distributions between SR images and their ground truth to recover identities in SR images. In addition, we provide a luminance-chrominance error (LCE) to separately infer brightness and color information in SR images. The LCE method not only reduces the dependency of color information by dividing brightness and color components but also enables our network to reflect differences between SR images and their ground truth in two color spaces of RGB and YUV. The proposed method facilitates the proposed SR network to elaborately restore facial components and generate high quality 8x scaled SR images with a lightweight network structure. Furthermore, our network is able to reconstruct an 128x128 SR image with 215 fps on a GTX 1080Ti GPU. Extensive experiments demonstrate that our network qualitatively and quantitatively outperforms state-of-the-art methods on two challenging datasets: CelebA and VGGFace2.



### Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events
- **Arxiv ID**: http://arxiv.org/abs/2008.11988v1
- **DOI**: 10.1145/3394171.3413973
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11988v1)
- **Published**: 2020-08-27 08:32:51+00:00
- **Updated**: 2020-08-27 08:32:51+00:00
- **Authors**: Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, Marius Kloft
- **Comment**: To be published as an oral paper in Proceedings of the 28th ACM
  International Conference on Multimedia (ACM MM '20). 9 pages, 7 figures
- **Journal**: None
- **Summary**: As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.



### A Flexible Selection Scheme for Minimum-Effort Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.11995v1
- **DOI**: 10.1109/WACV45572.2020.9093635
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11995v1)
- **Published**: 2020-08-27 08:57:30+00:00
- **Updated**: 2020-08-27 08:57:30+00:00
- **Authors**: Amelie Royer, Christoph H. Lampert
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: Fine-tuning is a popular way of exploiting knowledge contained in a pre-trained convolutional network for a new visual recognition task. However, the orthogonal setting of transferring knowledge from a pretrained network to a visually different yet semantically close source is rarely considered: This commonly happens with real-life data, which is not necessarily as clean as the training source (noise, geometric transformations, different modalities, etc.). To tackle such scenarios, we introduce a new, generalized form of fine-tuning, called flex-tuning, in which any individual unit (e.g. layer) of a network can be tuned, and the most promising one is chosen automatically. In order to make the method appealing for practical use, we propose two lightweight and faster selection procedures that prove to be good approximations in practice. We study these selection criteria empirically across a variety of domain shifts and data scarcity scenarios, and show that fine-tuning individual units, despite its simplicity, yields very good results as an adaptation technique. As it turns out, in contrast to common practice, rather than the last fully-connected unit it is best to tune an intermediate or early one in many domain-shift scenarios, which is accurately detected by flex-tuning.



### Deep Spatial Transformation for Pose-Guided Person Image Generation and Animation
- **Arxiv ID**: http://arxiv.org/abs/2008.12606v1
- **DOI**: 10.1109/TIP.2020.3018224
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12606v1)
- **Published**: 2020-08-27 08:59:44+00:00
- **Updated**: 2020-08-27 08:59:44+00:00
- **Authors**: Yurui Ren, Ge Li, Shan Liu, Thomas H. Li
- **Comment**: arXiv admin note: text overlap with arXiv:2003.00696
- **Journal**: None
- **Summary**: Pose-guided person image generation and animation aim to transform a source person image to target poses. These tasks require spatial manipulation of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. This framework first estimates global flow fields between sources and targets. Then, corresponding local source feature patches are sampled with content-aware local attention coefficients. We show that our framework can spatially transform the inputs in an efficient manner. Meanwhile, we further model the temporal consistency for the person image animation task to generate coherent videos. The experiment results of both image generation and animation tasks demonstrate the superiority of our model. Besides, additional results of novel view synthesis and face image animation show that our model is applicable to other tasks requiring spatial transformation. The source code of our project is available at https://github.com/RenYurui/Global-Flow-Local-Attention.



### How semantic and geometric information mutually reinforce each other in ToF object localization
- **Arxiv ID**: http://arxiv.org/abs/2008.12002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12002v1)
- **Published**: 2020-08-27 09:13:26+00:00
- **Updated**: 2020-08-27 09:13:26+00:00
- **Authors**: Antoine Vanderschueren, Victor Joos, Christophe De Vleeschouwer
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to localize a 3D object from the intensity and depth information images provided by a Time-of-Flight (ToF) sensor. Our method uses two CNNs. The first one uses raw depth and intensity images as input, to segment the floor pixels, from which the extrinsic parameters of the camera are estimated. The second CNN is in charge of segmenting the object-of-interest. As a main innovation, it exploits the calibration estimated from the prediction of the first CNN to represent the geometric depth information in a coordinate system that is attached to the ground, and is thus independent of the camera elevation. In practice, both the height of pixels with respect to the ground, and the orientation of normals to the point cloud are provided as input to the second CNN. Given the segmentation predicted by the second CNN, the object is localized based on point cloud alignment with a reference model. Our experiments demonstrate that our proposed two-step approach improves segmentation and localization accuracy by a significant margin compared to a conventional CNN architecture, ignoring calibration and height maps, but also compared to PointNet++.



### A survey on applications of augmented, mixed and virtual reality for nature and environment
- **Arxiv ID**: http://arxiv.org/abs/2008.12024v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2008.12024v2)
- **Published**: 2020-08-27 09:59:27+00:00
- **Updated**: 2020-08-28 08:47:26+00:00
- **Authors**: Jason Rambach, Gergana Lilligreen, Alexander Schäfer, Ramya Bankanal, Alexander Wiebel, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are technologies of great potential due to the engaging and enriching experiences they are capable of providing. Their use is rapidly increasing in diverse fields such as medicine, manufacturing or entertainment. However, the possibilities that AR, VR and MR offer in the area of environmental applications are not yet widely explored. In this paper we present the outcome of a survey meant to discover and classify existing AR/VR/MR applications that can benefit the environment or increase awareness on environmental issues. We performed an exhaustive search over several online publication access platforms and past proceedings of major conferences in the fields of AR/VR/MR. Identified relevant papers were filtered based on novelty, technical soundness, impact and topic relevance, and classified into different categories. Referring to the selected papers, we discuss how the applications of each category are contributing to environmental protection, preservation and sensitization purposes. We further analyse these approaches as well as possible future directions in the scope of existing and upcoming AR/VR/MR enabling technologies.



### Meta-Learning with Shared Amortized Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/2008.12037v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.12037v1)
- **Published**: 2020-08-27 10:28:13+00:00
- **Updated**: 2020-08-27 10:28:13+00:00
- **Authors**: Ekaterina Iakovleva, Jakob Verbeek, Karteek Alahari
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: We propose a novel amortized variational inference scheme for an empirical Bayes meta-learning model, where model parameters are treated as latent variables. We learn the prior distribution over model parameters conditioned on limited training data using a variational autoencoder approach. Our framework proposes sharing the same amortized inference network between the conditional prior and variational posterior distributions over the model parameters. While the posterior leverages both the labeled support and query data, the conditional prior is based only on the labeled support data. We show that in earlier work, relying on Monte-Carlo approximation, the conditional prior collapses to a Dirac delta function. In contrast, our variational approach prevents this collapse and preserves uncertainty over the model parameters. We evaluate our approach on the miniImageNet, CIFAR-FS and FC100 datasets, and present results demonstrating its advantages over previous work.



### Inner Eye Canthus Localization for Human Body Temperature Screening
- **Arxiv ID**: http://arxiv.org/abs/2008.12046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12046v1)
- **Published**: 2020-08-27 10:47:57+00:00
- **Updated**: 2020-08-27 10:47:57+00:00
- **Authors**: Claudio Ferrari, Lorenzo Berlincioni, Marco Bertini, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an automatic approach for localizing the inner eye canthus in thermal face images. We first coarsely detect 5 facial keypoints corresponding to the center of the eyes, the nosetip and the ears. Then we compute a sparse 2D-3D points correspondence using a 3D Morphable Face Model (3DMM). This correspondence is used to project the entire 3D face onto the image, and subsequently locate the inner eye canthus. Detecting this location allows to obtain the most precise body temperature measurement for a person using a thermal camera. We evaluated the approach on a thermal face dataset provided with manually annotated landmarks. However, such manual annotations are normally conceived to identify facial parts such as eyes, nose and mouth, and are not specifically tailored for localizing the eye canthus region. As additional contribution, we enrich the original dataset by using the annotated landmarks to deform and project the 3DMM onto the images. Then, by manually selecting a small region corresponding to the eye canthus, we enrich the dataset with additional annotations. By using the manual landmarks, we ensure the correctness of the 3DMM projection, which can be used as ground-truth for future evaluations. Moreover, we supply the dataset with the 3D head poses and per-point visibility masks for detecting self-occlusions. The data will be publicly released.



### Compensation Tracker: Reprocessing Lost Object for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.12052v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12052v4)
- **Published**: 2020-08-27 10:59:54+00:00
- **Updated**: 2022-02-05 13:48:43+00:00
- **Authors**: Zhibo Zou, Junjie Huang, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking by detection paradigm is one of the most popular object tracking methods. However, it is very dependent on the performance of the detector. When the detector has a behavior of missing detection, the tracking result will be directly affected. In this paper, we analyze the phenomenon of the lost tracking object in real-time tracking model on MOT2020 dataset. Based on simple and traditional methods, we propose a compensation tracker to further alleviate the lost tracking problem caused by missing detection. It consists of a motion compensation module and an object selection module. The proposed method not only can re-track missing tracking objects from lost objects, but also does not require additional networks so as to maintain speed-accuracy trade-off of the real-time model. Our method only needs to be embedded into the tracker to work without re-training the network. Experiments show that the compensation tracker can efficaciously improve the performance of the model and reduce identity switches. With limited costs, the compensation tracker successfully enhances the baseline tracking performance by a large margin and reaches 66% of MOTA and 67% of IDF1 on MOT2020 dataset.



### Propensity-to-Pay: Machine Learning for Estimating Prediction Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2008.12065v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.12065v1)
- **Published**: 2020-08-27 11:49:25+00:00
- **Updated**: 2020-08-27 11:49:25+00:00
- **Authors**: Md Abul Bashar, Astin-Walmsley Kieren, Heath Kerina, Richi Nayak
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting a customer's propensity-to-pay at an early point in the revenue cycle can provide organisations many opportunities to improve the customer experience, reduce hardship and reduce the risk of impaired cash flow and occurrence of bad debt. With the advancements in data science; machine learning techniques can be used to build models to accurately predict a customer's propensity-to-pay. Creating effective machine learning models without access to large and detailed datasets presents some significant challenges. This paper presents a case-study, conducted on a dataset from an energy organisation, to explore the uncertainty around the creation of machine learning models that are able to predict residential customers entering financial hardship which then reduces their ability to pay energy bills. Incorrect predictions can result in inefficient resource allocation and vulnerable customers not being proactively identified. This study investigates machine learning models' ability to consider different contexts and estimate the uncertainty in the prediction. Seven models from four families of machine learning algorithms are investigated for their novel utilisation. A novel concept of utilising a Baysian Neural Network to the binary classification problem of propensity-to-pay energy bills is proposed and explored for deployment.



### Minimal Adversarial Examples for Deep Learning on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.12066v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12066v4)
- **Published**: 2020-08-27 11:50:45+00:00
- **Updated**: 2021-09-17 09:16:24+00:00
- **Authors**: Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung
- **Comment**: ICCV 2021 camera-ready paper (8 pages)
- **Journal**: None
- **Summary**: With recent developments of convolutional neural networks, deep learning for 3D point clouds has shown significant progress in various 3D scene understanding tasks, e.g., object recognition, semantic segmentation. In a safety-critical environment, it is however not well understood how such deep learning models are vulnerable to adversarial examples. In this work, we explore adversarial attacks for point cloud-based neural networks. We propose a unified formulation for adversarial point cloud generation that can generalise two different attack strategies. Our method generates adversarial examples by attacking the classification ability of point cloud-based networks while considering the perceptibility of the examples and ensuring the minimal level of point manipulations. Experimental results show that our method achieves the state-of-the-art performance with higher than 89% and 90% of attack success rate on synthetic and real-world data respectively, while manipulating only about 4% of the total points.



### DMD: A Large-Scale Multi-Modal Driver Monitoring Dataset for Attention and Alertness Analysis
- **Arxiv ID**: http://arxiv.org/abs/2008.12085v1
- **DOI**: 10.1007/978-3-030-66823-5_23
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12085v1)
- **Published**: 2020-08-27 12:33:54+00:00
- **Updated**: 2020-08-27 12:33:54+00:00
- **Authors**: Juan Diego Ortega, Neslihan Kose, Paola Cañas, Min-An Chao, Alexander Unnervik, Marcos Nieto, Oihana Otaegui, Luis Salgado
- **Comment**: Accepted to ECCV 2020 workshop - Assistive Computer Vision and
  Robotics
- **Journal**: None
- **Summary**: Vision is the richest and most cost-effective technology for Driver Monitoring Systems (DMS), especially after the recent success of Deep Learning (DL) methods. The lack of sufficiently large and comprehensive datasets is currently a bottleneck for the progress of DMS development, crucial for the transition of automated driving from SAE Level-2 to SAE Level-3. In this paper, we introduce the Driver Monitoring Dataset (DMD), an extensive dataset which includes real and simulated driving scenarios: distraction, gaze allocation, drowsiness, hands-wheel interaction and context data, in 41 hours of RGB, depth and IR videos from 3 cameras capturing face, body and hands of 37 drivers. A comparison with existing similar datasets is included, which shows the DMD is more extensive, diverse, and multi-purpose. The usage of the DMD is illustrated by extracting a subset of it, the dBehaviourMD dataset, containing 13 distraction activities, prepared to be used in DL training processes. Furthermore, we propose a robust and real-time driver behaviour recognition system targeting a real-world application that can run on cost-efficient CPU-only platforms, based on the dBehaviourMD. Its performance is evaluated with different types of fusion strategies, which all reach enhanced accuracy still providing real-time response.



### MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.12094v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12094v1)
- **Published**: 2020-08-27 13:04:27+00:00
- **Updated**: 2020-08-27 13:04:27+00:00
- **Authors**: Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, Cho-jui Hsieh
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) has been one of the most popu-lar methods to learn a compact model. However, it still suffers from highdemand in time and computational resources caused by sequential train-ing pipeline. Furthermore, the soft targets from deeper models do notoften serve as good cues for the shallower models due to the gap of com-patibility. In this work, we consider these two problems at the same time.Specifically, we propose that better soft targets with higher compatibil-ity can be generated by using a label generator to fuse the feature mapsfrom deeper stages in a top-down manner, and we can employ the meta-learning technique to optimize this label generator. Utilizing the softtargets learned from the intermediate feature maps of the model, we canachieve better self-boosting of the network in comparison with the state-of-the-art. The experiments are conducted on two standard classificationbenchmarks, namely CIFAR-100 and ILSVRC2012. We test various net-work architectures to show the generalizability of our MetaDistiller. Theexperiments results on two datasets strongly demonstrate the effective-ness of our method.



### Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images
- **Arxiv ID**: http://arxiv.org/abs/2008.12165v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12165v2)
- **Published**: 2020-08-27 14:46:22+00:00
- **Updated**: 2020-12-08 23:43:12+00:00
- **Authors**: Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Image features for retrieval-based localization must be invariant to dynamic objects (e.g. cars) as well as seasonal and daytime changes. Such invariances are, up to some extent, learnable with existing methods using triplet-like losses, given a large number of diverse training images. However, due to the high algorithmic training complexity, there exists insufficient comparison between different loss functions on large datasets. In this paper, we train and evaluate several localization methods on three different benchmark datasets, including Oxford RobotCar with over one million images. This large scale evaluation yields valuable insights into the generalizability and performance of retrieval-based localization. Based on our findings, we develop a novel method for learning more accurate and better generalizing localization features. It consists of two main contributions: (i) a feature volume-based loss function, and (ii) hard positive and pairwise negative mining. On the challenging Oxford RobotCar night condition, our method outperforms the well-known triplet loss by 24.4% in localization accuracy within 5m.



### Instance Adaptive Self-Training for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.12197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12197v1)
- **Published**: 2020-08-27 15:50:27+00:00
- **Updated**: 2020-08-27 15:50:27+00:00
- **Authors**: Ke Mei, Chuang Zhu, Jiaqi Zou, Shanghang Zhang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYNTHIA to Cityscapes' demonstrate the superior performance of our approach compared with the state-of-the-art methods.



### Random Style Transfer based Domain Generalization Networks Integrating Shape and Spatial Information
- **Arxiv ID**: http://arxiv.org/abs/2008.12205v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12205v2)
- **Published**: 2020-08-27 16:00:21+00:00
- **Updated**: 2020-09-03 11:18:42+00:00
- **Authors**: Lei Li, Veronika A. Zimmer, Wangbin Ding, Fuping Wu, Liqin Huang, Julia A. Schnabel, Xiahai Zhuang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Deep learning (DL)-based models have demonstrated good performance in medical image segmentation. However, the models trained on a known dataset often fail when performed on an unseen dataset collected from different centers, vendors and disease populations. In this work, we present a random style transfer network to tackle the domain generalization problem for multi-vendor and center cardiac image segmentation. Style transfer is used to generate training data with a wider distribution/ heterogeneity, namely domain augmentation. As the target domain could be unknown, we randomly generate a modality vector for the target modality in the style transfer stage, to simulate the domain shift for unknown domains. The model can be trained in a semi-supervised manner by simultaneously optimizing a supervised segmentation and an unsupervised style translation objective. Besides, the framework incorporates the spatial information and shape prior of the target by introducing two regularization terms. We evaluated the proposed framework on 40 subjects from the M\&Ms challenge2020, and obtained promising performance in the segmentation for data from unknown vendors and centers.



### SketchEmbedNet: Learning Novel Concepts by Imitating Drawings
- **Arxiv ID**: http://arxiv.org/abs/2009.04806v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.04806v4)
- **Published**: 2020-08-27 16:43:28+00:00
- **Updated**: 2021-06-22 19:45:09+00:00
- **Authors**: Alexander Wang, Mengye Ren, Richard S. Zemel
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: Sketch drawings capture the salient information of visual concepts. Previous work has shown that neural networks are capable of producing sketches of natural objects drawn from a small number of classes. While earlier approaches focus on generation quality or retrieval, we explore properties of image representations learned by training a model to produce sketches of images. We show that this generative, class-agnostic model produces informative embeddings of images from novel examples, classes, and even novel datasets in a few-shot setting. Additionally, we find that these learned representations exhibit interesting structure and compositionality.



### DeepFake Detection Based on the Discrepancy Between the Face and its Context
- **Arxiv ID**: http://arxiv.org/abs/2008.12262v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12262v1)
- **Published**: 2020-08-27 17:04:46+00:00
- **Updated**: 2020-08-27 17:04:46+00:00
- **Authors**: Yuval Nirkin, Lior Wolf, Yosi Keller, Tal Hassner
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for detecting face swapping and other identity manipulations in single images. Face swapping methods, such as DeepFake, manipulate the face region, aiming to adjust the face to the appearance of its context, while leaving the context unchanged. We show that this modus operandi produces discrepancies between the two regions. These discrepancies offer exploitable telltale signs of manipulation. Our approach involves two networks: (i) a face identification network that considers the face region bounded by a tight semantic segmentation, and (ii) a context recognition network that considers the face context (e.g., hair, ears, neck). We describe a method which uses the recognition signals from our two networks to detect such discrepancies, providing a complementary detection signal that improves conventional real vs. fake classifiers commonly used for detecting fake images. Our method achieves state of the art results on the FaceForensics++, Celeb-DF-v2, and DFDC benchmarks for face manipulation detection, and even generalizes to detect fakes produced by unseen methods.



### Monocular, One-stage, Regression of Multiple 3D People
- **Arxiv ID**: http://arxiv.org/abs/2008.12272v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12272v4)
- **Published**: 2020-08-27 17:21:47+00:00
- **Updated**: 2021-09-16 11:41:15+00:00
- **Authors**: Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J. Black, Tao Mei
- **Comment**: ICCV 2021, Code https://github.com/Arthur151/ROMP
- **Journal**: None
- **Summary**: This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code is the first real-time implementation of monocular multi-person 3D mesh regression.



### learn2learn: A Library for Meta-Learning Research
- **Arxiv ID**: http://arxiv.org/abs/2008.12284v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.12284v2)
- **Published**: 2020-08-27 17:41:34+00:00
- **Updated**: 2020-08-28 03:48:50+00:00
- **Authors**: Sébastien M. R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner, Konstantinos Saitas Zarkias
- **Comment**: Software available at: https://github.com/learnables/learn2learn
- **Journal**: None
- **Summary**: Meta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -- a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas.   This manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research.



### Reducing Drift in Structure From Motion Using Extended Features
- **Arxiv ID**: http://arxiv.org/abs/2008.12295v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12295v3)
- **Published**: 2020-08-27 17:59:04+00:00
- **Updated**: 2020-10-14 01:59:53+00:00
- **Authors**: Aleksander Holynski, David Geraghty, Jan-Michael Frahm, Chris Sweeney, Richard Szeliski
- **Comment**: 3DV 2020
- **Journal**: None
- **Summary**: Low-frequency long-range errors (drift) are an endemic problem in 3D structure from motion, and can often hamper reasonable reconstructions of the scene. In this paper, we present a method to dramatically reduce scale and positional drift by using extended structural features such as planes and vanishing points. Unlike traditional feature matches, our extended features are able to span non-overlapping input images, and hence provide long-range constraints on the scale and shape of the reconstruction. We add these features as additional constraints to a state-of-the-art global structure from motion algorithm and demonstrate that the added constraints enable the reconstruction of particularly drift-prone sequences such as long, low field-of-view videos without inertial measurements. Additionally, we provide an analysis of the drift-reducing capabilities of these constraints by evaluating on a synthetic dataset. Our structural features are able to significantly reduce drift for scenes that contain long-spanning man-made structures, such as aligned rows of windows or planar building facades.



### One Shot 3D Photography
- **Arxiv ID**: http://arxiv.org/abs/2008.12298v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.12298v2)
- **Published**: 2020-08-27 17:59:31+00:00
- **Updated**: 2020-09-01 14:52:55+00:00
- **Authors**: Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge, Yangming Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu, Peizhao Zhang, Zijian He, Peter Vajda, Ayush Saraf, Michael Cohen
- **Comment**: Project page:
  https://facebookresearch.github.io/one_shot_3d_photography/ Code:
  https://github.com/facebookresearch/one_shot_3d_photography
- **Journal**: ACM Transactions on Graphics (Proceedings of SIGGRAPH 2020),
  Volume 39, Number 4, 2020
- **Summary**: 3D photography is a new medium that allows viewers to more fully experience a captured moment. In this work, we refer to a 3D photo as one that displays parallax induced by moving the viewpoint (as opposed to a stereo pair with a fixed viewpoint). 3D photos are static in time, like traditional photos, but are displayed with interactive parallax on mobile or desktop screens, as well as on Virtual Reality devices, where viewing it also includes stereo. We present an end-to-end system for creating and viewing 3D photos, and the algorithmic and design choices therein. Our 3D photos are captured in a single shot and processed directly on a mobile device. The method starts by estimating depth from the 2D input image using a new monocular depth estimation network that is optimized for mobile devices. It performs competitively to the state-of-the-art, but has lower latency and peak memory consumption and uses an order of magnitude fewer parameters. The resulting depth is lifted to a layered depth image, and new geometry is synthesized in parallax regions. We synthesize color texture and structures in the parallax regions as well, using an inpainting network, also optimized for mobile devices, on the LDI directly. Finally, we convert the result into a mesh-based representation that can be efficiently transmitted and rendered even on low-end devices and over poor network connections. Altogether, the processing takes just a few seconds on a mobile device, and the result can be instantly viewed and shared. We perform extensive quantitative evaluation to validate our system and compare its new components against the current state-of-the-art.



### Learning Representations of Endoscopic Videos to Detect Tool Presence Without Supervision
- **Arxiv ID**: http://arxiv.org/abs/2008.12321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12321v1)
- **Published**: 2020-08-27 18:23:05+00:00
- **Updated**: 2020-08-27 18:23:05+00:00
- **Authors**: David Z. Li, Masaru Ishii, Russell H. Taylor, Gregory D. Hager, Ayushi Sinha
- **Comment**: 10 pages, 4 figures, CLIP 2020
- **Journal**: None
- **Summary**: In this work, we explore whether it is possible to learn representations of endoscopic video frames to perform tasks such as identifying surgical tool presence without supervision. We use a maximum mean discrepancy (MMD) variational autoencoder (VAE) to learn low-dimensional latent representations of endoscopic videos and manipulate these representations to distinguish frames containing tools from those without tools. We use three different methods to manipulate these latent representations in order to predict tool presence in each frame. Our fully unsupervised methods can identify whether endoscopic video frames contain tools with average precision of 71.56, 73.93, and 76.18, respectively, comparable to supervised methods. Our code is available at https://github.com/zdavidli/tool-presence/



### A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/2008.12328v5
- **DOI**: 10.1109/TPAMI.2021.3074805
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12328v5)
- **Published**: 2020-08-27 18:39:24+00:00
- **Updated**: 2023-04-06 15:49:54+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-adopted definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propose a background-agnostic framework that learns from training videos containing only normal events. Our framework is composed of an object detector, a set of appearance and motion auto-encoders, and a set of classifiers. Since our framework only looks at object detections, it can be applied to different scenes, provided that normal events are defined identically across scenes and that the single main factor of variation is the background. To overcome the lack of abnormal data during training, we propose an adversarial learning strategy for the auto-encoders. We create a scene-agnostic set of out-of-domain pseudo-abnormal examples, which are correctly reconstructed by the auto-encoders before applying gradient ascent on the pseudo-abnormal examples. We further utilize the pseudo-abnormal examples to serve as abnormal examples when training appearance-based and motion-based binary classifiers to discriminate between normal and abnormal latent features and reconstructions. We compare our framework with the state-of-the-art methods on four benchmark data sets, using various evaluation metrics. Compared to existing methods, the empirical results indicate that our approach achieves favorable performance on all data sets. In addition, we provide region-based and track-based annotations for two large-scale abnormal event detection data sets from the literature, namely ShanghaiTech and Subway.



### Adversarially Robust Learning via Entropic Regularization
- **Arxiv ID**: http://arxiv.org/abs/2008.12338v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.12338v2)
- **Published**: 2020-08-27 18:54:43+00:00
- **Updated**: 2021-02-19 15:39:02+00:00
- **Authors**: Gauri Jagatap, Ameya Joshi, Animesh Basak Chowdhury, Siddharth Garg, Chinmay Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an additional entropic regularization. Our loss function considers the contribution of adversarial samples that are drawn from a specially designed distribution in the data space that assigns high probability to points with high loss and in the immediate neighborhood of training samples. Our proposed algorithms optimize this loss to seek adversarially robust valleys of the loss landscape. Our approach achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10.



### A Federated Approach for Fine-Grained Classification of Fashion Apparel
- **Arxiv ID**: http://arxiv.org/abs/2008.12350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12350v1)
- **Published**: 2020-08-27 19:44:43+00:00
- **Updated**: 2020-08-27 19:44:43+00:00
- **Authors**: Tejaswini Mallavarapu, Luke Cranfill, Junggab Son, Eun Hye Kim, Reza M. Parizi, John Morris
- **Comment**: 11 pages, 4 figures, 5 tables, submitted to IEEE ACCESS (under
  review)
- **Journal**: None
- **Summary**: As online retail services proliferate and are pervasive in modern lives, applications for classifying fashion apparel features from image data are becoming more indispensable. Online retailers, from leading companies to start-ups, can leverage such applications in order to increase profit margin and enhance the consumer experience. Many notable schemes have been proposed to classify fashion items, however, the majority of which focused upon classifying basic-level categories, such as T-shirts, pants, skirts, shoes, bags, and so forth. In contrast to most prior efforts, this paper aims to enable an in-depth classification of fashion item attributes within the same category. Beginning with a single dress, we seek to classify the type of dress hem, the hem length, and the sleeve length. The proposed scheme is comprised of three major stages: (a) localization of a target item from an input image using semantic segmentation, (b) detection of human key points (e.g., point of shoulder) using a pre-trained CNN and a bounding box, and (c) three phases to classify the attributes using a combination of algorithmic approaches and deep neural networks. The experimental results demonstrate that the proposed scheme is highly effective, with all categories having average precision of above 93.02%, and outperforms existing Convolutional Neural Networks (CNNs)-based schemes.



### Analyzing Worldwide Social Distancing through Large-Scale Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2008.12363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12363v1)
- **Published**: 2020-08-27 20:20:11+00:00
- **Updated**: 2020-08-27 20:20:11+00:00
- **Authors**: Isha Ghodgaonkar, Subhankar Chakraborty, Vishnu Banna, Shane Allcroft, Mohammed Metwaly, Fischer Bordwell, Kohsuke Kimura, Xinxin Zhao, Abhinav Goel, Caleb Tung, Akhil Chinnakotla, Minghao Xue, Yung-Hsiang Lu, Mark Daniel Ward, Wei Zakharov, David S. Ebert, David M. Barbarash, George K. Thiruvathukal
- **Comment**: 10 pages, 15 figures
- **Journal**: None
- **Summary**: In order to contain the COVID-19 pandemic, countries around the world have introduced social distancing guidelines as public health interventions to reduce the spread of the disease. However, monitoring the efficacy of these guidelines at a large scale (nationwide or worldwide) is difficult. To make matters worse, traditional observational methods such as in-person reporting is dangerous because observers may risk infection. A better solution is to observe activities through network cameras; this approach is scalable and observers can stay in safe locations. This research team has created methods that can discover thousands of network cameras worldwide, retrieve data from the cameras, analyze the data, and report the sizes of crowds as different countries issued and lifted restrictions (also called ''lockdown''). We discover 11,140 network cameras that provide real-time data and we present the results across 15 countries. We collect data from these cameras beginning April 2020 at approximately 0.5TB per week. After analyzing 10,424,459 images from still image cameras and frames extracted periodically from video, the data reveals that the residents in some countries exhibited more activity (judged by numbers of people and vehicles) after the restrictions were lifted. In other countries, the amounts of activities showed no obvious changes during the restrictions and after the restrictions were lifted. The data further reveals whether people stay ''social distancing'', at least 6 feet apart. This study discerns whether social distancing is being followed in several types of locations and geographical locations worldwide and serve as an early indicator whether another wave of infections is likely to occur soon.



### Improving the Segmentation of Scanning Probe Microscope Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.12371v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12371v1)
- **Published**: 2020-08-27 20:49:59+00:00
- **Updated**: 2020-08-27 20:49:59+00:00
- **Authors**: Steff Farley, Jo E. A. Hodgkinson, Oliver M. Gordon, Joanna Turner, Andrea Soltoggio, Philip J. Moriarty, Eugenie Hunsicker
- **Comment**: 21 pages, 10 figures
- **Journal**: None
- **Summary**: A wide range of techniques can be considered for segmentation of images of nanostructured surfaces. Manually segmenting these images is time-consuming and results in a user-dependent segmentation bias, while there is currently no consensus on the best automated segmentation methods for particular techniques, image classes, and samples. Any image segmentation approach must minimise the noise in the images to ensure accurate and meaningful statistical analysis can be carried out. Here we develop protocols for the segmentation of images of 2D assemblies of gold nanoparticles formed on silicon surfaces via deposition from an organic solvent. The evaporation of the solvent drives far-from-equilibrium self-organisation of the particles, producing a wide variety of nano- and micro-structured patterns. We show that a segmentation strategy using the U-Net convolutional neural network outperforms traditional automated approaches and has particular potential in the processing of images of nanostructured systems.



### Measuring the Biases and Effectiveness of Content-Style Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2008.12378v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12378v4)
- **Published**: 2020-08-27 21:41:37+00:00
- **Updated**: 2021-09-15 19:48:26+00:00
- **Authors**: Xiao Liu, Spyridon Thermos, Gabriele Valvano, Agisilaos Chartsias, Alison O'Neil, Sotirios A. Tsaftaris
- **Comment**: 28 pages, 10 figures
- **Journal**: None
- **Summary**: A recent spate of state-of-the-art semi- and un-supervised solutions disentangle and encode image "content" into a spatial tensor and image appearance or "style" into a vector, to achieve good performance in spatially equivariant tasks (e.g. image-to-image translation). To achieve this, they employ different model design, learning objective, and data biases. While considerable effort has been made to measure disentanglement in vector representations, and assess its impact on task performance, such analysis for (spatial) content - style disentanglement is lacking. In this paper, we conduct an empirical study to investigate the role of different biases in content-style disentanglement settings and unveil the relationship between the degree of disentanglement and task performance. In particular, we consider the setting where we: (i) identify key design choices and learning constraints for three popular content-style disentanglement models; (ii) relax or remove such constraints in an ablation fashion; and (iii) use two metrics to measure the degree of disentanglement and assess its effect on each task performance. Our experiments reveal that there is a "sweet spot" between disentanglement, task performance and - surprisingly - content interpretability, suggesting that blindly forcing for higher disentanglement can hurt model performance and content factors semanticness. Our findings, as well as the used task-independent metrics, can be used to guide the design and selection of new models for tasks where content-style representations are useful.



### Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2008.12380v2
- **DOI**: 10.1038/s42256-021-00379-y
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12380v2)
- **Published**: 2020-08-27 21:57:07+00:00
- **Updated**: 2021-06-22 19:37:38+00:00
- **Authors**: Alvaro Gomariz, Tiziano Portenier, Patrick M. Helbling, Stephan Isringhausen, Ute Suessbier, César Nombela-Arrieta, Orcun Goksel
- **Comment**: Main: 21 pages, 6 figures, 1 table. Supplementary: 5 pages, 7
  figures, 3 tables
- **Journal**: Nature Machine Intelligence (2021)
- **Summary**: Fluorescence microscopy allows for a detailed inspection of cells, cellular networks, and anatomical landmarks by staining with a variety of carefully-selected markers visualized as color channels. Quantitative characterization of structures in acquired images often relies on automatic image analysis methods. Despite the success of deep learning methods in other vision applications, their potential for fluorescence image analysis remains underexploited. One reason lies in the considerable workload required to train accurate models, which are normally specific for a given combination of markers, and therefore applicable to a very restricted number of experimental settings. We herein propose Marker Sampling and Excite, a neural network approach with a modality sampling strategy and a novel attention module that together enable (i) flexible training with heterogeneous datasets with combinations of markers and (ii) successful utility of learned models on arbitrary subsets of markers prospectively. We show that our single neural network solution performs comparably to an upper bound scenario where an ensemble of many networks is na\"ively trained for each possible marker combination separately. In addition, we demonstrate the feasibility of this framework in high-throughput biological analysis by revising a recent quantitative characterization of bone marrow vasculature in 3D confocal microscopy datasets and further confirm the validity of our approach on an additional, significantly different dataset of microvessels in fetal liver tissues. Not only can our work substantially ameliorate the use of deep learning in fluorescence microscopy analysis, but it can also be utilized in other fields with incomplete data acquisitions and missing modalities.



### Adversarial Training for Multi-Channel Sign Language Production
- **Arxiv ID**: http://arxiv.org/abs/2008.12405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12405v1)
- **Published**: 2020-08-27 23:05:54+00:00
- **Updated**: 2020-08-27 23:05:54+00:00
- **Authors**: Ben Saunders, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Sign Languages are rich multi-channel languages, requiring articulation of both manual (hands) and non-manual (face and body) features in a precise, intricate manner. Sign Language Production (SLP), the automatic translation from spoken to sign languages, must embody this full sign morphology to be truly understandable by the Deaf community. Previous work has mainly focused on manual feature production, with an under-articulated output caused by regression to the mean.   In this paper, we propose an Adversarial Multi-Channel approach to SLP. We frame sign production as a minimax game between a transformer-based Generator and a conditional Discriminator. Our adversarial discriminator evaluates the realism of sign production conditioned on the source text, pushing the generator towards a realistic and articulate output. Additionally, we fully encapsulate sign articulators with the inclusion of non-manual features, producing facial features and mouthing patterns.   We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, and report state-of-the art SLP back-translation performance for manual production. We set new benchmarks for the production of multi-channel sign to underpin future research into realistic SLP.



### W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in Ultrasound Images by Expanding U-Net to Incorporate Ultrasound RF Waveform Data
- **Arxiv ID**: http://arxiv.org/abs/2008.12413v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12413v2)
- **Published**: 2020-08-27 23:53:20+00:00
- **Updated**: 2020-09-02 09:14:27+00:00
- **Authors**: Gautam Rajendrakumar Gare, Jiayuan Li, Rohan Joshi, Mrunal Prashant Vaze, Rishikesh Magar, Michael Yousefpour, Ricardo Luis Rodriguez, John Micheal Galeotti
- **Comment**: The paper is currently under review for publication in a
  peer-reviewed journal
- **Journal**: None
- **Summary**: We present W-Net, a novel Convolution Neural Network (CNN) framework that employs raw ultrasound waveforms from each A-scan, typically referred to as ultrasound Radio Frequency (RF) data, in addition to the gray ultrasound image to semantically segment and label tissues. Unlike prior work, we seek to label every pixel in the image, without the use of a background class. To the best of our knowledge, this is also the first deep-learning or CNN approach for segmentation that analyses ultrasound raw RF data along with the gray image. International patent(s) pending [PCT/US20/37519]. We chose subcutaneous tissue (SubQ) segmentation as our initial clinical goal since it has diverse intermixed tissues, is challenging to segment, and is an underrepresented research area. SubQ potential applications include plastic surgery, adipose stem-cell harvesting, lymphatic monitoring, and possibly detection/treatment of certain types of tumors. A custom dataset consisting of hand-labeled images by an expert clinician and trainees are used for the experimentation, currently labeled into the following categories: skin, fat, fat fascia/stroma, muscle and muscle fascia. We compared our results with U-Net and Attention U-Net. Our novel \emph{W-Net}'s RF-Waveform input and architecture increased mIoU accuracy (averaged across all tissue classes) by 4.5\% and 4.9\% compared to regular U-Net and Attention U-Net, respectively. We present analysis as to why the Muscle fascia and Fat fascia/stroma are the most difficult tissues to label. Muscle fascia in particular, the most difficult anatomic class to recognize for both humans and AI algorithms, saw mIoU improvements of 13\% and 16\% from our W-Net vs U-Net and Attention U-Net respectively.



