# Arxiv Papers in cs.CV on 2020-08-08
### VPC-Net: Completion of 3D Vehicles from MLS Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.03404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03404v2)
- **Published**: 2020-08-08 00:22:43+00:00
- **Updated**: 2021-02-01 16:32:40+00:00
- **Authors**: Yan Xia, Yusheng Xu, Cheng Wang, Uwe Stilla
- **Comment**: accepted by ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: As a dynamic and essential component in the road environment of urban scenarios, vehicles are the most popular investigation targets. To monitor their behavior and extract their geometric characteristics, an accurate and instant measurement of vehicles plays a vital role in traffic and transportation fields. Point clouds acquired from the mobile laser scanning (MLS) system deliver 3D information of road scenes with unprecedented detail. They have proven to be an adequate data source in the fields of intelligent transportation and autonomous driving, especially for extracting vehicles. However, acquired 3D point clouds of vehicles from MLS systems are inevitably incomplete due to object occlusion or self-occlusion. To tackle this problem, we proposed a neural network to synthesize complete, dense, and uniform point clouds for vehicles from MLS data, named Vehicle Points Completion-Net (VPC-Net). In this network, we introduce a new encoder module to extract global features from the input instance, consisting of a spatial transformer network and point feature enhancement layer. Moreover, a new refiner module is also presented to preserve the vehicle details from inputs and refine the complete outputs with fine-grained information. Given sparse and partial point clouds as inputs, the network can generate complete and realistic vehicle structures and keep the fine-grained details from the partial inputs. We evaluated the proposed VPC-Net in different experiments using synthetic and real-scan datasets and applied the results to 3D vehicle monitoring tasks. Quantitative and qualitative experiments demonstrate the promising performance of the proposed VPC-Net and show state-of-the-art results.



### Exploring the parameter reusability of CNN
- **Arxiv ID**: http://arxiv.org/abs/2008.03411v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03411v2)
- **Published**: 2020-08-08 01:23:22+00:00
- **Updated**: 2020-09-18 04:23:25+00:00
- **Authors**: Wei Wang, Lin Cheng, Yanjie Zhu, Dong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent times, using small data to train networks has become a hot topic in the field of deep learning. Reusing pre-trained parameters is one of the most important strategies to address the issue of semi-supervised and transfer learning. However, the fundamental reason for the success of these methods is still unclear. In this paper, we propose a solution that can not only judge whether a given network is reusable or not based on the performance of reusing convolution kernels but also judge which layers' parameters of the given network can be reused, based on the performance of reusing corresponding parameters and, ultimately, judge whether those parameters are reusable or not in a target task based on the root mean square error (RMSE) of the corresponding convolution kernels. Specifically, we define that the success of a CNN's parameter reuse depends upon two conditions: first, the network is a reusable network; and second, the RMSE between the convolution kernels from the source domain and target domain is small enough. The experimental results demonstrate that the performance of reused parameters applied to target tasks, when these conditions are met, is significantly improved.



### Two-branch Recurrent Network for Isolating Deepfakes in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.03412v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03412v3)
- **Published**: 2020-08-08 01:38:56+00:00
- **Updated**: 2020-09-04 01:03:55+00:00
- **Authors**: Iacopo Masi, Aditya Killekar, Royston Marian Mascarenhas, Shenoy Pratik Gurudatt, Wael AbdAlmageed
- **Comment**: To appear in the 16th European Conference on Computer Vision ECCV
  2020 (added link to our demo and to the video presentation)
- **Journal**: None
- **Summary**: The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low false alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low false alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.



### Using UNet and PSPNet to explore the reusability principle of CNN parameters
- **Arxiv ID**: http://arxiv.org/abs/2008.03414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03414v1)
- **Published**: 2020-08-08 01:51:08+00:00
- **Updated**: 2020-08-08 01:51:08+00:00
- **Authors**: Wei Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2008.03411
- **Journal**: None
- **Summary**: How to reduce the requirement on training dataset size is a hot topic in deep learning community. One straightforward way is to reuse some pre-trained parameters. Some previous work like Deep transfer learning reuse the model parameters trained for the first task as the starting point for the second task, and semi-supervised learning is trained upon a combination of labeled and unlabeled data. However, the fundamental reason of the success of these methods is unclear. In this paper, the reusability of parameters in each layer of a deep convolutional neural network is experimentally quantified by using a network to do segmentation and auto-encoder task. This paper proves that network parameters can be reused for two reasons: first, the network features are general; Second, there is little difference between the pre-trained parameters and the ideal network parameters. Through the use of parameter replacement and comparison, we demonstrate that reusability is different in BN(Batch Normalization)[7] layer and Convolution layer and some observations: (1)Running mean and running variance plays an important role than Weight and Bias in BN layer.(2)The weight and bias can be reused in BN layers.( 3) The network is very sensitive to the weight of convolutional layer.(4) The bias in Convolution layers are not sensitive, and it can be reused directly.



### Recent Advances and New Guidelines on Hyperspectral and Multispectral Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2008.03426v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03426v1)
- **Published**: 2020-08-08 03:05:46+00:00
- **Updated**: 2020-08-08 03:05:46+00:00
- **Authors**: Renwei Dian, Shutao Li, Bin Sun, Anjing Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) with high spectral resolution often suffers from low spatial resolution owing to the limitations of imaging sensors. Image fusion is an effective and economical way to enhance the spatial resolution of HSI, which combines HSI with higher spatial resolution multispectral image (MSI) of the same scenario. In the past years, many HSI and MSI fusion algorithms are introduced to obtain high-resolution HSI. However, it lacks a full-scale review for the newly proposed HSI and MSI fusion approaches. To tackle this problem,this work gives a comprehensive review and new guidelines for HSI-MSI fusion. According to the characteristics of HSI-MSI fusion methods, they are categorized as four categories, including pan-sharpening based approaches, matrix factorization based approaches, tensor representation based approaches, and deep convolution neural network based approaches. We make a detailed introduction, discussions, and comparison for the fusion methods in each category. Additionally, the existing challenges and possible future directions for the HSI-MSI fusion are presented.



### Meta Feature Modulator for Long-tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.03428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03428v1)
- **Published**: 2020-08-08 03:19:03+00:00
- **Updated**: 2020-08-08 03:19:03+00:00
- **Authors**: Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks often degrade significantly when training data suffer from class imbalance problems. Existing approaches, e.g., re-sampling and re-weighting, commonly address this issue by rearranging the label distribution of training data to train the networks fitting well to the implicit balanced label distribution. However, most of them hinder the representative ability of learned features due to insufficient use of intra/inter-sample information of training data. To address this issue, we propose meta feature modulator (MFM), a meta-learning framework to model the difference between the long-tailed training data and the balanced meta data from the perspective of representation learning. Concretely, we employ learnable hyper-parameters (dubbed modulation parameters) to adaptively scale and shift the intermediate features of classification networks, and the modulation parameters are optimized together with the classification network parameters guided by a small amount of balanced meta data. We further design a modulator network to guide the generation of the modulation parameters, and such a meta-learner can be readily adapted to train the classification network on other long-tailed datasets. Extensive experiments on benchmark vision datasets substantiate the superiority of our approach on long-tailed recognition tasks beyond other state-of-the-art methods.



### Auto-weighting for Breast Cancer Classification in Multimodal Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2008.03435v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03435v1)
- **Published**: 2020-08-08 03:42:00+00:00
- **Updated**: 2020-08-08 03:42:00+00:00
- **Authors**: Wang Jian, Miao Juzheng, Yang Xin, Li Rui, Zhou Guangquan, Huang Yuhao, Lin Zehui, Xue Wufeng, Jia Xiaohong, Zhou Jianqiao, Huang Ruobing, Ni Dong
- **Comment**: Early Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Breast cancer is the most common invasive cancer in women. Besides the primary B-mode ultrasound screening, sonographers have explored the inclusion of Doppler, strain and shear-wave elasticity imaging to advance the diagnosis. However, recognizing useful patterns in all types of images and weighing up the significance of each modality can elude less-experienced clinicians. In this paper, we explore, for the first time, an automatic way to combine the four types of ultrasonography to discriminate between benign and malignant breast nodules. A novel multimodal network is proposed, along with promising learnability and simplicity to improve classification accuracy. The key is using a weight-sharing strategy to encourage interactions between modalities and adopting an additional cross-modalities objective to integrate global information. In contrast to hardcoding the weights of each modality in the model, we embed it in a Reinforcement Learning framework to learn this weighting in an end-to-end manner. Thus the model is trained to seek the optimal multimodal combination without handcrafted heuristics. The proposed framework is evaluated on a dataset contains 1616 set of multimodal images. Results showed that the model scored a high classification accuracy of 95.4%, which indicates the efficiency of the proposed method.



### Dimensionality Reduction via Diffusion Map Improved with Supervised Linear Projection
- **Arxiv ID**: http://arxiv.org/abs/2008.03440v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.03440v1)
- **Published**: 2020-08-08 04:26:07+00:00
- **Updated**: 2020-08-08 04:26:07+00:00
- **Authors**: Bowen Jiang, Maohao Shen
- **Comment**: This paper is accepted to be published in the 27th IEEE International
  Conference on Image Processing (ICIP 2020). Two authors contribute equally to
  the work
- **Journal**: None
- **Summary**: When performing classification tasks, raw high dimensional features often contain redundant information, and lead to increased computational complexity and overfitting. In this paper, we assume the data samples lie on a single underlying smooth manifold, and define intra-class and inter-class similarities using pairwise local kernel distances. We aim to find a linear projection to maximize the intra-class similarities and minimize the inter-class similarities simultaneously, so that the projected low dimensional data has optimized pairwise distances based on the label information, which is more suitable for a Diffusion Map to do further dimensionality reduction. Numerical experiments on several benchmark datasets show that our proposed approaches are able to extract low dimensional discriminate features that could help us achieve higher classification accuracy.



### Hard Class Rectification for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.03455v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03455v2)
- **Published**: 2020-08-08 06:21:58+00:00
- **Updated**: 2021-04-20 00:03:55+00:00
- **Authors**: Yunlong Zhang, Changxing Jing, Huangxing Lin, Chaoqi Chen, Yue Huang, Xinghao Ding, Yang Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to transfer knowledge from a label-rich and related domain (source domain) to a label-scare domain (target domain). Pseudo-labeling has recently been widely explored and used in DA. However, this line of research is still confined to the inaccuracy of pseudo-labels. In this paper, we reveal an interesting observation that the target samples belonging to the classes with larger domain shift are easier to be misclassified compared with the other classes. These classes are called hard class, which deteriorates the performance of DA and restricts the applications of DA. We propose a novel framework, called Hard Class Rectification Pseudo-labeling (HCRPL), to alleviate the hard class problem from two aspects. First, as is difficult to identify the target samples as hard class, we propose a simple yet effective scheme, named Adaptive Prediction Calibration (APC), to calibrate the predictions of the target samples according to the difficulty degree for each class. Second, we further consider that the predictions of target samples belonging to the hard class are vulnerable to perturbations. To prevent these samples to be misclassified easily, we introduce Temporal-Ensembling (TE) and Self-Ensembling (SE) to obtain consistent predictions. The proposed method is evaluated in both unsupervised domain adaptation (UDA) and semi-supervised domain adaptation (SSDA). The experimental results on several real-world cross-domain benchmarks, including ImageCLEF, Office-31 and Office-Home, substantiates the superiority of the proposed method.



### PAN: Towards Fast Action Recognition via Learning Persistence of Appearance
- **Arxiv ID**: http://arxiv.org/abs/2008.03462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03462v1)
- **Published**: 2020-08-08 07:09:54+00:00
- **Updated**: 2020-08-08 07:09:54+00:00
- **Authors**: Can Zhang, Yuexian Zou, Guang Chen, Lei Gan
- **Comment**: submitted to TIP
- **Journal**: None
- **Summary**: Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.



### Automated Claustrum Segmentation in Human Brain MRI Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.03465v3
- **DOI**: 10.1002/hbm.25655
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03465v3)
- **Published**: 2020-08-08 07:25:48+00:00
- **Updated**: 2023-05-23 12:11:00+00:00
- **Authors**: Hongwei Li, Aurore Menegaux, Benita Schmitz-Koep, Antonia Neubauer, Felix JB Bäuerlein, Suprosanna Shit, Christian Sorg, Bjoern Menze, Dennis Hedderich
- **Comment**: final version
- **Journal**: None
- **Summary**: In the last two decades, neuroscience has produced intriguing evidence for a central role of the claustrum in mammalian forebrain structure and function. However, relatively few in vivo studies of the claustrum exist in humans. A reason for this may be the delicate and sheet-like structure of the claustrum lying between the insular cortex and the putamen, which makes it not amenable to conventional segmentation methods. Recently, Deep Learning (DL) based approaches have been successfully introduced for automated segmentation of complex, subcortical brain structures. In the following, we present a multi-view DL-based approach to segment the claustrum in T1-weighted MRI scans. We trained and evaluated the proposed method in 181 individuals, using bilateral manual claustrum annotations by an expert neuroradiologist as the reference standard. Cross-validation experiments yielded median volumetric similarity, robust Hausdorff distance, and Dice score of 93.3%, 1.41mm, and 71.8%, respectively, representing equal or superior segmentation performance compared to human intra-rater reliability. The leave-one-scanner-out evaluation showed good transferability of the algorithm to images from unseen scanners at slightly inferior performance. Furthermore, we found that DL-based claustrum segmentation benefits from multi-view information and requires a sample size of around 75 MRI scans in the training set. We conclude that the developed algorithm allows for robust automated claustrum segmentation and thus yields considerable potential for facilitating MRI-based research of the human claustrum. The software and models of our method are made publicly available.



### RPT: Learning Point Set Representation for Siamese Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.03467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03467v2)
- **Published**: 2020-08-08 07:42:58+00:00
- **Updated**: 2020-09-02 01:27:02+00:00
- **Authors**: Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu, Jun Yin
- **Comment**: Accepted to ECCV2020 Workshop
- **Journal**: None
- **Summary**: While remarkable progress has been made in robust visual tracking, accurate target state estimation still remains a highly challenging problem. In this paper, we argue that this issue is closely related to the prevalent bounding box representation, which provides only a coarse spatial extent of object. Thus an effcient visual tracking framework is proposed to accurately estimate the target state with a finer representation as a set of representative points. The point set is trained to indicate the semantically and geometrically significant positions of target region, enabling more fine-grained localization and modeling of object appearance. We further propose a multi-level aggregation strategy to obtain detailed structure information by fusing hierarchical convolution layers. Extensive experiments on several challenging benchmarks including OTB2015, VOT2018, VOT2019 and GOT-10k demonstrate that our method achieves new state-of-the-art performance while running at over 20 FPS.



### Visual Pattern Recognition with on On-chip Learning: towards a Fully Neuromorphic Approach
- **Arxiv ID**: http://arxiv.org/abs/2008.03470v1
- **DOI**: 10.1109/ISCAS45731.2020.9180628
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03470v1)
- **Published**: 2020-08-08 08:07:36+00:00
- **Updated**: 2020-08-08 08:07:36+00:00
- **Authors**: Sandro Baumgartner, Alpha Renner, Raphaela Kreiser, Dongchen Liang, Giacomo Indiveri, Yulia Sandamirskaya
- **Comment**: 5 pages. Accepted to ISCAS 2020 conference
- **Journal**: None
- **Summary**: We present a spiking neural network (SNN) for visual pattern recognition with on-chip learning on neuromorphichardware. We show how this network can learn simple visual patterns composed of horizontal and vertical bars sensed by a Dynamic Vision Sensor, using a local spike-based plasticity rule. During recognition, the network classifies the pattern's identity while at the same time estimating its location and scale. We build on previous work that used learning with neuromorphic hardware in the loop and demonstrate that the proposed network can properly operate with on-chip learning, demonstrating a complete neuromorphic pattern learning and recognition setup. Our results show that the network is robust against noise on the input (no accuracy drop when adding 130% noise) and against up to 20% noise in the neuron parameters.



### Representation Learning via Cauchy Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2008.03473v1
- **DOI**: 10.1109/ACCESS.2021.3096643
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03473v1)
- **Published**: 2020-08-08 08:21:44+00:00
- **Updated**: 2020-08-08 08:21:44+00:00
- **Authors**: Perla Mayo, Oktay Karakuş, Robin Holmes, Alin Achim
- **Comment**: 19 pages, 9 figures, journal draft
- **Journal**: None
- **Summary**: In representation learning, Convolutional Sparse Coding (CSC) enables unsupervised learning of features by jointly optimising both an \(\ell_2\)-norm fidelity term and a sparsity enforcing penalty. This work investigates using a regularisation term derived from an assumed Cauchy prior for the coefficients of the feature maps of a CSC generative model. The sparsity penalty term resulting from this prior is solved via its proximal operator, which is then applied iteratively, element-wise, on the coefficients of the feature maps to optimise the CSC cost function. The performance of the proposed Iterative Cauchy Thresholding (ICT) algorithm in reconstructing natural images is compared against the common choice of \(\ell_1\)-norm optimised via soft and hard thresholding. ICT outperforms IHT and IST in most of these reconstruction experiments across various datasets, with an average PSNR of up to 11.30 and 7.04 above ISTA and IHT respectively.



### Hierarchical Bi-Directional Feature Perception Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.03509v1
- **DOI**: 10.1145/3394171.3413689
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03509v1)
- **Published**: 2020-08-08 12:33:32+00:00
- **Updated**: 2020-08-08 12:33:32+00:00
- **Authors**: Zhipu Liu, Lei Zhang, Yang Yang
- **Comment**: Accepted by ACM MM2020
- **Journal**: None
- **Summary**: Previous Person Re-Identification (Re-ID) models aim to focus on the most discriminative region of an image, while its performance may be compromised when that region is missing caused by camera viewpoint changes or occlusion. To solve this issue, we propose a novel model named Hierarchical Bi-directional Feature Perception Network (HBFP-Net) to correlate multi-level information and reinforce each other. First, the correlation maps of cross-level feature-pairs are modeled via low-rank bilinear pooling. Then, based on the correlation maps, Bi-directional Feature Perception (BFP) module is employed to enrich the attention regions of high-level feature, and to learn abstract and specific information in low-level feature. And then, we propose a novel end-to-end hierarchical network which integrates multi-level augmented features and inputs the augmented low- and middle-level features to following layers to retrain a new powerful network. What's more, we propose a novel trainable generalized pooling, which can dynamically select any value of all locations in feature maps to be activated. Extensive experiments implemented on the mainstream evaluation datasets including Market-1501, CUHK03 and DukeMTMC-ReID show that our method outperforms the recent SOTA Re-ID models.



### Single-Shot Two-Pronged Detector with Rectified IoU Loss
- **Arxiv ID**: http://arxiv.org/abs/2008.03511v1
- **DOI**: 10.1145/3394171.3413691
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03511v1)
- **Published**: 2020-08-08 12:36:55+00:00
- **Updated**: 2020-08-08 12:36:55+00:00
- **Authors**: Keyang Wang, Lei Zhang
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: In the CNN based object detectors, feature pyramids are widely exploited to alleviate the problem of scale variation across object instances. These object detectors, which strengthen features via a top-down pathway and lateral connections, are mainly to enrich the semantic information of low-level features, but ignore the enhancement of high-level features. This can lead to an imbalance between different levels of features, in particular a serious lack of detailed information in the high-level features, which makes it difficult to get accurate bounding boxes. In this paper, we introduce a novel two-pronged transductive idea to explore the relationship among different layers in both backward and forward directions, which can enrich the semantic information of low-level features and detailed information of high-level features at the same time. Under the guidance of the two-pronged idea, we propose a Two-Pronged Network (TPNet) to achieve bidirectional transfer between high-level features and low-level features, which is useful for accurately detecting object at different scales. Furthermore, due to the distribution imbalance between the hard and easy samples in single-stage detectors, the gradient of localization loss is always dominated by the hard examples that have poor localization accuracy. This will enable the model to be biased toward the hard samples. So in our TPNet, an adaptive IoU based localization loss, named Rectified IoU (RIoU) loss, is proposed to rectify the gradients of each kind of samples. The Rectified IoU loss increases the gradients of examples with high IoU while suppressing the gradients of examples with low IoU, which can improve the overall localization accuracy of model. Extensive experiments demonstrate the superiority of our TPNet and RIoU loss.



### Hard Negative Samples Emphasis Tracker without Anchors
- **Arxiv ID**: http://arxiv.org/abs/2008.03512v1
- **DOI**: 10.1145/3394171.3413692
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03512v1)
- **Published**: 2020-08-08 12:38:38+00:00
- **Updated**: 2020-08-08 12:38:38+00:00
- **Authors**: Zhongzhou Zhang, Lei Zhang
- **Comment**: accepted by ACM Mutlimedia Conference, 2020
- **Journal**: None
- **Summary**: Trackers based on Siamese network have shown tremendous success, because of their balance between accuracy and speed. Nevertheless, with tracking scenarios becoming more and more sophisticated, most existing Siamese-based approaches ignore the addressing of the problem that distinguishes the tracking target from hard negative samples in the tracking phase. The features learned by these networks lack of discrimination, which significantly weakens the robustness of Siamese-based trackers and leads to suboptimal performance. To address this issue, we propose a simple yet efficient hard negative samples emphasis method, which constrains Siamese network to learn features that are aware of hard negative samples and enhance the discrimination of embedding features. Through a distance constraint, we force to shorten the distance between exemplar vector and positive vectors, meanwhile, enlarge the distance between exemplar vector and hard negative vectors. Furthermore, we explore a novel anchor-free tracking framework in a per-pixel prediction fashion, which can significantly reduce the number of hyper-parameters and simplify the tracking process by taking full advantage of the representation of convolutional neural network. Extensive experiments on six standard benchmark datasets demonstrate that the proposed method can perform favorable results against state-of-the-art approaches.



### NASB: Neural Architecture Search for Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.03515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03515v1)
- **Published**: 2020-08-08 13:06:11+00:00
- **Updated**: 2020-08-08 13:06:11+00:00
- **Authors**: Baozhou Zhu, Zaid Al-Ars, Peter Hofstee
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Convolutional Neural Networks (CNNs) have significantly reduced the number of arithmetic operations and the size of memory storage needed for CNNs, which makes their deployment on mobile and embedded systems more feasible. However, the CNN architecture after binarizing requires to be redesigned and refined significantly due to two reasons: 1. the large accumulation error of binarization in the forward propagation, and 2. the severe gradient mismatch problem of binarization in the backward propagation. Even though the substantial effort has been invested in designing architectures for single and multiple binary CNNs, it is still difficult to find an optimal architecture for binary CNNs. In this paper, we propose a strategy, named NASB, which adopts Neural Architecture Search (NAS) to find an optimal architecture for the binarization of CNNs. Due to the flexibility of this automated strategy, the obtained architecture is not only suitable for binarization but also has low overhead, achieving a better trade-off between the accuracy and computational complexity of hand-optimized binary CNNs. The implementation of NASB strategy is evaluated on the ImageNet dataset and demonstrated as a better solution compared to existing quantized CNNs. With the insignificant overhead increase, NASB outperforms existing single and multiple binary CNNs by up to 4.0% and 1.0% Top-1 accuracy respectively, bringing them closer to the precision of their full precision counterpart. The code and pretrained models will be publicly available.



### Towards Lossless Binary Convolutional Neural Networks Using Piecewise Approximation
- **Arxiv ID**: http://arxiv.org/abs/2008.03520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03520v2)
- **Published**: 2020-08-08 13:32:33+00:00
- **Updated**: 2020-08-29 19:06:19+00:00
- **Authors**: Baozhou Zhu, Zaid Al-Ars, Wei Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Convolutional Neural Networks (CNNs) can significantly reduce the number of arithmetic operations and the size of memory storage, which makes the deployment of CNNs on mobile or embedded systems more promising. However, the accuracy degradation of single and multiple binary CNNs is unacceptable for modern architectures and large scale datasets like ImageNet. In this paper, we proposed a Piecewise Approximation (PA) scheme for multiple binary CNNs which lessens accuracy loss by approximating full precision weights and activations efficiently and maintains parallelism of bitwise operations to guarantee efficiency. Unlike previous approaches, the proposed PA scheme segments piece-wisely the full precision weights and activations, and approximates each piece with a scaling coefficient. Our implementation on ResNet with different depths on ImageNet can reduce both Top-1 and Top-5 classification accuracy gap compared with full precision to approximately 1.0%. Benefited from the binarization of the downsampling layer, our proposed PA-ResNet50 requires less memory usage and two times Flops than single binary CNNs with 4 weights and 5 activations bases. The PA scheme can also generalize to other architectures like DenseNet and MobileNet with similar approximation power as ResNet which is promising for other tasks using binary convolutions. The code and pretrained models will be publicly available.



### Unravelling Small Sample Size Problems in the Deep Learning World
- **Arxiv ID**: http://arxiv.org/abs/2008.03522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03522v1)
- **Published**: 2020-08-08 13:35:49+00:00
- **Updated**: 2020-08-08 13:35:49+00:00
- **Authors**: Rohit Keshari, Soumyadeep Ghosh, Saheb Chhabra, Mayank Vatsa, Richa Singh
- **Comment**: 3 figures, 2 tables, accepted in BigMM 2020
- **Journal**: None
- **Summary**: The growth and success of deep learning approaches can be attributed to two major factors: availability of hardware resources and availability of large number of training samples. For problems with large training databases, deep learning models have achieved superlative performances. However, there are a lot of \textit{small sample size or $S^3$} problems for which it is not feasible to collect large training databases. It has been observed that deep learning models do not generalize well on $S^3$ problems and specialized solutions are required. In this paper, we first present a review of deep learning algorithms for small sample size problems in which the algorithms are segregated according to the space in which they operate, i.e. input space, model space, and feature space. Secondly, we present Dynamic Attention Pooling approach which focuses on extracting global information from the most discriminative sub-part of the feature map. The performance of the proposed dynamic attention pooling is analyzed with state-of-the-art ResNet model on relatively small publicly available datasets such as SVHN, C10, C100, and TinyImageNet.



### Multimodal Image-to-Image Translation via Mutual Information Estimation and Maximization
- **Arxiv ID**: http://arxiv.org/abs/2008.03529v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.03529v7)
- **Published**: 2020-08-08 14:09:23+00:00
- **Updated**: 2021-05-08 14:15:56+00:00
- **Authors**: Zhiwen Zuo, Lei Zhao, Zhizhong Wang, Haibo Chen, Ailin Li, Qijiang Xu, Wei Xing, Dongming Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal image-to-image translation (I2IT) aims to learn a conditional distribution that explores multiple possible images in the target domain given an input image in the source domain. Conditional generative adversarial networks (cGANs) are often adopted for modeling such a conditional distribution. However, cGANs are prone to ignore the latent code and learn a unimodal distribution in conditional image synthesis, which is also known as the mode collapse issue of GANs. To solve the problem, we propose a simple yet effective method that explicitly estimates and maximizes the mutual information between the latent code and the output image in cGANs by using a deep mutual information neural estimator in this paper. Maximizing the mutual information strengthens the statistical dependency between the latent code and the output image, which prevents the generator from ignoring the latent code and encourages cGANs to fully utilize the latent code for synthesizing diverse results. Our method not only provides a new perspective from information theory to improve diversity for I2IT but also achieves disentanglement between the source domain content and the target domain style for free.



### How Trustworthy are Performance Evaluations for Basic Vision Tasks?
- **Arxiv ID**: http://arxiv.org/abs/2008.03533v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03533v4)
- **Published**: 2020-08-08 14:21:15+00:00
- **Updated**: 2022-07-22 09:48:22+00:00
- **Authors**: Tran Thien Dat Nguyen, Hamid Rezatofighi, Ba-Ngu Vo, Ba-Tuong Vo, Silvio Savarese, Ian Reid
- **Comment**: Tran Thien Dat Nguyen and Hamid Rezatofighi have contributed equally
- **Journal**: None
- **Summary**: This paper examines performance evaluation criteria for basic vision tasks involving sets of objects namely, object detection, instance-level segmentation and multi-object tracking. The rankings of algorithms by an existing criterion can fluctuate with different choices of parameters, e.g. Intersection over Union (IoU) threshold, making their evaluations unreliable. More importantly, there is no means to verify whether we can trust the evaluations of a criterion. This work suggests a notion of trustworthiness for performance criteria, which requires (i) robustness to parameters for reliability, (ii) contextual meaningfulness in sanity tests, and (iii) consistency with mathematical requirements such as the metric properties. We observe that these requirements were overlooked by many widely-used criteria, and explore alternative criteria using metrics for sets of shapes. We also assess all these criteria based on the suggested requirements for trustworthiness.



### HASeparator: Hyperplane-Assisted Softmax
- **Arxiv ID**: http://arxiv.org/abs/2008.03539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5; I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.03539v1)
- **Published**: 2020-08-08 15:24:56+00:00
- **Updated**: 2020-08-08 15:24:56+00:00
- **Authors**: Ioannis Kansizoglou, Nicholas Santavas, Loukas Bampis, Antonios Gasteratos
- **Comment**: Submitted to IEEE ICMLA 2020
- **Journal**: None
- **Summary**: Efficient feature learning with Convolutional Neural Networks (CNNs) constitutes an increasingly imperative property since several challenging tasks of computer vision tend to require cascade schemes and modalities fusion. Feature learning aims at CNN models capable of extracting embeddings, exhibiting high discrimination among the different classes, as well as intra-class compactness. In this paper, a novel approach is introduced that has separator, which focuses on an effective hyperplane-based segregation of the classes instead of the common class centers separation scheme. Accordingly, an innovatory separator, namely the Hyperplane-Assisted Softmax separator (HASeparator), is proposed that demonstrates superior discrimination capabilities, as evaluated on popular image classification benchmarks.



### Online Multi-modal Person Search in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.03546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03546v1)
- **Published**: 2020-08-08 15:48:32+00:00
- **Updated**: 2020-08-08 15:48:32+00:00
- **Authors**: Jiangyue Xia, Anyi Rao, Qingqiu Huang, Linning Xu, Jiangtao Wen, Dahua Lin
- **Comment**: ECCV2020. Project page:
  http://movienet.site/projects/eccv20onlineperson.html
- **Journal**: None
- **Summary**: The task of searching certain people in videos has seen increasing potential in real-world applications, such as video organization and editing. Most existing approaches are devised to work in an offline manner, where identities can only be inferred after an entire video is examined. This working manner precludes such methods from being applied to online services or those applications that require real-time responses. In this paper, we propose an online person search framework, which can recognize people in a video on the fly. This framework maintains a multimodal memory bank at its heart as the basis for person recognition, and updates it dynamically with a policy obtained by reinforcement learning. Our experiments on a large movie dataset show that the proposed method is effective, not only achieving remarkable improvements over online schemes but also outperforming offline methods.



### A Unified Framework for Shot Type Classification Based on Subject Centric Lens
- **Arxiv ID**: http://arxiv.org/abs/2008.03548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03548v1)
- **Published**: 2020-08-08 15:49:40+00:00
- **Updated**: 2020-08-08 15:49:40+00:00
- **Authors**: Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, Dahua Lin
- **Comment**: ECCV2020. Project page: https://anyirao.com/projects/ShotType.html
- **Journal**: None
- **Summary**: Shots are key narrative elements of various videos, e.g. movies, TV series, and user-generated videos that are thriving over the Internet. The types of shots greatly influence how the underlying ideas, emotions, and messages are expressed. The technique to analyze shot types is important to the understanding of videos, which has seen increasing demand in real-world applications in this era. Classifying shot type is challenging due to the additional information required beyond the video content, such as the spatial composition of a frame and camera movement. To address these issues, we propose a learning framework Subject Guidance Network (SGNet) for shot type recognition. SGNet separates the subject and background of a shot into two streams, serving as separate guidance maps for scale and movement type classification respectively. To facilitate shot type analysis and model evaluations, we build a large-scale dataset MovieShots, which contains 46K shots from 7K movie trailers with annotations of their scale and movement types. Experiments show that our framework is able to recognize these two attributes of shot accurately, outperforming all the previous methods.



### Learning CNN filters from user-drawn image markers for coconut-tree image classification
- **Arxiv ID**: http://arxiv.org/abs/2008.03549v2
- **DOI**: 10.1109/LGRS.2020.3020098
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03549v2)
- **Published**: 2020-08-08 15:50:23+00:00
- **Updated**: 2020-08-27 23:02:43+00:00
- **Authors**: Italos Estilon de Souza, Alexandre Xavier Falcão
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying species of trees in aerial images is essential for land-use classification, plantation monitoring, and impact assessment of natural disasters. The manual identification of trees in aerial images is tedious, costly, and error-prone, so automatic classification methods are necessary. Convolutional Neural Network (CNN) models have well succeeded in image classification applications from different domains. However, CNN models usually require intensive manual annotation to create large training sets. One may conceptually divide a CNN into convolutional layers for feature extraction and fully connected layers for feature space reduction and classification. We present a method that needs a minimal set of user-selected images to train the CNN's feature extractor, reducing the number of required images to train the fully connected layers. The method learns the filters of each convolutional layer from user-drawn markers in image regions that discriminate classes, allowing better user control and understanding of the training process. It does not rely on optimization based on backpropagation, and we demonstrate its advantages on the binary classification of coconut-tree aerial images against one of the most popular CNN models.



### Forming Local Intersections of Projections for Classifying and Searching Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2008.03553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03553v1)
- **Published**: 2020-08-08 16:32:04+00:00
- **Updated**: 2020-08-08 16:32:04+00:00
- **Authors**: Aditya Sriram, Shivam Kalra, Morteza Babaie, Brady Kieffer, Waddah Al Drobi, Shahryar Rahnamayan, Hany Kashani, Hamid R. Tizhoosh
- **Comment**: To appear in International Conference on AI in Medicine (AIME 2020)
- **Journal**: None
- **Summary**: In this paper, we propose a novel image descriptor called Forming Local Intersections of Projections (FLIP) and its multi-resolution version (mFLIP) for representing histopathology images. The descriptor is based on the Radon transform wherein we apply parallel projections in small local neighborhoods of gray-level images. Using equidistant projection directions in each window, we extract unique and invariant characteristics of the neighborhood by taking the intersection of adjacent projections. Thereafter, we construct a histogram for each image, which we call the FLIP histogram. Various resolutions provide different FLIP histograms which are then concatenated to form the mFLIP descriptor. Our experiments included training common networks from scratch and fine-tuning pre-trained networks to benchmark our proposed descriptor. Experiments are conducted on the publicly available dataset KIMIA Path24 and KIMIA Path960. For both of these datasets, FLIP and mFLIP descriptors show promising results in all experiments.Using KIMIA Path24 data, FLIP outperformed non-fine-tuned Inception-v3 and fine-tuned VGG16 and mFLIP outperformed fine-tuned Inception-v3 in feature extracting.



### Assisting Scene Graph Generation with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2008.03555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03555v1)
- **Published**: 2020-08-08 16:38:03+00:00
- **Updated**: 2020-08-08 16:38:03+00:00
- **Authors**: Sandeep Inuganti, Vineeth N Balasubramanian
- **Comment**: None
- **Journal**: None
- **Summary**: Research in scene graph generation has quickly gained traction in the past few years because of its potential to help in downstream tasks like visual question answering, image captioning, etc. Many interesting approaches have been proposed to tackle this problem. Most of these works have a pre-trained object detection model as a preliminary feature extractor. Therefore, getting object bounding box proposals from the object detection model is relatively cheaper. We take advantage of this ready availability of bounding box annotations produced by the pre-trained detector. We propose a set of three novel yet simple self-supervision tasks and train them as auxiliary multi-tasks to the main model. While comparing, we train the base-model from scratch with these self-supervision tasks, we achieve state-of-the-art results in all the metrics and recall settings. We also resolve some of the confusion between two types of relationships: geometric and possessive, by training the model with the proposed self-supervision losses. We use the benchmark dataset, Visual Genome to conduct our experiments and show our results.



### LPMNet: Latent Part Modification and Generation for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.03560v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03560v3)
- **Published**: 2020-08-08 17:24:37+00:00
- **Updated**: 2021-02-25 15:44:08+00:00
- **Authors**: Cihan Öngün, Alptekin Temizel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on latent modification and generation of 3D point cloud object models with respect to their semantic parts. Different to the existing methods which use separate networks for part generation and assembly, we propose a single end-to-end Autoencoder model that can handle generation and modification of both semantic parts, and global shapes. The proposed method supports part exchange between 3D point cloud models and composition by different parts to form new models by directly editing latent representations. This holistic approach does not need part-based training to learn part representations and does not introduce any extra loss besides the standard reconstruction loss. The experiments demonstrate the robustness of the proposed method with different object categories and varying number of points. The method can generate new models by integration of generative models such as GANs and VAEs and can work with unannotated point clouds by integration of a segmentation module.



### Cross-modal Center Loss
- **Arxiv ID**: http://arxiv.org/abs/2008.03561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03561v1)
- **Published**: 2020-08-08 17:26:35+00:00
- **Updated**: 2020-08-08 17:26:35+00:00
- **Authors**: Longlong Jing, Elahe Vahdani, Jiaxing Tan, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal retrieval aims to learn discriminative and modal-invariant features for data from different modalities. Unlike the existing methods which usually learn from the features extracted by offline networks, in this paper, we propose an approach to jointly train the components of cross-modal retrieval framework with metadata, and enable the network to find optimal features. The proposed end-to-end framework is updated with three loss functions: 1) a novel cross-modal center loss to eliminate cross-modal discrepancy, 2) cross-entropy loss to maximize inter-class variations, and 3) mean-square-error loss to reduce modality variations. In particular, our proposed cross-modal center loss minimizes the distances of features from objects belonging to the same class across all modalities. Extensive experiments have been conducted on the retrieval tasks across multi-modalities, including 2D image, 3D point cloud, and mesh data. The proposed framework significantly outperforms the state-of-the-art methods on the ModelNet40 dataset.



### From Rain Generation to Rain Removal
- **Arxiv ID**: http://arxiv.org/abs/2008.03580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03580v2)
- **Published**: 2020-08-08 18:56:51+00:00
- **Updated**: 2020-12-04 06:57:25+00:00
- **Authors**: Hong Wang, Zongsheng Yue, Qi Xie, Qian Zhao, Yefeng Zheng, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: For the single image rain removal (SIRR) task, the performance of deep learning (DL)-based methods is mainly affected by the designed deraining models and training datasets. Most of current state-of-the-art focus on constructing powerful deep models to obtain better deraining results. In this paper, to further improve the deraining performance, we novelly attempt to handle the SIRR task from the perspective of training datasets by exploring a more efficient way to synthesize rainy images. Specifically, we build a full Bayesian generative model for rainy image where the rain layer is parameterized as a generator with the input as some latent variables representing the physical structural rain factors, e.g., direction, scale, and thickness. To solve this model, we employ the variational inference framework to approximate the expected statistical distribution of rainy image in a data-driven manner. With the learned generator, we can automatically and sufficiently generate diverse and non-repetitive training pairs so as to efficiently enrich and augment the existing benchmark datasets. User study qualitatively and quantitatively evaluates the realism of generated rainy images. Comprehensive experiments substantiate that the proposed model can faithfully extract the complex rain distribution that not only helps significantly improve the deraining performance of current deep single image derainers, but also largely loosens the requirement of large training sample pre-collection for the SIRR task.



### Speech Driven Talking Face Generation from a Single Image and an Emotion Condition
- **Arxiv ID**: http://arxiv.org/abs/2008.03592v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.03592v2)
- **Published**: 2020-08-08 20:46:31+00:00
- **Updated**: 2021-07-21 22:45:01+00:00
- **Authors**: Sefik Emre Eskimez, You Zhang, Zhiyao Duan
- **Comment**: Accepted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speech-driven talking face generation. Specifically, we design an end-to-end talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video synchronized with the speech and expressing the conditioned emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a state-of-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a human emotion recognition pilot study using generated videos with mismatched emotions among the audio and visual modalities. Results show that humans respond to the visual modality more significantly than the audio modality on this task.



### Enhance CNN Robustness Against Noises for Classification of 12-Lead ECG with Variable Length
- **Arxiv ID**: http://arxiv.org/abs/2008.03609v4
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03609v4)
- **Published**: 2020-08-08 22:21:24+00:00
- **Updated**: 2020-11-30 20:09:42+00:00
- **Authors**: Linhai Ma, Liang Liang
- **Comment**: This paper is accepted by 19TH IEEE International Conference on
  Machine Learning and Applications (ICMLA2020)
- **Journal**: None
- **Summary**: Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial noises which are subtle changes in input of a DNN and lead to a wrong class-label prediction with a high confidence. Thus, it is challenging and essential to improve robustness of DNNs against adversarial noises for ECG signal classification, a life-critical application. In this work, we designed a CNN for classification of 12-lead ECG signals with variable length, and we applied three defense methods to improve robustness of this CNN for this classification task. The ECG data in this study is very challenging because the sample size is limited, and the length of each ECG recording varies in a large range. The evaluation results show that our customized CNN reached satisfying F1 score and average accuracy, comparable to the top-6 entries in the CPSC2018 ECG classification challenge, and the defense methods enhanced robustness of our CNN against adversarial noises and white noises, with a minimal reduction in accuracy on clean data.



### Tracking in Crowd is Challenging: Analyzing Crowd based on Physical Characteristics
- **Arxiv ID**: http://arxiv.org/abs/2008.03614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03614v1)
- **Published**: 2020-08-08 22:42:25+00:00
- **Updated**: 2020-08-08 22:42:25+00:00
- **Authors**: Constantinou Miti, Demetriou Zatte, Siraj Sajid Gondal
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, the safety of people has become a very important problem in different places including subway station, universities, colleges, airport, shopping mall and square, city squares. Therefore, considering intelligence event detection systems is more and urgently required. The event detection method is developed to identify abnormal behavior intelligently, so public can take action as soon as possible to prevent unwanted activities. The problem is very challenging due to high crowd density in different areas. One of these issues is occlusion due to which individual tracking and analysis becomes impossible as shown in Fig. 1. Secondly, more challenging is the proper representation of individual behavior in the crowd. We consider a novel method to deal with these challenges. Considering the challenge of tracking, we partition complete frame into smaller patches, and extract motion pattern to demonstrate the motion in each individual patch. For this purpose, our work takes into account KLT corners as consolidated features to describe moving regions and track these features by considering optical flow method. To embed motion patterns, we develop and consider the distribution of all motion information in a patch as Gaussian distribution, and formulate parameters of Gaussian model as our motion pattern descriptor.



