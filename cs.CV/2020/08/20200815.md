# Arxiv Papers in cs.CV on 2020-08-15
### Object Detection with a Unified Label Space from Multiple Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.06614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06614v1)
- **Published**: 2020-08-15 00:51:27+00:00
- **Updated**: 2020-08-15 00:51:27+00:00
- **Authors**: Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan Chandraker, Ying Wu
- **Comment**: To appear in ECCV 2020, project page
  http://www.nec-labs.com/~mas/UniDet/
- **Journal**: None
- **Summary**: Given multiple datasets with different label spaces, the goal of this work is to train a single object detector predicting over the union of all the label spaces. The practical benefits of such an object detector are obvious and significant application-relevant categories can be picked and merged form arbitrary existing datasets. However, naive merging of datasets is not possible in this case, due to inconsistent object annotations. Consider an object category like faces that is annotated in one dataset, but is not annotated in another dataset, although the object itself appears in the latter images. Some categories, like face here, would thus be considered foreground in one dataset, but background in another. To address this challenge, we design a framework which works with such partial annotations, and we exploit a pseudo labeling approach that we adapt for our specific case. We propose loss functions that carefully integrate partial but correct annotations with complementary but noisy pseudo labels. Evaluation in the proposed novel setting requires full annotation on the test set. We collect the required annotations and define a new challenging experimental setup for this task based one existing public datasets. We show improved performances compared to competitive baselines and appropriate adaptations of existing work.



### Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion
- **Arxiv ID**: http://arxiv.org/abs/2008.06630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.06630v1)
- **Published**: 2020-08-15 02:29:13+00:00
- **Updated**: 2020-08-15 02:29:13+00:00
- **Authors**: Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Wolfram Burgard, Greg Shakhnarovich, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has emerged as a powerful tool for depth and ego-motion estimation, leading to state-of-the-art results on benchmark datasets. However, one significant limitation shared by current methods is the assumption of a known parametric camera model -- usually the standard pinhole geometry -- leading to failure when applied to imaging systems that deviate significantly from this assumption (e.g., catadioptric cameras or underwater imaging). In this work, we show that self-supervision can be used to learn accurate depth and ego-motion estimation without prior knowledge of the camera model. Inspired by the geometric model of Grossberg and Nayar, we introduce Neural Ray Surfaces (NRS), convolutional networks that represent pixel-wise projection rays, approximating a wide range of cameras. NRS are fully differentiable and can be learned end-to-end from unlabeled raw videos. We demonstrate the use of NRS for self-supervised learning of visual odometry and depth estimation from raw videos obtained using a wide variety of camera systems, including pinhole, fisheye, and catadioptric.



### Dehaze-GLCGAN: Unpaired Single Image De-hazing via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2008.06632v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06632v1)
- **Published**: 2020-08-15 02:43:00+00:00
- **Updated**: 2020-08-15 02:43:00+00:00
- **Authors**: Zahra Anvari, Vassilis Athitsos
- **Comment**: None
- **Journal**: None
- **Summary**: Single image de-hazing is a challenging problem, and it is far from solved. Most current solutions require paired image datasets that include both hazy images and their corresponding haze-free ground-truth images. However, in reality, lighting conditions and other factors can produce a range of haze-free images that can serve as ground truth for a hazy image, and a single ground truth image cannot capture that range. This limits the scalability and practicality of paired image datasets in real-world applications. In this paper, we focus on unpaired single image de-hazing and we do not rely on the ground truth image or physical scattering model. We reduce the image de-hazing problem to an image-to-image translation problem and propose a dehazing Global-Local Cycle-consistent Generative Adversarial Network (Dehaze-GLCGAN). Generator network of Dehaze-GLCGAN combines an encoder-decoder architecture with residual blocks to better recover the haze free scene. We also employ a global-local discriminator structure to deal with spatially varying haze. Through ablation study, we demonstrate the effectiveness of different factors in the performance of the proposed network. Our extensive experiments over three benchmark datasets show that our network outperforms previous work in terms of PSNR and SSIM while being trained on smaller amount of data compared to other methods.



### Cross-Modality Multi-Atlas Segmentation Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.08946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08946v1)
- **Published**: 2020-08-15 02:57:23+00:00
- **Updated**: 2020-08-15 02:57:23+00:00
- **Authors**: Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Both image registration and label fusion in the multi-atlas segmentation (MAS) rely on the intensity similarity between target and atlas images. However, such similarity can be problematic when target and atlas images are acquired using different imaging protocols. High-level structure information can provide reliable similarity measurement for cross-modality images when cooperating with deep neural networks (DNNs). This work presents a new MAS framework for cross-modality images, where both image registration and label fusion are achieved by DNNs. For image registration, we propose a consistent registration network, which can jointly estimate forward and backward dense displacement fields (DDFs). Additionally, an invertible constraint is employed in the network to reduce the correspondence ambiguity of the estimated DDFs. For label fusion, we adapt a few-shot learning network to measure the similarity of atlas and target patches. Moreover, the network can be seamlessly integrated into the patch-based label fusion. The proposed framework is evaluated on the MM-WHS dataset of MICCAI 2017. Results show that the framework is effective in both cross-modality registration and segmentation.



### Evolving Deep Convolutional Neural Networks for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2008.06634v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06634v1)
- **Published**: 2020-08-15 03:04:11+00:00
- **Updated**: 2020-08-15 03:04:11+00:00
- **Authors**: Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang
- **Comment**: 8 pages, 4 figures, to be published in IJCNN 2020
- **Journal**: None
- **Summary**: Hyperspectral images (HSIs) are susceptible to various noise factors leading to the loss of information, and the noise restricts the subsequent HSIs object detection and classification tasks. In recent years, learning-based methods have demonstrated their superior strengths in denoising the HSIs. Unfortunately, most of the methods are manually designed based on the extensive expertise that is not necessarily available to the users interested. In this paper, we propose a novel algorithm to automatically build an optimal Convolutional Neural Network (CNN) to effectively denoise HSIs. Particularly, the proposed algorithm focuses on the architectures and the initialization of the connection weights of the CNN. The experiments of the proposed algorithm have been well-designed and compared against the state-of-the-art peer competitors, and the experimental results demonstrate the competitive performance of the proposed algorithm in terms of the different evaluation metrics, visual assessments, and the computational complexity.



### Graph Edit Distance Reward: Learning to Edit Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/2008.06651v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.m I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2008.06651v1)
- **Published**: 2020-08-15 04:52:16+00:00
- **Updated**: 2020-08-15 04:52:16+00:00
- **Authors**: Lichang Chen, Guosheng Lin, Shijie Wang, Qingyao Wu
- **Comment**: 14 pages, 6 figures, ECCV camera ready version
- **Journal**: None
- **Summary**: Scene Graph, as a vital tool to bridge the gap between language domain and image domain, has been widely adopted in the cross-modality task like VQA. In this paper, we propose a new method to edit the scene graph according to the user instructions, which has never been explored. To be specific, in order to learn editing scene graphs as the semantics given by texts, we propose a Graph Edit Distance Reward, which is based on the Policy Gradient and Graph Matching algorithm, to optimize neural symbolic model. In the context of text-editing image retrieval, we validate the effectiveness of our method in CSS and CRIR dataset. Besides, CRIR is a new synthetic dataset generated by us, which we will publish it soon for future use.



### Object Detection in the Context of Mobile Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2008.06655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06655v1)
- **Published**: 2020-08-15 05:15:00+00:00
- **Updated**: 2020-08-15 05:15:00+00:00
- **Authors**: Xiang Li, Yuan Tian, Fuyao Zhang, Shuxue Quan, Yi Xu
- **Comment**: accepted to IEEE International Symposium on Mixed and Augmented
  Reality (ISMAR) 2020
- **Journal**: None
- **Summary**: In the past few years, numerous Deep Neural Network (DNN) models and frameworks have been developed to tackle the problem of real-time object detection from RGB images. Ordinary object detection approaches process information from the images only, and they are oblivious to the camera pose with regard to the environment and the scale of the environment. On the other hand, mobile Augmented Reality (AR) frameworks can continuously track a camera's pose within the scene and can estimate the correct scale of the environment by using Visual-Inertial Odometry (VIO). In this paper, we propose a novel approach that combines the geometric information from VIO with semantic information from object detectors to improve the performance of object detection on mobile devices. Our approach includes three components: (1) an image orientation correction method, (2) a scale-based filtering approach, and (3) an online semantic map. Each component takes advantage of the different characteristics of the VIO-based AR framework. We implemented the AR-enhanced features using ARCore and the SSD Mobilenet model on Android phones. To validate our approach, we manually labeled objects in image sequences taken from 12 room-scale AR sessions. The results show that our approach can improve on the accuracy of generic object detectors by 12% on our dataset.



### ECG beats classification via online sparse dictionary and time pyramid matching
- **Arxiv ID**: http://arxiv.org/abs/2008.06672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06672v1)
- **Published**: 2020-08-15 08:10:21+00:00
- **Updated**: 2020-08-15 08:10:21+00:00
- **Authors**: Nanyu Li, Yujuan Si, Duo Deng, Chunyu Yuan
- **Comment**: 7 pages,5 figure
- **Journal**: 17th IEEE International Conference on Communication
  Technology(ICCT 2017)
- **Summary**: Recently, the Bag-Of-Word (BOW) algorithm provides efficient features and promotes the accuracy of the ECG classification system. However, BOW algorithm has two shortcomings: (1). it has large quantization errors and poor reconstruction performance; (2). it loses heart beat's time information, and may provide confusing features for different kinds of heart beats. Furthermore, ECG classification system can be used for long time monitoring and analysis of cardiovascular patients, while a huge amount of data will be produced, so we urgently need an efficient compression algorithm. In view of the above problems, we use the wavelet feature to construct the sparse dictionary, which lower the quantization error to a minimum. In order to reduce the complexity of our algorithm and adapt to large-scale heart beats operation, we combine the Online Dictionary Learning with Feature-sign algorithm to update the dictionary and coefficients. Coefficients matrix is used to represent ECG beats, which greatly reduces the memory consumption, and solve the problem of quantitative error simultaneously. Finally, we construct the pyramid to match coefficients of each ECG beat. Thus, we obtain the features that contain the beat time information by time stochastic pooling. It is efficient to solve the problem of losing time information. The experimental results show that: on the one hand, the proposed algorithm has advantages of high reconstruction performance for BOW, this storage method is high fidelity and low memory consumption; on the other hand, our algorithm yields highest accuracy in ECG beats classification; so this method is more suitable for large-scale heart beats data storage and classification.



### BroadFace: Looking at Tens of Thousands of People at Once for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.06674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06674v1)
- **Published**: 2020-08-15 08:17:25+00:00
- **Updated**: 2020-08-15 08:17:25+00:00
- **Authors**: Yonghyun Kim, Wonpyo Park, Jongju Shin
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.



### Curriculum Learning for Recurrent Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06698v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2008.06698v1)
- **Published**: 2020-08-15 10:51:22+00:00
- **Updated**: 2020-08-15 10:51:22+00:00
- **Authors**: Maria Gonzalez-i-Calabuig, Carles Ventura, Xavier Giró-i-Nieto
- **Comment**: Extended abstract accepted at ECCV 2020 Women in Computer Vision
  (WiCV) & Perception for Autonomous Driving (PAD) Workshops
- **Journal**: None
- **Summary**: Video object segmentation can be understood as a sequence-to-sequence task that can benefit from the curriculum learning strategies for better and faster training of deep neural networks. This work explores different schedule sampling and frame skipping variations to significantly improve the performance of a recurrent architecture. Our results on the car class of the KITTI-MOTS challenge indicate that, surprisingly, an inverse schedule sampling is a better option than a classic forward one. Also, that a progressive skipping of frames during training is beneficial, but only when training with the ground truth masks instead of the predicted ones. Source code and trained models are available at http://imatge-upc.github.io/rvos-mots/.



### Single image dehazing for a variety of haze scenarios using back projected pyramid network
- **Arxiv ID**: http://arxiv.org/abs/2008.06713v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06713v1)
- **Published**: 2020-08-15 13:09:34+00:00
- **Updated**: 2020-08-15 13:09:34+00:00
- **Authors**: Ayush Singh, Ajay Bhave, Dilip K. Prasad
- **Comment**: 16 pages, 8 figures, to be published in Computer Vision ECCV 2020
  Workshops
- **Journal**: None
- **Summary**: Learning to dehaze single hazy images, especially using a small training dataset is quite challenging. We propose a novel generative adversarial network architecture for this problem, namely back projected pyramid network (BPPNet), that gives good performance for a variety of challenging haze conditions, including dense haze and inhomogeneous haze. Our architecture incorporates learning of multiple levels of complexities while retaining spatial context through iterative blocks of UNets and structural information of multiple scales through a novel pyramidal convolution block. These blocks together for the generator and are amenable to learning through back projection. We have shown that our network can be trained without over-fitting using as few as 20 image pairs of hazy and non-hazy images. We report the state of the art performances on NTIRE 2018 homogeneous haze datasets for indoor and outdoor images, NTIRE 2019 denseHaze dataset, and NTIRE 2020 non-homogeneous haze dataset.



### A Deep Convolutional Neural Network for the Detection of Polyps in Colonoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2008.06721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06721v1)
- **Published**: 2020-08-15 13:55:44+00:00
- **Updated**: 2020-08-15 13:55:44+00:00
- **Authors**: Tariq Rahim, Syed Ali Hassan, Soo Young Shin
- **Comment**: 21Pages,7 Figues,Journal
- **Journal**: None
- **Summary**: Computerized detection of colonic polyps remains an unsolved issue because of the wide variation in the appearance, texture, color, size, and presence of the multiple polyp-like imitators during colonoscopy. In this paper, we propose a deep convolutional neural network based model for the computerized detection of polyps within colonoscopy images. The proposed model comprises 16 convolutional layers with 2 fully connected layers, and a Softmax layer, where we implement a unique approach using different convolutional kernels within the same hidden layer for deeper feature extraction. We applied two different activation functions, MISH and rectified linear unit activation functions for deeper propagation of information and self regularized smooth non-monotonicity. Furthermore, we used a generalized intersection of union, thus overcoming issues such as scale invariance, rotation, and shape. Data augmentation techniques such as photometric and geometric distortions are adapted to overcome the obstacles faced in polyp detection. Detailed benchmarked results are provided, showing better performance in terms of precision, sensitivity, F1- score, F2- score, and dice-coefficient, thus proving the efficacy of the proposed model.



### New Normal: Cooperative Paradigm for Covid-19 Timely Detection and Containment using Internet of Things and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.12103v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12103v1)
- **Published**: 2020-08-15 14:33:53+00:00
- **Updated**: 2020-08-15 14:33:53+00:00
- **Authors**: Farooque Hassan Kumbhar, Syed Ali Hassan, Soo Young Shin
- **Comment**: None
- **Journal**: None
- **Summary**: The spread of the novel coronavirus (COVID-19) has caused trillions of dollars in damages to the governments and health authorities by affecting the global economies. The purpose of this study is to introduce a connected smart paradigm that not only detects the possible spread of viruses but also helps to restart businesses/economies, and resume social life. We are proposing a connected Internet of Things ( IoT) based paradigm that makes use of object detection based on convolution neural networks (CNN), smart wearable and connected e-health to avoid current and future outbreaks. First, connected surveillance cameras feed continuous video stream to the server where we detect the inter-object distance to identify any social distancing violations. A violation activates area-based monitoring of active smartphone users and their current state of illness. In case a confirmed patient or a person with high symptoms is present, the system tracks exposed and infected people and appropriate measures are put into actions. We evaluated the proposed scheme for social distancing violation detection using YOLO (you only look once) v2 and v3, and for infection spread tracing using Python simulation.



### Model Patching: Closing the Subgroup Performance Gap with Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06775v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06775v1)
- **Published**: 2020-08-15 20:01:23+00:00
- **Updated**: 2020-08-15 20:01:23+00:00
- **Authors**: Karan Goel, Albert Gu, Yixuan Li, Christopher Ré
- **Comment**: None
- **Journal**: None
- **Summary**: Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.



### Automated Detection of Cortical Lesions in Multiple Sclerosis Patients with 7T MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.06780v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06780v1)
- **Published**: 2020-08-15 20:35:12+00:00
- **Updated**: 2020-08-15 20:35:12+00:00
- **Authors**: Francesco La Rosa, Erin S Beck, Ahmed Abdulkadir, Jean-Philippe Thiran, Daniel S Reich, Pascal Sati, Meritxell Bach Cuadra
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: The automated detection of cortical lesions (CLs) in patients with multiple sclerosis (MS) is a challenging task that, despite its clinical relevance, has received very little attention. Accurate detection of the small and scarce lesions requires specialized sequences and high or ultra-high field MRI. For supervised training based on multimodal structural MRI at 7T, two experts generated ground truth segmentation masks of 60 patients with 2014 CLs. We implemented a simplified 3D U-Net with three resolution levels (3D U-Net-). By increasing the complexity of the task (adding brain tissue segmentation), while randomly dropping input channels during training, we improved the performance compared to the baseline. Considering a minimum lesion size of 0.75 {\mu}L, we achieved a lesion-wise cortical lesion detection rate of 67% and a false positive rate of 42%. However, 393 (24%) of the lesions reported as false positives were post-hoc confirmed as potential or definite lesions by an expert. This indicates the potential of the proposed method to support experts in the tedious process of CL manual segmentation.



### Cluster-level Feature Alignment for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2008.06810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06810v1)
- **Published**: 2020-08-15 23:47:47+00:00
- **Updated**: 2020-08-15 23:47:47+00:00
- **Authors**: Qiuyu Chen, Wei Zhang, Jianping Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Instance-level alignment is widely exploited for person re-identification, e.g. spatial alignment, latent semantic alignment and triplet alignment. This paper probes another feature alignment modality, namely cluster-level feature alignment across whole dataset, where the model can see not only the sampled images in local mini-batch but the global feature distribution of the whole dataset from distilled anchors. Towards this aim, we propose anchor loss and investigate many variants of cluster-level feature alignment, which consists of iterative aggregation and alignment from the overview of dataset. Our extensive experiments have demonstrated that our methods can provide consistent and significant performance improvement with small training efforts after the saturation of traditional training. In both theoretical and experimental aspects, our proposed methods can result in more stable and guided optimization towards better representation and generalization for well-aligned embedding.



