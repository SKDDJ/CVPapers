# Arxiv Papers in cs.CV on 2020-08-13
### Towards Modality Transferable Visual Information Representation with Optimal Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2008.05642v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05642v1)
- **Published**: 2020-08-13 01:52:40+00:00
- **Updated**: 2020-08-13 01:52:40+00:00
- **Authors**: Rongqun Lin, Linwei Zhu, Shiqi Wang, Sam Kwong
- **Comment**: Accepted in ACM Multimedia 2020
- **Journal**: None
- **Summary**: Compactly representing the visual signals is of fundamental importance in various image/video-centered applications. Although numerous approaches were developed for improving the image and video coding performance by removing the redundancies within visual signals, much less work has been dedicated to the transformation of the visual signals to another well-established modality for better representation capability. In this paper, we propose a new scheme for visual signal representation that leverages the philosophy of transferable modality. In particular, the deep learning model, which characterizes and absorbs the statistics of the input scene with online training, could be efficiently represented in the sense of rate-utility optimization to serve as the enhancement layer in the bitstream. As such, the overall performance can be further guaranteed by optimizing the new modality incorporated. The proposed framework is implemented on the state-of-the-art video coding standard (i.e., versatile video coding), and significantly better representation capability has been observed based on extensive evaluations.



### Few shot clustering for indoor occupancy detection with extremely low-quality images from battery free cameras
- **Arxiv ID**: http://arxiv.org/abs/2008.05654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05654v1)
- **Published**: 2020-08-13 02:47:01+00:00
- **Updated**: 2020-08-13 02:47:01+00:00
- **Authors**: Homagni Saha, Sin Yong Tan, Ali Saffari, Mohamad Katanbaf, Joshua R. Smith, Soumik Sarkar
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Reliable detection of human occupancy in indoor environments is critical for various energy efficiency, security, and safety applications. We consider this challenge of occupancy detection using extremely low-quality, privacy-preserving images from low power image sensors. We propose a combined few shot learning and clustering algorithm to address this challenge that has very low commissioning and maintenance cost. While the few shot learning concept enables us to commission our system with a few labeled examples, the clustering step serves the purpose of online adaptation to changing imaging environment over time. Apart from validating and comparing our algorithm on benchmark datasets, we also demonstrate performance of our algorithm on streaming images collected from real homes using our novel battery free camera hardware.



### ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2008.05655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05655v1)
- **Published**: 2020-08-13 02:48:27+00:00
- **Updated**: 2020-08-13 02:48:27+00:00
- **Authors**: Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, Xiaolin Wei, Shuqiang Jiang
- **Comment**: Accepted by ACM Multimedia 2020
- **Journal**: None
- **Summary**: Food recognition has received more and more attention in the multimedia community for its various real-world applications, such as diet management and self-service restaurants. A large-scale ontology of food images is urgently needed for developing advanced large-scale food recognition algorithms, as well as for providing the benchmark dataset for such algorithms. To encourage further progress in food recognition, we introduce the dataset ISIA Food- 500 with 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume. Furthermore, we propose a stacked global-local attention network, which consists of two sub-networks for food recognition. One subnetwork first utilizes hybrid spatial-channel attention to extract more discriminative features, and then aggregates these multi-scale discriminative features from multiple layers into global-level representation (e.g., texture and shape information about food). The other one generates attentional regions (e.g., ingredient relevant regions) from different regions via cascaded spatial transformers, and further aggregates these multi-scale regional features from different layers into local-level representation. These two types of features are finally fused as comprehensive representation for food recognition. Extensive experiments on ISIA Food-500 and other two popular benchmark datasets demonstrate the effectiveness of our proposed method, and thus can be considered as one strong baseline. The dataset, code and models can be found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.



### Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear Segmentation in Digital Pathology Images
- **Arxiv ID**: http://arxiv.org/abs/2008.05657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05657v1)
- **Published**: 2020-08-13 02:59:31+00:00
- **Updated**: 2020-08-13 02:59:31+00:00
- **Authors**: Jie Song, Liang Xiao, Mohsen Molaei, Zhichao Lian
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose an easily trained yet powerful representation learning approach with performance highly competitive to deep neural networks in a digital pathology image segmentation task. The method, called sparse coding driven deep decision tree ensembles that we abbreviate as ScD2TE, provides a new perspective on representation learning. We explore the possibility of stacking several layers based on non-differentiable pairwise modules and generate a densely concatenated architecture holding the characteristics of feature map reuse and end-to-end dense learning. Under this architecture, fast convolutional sparse coding is used to extract multi-level features from the output of each layer. In this way, rich image appearance models together with more contextual information are integrated by learning a series of decision tree ensembles. The appearance and the high-level context features of all the previous layers are seamlessly combined by concatenating them to feed-forward as input, which in turn makes the outputs of subsequent layers more accurate and the whole model efficient to train. Compared with deep neural networks, our proposed ScD2TE does not require back-propagation computation and depends on less hyper-parameters. ScD2TE is able to achieve a fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the superiority of our segmentation technique by evaluating it on the multi-disease state and multi-organ dataset where consistently higher performances were obtained for comparison against several state-of-the-art deep learning methods such as convolutional neural networks (CNN), fully convolutional networks (FCN), etc.



### What Should Not Be Contrastive in Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05659v2)
- **Published**: 2020-08-13 03:02:32+00:00
- **Updated**: 2021-03-18 21:08:52+00:00
- **Authors**: Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell
- **Comment**: Published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions.



### Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2008.05667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05667v1)
- **Published**: 2020-08-13 03:20:01+00:00
- **Updated**: 2020-08-13 03:20:01+00:00
- **Authors**: Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B. Bruce
- **Comment**: Accepted to BMVC 2020 (Oral)
- **Journal**: None
- **Summary**: In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. The premise is based on the notion of feature binding, which is defined as the process by which activation's spread across space and layers in the network are successfully integrated to arrive at a correct inference decision. In our work, this is accomplished for the task of dense image labelling by blending images based on their class labels, and then training a feature binding network, which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation network while simultaneously increasing robustness to adversarial attacks.



### Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.05676v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05676v2)
- **Published**: 2020-08-13 03:52:37+00:00
- **Updated**: 2021-03-03 04:51:39+00:00
- **Authors**: Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, Junsong Yuan
- **Comment**: Accepted to ACM MM 2020
- **Journal**: None
- **Summary**: Despite the previous success of object analysis, detecting and segmenting a large number of object categories with a long-tailed data distribution remains a challenging problem and is less investigated. For a large-vocabulary classifier, the chance of obtaining noisy logits is much higher, which can easily lead to a wrong recognition. In this paper, we exploit prior knowledge of the relations among object categories to cluster fine-grained classes into coarser parent classes, and construct a classification tree that is responsible for parsing an object instance into a fine-grained category via its parent class. In the classification tree, as the number of parent class nodes are significantly less, their logits are less noisy and can be utilized to suppress the wrong/noisy logits existed in the fine-grained class nodes. As the way to construct the parent class is not unique, we further build multiple trees to form a classification forest where each tree contributes its vote to the fine-grained classification. To alleviate the imbalanced learning caused by the long-tail phenomena, we propose a simple yet effective resampling method, NMS Resampling, to re-balance the data distribution. Our method, termed as Forest R-CNN, can serve as a plug-and-play module being applied to most object recognition models for recognizing more than 1000 categories. Extensive experiments are performed on the large vocabulary dataset LVIS. Compared with the Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance with 11.5% and 3.9% AP improvements on the rare categories and overall categories, respectively. Moreover, we achieve state-of-the-art results on the LVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.



### Visual Localization for Autonomous Driving: Mapping the Accurate Location in the City Maze
- **Arxiv ID**: http://arxiv.org/abs/2008.05678v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05678v3)
- **Published**: 2020-08-13 03:59:34+00:00
- **Updated**: 2020-10-20 01:19:44+00:00
- **Authors**: Dongfang Liu, Yiming Cui, Xiaolei Guo, Wei Ding, Baijian Yang, Yingjie Chen
- **Comment**: Accepted at ICPR 2020, 8 pages, 10 figures. Code will be released
  soon
- **Journal**: None
- **Summary**: Accurate localization is a foundational capacity, required for autonomous vehicles to accomplish other tasks such as navigation or path planning. It is a common practice for vehicles to use GPS to acquire location information. However, the application of GPS can result in severe challenges when vehicles run within the inner city where different kinds of structures may shadow the GPS signal and lead to inaccurate location results. To address the localization challenges of urban settings, we propose a novel feature voting technique for visual localization. Different from the conventional front-view-based method, our approach employs views from three directions (front, left, and right) and thus significantly improves the robustness of location prediction. In our work, we craft the proposed feature voting method into three state-of-the-art visual localization networks and modify their architectures properly so that they can be applied for vehicular operation. Extensive field test results indicate that our approach can predict location robustly even in challenging inner-city settings. Our research sheds light on using the visual localization approach to help autonomous vehicles to find accurate location information in a city maze, within a desirable time constraint.



### Weight Training Analysis of Sportsmen with Kinect Bioinformatics for Form Improvement
- **Arxiv ID**: http://arxiv.org/abs/2009.09776v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09776v1)
- **Published**: 2020-08-13 04:52:31+00:00
- **Updated**: 2020-08-13 04:52:31+00:00
- **Authors**: Muhammad Umair Khan, Khawar Saeed, Sidra Qadeer
- **Comment**: None
- **Journal**: None
- **Summary**: Sports franchises invest a lot in training their athletes. use of latest technology for this purpose is also very common. We propose a system of capturing motion of athletes during weight training and analyzing that data to find out any shortcomings and imperfections. Our system uses Kinect depth image to compute different parameters of athlete's selected joints. These parameters are passed through certain algorithms to process them and formulate results on their basis. Some parameters like range of motion, speed and balance can be analyzed in real time. But for comparison to be performed between motions, data is first recorded and stored and then processed for accurate results. Our results depict that this system can be easily deployed and implemented to provide a very valuable insight to dynamics of a work out and help an athlete in improving his form.



### What leads to generalization of object proposals?
- **Arxiv ID**: http://arxiv.org/abs/2008.05700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05700v1)
- **Published**: 2020-08-13 05:51:35+00:00
- **Updated**: 2020-08-13 05:51:35+00:00
- **Authors**: Rui Wang, Dhruv Mahajan, Vignesh Ramanathan
- **Comment**: None
- **Journal**: None
- **Summary**: Object proposal generation is often the first step in many detection models. It is lucrative to train a good proposal model, that generalizes to unseen classes. This could help scaling detection models to larger number of classes with fewer annotations. Motivated by this, we study how a detection model trained on a small set of source classes can provide proposals that generalize to unseen classes. We systematically study the properties of the dataset - visual diversity and label space granularity - required for good generalization. We show the trade-off between using fine-grained labels and coarse labels. We introduce the idea of prototypical classes: a set of sufficient and necessary classes required to train a detection model to obtain generalized proposals in a more data-efficient way. On the Open Images V4 dataset, we show that only 25% of the classes can be selected to form such a prototypical set. The resulting proposals from a model trained with these classes is only 4.3% worse than using all the classes, in terms of average recall (AR). We also demonstrate that Faster R-CNN model leads to better generalization of proposals compared to a single-stage network like RetinaNet.



### Network Architecture Search for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.05706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05706v1)
- **Published**: 2020-08-13 06:15:57+00:00
- **Updated**: 2020-08-13 06:15:57+00:00
- **Authors**: Yichen Li, Xingchao Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks have been used to learn transferable representations for domain adaptation. Existing deep domain adaptation methods systematically employ popular hand-crafted networks designed specifically for image-classification tasks, leading to sub-optimal domain adaptation performance. In this paper, we present Neural Architecture Search for Domain Adaptation (NASDA), a principle framework that leverages differentiable neural architecture search to derive the optimal network architecture for domain adaptation task. NASDA is designed with two novel training strategies: neural architecture search with multi-kernel Maximum Mean Discrepancy to derive the optimal architecture, and adversarial training between a feature generator and a batch of classifiers to consolidate the feature generator. We demonstrate experimentally that NASDA leads to state-of-the-art performance on several domain adaptation benchmarks.



### Robust Image Matching By Dynamic Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2008.05708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05708v1)
- **Published**: 2020-08-13 06:21:33+00:00
- **Updated**: 2020-08-13 06:21:33+00:00
- **Authors**: Hao Huang, Jianchun Chen, Xiang Li, Lingjing Wang, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating dense correspondences between images is a long-standing image under-standing task. Recent works introduce convolutional neural networks (CNNs) to extract high-level feature maps and find correspondences through feature matching. However,high-level feature maps are in low spatial resolution and therefore insufficient to provide accurate and fine-grained features to distinguish intra-class variations for correspondence matching. To address this problem, we generate robust features by dynamically selecting features at different scales. To resolve two critical issues in feature selection,i.e.,how many and which scales of features to be selected, we frame the feature selection process as a sequential Markov decision-making process (MDP) and introduce an optimal selection strategy using reinforcement learning (RL). We define an RL environment for image matching in which each individual action either requires new features or terminates the selection episode by referring a matching score. Deep neural networks are incorporated into our method and trained for decision making. Experimental results show that our method achieves comparable/superior performance with state-of-the-art methods on three benchmarks, demonstrating the effectiveness of our feature selection strategy.



### Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
- **Arxiv ID**: http://arxiv.org/abs/2008.05711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05711v1)
- **Published**: 2020-08-13 06:29:01+00:00
- **Updated**: 2020-08-13 06:29:01+00:00
- **Authors**: Jonah Philion, Sanja Fidler
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot .



### Modeling Caricature Expressions by 3D Blendshape and Dynamic Texture
- **Arxiv ID**: http://arxiv.org/abs/2008.05714v1
- **DOI**: 10.1145/3394171.3413643
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05714v1)
- **Published**: 2020-08-13 06:31:01+00:00
- **Updated**: 2020-08-13 06:31:01+00:00
- **Authors**: Keyu Chen, Jianmin Zheng, Jianfei Cai, Juyong Zhang
- **Comment**: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
- **Journal**: None
- **Summary**: The problem of deforming an artist-drawn caricature according to a given normal face expression is of interest in applications such as social media, animation and entertainment. This paper presents a solution to the problem, with an emphasis on enhancing the ability to create desired expressions and meanwhile preserve the identity exaggeration style of the caricature, which imposes challenges due to the complicated nature of caricatures. The key of our solution is a novel method to model caricature expression, which extends traditional 3DMM representation to caricature domain. The method consists of shape modelling and texture generation for caricatures. Geometric optimization is developed to create identity-preserving blendshapes for reconstructing accurate and stable geometric shape, and a conditional generative adversarial network (cGAN) is designed for generating dynamic textures under target expressions. The combination of both shape and texture components makes the non-trivial expressions of a caricature be effectively defined by the extension of the popular 3DMM representation and a caricature can thus be flexibly deformed into arbitrary expressions with good results visually in both shape and color spaces. The experiments demonstrate the effectiveness of the proposed method.



### Alleviating Human-level Shift : A Robust Domain Adaptation Method for Multi-person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.05717v1
- **DOI**: 10.1145/3394171.3414040
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05717v1)
- **Published**: 2020-08-13 06:41:49+00:00
- **Updated**: 2020-08-13 06:41:49+00:00
- **Authors**: Xixia Xu, Qi Zou, Xue Lin
- **Comment**: Accepted By ACM MM'2020
- **Journal**: None
- **Summary**: Human pose estimation has been widely studied with much focus on supervised learning requiring sufficient annotations. However, in real applications, a pretrained pose estimation model usually need be adapted to a novel domain with no labels or sparse labels. Such domain adaptation for 2D pose estimation hasn't been explored. The main reason is that a pose, by nature, has typical topological structure and needs fine-grained features in local keypoints. While existing adaptation methods do not consider topological structure of object-of-interest and they align the whole images coarsely. Therefore, we propose a novel domain adaptation method for multi-person pose estimation to conduct the human-level topological structure alignment and fine-grained feature alignment. Our method consists of three modules: Cross-Attentive Feature Alignment (CAFA), Intra-domain Structure Adaptation (ISA) and Inter-domain Human-Topology Alignment (IHTA) module. The CAFA adopts a bidirectional spatial attention module (BSAM)that focuses on fine-grained local feature correlation between two humans to adaptively aggregate consistent features for adaptation. We adopt ISA only in semi-supervised domain adaptation (SSDA) to exploit the corresponding keypoint semantic relationship for reducing the intra-domain bias. Most importantly, we propose an IHTA to learn more domain-invariant human topological representation for reducing the inter-domain discrepancy. We model the human topological structure via the graph convolution network (GCN), by passing messages on which, high-order relations can be considered. This structure preserving alignment based on GCN is beneficial to the occluded or extreme pose inference. Extensive experiments are conducted on two popular benchmarks and results demonstrate the competency of our method compared with existing supervised approaches.



### Learning Temporally Invariant and Localizable Features via Data Augmentation for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.05721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05721v1)
- **Published**: 2020-08-13 06:56:52+00:00
- **Updated**: 2020-08-13 06:56:52+00:00
- **Authors**: Taeoh Kim, Hyeongmin Lee, MyeongAh Cho, Ho Seong Lee, Dong Heon Cho, Sangyoun Lee
- **Comment**: European Conference on Computer Vision (ECCV) 2020, 1st Visual
  Inductive Priors for Data-Efficient Deep Learning Workshop (Oral)
- **Journal**: None
- **Summary**: Deep-Learning-based video recognition has shown promising improvements along with the development of large-scale datasets and spatiotemporal network architectures. In image recognition, learning spatially invariant features is a key factor in improving recognition performance and robustness. Data augmentation based on visual inductive priors, such as cropping, flipping, rotating, or photometric jittering, is a representative approach to achieve these features. Recent state-of-the-art recognition solutions have relied on modern data augmentation strategies that exploit a mixture of augmentation operations. In this study, we extend these strategies to the temporal dimension for videos to learn temporally invariant or temporally localizable features to cover temporal perturbations or complex actions in videos. Based on our novel temporal data augmentation algorithms, video recognition performances are improved using only a limited amount of training data compared to the spatial-only data augmentation algorithms, including the 1st Visual Inductive Priors (VIPriors) for data-efficient action recognition challenge. Furthermore, learned features are temporally localizable that cannot be achieved using spatial augmentation algorithms. Our source code is available at https://github.com/taeoh-kim/temporal_data_augmentation.



### Contextual Diversity for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05723v1)
- **Published**: 2020-08-13 07:04:15+00:00
- **Updated**: 2020-08-13 07:04:15+00:00
- **Authors**: Sharat Agarwal, Himanshu Arora, Saket Anand, Chetan Arora
- **Comment**: A variant of this report is accepted in ECCV 2020
- **Journal**: None
- **Summary**: Requirement of large annotated datasets restrict the use of deep convolutional neural networks (CNNs) for many practical applications. The problem can be mitigated by using active learning (AL) techniques which, under a given annotation budget, allow to select a subset of data that yields maximum accuracy upon fine tuning. State of the art AL approaches typically rely on measures of visual diversity or prediction uncertainty, which are unable to effectively capture the variations in spatial context. On the other hand, modern CNN architectures make heavy use of spatial context for achieving highly accurate predictions. Since the context is difficult to evaluate in the absence of ground-truth labels, we introduce the notion of contextual diversity that captures the confusion associated with spatially co-occurring classes. Contextual Diversity (CD) hinges on a crucial observation that the probability vector predicted by a CNN for a region of interest typically contains information from a larger receptive field. Exploiting this observation, we use the proposed CD measure within two AL frameworks: (1) a core-set based strategy and (2) a reinforcement learning based policy, for active frame selection. Our extensive empirical evaluation establish state of the art results for active learning on benchmark datasets of Semantic Segmentation, Object Detection and Image Classification. Our ablation studies show clear advantages of using contextual diversity for active learning. The source code and additional results are available at https://github.com/sharat29ag/CDAL.



### ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2008.05731v3
- **DOI**: 10.1016/j.media.2021.102118
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05731v3)
- **Published**: 2020-08-13 07:34:05+00:00
- **Updated**: 2021-07-22 12:16:08+00:00
- **Authors**: Gwenolé Quellec, Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze, Pascale Massin, Béatrice Cochener
- **Comment**: None
- **Journal**: Medical Image Analysis, Volume 72, August 2021, 102118
- **Summary**: In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the "black-box" nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.



### An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.05732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05732v1)
- **Published**: 2020-08-13 07:37:27+00:00
- **Updated**: 2020-08-13 07:37:27+00:00
- **Authors**: Kenneth Lai, Svetlana Yanushkevich
- **Comment**: Accepted at International Joint Conference on Neural Network
- **Journal**: None
- **Summary**: The focus of this paper is dynamic gesture recognition in the context of the interaction between humans and machines. We propose a model consisting of two sub-networks, a transformer and an ordered-neuron long-short-term-memory (ON-LSTM) based recurrent neural network (RNN). Each sub-network is trained to perform the task of gesture recognition using only skeleton joints. Since each sub-network extracts different types of features due to the difference in architecture, the knowledge can be shared between the sub-networks. Through knowledge distillation, the features and predictions from each sub-network are fused together into a new fusion classifier. In addition, a cyclical learning rate can be used to generate a series of models that are combined in an ensemble, in order to yield a more generalizable prediction. The proposed ensemble of knowledge-sharing models exhibits an overall accuracy of 86.11% using only skeleton information, as tested using the Dynamic Hand Gesture-14/28 dataset



### Reliability of Decision Support in Cross-spectral Biometric-enabled Systems
- **Arxiv ID**: http://arxiv.org/abs/2008.05735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05735v1)
- **Published**: 2020-08-13 07:43:14+00:00
- **Updated**: 2020-08-13 07:43:14+00:00
- **Authors**: Kenneth Lai, Svetlana N. Yanushkevich, Vlad Shmerko
- **Comment**: submitted to IEEE International Conference on Systems, Man, and
  Cybernetics
- **Journal**: None
- **Summary**: This paper addresses the evaluation of the performance of the decision support system that utilizes face and facial expression biometrics. The evaluation criteria include risk of error and related reliability of decision, as well as their contribution to the changes in the perceived operator's trust in the decision. The relevant applications include human behavior monitoring and stress detection in individuals and teams, and in situational awareness system. Using an available database of cross-spectral videos of faces and facial expressions, we conducted a series of experiments that demonstrate the phenomenon of biases in biometrics that affect the evaluated measures of the performance in human-machine systems.



### SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2008.05742v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05742v3)
- **Published**: 2020-08-13 07:59:25+00:00
- **Updated**: 2021-06-10 03:26:02+00:00
- **Authors**: Jiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, Kui Jia
- **Comment**: 17 pages, 13 figures; TPAMI 2021
- **Journal**: None
- **Summary**: This paper focuses on the challenging task of learning 3D object surface reconstructions from RGB images. Existingmethods achieve varying degrees of success by using different surface representations. However, they all have their own drawbacks,and cannot properly reconstruct the surface shapes of complex topologies, arguably due to a lack of constraints on the topologicalstructures in their learning frameworks. To this end, we propose to learn and use the topology-preserved, skeletal shape representationto assist the downstream task of object surface reconstruction from RGB images. Technically, we propose the novelSkeletonNetdesign that learns a volumetric representation of a skeleton via a bridged learning of a skeletal point set, where we use paralleldecoders each responsible for the learning of points on 1D skeletal curves and 2D skeletal sheets, as well as an efficient module ofglobally guided subvolume synthesis for a refined, high-resolution skeletal volume; we present a differentiablePoint2Voxellayer tomake SkeletonNet end-to-end and trainable. With the learned skeletal volumes, we propose two models, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which respectivelybuild upon and improve over the existing frameworks of explicit mesh deformation and implicit field learning for the downstream surfacereconstruction task. We conduct thorough experiments that verify the efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing methods as well, and they have their own merits when measured by different metrics. Additional results ingeneralized task settings further demonstrate the usefulness of our proposed methods. We have made both our implementation codeand the ShapeNet-Skeleton dataset publicly available at ble at https://github.com/tangjiapeng/SkeletonNet.



### Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical Planes
- **Arxiv ID**: http://arxiv.org/abs/2008.05743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05743v1)
- **Published**: 2020-08-13 08:01:48+00:00
- **Updated**: 2020-08-13 08:01:48+00:00
- **Authors**: Istan Gergo Gal, Daniel Barath, Levente Hajder
- **Comment**: None
- **Journal**: None
- **Summary**: We propose two novel solvers for estimating the egomotion of a calibrated camera mounted to a moving vehicle from a single affine correspondence via recovering special homographies. For the first class of solvers, the sought plane is expected to be perpendicular to one of the camera axes. For the second class, the plane is orthogonal to the ground with unknown normal, e.g., it is a building facade. Both methods are solved via a linear system with a small coefficient matrix, thus, being extremely efficient. Both the minimal and over-determined cases can be solved by the proposed methods. They are tested on synthetic data and on publicly available real-world datasets. The novel methods are more accurate or comparable to the traditional algorithms and are faster when included in state of the art robust estimators.



### Adversarial Knowledge Transfer from Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2008.05746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05746v1)
- **Published**: 2020-08-13 08:04:27+00:00
- **Updated**: 2020-08-13 08:04:27+00:00
- **Authors**: Akash Gupta, Rameswar Panda, Sujoy Paul, Jianming Zhang, Amit K. Roy-Chowdhury
- **Comment**: Accepted to ACM Multimedia 2020
- **Journal**: None
- **Summary**: While machine learning approaches to visual recognition offer great promise, most of the existing methods rely heavily on the availability of large quantities of labeled training data. However, in the vast majority of real-world settings, manually collecting such large labeled datasets is infeasible due to the cost of labeling data or the paucity of data in a given domain. In this paper, we present a novel Adversarial Knowledge Transfer (AKT) framework for transferring knowledge from internet-scale unlabeled data to improve the performance of a classifier on a given visual recognition task. The proposed adversarial learning framework aligns the feature space of the unlabeled source data with the labeled target data such that the target classifier can be used to predict pseudo labels on the source data. An important novel aspect of our method is that the unlabeled source data can be of different classes from those of the labeled target data, and there is no need to define a separate pretext task, unlike some existing approaches. Extensive experiments well demonstrate that models learned using our approach hold a lot of promise across a variety of visual recognition tasks on multiple standard datasets.



### AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2008.05753v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05753v1)
- **Published**: 2020-08-13 08:30:23+00:00
- **Updated**: 2020-08-13 08:30:23+00:00
- **Authors**: Jawook Gu, Jong Chul Ye
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Recently, deep learning approaches have been extensively studied for low-dose CT denoising thanks to its superior performance despite the fast computational time. In particular, cycleGAN has been demonstrated as a powerful unsupervised learning scheme to improve the low-dose CT image quality without requiring matched high-dose reference data. Unfortunately, one of the main limitations of the cycleGAN approach is that it requires two deep neural network generators at the training phase, although only one of them is used at the inference phase. The secondary auxiliary generator is needed to enforce the cycle-consistency, but the additional memory requirement and increases of the learnable parameters are the main huddles for cycleGAN training. To address this issue, here we propose a novel cycleGAN architecture using a single switchable generator. In particular, a single generator is implemented using adaptive instance normalization (AdaIN) layers so that the baseline generator converting a low-dose CT image to a routine-dose CT image can be switched to a generator converting high-dose to low-dose by simply changing the AdaIN code. Thanks to the shared baseline network, the additional memory requirement and weight increases are minimized, and the training can be done more stably even with small training data. Experimental results show that the proposed method outperforms the previous cycleGAN approaches while using only about half the parameters.



### Powers of layers for image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/2008.05763v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05763v1)
- **Published**: 2020-08-13 09:02:17+00:00
- **Updated**: 2020-08-13 09:02:17+00:00
- **Authors**: Hugo Touvron, Matthijs Douze, Matthieu Cord, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance of our model is comparable or better than CycleGAN with significantly fewer parameters.



### Revisiting Temporal Modeling for Video Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.05765v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05765v2)
- **Published**: 2020-08-13 09:09:37+00:00
- **Updated**: 2020-08-20 02:00:20+00:00
- **Authors**: Takashi Isobe, Fang Zhu, Xu Jia, Shengjin Wang
- **Comment**: BMVC 2020
- **Journal**: None
- **Summary**: Video super-resolution plays an important role in surveillance video analysis and ultra-high-definition video display, which has drawn much attention in both the research and industrial communities. Although many deep learning-based VSR methods have been proposed, it is hard to directly compare these methods since the different loss functions and training datasets have a significant impact on the super-resolution results. In this work, we carefully study and compare three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion and Recurrent Neural Network) for video super-resolution. We also propose a novel Recurrent Residual Network (RRN) for efficient video super-resolution, where residual learning is utilized to stabilize the training of RNN and meanwhile to boost the super-resolution performance. Extensive experiments show that the proposed RRN is highly computational efficiency and produces temporal consistent VSR results with finer details than other temporal modeling methods. Besides, the proposed method achieves state-of-the-art results on several widely used benchmarks.



### Weight Equalizing Shift Scaler-Coupled Post-training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2008.05767v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05767v1)
- **Published**: 2020-08-13 09:19:57+00:00
- **Updated**: 2020-08-13 09:19:57+00:00
- **Authors**: Jihun Oh, SangJeong Lee, Meejeong Park, Pooni Walagaurav, Kiseok Kwon
- **Comment**: 9 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Post-training, layer-wise quantization is preferable because it is free from retraining and is hardware-friendly. Nevertheless, accuracy degradation has occurred when a neural network model has a big difference of per-out-channel weight ranges. In particular, the MobileNet family has a tragedy drop in top-1 accuracy from 70.60% ~ 71.87% to 0.1% on the ImageNet dataset after 8-bit weight quantization. To mitigate this significant accuracy reduction, we propose a new weight equalizing shift scaler, i.e. rescaling the weight range per channel by a 4-bit binary shift, prior to a layer-wise quantization. To recover the original output range, inverse binary shifting is efficiently fused to the existing per-layer scale compounding in the fixed-computing convolutional operator of the custom neural processing unit. The binary shift is a key feature of our algorithm, which significantly improved the accuracy performance without impeding the memory footprint. As a result, our proposed method achieved a top-1 accuracy of 69.78% ~ 70.96% in MobileNets and showed robust performance in varying network models and tasks, which is competitive to channel-wise quantization results.



### Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses
- **Arxiv ID**: http://arxiv.org/abs/2008.05770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05770v1)
- **Published**: 2020-08-13 09:26:01+00:00
- **Updated**: 2020-08-13 09:26:01+00:00
- **Authors**: Chen Li, Gim Hee Lee
- **Comment**: Accepted to BMVC2020
- **Journal**: None
- **Summary**: 3D human pose estimation from a single image is an inverse problem due to the inherent ambiguity of the missing depth. Several previous works addressed the inverse problem by generating multiple hypotheses. However, these works are strongly supervised and require ground truth 2D-to-3D correspondences which can be difficult to obtain. In this paper, we propose a weakly supervised deep generative network to address the inverse problem and circumvent the need for ground truth 2D-to-3D correspondences. To this end, we design our network to model a proposal distribution which we use to approximate the unknown multi-modal target posterior distribution. We achieve the approximation by minimizing the KL divergence between the proposal and target distributions, and this leads to a 2D reprojection error and a prior loss term that can be weakly supervised. Furthermore, we determine the most probable solution as the conditional mode of the samples using the mean-shift algorithm. We evaluate our method on three benchmark datasets -- Human3.6M, MPII and MPI-INF-3DHP. Experimental results show that our approach is capable of generating multiple feasible hypotheses and achieves state-of-the-art results compared to existing weakly supervised approaches. Our source code is available at the project website.



### CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2008.05772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05772v1)
- **Published**: 2020-08-13 09:30:12+00:00
- **Updated**: 2020-08-13 09:30:12+00:00
- **Authors**: Boah Kim, Dong Hwan Kim, Seong Ho Park, Jieun Kim, June-Goo Lee, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a fundamental task in medical image analysis. Recently, deep learning based image registration methods have been extensively investigated due to their excellent performance despite the ultra-fast computational time. However, the existing deep learning methods still have limitation in the preservation of original topology during the deformation with registration vector fields. To address this issues, here we present a cycle-consistent deformable image registration. The cycle consistency enhances image registration performance by providing an implicit regularization to preserve topology during the deformation. The proposed method is so flexible that can be applied for both 2D and 3D registration problems for various applications, and can be easily extended to multi-scale implementation to deal with the memory issues in large volume registration. Experimental results on various datasets from medical and non-medical applications demonstrate that the proposed method provides effective and accurate registration on diverse image pairs within a few seconds. Qualitative and quantitative evaluations on deformation fields also verify the effectiveness of the cycle consistency of the proposed method.



### Integrating uncertainty in deep neural networks for MRI based stroke analysis
- **Arxiv ID**: http://arxiv.org/abs/2008.06332v1
- **DOI**: 10.1016/j.media.2020.101790
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06332v1)
- **Published**: 2020-08-13 09:50:17+00:00
- **Updated**: 2020-08-13 09:50:17+00:00
- **Authors**: Lisa Herzog, Elvis Murina, Oliver Dürr, Susanne Wegener, Beate Sick
- **Comment**: 21 pages, 13 figures
- **Journal**: Medical Image Analysis (2020): 101790
- **Summary**: At present, the majority of the proposed Deep Learning (DL) methods provide point predictions without quantifying the models uncertainty. However, a quantification of the reliability of automated image analysis is essential, in particular in medicine when physicians rely on the results for making critical treatment decisions. In this work, we provide an entire framework to diagnose ischemic stroke patients incorporating Bayesian uncertainty into the analysis procedure. We present a Bayesian Convolutional Neural Network (CNN) yielding a probability for a stroke lesion on 2D Magnetic Resonance (MR) images with corresponding uncertainty information about the reliability of the prediction. For patient-level diagnoses, different aggregation methods are proposed and evaluated, which combine the single image-level predictions. Those methods take advantage of the uncertainty in image predictions and report model uncertainty at the patient-level. In a cohort of 511 patients, our Bayesian CNN achieved an accuracy of 95.33% at the image-level representing a significant improvement of 2% over a non-Bayesian counterpart. The best patient aggregation method yielded 95.89% of accuracy. Integrating uncertainty information about image predictions in aggregation models resulted in higher uncertainty measures to false patient classifications, which enabled to filter critical patient diagnoses that are supposed to be closer examined by a medical doctor. We therefore recommend using Bayesian approaches not only for improved image-level prediction and uncertainty estimation but also for the detection of uncertain aggregations at the patient-level.



### Multi-Modality Pathology Segmentation Framework: Application to Cardiac Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2008.05780v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05780v1)
- **Published**: 2020-08-13 09:57:04+00:00
- **Updated**: 2020-08-13 09:57:04+00:00
- **Authors**: Zhen Zhang, Chenyu Liu, Wangbin Ding, Sihan Wang, Chenhao Pei, Mingjing Yang, Liqin Huang
- **Comment**: 12 pages,MyoPS 2020
- **Journal**: None
- **Summary**: Multi-sequence of cardiac magnetic resonance (CMR) images can provide complementary information for myocardial pathology (scar and edema). However, it is still challenging to fuse these underlying information for pathology segmentation effectively. This work presents an automatic cascade pathology segmentation framework based on multi-modality CMR images. It mainly consists of two neural networks: an anatomical structure segmentation network (ASSN) and a pathological region segmentation network (PRSN). Specifically, the ASSN aims to segment the anatomical structure where the pathology may exist, and it can provide a spatial prior for the pathological region segmentation. In addition, we integrate a denoising auto-encoder (DAE) into the ASSN to generate segmentation results with plausible shapes. The PRSN is designed to segment pathological region based on the result of ASSN, in which a fusion block based on channel attention is proposed to better aggregate multi-modality information from multi-modality CMR images. Experiments from the MyoPS2020 challenge dataset show that our framework can achieve promising performance for myocardial scar and edema segmentation.



### Predicting Visual Overlap of Images Through Interpretable Non-Metric Box Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2008.05785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05785v1)
- **Published**: 2020-08-13 10:01:07+00:00
- **Updated**: 2020-08-13 10:01:07+00:00
- **Authors**: Anita Rau, Guillermo Garcia-Hernando, Danail Stoyanov, Gabriel J. Brostow, Daniyar Turmukhambetov
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: To what extent are two images picturing the same 3D surfaces? Even when this is a known scene, the answer typically requires an expensive search across scale space, with matching and geometric verification of large sets of local features. This expense is further multiplied when a query image is evaluated against a gallery, e.g. in visual relocalization. While we don't obviate the need for geometric verification, we propose an interpretable image-embedding that cuts the search in scale space to essentially a lookup.   Our approach measures the asymmetric relation between two images. The model then learns a scene-specific measure of similarity, from training examples with known 3D visible-surface overlaps. The result is that we can quickly identify, for example, which test image is a close-up version of another, and by what scale factor. Subsequently, local features need only be detected at that scale. We validate our scene-specific model by showing how this embedding yields competitive image-matching results, while being simpler, faster, and also interpretable by humans.



### Shift Equivariance in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.05787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05787v1)
- **Published**: 2020-08-13 10:02:02+00:00
- **Updated**: 2020-08-13 10:02:02+00:00
- **Authors**: Marco Manfredi, Yu Wang
- **Comment**: Accepted at ECCV 2020 Workshop: Beyond mAP: Reassessing the
  Evaluation of Object Detectors
- **Journal**: None
- **Summary**: Robustness to small image translations is a highly desirable property for object detectors. However, recent works have shown that CNN-based classifiers are not shift invariant. It is unclear to what extent this could impact object detection, mainly because of the architectural differences between the two and the dimensionality of the prediction space of modern detectors. To assess shift equivariance of object detection models end-to-end, in this paper we propose an evaluation metric, built upon a greedy search of the lower and upper bounds of the mean average precision on a shifted image set. Our new metric shows that modern object detection architectures, no matter if one-stage or two-stage, anchor-based or anchor-free, are sensitive to even one pixel shift to the input images. Furthermore, we investigate several possible solutions to this problem, both taken from the literature and newly proposed, quantifying the effectiveness of each one with the suggested metric. Our results indicate that none of these methods can provide full shift equivariance. Measuring and analyzing the extent of shift variance of different models and the contributions of possible factors, is a first step towards being able to devise methods that mitigate or even leverage such variabilities.



### Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05789v1
- **DOI**: 10.1145/3394171.3413869
- **Categories**: **cs.MM**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05789v1)
- **Published**: 2020-08-13 10:08:12+00:00
- **Updated**: 2020-08-13 10:08:12+00:00
- **Authors**: Ying Cheng, Ruize Wang, Zhihao Pan, Rui Feng, Yuejie Zhang
- **Comment**: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
- **Journal**: None
- **Summary**: When watching videos, the occurrence of a visual event is often accompanied by an audio event, e.g., the voice of lip motion, the music of playing instruments. There is an underlying correlation between audio and visual events, which can be utilized as free supervised information to train a neural network by solving the pretext task of audio-visual synchronization. In this paper, we propose a novel self-supervised framework with co-attention mechanism to learn generic cross-modal representations from unlabelled videos in the wild, and further benefit downstream tasks. Specifically, we explore three different co-attention modules to focus on discriminative visual regions correlated to the sounds and introduce the interactions between them. Experiments show that our model achieves state-of-the-art performance on the pretext task while having fewer parameters compared with existing methods. To further evaluate the generalizability and transferability of our approach, we apply the pre-trained model on two downstream tasks, i.e., sound source localization and action recognition. Extensive experiments demonstrate that our model provides competitive results with other self-supervised methods, and also indicate that our approach can tackle the challenging scenes which contain multiple sound sources.



### Localizing the Common Action Among a Few Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.05826v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05826v2)
- **Published**: 2020-08-13 11:31:23+00:00
- **Updated**: 2020-08-25 12:30:01+00:00
- **Authors**: Pengwan Yang, Vincent Tao Hu, Pascal Mettes, Cees G. M. Snoek
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: This paper strives to localize the temporal extent of an action in a long untrimmed video. Where existing work leverages many examples with their start, their ending, and/or the class of the action during training time, we propose few-shot common action localization. The start and end of an action in a long untrimmed video is determined based on just a hand-full of trimmed video examples containing the same action, without knowing their common class label. To address this task, we introduce a new 3D convolutional network architecture able to align representations from the support videos with the relevant query video segments. The network contains: (\textit{i}) a mutual enhancement module to simultaneously complement the representation of the few trimmed support videos and the untrimmed query video; (\textit{ii}) a progressive alignment module that iteratively fuses the support videos into the query branch; and (\textit{iii}) a pairwise matching module to weigh the importance of different support videos. Evaluation of few-shot common action localization in untrimmed videos containing a single or multiple action instances demonstrates the effectiveness and general applicability of our proposal.



### Recurrent Deconvolutional Generative Adversarial Networks with Application to Text Guided Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.05856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05856v1)
- **Published**: 2020-08-13 12:22:27+00:00
- **Updated**: 2020-08-13 12:22:27+00:00
- **Authors**: Hongyuan Yu, Yan Huang, Lihong Pi, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel model for video generation and especially makes the attempt to deal with the problem of video generation from text descriptions, i.e., synthesizing realistic videos conditioned on given texts. Existing video generation methods cannot be easily adapted to handle this task well, due to the frame discontinuity issue and their text-free generation schemes. To address these problems, we propose a recurrent deconvolutional generative adversarial network (RD-GAN), which includes a recurrent deconvolutional network (RDN) as the generator and a 3D convolutional neural network (3D-CNN) as the discriminator. The RDN is a deconvolutional version of conventional recurrent neural network, which can well model the long-range temporal dependency of generated video frames and make good use of conditional information. The proposed model can be jointly trained by pushing the RDN to generate realistic videos so that the 3D-CNN cannot distinguish them from real ones. We apply the proposed RD-GAN to a series of tasks including conventional video generation, conditional video generation, video prediction and video classification, and demonstrate its effectiveness by achieving well performance.



### Self-supervised Video Representation Learning by Pace Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.05861v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05861v2)
- **Published**: 2020-08-13 12:40:24+00:00
- **Updated**: 2020-09-04 08:05:35+00:00
- **Authors**: Jiangliu Wang, Jianbo Jiao, Yun-Hui Liu
- **Comment**: Correct some typos;Update some cocurent works accepted by ECCV 2020
- **Journal**: None
- **Summary**: This paper addresses the problem of self-supervised video representation learning from a new perspective -- by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.



### DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.05865v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05865v4)
- **Published**: 2020-08-13 12:51:17+00:00
- **Updated**: 2022-10-15 03:51:50+00:00
- **Authors**: Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets.



### Motion Similarity Modeling -- A State of the Art Report
- **Arxiv ID**: http://arxiv.org/abs/2008.05872v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2008.05872v1)
- **Published**: 2020-08-13 13:08:30+00:00
- **Updated**: 2020-08-13 13:08:30+00:00
- **Authors**: Anna Sebernegg, Peter Kán, Hannes Kaufmann
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of human motion opens up a wide range of possibilities, such as realistic training simulations or authentic motions in robotics or animation. One of the problems underlying motion analysis is the meaningful comparison of actions based on similarity measures. Since the motion analysis is application-dependent, it is essential to find the appropriate motion similarity method for the particular use case. This state of the art report provides an overview of human motion analysis and different similarity modeling methods, while mainly focusing on approaches that work with 3D motion data. The survey summarizes various similarity aspects and features of motion and describes approaches to measuring the similarity between two actions.



### LGNN: A Context-aware Line Segment Detector
- **Arxiv ID**: http://arxiv.org/abs/2008.05892v2
- **DOI**: 10.1145/3394171.3413784
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05892v2)
- **Published**: 2020-08-13 13:23:18+00:00
- **Updated**: 2020-08-29 03:43:06+00:00
- **Authors**: Quan Meng, Jiakai Zhang, Qiang Hu, Xuming He, Jingyi Yu
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: We present a novel real-time line segment detection scheme called Line Graph Neural Network (LGNN). Existing approaches require a computationally expensive verification or postprocessing step. Our LGNN employs a deep convolutional neural network (DCNN) for proposing line segment directly, with a graph neural network (GNN) module for reasoning their connectivities. Specifically, LGNN exploits a new quadruplet representation for each line segment where the GNN module takes the predicted candidates as vertexes and constructs a sparse graph to enforce structural context. Compared with the state-of-the-art, LGNN achieves near real-time performance without compromising accuracy. LGNN further enables time-sensitive 3D applications. When a 3D point cloud is accessible, we present a multi-modal line segment classification technique for extracting a 3D wireframe of the environment robustly and efficiently.



### DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2008.05924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05924v1)
- **Published**: 2020-08-13 14:10:05+00:00
- **Updated**: 2020-08-13 14:10:05+00:00
- **Authors**: Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, Jiateng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, facial expression recognition (FER) in the wild has gained a lot of researchers' attention because it is a valuable topic to enable the FER techniques to move from the laboratory to the real applications. In this paper, we focus on this challenging but interesting topic and make contributions from three aspects. First, we present a new large-scale 'in-the-wild' dynamic facial expression database, DFEW (Dynamic Facial Expression in the Wild), consisting of over 16,000 video clips from thousands of movies. These video clips contain various challenging interferences in practical scenarios such as extreme illumination, occlusions, and capricious pose changes. Second, we propose a novel method called Expression-Clustered Spatiotemporal Feature Learning (EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct extensive benchmark experiments on DFEW using a lot of spatiotemporal deep feature learning methods as well as our proposed EC-STFL. Experimental results show that DFEW is a well-designed and challenging database, and the proposed EC-STFL can promisingly improve the performance of existing spatiotemporal deep neural networks in coping with the problem of dynamic FER in the wild. Our DFEW database is publicly available and can be freely downloaded from https://dfew-dataset.github.io/.



### End-to-end Contextual Perception and Prediction with Interaction Transformer
- **Arxiv ID**: http://arxiv.org/abs/2008.05927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.05927v1)
- **Published**: 2020-08-13 14:30:12+00:00
- **Updated**: 2020-08-13 14:30:12+00:00
- **Authors**: Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal, Raquel Urtasun
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D and nuScenes. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors.



### Automated detection and quantification of COVID-19 airspace disease on chest radiographs: A novel approach achieving radiologist-level performance using a CNN trained on digital reconstructed radiographs (DRRs) from CT-based ground-truth
- **Arxiv ID**: http://arxiv.org/abs/2008.06330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06330v1)
- **Published**: 2020-08-13 14:33:03+00:00
- **Updated**: 2020-08-13 14:33:03+00:00
- **Authors**: Eduardo Mortani Barbosa Jr., Warren B. Gefter, Rochelle Yang, Florin C. Ghesu, Siqi Liu, Boris Mailhe, Awais Mansoor, Sasa Grbic, Sebastian Piat, Guillaume Chabin, Vishwanath R S., Abishek Balachandran, Sebastian Vogt, Valentin Ziebandt, Steffen Kappler, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To leverage volumetric quantification of airspace disease (AD) derived from a superior modality (CT) serving as ground truth, projected onto digitally reconstructed radiographs (DRRs) to: 1) train a convolutional neural network to quantify airspace disease on paired CXRs; and 2) compare the DRR-trained CNN to expert human readers in the CXR evaluation of patients with confirmed COVID-19.   Materials and Methods: We retrospectively selected a cohort of 86 COVID-19 patients (with positive RT-PCR), from March-May 2020 at a tertiary hospital in the northeastern USA, who underwent chest CT and CXR within 48 hrs. The ground truth volumetric percentage of COVID-19 related AD (POv) was established by manual AD segmentation on CT. The resulting 3D masks were projected into 2D anterior-posterior digitally reconstructed radiographs (DRR) to compute area-based AD percentage (POa). A convolutional neural network (CNN) was trained with DRR images generated from a larger-scale CT dataset of COVID-19 and non-COVID-19 patients, automatically segmenting lungs, AD and quantifying POa on CXR. CNN POa results were compared to POa quantified on CXR by two expert readers and to the POv ground-truth, by computing correlations and mean absolute errors.   Results: Bootstrap mean absolute error (MAE) and correlations between POa and POv were 11.98% [11.05%-12.47%] and 0.77 [0.70-0.82] for average of expert readers, and 9.56%-9.78% [8.83%-10.22%] and 0.78-0.81 [0.73-0.85] for the CNN, respectively.   Conclusion: Our CNN trained with DRR using CT-derived airspace quantification achieved expert radiologist level of accuracy in the quantification of airspace disease on CXR, in patients with positive RT-PCR for COVID-19.



### Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations
- **Arxiv ID**: http://arxiv.org/abs/2008.05930v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05930v1)
- **Published**: 2020-08-13 14:40:46+00:00
- **Updated**: 2020-08-13 14:40:46+00:00
- **Authors**: Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, Raquel Urtasun
- **Comment**: European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: In this paper we propose a novel end-to-end learnable network that performs joint perception, prediction and motion planning for self-driving vehicles and produces interpretable intermediate representations. Unlike existing neural motion planners, our motion planning costs are consistent with our perception and prediction estimates. This is achieved by a novel differentiable semantic occupancy representation that is explicitly used as cost by the motion planning process. Our network is learned end-to-end from human demonstrations. The experiments in a large-scale manual-driving dataset and closed-loop simulation show that the proposed model significantly outperforms state-of-the-art planners in imitating the human behaviors while producing much safer trajectories.



### RGB cameras failures and their effects in autonomous driving applications
- **Arxiv ID**: http://arxiv.org/abs/2008.05938v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2008.05938v3)
- **Published**: 2020-08-13 14:47:50+00:00
- **Updated**: 2022-03-04 09:33:54+00:00
- **Authors**: Francesco Secci, Andrea Ceccarelli
- **Comment**: submitted
- **Journal**: None
- **Summary**: RGB cameras are one of the most relevant sensors for autonomous driving applications. It is undeniable that failures of vehicle cameras may compromise the autonomous driving task, possibly leading to unsafe behaviors when images that are subsequently processed by the driving system are altered. To support the definition of safe and robust vehicle architectures and intelligent systems, in this paper we define the failure modes of a vehicle camera, together with an analysis of effects and known mitigations. Further, we build a software library for the generation of the corresponding failed images and we feed them to six object detectors for mono and stereo cameras and to the self-driving agent of an autonomous driving simulator. The resulting misbehaviors with respect to operating with clean images allow a better understanding of failures effects and the related safety risks in image-based applications.



### Deep Domain Adaptation for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labelled Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.06392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06392v2)
- **Published**: 2020-08-13 15:42:23+00:00
- **Updated**: 2021-03-02 02:24:36+00:00
- **Authors**: Gnana Praveen R, Eric Granger, Patrick Cardinal
- **Comment**: Under review for a journal
- **Journal**: None
- **Summary**: Estimation of pain intensity from facial expressions captured in videos has an immense potential for health care applications. Given the challenges related to subjective variations of facial expressions, and operational capture conditions, the accuracy of state-of-the-art DL models for recognizing facial expressions may decline. Domain adaptation has been widely explored to alleviate the problem of domain shifts that typically occur between video data captured across various source and target domains. Moreover, given the laborious task of collecting and annotating videos, and subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning is gaining attention in such applications. State-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relationship among pain intensity levels, nor temporal coherence of multiple consecutive frames. This paper introduces a new DL model for weakly-supervised DA with ordinal regression that can be adapted using target domain videos with coarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among intensity levels assigned to target sequences, and associates multiple relevant frames to sequence-level labels. In particular, it learns discriminant and domain-invariant feature representations by integrating multiple instance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from target domain. The proposed approach was validated using RECOLA video dataset as fully-labeled source domain data, and UNBC-McMaster shoulder pain video dataset as weakly-labeled target domain data. We have also validated WSDA-OR on BIOVID and Fatigue datasets for sequence level estimation.



### Deep Learning to Quantify Pulmonary Edema in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2008.05975v2
- **DOI**: 10.1148/ryai.2021190228
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05975v2)
- **Published**: 2020-08-13 15:45:44+00:00
- **Updated**: 2021-01-07 16:12:01+00:00
- **Authors**: Steven Horng, Ruizhi Liao, Xin Wang, Sandeep Dalal, Polina Golland, Seth J Berkowitz
- **Comment**: The two first authors contributed equally
- **Journal**: None
- **Summary**: Purpose: To develop a machine learning model to classify the severity grades of pulmonary edema on chest radiographs.   Materials and Methods: In this retrospective study, 369,071 chest radiographs and associated radiology reports from 64,581 (mean age, 51.71; 54.51% women) patients from the MIMIC-CXR chest radiograph dataset were included. This dataset was split into patients with and without congestive heart failure (CHF). Pulmonary edema severity labels from the associated radiology reports were extracted from patients with CHF as four different ordinal levels: 0, no edema; 1, vascular congestion; 2, interstitial edema; and 3, alveolar edema. Deep learning models were developed using two approaches: a semi-supervised model using a variational autoencoder and a pre-trained supervised learning model using a dense neural network. Receiver operating characteristic curve analysis was performed on both models.   Results: The area under the receiver operating characteristic curve (AUC) for differentiating alveolar edema from no edema was 0.99 for the semi-supervised model and 0.87 for the pre-trained models. Performance of the algorithm was inversely related to the difficulty in categorizing milder states of pulmonary edema (shown as AUCs for semi-supervised model and pre-trained model, respectively): 2 versus 0, 0.88 and 0.81; 1 versus 0, 0.79 and 0.66; 3 versus 1, 0.93 and 0.82; 2 versus 1, 0.69 and 0.73; and, 3 versus 2, 0.88 and 0.63.   Conclusion: Deep learning models were trained on a large chest radiograph dataset and could grade the severity of pulmonary edema on chest radiographs with high performance.



### Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.05977v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.05977v1)
- **Published**: 2020-08-13 15:51:42+00:00
- **Updated**: 2020-08-13 15:51:42+00:00
- **Authors**: Ling-An Zeng, Fa-Ting Hong, Wei-Shi Zheng, Qi-Zhi Yu, Wei Zeng, Yao-Wei Wang, Jian-Huang Lai
- **Comment**: ACM International Conference on Multimedia 2020
- **Journal**: None
- **Summary**: The objective of action quality assessment is to score sports videos. However, most existing works focus only on video dynamic information (i.e., motion information) but ignore the specific postures that an athlete is performing in a video, which is important for action assessment in long videos. In this work, we present a novel hybrid dynAmic-static Context-aware attenTION NETwork (ACTION-NET) for action assessment in long videos. To learn more discriminative representations for videos, we not only learn the video dynamic information but also focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instance-wise graph convolutional network unit and an attention unit for both streams to extract more robust stream features, where the former is for exploring the relations between instances and the latter for assigning a proper weight to each instance. Finally, we combine the features of the two streams to regress the final video score, supervised by ground-truth scores given by experts. Additionally, we have collected and annotated the new Rhythmic Gymnastics dataset, which contains videos of four different types of gymnastics routines, for evaluation of action quality assessment in long videos. Extensive experimental results validate the efficacy of our proposed method, which outperforms related approaches. The codes and dataset are available at \url{https://github.com/lingan1996/ACTION-NET}.



### Black Magic in Deep Learning: How Human Skill Impacts Network Training
- **Arxiv ID**: http://arxiv.org/abs/2008.05981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05981v1)
- **Published**: 2020-08-13 15:56:14+00:00
- **Updated**: 2020-08-13 15:56:14+00:00
- **Authors**: Kanav Anand, Ziqi Wang, Marco Loog, Jan van Gemert
- **Comment**: presented at the British Machine Vision Conference, 2020
- **Journal**: None
- **Summary**: How does a user's prior experience with deep learning impact accuracy? We present an initial study based on 31 participants with different levels of experience. Their task is to perform hyperparameter optimization for a given deep learning architecture. The results show a strong positive correlation between the participant's experience and the final performance. They additionally indicate that an experienced participant finds better solutions using fewer resources on average. The data suggests furthermore that participants with no prior experience follow random strategies in their pursuit of optimal hyperparameters. Our study investigates the subjective human factor in comparisons of state of the art results and scientific reproducibility in deep learning.



### Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.06020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.06020v1)
- **Published**: 2020-08-13 17:20:02+00:00
- **Updated**: 2020-08-13 17:20:02+00:00
- **Authors**: Kelvin Wong, Qiang Zhang, Ming Liang, Bin Yang, Renjie Liao, Abbas Sadat, Raquel Urtasun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present a novel method for testing the safety of self-driving vehicles in simulation. We propose an alternative to sensor simulation, as sensor simulation is expensive and has large domain gaps. Instead, we directly simulate the outputs of the self-driving vehicle's perception and prediction system, enabling realistic motion planning testing. Specifically, we use paired data in the form of ground truth labels and real perception and prediction outputs to train a model that predicts what the online system will produce. Importantly, the inputs to our system consists of high definition maps, bounding boxes, and trajectories, which can be easily sketched by a test engineer in a matter of minutes. This makes our approach a much more scalable solution. Quantitative results on two large-scale datasets demonstrate that we can realistically test motion planning using our simulations.



### BioMetricNet: deep unconstrained face verification through learning of metrics regularized onto Gaussian distributions
- **Arxiv ID**: http://arxiv.org/abs/2008.06021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06021v1)
- **Published**: 2020-08-13 17:22:46+00:00
- **Updated**: 2020-08-13 17:22:46+00:00
- **Authors**: Arslan Ali, Matteo Testa, Tiziano Bianchi, Enrico Magli
- **Comment**: Accepted at ECCV20
- **Journal**: None
- **Summary**: We present BioMetricNet: a novel framework for deep unconstrained face verification which learns a regularized metric to compare facial features. Differently from popular methods such as FaceNet, the proposed approach does not impose any specific metric on facial features; instead, it shapes the decision space by learning a latent representation in which matching and non-matching pairs are mapped onto clearly separated and well-behaved target distributions. In particular, the network jointly learns the best feature representation, and the best metric that follows the target distributions, to be used to discriminate face images. In this paper we present this general framework, first of its kind for facial verification, and tailor it to Gaussian distributions. This choice enables the use of a simple linear decision boundary that can be tuned to achieve the desired trade-off between false alarm and genuine acceptance rate, and leads to a loss function that can be written in closed form. Extensive analysis and experimentation on publicly available datasets such as Labeled Faces in the wild (LFW), Youtube faces (YTF), Celebrities in Frontal-Profile in the Wild (CFP), and challenging datasets like cross-age LFW (CALFW), cross-pose LFW (CPLFW), In-the-wild Age Dataset (AgeDB) show a significant performance improvement and confirms the effectiveness and superiority of BioMetricNet over existing state-of-the-art methods.



### Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks in Highly Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.06029v2
- **DOI**: 10.1002/nbm.4798
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2008.06029v2)
- **Published**: 2020-08-13 17:36:59+00:00
- **Updated**: 2022-06-09 00:33:53+00:00
- **Authors**: Burhaneddin Yaman, Hongyi Gu, Seyed Amir Hossein Hosseini, Omer Burak Demirel, Steen Moeller, Jutta Ellermann, Kâmil Uğurbil, Mehmet Akçakaya
- **Comment**: None
- **Journal**: NMR in Biomedicine, 2022
- **Summary**: Self-supervised learning has shown great promise due to its capability to train deep learning MRI reconstruction methods without fully-sampled data. Current self-supervised learning methods for physics-guided reconstruction networks split acquired undersampled data into two disjoint sets, where one is used for data consistency (DC) in the unrolled network and the other to define the training loss. In this study, we propose an improved self-supervised learning strategy that more efficiently uses the acquired data to train a physics-guided reconstruction network without a database of fully-sampled data. The proposed multi-mask self-supervised learning via data undersampling (SSDU) applies a hold-out masking operation on acquired measurements to split it into multiple pairs of disjoint sets for each training sample, while using one of these pairs for DC units and the other for defining loss, thereby more efficiently using the undersampled data. Multi-mask SSDU is applied on fully-sampled 3D knee and prospectively undersampled 3D brain MRI datasets, for various acceleration rates and patterns, and compared to CG-SENSE and single-mask SSDU DL-MRI, as well as supervised DL-MRI when fully-sampled data is available. Results on knee MRI show that the proposed multi-mask SSDU outperforms SSDU and performs closely with supervised DL-MRI. A clinical reader study further ranks the multi-mask SSDU higher than supervised DL-MRI in terms of SNR and aliasing artifacts. Results on brain MRI show that multi-mask SSDU achieves better reconstruction quality compared to SSDU. Reader study demonstrates that multi-mask SSDU at R=8 significantly improves reconstruction compared to single-mask SSDU at R=8, as well as CG-SENSE at R=2.



### Towards Visually Explaining Similarity Models
- **Arxiv ID**: http://arxiv.org/abs/2008.06035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06035v2)
- **Published**: 2020-08-13 17:47:41+00:00
- **Updated**: 2020-10-13 17:00:38+00:00
- **Authors**: Meng Zheng, Srikrishna Karanam, Terrence Chen, Richard J. Radke, Ziyan Wu
- **Comment**: 13 pages, 10 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:1911.07381
- **Journal**: None
- **Summary**: We consider the problem of visually explaining similarity models, i.e., explaining why a model predicts two images to be similar in addition to producing a scalar score. While much recent work in visual model interpretability has focused on gradient-based attention, these methods rely on a classification module to generate visual explanations. Consequently, they cannot readily explain other kinds of models that do not use or need classification-like loss functions (e.g., similarity models trained with a metric learning loss). In this work, we bridge this crucial gap, presenting a method to generate gradient-based visual attention for image similarity predictors. By relying solely on the learned feature embedding, we show that our approach can be applied to any kind of CNN-based similarity architecture, an important step towards generic visual explainability. We show that our resulting attention maps serve more than just interpretability; they can be infused into the model learning process itself with new trainable constraints. We show that the resulting similarity models perform, and can be visually explained, better than the corresponding baseline models trained without these constraints. We demonstrate our approach using extensive experiments on three different kinds of tasks: generic image retrieval, person re-identification, and low-shot semantic segmentation.



### DSDNet: Deep Structured self-Driving Network
- **Arxiv ID**: http://arxiv.org/abs/2008.06041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06041v1)
- **Published**: 2020-08-13 17:54:06+00:00
- **Updated**: 2020-08-13 17:54:06+00:00
- **Authors**: Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, Raquel Urtasun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.



### Full-Body Awareness from Partial Observations
- **Arxiv ID**: http://arxiv.org/abs/2008.06046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06046v1)
- **Published**: 2020-08-13 17:59:11+00:00
- **Updated**: 2020-08-13 17:59:11+00:00
- **Authors**: Chris Rockwell, David F. Fouhey
- **Comment**: In ECCV 2020
- **Journal**: None
- **Summary**: There has been great progress in human 3D mesh recovery and great interest in learning about the world from consumer video data. Unfortunately current methods for 3D human mesh recovery work rather poorly on consumer video data, since on the Internet, unusual camera viewpoints and aggressive truncations are the norm rather than a rarity. We study this problem and make a number of contributions to address it: (i) we propose a simple but highly effective self-training framework that adapts human 3D mesh recovery systems to consumer videos and demonstrate its application to two recent systems; (ii) we introduce evaluation protocols and keypoint annotations for 13K frames across four consumer video datasets for studying this task, including evaluations on out-of-image keypoints; and (iii) we show that our method substantially improves PCK and human-subject judgments compared to baselines, both on test videos from the dataset it was trained on, as well as on three other datasets without further adaptation. Project website: https://crockwell.github.io/partial_humans



### Semantically Adversarial Learnable Filters
- **Arxiv ID**: http://arxiv.org/abs/2008.06069v3
- **DOI**: 10.1109/TIP.2021.3112290
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06069v3)
- **Published**: 2020-08-13 18:12:40+00:00
- **Updated**: 2022-04-05 21:03:21+00:00
- **Authors**: Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro
- **Comment**: 13 pages
- **Journal**: IEEE Transactions on Image Processing, 2021
- **Summary**: We present an adversarial framework to craft perturbations that mislead classifiers by accounting for the image content and the semantics of the labels. The proposed framework combines a structure loss and a semantic adversarial loss in a multi-task objective function to train a fully convolutional neural network. The structure loss helps generate perturbations whose type and magnitude are defined by a target image processing filter. The semantic adversarial loss considers groups of (semantic) labels to craft perturbations that prevent the filtered image {from} being classified with a label in the same group. We validate our framework with three different target filters, namely detail enhancement, log transformation and gamma correction filters; and evaluate the adversarially filtered images against three classifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show that the proposed framework generates filtered images with a high success rate, robustness, and transferability to unseen classifiers. We also discuss objective and subjective evaluations of the adversarial perturbations.



### MIXCAPS: A Capsule Network-based Mixture of Experts for Lung Nodule Malignancy Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.06072v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06072v1)
- **Published**: 2020-08-13 18:16:07+00:00
- **Updated**: 2020-08-13 18:16:07+00:00
- **Authors**: Parnian Afshar, Farnoosh Naderkhani, Anastasia Oikonomou, Moezedin Javad Rafiee, Arash Mohammadi, Konstantinos N. Plataniotis
- **Comment**: None
- **Journal**: None
- **Summary**: Lung diseases including infections such as Pneumonia, Tuberculosis, and novel Coronavirus (COVID-19), together with Lung Cancer are significantly widespread and are, typically, considered life threatening. In particular, lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, deep learning radiomics solutions have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNN) such as their inability to recognize detailed spatial relations. Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule network's capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network and a mixture of CNNs, with an accuracy of 92.88%, sensitivity of 93.2%, specificity of 92.3% and area under the curve of 0.963. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs.



### Novelty Detection Through Model-Based Characterization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.06094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06094v1)
- **Published**: 2020-08-13 20:03:25+00:00
- **Updated**: 2020-08-13 20:03:25+00:00
- **Authors**: Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib
- **Comment**: IEEE International Conference on Image Processing (ICIP) 2020
- **Journal**: None
- **Summary**: In this paper, we propose a model-based characterization of neural networks to detect novel input types and conditions. Novelty detection is crucial to identify abnormal inputs that can significantly degrade the performance of machine learning algorithms. Majority of existing studies have focused on activation-based representations to detect abnormal inputs, which limits the characterization of abnormality from a data perspective. However, a model perspective can also be informative in terms of the novelties and abnormalities. To articulate the significance of the model perspective in novelty detection, we utilize backpropagated gradients. We conduct a comprehensive analysis to compare the representation capability of gradients with that of activation and show that the gradients outperform the activation in novel class and condition detection. We validate our approach using four image recognition datasets including MNIST, Fashion-MNIST, CIFAR-10, and CURE-TSR. We achieve a significant improvement on all four datasets with an average AUROC of 0.953, 0.918, 0.582, and 0.746, respectively.



### Geometric Deep Learning for Post-Menstrual Age Prediction based on the Neonatal White Matter Cortical Surface
- **Arxiv ID**: http://arxiv.org/abs/2008.06098v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06098v2)
- **Published**: 2020-08-13 20:15:03+00:00
- **Updated**: 2020-09-27 22:01:44+00:00
- **Authors**: Vitalis Vosylius, Andy Wang, Cemlyn Waters, Alexey Zakharov, Francis Ward, Loic Le Folgoc, John Cupitt, Antonios Makropoulos, Andreas Schuh, Daniel Rueckert, Amir Alansary
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate estimation of the age in neonates is essential for measuring neurodevelopmental, medical, and growth outcomes. In this paper, we propose a novel approach to predict the post-menstrual age (PA) at scan, using techniques from geometric deep learning, based on the neonatal white matter cortical surface. We utilize and compare multiple specialized neural network architectures that predict the age using different geometric representations of the cortical surface; we compare MeshCNN, Pointnet++, GraphCNN, and a volumetric benchmark. The dataset is part of the Developing Human Connectome Project (dHCP), and is a cohort of healthy and premature neonates. We evaluate our approach on 650 subjects (727scans) with PA ranging from 27 to 45 weeks. Our results show accurate prediction of the estimated PA, with mean error less than one week.



### Effect of Architectures and Training Methods on the Performance of Learned Video Frame Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.06106v1
- **DOI**: 10.1109/ICIP.2019.8803624
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06106v1)
- **Published**: 2020-08-13 20:45:28+00:00
- **Updated**: 2020-08-13 20:45:28+00:00
- **Authors**: M. Akin Yilmaz, A. Murat Tekalp
- **Comment**: Accepted for publication at IEEE ICIP 2019
- **Journal**: None
- **Summary**: We analyze the performance of feedforward vs. recurrent neural network (RNN) architectures and associated training methods for learned frame prediction. To this effect, we trained a residual fully convolutional neural network (FCNN), a convolutional RNN (CRNN), and a convolutional long short-term memory (CLSTM) network for next frame prediction using the mean square loss. We performed both stateless and stateful training for recurrent networks. Experimental results show that the residual FCNN architecture performs the best in terms of peak signal to noise ratio (PSNR) at the expense of higher training and test (inference) computational complexity. The CRNN can be trained stably and very efficiently using the stateful truncated backpropagation through time procedure, and it requires an order of magnitude less inference runtime to achieve near real-time frame prediction with an acceptable performance.



### Can weight sharing outperform random architecture search? An investigation with TuNAS
- **Arxiv ID**: http://arxiv.org/abs/2008.06120v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.06120v1)
- **Published**: 2020-08-13 21:32:40+00:00
- **Updated**: 2020-08-13 21:32:40+00:00
- **Authors**: Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, Quoc Le
- **Comment**: Published at CVPR 2020
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2020, pp. 14323-14332
- **Summary**: Efficient Neural Architecture Search methods based on weight sharing have shown good promise in democratizing Neural Architecture Search for computer vision models. There is, however, an ongoing debate whether these efficient methods are significantly better than random search. Here we perform a thorough comparison between efficient and random search methods on a family of progressively larger and more challenging search spaces for image classification and detection on ImageNet and COCO. While the efficacies of both methods are problem-dependent, our experiments demonstrate that there are large, realistic tasks where efficient search methods can provide substantial gains over random search. In addition, we propose and evaluate techniques which improve the quality of searched architectures and reduce the need for manual hyper-parameter tuning.   Source code and experiment data are available at https://github.com/google-research/google-research/tree/master/tunas



### 3D Bird Reconstruction: a Dataset, Model, and Shape Recovery from a Single View
- **Arxiv ID**: http://arxiv.org/abs/2008.06133v1
- **DOI**: 10.1007/978-3-030-58523-5_1
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2008.06133v1)
- **Published**: 2020-08-13 23:29:04+00:00
- **Updated**: 2020-08-13 23:29:04+00:00
- **Authors**: Marc Badger, Yufu Wang, Adarsh Modh, Ammon Perkes, Nikos Kolotouros, Bernd G. Pfrommer, Marc F. Schmidt, Kostas Daniilidis
- **Comment**: In ECCV 2020
- **Journal**: ECCV 2020, vol 12363, pp 1-17
- **Summary**: Automated capture of animal pose is transforming how we study neuroscience and social behavior. Movements carry important social cues, but current methods are not able to robustly estimate pose and shape of animals, particularly for social animals such as birds, which are often occluded by each other and objects in the environment. To address this problem, we first introduce a model and multi-view optimization approach, which we use to capture the unique shape and pose space displayed by live birds. We then introduce a pipeline and experiments for keypoint, mask, pose, and shape regression that recovers accurate avian postures from single views. Finally, we provide extensive multi-view keypoint and mask annotations collected from a group of 15 social birds housed together in an outdoor aviary. The project website with videos, results, code, mesh model, and the Penn Aviary Dataset can be found at https://marcbadger.github.io/avian-mesh.



