# Arxiv Papers in cs.CV on 2020-08-30
### MDCN: Multi-scale Dense Cross Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.13084v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.13084v1)
- **Published**: 2020-08-30 03:50:19+00:00
- **Updated**: 2020-08-30 03:50:19+00:00
- **Authors**: Juncheng Li, Faming Fang, Jiaqian Li, Kangfu Mei, Guixu Zhang
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: Convolutional neural networks have been proven to be of great benefit for single-image super-resolution (SISR). However, previous works do not make full use of multi-scale features and ignore the inter-scale correlation between different upsampling factors, resulting in sub-optimal performance. Instead of blindly increasing the depth of the network, we are committed to mining image features and learning the inter-scale correlation between different upsampling factors. To achieve this, we propose a Multi-scale Dense Cross Network (MDCN), which achieves great performance with fewer parameters and less execution time. MDCN consists of multi-scale dense cross blocks (MDCBs), hierarchical feature distillation block (HFDB), and dynamic reconstruction block (DRB). Among them, MDCB aims to detect multi-scale features and maximize the use of image features flow at different scales, HFDB focuses on adaptively recalibrate channel-wise feature responses to achieve feature distillation, and DRB attempts to reconstruct SR images with different upsampling factors in a single model. It is worth noting that all these modules can run independently. It means that these modules can be selectively plugged into any CNN model to improve model performance. Extensive experiments show that MDCN achieves competitive results in SISR, especially in the reconstruction task with multiple upsampling factors. The code will be provided at https://github.com/MIVRC/MDCN-PyTorch.



### Transfer Learning-based Road Damage Detection for Multiple Countries
- **Arxiv ID**: http://arxiv.org/abs/2008.13101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2008.13101v1)
- **Published**: 2020-08-30 06:48:00+00:00
- **Updated**: 2020-08-30 06:48:00+00:00
- **Authors**: Deeksha Arya, Hiroya Maeda, Sanjay Kumar Ghosh, Durga Toshniwal, Alexander Mraz, Takehiro Kashiyama, Yoshihide Sekimoto
- **Comment**: 16 pages, 14 figures
- **Journal**: None
- **Summary**: Many municipalities and road authorities seek to implement automated evaluation of road damage. However, they often lack technology, know-how, and funds to afford state-of-the-art equipment for data collection and analysis of road damages. Although some countries, like Japan, have developed less expensive and readily available Smartphone-based methods for automatic road condition monitoring, other countries still struggle to find efficient solutions. This work makes the following contributions in this context. Firstly, it assesses the usability of the Japanese model for other countries. Secondly, it proposes a large-scale heterogeneous road damage dataset comprising 26620 images collected from multiple countries using smartphones. Thirdly, we propose generalized models capable of detecting and classifying road damages in more than one country. Lastly, we provide recommendations for readers, local agencies, and municipalities of other countries when one other country publishes its data and model for automatic road damage detection and classification. Our dataset is available at (https://github.com/sekilab/RoadDamageDetector/).



### Deep Hypergraph U-Net for Brain Graph Embedding and Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.13118v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.13118v1)
- **Published**: 2020-08-30 08:15:18+00:00
- **Updated**: 2020-08-30 08:15:18+00:00
- **Authors**: Mert Lostar, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: -Background. Network neuroscience examines the brain as a complex system represented by a network (or connectome), providing deeper insights into the brain morphology and function, allowing the identification of atypical brain connectivity alterations, which can be used as diagnostic markers of neurological disorders. -Existing Methods. Graph embedding methods which map data samples (e.g., brain networks) into a low dimensional space have been widely used to explore the relationship between samples for classification or prediction tasks. However, the majority of these works are based on modeling the pair-wise relationships between samples, failing to capture their higher-order relationships. -New Method. In this paper, inspired by the nascent field of geometric deep learning, we propose Hypergraph U-Net (HUNet), a novel data embedding framework leveraging the hypergraph structure to learn low-dimensional embeddings of data samples while capturing their high-order relationships. Specifically, we generalize the U-Net architecture, naturally operating on graphs, to hypergraphs by improving local feature aggregation and preserving the high-order relationships present in the data. -Results. We tested our method on small-scale and large-scale heterogeneous brain connectomic datasets including morphological and functional brain networks of autistic and demented patients, respectively. -Conclusion. Our HUNet outperformed state-of-the-art geometric graph and hypergraph data embedding techniques with a gain of 4-14% in classification accuracy, demonstrating both scalability and generalizability. HUNet code is available at https://github.com/basiralab/HUNet.



### Learn by Observation: Imitation Learning for Drone Patrolling from Videos of A Human Navigator
- **Arxiv ID**: http://arxiv.org/abs/2008.13193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.13193v1)
- **Published**: 2020-08-30 15:20:40+00:00
- **Updated**: 2020-08-30 15:20:40+00:00
- **Authors**: Yue Fan, Shilei Chu, Wei Zhang, Ran Song, Yibin Li
- **Comment**: Accepted by IROS 2020
- **Journal**: None
- **Summary**: We present an imitation learning method for autonomous drone patrolling based only on raw videos. Different from previous methods, we propose to let the drone learn patrolling in the air by observing and imitating how a human navigator does it on the ground. The observation process enables the automatic collection and annotation of data using inter-frame geometric consistency, resulting in less manual effort and high accuracy. Then a newly designed neural network is trained based on the annotated data to predict appropriate directions and translations for the drone to patrol in a lane-keeping manner as humans. Our method allows the drone to fly at a high altitude with a broad view and low risk. It can also detect all accessible directions at crossroads and further carry out the integration of available user instructions and autonomous patrolling control commands. Extensive experiments are conducted to demonstrate the accuracy of the proposed imitating learning process as well as the reliability of the holistic system for autonomous drone navigation. The codes, datasets as well as video demonstrations are available at https://vsislab.github.io/uavpatrol



### Finding Action Tubes with a Sparse-to-Dense Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.13196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.13196v1)
- **Published**: 2020-08-30 15:38:44+00:00
- **Updated**: 2020-08-30 15:38:44+00:00
- **Authors**: Yuxi Li, Weiyao Lin, Tao Wang, John See, Rui Qian, Ning Xu, Limin Wang, Shugong Xu
- **Comment**: 5 figures; AAAI 2020
- **Journal**: None
- **Summary**: The task of spatial-temporal action detection has attracted increasing attention among researchers. Existing dominant methods solve this problem by relying on short-term information and dense serial-wise detection on each individual frames or clips. Despite their effectiveness, these methods showed inadequate use of long-term information and are prone to inefficiency. In this paper, we propose for the first time, an efficient framework that generates action tube proposals from video streams with a single forward pass in a sparse-to-dense manner. There are two key characteristics in this framework: (1) Both long-term and short-term sampled information are explicitly utilized in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS) is designed to effectively approximate the tube output while keeping the system tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and UCFSports benchmark datasets, achieving promising results that are competitive to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our framework about 7.6 times more efficient than the nearest competitor.



### A Compact Deep Architecture for Real-time Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.13227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.13227v1)
- **Published**: 2020-08-30 17:47:16+00:00
- **Updated**: 2020-08-30 17:47:16+00:00
- **Authors**: Samad Zabihi, Hamed Rezazadegan Tavakoli, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency computation models aim to imitate the attention mechanism in the human visual system. The application of deep neural networks for saliency prediction has led to a drastic improvement over the last few years. However, deep models have a high number of parameters which makes them less suitable for real-time applications. Here we propose a compact yet fast model for real-time saliency prediction. Our proposed model consists of a modified U-net architecture, a novel fully connected layer, and central difference convolutional layers. The modified U-Net architecture promotes compactness and efficiency. The novel fully-connected layer facilitates the implicit capturing of the location-dependent information. Using the central difference convolutional layers at different scales enables capturing more robust and biologically motivated features. We compare our model with state of the art saliency models using traditional saliency scores as well as our newly devised scheme. Experimental results over four challenging saliency benchmark datasets demonstrate the effectiveness of our approach in striking a balance between accuracy and speed. Our model can be run in real-time which makes it appealing for edge devices and video processing.



### An evolutionary perspective on the design of neuromorphic shape filters
- **Arxiv ID**: http://arxiv.org/abs/2008.13229v1
- **DOI**: 10.1109/ACCESS.2020_3004412
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.13229v1)
- **Published**: 2020-08-30 17:53:44+00:00
- **Updated**: 2020-08-30 17:53:44+00:00
- **Authors**: Ernest Greene
- **Comment**: None
- **Journal**: IEEE Access, 2020, 8, 114228-114238
- **Summary**: A substantial amount of time and energy has been invested to develop machine vision using connectionist (neural network) principles. Most of that work has been inspired by theories advanced by neuroscientists and behaviorists for how cortical systems store stimulus information. Those theories call for information flow through connections among several neuron populations, with the initial connections being random (or at least non-functional). Then the strength or location of connections are modified through training trials to achieve an effective output, such as the ability to identify an object. Those theories ignored the fact that animals that have no cortex, e.g., fish, can demonstrate visual skills that outpace the best neural network models. Neural circuits that allow for immediate effective vision and quick learning have been preprogrammed by hundreds of millions of years of evolution and the visual skills are available shortly after hatching. Cortical systems may be providing advanced image processing, but most likely are using design principles that had been proven effective in simpler systems. The present article provides a brief overview of retinal and cortical mechanisms for registering shape information, with the hope that it might contribute to the design of shape-encoding circuits that more closely match the mechanisms of biological vision.



### Deep Volumetric Universal Lesion Detection using Light-Weight Pseudo 3D Convolution and Surface Point Regression
- **Arxiv ID**: http://arxiv.org/abs/2008.13254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.13254v1)
- **Published**: 2020-08-30 19:42:06+00:00
- **Updated**: 2020-08-30 19:42:06+00:00
- **Authors**: Jinzheng Cai, Ke Yan, Chi-Tung Cheng, Jing Xiao, Chien-Hung Liao, Le Lu, Adam P. Harrison
- **Comment**: Accepted by MICCAI2020
- **Journal**: None
- **Summary**: Identifying, measuring and reporting lesions accurately and comprehensively from patient CT scans are important yet time-consuming procedures for physicians. Computer-aided lesion/significant-findings detection techniques are at the core of medical imaging, which remain very challenging due to the tremendously large variability of lesion appearance, location and size distributions in 3D imaging. In this work, we propose a novel deep anchor-free one-stage VULD framework that incorporates (1) P3DC operators to recycle the architectural configurations and pre-trained weights from the off-the-shelf 2D networks, especially ones with large capacities to cope with data variance, and (2) a new SPR method to effectively regress the 3D lesion spatial extents by pinpointing their representative key points on lesion surfaces. Experimental validations are first conducted on the public large-scale NIH DeepLesion dataset where our proposed method delivers new state-of-the-art quantitative performance. We also test VULD on our in-house dataset for liver tumor detection. VULD generalizes well in both large-scale and small-sized tumor datasets in CT imaging.



