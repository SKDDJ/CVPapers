# Arxiv Papers in cs.CV on 2020-08-25
### Channel-Directed Gradients for Optimization of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10766v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.10766v1)
- **Published**: 2020-08-25 00:44:09+00:00
- **Updated**: 2020-08-25 00:44:09+00:00
- **Authors**: Dong Lao, Peihao Zhu, Peter Wonka, Ganesh Sundaramoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce optimization methods for convolutional neural networks that can be used to improve existing gradient-based optimization in terms of generalization error. The method requires only simple processing of existing stochastic gradients, can be used in conjunction with any optimizer, and has only a linear overhead (in the number of parameters) compared to computation of the stochastic gradient. The method works by computing the gradient of the loss function with respect to output-channel directed re-weighted L2 or Sobolev metrics, which has the effect of smoothing components of the gradient across a certain direction of the parameter tensor. We show that defining the gradients along the output channel direction leads to a performance boost, while other directions can be detrimental. We present the continuum theory of such gradients, its discretization, and application to deep networks. Experiments on benchmark datasets, several networks and baseline optimizers show that optimizers can be improved in generalization error by simply computing the stochastic gradient with respect to output-channel directed metrics.



### Image Colorization: A Survey and Dataset
- **Arxiv ID**: http://arxiv.org/abs/2008.10774v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10774v3)
- **Published**: 2020-08-25 01:22:52+00:00
- **Updated**: 2022-01-27 02:16:15+00:00
- **Authors**: Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz Khan, Abdul Wahab Muzaffar
- **Comment**: None
- **Journal**: None
- **Summary**: Image colorization is the process of estimating RGB colors for grayscale images or video frames to improve their aesthetic and perceptual quality. Deep learning techniques for image colorization have progressed notably over the last decade, calling the need for a systematic survey and benchmarking of these techniques. This article presents a comprehensive survey of recent state-of-the-art deep learning-based image colorization techniques, describing their fundamental block architectures, inputs, optimizers, loss functions, training protocols, and training data \textit{etc.} It categorizes the existing colorization techniques into seven classes and discusses important factors governing their performance, such as benchmark datasets and evaluation metrics. We highlight the limitations of existing datasets and introduce a new dataset specific to colorization. Using the existing datasets and our new one, we perform an extensive experimental evaluation of existing image colorization methods. Finally, we discuss the limitations of existing methods and recommend possible solutions as well as future research directions for this rapidly evolving topic of deep image colorization. Dataset and codes for evaluation are publicly available at https://github.com/saeed-anwar/ColorSurvey



### Learning Target Domain Specific Classifier for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.10785v1
- **DOI**: 10.1109/TNNLS.2020.2995648
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10785v1)
- **Published**: 2020-08-25 02:28:24+00:00
- **Updated**: 2020-08-25 02:28:24+00:00
- **Authors**: Chuan-Xian Ren, Pengfei Ge, Peiyi Yang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation~(UDA) aims at reducing the distribution discrepancy when transferring knowledge from a labeled source domain to an unlabeled target domain. Previous UDA methods assume that the source and target domains share an identical label space, which is unrealistic in practice since the label information of the target domain is agnostic. This paper focuses on a more realistic UDA scenario, i.e. partial domain adaptation (PDA), where the target label space is subsumed to the source label space. In the PDA scenario, the source outliers that are absent in the target domain may be wrongly matched to the target domain (technically named negative transfer), leading to performance degradation of UDA methods. This paper proposes a novel Target Domain Specific Classifier Learning-based Domain Adaptation (TSCDA) method. TSCDA presents a soft-weighed maximum mean discrepancy criterion to partially align feature distributions and alleviate negative transfer. Also, it learns a target-specific classifier for the target domain with pseudo-labels and multiple auxiliary classifiers, to further address classifier shift. A module named Peers Assisted Learning is used to minimize the prediction difference between multiple target-specific classifiers, which makes the classifiers more discriminant for the target domain. Extensive experiments conducted on three PDA benchmark datasets show that TSCDA outperforms other state-of-the-art methods with a large margin, e.g. $4\%$ and $5.6\%$ averagely on Office-31 and Office-Home, respectively.



### Dynamic Future Net: Diversified Human Motion Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.05109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05109v1)
- **Published**: 2020-08-25 02:31:41+00:00
- **Updated**: 2020-08-25 02:31:41+00:00
- **Authors**: Wenheng Chen, He Wang, Yi Yuan, Tianjia Shao, Kun Zhou
- **Comment**: Accepted by ACMMM 2020
- **Journal**: None
- **Summary**: Human motion modelling is crucial in many areas such as computer graphics, vision and virtual reality. Acquiring high-quality skeletal motions is difficult due to the need for specialized equipment and laborious manual post-posting, which necessitates maximizing the use of existing data to synthesize new data. However, it is a challenge due to the intrinsic motion stochasticity of human motion dynamics, manifested in the short and long terms. In the short term, there is strong randomness within a couple frames, e.g. one frame followed by multiple possible frames leading to different motion styles; while in the long term, there are non-deterministic action transitions. In this paper, we present Dynamic Future Net, a new deep learning model where we explicitly focuses on the aforementioned motion stochasticity by constructing a generative model with non-trivial modelling capacity in temporal stochasticity. Given limited amounts of data, our model can generate a large number of high-quality motions with arbitrary duration, and visually-convincing variations in both space and time. We evaluate our model on a wide range of motions and compare it with the state-of-the-art methods. Both qualitative and quantitative results show the superiority of our method, for its robustness, versatility and high-quality.



### Data Science for Motion and Time Analysis with Modern Motion Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/2008.10786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2008.10786v1)
- **Published**: 2020-08-25 02:33:33+00:00
- **Updated**: 2020-08-25 02:33:33+00:00
- **Authors**: Chiwoo Park, Sang Do Noh, Anuj Srivastava
- **Comment**: Keywords: motion and time study, motion sensors, Riemannian manifold,
  probability distribution on manifold, temporal evolution of probability
  distributions
- **Journal**: None
- **Summary**: The motion-and-time analysis has been a popular research topic in operations research, especially for analyzing work performances in manufacturing and service operations. It is regaining attention as continuous improvement tools for lean manufacturing and smart factory. This paper develops a framework for data-driven analysis of work motions and studies their correlations to work speeds or execution rates, using data collected from modern motion sensors. The past analyses largely relied on manual steps involving time-consuming stop-watching and video-taping, followed by manual data analysis. While modern sensing devices have automated the collection of motion data, the motion analytics that transform the new data into knowledge are largely underdeveloped. Unsolved technical questions include: How the motion and time information can be extracted from the motion sensor data, how work motions and execution rates are statistically modeled and compared, and what are the statistical correlations of motions to the rates? In this paper, we develop a novel mathematical framework for motion and time analysis with motion sensor data, by defining new mathematical representation spaces of human motions and execution rates and by developing statistical tools on these new spaces. This methodological research is demonstrated using five use cases applied to manufacturing motion data.



### Towards Structured Prediction in Bioinformatics with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.11546v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11546v1)
- **Published**: 2020-08-25 02:52:18+00:00
- **Updated**: 2020-08-25 02:52:18+00:00
- **Authors**: Yu Li
- **Comment**: PhD dissertatation
- **Journal**: None
- **Summary**: Using machine learning, especially deep learning, to facilitate biological research is a fascinating research direction. However, in addition to the standard classification or regression problems, in bioinformatics, we often need to predict more complex structured targets, such as 2D images and 3D molecular structures. The above complex prediction tasks are referred to as structured prediction. Structured prediction is more complicated than the traditional classification but has much broader applications, considering that most of the original bioinformatics problems have complex output objects. Due to the properties of those structured prediction problems, such as having problem-specific constraints and dependency within the labeling space, the straightforward application of existing deep learning models can lead to unsatisfactory results. Here, we argue that the following ideas can help resolve structured prediction problems in bioinformatics. Firstly, we can combine deep learning with other classic algorithms, such as probabilistic graphical models, which model the problem structure explicitly. Secondly, we can design the problem-specific deep learning architectures or methods by considering the structured labeling space and problem constraints, either explicitly or implicitly. We demonstrate our ideas with six projects from four bioinformatics subfields, including sequencing analysis, structure prediction, function annotation, and network analysis. The structured outputs cover 1D signals, 2D images, 3D structures, hierarchical labeling, and heterogeneous networks. With the help of the above ideas, all of our methods can achieve SOTA performance on the corresponding problems. The success of these projects motivates us to extend our work towards other more challenging but important problems, such as health-care problems, which can directly benefit people's health and wellness.



### Deep Variational Network Toward Blind Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2008.10796v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.10796v3)
- **Published**: 2020-08-25 03:30:53+00:00
- **Updated**: 2022-06-28 12:50:14+00:00
- **Authors**: Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, Deyu Meng, Kwan-Yen K. Wong
- **Comment**: Extended Version of VDNet (NeurIPS 2019)
- **Journal**: None
- **Summary**: Blind image restoration (IR) is a common yet challenging problem in computer vision. Classical model-based methods and recent deep learning (DL)-based methods represent two different methodologies for this problem, each with their own merits and drawbacks. In this paper, we propose a novel blind image restoration method, aiming to integrate both the advantages of them. Specifically, we construct a general Bayesian generative model for the blind IR, which explicitly depicts the degradation process. In this proposed model, a pixel-wise non-i.i.d. Gaussian distribution is employed to fit the image noise. It is with more flexibility than the simple i.i.d. Gaussian or Laplacian distributions as adopted in most of conventional methods, so as to handle more complicated noise types contained in the image degradation. To solve the model, we design a variational inference algorithm where all the expected posteriori distributions are parameterized as deep neural networks to increase their model capability. Notably, such an inference algorithm induces a unified framework to jointly deal with the tasks of degradation estimation and image restoration. Further, the degradation information estimated in the former task is utilized to guide the latter IR process. Experiments on two typical blind IR tasks, namely image denoising and super-resolution, demonstrate that the proposed method achieves superior performance over current state-of-the-arts.



### New Directions in Distributed Deep Learning: Bringing the Network at Forefront of IoT Design
- **Arxiv ID**: http://arxiv.org/abs/2008.10805v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.10805v1)
- **Published**: 2020-08-25 04:08:10+00:00
- **Updated**: 2020-08-25 04:08:10+00:00
- **Authors**: Kartikeya Bhardwaj, Wei Chen, Radu Marculescu
- **Comment**: This preprint is for personal use only. The official article will
  appear in proceedings of Design Automation Conference (DAC), 2020. This work
  was presented at the DAC 2020 special session on Edge-to-Cloud Neural
  Networks for Machine Learning Applications in Future IoT Systems
- **Journal**: None
- **Summary**: In this paper, we first highlight three major challenges to large-scale adoption of deep learning at the edge: (i) Hardware-constrained IoT devices, (ii) Data security and privacy in the IoT era, and (iii) Lack of network-aware deep learning algorithms for distributed inference across multiple IoT devices. We then provide a unified view targeting three research directions that naturally emerge from the above challenges: (1) Federated learning for training deep networks, (2) Data-independent deployment of learning algorithms, and (3) Communication-aware distributed inference. We believe that the above research directions need a network-centric approach to enable the edge intelligence and, therefore, fully exploit the true potential of IoT.



### 3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.11576v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11576v2)
- **Published**: 2020-08-25 04:32:29+00:00
- **Updated**: 2020-11-29 04:59:08+00:00
- **Authors**: Rupal Agravat, Mehul S Raval
- **Comment**: 11 pages, 3 figures, BRaTS 2020. arXiv admin note: text overlap with
  arXiv:1909.09399
- **Journal**: LNCS, Springer, 2021
- **Summary**: Glioma, the malignant brain tumor, requires immediate treatment to improve the survival of patients. Gliomas heterogeneous nature makes the segmentation difficult, especially for sub-regions like necrosis, enhancing tumor, non-enhancing tumor, and Edema. Deep neural networks like full convolution neural networks and ensemble of fully convolution neural networks are successful for Glioma segmentation. The paper demonstrates the use of a 3D fully convolution neural network with a three layer encoder decoder approach for layer arrangement. The encoder blocks include the dense modules, and decoder blocks include convolution modules. The input to the network is 3D patches. The loss function combines dice loss and focal loss functions. The validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor uses shape, volumetric, and age features extracted from ground truth for overall survival prediction. The regressor achieves an accuracy of 44.8% on the validation set.



### A Critical Analysis of Patch Similarity Based Image Denoising Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2008.10824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10824v1)
- **Published**: 2020-08-25 05:30:37+00:00
- **Updated**: 2020-08-25 05:30:37+00:00
- **Authors**: Varuna De Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is a classical signal processing problem that has received significant interest within the image processing community during the past two decades. Most of the algorithms for image denoising has focused on the paradigm of non-local similarity, where image blocks in the neighborhood that are similar, are collected to build a basis for reconstruction. Through rigorous experimentation, this paper reviews multiple aspects of image denoising algorithm development based on non-local similarity. Firstly, the concept of non-local similarity as a foundational quality that exists in natural images has not received adequate attention. Secondly, the image denoising algorithms that are developed are a combination of multiple building blocks, making comparison among them a tedious task. Finally, most of the work surrounding image denoising presents performance results based on Peak-Signal-to-Noise Ratio (PSNR) between a denoised image and a reference image (which is perturbed with Additive White Gaussian Noise). This paper starts with a statistical analysis on non-local similarity and its effectiveness under various noise levels, followed by a theoretical comparison of different state-of-the-art image denoising algorithms. Finally, we argue for a methodological overhaul to incorporate no-reference image quality measures and unprocessed images (raw) during performance evaluation of image denoising algorithms.



### Weakly Supervised Learning with Side Information for Noisy Labeled Images
- **Arxiv ID**: http://arxiv.org/abs/2008.11586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11586v2)
- **Published**: 2020-08-25 05:43:48+00:00
- **Updated**: 2020-09-04 08:02:28+00:00
- **Authors**: Lele Cheng, Xiangzeng Zhou, Liming Zhao, Dangwei Li, Hong Shang, Yun Zheng, Pan Pan, Yinghui Xu
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: In many real-world datasets, like WebVision, the performance of DNN based classifier is often limited by the noisy labeled data. To tackle this problem, some image related side information, such as captions and tags, often reveal underlying relationships across images. In this paper, we present an efficient weakly supervised learning by using a Side Information Network (SINet), which aims to effectively carry out a large scale classification with severely noisy labels. The proposed SINet consists of a visual prototype module and a noise weighting module. The visual prototype module is designed to generate a compact representation for each category by introducing the side information. The noise weighting module aims to estimate the correctness of each noisy image and produce a confidence score for image ranking during the training procedure. The propsed SINet can largely alleviate the negative impact of noisy image labels, and is beneficial to train a high performance CNN based classifier. Besides, we released a fine-grained product dataset called AliProducts, which contains more than 2.5 million noisy web images crawled from the internet by using queries generated from 50,000 fine-grained semantic classes. Extensive experiments on several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our proposed AliProducts achieve state-of-the-art performance. The SINet has won the first place in the classification task on WebVision Challenge 2019, and outperformed other competitors by a large margin.



### CDeC-Net: Composite Deformable Cascade Network for Table Detection in Document Images
- **Arxiv ID**: http://arxiv.org/abs/2008.10831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10831v1)
- **Published**: 2020-08-25 05:53:59+00:00
- **Updated**: 2020-08-25 05:53:59+00:00
- **Authors**: Madhav Agarwal, Ajoy Mondal, C. V. Jawahar
- **Comment**: 12
- **Journal**: None
- **Summary**: Localizing page elements/objects such as tables, figures, equations, etc. is the primary step in extracting information from document images. We propose a novel end-to-end trainable deep network, (CDeC-Net) for detecting tables present in the documents. The proposed network consists of a multistage extension of Mask R-CNN with a dual backbone having deformable convolution for detecting tables varying in scale with high detection accuracy at higher IoU threshold. We empirically evaluate CDeC-Net on all the publicly available benchmark datasets - ICDAR-2013, ICDAR-2017, ICDAR-2019,UNLV, Marmot, PubLayNet, and TableBank - with extensive experiments.   Our solution has three important properties: (i) a single trained model CDeC-Net{\ddag} performs well across all the popular benchmark datasets; (ii) we report excellent performances across multiple, including higher, thresholds of IoU; (iii) by following the same protocol of the recent papers for each of the benchmarks, we consistently demonstrate the superior quantitative performance. Our code and models will be publicly released for enabling the reproducibility of the results.



### Adaptive Context-Aware Multi-Modal Network for Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2008.10833v1
- **DOI**: 10.1109/TIP.2021.3079821
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10833v1)
- **Published**: 2020-08-25 06:00:06+00:00
- **Updated**: 2020-08-25 06:00:06+00:00
- **Authors**: Shanshan Zhao, Mingming Gong, Huan Fu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Depth completion aims to recover a dense depth map from the sparse depth data and the corresponding single RGB image. The observed pixels provide the significant guidance for the recovery of the unobserved pixels' depth. However, due to the sparsity of the depth data, the standard convolution operation, exploited by most of existing methods, is not effective to model the observed contexts with depth values. To address this issue, we propose to adopt the graph propagation to capture the observed spatial contexts. Specifically, we first construct multiple graphs at different scales from observed pixels. Since the graph structure varies from sample to sample, we then apply the attention mechanism on the propagation, which encourages the network to model the contextual information adaptively. Furthermore, considering the mutli-modality of input data, we exploit the graph propagation on the two modalities respectively to extract multi-modal representations. Finally, we introduce the symmetric gated fusion strategy to exploit the extracted multi-modal features effectively. The proposed strategy preserves the original information for one modality and also absorbs complementary information from the other through learning the adaptive gating weights. Our model, named Adaptive Context-Aware Multi-Modal Network (ACMNet), achieves the state-of-the-art performance on two benchmarks, {\it i.e.}, KITTI and NYU-v2, and at the same time has fewer parameters than latest models. Our code is available at: \url{https://github.com/sshan-zhao/ACMNet}.



### Graphical Object Detection in Document Images
- **Arxiv ID**: http://arxiv.org/abs/2008.10843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10843v1)
- **Published**: 2020-08-25 06:35:57+00:00
- **Updated**: 2020-08-25 06:35:57+00:00
- **Authors**: Ranajit Saha, Ajoy Mondal, C. V. Jawahar
- **Comment**: 8
- **Journal**: ICDAR 2019
- **Summary**: Graphical elements: particularly tables and figures contain a visual summary of the most valuable information contained in a document. Therefore, localization of such graphical objects in the document images is the initial step to understand the content of such graphical objects or document images. In this paper, we present a novel end-to-end trainable deep learning based framework to localize graphical objects in the document images called as Graphical Object Detection (GOD). Our framework is data-driven and does not require any heuristics or meta-data to locate graphical objects in the document images. The GOD explores the concept of transfer learning and domain adaptation to handle scarcity of labeled training images for graphical object detection task in the document images. Performance analysis carried out on the various public benchmark data sets: ICDAR-2013, ICDAR-POD2017,and UNLV shows that our model yields promising results as compared to state-of-the-art techniques.



### Discriminability Distillation in Group Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.10850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10850v2)
- **Published**: 2020-08-25 07:15:51+00:00
- **Updated**: 2020-09-01 07:33:34+00:00
- **Authors**: Manyuan Zhang, Guanglu Song, Hang Zhou, Yu Liu
- **Comment**: To appear in Proceedings of the European Conference on Computer
  Vision (ECCV), 2020
- **Journal**: None
- **Summary**: Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.



### Two-Stream Networks for Lane-Change Prediction of Surrounding Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2008.10869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.10869v1)
- **Published**: 2020-08-25 07:59:15+00:00
- **Updated**: 2020-08-25 07:59:15+00:00
- **Authors**: David Fernández-Llorca, Mahdi Biparva, Rubén Izquierdo-Gonzalo, John K. Tsotsos
- **Comment**: This work has been accepted at the IEEE Intelligent Transportation
  Systems Conference 2020
- **Journal**: None
- **Summary**: In highway scenarios, an alert human driver will typically anticipate early cut-in and cut-out maneuvers of surrounding vehicles using only visual cues. An automated system must anticipate these situations at an early stage too, to increase the safety and the efficiency of its performance. To deal with lane-change recognition and prediction of surrounding vehicles, we pose the problem as an action recognition/prediction problem by stacking visual cues from video cameras. Two video action recognition approaches are analyzed: two-stream convolutional networks and spatiotemporal multiplier networks. Different sizes of the regions around the vehicles are analyzed, evaluating the importance of the interaction between vehicles and the context information in the performance. In addition, different prediction horizons are evaluated. The obtained results demonstrate the potential of these methodologies to serve as robust predictors of future lane-changes of surrounding vehicles in time horizons between 1 and 2 seconds.



### Confidence-aware Adversarial Learning for Self-supervised Semantic Matching
- **Arxiv ID**: http://arxiv.org/abs/2008.10902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10902v1)
- **Published**: 2020-08-25 09:15:48+00:00
- **Updated**: 2020-08-25 09:15:48+00:00
- **Authors**: Shuaiyi Huang, Qiuyue Wang, Xuming He
- **Comment**: PRCV 2020
- **Journal**: None
- **Summary**: In this paper, we aim to address the challenging task of semantic matching where matching ambiguity is difficult to resolve even with learned deep features. We tackle this problem by taking into account the confidence in predictions and develop a novel refinement strategy to correct partial matching errors. Specifically, we introduce a Confidence-Aware Semantic Matching Network (CAMNet) which instantiates two key ideas of our approach. First, we propose to estimate a dense confidence map for a matching prediction through self-supervised learning. Second, based on the estimated confidence, we refine initial predictions by propagating reliable matching to the rest of locations on the image plane. In addition, we develop a new hybrid loss in which we integrate a semantic alignment loss with a confidence loss, and an adversarial loss that measures the quality of semantic correspondence. We are the first that exploit confidence during refinement to improve semantic matching accuracy and develop an end-to-end self-supervised adversarial learning procedure for the entire matching network. We evaluate our method on two public benchmarks, on which we achieve top performance over the prior state of the art. We will release our source code at https://github.com/ShuaiyiHuang/CAMNet.



### MonStereo: When Monocular and Stereo Meet at the Tail of 3D Human Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.10913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10913v2)
- **Published**: 2020-08-25 09:47:58+00:00
- **Updated**: 2021-03-22 16:59:49+00:00
- **Authors**: Lorenzo Bertoni, Sven Kreiss, Taylor Mordan, Alexandre Alahi
- **Comment**: Accepted at the IEEE International Conference on Robotics and
  Automation (ICRA) 2021
- **Journal**: None
- **Summary**: Monocular and stereo visions are cost-effective solutions for 3D human localization in the context of self-driving cars or social robots. However, they are usually developed independently and have their respective strengths and limitations. We propose a novel unified learning framework that leverages the strengths of both monocular and stereo cues for 3D human localization. Our method jointly (i) associates humans in left-right images, (ii) deals with occluded and distant cases in stereo settings by relying on the robustness of monocular cues, and (iii) tackles the intrinsic ambiguity of monocular perspective projection by exploiting prior knowledge of the human height distribution. We specifically evaluate outliers as well as challenging instances, such as occluded and far-away pedestrians, by analyzing the entire error distribution and by estimating calibrated confidence intervals. Finally, we critically review the official KITTI 3D metrics and propose a practical 3D localization metric tailored for humans.



### Towards End-to-end Car License Plate Location and Recognition in Unconstrained Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2008.10916v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10916v2)
- **Published**: 2020-08-25 09:51:33+00:00
- **Updated**: 2022-07-11 04:50:00+00:00
- **Authors**: Shuxin Qin, Sijiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from the rapid development of convolutional neural networks, the performance of car license plate detection and recognition has been largely improved. Nonetheless, most existing methods solve detection and recognition problems separately, and focus on specific scenarios, which hinders the deployment for real-world applications. To overcome these challenges, we present an efficient and accurate framework to solve the license plate detection and recognition tasks simultaneously. It is a lightweight and unified deep neural network, that can be optimized end-to-end and work in real-time. Specifically, for unconstrained scenarios, an anchor-free method is adopted to efficiently detect the bounding box and four corners of a license plate, which are used to extract and rectify the target region features. Then, a novel convolutional neural network branch is designed to further extract features of characters without segmentation. Finally, the recognition task is treated as sequence labeling problems, which are solved by Connectionist Temporal Classification (CTC) directly. Several public datasets including images collected from different scenarios under various conditions are chosen for evaluation. Experimental results indicate that the proposed method significantly outperforms the previous state-of-the-art methods in both speed and precision.



### Think about boundary: Fusing multi-level boundary information for landmark heatmap regression
- **Arxiv ID**: http://arxiv.org/abs/2008.10924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10924v1)
- **Published**: 2020-08-25 10:14:13+00:00
- **Updated**: 2020-08-25 10:14:13+00:00
- **Authors**: Jinheng Xie, Jun Wan, Linlin Shen, Zhihui Lai
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Although current face alignment algorithms have obtained pretty good performances at predicting the location of facial landmarks, huge challenges remain for faces with severe occlusion and large pose variations, etc. On the contrary, semantic location of facial boundary is more likely to be reserved and estimated on these scenes. Therefore, we study a two-stage but end-to-end approach for exploring the relationship between the facial boundary and landmarks to get boundary-aware landmark predictions, which consists of two modules: the self-calibrated boundary estimation (SCBE) module and the boundary-aware landmark transform (BALT) module. In the SCBE module, we modify the stem layers and employ intermediate supervision to help generate high-quality facial boundary heatmaps. Boundary-aware features inherited from the SCBE module are integrated into the BALT module in a multi-scale fusion framework to better model the transformation from boundary to landmark heatmap. Experimental results conducted on the challenging benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in the literature.



### A comparison of deep machine learning algorithms in COVID-19 disease diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2008.11639v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11639v2)
- **Published**: 2020-08-25 10:51:54+00:00
- **Updated**: 2020-10-09 07:25:26+00:00
- **Authors**: Samir S. Yadav, Jasminder Kaur Sandhu, Mininath R. Bendre, Pratap S. Vikhe, Amandeep Kaur
- **Comment**: 16 pages, 10 figures, 11 tables
- **Journal**: None
- **Summary**: The aim of the work is to use deep neural network models for solving the problem of image recognition. These days, every human being is threatened by a harmful coronavirus disease, also called COVID-19 disease. The spread of coronavirus affects the economy of many countries in the world. To find COVID-19 patients early is very essential to avoid the spread and harm to society. Pathological tests and Chromatography(CT) scans are helpful for the diagnosis of COVID-19. However, these tests are having drawbacks such as a large number of false positives, and cost of these tests are so expensive. Hence, it requires finding an easy, accurate, and less expensive way for the detection of the harmful COVID-19 disease. Chest-x-ray can be useful for the detection of this disease. Therefore, in this work chest, x-ray images are used for the diagnosis of suspected COVID-19 patients using modern machine learning techniques. The analysis of the results is carried out and conclusions are made about the effectiveness of deep machine learning algorithms in image recognition problems.



### AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with Spatially-Aware Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2008.10960v2
- **DOI**: 10.1007/978-3-030-67070-2_37
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10960v2)
- **Published**: 2020-08-25 12:35:48+00:00
- **Updated**: 2020-08-26 09:51:12+00:00
- **Authors**: Julien Despois, Frederic Flament, Matthieu Perrot
- **Comment**: Project page: https://despoisj.github.io/AgingMapGAN/
- **Journal**: None
- **Summary**: Existing approaches and datasets for face aging produce results skewed towards the mean, with individual variations and expression wrinkles often invisible or overlooked in favor of global patterns such as the fattening of the face. Moreover, they offer little to no control over the way the faces are aged and can difficultly be scaled to large images, thus preventing their usage in many real-world applications. To address these limitations, we present an approach to change the appearance of a high-resolution image using ethnicity-specific aging information and weak spatial supervision to guide the aging process. We demonstrate the advantage of our proposed method in terms of quality, control, and how it can be used on high-definition images while limiting the computational overhead.



### In-Home Daily-Life Captioning Using Radio Signals
- **Arxiv ID**: http://arxiv.org/abs/2008.10966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10966v1)
- **Published**: 2020-08-25 12:45:39+00:00
- **Updated**: 2020-08-25 12:45:39+00:00
- **Authors**: Lijie Fan, Tianhong Li, Yuan Yuan, Dina Katabi
- **Comment**: ECCV 2020. The first two authors contributed equally to this paper
- **Journal**: None
- **Summary**: This paper aims to caption daily life --i.e., to create a textual description of people's activities and interactions with objects in their homes. Addressing this problem requires novel methods beyond traditional video captioning, as most people would have privacy concerns about deploying cameras throughout their homes. We introduce RF-Diary, a new model for captioning daily life by analyzing the privacy-preserving radio signal in the home with the home's floormap. RF-Diary can further observe and caption people's life through walls and occlusions and in dark settings. In designing RF-Diary, we exploit the ability of radio signals to capture people's 3D dynamics, and use the floormap to help the model learn people's interactions with objects. We also use a multi-modal feature alignment training scheme that leverages existing video-based captioning datasets to improve the performance of our radio-based captioning model. Extensive experimental results demonstrate that RF-Diary generates accurate captions under visible conditions. It also sustains its good performance in dark or occluded settings, where video-based captioning approaches fail to generate meaningful captions. For more information, please visit our project webpage: http://rf-diary.csail.mit.edu



### Active Class Incremental Learning for Imbalanced Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.10968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10968v1)
- **Published**: 2020-08-25 12:47:09+00:00
- **Updated**: 2020-08-25 12:47:09+00:00
- **Authors**: Eden Belouadah, Adrian Popescu, Umang Aggarwal, Léo Saci
- **Comment**: Accepted in IPCV workshop from ECCV2020
- **Journal**: None
- **Summary**: Incremental Learning (IL) allows AI systems to adapt to streamed data. Most existing algorithms make two strong hypotheses which reduce the realism of the incremental scenario: (1) new data are assumed to be readily annotated when streamed and (2) tests are run with balanced datasets while most real-life datasets are actually imbalanced. These hypotheses are discarded and the resulting challenges are tackled with a combination of active and imbalanced learning. We introduce sample acquisition functions which tackle imbalance and are compatible with IL constraints. We also consider IL as an imbalanced learning problem instead of the established usage of knowledge distillation against catastrophic forgetting. Here, imbalance effects are reduced during inference through class prediction scaling. Evaluation is done with four visual datasets and compares existing and proposed sample acquisition functions. Results indicate that the proposed contributions have a positive effect and reduce the gap between active and standard IL performance.



### Protect, Show, Attend and Tell: Empowering Image Captioning Models with Ownership Protection
- **Arxiv ID**: http://arxiv.org/abs/2008.11009v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2008.11009v2)
- **Published**: 2020-08-25 13:48:35+00:00
- **Updated**: 2021-08-31 09:36:59+00:00
- **Authors**: Jian Han Lim, Chee Seng Chan, Kam Woh Ng, Lixin Fan, Qiang Yang
- **Comment**: Accepted at Pattern Recognition, 17 pages
- **Journal**: None
- **Summary**: By and large, existing Intellectual Property (IP) protection on deep neural networks typically i) focus on image classification task only, and ii) follow a standard digital watermarking framework that was conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that the current digital watermarking framework is insufficient to protect image captioning tasks that are often regarded as one of the frontiers AI problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect the image captioning model. From empirical points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose of infringement. To the best of our knowledge, this work is the first to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks. Code is available at https://github.com/jianhanlim/ipr-imagecaptioning



### Efficient Blind-Spot Neural Network Architecture for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2008.11010v1
- **DOI**: 10.1109/SDS49233.2020.00022
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.11010v1)
- **Published**: 2020-08-25 13:48:40+00:00
- **Updated**: 2020-08-25 13:48:40+00:00
- **Authors**: David Honzátko, Siavash A. Bigdeli, Engin Türetken, L. Andrea Dunbar
- **Comment**: None
- **Journal**: 2020 7th Swiss Conference on Data Science (SDS), Luzern,
  Switzerland, 2020, pp. 59-60
- **Summary**: Image denoising is an essential tool in computational photography. Standard denoising techniques, which use deep neural networks at their core, require pairs of clean and noisy images for its training. If we do not possess the clean samples, we can use blind-spot neural network architectures, which estimate the pixel value based on the neighbouring pixels only. These networks thus allow training on noisy images directly, as they by-design avoid trivial solutions. Nowadays, the blind-spot is mostly achieved using shifted convolutions or serialization. We propose a novel fully convolutional network architecture that uses dilations to achieve the blind-spot property. Our network improves the performance over the prior work and achieves state-of-the-art results on established datasets.



### Label Decoupling Framework for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.11048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11048v1)
- **Published**: 2020-08-25 14:23:38+00:00
- **Updated**: 2020-08-25 14:23:38+00:00
- **Authors**: Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, Qi Tian
- **Comment**: Accepted by CVPR2020, https://github.com/weijun88/LDF
- **Journal**: None
- **Summary**: To get more accurate saliency maps, recent methods mainly focus on aggregating multi-level features from fully convolutional network (FCN) and introducing edge information as auxiliary supervision. Though remarkable progress has been achieved, we observe that the closer the pixel is to the edge, the more difficult it is to be predicted, because edge pixels have a very imbalance distribution. To address this problem, we propose a label decoupling framework (LDF) which consists of a label decoupling (LD) procedure and a feature interaction network (FIN). LD explicitly decomposes the original saliency map into body map and detail map, where body map concentrates on center areas of objects and detail map focuses on regions around edges. Detail map works better because it involves much more pixels than traditional edge supervision. Different from saliency map, body map discards edge pixels and only pays attention to center areas. This successfully avoids the distraction from edge pixels during training. Therefore, we employ two branches in FIN to deal with body map and detail map respectively. Feature interaction (FI) is designed to fuse the two complementary branches to predict the saliency map, which is then used to refine the two branches again. This iterative refinement is helpful for learning better representations and more precise saliency maps. Comprehensive experiments on six benchmark datasets demonstrate that LDF outperforms state-of-the-art approaches on different evaluation metrics.



### On estimating gaze by self-attention augmented convolutions
- **Arxiv ID**: http://arxiv.org/abs/2008.11055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11055v2)
- **Published**: 2020-08-25 14:29:05+00:00
- **Updated**: 2020-11-03 13:49:19+00:00
- **Authors**: Gabriel Lefundes, Luciano Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of 3D gaze is highly relevant to multiple fields, including but not limited to interactive systems, specialized human-computer interfaces, and behavioral research. Although recently deep learning methods have boosted the accuracy of appearance-based gaze estimation, there is still room for improvement in the network architectures for this particular task. Therefore we propose here a novel network architecture grounded on self-attention augmented convolutions to improve the quality of the learned features during the training of a shallower residual network. The rationale is that self-attention mechanism can help outperform deeper architectures by learning dependencies between distant regions in full-face images. This mechanism can also create better and more spatially-aware feature representations derived from the face and eye images before gaze regression. We dubbed our framework ARes-gaze, which explores our Attention-augmented ResNet (ARes-14) as twin convolutional backbones. In our experiments, results showed a decrease of the average angular error by 2.38% when compared to state-of-the-art methods on the MPIIFaceGaze data set, and a second-place on the EyeDiap data set. It is noteworthy that our proposed framework was the only one to reach high accuracy simultaneously on both data sets.



### GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.11062v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.11062v1)
- **Published**: 2020-08-25 14:39:42+00:00
- **Updated**: 2020-08-25 14:39:42+00:00
- **Authors**: Haotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, Zhangyang Wang
- **Comment**: ECCV 2020 spotlight
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have gained increasing popularity in various computer vision applications, and recently start to be deployed to resource-constrained mobile devices. Similar to other deep models, state-of-the-art GANs suffer from high parameter complexities. That has recently motivated the exploration of compressing GANs (usually generators). Compared to the vast literature and prevailing success in compressing deep classifiers, the study of GAN compression remains in its infancy, so far leveraging individual compression techniques instead of more sophisticated combinations. We observe that due to the notorious instability of training GANs, heuristically stacking different compression techniques will result in unsatisfactory results. To this end, we propose the first unified optimization framework combining multiple compression means for GAN compression, dubbed GAN Slimming (GS). GS seamlessly integrates three mainstream compression techniques: model distillation, channel pruning and quantization, together with the GAN minimax objective, into one unified optimization form, that can be efficiently optimized from end to end. Without bells and whistles, GS largely outperforms existing options in compressing image-to-image translation GANs. Specifically, we apply GS to compress CartoonGAN, a state-of-the-art style transfer network, by up to 47 times, with minimal visual quality degradation. Codes and pre-trained models can be found at https://github.com/TAMU-VITA/GAN-Slimming.



### Mask-guided sample selection for Semi-Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.11073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11073v1)
- **Published**: 2020-08-25 14:44:58+00:00
- **Updated**: 2020-08-25 14:44:58+00:00
- **Authors**: Miriam Bellver, Amaia Salvador, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: Preprint submitted to Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Image segmentation methods are usually trained with pixel-level annotations, which require significant human effort to collect. The most common solution to address this constraint is to implement weakly-supervised pipelines trained with lower forms of supervision, such as bounding boxes or scribbles. Another option are semi-supervised methods, which leverage a large amount of unlabeled data and a limited number of strongly-labeled samples. In this second setup, samples to be strongly-annotated can be selected randomly or with an active learning mechanism that chooses the ones that will maximize the model performance. In this work, we propose a sample selection approach to decide which samples to annotate for semi-supervised instance segmentation. Our method consists in first predicting pseudo-masks for the unlabeled pool of samples, together with a score predicting the quality of the mask. This score is an estimate of the Intersection Over Union (IoU) of the segment with the ground truth mask. We study which samples are better to annotate given the quality score, and show how our approach outperforms a random selection, leading to improved performance for semi-supervised instance segmentation with low annotation budgets.



### Using the discrete radon transformation for grayscale image moments
- **Arxiv ID**: http://arxiv.org/abs/2008.11083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11083v1)
- **Published**: 2020-08-25 14:55:53+00:00
- **Updated**: 2020-08-25 14:55:53+00:00
- **Authors**: William Diggin, Michael Diggin
- **Comment**: None
- **Journal**: None
- **Summary**: Image moments are weighted sums over pixel values in a given image and are used in object detection and localization. Raw image moments are derived directly from the image and are fundamental in deriving moment invariants quantities. The current general algorithm for raw image moments is computationally expensive and the number of multiplications needed scales with the number of pixels in the image. For an image of size (N,M), it has O(NM) multiplications. In this paper we outline an algorithm using the Discrete Radon Transformation for computing the raw image moments of a grayscale image. It reduces two dimensional moment calculations to linear combinations of one dimensional moment calculations. We show that the number of multiplications needed scales as O(N + M), making it faster then the most widely used algorithm of raw image moments.



### Software Effort Estimation using parameter tuned Models
- **Arxiv ID**: http://arxiv.org/abs/2009.01660v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01660v1)
- **Published**: 2020-08-25 15:18:59+00:00
- **Updated**: 2020-08-25 15:18:59+00:00
- **Authors**: Akanksha Baghel, Meemansa Rathod, Pradeep Singh
- **Comment**: Nine Tables
- **Journal**: None
- **Summary**: Software estimation is one of the most important activities in the software project. The software effort estimation is required in the early stages of software life cycle. Project Failure is the major problem undergoing nowadays as seen by software project managers. The imprecision of the estimation is the reason for this problem. Assize of software size grows, it also makes a system complex, thus difficult to accurately predict the cost of software development process. The greatest pitfall of the software industry was the fast-changing nature of software development which has made it difficult to develop parametric models that yield high accuracy for software development in all domains. We need the development of useful models that accurately predict the cost of developing a software product. This study presents the novel analysis of various regression models with hyperparameter tuning to get the effective model. Nine different regression techniques are considered for model development



### Improving Deep Stereo Network Generalization with Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2008.11098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11098v1)
- **Published**: 2020-08-25 15:24:02+00:00
- **Updated**: 2020-08-25 15:24:02+00:00
- **Authors**: Jialiang Wang, Varun Jampani, Deqing Sun, Charles Loop, Stan Birchfield, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end deep learning methods have advanced stereo vision in recent years and obtained excellent results when the training and test data are similar. However, large datasets of diverse real-world scenes with dense ground truth are difficult to obtain and currently not publicly available to the research community. As a result, many algorithms rely on small real-world datasets of similar scenes or synthetic datasets, but end-to-end algorithms trained on such datasets often generalize poorly to different images that arise in real-world applications. As a step towards addressing this problem, we propose to incorporate prior knowledge of scene geometry into an end-to-end stereo network to help networks generalize better. For a given network, we explicitly add a gradient-domain smoothness prior and occlusion reasoning into the network training, while the architecture remains unchanged during inference. Experimentally, we show consistent improvements if we train on synthetic datasets and test on the Middlebury (real images) dataset. Noticeably, we improve PSM-Net accuracy on Middlebury from 5.37 MAE to 3.21 MAE without sacrificing speed.



### Masked Face Recognition for Secure Authentication
- **Arxiv ID**: http://arxiv.org/abs/2008.11104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11104v1)
- **Published**: 2020-08-25 15:33:59+00:00
- **Updated**: 2020-08-25 15:33:59+00:00
- **Authors**: Aqeel Anwar, Arijit Raychowdhury
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: With the recent world-wide COVID-19 pandemic, using face masks have become an important part of our lives. People are encouraged to cover their faces when in public area to avoid the spread of infection. The use of these face masks has raised a serious question on the accuracy of the facial recognition system used for tracking school/office attendance and to unlock phones. Many organizations use facial recognition as a means of authentication and have already developed the necessary datasets in-house to be able to deploy such a system. Unfortunately, masked faces make it difficult to be detected and recognized, thereby threatening to make the in-house datasets invalid and making such facial recognition systems inoperable. This paper addresses a methodology to use the current facial datasets by augmenting it with tools that enable masked faces to be recognized with low false-positive rates and high overall accuracy, without requiring the user dataset to be recreated by taking new pictures for authentication. We present an open-source tool, MaskTheFace to mask faces effectively creating a large dataset of masked faces. The dataset generated with this tool is then used towards training an effective facial recognition system with target accuracy for masked faces. We report an increase of 38% in the true positive rate for the Facenet system. We also test the accuracy of re-trained system on a custom real-world dataset MFR2 and report similar accuracy.



### Measure Anatomical Thickness from Cardiac MRI with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.11109v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11109v1)
- **Published**: 2020-08-25 15:37:57+00:00
- **Updated**: 2020-08-25 15:37:57+00:00
- **Authors**: Qiaoying Huang, Eric Z. Chen, Hanchao Yu, Yimo Guo, Terrence Chen, Dimitris Metaxas, Shanhui Sun
- **Comment**: Accepted by STACOM 2020
- **Journal**: None
- **Summary**: Accurate estimation of shape thickness from medical images is crucial in clinical applications. For example, the thickness of myocardium is one of the key to cardiac disease diagnosis. While mathematical models are available to obtain accurate dense thickness estimation, they suffer from heavy computational overhead due to iterative solvers. To this end, we propose novel methods for dense thickness estimation, including a fast solver that estimates thickness from binary annular shapes and an end-to-end network that estimates thickness directly from raw cardiac images.We test the proposed models on three cardiac datasets and one synthetic dataset, achieving impressive results and generalizability on all. Thickness estimation is performed without iterative solvers or manual correction, which is 100 times faster than the mathematical model. We also analyze thickness patterns on different cardiac pathologies with a standard clinical model and the results demonstrate the potential clinical value of our method for thickness based cardiac disease diagnosis.



### Spatiotemporal Action Recognition in Restaurant Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.11149v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11149v1)
- **Published**: 2020-08-25 16:30:01+00:00
- **Updated**: 2020-08-25 16:30:01+00:00
- **Authors**: Akshat Gupta, Milan Desai, Wusheng Liang, Magesh Kannan
- **Comment**: None
- **Journal**: None
- **Summary**: Spatiotemporal action recognition is the task of locating and classifying actions in videos. Our project applies this task to analyzing video footage of restaurant workers preparing food, for which potential applications include automated checkout and inventory management. Such videos are quite different from the standardized datasets that researchers are used to, as they involve small objects, rapid actions, and notoriously unbalanced data classes. We explore two approaches. The first approach involves the familiar object detector You Only Look Once, and another applying a recently proposed analogue for action recognition, You Only Watch Once. In the first, we design and implement a novel, recurrent modification of YOLO using convolutional LSTMs and explore the various subtleties in the training of such a network. In the second, we study the ability of YOWOs three dimensional convolutions to capture the spatiotemporal features of our unique dataset



### FastSal: a Computationally Efficient Network for Visual Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.11151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11151v1)
- **Published**: 2020-08-25 16:32:33+00:00
- **Updated**: 2020-08-25 16:32:33+00:00
- **Authors**: Feiyan Hu, Kevin McGuinness
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on the problem of visual saliency prediction, predicting regions of an image that tend to attract human visual attention, under a constrained computational budget. We modify and test various recent efficient convolutional neural network architectures like EfficientNet and MobileNetV2 and compare them with existing state-of-the-art saliency models such as SalGAN and DeepGaze II both in terms of standard accuracy metrics like AUC and NSS, and in terms of the computational complexity and model size. We find that MobileNetV2 makes an excellent backbone for a visual saliency model and can be effective even without a complex decoder. We also show that knowledge transfer from a more computationally expensive model like DeepGaze II can be achieved via pseudo-labelling an unlabelled dataset, and that this approach gives result on-par with many state-of-the-art algorithms with a fraction of the computational cost and model size. Source code is available at https://github.com/feiyanhu/FastSal.



### Detection of Retinal Blood Vessels by using Gabor filter with Entropic threshold
- **Arxiv ID**: http://arxiv.org/abs/2008.11508v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11508v1)
- **Published**: 2020-08-25 16:51:12+00:00
- **Updated**: 2020-08-25 16:51:12+00:00
- **Authors**: Mohamed. I. Waly, Ahmed El-Hossiny
- **Comment**: None
- **Journal**: Vol. 4, issue 2, November 2016 - Safar 1438
- **Summary**: Diabetic retinopathy is the basic reason for visual deficiency. This paper introduces a programmed strategy to identify and dispense with the blood vessels. The location of the blood vessels is the fundamental stride in the discovery of diabetic retinopathy because the blood vessels are the typical elements of the retinal picture. The location of the blood vessels can help the ophthalmologists to recognize the sicknesses prior and quicker. The blood vessels recognized and wiped out by utilizing Gobar filter on two freely accessible retinal databases which are STARE and DRIVE. The exactness of segmentation calculation is assessed quantitatively by contrasting the physically sectioned pictures and the comparing yield pictures, the Gabor filter with Entropic threshold vessel pixel segmentation by Entropic thresholding is better vessels with less false positive portion rate.



### End-to-End 3D Multi-Object Tracking and Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2008.11598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.11598v1)
- **Published**: 2020-08-25 16:54:46+00:00
- **Updated**: 2020-08-25 16:54:46+00:00
- **Authors**: Xinshuo Weng, Ye Yuan, Kris Kitani
- **Comment**: Extended abstract. The first two authors contributed equally. Project
  website: http://www.xinshuoweng.com/projects/GNNTrkForecast. arXiv admin
  note: substantial text overlap with arXiv:2003.07847
- **Journal**: None
- **Summary**: 3D multi-object tracking (MOT) and trajectory forecasting are two critical components in modern 3D perception systems. We hypothesize that it is beneficial to unify both tasks under one framework to learn a shared feature representation of agent interaction. To evaluate this hypothesis, we propose a unified solution for 3D MOT and trajectory forecasting which also incorporates two additional novel computational units. First, we employ a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which multiple agents interact with one another. The GNN is able to model complex hierarchical interactions, improve the discriminative feature learning for MOT association, and provide socially-aware context for trajectory forecasting. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating many duplicate trajectory samples. We show that our method achieves state-of-the-art performance on the KITTI dataset. Our project website is at http://www.xinshuoweng.com/projects/GNNTrkForecast.



### Are Deep Neural Networks "Robust"?
- **Arxiv ID**: http://arxiv.org/abs/2008.12650v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12650v1)
- **Published**: 2020-08-25 16:57:19+00:00
- **Updated**: 2020-08-25 16:57:19+00:00
- **Authors**: Peter Meer
- **Comment**: None
- **Journal**: None
- **Summary**: Separating outliers from inliers is the definition of robustness in computer vision. This essay delineates how deep neural networks are different than typical robust estimators. Deep neural networks not robust by this traditional definition.



### Boundary Uncertainty in a Single-Stage Temporal Action Localization Network
- **Arxiv ID**: http://arxiv.org/abs/2008.11170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11170v1)
- **Published**: 2020-08-25 17:04:39+00:00
- **Updated**: 2020-08-25 17:04:39+00:00
- **Authors**: Ting-Ting Xie, Christos Tzelepis, Ioannis Patras
- **Comment**: Tech report
- **Journal**: None
- **Summary**: In this paper, we address the problem of temporal action localization with a single stage neural network. In the proposed architecture we model the boundary predictions as uni-variate Gaussian distributions in order to model their uncertainties, which is the first in this area to the best of our knowledge. We use two uncertainty-aware boundary regression losses: first, the Kullback-Leibler divergence between the ground truth location of the boundary and the Gaussian modeling the prediction of the boundary and second, the expectation of the $\ell_1$ loss under the same Gaussian. We show that with both uncertainty modeling approaches improve the detection performance by more than $1.5\%$ in mAP@tIoU=0.5 and that the proposed simple one-stage network performs closely to more complex one and two stage networks.



### Learning Obstacle Representations for Neural Motion Planning
- **Arxiv ID**: http://arxiv.org/abs/2008.11174v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.11174v4)
- **Published**: 2020-08-25 17:12:32+00:00
- **Updated**: 2020-11-07 11:30:09+00:00
- **Authors**: Robin Strudel, Ricardo Garcia, Justin Carpentier, Jean-Paul Laumond, Ivan Laptev, Cordelia Schmid
- **Comment**: CoRL 2020. See the project webpage at
  https://www.di.ens.fr/willow/research/nmp_repr/
- **Journal**: None
- **Summary**: Motion planning and obstacle avoidance is a key challenge in robotics applications. While previous work succeeds to provide excellent solutions for known environments, sensor-based motion planning in new and dynamic environments remains difficult. In this work we address sensor-based motion planning from a learning perspective. Motivated by recent advances in visual recognition, we argue the importance of learning appropriate representations for motion planning. We propose a new obstacle representation based on the PointNet architecture and train it jointly with policies for obstacle avoidance. We experimentally evaluate our approach for rigid body motion planning in challenging environments and demonstrate significant improvements of the state of the art in terms of accuracy and efficiency.



### Bias-Awareness for Zero-Shot Learning the Seen and Unseen
- **Arxiv ID**: http://arxiv.org/abs/2008.11185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11185v1)
- **Published**: 2020-08-25 17:38:40+00:00
- **Updated**: 2020-08-25 17:38:40+00:00
- **Authors**: William Thong, Cees G. M. Snoek
- **Comment**: Accepted at British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: Generalized zero-shot learning recognizes inputs from both seen and unseen classes. Yet, existing methods tend to be biased towards the classes seen during training. In this paper, we strive to mitigate this bias. We propose a bias-aware learner to map inputs to a semantic embedding space for generalized zero-shot learning. During training, the model learns to regress to real-valued class prototypes in the embedding space with temperature scaling, while a margin-based bidirectional entropy term regularizes seen and unseen probabilities. Relying on a real-valued semantic embedding space provides a versatile approach, as the model can operate on different types of semantic information for both seen and unseen classes. Experiments are carried out on four benchmarks for generalized zero-shot learning and demonstrate the benefits of the proposed bias-aware classifier, both as a stand-alone method or in combination with generated features.



### GRAB: A Dataset of Whole-Body Human Grasping of Objects
- **Arxiv ID**: http://arxiv.org/abs/2008.11200v1
- **DOI**: 10.1007/978-3-030-58548-8_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11200v1)
- **Published**: 2020-08-25 17:57:55+00:00
- **Updated**: 2020-08-25 17:57:55+00:00
- **Authors**: Omid Taheri, Nima Ghorbani, Michael J. Black, Dimitrios Tzionas
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While "grasping" is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of "whole-body grasps". Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de.



### Deep Active Learning in Remote Sensing for data efficient Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.11201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11201v1)
- **Published**: 2020-08-25 17:58:17+00:00
- **Updated**: 2020-08-25 17:58:17+00:00
- **Authors**: Vít Růžička, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler
- **Comment**: 10 pages, 5 figures, ECML/PKDD Workshop on Machine Learning for Earth
  Observation, 2020
- **Journal**: None
- **Summary**: We investigate active learning in the context of deep neural network models for change detection and map updating. Active learning is a natural choice for a number of remote sensing tasks, including the detection of local surface changes: changes are on the one hand rare and on the other hand their appearance is varied and diffuse, making it hard to collect a representative training set in advance. In the active learning setting, one starts from a minimal set of training examples and progressively chooses informative samples that are annotated by a user and added to the training set. Hence, a core component of an active learning system is a mechanism to estimate model uncertainty, which is then used to pick uncertain, informative samples. We study different mechanisms to capture and quantify this uncertainty when working with deep networks, based on the variance or entropy across explicit or implicit model ensembles. We show that active learning successfully finds highly informative samples and automatically balances the training distribution, and reaches the same performance as a model supervised with a large, pre-annotated training set, with $\approx$99% fewer annotated samples.



### Learning to Learn in a Semi-Supervised Fashion
- **Arxiv ID**: http://arxiv.org/abs/2008.11203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11203v1)
- **Published**: 2020-08-25 17:59:53+00:00
- **Updated**: 2020-08-25 17:59:53+00:00
- **Authors**: Yun-Chun Chen, Chao-Te Chou, Yu-Chiang Frank Wang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: To address semi-supervised learning from both labeled and unlabeled data, we present a novel meta-learning scheme. We particularly consider that labeled and unlabeled data share disjoint ground truth label sets, which can be seen tasks like in person re-identification or image retrieval. Our learning scheme exploits the idea of leveraging information from labeled to unlabeled data. Instead of fitting the associated class-wise similarity scores as most meta-learning algorithms do, we propose to derive semantics-oriented similarity representations from labeled data, and transfer such representation to unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning scheme, which can be applied to fully supervised learning tasks for improved performance. Our experiments on various tasks and settings confirm the effectiveness of our proposed approach and its superiority over the state-of-the-art methods.



### Flood Extent Mapping based on High Resolution Aerial Imagery and DEM: A Hidden Markov Tree Approach
- **Arxiv ID**: http://arxiv.org/abs/2008.11230v2
- **DOI**: 10.1080/01431161.2020.1823514
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11230v2)
- **Published**: 2020-08-25 18:35:28+00:00
- **Updated**: 2021-01-07 22:40:58+00:00
- **Authors**: Zhe Jiang, Arpan Man Sainju
- **Comment**: None
- **Journal**: None
- **Summary**: Flood extent mapping plays a crucial role in disaster management and national water forecasting. In recent years, high-resolution optical imagery becomes increasingly available with the deployment of numerous small satellites and drones. However, analyzing such imagery data to extract flood extent poses unique challenges due to the rich noise and shadows, obstacles (e.g., tree canopies, clouds), and spectral confusion between pixel classes (flood, dry) due to spatial heterogeneity. Existing machine learning techniques often focus on spectral and spatial features from raster images without fully incorporating the geographic terrain within classification models. In contrast, we recently proposed a novel machine learning model called geographical hidden Markov tree that integrates spectral features of pixels and topographic constraints from Digital Elevation Model (DEM) data (i.e., water flow directions) in a holistic manner. This paper evaluates the model through case studies on high-resolution aerial imagery from the National Oceanic and Atmospheric Administration (NOAA) National Geodetic Survey together with DEM. Three scenes are selected in heavily vegetated floodplains near the cities of Grimesland and Kinston in North Carolina during Hurricane Matthew floods in 2016. Results show that the proposed hidden Markov tree model outperforms several state of the art machine learning algorithms (e.g., random forests, gradient boosted model) by an improvement of F-score (the harmonic mean of the user's accuracy and producer's accuracy) from around 70% to 80% to over 95% on our datasets.



### HoloLens 2 Research Mode as a Tool for Computer Vision Research
- **Arxiv ID**: http://arxiv.org/abs/2008.11239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11239v1)
- **Published**: 2020-08-25 19:05:38+00:00
- **Updated**: 2020-08-25 19:05:38+00:00
- **Authors**: Dorin Ungureanu, Federica Bogo, Silvano Galliani, Pooja Sama, Xin Duan, Casey Meekhof, Jan Stühmer, Thomas J. Cashman, Bugra Tekin, Johannes L. Schönberger, Pawel Olszta, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful sensing devices with integrated compute capabilities, which makes it an ideal platform for computer vision research. In this technical report, we present HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw sensor streams. We provide an overview of the API and explain how it can be used to build mixed reality applications based on processing sensor data. We also show how to combine the Research Mode sensor data with the built-in eye and hand tracking capabilities provided by HoloLens 2. By releasing the Research Mode API and a set of open-source tools, we aim to foster further research in the fields of computer vision as well as robotics and encourage contributions from the research community.



### Temporal Action Localization with Variance-Aware Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.11254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11254v1)
- **Published**: 2020-08-25 20:12:59+00:00
- **Updated**: 2020-08-25 20:12:59+00:00
- **Authors**: Ting-Ting Xie, Christos Tzelepis, Ioannis Patras
- **Comment**: Journal paper; Under review
- **Journal**: None
- **Summary**: This work addresses the problem of temporal action localization with Variance-Aware Networks (VAN), i.e., DNNs that use second-order statistics in the input and/or the output of regression tasks. We first propose a network (VANp) that when presented with the second-order statistics of the input, i.e., each sample has a mean and a variance, it propagates the mean and the variance throughout the network to deliver outputs with second order statistics. In this framework, both the input and the output could be interpreted as Gaussians. To do so, we derive differentiable analytic solutions, or reasonable approximations, to propagate across commonly used NN layers. To train the network, we define a differentiable loss based on the KL-divergence between the predicted Gaussian and a Gaussian around the ground truth action borders, and use standard back-propagation. Importantly, the variances propagation in VANp does not require any additional parameters, and during testing, does not require any additional computations either. In action localization, the means and the variances of the input are computed at pooling operations, that are typically used to bring arbitrarily long videos to a vector with fixed dimensions. Second, we propose two alternative formulations that augment the first (respectively, the last) layer of a regression network with additional parameters so as to take in the input (respectively, predict in the output) both means and variances. Results in the action localization problem show that the incorporation of second order statistics improves over the baseline network, and that VANp surpasses the accuracy of virtually all other two-stage networks without involving any additional parameters.



### Deep Neural Network for 3D Surface Segmentation based on Contour Tree Hierarchy
- **Arxiv ID**: http://arxiv.org/abs/2008.11269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11269v1)
- **Published**: 2020-08-25 20:54:27+00:00
- **Updated**: 2020-08-25 20:54:27+00:00
- **Authors**: Wenchong He, Arpan Man Sainju, Zhe Jiang, Da Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Given a 3D surface defined by an elevation function on a 2D grid as well as non-spatial features observed at each pixel, the problem of surface segmentation aims to classify pixels into contiguous classes based on both non-spatial features and surface topology. The problem has important applications in hydrology, planetary science, and biochemistry but is uniquely challenging for several reasons. First, the spatial extent of class segments follows surface contours in the topological space, regardless of their spatial shapes and directions. Second, the topological structure exists in multiple spatial scales based on different surface resolutions. Existing widely successful deep learning models for image segmentation are often not applicable due to their reliance on convolution and pooling operations to learn regular structural patterns on a grid. In contrast, we propose to represent surface topological structure by a contour tree skeleton, which is a polytree capturing the evolution of surface contours at different elevation levels. We further design a graph neural network based on the contour tree hierarchy to model surface topological structure at different spatial scales. Experimental evaluations based on real-world hydrological datasets show that our model outperforms several baseline methods in classification accuracy.



### Properties Of Winning Tickets On Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.12141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12141v1)
- **Published**: 2020-08-25 21:36:56+00:00
- **Updated**: 2020-08-25 21:36:56+00:00
- **Authors**: Sherin Muckatira
- **Comment**: 11 pages, 11 figures, presented at WiCV workshop at ECCV 2020
- **Journal**: None
- **Summary**: Skin cancer affects a large population every year -- automated skin cancer detection algorithms can thus greatly help clinicians. Prior efforts involving deep learning models have high detection accuracy. However, most of the models have a large number of parameters, with some works even using an ensemble of models to achieve good accuracy. In this paper, we investigate a recently proposed pruning technique called Lottery Ticket Hypothesis. We find that iterative pruning of the network resulted in improved accuracy, compared to that of the unpruned network, implying that -- the lottery ticket hypothesis can be applied to the problem of skin cancer detection and this hypothesis can result in a smaller network for inference. We also examine the accuracy across sub-groups -- created by gender and age -- and it was found that some sub-groups show a larger increase in accuracy than others.



### Robust Character Labeling in Movie Videos: Data Resources and Self-supervised Feature Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.11289v2
- **DOI**: 10.1109/TMM.2021.3096155
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.11289v2)
- **Published**: 2020-08-25 22:07:41+00:00
- **Updated**: 2022-02-25 23:18:30+00:00
- **Authors**: Krishna Somandepalli, Rajat Hebbar, Shrikanth Narayanan
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia (2021)
- **Summary**: Robust face clustering is a vital step in enabling computational understanding of visual character portrayal in media. Face clustering for long-form content is challenging because of variations in appearance and lack of supporting large-scale labeled data. Our work in this paper focuses on two key aspects of this problem: the lack of domain-specific training or benchmark datasets, and adapting face embeddings learned on web images to long-form content, specifically movies. First, we present a dataset of over 169,000 face tracks curated from 240 Hollywood movies with weak labels on whether a pair of face tracks belong to the same or a different character. We propose an offline algorithm based on nearest-neighbor search in the embedding space to mine hard-examples from these tracks. We then investigate triplet-loss and multiview correlation-based methods for adapting face embeddings to hard-examples. Our experimental results highlight the usefulness of weakly labeled data for domain-specific feature adaptation. Overall, we find that multiview correlation-based adaptation yields more discriminative and robust face embeddings. Its performance on downstream face verification and clustering tasks is comparable to that of the state-of-the-art results in this domain. We also present the SAIL-Movie Character Benchmark corpus developed to augment existing benchmarks. It consists of racially diverse actors and provides face-quality labels for subsequent error analysis. We hope that the large-scale datasets developed in this work can further advance automatic character labeling in videos. All resources are available freely at https://sail.usc.edu/~ccmi/multiface.



### Transductive Information Maximization For Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.11297v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.11297v3)
- **Published**: 2020-08-25 22:38:41+00:00
- **Updated**: 2020-10-23 17:36:19+00:00
- **Authors**: Malik Boudiaf, Ziko Imtiaz Masud, Jérôme Rony, José Dolz, Pablo Piantanida, Ismail Ben Ayed
- **Comment**: NeurIPS 2020. Code available at https://github.com/mboudiaf/TIM
- **Journal**: None
- **Summary**: We introduce Transductive Infomation Maximization (TIM) for few-shot learning. Our method maximizes the mutual information between the query features and their label predictions for a given few-shot task, in conjunction with a supervision loss based on the support set. Furthermore, we propose a new alternating-direction solver for our mutual-information loss, which substantially speeds up transductive-inference convergence over gradient-based optimization, while yielding similar accuracy. TIM inference is modular: it can be used on top of any base-training feature extractor. Following standard transductive few-shot settings, our comprehensive experiments demonstrate that TIM outperforms state-of-the-art methods significantly across various datasets and networks, while used on top of a fixed feature extractor trained with simple cross-entropy on the base classes, without resorting to complex meta-learning schemes. It consistently brings between 2% and 5% improvement in accuracy over the best performing method, not only on all the well-established few-shot benchmarks but also on more challenging scenarios,with domain shifts and larger numbers of classes.



### Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses
- **Arxiv ID**: http://arxiv.org/abs/2008.11300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.11300v1)
- **Published**: 2020-08-25 22:51:51+00:00
- **Updated**: 2020-08-25 22:51:51+00:00
- **Authors**: Fu Lin, Rohit Mittapalli, Prithvijit Chattopadhyay, Daniel Bolya, Judy Hoffman
- **Comment**: ECCV 2020 Workshop on Adversarial Robustness in the Real World
- **Journal**: None
- **Summary**: Convolutional Neural Networks have been shown to be vulnerable to adversarial examples, which are known to locate in subspaces close to where normal data lies but are not naturally occurring and of low probability. In this work, we investigate the potential effect defense techniques have on the geometry of the likelihood landscape - likelihood of the input images under the trained model. We first propose a way to visualize the likelihood landscape leveraging an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense techniques results in a similar effect of flattening the likelihood landscape. We further explore directly regularizing towards a flat landscape for adversarial robustness.



