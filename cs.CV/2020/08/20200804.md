# Arxiv Papers in cs.CV on 2020-08-04
### Central object segmentation by deep learning for fruits and other roundish objects
- **Arxiv ID**: http://arxiv.org/abs/2008.01251v2
- **DOI**: 10.3390/s21216999
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01251v2)
- **Published**: 2020-08-04 00:13:40+00:00
- **Updated**: 2020-12-06 12:34:06+00:00
- **Authors**: Motohisa Fukuda, Takashi Okuno, Shinya Yuki
- **Comment**: The version 2 contains a new section about the automatic processing
  of time series photos. All the programs are available at
  https://github.com/MotohisaFukuda/CROP
- **Journal**: Sensors 2021, 21(21), 6999
- **Summary**: We present CROP (Central Roundish Object Painter), which identifies and paints the object at the center of an RGB image. Primarily CROP works for roundish fruits in various illumination conditions, but surprisingly, it could also deal with images of other organic or inorganic materials, or ones by optical and electron microscopes, although CROP was trained solely by 172 images of fruits. The method involves image segmentation by deep learning, and the architecture of the neural network is a deeper version of the original U-Net. This technique could provide us with a means of automatically collecting statistical data of fruit growth in farms. As an example, we describe our experiment of processing 510 time series photos automatically to collect the data on the size and the position of the target fruit. Our trained neural network CROP and the above automatic programs are available on GitHub with user-friendly interface programs.



### Geometric Interpretations of the Normalized Epipolar Error
- **Arxiv ID**: http://arxiv.org/abs/2008.01254v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01254v7)
- **Published**: 2020-08-04 00:27:01+00:00
- **Updated**: 2020-12-30 21:31:43+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we provide geometric interpretations of the normalized epipolar error. Most notably, we show that it is directly related to the following quantities: (1) the shortest distance between the two backprojected rays, (2) the dihedral angle between the two bounding epipolar planes, and (3) the $L_1$-optimal angular reprojection error.



### Robust Uncertainty-Aware Multiview Triangulation
- **Arxiv ID**: http://arxiv.org/abs/2008.01258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01258v2)
- **Published**: 2020-08-04 00:47:42+00:00
- **Updated**: 2020-08-05 14:52:00+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a robust and efficient method for multiview triangulation and uncertainty estimation. Our contribution is threefold: First, we propose an outlier rejection scheme using two-view RANSAC with the midpoint method. By prescreening the two-view samples prior to triangulation, we achieve the state-of-the-art efficiency. Second, we compare different local optimization methods for refining the initial solution and the inlier set. With an iterative update of the inlier set, we show that the optimization provides significant improvement in accuracy and robustness. Third, we model the uncertainty of a triangulated point as a function of three factors: the number of cameras, the mean reprojection error and the maximum parallax angle. Learning this model allows us to quickly interpolate the uncertainty at test time. We validate our method through an extensive evaluation.



### Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.01270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01270v1)
- **Published**: 2020-08-04 01:53:56+00:00
- **Updated**: 2020-08-04 01:53:56+00:00
- **Authors**: Mingmin Zhen, Shiwei Li, Lei Zhou, Jiaxiang Shang, Haoan Feng, Tian Fang, Long Quan
- **Comment**: None
- **Journal**: European Conference on Computer Vision 2020
- **Summary**: In this paper, we introduce a novel network, called discriminative feature network (DFNet), to address the unsupervised video object segmentation task. To capture the inherent correlation among video frames, we learn discriminative features (D-features) from the input images that reveal feature distribution from a global perspective. The D-features are then used to establish correspondence with all features of test image under conditional random field (CRF) formulation, which is leveraged to enforce consistency between pixels. The experiments verify that DFNet outperforms state-of-the-art methods by a large margin with a mean IoU score of 83.4% and ranks first on the DAVIS-2016 leaderboard while using much fewer parameters and achieving much more efficient performance in the inference phase. We further evaluate DFNet on the FBMS dataset and the video saliency dataset ViSal, reaching a new state-of-the-art. To further demonstrate the generalizability of our framework, DFNet is also applied to the image object co-segmentation task. We perform experiments on a challenging dataset PASCAL-VOC and observe the superiority of DFNet. The thorough experiments verify that DFNet is able to capture and mine the underlying relations of images and discover the common foreground objects.



### Design and Deployment of Photo2Building: A Cloud-based Procedural Modeling Tool as a Service
- **Arxiv ID**: http://arxiv.org/abs/2008.01286v1
- **DOI**: 10.1145/3311790.3396670
- **Categories**: **cs.DC**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.01286v1)
- **Published**: 2020-08-04 02:43:33+00:00
- **Updated**: 2020-08-04 02:43:33+00:00
- **Authors**: Manush Bhatt, Rajesh Kalyanam, Gen Nishida, Liu He, Christopher May, Dev Niyogi, Daniel Aliaga
- **Comment**: 7 pages, 7 figures, PEARC '20: Practice and Experience in Advanced
  Research Computing, July 26--30, 2020, Portland, OR, USA
- **Journal**: ACM, PEARC 2020
- **Summary**: We present a Photo2Building tool to create a plausible 3D model of a building from only a single photograph. Our tool is based on a prior desktop version which, as described in this paper, is converted into a client-server model, with job queuing, web-page support, and support of concurrent usage. The reported cloud-based web-accessible tool can reconstruct a building in 40 seconds on average and costing only 0.60 USD with current pricing. This provides for an extremely scalable and possibly widespread tool for creating building models for use in urban design and planning applications. With the growing impact of rapid urbanization on weather and climate and resource availability, access to such a service is expected to help a wide variety of users such as city planners, urban meteorologists worldwide in the quest to improved prediction of urban weather and designing climate-resilient cities of the future.



### Tracking Emerges by Looking Around Static Scenes, with Neural 3D Mapping
- **Arxiv ID**: http://arxiv.org/abs/2008.01295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01295v1)
- **Published**: 2020-08-04 02:59:23+00:00
- **Updated**: 2020-08-04 02:59:23+00:00
- **Authors**: Adam W. Harley, Shrinidhi K. Lakshmikanth, Paul Schydlo, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We hypothesize that an agent that can look around in static scenes can learn rich visual representations applicable to 3D object tracking in complex dynamic scenes. We are motivated in this pursuit by the fact that the physical world itself is mostly static, and multiview correspondence labels are relatively cheap to collect in static scenes, e.g., by triangulation. We propose to leverage multiview data of \textit{static points} in arbitrary scenes (static or dynamic), to learn a neural 3D mapping module which produces features that are correspondable across time. The neural 3D mapper consumes RGB-D data as input, and produces a 3D voxel grid of deep features as output. We train the voxel features to be correspondable across viewpoints, using a contrastive loss, and correspondability across time emerges automatically. At test time, given an RGB-D video with approximate camera poses, and given the 3D box of an object to track, we track the target object by generating a map of each timestep and locating the object's features within each map. In contrast to models that represent video streams in 2D or 2.5D, our model's 3D scene representation is disentangled from projection artifacts, is stable under camera motion, and is robust to partial occlusions. We test the proposed architectures in challenging simulated and real data, and show that our unsupervised 3D object trackers outperform prior unsupervised 2D and 2.5D trackers, and approach the accuracy of supervised trackers. This work demonstrates that 3D object trackers can emerge without tracking labels, through multiview self-supervision on static data.



### Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization
- **Arxiv ID**: http://arxiv.org/abs/2008.01296v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01296v3)
- **Published**: 2020-08-04 02:59:42+00:00
- **Updated**: 2020-08-10 03:20:17+00:00
- **Authors**: Feihu Huang, Songcan Chen, Heng Huang
- **Comment**: Published in ICML 2019, 43 pages. arXiv admin note: text overlap with
  arXiv:1907.13463
- **Journal**: None
- **Summary**: In this paper, we propose a faster stochastic alternating direction method of multipliers (ADMM) for nonconvex optimization by using a new stochastic path-integrated differential estimator (SPIDER), called as SPIDER-ADMM. Moreover, we prove that the SPIDER-ADMM achieves a record-breaking incremental first-order oracle (IFO) complexity of $\mathcal{O}(n+n^{1/2}\epsilon^{-1})$ for finding an $\epsilon$-approximate stationary point, which improves the deterministic ADMM by a factor $\mathcal{O}(n^{1/2})$, where $n$ denotes the sample size. As one of major contribution of this paper, we provide a new theoretical analysis framework for nonconvex stochastic ADMM methods with providing the optimal IFO complexity. Based on this new analysis framework, we study the unsolved optimal IFO complexity of the existing non-convex SVRG-ADMM and SAGA-ADMM methods, and prove they have the optimal IFO complexity of $\mathcal{O}(n+n^{2/3}\epsilon^{-1})$. Thus, the SPIDER-ADMM improves the existing stochastic ADMM methods by a factor of $\mathcal{O}(n^{1/6})$. Moreover, we extend SPIDER-ADMM to the online setting, and propose a faster online SPIDER-ADMM. Our theoretical analysis shows that the online SPIDER-ADMM has the IFO complexity of $\mathcal{O}(\epsilon^{-\frac{3}{2}})$, which improves the existing best results by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$. Finally, the experimental results on benchmark datasets validate that the proposed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex optimization.



### Structural Plan of Indoor Scenes with Personalized Preferences
- **Arxiv ID**: http://arxiv.org/abs/2008.01323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01323v2)
- **Published**: 2020-08-04 04:46:19+00:00
- **Updated**: 2020-08-05 13:30:45+00:00
- **Authors**: Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng, Changyu Sun
- **Comment**: Accepted by the 8th International Workshop on Assistive Computer
  Vision and Robotics (ACVR) in Conjunction with ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose an assistive model that supports professional interior designers to produce industrial interior decoration solutions and to meet the personalized preferences of the property owners. The proposed model is able to automatically produce the layout of objects of a particular indoor scene according to property owners' preferences. In particular, the model consists of the extraction of abstract graph, conditional graph generation, and conditional scene instantiation. We provide an interior layout dataset that contains real-world 11000 designs from professional designers. Our numerical results on the dataset demonstrate the effectiveness of the proposed model compared with the state-of-art methods.



### Temporal Context Aggregation for Video Retrieval with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01334v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.01334v2)
- **Published**: 2020-08-04 05:24:20+00:00
- **Updated**: 2020-09-30 08:21:08+00:00
- **Authors**: Jie Shao, Xin Wen, Bingchen Zhao, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.



### Hierarchical Context Embedding for Region-based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.01338v1
- **DOI**: 10.1109/TIP.2021.3099733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01338v1)
- **Published**: 2020-08-04 05:33:22+00:00
- **Updated**: 2020-08-04 05:33:22+00:00
- **Authors**: Zhao-Min Chen, Xin Jin, Borui Zhao, Xiu-Shen Wei, Yanwen Guo
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: State-of-the-art two-stage object detectors apply a classifier to a sparse set of object proposals, relying on region-wise features extracted by RoIPool or RoIAlign as inputs. The region-wise features, in spite of aligning well with the proposal locations, may still lack the crucial context information which is necessary for filtering out noisy background detections, as well as recognizing objects possessing no distinctive appearances. To address this issue, we present a simple but effective Hierarchical Context Embedding (HCE) framework, which can be applied as a plug-and-play component, to facilitate the classification ability of a series of region-based detectors by mining contextual cues. Specifically, to advance the recognition of context-dependent object categories, we propose an image-level categorical embedding module which leverages the holistic image-level context to learn object-level concepts. Then, novel RoI features are generated by exploiting hierarchically embedded context information beneath both whole images and interested regions, which are also complementary to conventional RoI features. Moreover, to make full use of our hierarchical contextual RoI features, we propose the early-and-late fusion strategies (i.e., feature fusion and confidence fusion), which can be combined to boost the classification accuracy of region-based detectors. Comprehensive experiments demonstrate that our HCE framework is flexible and generalizable, leading to significant and consistent improvements upon various region-based detectors, including FPN, Cascade R-CNN and Mask R-CNN.



### Appearance Consensus Driven Self-Supervised Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/2008.01341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01341v1)
- **Published**: 2020-08-04 05:40:39+00:00
- **Updated**: 2020-08-04 05:40:39+00:00
- **Authors**: Jogendra Nath Kundu, Mugalodi Rakesh, Varun Jampani, Rahul Mysore Venkatesh, R. Venkatesh Babu
- **Comment**: ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: We present a self-supervised human mesh recovery framework to infer human pose and shape from monocular images in the absence of any paired supervision. Recent advances have shifted the interest towards directly regressing parameters of a parametric human model by supervising them on large-scale datasets with 2D landmark annotations. This limits the generalizability of such approaches to operate on images from unlabeled wild environments. Acknowledging this we propose a novel appearance consensus driven self-supervised objective. To effectively disentangle the foreground (FG) human we rely on image pairs depicting the same person (consistent FG) in varied pose and background (BG) which are obtained from unlabeled wild videos. The proposed FG appearance consistency objective makes use of a novel, differentiable Color-recovery module to obtain vertex colors without the need for any appearance network; via efficient realization of color-picking and reflectional symmetry. We achieve state-of-the-art results on the standard model-based 3D pose estimation benchmarks at comparable supervision levels. Furthermore, the resulting colored mesh prediction opens up the usage of our framework for a variety of appearance-related tasks beyond the pose and shape estimation, thus establishing our superior generalizability.



### LoCo: Local Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01342v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01342v2)
- **Published**: 2020-08-04 05:41:29+00:00
- **Updated**: 2020-11-29 08:54:06+00:00
- **Authors**: Yuwen Xiong, Mengye Ren, Raquel Urtasun
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that creates synchronization constraints in the weight update step across layers and is not biologically plausible. Recent advances in unsupervised contrastive representation learning point to the question of whether a learning algorithm can also be made local, that is, the updates of lower layers do not directly depend on the computation of upper layers. While Greedy InfoMax separately learns each block with a local objective, we found that it consistently hurts readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly send feedbacks to lower blocks. This simple design closes the performance gap between local learning and end-to-end contrastive learning algorithms for the first time. Aside from standard ImageNet experiments, we also show results on complex downstream tasks such as object detection and instance segmentation directly using readout features.



### A Novel Indoor Positioning System for unprepared firefighting scenarios
- **Arxiv ID**: http://arxiv.org/abs/2008.01344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01344v1)
- **Published**: 2020-08-04 05:46:03+00:00
- **Updated**: 2020-08-04 05:46:03+00:00
- **Authors**: Vamsi Karthik Vadlamani, Manish Bhattarai, Meenu Ajith, Manel Martınez-Ramon
- **Comment**: None
- **Journal**: None
- **Summary**: Situational awareness and Indoor location tracking for firefighters is one of the tasks with paramount importance in search and rescue operations. For Indoor Positioning systems (IPS), GPS is not the best possible solution. There are few other techniques like dead reckoning, Wifi and bluetooth based triangulation, Structure from Motion (SFM) based scene reconstruction for Indoor positioning system. However due to high temperatures, the rapidly changing environment of fires, and low parallax in the thermal images, these techniques are not suitable for relaying the necessary information in a fire fighting environment needed to increase situational awareness in real time. In fire fighting environments, thermal imaging cameras are used due to smoke and low visibility hence obtaining relative orientation from the vanishing point estimation is very difficult. The following technique that is the content of this research implements a novel optical flow based video compass for orientation estimation and fused IMU data based activity recognition for IPS. This technique helps first responders to go into unprepared, unknown environments and still maintain situational awareness like the orientation and, position of the victim fire fighters.



### Two-Stage Deep Learning for Accelerated 3D Time-of-Flight MRA without Matched Training Data
- **Arxiv ID**: http://arxiv.org/abs/2008.01362v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01362v1)
- **Published**: 2020-08-04 06:36:38+00:00
- **Updated**: 2020-08-04 06:36:38+00:00
- **Authors**: Hyungjin Chung, Eunju Cha, Leonard Sunwoo, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Time-of-flight magnetic resonance angiography (TOF-MRA) is one of the most widely used non-contrast MR imaging methods to visualize blood vessels, but due to the 3-D volume acquisition highly accelerated acquisition is necessary. Accordingly, high quality reconstruction from undersampled TOF-MRA is an important research topic for deep learning. However, most existing deep learning works require matched reference data for supervised training, which are often difficult to obtain. By extending the recent theoretical understanding of cycleGAN from the optimal transport theory, here we propose a novel two-stage unsupervised deep learning approach, which is composed of the multi-coil reconstruction network along the coronal plane followed by a multi-planar refinement network along the axial plane. Specifically, the first network is trained in the square-root of sum of squares (SSoS) domain to achieve high quality parallel image reconstruction, whereas the second refinement network is designed to efficiently learn the characteristics of highly-activated blood flow using double-headed max-pool discriminator. Extensive experiments demonstrate that the proposed learning process without matched reference exceeds performance of state-of-the-art compressed sensing (CS)-based method and provides comparable or even better results than supervised learning approaches.



### 1st Place Solutions of Waymo Open Dataset Challenge 2020 -- 2D Object Detection Track
- **Arxiv ID**: http://arxiv.org/abs/2008.01365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01365v1)
- **Published**: 2020-08-04 06:46:28+00:00
- **Updated**: 2020-08-04 06:46:28+00:00
- **Authors**: Zehao Huang, Zehui Chen, Qiaofei Li, Hongkai Zhang, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we present our solutions of Waymo Open Dataset (WOD) Challenge 2020 - 2D Object Track. We adopt FPN as our basic framework. Cascade RCNN, stacked PAFPN Neck and Double-Head are used for performance improvements. In order to handle the small object detection problem in WOD, we use very large image scales for both training and testing. Using our methods, our team RW-TSDet achieved the 1st place in the 2D Object Detection Track.



### ExchNet: A Unified Hashing Network for Large-Scale Fine-Grained Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.01369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01369v1)
- **Published**: 2020-08-04 07:01:32+00:00
- **Updated**: 2020-08-04 07:01:32+00:00
- **Authors**: Quan Cui, Qing-Yuan Jiang, Xiu-Shen Wei, Wu-Jun Li, Osamu Yoshie
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Retrieving content relevant images from a large-scale fine-grained dataset could suffer from intolerably slow query speed and highly redundant storage cost, due to high-dimensional real-valued embeddings which aim to distinguish subtle visual differences of fine-grained objects. In this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search and storage efficiency of hash learning to alleviate the aforementioned problems. Specifically, we propose a unified end-to-end trainable network, termed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features to represent object parts and whole fine-grained objects, respectively. Furthermore, to ensure the discriminative ability and semantic meaning's consistency of these part-level features across images, we design a local feature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize the whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our proposal consistently outperforms state-of-the-art generic hashing methods on five fine-grained datasets, which shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and storage reduction, revealing its efficiency and practicality.



### Neuromorphic Computing for Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.01380v2
- **DOI**: 10.1371/journal.pone.0264364
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01380v2)
- **Published**: 2020-08-04 07:34:07+00:00
- **Updated**: 2021-08-17 22:59:17+00:00
- **Authors**: Te-Yuan Liu, Ata Mahjoubfar, Daniel Prusinski, Luis Stevens
- **Comment**: None
- **Journal**: PLoS ONE 17(4): e0264364, 2022
- **Summary**: Neuromorphic computing mimics the neural activity of the brain through emulating spiking neural networks. In numerous machine learning tasks, neuromorphic chips are expected to provide superior solutions in terms of cost and power efficiency. Here, we explore the application of Loihi, a neuromorphic computing chip developed by Intel, for the computer vision task of image retrieval. We evaluated the functionalities and the performance metrics that are critical in content-based visual search and recommender systems using deep-learning embeddings. Our results show that the neuromorphic solution is about 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and 12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a lightweight convolutional neural network without batching while maintaining the same level of matching accuracy. The study validates the potential of neuromorphic computing in low-power image retrieval, as a complementary paradigm to the existing von Neumann architectures.



### Learning Invariant Feature Representation to Improve Generalization across Chest X-ray Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.04152v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04152v1)
- **Published**: 2020-08-04 07:41:15+00:00
- **Updated**: 2020-08-04 07:41:15+00:00
- **Authors**: Sandesh Ghimire, Satyananda Kashyap, Joy T. Wu, Alexandros Karargyris, Mehdi Moradi
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2020), in
  conjunction with MICCAI 2020, Oct. 4, 2020
- **Journal**: None
- **Summary**: Chest radiography is the most common medical image examination for screening and diagnosis in hospitals. Automatic interpretation of chest X-rays at the level of an entry-level radiologist can greatly benefit work prioritization and assist in analyzing a larger population. Subsequently, several datasets and deep learning-based solutions have been proposed to identify diseases based on chest X-ray images. However, these methods are shown to be vulnerable to shift in the source of data: a deep learning model performing well when tested on the same dataset as training data, starts to perform poorly when it is tested on a dataset from a different source. In this work, we address this challenge of generalization to a new source by forcing the network to learn a source-invariant representation. By employing an adversarial training strategy, we show that a network can be forced to learn a source-invariant representation. Through pneumonia-classification experiments on multi-source chest X-ray datasets, we show that this algorithm helps in improving classification accuracy on a new source of X-ray dataset.



### Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.01388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01388v1)
- **Published**: 2020-08-04 07:54:25+00:00
- **Updated**: 2020-08-04 07:54:25+00:00
- **Authors**: Jogendra Nath Kundu, Ambareesh Revanur, Govind Vitthal Waghmare, Rahul Mysore Venkatesh, R. Venkatesh Babu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present a deployment friendly, fast bottom-up framework for multi-person 3D human pose estimation. We adopt a novel neural representation of multi-person 3D pose which unifies the position of person instances with their corresponding 3D pose representation. This is realized by learning a generative pose embedding which not only ensures plausible 3D pose predictions, but also eliminates the usual keypoint grouping operation as employed in prior bottom-up approaches. Further, we propose a practical deployment paradigm where paired 2D or 3D pose annotations are unavailable. In the absence of any paired supervision, we leverage a frozen network, as a teacher model, which is trained on an auxiliary task of multi-person 2D pose estimation. We cast the learning as a cross-modal alignment problem and propose training objectives to realize a shared latent space between two diverse modalities. We aim to enhance the model's ability to perform beyond the limiting teacher network by enriching the latent-to-3D pose mapping using artificially synthesized multi-person 3D scene samples. Our approach not only generalizes to in-the-wild images, but also yields a superior trade-off between speed and performance, compared to prior top-down approaches. Our approach also yields state-of-the-art multi-person 3D pose estimation performance among the bottom-up approaches under consistent supervision levels.



### Class-Incremental Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.01389v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01389v1)
- **Published**: 2020-08-04 07:55:03+00:00
- **Updated**: 2020-08-04 07:55:03+00:00
- **Authors**: Jogendra Nath Kundu, Rahul Mysore Venkatesh, Naveen Venkat, Ambareesh Revanur, R. Venkatesh Babu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We introduce a practical Domain Adaptation (DA) paradigm called Class-Incremental Domain Adaptation (CIDA). Existing DA methods tackle domain-shift but are unsuitable for learning novel target-domain classes. Meanwhile, class-incremental (CI) methods enable learning of new classes in absence of source training data but fail under a domain-shift without labeled supervision. In this work, we effectively identify the limitations of these approaches in the CIDA paradigm. Motivated by theoretical and empirical observations, we propose an effective method, inspired by prototypical networks, that enables classification of target samples into both shared and novel (one-shot) target classes, even under a domain-shift. Our approach yields superior performance as compared to both DA and CI methods in the CIDA paradigm.



### Learning Visual Representations with Caption Annotations
- **Arxiv ID**: http://arxiv.org/abs/2008.01392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01392v1)
- **Published**: 2020-08-04 08:04:16+00:00
- **Updated**: 2020-08-04 08:04:16+00:00
- **Authors**: Mert Bulent Sariyildiz, Julien Perez, Diane Larlus
- **Comment**: Accepted to the 2020 European Conference on Computer Vision
- **Journal**: None
- **Summary**: Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: https://europe.naverlabs.com/icmlm.



### Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.01403v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2008.01403v2)
- **Published**: 2020-08-04 08:25:24+00:00
- **Updated**: 2020-08-13 01:56:06+00:00
- **Authors**: Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou, Zichuan Xu
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Query-based moment localization is a new task that localizes the best matched segment in an untrimmed video according to a given sentence query. In this localization task, one should pay more attention to thoroughly mine visual and linguistic information. To this end, we propose a novel Cross- and Self-Modal Graph Attention Network (CSMGAN) that recasts this task as a process of iterative messages passing over a joint graph. Specifically, the joint graph consists of Cross-Modal interaction Graph (CMG) and Self-Modal relation Graph (SMG), where frames and words are represented as nodes, and the relations between cross- and self-modal node pairs are described by an attention mechanism. Through parametric message passing, CMG highlights relevant instances across video and sentence, and then SMG models the pairwise relation inside each modality for frame (word) correlating. With multiple layers of such a joint graph, our CSMGAN is able to effectively capture high-order interactions between two modalities, thus enabling a further precise localization. Besides, to better comprehend the contextual details in the query, we develop a hierarchical sentence encoder to enhance the query understanding. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed model, and GCSMAN significantly outperforms the state-of-the-arts.



### MSDPN: Monocular Depth Prediction with Partial Laser Observation using Multi-stage Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.01405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.01405v1)
- **Published**: 2020-08-04 08:27:40+00:00
- **Updated**: 2020-08-04 08:27:40+00:00
- **Authors**: Hyungtae Lim, Hyeonjae Gil, Hyun Myung
- **Comment**: 8 pages, 8 figures, IEEE/RSJ Intelligent Robots and Systems
- **Journal**: None
- **Summary**: In this study, a deep-learning-based multi-stage network architecture called Multi-Stage Depth Prediction Network (MSDPN) is proposed to predict a dense depth map using a 2D LiDAR and a monocular camera. Our proposed network consists of a multi-stage encoder-decoder architecture and Cross Stage Feature Aggregation (CSFA). The proposed multi-stage encoder-decoder architecture alleviates the partial observation problem caused by the characteristics of a 2D LiDAR, and CSFA prevents the multi-stage network from diluting the features and allows the network to learn the inter-spatial relationship between features better. Previous works use sub-sampled data from the ground truth as an input rather than actual 2D LiDAR data. In contrast, our approach trains the model and conducts experiments with a physically-collected 2D LiDAR dataset. To this end, we acquired our own dataset called KAIST RGBD-scan dataset and validated the effectiveness and the robustness of MSDPN under realistic conditions. As verified experimentally, our network yields promising performance against state-of-the-art methods. Additionally, we analyzed the performance of different input methods and confirmed that the reference depth map is robust in untrained scenarios.



### Deep Parallel MRI Reconstruction Network Without Coil Sensitivities
- **Arxiv ID**: http://arxiv.org/abs/2008.01410v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01410v3)
- **Published**: 2020-08-04 08:39:36+00:00
- **Updated**: 2020-08-18 15:03:33+00:00
- **Authors**: Wanyu Bian, Yunmei Chen, Xiaojing Ye
- **Comment**: Accepted by MICCAI international workshop MLMIR 2020
- **Journal**: None
- **Summary**: We propose a novel deep neural network architecture by mapping the robust proximal gradient scheme for fast image reconstruction in parallel MRI (pMRI) with regularization function trained from data. The proposed network learns to adaptively combine the multi-coil images from incomplete pMRI data into a single image with homogeneous contrast, which is then passed to a nonlinear encoder to efficiently extract sparse features of the image. Unlike most of existing deep image reconstruction networks, our network does not require knowledge of sensitivity maps, which can be difficult to estimate accurately, and have been a major bottleneck of image reconstruction in real-world pMRI applications. The experimental results demonstrate the promising performance of our method on a variety of pMRI imaging data sets.



### Memory Efficient Class-Incremental Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.01411v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01411v2)
- **Published**: 2020-08-04 08:39:40+00:00
- **Updated**: 2021-05-18 13:32:08+00:00
- **Authors**: Hanbin Zhao, Hui Wang, Yongjian Fu, Fei Wu, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the memory-resource-limited constraints, class-incremental learning (CIL) usually suffers from the "catastrophic forgetting" problem when updating the joint classification model on the arrival of newly added classes. To cope with the forgetting problem, many CIL methods transfer the knowledge of old classes by preserving some exemplar samples into the size-constrained memory buffer. To utilize the memory buffer more efficiently, we propose to keep more auxiliary low-fidelity exemplar samples rather than the original real high-fidelity exemplar samples. Such a memory-efficient exemplar preserving scheme makes the old-class knowledge transfer more effective. However, the low-fidelity exemplar samples are often distributed in a different domain away from that of the original exemplar samples, that is, a domain shift. To alleviate this problem, we propose a duplet learning scheme that seeks to construct domain-compatible feature extractors and classifiers, which greatly narrows down the above domain gap. As a result, these low-fidelity auxiliary exemplar samples have the ability to moderately replace the original exemplar samples with a lower memory cost. In addition, we present a robust classifier adaptation scheme, which further refines the biased classifier (learned with the samples containing distillation label knowledge about old classes) with the help of the samples of pure true class labels. Experimental results demonstrate the effectiveness of this work against the state-of-the-art approaches.



### Hyperspectral Image Classification with Spatial Consistence Using Fully Convolutional Spatial Propagation Network
- **Arxiv ID**: http://arxiv.org/abs/2008.01421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01421v1)
- **Published**: 2020-08-04 09:05:52+00:00
- **Updated**: 2020-08-04 09:05:52+00:00
- **Authors**: Yenan Jiang, Ying Li, Shanrong Zou, Haokui Zhang, Yunpeng Bai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep convolutional neural networks (CNNs) have shown impressive ability to represent hyperspectral images (HSIs) and achieved encouraging results in HSI classification. However, the existing CNN-based models operate at the patch-level, in which pixel is separately classified into classes using a patch of images around it. This patch-level classification will lead to a large number of repeated calculations, and it is difficult to determine the appropriate patch size that is beneficial to classification accuracy. In addition, the conventional CNN models operate convolutions with local receptive fields, which cause failures in modeling contextual spatial information. To overcome the aforementioned limitations, we propose a novel end-to-end, pixels-to-pixels fully convolutional spatial propagation network (FCSPN) for HSI classification. Our FCSPN consists of a 3D fully convolution network (3D-FCN) and a convolutional spatial propagation network (CSPN). Specifically, the 3D-FCN is firstly introduced for reliable preliminary classification, in which a novel dual separable residual (DSR) unit is proposed to effectively capture spectral and spatial information simultaneously with fewer parameters. Moreover, the channel-wise attention mechanism is adapted in the 3D-FCN to grasp the most informative channels from redundant channel information. Finally, the CSPN is introduced to capture the spatial correlations of HSI via learning a local linear spatial propagation, which allows maintaining the HSI spatial consistency and further refining the classification results. Experimental results on three HSI benchmark datasets demonstrate that the proposed FCSPN achieves state-of-the-art performance on HSI classification.



### Predicting the Blur Visual Discomfort for Natural Scenes by the Loss of Positional Information
- **Arxiv ID**: http://arxiv.org/abs/2008.01429v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01429v2)
- **Published**: 2020-08-04 09:30:38+00:00
- **Updated**: 2020-11-12 19:17:59+00:00
- **Authors**: Elio D. Di Claudio, Paolo Giannitrapani, Giovanni Jacovitti
- **Comment**: 12 pages, 8 figures, article submitted to Vision Research (Elsevier)
  Journal in July 2020
- **Journal**: None
- **Summary**: The perception of the blur due to accommodation failures, insufficient optical correction or imperfect image reproduction is a common source of visual discomfort, usually attributed to an anomalous and annoying distribution of the image spectrum in the spatial frequency domain. In the present paper, this discomfort is attributed to a loss of the localization accuracy of the observed patterns. It is assumed, as a starting perceptual principle, that the visual system is optimally adapted to pattern localization in a natural environment. Thus, since the best possible accuracy of the image patterns localization is indicated by the positional Fisher Information, it is argued that the blur discomfort is strictly related to a loss of this information. Following this concept, a receptive field functional model, tuned to common features of natural scenes, is adopted to predict the visual discomfort. It is a complex-valued operator, orientation-selective both in the space domain and in the spatial frequency domain. Starting from the case of Gaussian blur, the analysis is extended to a generic type of blur by applying a positional Fisher Information equivalence criterion. Out-of-focus blur and astigmatic blur are presented as significant examples. The validity of the proposed model is verified by comparing its predictions with subjective ratings. The model fits linearly with the experiments reported in independent databases, based on different protocols and settings.



### A non-discriminatory approach to ethical deep learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01430v1
- **DOI**: 10.1109/TrustCom50675.2020.00126
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01430v1)
- **Published**: 2020-08-04 09:33:02+00:00
- **Updated**: 2020-08-04 09:33:02+00:00
- **Authors**: Enzo Tartaglione, Marco Grangetto
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks perform state-of-the-art in an ever-growing number of tasks, nowadays they are used to solve an incredibly large variety of tasks. However, typical training strategies do not take into account lawful, ethical and discriminatory potential issues the trained ANN models could incur in. In this work we propose NDR, a non-discriminatory regularization strategy to prevent the ANN model to solve the target task using some discriminatory features like, for example, the ethnicity in an image classification task for human faces. In particular, a part of the ANN model is trained to hide the discriminatory information such that the rest of the network focuses in learning the given learning task. Our experiments show that NDR can be exploited to achieve non-discriminatory models with both minimal computational overhead and performance loss.



### Boundary Content Graph Neural Network for Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.01432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01432v1)
- **Published**: 2020-08-04 09:35:11+00:00
- **Updated**: 2020-08-04 09:35:11+00:00
- **Authors**: Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue Liu, Junhui Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-of-the-art methods in both temporal action proposal and temporal action detection tasks.



### Addressing the Cold-Start Problem in Outfit Recommendation Using Visual Preference Modelling
- **Arxiv ID**: http://arxiv.org/abs/2008.01437v1
- **DOI**: 10.1109/BigMM50055.2020.00043
- **Categories**: **cs.CV**, cs.MM, I.4; I.5; H.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.01437v1)
- **Published**: 2020-08-04 10:07:09+00:00
- **Updated**: 2020-08-04 10:07:09+00:00
- **Authors**: Dhruv Verma, Kshitij Gulati, Rajiv Ratn Shah
- **Comment**: Sixth IEEE International Conference on Multimedia Big Data (BigMM'20)
- **Journal**: None
- **Summary**: With the global transformation of the fashion industry and a rise in the demand for fashion items worldwide, the need for an effectual fashion recommendation has never been more. Despite various cutting-edge solutions proposed in the past for personalising fashion recommendation, the technology is still limited by its poor performance on new entities, i.e. the cold-start problem. In this paper, we attempt to address the cold-start problem for new users, by leveraging a novel visual preference modelling approach on a small set of input images. We demonstrate the use of our approach with feature-weighted clustering to personalise occasion-oriented outfit recommendation. Quantitatively, our results show that the proposed visual preference modelling approach outperforms state of the art in terms of clothing attribute prediction. Qualitatively, through a pilot study, we demonstrate the efficacy of our system to provide diverse and personalised recommendations in cold-start scenarios.



### Controlling Information Capacity of Binary Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.01438v1
- **DOI**: 10.1016/j.patrec.2020.07.033
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.01438v1)
- **Published**: 2020-08-04 10:08:28+00:00
- **Updated**: 2020-08-04 10:08:28+00:00
- **Authors**: Dmitry Ignatov, Andrey Ignatov
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the growing popularity of deep learning technologies, high memory requirements and power consumption are essentially limiting their application in mobile and IoT areas. While binary convolutional networks can alleviate these problems, the limited bitwidth of weights is often leading to significant degradation of prediction accuracy. In this paper, we present a method for training binary networks that maintains a stable predefined level of their information capacity throughout the training process by applying Shannon entropy based penalty to convolutional filters. The results of experiments conducted on SVHN, CIFAR and ImageNet datasets demonstrate that the proposed approach can statistically significantly improve the accuracy of binary networks.



### Prior Guided Feature Enrichment Network for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.01449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01449v1)
- **Published**: 2020-08-04 10:41:32+00:00
- **Updated**: 2020-08-04 10:41:32+00:00
- **Authors**: Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, Jiaya Jia
- **Comment**: 16 pages. To appear in TPAMI
- **Journal**: None
- **Summary**: State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hardly work on unseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly adapts to new classes with a few labeled support samples. Theses frameworks still face the challenge of generalization ability reduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency between query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It consists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also improves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching query features with support features and prior masks. Extensive experiments on PASCAL-5$^i$ and COCO prove that the proposed prior generation method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by a large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples. Our code is available at https://github.com/Jia-Research-Lab/PFENet/.



### Prime-Aware Adaptive Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.01458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01458v1)
- **Published**: 2020-08-04 10:53:12+00:00
- **Updated**: 2020-08-04 10:53:12+00:00
- **Authors**: Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, Yichen Wei
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Knowledge distillation(KD) aims to improve the performance of a student network by mimicing the knowledge from a powerful teacher network. Existing methods focus on studying what knowledge should be transferred and treat all samples equally during training. This paper introduces the adaptive sample weighting to KD. We discover that previous effective hard mining methods are not appropriate for distillation. Furthermore, we propose Prime-Aware Adaptive Distillation (PAD) by the incorporation of uncertainty learning. PAD perceives the prime samples in distillation and then emphasizes their effect adaptively. PAD is fundamentally different from and would refine existing methods with the innovative view of unequal training. For this reason, PAD is versatile and has been applied in various tasks including classification, metric learning, and object detection. With ten teacher-student combinations on six datasets, PAD promotes the performance of existing distillation methods and outperforms recent state-of-the-art methods.



### Spherical Feature Transform for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01469v1)
- **Published**: 2020-08-04 11:32:23+00:00
- **Updated**: 2020-08-04 11:32:23+00:00
- **Authors**: Yuke Zhu, Yan Bai, Yichen Wei
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Data augmentation in feature space is effective to increase data diversity. Previous methods assume that different classes have the same covariance in their feature distributions. Thus, feature transform between different classes is performed via translation. However, this approach is no longer valid for recent deep metric learning scenarios, where feature normalization is widely adopted and all features lie on a hypersphere.   This work proposes a novel spherical feature transform approach. It relaxes the assumption of identical covariance between classes to an assumption of similar covariances of different classes on a hypersphere. Consequently, the feature transform is performed by a rotation that respects the spherical data distributions. We provide a simple and effective training method, and in depth analysis on the relation between the two different transforms. Comprehensive experiments on various deep metric learning benchmarks and different baselines verify that our method achieves consistent performance improvement and state-of-the-art results.



### Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap
- **Arxiv ID**: http://arxiv.org/abs/2008.01475v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01475v2)
- **Published**: 2020-08-04 11:57:03+00:00
- **Updated**: 2020-08-05 03:30:13+00:00
- **Authors**: Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu, Zhengsu Chen, Lanfei Wang, An Xiao, Jianlong Chang, Xiaopeng Zhang, Qi Tian
- **Comment**: 24 pages, 3 figures, 2 tables, meta data updated
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has attracted increasing attentions in both academia and industry. In the early age, researchers mostly applied individual search methods which sample and evaluate the candidate architectures separately and thus incur heavy computational overheads. To alleviate the burden, weight-sharing methods were proposed in which exponentially many architectures share weights in the same super-network, and the costly training procedure is performed only once. These methods, though being much faster, often suffer the issue of instability. This paper provides a literature review on NAS, in particular the weight-sharing methods, and points out that the major challenge comes from the optimization gap between the super-network and the sub-architectures. From this perspective, we summarize existing approaches into several categories according to their efforts in bridging the gap, and analyze both advantages and disadvantages of these methodologies. Finally, we share our opinions on the future directions of NAS and AutoML. Due to the expertise of the authors, this paper mainly focuses on the application of NAS to computer vision problems and may bias towards the work in our group.



### Learning Interpretable Microscopic Features of Tumor by Multi-task Adversarial CNNs To Improve Generalization
- **Arxiv ID**: http://arxiv.org/abs/2008.01478v3
- **DOI**: 10.59275/j.melba.2023-3462
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01478v3)
- **Published**: 2020-08-04 12:10:35+00:00
- **Updated**: 2023-06-21 12:25:28+00:00
- **Authors**: Mara Graziani, Sebastian Otalora, Stephane Marchand-Maillet, Henning Muller, Vincent Andrearczyk
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:011
- **Journal**: None
- **Summary**: Adopting Convolutional Neural Networks (CNNs) in the daily routine of primary diagnosis requires not only near-perfect precision, but also a sufficient degree of generalization to data acquisition shifts and transparency. Existing CNN models act as black boxes, not ensuring to the physicians that important diagnostic features are used by the model. Building on top of successfully existing techniques such as multi-task learning, domain adversarial training and concept-based interpretability, this paper addresses the challenge of introducing diagnostic factors in the training objectives. Here we show that our architecture, by learning end-to-end an uncertainty-based weighting combination of multi-task and adversarial losses, is encouraged to focus on pathology features such as density and pleomorphism of nuclei, e.g. variations in size and appearance, while discarding misleading features such as staining differences. Our results on breast lymph node tissue show significantly improved generalization in the detection of tumorous tissue, with best average AUC 0.89 (0.01) against the baseline AUC 0.86 (0.005). By applying the interpretability technique of linearly probing intermediate representations, we also demonstrate that interpretable pathology features such as nuclei density are learned by the proposed CNN architecture, confirming the increased transparency of this model. This result is a starting point towards building interpretable multi-task architectures that are robust to data heterogeneity. Our code is available at https://github.com/maragraziani/multitask_adversarial



### Tracking Skin Colour and Wrinkle Changes During Cosmetic Product Trials Using Smartphone Images
- **Arxiv ID**: http://arxiv.org/abs/2008.01483v1
- **DOI**: 10.1111/srt.12928
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01483v1)
- **Published**: 2020-08-04 12:19:44+00:00
- **Updated**: 2020-08-04 12:19:44+00:00
- **Authors**: Alan F. Smeaton, Swathikiran Srungavarapu, Cyril Messaraa, Claire Tansey
- **Comment**: 17 pages, 12 figures, 3 tables. This is the submitted version, the
  definitive published version is at
  https://onlinelibrary.wiley.com/doi/abs/10.1111/srt.12928
- **Journal**: None
- **Summary**: Background: To explore how the efficacy of product trials for skin cosmetics can be improved through the use of consumer-level images taken by volunteers using a conventional smartphone.   Materials and Methods: 12 women aged 30 to 60 years participated in a product trial and had close-up images of the cheek and temple regions of their faces taken with a high-resolution Antera 3D CS camera at the start and end of a 4-week period. Additionally, they each had ``selfies'' of the same regions of their faces taken regularly throughout the trial period. Automatic image analysis to identify changes in skin colour used three kinds of colour normalisation and analysis for wrinkle composition identified edges and calculated their magnitude.   Results: Images taken at the start and end of the trial acted as baseline ground truth for normalisation of smartphone images and showed large changes in both colour and wrinkle magnitude during the trial for many volunteers. Conclusions: Results demonstrate that regular use of selfie smartphone images within trial periods can add value to interpretation of the efficacy of the trial.



### Learning Stereo from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2008.01484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01484v2)
- **Published**: 2020-08-04 12:22:21+00:00
- **Updated**: 2020-08-20 18:11:25+00:00
- **Authors**: Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel J. Brostow, Michael Firman
- **Comment**: Accepted as an oral presentation at ECCV 2020
- **Journal**: None
- **Summary**: Supervised deep networks are among the best methods for finding correspondences in stereo image pairs. Like all supervised approaches, these networks require ground truth data during training. However, collecting large quantities of accurate dense correspondence data is very challenging. We propose that it is unnecessary to have such a high reliance on ground truth depths or even corresponding stereo pairs. Inspired by recent progress in monocular depth estimation, we generate plausible disparity maps from single images. In turn, we use those flawed disparity maps in a carefully designed pipeline to generate stereo training pairs. Training in this manner makes it possible to convert any collection of single RGB images into stereo training data. This results in a significant reduction in human effort, with no need to collect real depths or to hand-design synthetic data. We can consequently train a stereo matching network from scratch on datasets like COCO, which were previously hard to exploit for stereo. Through extensive experiments we show that our approach outperforms stereo networks trained with standard synthetic datasets, when evaluated on KITTI, ETH3D, and Middlebury.



### Multiple Code Hashing for Efficient Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.01503v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01503v1)
- **Published**: 2020-08-04 13:18:19+00:00
- **Updated**: 2020-08-04 13:18:19+00:00
- **Authors**: Ming-Wei Li, Qing-Yuan Jiang, Wu-Jun Li
- **Comment**: 12 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Due to its low storage cost and fast query speed, hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query, which can enable search at a constant or sub-linear time cost. However, existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios, since they learn only one hash code for each image. More specifically, by using one hash code to represent one image, existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result, a large number of hash buckets need to be visited for retrieving similar images, based on the learned codes. This will deteriorate the efficiency of hash bucket search. In this paper, we propose a novel hashing framework, called multiple code hashing (MCH), to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image, with each code representing a different region of the image. Furthermore, we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge, this is the first work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a significant improvement in hash bucket search, compared with existing methods that learn only one hash code for each image.



### Online Continual Learning under Extreme Memory Constraints
- **Arxiv ID**: http://arxiv.org/abs/2008.01510v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01510v3)
- **Published**: 2020-08-04 13:25:26+00:00
- **Updated**: 2022-01-12 14:09:48+00:00
- **Authors**: Enrico Fini, Stéphane Lathuilière, Enver Sangineto, Moin Nabi, Elisa Ricci
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Continual Learning (CL) aims to develop agents emulating the human ability to sequentially learn new tasks while being able to retain knowledge obtained from past experiences. In this paper, we introduce the novel problem of Memory-Constrained Online Continual Learning (MC-OCL) which imposes strict constraints on the memory overhead that a possible algorithm can use to avoid catastrophic forgetting. As most, if not all, previous CL methods violate these constraints, we propose an algorithmic solution to MC-OCL: Batch-level Distillation (BLD), a regularization-based CL approach, which effectively balances stability and plasticity in order to learn from data streams, while preserving the ability to solve old tasks through distillation. Our extensive experimental evaluation, conducted on three publicly available benchmarks, empirically demonstrates that our approach successfully addresses the MC-OCL problem and achieves comparable accuracy to prior distillation methods requiring higher memory overhead.



### COVID-19 in CXR: from Detection and Severity Scoring to Patient Disease Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2008.02150v2
- **DOI**: 10.1109/JBHI.2021.3069169
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02150v2)
- **Published**: 2020-08-04 13:50:35+00:00
- **Updated**: 2021-03-31 09:37:30+00:00
- **Authors**: Rula Amer, Maayan Frid-Adar, Ophir Gozes, Jannette Nassar, Hayit Greenspan
- **Comment**: paper was accepted to JBHI IEEE journal
- **Journal**: IEEE J Biomed Health Inform. 2021 Mar 26;PP
- **Summary**: In this work, we estimate the severity of pneumonia in COVID-19 patients and conduct a longitudinal study of disease progression. To achieve this goal, we developed a deep learning model for simultaneous detection and segmentation of pneumonia in chest Xray (CXR) images and generalized to COVID-19 pneumonia. The segmentations were utilized to calculate a "Pneumonia Ratio" which indicates the disease severity. The measurement of disease severity enables to build a disease extent profile over time for hospitalized patients. To validate the model relevance to the patient monitoring task, we developed a validation strategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs - synthetic Xray) from serial CT scans; we then compared the disease progression profiles that were generated from the DRRs to those that were generated from CT volumes.



### Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.01550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01550v1)
- **Published**: 2020-08-04 13:56:19+00:00
- **Updated**: 2020-08-04 13:56:19+00:00
- **Authors**: Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, Dahua Lin
- **Comment**: Source code: https://github.com/xinge008/Cylinder3D
- **Journal**: None
- **Summary**: State-of-the-art methods for large-scale driving-scene LiDAR semantic segmentation often project and process the point clouds in the 2D space. The projection methods includes spherical projection, bird-eye view projection, etc. Although this process makes the point cloud suitable for the 2D CNN-based networks, it inevitably alters and abandons the 3D topology and geometric relations. A straightforward solution to tackle the issue of 3D-to-2D projection is to keep the 3D representation and process the points in the 3D space. In this work, we first perform an in-depth analysis for different representations and backbones in 2D and 3D spaces, and reveal the effectiveness of 3D representations and networks on LiDAR segmentation. Then, we develop a 3D cylinder partition and a 3D cylinder convolution based framework, termed as Cylinder3D, which exploits the 3D topology relations and structures of driving-scene point clouds. Moreover, a dimension-decomposition based context modeling module is introduced to explore the high-rank context information in point clouds in a progressive manner. We evaluate the proposed model on a large-scale driving-scene dataset, i.e. SematicKITTI. Our method achieves state-of-the-art performance and outperforms existing methods by 6% in terms of mIoU.



### Revisiting Robust Model Fitting Using Truncated Loss
- **Arxiv ID**: http://arxiv.org/abs/2008.01574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01574v2)
- **Published**: 2020-08-04 14:10:41+00:00
- **Updated**: 2023-06-25 13:10:25+00:00
- **Authors**: Fei Wen, Hewen Wei, Yipeng Liu, Peilin Liu
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Robust fitting is a fundamental problem in low-level vision, which is typically achieved by maximum consensus (MC) estimators to identify inliers first or by M-estimators directly. While these two methods are discriminately preferred in different applications, truncated loss based M-estimators are similar to MC as they can also identify inliers. This work revisits a formulation that achieves simultaneous inlier identification and model estimation (SIME) using truncated loss. It has a generalized form adapts to both linear and nonlinear residual models. We show that as SIME takes fitting residual into account in finding inliers, its lowest achievable residual in model fitting is lower than that of MC robust fitting. Then, an alternating minimization (AM) algorithm is employed to solve the SIME formulation. Meanwhile, a semidefinite relaxation (SDR) embedded AM algorithm is developed in order to ease the high nonconvexity of the SIME formulation. Furthermore, the new algorithms are applied to various 2D/3D registration problems. Experimental results show that the new algorithms significantly outperform RANSAC and deterministic approximate MC methods at high outlier ratios. Besides, in rotation and Euclidean registration problems, the new algorithms also compare favorably with state-of-the-art registration methods, especially in high noise and outliers. Code is available at \textit{https://github.com/FWen/mcme.git}.



### Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions
- **Arxiv ID**: http://arxiv.org/abs/2008.01576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01576v2)
- **Published**: 2020-08-04 14:15:40+00:00
- **Updated**: 2021-04-21 13:31:55+00:00
- **Authors**: Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, Hongsheng Li
- **Comment**: ECCV 2020. Introduction video at https://youtu.be/8E3bwvjCHYE and
  code at https://github.com/xh-liu/Open-Edit
- **Journal**: None
- **Summary**: We propose a novel algorithm, named Open-Edit, which is the first attempt on open-domain image manipulation with open-vocabulary instructions. It is a challenging task considering the large variation of image domains and the lack of training supervision. Our approach takes advantage of the unified visual-semantic embedding space pretrained on a general image-caption dataset, and manipulates the embedded visual features by applying text-guided vector arithmetic on the image feature maps. A structure-preserving image decoder then generates the manipulated images from the manipulated feature maps. We further propose an on-the-fly sample-specific optimization approach with cycle-consistency constraints to regularize the manipulated images and force them to preserve details of the source images. Our approach shows promising results in manipulating open-vocabulary color, texture, and high-level attributes for various scenarios of open-domain images.



### Evaluating the performance of the LIME and Grad-CAM explanation methods on a LEGO multi-label image classification task
- **Arxiv ID**: http://arxiv.org/abs/2008.01584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01584v1)
- **Published**: 2020-08-04 14:27:13+00:00
- **Updated**: 2020-08-04 14:27:13+00:00
- **Authors**: David Cian, Jan van Gemert, Attila Lengyel
- **Comment**: Supervision by Jan van Gemert and Attila Lengyel
- **Journal**: None
- **Summary**: In this paper, we run two methods of explanation, namely LIME and Grad-CAM, on a convolutional neural network trained to label images with the LEGO bricks that are visible in them. We evaluate them on two criteria, the improvement of the network's core performance and the trust they are able to generate for users of the system. We find that in general, Grad-CAM seems to outperform LIME on this specific task: it yields more detailed insight from the point of view of core performance and 80\% of respondents asked to choose between them when it comes to the trust they inspire in the model choose Grad-CAM. However, we also posit that it is more useful to employ these two methods together, as the insights they yield are complementary.



### Shape Consistent 2D Keypoint Estimation under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2008.01589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01589v1)
- **Published**: 2020-08-04 14:32:06+00:00
- **Updated**: 2020-08-04 14:32:06+00:00
- **Authors**: Levi O. Vasconcelos, Massimiliano Mancini, Davide Boscaini, Samuel Rota Bulo, Barbara Caputo, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: Recent unsupervised domain adaptation methods based on deep architectures have shown remarkable performance not only in traditional classification tasks but also in more complex problems involving structured predictions (e.g. semantic segmentation, depth estimation). Following this trend, in this paper we present a novel deep adaptation framework for estimating keypoints under domain shift}, i.e. when the training (source) and the test (target) images significantly differ in terms of visual appearance. Our method seamlessly combines three different components: feature alignment, adversarial training and self-supervision. Specifically, our deep architecture leverages from domain-specific distribution alignment layers to perform target adaptation at the feature level. Furthermore, a novel loss is proposed which combines an adversarial term for ensuring aligned predictions in the output space and a geometric consistency term which guarantees coherent predictions between a target sample and its perturbed version. Our extensive experimental evaluation conducted on three publicly available benchmarks shows that our approach outperforms state-of-the-art domain adaptation methods in the 2D keypoint prediction task.



### Land Use and Land Cover Classification using a Human Group based Particle Swarm Optimization Algorithm with a LSTM classifier on hybrid-pre-processing Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2008.01635v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01635v2)
- **Published**: 2020-08-04 15:30:10+00:00
- **Updated**: 2020-11-07 18:33:09+00:00
- **Authors**: R. Ganesh Babu, K. Uma Maheswari, C. Zarro, B. D. Parameshachari, S. L. Ullo
- **Comment**: 21 pages, 11 figures, submitted to the Special Issue Multimedia
  Vision and Machine Learning for Remote Sensing di MDPI Remote Sensing (in
  review)
- **Journal**: None
- **Summary**: Land use and land cover (LULC) classification using remote sensing imagery plays a vital role in many environment modeling and land use inventories. In this study, a hybrid feature optimization algorithm along with a deep learning classifier is proposed to improve the performance of LULC classification, helping to predict wildlife habitat, deteriorating environmental quality, haphazard, etc. LULC classification is assessed using Sat 4, Sat 6 and Eurosat datasets. After the selection of remote sensing images, normalization and histogram equalization methods are used to improve the quality of the images. Then, a hybrid optimization is accomplished by using the Local Gabor Binary Pattern Histogram Sequence (LGBPHS), the Histogram of Oriented Gradient (HOG) and Haralick texture features, for the feature extraction from the selected images. The benefits of this hybrid optimization are a high discriminative power and invariance to color and grayscale images. Next, a Human Group based Particle Swarm Optimization (PSO) algorithm is applied to select the optimal features, whose benefits are fast convergence rate and easy to implement. After selecting the optimal feature values, a Long Short Term Memory (LSTM) network is utilized to classify the LULC classes. Experimental results showed that the Human Group based PSO algorithm with a LSTM classifier effectively well differentiates the land use and land cover classes in terms of classification accuracy, recall and precision. An improvement of 2.56% in accuracy is achieved compared to the existing models GoogleNet, VGG, AlexNet, ConvNet, when the proposed method is applied.



### PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations
- **Arxiv ID**: http://arxiv.org/abs/2008.01639v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.01639v2)
- **Published**: 2020-08-04 15:34:46+00:00
- **Updated**: 2021-02-05 13:32:02+00:00
- **Authors**: Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Carsten Stoll, Christian Theobalt
- **Comment**: 25 pages, including supplementary material. Code:
  https://github.com/edgar-tr/patchnets Project page:
  https://gvv.mpi-inf.mpg.de/projects/PatchNets/
- **Journal**: None
- **Summary**: Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.



### Simultaneous Semantic Alignment Network for Heterogeneous Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.01677v2
- **DOI**: 10.1145/3394171.3413995
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01677v2)
- **Published**: 2020-08-04 16:20:37+00:00
- **Updated**: 2020-08-05 03:04:20+00:00
- **Authors**: Shuang Li, Binhui Xie, Jiashu Wu, Ying Zhao, Chi Harold Liu, Zhengming Ding
- **Comment**: Accepted at ACM MM 2020
- **Journal**: None
- **Summary**: Heterogeneous domain adaptation (HDA) transfers knowledge across source and target domains that present heterogeneities e.g., distinct domain distributions and difference in feature type or dimension. Most previous HDA methods tackle this problem through learning a domain-invariant feature subspace to reduce the discrepancy between domains. However, the intrinsic semantic properties contained in data are under-explored in such alignment strategy, which is also indispensable to achieve promising adaptability. In this paper, we propose a Simultaneous Semantic Alignment Network (SSAN) to simultaneously exploit correlations among categories and align the centroids for each category across domains. In particular, we propose an implicit semantic correlation loss to transfer the correlation knowledge of source categorical prediction distributions to target domain. Meanwhile, by leveraging target pseudo-labels, a robust triplet-centroid alignment mechanism is explicitly applied to align feature representations for each category. Notably, a pseudo-label refinement procedure with geometric similarity involved is introduced to enhance the target pseudo-label assignment accuracy. Comprehensive experiments on various HDA tasks across text-to-image, image-to-image and text-to-text successfully validate the superiority of our SSAN against state-of-the-art HDA methods. The code is publicly available at https://github.com/BIT-DA/SSAN.



### Applying Incremental Deep Neural Networks-based Posture Recognition Model for Injury Risk Assessment in Construction
- **Arxiv ID**: http://arxiv.org/abs/2008.01679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01679v1)
- **Published**: 2020-08-04 16:27:25+00:00
- **Updated**: 2020-08-04 16:27:25+00:00
- **Authors**: Junqi Zhao, Esther Obonyo
- **Comment**: 27 pages, journal manuscript
- **Journal**: None
- **Summary**: Monitoring awkward postures is a proactive prevention for Musculoskeletal Disorders (MSDs)in construction. Machine Learning (ML) models have shown promising results for posture recognition from Wearable Sensors. However, further investigations are needed concerning: i) Incremental Learning (IL), where trained models adapt to learn new postures and control the forgetting of learned postures; ii) MSDs assessment with recognized postures. This study proposed an incremental Convolutional Long Short-Term Memory (CLN) model, investigated effective IL strategies, and evaluated MSDs assessment using recognized postures. Tests with nine workers showed the CLN model with shallow convolutional layers achieved high recognition performance (F1 Score) under personalized (0.87) and generalized (0.84) modeling. Generalized shallow CLN model under Many-to-One IL scheme can balance the adaptation (0.73) and forgetting of learnt subjects (0.74). MSDs assessment using postures recognized from incremental CLN model had minor difference with ground-truth, which demonstrates the high potential for automated MSDs monitoring in construction.



### SoloGAN: Multi-domain Multimodal Unpaired Image-to-Image Translation via a Single Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2008.01681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01681v3)
- **Published**: 2020-08-04 16:31:15+00:00
- **Updated**: 2022-06-28 18:35:53+00:00
- **Authors**: Shihua Huang, Cheng He, Ran Cheng
- **Comment**: pages 14, 15 figures
- **Journal**: IEEE Transactions on Artificial Intelligence 2022
- **Summary**: Despite significant advances in image-to-image (I2I) translation with generative adversarial networks (GANs), it remains challenging to effectively translate an image to a set of diverse images in multiple target domains using a single pair of generator and discriminator. Existing I2I translation methods adopt multiple domain-specific content encoders for different domains, where each domain-specific content encoder is trained with images from the same domain only. Nevertheless, we argue that the content (domain-invariance) features should be learned from images among all of the domains. Consequently, each domain-specific content encoder of existing schemes fails to extract the domain-invariant features efficiently. To address this issue, we present a flexible and general SoloGAN model for efficient multimodal I2I translation among multiple domains with unpaired data. In contrast to existing methods, the SoloGAN algorithm uses a single projection discriminator with an additional auxiliary classifier and shares the encoder and generator for all domains. Consequently, the SoloGAN can be trained effectively with images from all domains such that the domain-invariance content representation can be efficiently extracted. Qualitative and quantitative results over a wide range of datasets against several counterparts and variants of the SoloGAN demonstrate the merits of the method, especially for challenging I2I translation datasets, i.e., datasets involving extreme shape variations or need to keep the complex backgrounds unchanged after translations. Furthermore, we demonstrate the contribution of each component in SoloGAN by ablation studies.



### MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition in UAV Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.01699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01699v2)
- **Published**: 2020-08-04 17:02:29+00:00
- **Updated**: 2020-08-08 04:28:59+00:00
- **Authors**: Murari Mandal, Lav Kush Kumar, Santosh Kumar Vipparthi
- **Comment**: None
- **Journal**: ACM Multimedia (ACM MM-2020)
- **Summary**: Visual data collected from Unmanned Aerial Vehicles (UAVs) has opened a new frontier of computer vision that requires automated analysis of aerial images/videos. However, the existing UAV datasets primarily focus on object detection. An object detector does not differentiate between the moving and non-moving objects. Given a real-time UAV video stream, how can we both localize and classify the moving objects, i.e. perform moving object recognition (MOR)? The MOR is one of the essential tasks to support various UAV vision-based applications including aerial surveillance, search and rescue, event recognition, urban and rural scene understanding.To the best of our knowledge, no labeled dataset is available for MOR evaluation in UAV videos. Therefore, in this paper, we introduce MOR-UAV, a large-scale video dataset for MOR in aerial videos. We achieve this by labeling axis-aligned bounding boxes for moving objects which requires less computational resources than producing pixel-level estimates. We annotate 89,783 moving object instances collected from 30 UAV videos, consisting of 10,948 frames in various scenarios such as weather conditions, occlusion, changing flying altitude and multiple camera views. We assigned the labels for two categories of vehicles (car and heavy vehicle). Furthermore, we propose a deep unified framework MOR-UAVNet for MOR in UAV videos. Since, this is a first attempt for MOR in UAV videos, we present 16 baseline results based on the proposed framework over the MOR-UAV dataset through quantitative and qualitative experiments. We also analyze the motion-salient regions in the network through multiple layer visualizations. The MOR-UAVNet works online at inference as it requires only few past frames. Moreover, it doesn't require predefined target initialization from user. Experiments also demonstrate that the MOR-UAV dataset is quite challenging.



### Progressive Update Guided Interdependent Networks for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2008.01701v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01701v4)
- **Published**: 2020-08-04 17:05:48+00:00
- **Updated**: 2023-06-07 17:28:39+00:00
- **Authors**: Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, Prabir Kumar Biswas
- **Comment**: First two authors contributed equally. This work has been submitted
  to the IEEE for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible. Project
  Website: https://aupendu.github.io/progressive-dehaze
- **Journal**: None
- **Summary**: Images with haze of different varieties often pose a significant challenge to dehazing. Therefore, guidance by estimates of haze parameters related to the variety would be beneficial, and their progressive update jointly with haze reduction will allow effective dehazing. To this end, we propose a multi-network dehazing framework containing novel interdependent dehazing and haze parameter updater networks that operate in a progressive manner. The haze parameters, transmission map and atmospheric light, are first estimated using dedicated convolutional networks that allow color-cast handling. The estimated parameters are then used to guide our dehazing module, where the estimates are progressively updated by novel convolutional networks. The updating takes place jointly with progressive dehazing using a network that invokes inter-step dependencies. The joint progressive updating and dehazing gradually modify the haze parameter values toward achieving effective dehazing. Through different studies, our dehazing framework is shown to be more effective than image-to-image mapping and predefined haze formation model based dehazing. The framework is also found capable of handling a wide variety of hazy conditions wtih different types and amounts of haze and color casts. Our dehazing framework is qualitatively and quantitatively found to outperform the state-of-the-art on synthetic and real-world hazy images of multiple datasets with varied haze conditions.



### An artificial intelligence system for predicting the deterioration of COVID-19 patients in the emergency department
- **Arxiv ID**: http://arxiv.org/abs/2008.01774v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01774v2)
- **Published**: 2020-08-04 19:20:31+00:00
- **Updated**: 2020-11-04 02:36:36+00:00
- **Authors**: Farah E. Shamout, Yiqiu Shen, Nan Wu, Aakash Kaku, Jungkyu Park, Taro Makino, Stanisław Jastrzębski, Jan Witowski, Duo Wang, Ben Zhang, Siddhant Dogra, Meng Cao, Narges Razavian, David Kudlowitz, Lea Azour, William Moore, Yvonne W. Lui, Yindalon Aphinyanaphongs, Carlos Fernandez-Granda, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: During the coronavirus disease 2019 (COVID-19) pandemic, rapid and accurate triage of patients at the emergency department is critical to inform decision-making. We propose a data-driven approach for automatic prediction of deterioration risk using a deep neural network that learns from chest X-ray images and a gradient boosting model that learns from routine clinical variables. Our AI prognosis system, trained using data from 3,661 patients, achieves an area under the receiver operating characteristic curve (AUC) of 0.786 (95% CI: 0.745-0.830) when predicting deterioration within 96 hours. The deep neural network extracts informative areas of chest X-ray images to assist clinicians in interpreting the predictions and performs comparably to two radiologists in a reader study. In order to verify performance in a real clinical setting, we silently deployed a preliminary version of the deep neural network at New York University Langone Health during the first wave of the pandemic, which produced accurate predictions in real-time. In summary, our findings demonstrate the potential of the proposed system for assisting front-line physicians in the triage of COVID-19 patients.



### Making Sense of CNNs: Interpreting Deep Representations & Their Invariances with INNs
- **Arxiv ID**: http://arxiv.org/abs/2008.01777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01777v1)
- **Published**: 2020-08-04 19:27:46+00:00
- **Updated**: 2020-08-04 19:27:46+00:00
- **Authors**: Robin Rombach, Patrick Esser, Björn Ommer
- **Comment**: ECCV 2020. Project page and code at
  https://compvis.github.io/invariances/
- **Journal**: None
- **Summary**: To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance. Our implementation is available at https://compvis.github.io/invariances/ .



### PAI-BPR: Personalized Outfit Recommendation Scheme with Attribute-wise Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2008.01780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.01780v1)
- **Published**: 2020-08-04 19:30:06+00:00
- **Updated**: 2020-08-04 19:30:06+00:00
- **Authors**: Dikshant Sagar, Jatin Garg, Prarthana Kansal, Sejal Bhalla, Rajiv Ratn Shah, Yi Yu
- **Comment**: 10 pages, 5 figures, to be published in IEEE BigMM, 2020
- **Journal**: None
- **Summary**: Fashion is an important part of human experience. Events such as interviews, meetings, marriages, etc. are often based on clothing styles. The rise in the fashion industry and its effect on social influencing have made outfit compatibility a need. Thus, it necessitates an outfit compatibility model to aid people in clothing recommendation. However, due to the highly subjective nature of compatibility, it is necessary to account for personalization. Our paper devises an attribute-wise interpretable compatibility scheme with personal preference modelling which captures user-item interaction along with general item-item interaction. Our work solves the problem of interpretability in clothing matching by locating the discordant and harmonious attributes between fashion items. Extensive experiment results on IQON3000, a publicly available real-world dataset, verify the effectiveness of the proposed model.



### Entropy Guided Adversarial Model for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.01786v1
- **DOI**: 10.1016/j.neucom.2020.11.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01786v1)
- **Published**: 2020-08-04 19:39:12+00:00
- **Updated**: 2020-08-04 19:39:12+00:00
- **Authors**: Sabrina Narimene Benassou, Wuzhen Shi, Feng Jiang
- **Comment**: None
- **Journal**: Neurocomputing, Volume 429, 2021, Pages 60-68, ISSN 0925-2312
- **Summary**: Weakly Supervised Object Localization is challenging because of the lack of bounding box annotations. Previous works tend to generate a class activation map i.e CAM to localize the object. Unfortunately, the network activates only the features that discriminate the object and does not activate the whole object. Some methods tend to remove some parts of the object to force the CNN to detect other features, whereas, others change the network structure to generate multiple CAMs from different levels of the model. In this present article, we propose to take advantage of the generalization ability of the network and train the model using clean examples and adversarial examples to localize the whole object. Adversarial examples are typically used to train robust models and are images where a perturbation is added. To get a good classification accuracy, the CNN trained with adversarial examples is forced to detect more features that discriminate the object. We futher propose to apply the shannon entropy on the CAMs generated by the network to guide it during training. Our method does not erase any part of the image neither does it change the network architecure and extensive experiments show that our Entropy Guided Adversarial model (EGA model) improved performance on state of the arts benchmarks for both localization and classification accuracy.



### Deep Learning Based Early Diagnostics of Parkinsons Disease
- **Arxiv ID**: http://arxiv.org/abs/2008.01792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PL, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01792v1)
- **Published**: 2020-08-04 19:50:52+00:00
- **Updated**: 2020-08-04 19:50:52+00:00
- **Authors**: Elcin Huseyn
- **Comment**: None
- **Journal**: None
- **Summary**: In the world, about 7 to 10 million elderly people are suffering from Parkinson's Disease (PD) disease. Parkinson's disease is a common neurological degenerative disease, and its clinical characteristics are Tremors, rigidity, bradykinesia, and decreased autonomy. Its clinical manifestations are very similar to Multiple System Atrophy (MSA) disorders. Studies have shown that patients with Parkinson's disease often reach an irreparable situation when diagnosed, so As Parkinson's disease can be distinguished from MSA disease and get an early diagnosis, people are constantly exploring new methods. With the advent of the era of big data, deep learning has made major breakthroughs in image recognition and classification. Therefore, this study proposes to use The deep learning method to realize the diagnosis of Parkinson's disease, multiple system atrophy, and healthy people. This data source is from Istanbul University Cerrahpasa Faculty of Medicine Hospital. The processing of the original magnetic resonance image (Magnetic Resonance Image, MRI) is guided by the doctor of Istanbul University Cerrahpasa Faculty of Medicine Hospital. The focus of this experiment is to improve the existing neural network so that it can obtain good results in medical image recognition and diagnosis. An improved algorithm was proposed based on the pathological characteristics of Parkinson's disease, and good experimental results were obtained by comparing indicators such as model loss and accuracy.



### Fast Nonconvex $T_2^*$ Mapping Using ADMM
- **Arxiv ID**: http://arxiv.org/abs/2008.01806v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01806v1)
- **Published**: 2020-08-04 20:08:43+00:00
- **Updated**: 2020-08-04 20:08:43+00:00
- **Authors**: Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance (MR)-$T_2^*$ mapping is widely used to study hemorrhage, calcification and iron deposition in various clinical applications, it provides a direct and precise mapping of desired contrast in the tissue. However, the long acquisition time required by conventional 3D high-resolution $T_2^*$ mapping method causes discomfort to patients and introduces motion artifacts to reconstructed images, which limits its wider applicability. In this paper we address this issue by performing $T_2^*$ mapping from undersampled data using compressive sensing (CS). We formulate the reconstruction as a nonconvex problem that can be decomposed into two subproblems. They can be solved either separately via the standard approach or jointly via the alternating direction method of multipliers (ADMM). Compared to previous CS-based approaches that only apply sparse regularization on the spin density $\boldsymbol X_0$ and the relaxation rate $\boldsymbol R_2^*$, our formulation enforces additional sparse priors on the $T_2^*$-weighted images at multiple echoes to improve the reconstruction performance. We performed convergence analysis of the proposed algorithm, evaluated its performance on in vivo data, and studied the effects of different sampling schemes. Experimental results showed that the proposed joint-recovery approach generally outperforms the state-of-the-art method, especially in the low-sampling rate regime, making it a preferred choice to perform fast 3D $T_2^*$ mapping in practice. The framework adopted in this work can be easily extended to other problems arising from MR or other imaging modalities with non-linearly coupled variables.



### High resolution neural texture synthesis with long range constraints
- **Arxiv ID**: http://arxiv.org/abs/2008.01808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01808v1)
- **Published**: 2020-08-04 20:11:27+00:00
- **Updated**: 2020-08-04 20:11:27+00:00
- **Authors**: Nicolas Gonthier, Yann Gousseau, Saïd Ladjal
- **Comment**: 25 pages, 18 figures. LOW RESOLUTION PDF: Images may show compression
  artifacts
- **Journal**: None
- **Summary**: The field of texture synthesis has witnessed important progresses over the last years, most notably through the use of Convolutional Neural Networks. However, neural synthesis methods still struggle to reproduce large scale structures, especially with high resolution textures. To address this issue, we first introduce a simple multi-resolution framework that efficiently accounts for long-range dependency. Then, we show that additional statistical constraints further improve the reproduction of textures with strong regularity. This can be achieved by constraining both the Gram matrices of a neural network and the power spectrum of the image. Alternatively one may constrain only the autocorrelation of the features of the network and drop the Gram matrices constraints. In an experimental part, the proposed methods are then extensively tested and compared to alternative approaches, both in an unsupervised way and through a user study. Experiments show the interest of the multi-scale scheme for high resolution textures and the interest of combining it with additional constraints for regular textures.



### Deep Multi Depth Panoramas for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.01815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.01815v1)
- **Published**: 2020-08-04 20:29:15+00:00
- **Updated**: 2020-08-04 20:29:15+00:00
- **Authors**: Kai-En Lin, Zexiang Xu, Ben Mildenhall, Pratul P. Srinivasan, Yannick Hold-Geoffroy, Stephen DiVerdi, Qi Sun, Kalyan Sunkavalli, Ravi Ramamoorthi
- **Comment**: Published at the European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: We propose a learning-based approach for novel view synthesis for multi-camera 360$^{\circ}$ panorama capture rigs. Previous work constructs RGBD panoramas from such data, allowing for view synthesis with small amounts of translation, but cannot handle the disocclusions and view-dependent effects that are caused by large translations. To address this issue, we present a novel scene representation - Multi Depth Panorama (MDP) - that consists of multiple RGBD$\alpha$ panoramas that represent both scene geometry and appearance. We demonstrate a deep neural network-based method to reconstruct MDPs from multi-camera 360$^{\circ}$ images. MDPs are more compact than previous 3D scene representations and enable high-quality, efficient new view rendering. We demonstrate this via experiments on both synthetic and real data and comparisons with previous state-of-the-art methods spanning both learning-based approaches and classical RGBD-based methods.



### Graph Convolution with Low-rank Learnable Local Filters
- **Arxiv ID**: http://arxiv.org/abs/2008.01818v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01818v2)
- **Published**: 2020-08-04 20:34:59+00:00
- **Updated**: 2020-10-11 17:07:57+00:00
- **Authors**: Xiuyuan Cheng, Zichen Miao, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric variations like rotation, scaling, and viewpoint changes pose a significant challenge to visual understanding. One common solution is to directly model certain intrinsic structures, e.g., using landmarks. However, it then becomes non-trivial to build effective deep models, especially when the underlying non-Euclidean grid is irregular and coarse. Recent deep models using graph convolutions provide an appropriate framework to handle such non-Euclidean data, but many of them, particularly those based on global graph Laplacians, lack expressiveness to capture local features required for representation of signals lying on the non-Euclidean grid. The current paper introduces a new type of graph convolution with learnable low-rank local filters, which is provably more expressive than previous spectral graph convolution methods. The model also provides a unified framework for both spectral and spatial graph convolutions. To improve model robustness, regularization by local graph Laplacians is introduced. The representation stability against input graph data perturbation is theoretically proved, making use of the graph filter locality and the local graph regularization. Experiments on spherical mesh data, real-world facial expression recognition/skeleton-based action recognition data, and data with simulated graph noise show the empirical advantage of the proposed model.



### Stabilizing Deep Tomographic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.01846v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01846v5)
- **Published**: 2020-08-04 21:35:32+00:00
- **Updated**: 2021-09-13 16:25:34+00:00
- **Authors**: Weiwen Wu, Dianlin Hu, Wenxiang Cong, Hongming Shan, Shaoyu Wang, Chuang Niu, Pingkun Yan, Hengyong Yu, Varut Vardhanabhuti, Ge Wang
- **Comment**: 78 pages, 30 figures, 149 references
- **Journal**: None
- **Summary**: Tomographic image reconstruction with deep learning is an emerging field, but a recent landmark study reveals that several deep reconstruction networks are unstable for computed tomography (CT) and magnetic resonance imaging (MRI). Specifically, three kinds of instabilities were reported: (1) strong image artefacts from tiny perturbations, (2) small features missing in a deeply reconstructed image, and (3) decreased imaging performance with increased input data. On the other hand, compressed sensing (CS) inspired reconstruction methods do not suffer from these instabilities because of their built-in kernel awareness. For deep reconstruction to realize its full potential and become a mainstream approach for tomographic imaging, it is thus critically important to meet this challenge by stabilizing deep reconstruction networks. Here we propose an Analytic Compressed Iterative Deep (ACID) framework to address this challenge. ACID synergizes a deep reconstruction network trained on big data, kernel awareness from CS-inspired processing, and iterative refinement to minimize the data residual relative to real measurement. Our study demonstrates that the deep reconstruction using ACID is accurate and stable, and sheds light on the converging mechanism of the ACID iteration under a Bounded Relative Error Norm (BREN) condition. In particular, the study shows that ACID-based reconstruction is resilient against adversarial attacks, superior to classic sparsity-regularized reconstruction alone, and eliminates the three kinds of instabilities. We anticipate that this integrative data-driven approach will help promote development and translation of deep tomographic image reconstruction networks into clinical applications.



### Importance of Self-Consistency in Active Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.01860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01860v1)
- **Published**: 2020-08-04 22:18:35+00:00
- **Updated**: 2020-08-04 22:18:35+00:00
- **Authors**: S. Alireza Golestaneh, Kris M. Kitani
- **Comment**: Accepted in The British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: We address the task of active learning in the context of semantic segmentation and show that self-consistency can be a powerful source of self-supervision to greatly improve the performance of a data-driven model with access to only a small amount of labeled data. Self-consistency uses the simple observation that the results of semantic segmentation for a specific image should not change under transformations like horizontal flipping (i.e., the results should only be flipped). In other words, the output of a model should be consistent under equivariant transformations. The self-supervisory signal of self-consistency is particularly helpful during active learning since the model is prone to overfitting when there is only a small amount of labeled training data. In our proposed active learning framework, we iteratively extract small image patches that need to be labeled, by selecting image patches that have high uncertainty (high entropy) under equivariant transformations. We enforce pixel-wise self-consistency between the outputs of segmentation network for each image and its transformation (horizontally flipped) to utilize the rich self-supervisory information and reduce the uncertainty of the network. In this way, we are able to find the image patches over which the current model struggles the most to classify. By iteratively training over these difficult image patches, our experiments show that our active learning approach reaches $\sim96\%$ of the top performance of a model trained on all data, by using only $12\%$ of the total data on benchmark semantic segmentation datasets (e.g., CamVid and Cityscapes).



### From Human Mesenchymal Stromal Cells to Osteosarcoma Cells Classification by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01864v1
- **DOI**: 10.3233/JIFS-179332
- **Categories**: **cs.CV**, I.4; I.4.6; I.2.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.01864v1)
- **Published**: 2020-08-04 22:23:58+00:00
- **Updated**: 2020-08-04 22:23:58+00:00
- **Authors**: Mario D'Acunto, Massimo Martinelli, Davide Moroni
- **Comment**: Submitted authors' version
- **Journal**: Journal of Intelligent & Fuzzy Systems, vol. 37, no. 6, pp.
  7199-7206, 2019
- **Summary**: Early diagnosis of cancer often allows for a more vast choice of therapy opportunities. After a cancer diagnosis, staging provides essential information about the extent of disease in the body and the expected response to a particular treatment. The leading importance of classifying cancer patients at the early stage into high or low-risk groups has led many research teams, both from the biomedical and bioinformatics field, to study the application of Deep Learning (DL) methods. The ability of DL to detect critical features from complex datasets is a significant achievement in early diagnosis and cell cancer progression. In this paper, we focus the attention on osteosarcoma. Osteosarcoma is one of the primary malignant bone tumors which usually afflicts people in adolescence. Our contribution to the classification of osteosarcoma cells is made as follows: a DL approach is applied to discriminate human Mesenchymal Stromal Cells (MSCs) from osteosarcoma cells and to classify the different cell populations under investigation. Glass slides of differ-ent cell populations were cultured including MSCs, differentiated in healthy bone cells (osteoblasts) and osteosarcoma cells, both single cell populations or mixed. Images of such samples of isolated cells (single-type of mixed) are recorded with traditional optical microscopy. DL is then applied to identify and classify single cells. Proper data augmentation techniques and cross-fold validation are used to appreciate the capabilities of a convolutional neural network to address the cell detection and classification problem. Based on the results obtained on individual cells, and to the versatility and scalability of our DL approach, the next step will be its application to discriminate and classify healthy or cancer tissues to advance digital pathology.



### Implicit Saliency in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.01874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.01874v1)
- **Published**: 2020-08-04 23:14:24+00:00
- **Updated**: 2020-08-04 23:14:24+00:00
- **Authors**: Yutong Sun, Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show that existing recognition and localization deep architectures, that have not been exposed to eye tracking data or any saliency datasets, are capable of predicting the human visual saliency. We term this as implicit saliency in deep neural networks. We calculate this implicit saliency using expectancy-mismatch hypothesis in an unsupervised fashion. Our experiments show that extracting saliency in this fashion provides comparable performance when measured against the state-of-art supervised algorithms. Additionally, the robustness outperforms those algorithms when we add large noise to the input images. Also, we show that semantic features contribute more than low-level features for human visual saliency detection.



### Rotation-Invariant Gait Identification with Quaternion Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07393v1)
- **Published**: 2020-08-04 23:22:12+00:00
- **Updated**: 2020-08-04 23:22:12+00:00
- **Authors**: Bowen Jing, Vinay Prabhu, Angela Gu, John Whaley
- **Comment**: None
- **Journal**: None
- **Summary**: A desireable property of accelerometric gait-based identification systems is robustness to new device orientations presented by users during testing but unseen during the training phase. However, traditional Convolutional neural networks (CNNs) used in these systems compensate poorly for such transformations. In this paper, we target this problem by introducing Quaternion CNN, a network architecture which is intrinsically layer-wise equivariant and globally invariant under 3D rotations of an array of input vectors. We show empirically that this network indeed significantly outperforms a traditional CNN in a multi-user rotation-invariant gait classification setting .Lastly, we demonstrate how the kernels learned by this QCNN can also be visualized as basis-independent but origin- and chirality-dependent trajectory fragments in the euclidean space, thus yielding a novel mode of feature visualization and extraction.



### An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites
- **Arxiv ID**: http://arxiv.org/abs/2008.01882v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01882v3)
- **Published**: 2020-08-04 23:51:06+00:00
- **Updated**: 2020-12-21 20:37:19+00:00
- **Authors**: Giovanni Pasqualino, Antonino Furnari, Giovanni Signorello, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing artworks in a cultural site using images acquired from the user's point of view (First Person Vision) allows to build interesting applications for both the visitors and the site managers. However, current object detection algorithms working in fully supervised settings need to be trained with large quantities of labeled data, whose collection requires a lot of times and high costs in order to achieve good performance. Using synthetic data generated from the 3D model of the cultural site to train the algorithms can reduce these costs. On the other hand, when these models are tested with real images, a significant drop in performance is observed due to the differences between real and synthetic images. In this study we consider the problem of Unsupervised Domain Adaptation for object detection in cultural sites. To address this problem, we created a new dataset containing both synthetic and real images of 16 different artworks. We hence investigated different domain adaptation techniques based on one-stage and two-stage object detector, image-to-image translation and feature alignment. Based on the observation that single-stage detectors are more robust to the domain shift in the considered settings, we proposed a new method which builds on RetinaNet and feature alignment that we called DA-RetinaNet. The proposed approach achieves better results than compared methods on the proposed dataset and on Cityscapes. To support research in this field we release the dataset at the following link https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed architecture at https://github.com/fpv-iplab/DA-RetinaNet.



