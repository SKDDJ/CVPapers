# Arxiv Papers in cs.CV on 2020-08-22
### ScribbleBox: Interactive Annotation Framework for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.09721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.09721v1)
- **Published**: 2020-08-22 00:33:10+00:00
- **Updated**: 2020-08-22 00:33:10+00:00
- **Authors**: Bowen Chen, Huan Ling, Xiaohui Zeng, Gao Jun, Ziyue Xu, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Manually labeling video datasets for segmentation tasks is extremely time consuming. In this paper, we introduce ScribbleBox, a novel interactive framework for annotating object instances with masks in videos. In particular, we split annotation into two steps: annotating objects with tracked boxes, and labeling masks inside these tracks. We introduce automation and interaction in both steps. Box tracks are annotated efficiently by approximating the trajectory using a parametric curve with a small number of control points which the annotator can interactively correct. Our approach tolerates a modest amount of noise in the box placements, thus typically only a few clicks are needed to annotate tracked boxes to a sufficient accuracy. Segmentation masks are corrected via scribbles which are efficiently propagated through time. We show significant performance gains in annotation efficiency over past work. We show that our ScribbleBox approach reaches 88.92% J&F on DAVIS2017 with 9.14 clicks per box track, and 4 frames of scribble annotation.



### PNEN: Pyramid Non-Local Enhanced Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.09742v1
- **DOI**: 10.1109/TIP.2020.3019644
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09742v1)
- **Published**: 2020-08-22 03:10:48+00:00
- **Updated**: 2020-08-22 03:10:48+00:00
- **Authors**: Feida Zhu, Chaowei Fang, Kai-Kuang Ma
- **Comment**: Accepted by Transactions on Image Processing
- **Journal**: None
- **Summary**: Existing neural networks proposed for low-level image processing tasks are usually implemented by stacking convolution layers with limited kernel size. Every convolution layer merely involves in context information from a small local neighborhood. More contextual features can be explored as more convolution layers are adopted. However it is difficult and costly to take full advantage of long-range dependencies. We propose a novel non-local module, Pyramid Non-local Block, to build up connection between every pixel and all remain pixels. The proposed module is capable of efficiently exploiting pairwise dependencies between different scales of low-level structures. The target is fulfilled through first learning a query feature map with full resolution and a pyramid of reference feature maps with downscaled resolutions. Then correlations with multi-scale reference features are exploited for enhancing pixel-level feature representation. The calculation procedure is economical considering memory consumption and computational cost. Based on the proposed module, we devise a Pyramid Non-local Enhanced Networks for edge-preserving image smoothing which achieves state-of-the-art performance in imitating three classical image smoothing algorithms. Additionally, the pyramid non-local block can be directly incorporated into convolution neural networks for other image restoration tasks. We integrate it into two existing methods for image denoising and single image super-resolution, achieving consistently improved performance.



### A Efficient Multimodal Framework for Large Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals
- **Arxiv ID**: http://arxiv.org/abs/2008.09743v2
- **DOI**: 10.1145/3490686
- **Categories**: **cs.SD**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09743v2)
- **Published**: 2020-08-22 03:13:20+00:00
- **Updated**: 2021-12-02 03:04:51+00:00
- **Authors**: Guanghao Yin, Shouqian Sun, Dian Yu, Dejian Li, Kejun Zhang
- **Comment**: ACM Transactions on Multimedia Computing, Communications, and
  Applications (Acceptance 07-Oct-2021)
- **Journal**: None
- **Summary**: Considerable attention has been paid for physiological signal-based emotion recognition in field of affective computing. For the reliability and user friendly acquisition, Electrodermal Activity (EDA) has great advantage in practical applications. However, the EDA-based emotion recognition with hundreds of subjects still lacks effective solution. In this paper, our work makes an attempt to fuse the subject individual EDA features and the external evoked music features. And we propose an end-to-end multimodal framework, the 1-dimensional residual temporal and channel attention network (RTCAN-1D). For EDA features, the novel convex optimization-based EDA (CvxEDA) method is applied to decompose EDA signals into pahsic and tonic signals for mining the dynamic and steady features. The channel-temporal attention mechanism for EDA-based emotion recognition is firstly involved to improve the temporal- and channel-wise representation. For music features, we process the music signal with the open source toolkit openSMILE to obtain external feature vectors. The individual emotion features from EDA signals and external emotion benchmarks from music are fused in the classifing layers. We have conducted systematic comparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classes valance/arousal emotion recognition. Our proposed RTCAN-1D outperforms the existing state-of-the-art models, which also validate that our work provides an reliable and efficient solution for large scale emotion recognition. Our code has been released at https://github.com/guanghaoyin/RTCAN-1D.



### Towards Improved Human Action Recognition Using Convolutional Neural Networks and Multimodal Fusion of Depth and Inertial Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/2008.09747v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09747v1)
- **Published**: 2020-08-22 03:41:34+00:00
- **Updated**: 2020-08-22 03:41:34+00:00
- **Authors**: Zeeshan Ahmad, Naimul Khan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper attempts at improving the accuracy of Human Action Recognition (HAR) by fusion of depth and inertial sensor data. Firstly, we transform the depth data into Sequential Front view Images(SFI) and fine-tune the pre-trained AlexNet on these images. Then, inertial data is converted into Signal Images (SI) and another convolutional neural network (CNN) is trained on these images. Finally, learned features are extracted from both CNN, fused together to make a shared feature layer, and these features are fed to the classifier. We experiment with two classifiers, namely Support Vector Machines (SVM) and softmax classifier and compare their performances. The recognition accuracies of each modality, depth data alone and sensor data alone are also calculated and compared with fusion based accuracies to highlight the fact that fusion of modalities yields better results than individual modalities. Experimental results on UTD-MHAD and Kinect 2D datasets show that proposed method achieves state of the art results when compared to other recently proposed visual-inertial action recognition methods.



### Multidomain Multimodal Fusion For Human Action Recognition Using Inertial Sensors
- **Arxiv ID**: http://arxiv.org/abs/2008.09748v1
- **DOI**: 10.1109/BigMM.2019.00074
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.09748v1)
- **Published**: 2020-08-22 03:46:12+00:00
- **Updated**: 2020-08-22 03:46:12+00:00
- **Authors**: Zeeshan Ahmad, Naimul Khan
- **Comment**: None
- **Journal**: None
- **Summary**: One of the major reasons for misclassification of multiplex actions during action recognition is the unavailability of complementary features that provide the semantic information about the actions. In different domains these features are present with different scales and intensities. In existing literature, features are extracted independently in different domains, but the benefits from fusing these multidomain features are not realized. To address this challenge and to extract complete set of complementary information, in this paper, we propose a novel multidomain multimodal fusion framework that extracts complementary and distinct features from different domains of the input modality. We transform input inertial data into signal images, and then make the input modality multidomain and multimodal by transforming spatial domain information into frequency and time-spectrum domain using Discrete Fourier Transform (DFT) and Gabor wavelet transform (GWT) respectively. Features in different domains are extracted by Convolutional Neural networks (CNNs) and then fused by Canonical Correlation based Fusion (CCF) for improving the accuracy of human action recognition. Experimental results on three inertial datasets show the superiority of the proposed method when compared to the state-of-the-art.



### Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral Constrained Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2008.09753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09753v2)
- **Published**: 2020-08-22 04:25:08+00:00
- **Updated**: 2021-06-10 14:22:11+00:00
- **Authors**: Yi-Si Luo, Xi-Le Zhao, Tai-Xiang Jiang, Yu-Bang Zheng, Yi Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural network (CNN)-based methods are proposed for hyperspectral images (HSIs) denoising. Among them, unsupervised methods such as the deep image prior (DIP) have received much attention because these methods do not require any training data. However, DIP suffers from the semi-convergence behavior, i.e., the iteration of DIP needs to terminate by referring to the ground-truth image at the optimal iteration point. In this paper, we propose the spatial-spectral constrained deep image prior (S2DIP) for HSI mixed noise removal. Specifically, we incorporate DIP with a spatial-spectral total variation (SSTV) term to fully preserve the spatial-spectral local smoothness of the HSI and an $\ell_1$-norm term to capture the complex sparse noise. The proposed S2DIP jointly leverages the expressive power brought from the deep CNN without any training data and exploits the HSI and noise structures via hand-crafted priors. Thus, our method avoids the semi-convergence behavior, showing higher stabilities than DIP. Meanwhile, our method largely enhances the HSI denoising ability of DIP. To tackle the proposed denoising model, we develop an alternating direction multiplier method algorithm. Extensive experiments demonstrate that the proposed S2DIP outperforms optimization-based and supervised CNN-based state-of-the-art HSI denoising methods.



### What am I allowed to do here?: Online Learning of Context-Specific Norms by Pepper
- **Arxiv ID**: http://arxiv.org/abs/2009.05105v2
- **DOI**: 10.1007/978-3-030-62056-1_19
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.05105v2)
- **Published**: 2020-08-22 07:27:02+00:00
- **Updated**: 2021-01-13 06:57:12+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: The final authenticated publication is available online at
  https://doi.org/10.1007/978-3-030-62056-1_19
- **Journal**: International Conference on Social Robotics (ICSR), 2020
- **Summary**: Social norms support coordination and cooperation in society. With social robots becoming increasingly involved in our society, they also need to follow the social norms of the society. This paper presents a computational framework for learning contexts and the social norms present in a context in an online manner on a robot. The paper utilizes a recent state-of-the-art approach for incremental learning and adapts it for online learning of scenes (contexts). The paper further utilizes Dempster-Schafer theory to model context-specific norms. After learning the scenes (contexts), we use active learning to learn related norms. We test our approach on the Pepper robot by taking it through different scene locations. Our results show that Pepper can learn different scenes and related norms simply by communicating with a human partner in an online manner.



### A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability
- **Arxiv ID**: http://arxiv.org/abs/2008.09772v3
- **DOI**: 10.1109/TMI.2020.3037771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09772v3)
- **Published**: 2020-08-22 07:48:04+00:00
- **Updated**: 2020-11-11 10:49:15+00:00
- **Authors**: Yi Zhou, Boyang Wang, Lei Huang, Shanshan Cui, Ling Shao
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging (2020)
- **Summary**: People with diabetes are at risk of developing an eye disease called diabetic retinopathy (DR). This disease occurs when high blood glucose levels cause damage to blood vessels in the retina. Computer-aided DR diagnosis is a promising tool for early detection of DR and severity grading, due to the great success of deep learning. However, most current DR diagnosis systems do not achieve satisfactory performance or interpretability for ophthalmologists, due to the lack of training data with consistent and fine-grained annotations. To address this problem, we construct a large fine-grained annotated DR dataset containing 2,842 images (FGADR). This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis. We set up three benchmark tasks for evaluation: 1. DR lesion segmentation; 2. DR grading by joint classification and segmentation; 3. Transfer learning for ocular multi-disease identification. Moreover, a novel inductive transfer learning method is introduced for the third task. Extensive experiments using different state-of-the-art methods are conducted on our FGADR dataset, which can serve as baselines for future research.



### Chest Area Segmentation in Depth Images of Sleeping Patients
- **Arxiv ID**: http://arxiv.org/abs/2008.09773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09773v1)
- **Published**: 2020-08-22 07:54:32+00:00
- **Updated**: 2020-08-22 07:54:32+00:00
- **Authors**: Yoav Goldstein, Martin Schätz, Mireille Avigal
- **Comment**: 13 pages, 15 figures
- **Journal**: None
- **Summary**: Although the field of sleep study has greatly developed over the recent years, the most common and efficient way to detect sleep issues remains a sleep examination performed in a sleep laboratory, in a procedure called Polysomnography (PSG). This examination measures several vital signals during a full night's sleep using multiple sensors connected to the patient's body. Yet, despite being the golden standard, the connection of the sensors and the unfamiliar environment inevitably impact the quality of the patient's sleep and the examination itself. Therefore, with the novel development of more accurate and affordable 3D sensing devices, new approaches for non-contact sleep study emerged. These methods utilize different techniques with the purpose to extract the same sleep parameters, but remotely, eliminating the need of any physical connections to the patient's body. However, in order to enable reliable remote extraction, these methods require accurate identification of the basic Region of Interest (ROI) i.e. the chest area of the patient, a task that is currently holding back the development process, as it is performed manually for each patient. In this study, we propose an automatic chest area segmentation algorithm, that given an input set of 3D frames of a sleeping patient, outputs a segmentation image with the pixels that correspond to the chest area, and can then be used as an input to subsequent sleep analysis algorithms. Except for significantly speeding up the development process of the non-contact methods, accurate automatic segmentation can also enable a more precise feature extraction and it is shown it is already improving sensitivity of prior solutions on average 46.9% better compared to manual ROI selection. All mentioned will place the extraction algorithms of the non-contact methods as a leading candidate to replace the existing traditional methods used today.



### Traffic-Aware Multi-Camera Tracking of Vehicles Based on ReID and Camera Link Model
- **Arxiv ID**: http://arxiv.org/abs/2008.09785v2
- **DOI**: 10.1145/3394171.3413863
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09785v2)
- **Published**: 2020-08-22 08:54:47+00:00
- **Updated**: 2020-08-30 04:47:55+00:00
- **Authors**: Hung-Min Hsu, Yizhou Wang, Jenq-Neng Hwang
- **Comment**: Accepted by ACM International Conference on Multimedia 2020
- **Journal**: None
- **Summary**: Multi-target multi-camera tracking (MTMCT), i.e., tracking multiple targets across multiple cameras, is a crucial technique for smart city applications. In this paper, we propose an effective and reliable MTMCT framework for vehicles, which consists of a traffic-aware single camera tracking (TSCT) algorithm, a trajectory-based camera link model (CLM) for vehicle re-identification (ReID), and a hierarchical clustering algorithm to obtain the cross camera vehicle trajectories. First, the TSCT, which jointly considers vehicle appearance, geometric features, and some common traffic scenarios, is proposed to track the vehicles in each camera separately. Second, the trajectory-based CLM is adopted to facilitate the relationship between each pair of adjacently connected cameras and add spatio-temporal constraints for the subsequent vehicle ReID with temporal attention. Third, the hierarchical clustering algorithm is used to merge the vehicle trajectories among all the cameras to obtain the final MTMCT results. Our proposed MTMCT is evaluated on the CityFlow dataset and achieves a new state-of-the-art performance with IDF1 of 74.93%.



### Identity-Aware Multi-Sentence Video Description
- **Arxiv ID**: http://arxiv.org/abs/2008.09791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09791v1)
- **Published**: 2020-08-22 09:50:43+00:00
- **Updated**: 2020-08-22 09:50:43+00:00
- **Authors**: Jae Sung Park, Trevor Darrell, Anna Rohrbach
- **Comment**: Project link at
  https://sites.google.com/site/describingmovies/lsmdc-2019/
- **Journal**: None
- **Summary**: Standard video and movie description tasks abstract away from person identities, thus failing to link identities across sentences. We propose a multi-sentence Identity-Aware Video Description task, which overcomes this limitation and requires to re-identify persons locally within a set of consecutive clips. We introduce an auxiliary task of Fill-in the Identity, that aims to predict persons' IDs consistently within a set of clips, when the video descriptions are given. Our proposed approach to this task leverages a Transformer architecture allowing for coherent joint prediction of multiple IDs. One of the key components is a gender-aware textual representation as well an additional gender prediction objective in the main model. This auxiliary task allows us to propose a two-stage approach to Identity-Aware Video Description. We first generate multi-sentence video descriptions, and then apply our Fill-in the Identity model to establish links between the predicted person entities. To be able to tackle both tasks, we augment the Large Scale Movie Description Challenge (LSMDC) benchmark with new annotations suited for our problem statement. Experiments show that our proposed Fill-in the Identity model is superior to several baselines and recent works, and allows us to generate descriptions with locally re-identified people.



### Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory
- **Arxiv ID**: http://arxiv.org/abs/2008.09809v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09809v6)
- **Published**: 2020-08-22 11:01:46+00:00
- **Updated**: 2021-07-06 07:49:04+00:00
- **Authors**: Jialun Liu, Jingwei Zhang, Yi yang, Wenhui Li, Chi Zhang, Yifan Sun
- **Comment**: We modify the method based on the arXiv version. We don't want to
  publish it ,now. So we want to withdraw the arXiv version
- **Journal**: None
- **Summary**: This paper considers deep visual recognition on long-tailed data. To be general, we consider two applied scenarios, \ie, deep classification and deep metric learning. Under the long-tailed data distribution, the majority classes (\ie, tail classes) only occupy relatively few samples and are prone to lack of within-class diversity. A radical solution is to augment the tail classes with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ). We observe that during training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of \emph{weight jitters}. Consequentially, given a same image as the input, two historical editions of the model generate two different features in the deeply-embedded space, resulting in \emph{feature jitters}. Using a memory bank, we collect these (model or feature) jitters across multiple training iterations and get the so-called Memory-based Jitter. The accumulated jitters enhance the within-class diversity for the tail classes and consequentially improves long-tailed visual recognition. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, \emph{i.e.}, deep image classification and deep metric learning (on long-tailed data). Extensive experiments on five long-tailed classification benchmarks and two deep metric learning benchmarks demonstrate significant improvement. Moreover, the achieved performance are on par with the state of the art on both tasks.



### Self-Competitive Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.09824v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09824v1)
- **Published**: 2020-08-22 12:28:35+00:00
- **Updated**: 2020-08-22 12:28:35+00:00
- **Authors**: Iman Saberi, Fathiyeh Faghih
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have improved the accuracy of classification problems in lots of applications. One of the challenges in training a DNN is its need to be fed by an enriched dataset to increase its accuracy and avoid it suffering from overfitting. One way to improve the generalization of DNNs is to augment the training data with new synthesized adversarial samples. Recently, researchers have worked extensively to propose methods for data augmentation. In this paper, we generate adversarial samples to refine the Domains of Attraction (DoAs) of each class. In this approach, at each stage, we use the model learned by the primary and generated adversarial data (up to that stage) to manipulate the primary data in a way that look complicated to the DNN. The DNN is then retrained using the augmented data and then it again generates adversarial data that are hard to predict for itself. As the DNN tries to improve its accuracy by competing with itself (generating hard samples and then learning them), the technique is called Self-Competitive Neural Network (SCNN). To generate such samples, we pose the problem as an optimization task, where the network weights are fixed and use a gradient descent based method to synthesize adversarial samples that are on the boundary of their true labels and the nearest wrong labels. Our experimental results show that data augmentation using SCNNs can significantly increase the accuracy of the original network. As an example, we can mention improving the accuracy of a CNN trained with 1000 limited training data of MNIST dataset from 94.26% to 98.25%.



### From noisy point clouds to complete ear shapes: unsupervised pipeline
- **Arxiv ID**: http://arxiv.org/abs/2008.09831v3
- **DOI**: 10.1109/ACCESS.2021.3111811
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09831v3)
- **Published**: 2020-08-22 13:20:43+00:00
- **Updated**: 2022-02-04 07:49:06+00:00
- **Authors**: Filipa Valdeira, Ricardo Ferreira, Alessandra Micheletti, Cláudia Soares
- **Comment**: None
- **Journal**: IEEE Access 9 (2021) 127720-127734
- **Summary**: Ears are a particularly difficult region of the human face to model, not only due to the non-rigid deformations existing between shapes but also to the challenges in processing the retrieved data. The first step towards obtaining a good model is to have complete scans in correspondence, but these usually present a higher amount of occlusions, noise and outliers when compared to most face regions, thus requiring a specific procedure. Therefore, we propose a complete pipeline taking as input unordered 3D point clouds with the aforementioned problems, and producing as output a dataset in correspondence, with completion of the missing data. We provide a comparison of several state-of-the-art registration methods and propose a new approach for one of the steps of the pipeline, with better performance for our data.



### Revisiting Anchor Mechanisms for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.09837v1
- **DOI**: 10.1109/TIP.2020.3016486
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09837v1)
- **Published**: 2020-08-22 13:39:29+00:00
- **Updated**: 2020-08-22 13:39:29+00:00
- **Authors**: Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, Junwei Han
- **Comment**: Accept by TIP, code: https://github.com/LeYangNwpu/A2Net
- **Journal**: None
- **Summary**: Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.



### Data augmentation techniques for the Video Question Answering task
- **Arxiv ID**: http://arxiv.org/abs/2008.09849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09849v1)
- **Published**: 2020-08-22 14:34:55+00:00
- **Updated**: 2020-08-22 14:34:55+00:00
- **Authors**: Alex Falcon, Oswald Lanz, Giuseppe Serra
- **Comment**: 16 pages, 5 figures; to be published in Egocentric Perception,
  Interaction and Computing (EPIC) Workshop Proceedings, at ECCV 2020
- **Journal**: None
- **Summary**: Video Question Answering (VideoQA) is a task that requires a model to analyze and understand both the visual content given by the input video and the textual part given by the question, and the interaction between them in order to produce a meaningful answer. In our work we focus on the Egocentric VideoQA task, which exploits first-person videos, because of the importance of such task which can have impact on many different fields, such as those pertaining the social assistance and the industrial training. Recently, an Egocentric VideoQA dataset, called EgoVQA, has been released. Given its small size, models tend to overfit quickly. To alleviate this problem, we propose several augmentation techniques which give us a +5.5% improvement on the final accuracy over the considered baseline.



### Emergent symbolic language based deep medical image classification
- **Arxiv ID**: http://arxiv.org/abs/2008.09860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09860v1)
- **Published**: 2020-08-22 15:53:29+00:00
- **Updated**: 2020-08-22 15:53:29+00:00
- **Authors**: Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, Peter Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning systems for medical image classification have demonstrated exceptional capabilities for distinguishing between image based medical categories. However, they are severely hindered by their ina-bility to explain the reasoning behind their decision making. This is partly due to the uninterpretable continuous latent representations of neural net-works. Emergent languages (EL) have recently been shown to enhance the capabilities of neural networks by equipping them with symbolic represen-tations in the framework of referential games. Symbolic representations are one of the cornerstones of highly explainable good old fashioned AI (GOFAI) systems. In this work, we demonstrate for the first time, the emer-gence of deep symbolic representations of emergent language in the frame-work of image classification. We show that EL based classification models can perform as well as, if not better than state of the art deep learning mod-els. In addition, they provide a symbolic representation that opens up an entire field of possibilities of interpretable GOFAI methods involving symbol manipulation. We demonstrate the EL classification framework on immune cell marker based cell classification and chest X-ray classification using the CheXpert dataset. Code is available online at https://github.com/AriChow/EL.



### Symbolic Semantic Segmentation and Interpretation of COVID-19 Lung Infections in Chest CT volumes based on Emergent Languages
- **Arxiv ID**: http://arxiv.org/abs/2008.09866v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09866v1)
- **Published**: 2020-08-22 16:19:11+00:00
- **Updated**: 2020-08-22 16:19:11+00:00
- **Authors**: Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, Jianwei Qiu, Peter Tu
- **Comment**: None
- **Journal**: None
- **Summary**: The coronavirus disease (COVID-19) has resulted in a pandemic crippling the a breadth of services critical to daily life. Segmentation of lung infections in computerized tomography (CT) slices could be be used to improve diagnosis and understanding of COVID-19 in patients. Deep learning systems lack interpretability because of their black box nature. Inspired by human communication of complex ideas through language, we propose a symbolic framework based on emergent languages for the segmentation of COVID-19 infections in CT scans of lungs. We model the cooperation between two artificial agents - a Sender and a Receiver. These agents synergistically cooperate using emergent symbolic language to solve the task of semantic segmentation. Our game theoretic approach is to model the cooperation between agents unlike Generative Adversarial Networks (GANs). The Sender retrieves information from one of the higher layers of the deep network and generates a symbolic sentence sampled from a categorical distribution of vocabularies. The Receiver ingests the stream of symbols and cogenerates the segmentation mask. A private emergent language is developed that forms the communication channel used to describe the task of segmentation of COVID infections. We augment existing state of the art semantic segmentation architectures with our symbolic generator to form symbolic segmentation models. Our symbolic segmentation framework achieves state of the art performance for segmentation of lung infections caused by COVID-19. Our results show direct interpretation of symbolic sentences to discriminate between normal and infected regions, infection morphology and image characteristics. We show state of the art results for segmentation of COVID-19 lung infections in CT.



### NCS4CVR: Neuron-Connection Sharing for Multi-Task Learning in Video Conversion Rate Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.09872v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, H.3.3; H.3.5; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2008.09872v3)
- **Published**: 2020-08-22 16:48:08+00:00
- **Updated**: 2023-02-16 09:56:19+00:00
- **Authors**: Xuanji Xiao, Huabin Chen, Yuzhen Liu, Xing Yao, Pei Liu, Chaosheng Fan, Nian Ji, Xirong Jiang
- **Comment**: 6 pages,4 figures
- **Journal**: None
- **Summary**: Click-through rate (CTR) and post-click conversion rate (CVR) predictions are two fundamental modules in industrial ranking systems such as recommender systems, advertising, and search engines. Since CVR involves much fewer samples than CTR (known as the CVR data sparsity problem), most of the existing works try to leverage CTR&CVR multi-task learning to improve CVR performance. However, typical coarse-grained sub-network/layer sharing methods may introduce conflicts and lead to performance degradation, since not every neuron or neuron connection in one layer should be shared between CVR and CTR tasks. This is because users may have different fine-grained content feature preferences between deep consumption and click behavior, represented by CVR and CTR, respectively. To address this sharing&conflict problem, we propose a novel multi-task CVR modeling scheme with neuron-connection level sharing named NCS4CVR, which can automatically and flexibly learn which neuron weights are shared or not shared without artificial experience. Compared with previous layer-level sharing methods, this is the first time that a fine-grained CTR&CVR sharing method at the neuron connection level is proposed, which is a research paradigm shift in the sharing level. Both offline and online experiments demonstrate that our method outperforms both the single-task model and the layer-level sharing model. Our proposed method has now been successfully deployed in an industry video recommender system serving major traffic.



### Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss
- **Arxiv ID**: http://arxiv.org/abs/2008.09880v1
- **DOI**: 10.1109/TAI.2020.3026982
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09880v1)
- **Published**: 2020-08-22 17:13:33+00:00
- **Updated**: 2020-08-22 17:13:33+00:00
- **Authors**: Ujjal Kr Dutta, Mehrtash Harandi, Chellu Chandra Sekhar
- **Comment**: In the IEEE Transactions on Artificial Intelligence (IEEE TAI)
- **Journal**: None
- **Summary**: Metric learning is an important problem in machine learning. It aims to group similar examples together. Existing state-of-the-art metric learning approaches require class labels to learn a metric. As obtaining class labels in all applications is not feasible, we propose an unsupervised approach that learns a metric without making use of class labels. The lack of class labels is compensated by obtaining pseudo-labels of data using a graph-based clustering approach. The pseudo-labels are used to form triplets of examples, which guide the metric learning. We propose a probabilistic loss that minimizes the chances of each triplet violating an angular constraint. A weight function, and an orthogonality constraint in the objective speeds up the convergence and avoids a model collapse. We also provide a stochastic formulation of our method to scale up to large-scale datasets. Our studies demonstrate the competitiveness of our approach against state-of-the-art methods. We also thoroughly study the effect of the different components of our method.



### Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary Edema Assessment
- **Arxiv ID**: http://arxiv.org/abs/2008.09884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09884v1)
- **Published**: 2020-08-22 17:28:39+00:00
- **Updated**: 2020-08-22 17:28:39+00:00
- **Authors**: Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob Andreas, Xin Wang, Seth Berkowitz, Steven Horng, Peter Szolovits, Polina Golland
- **Comment**: The two first authors contributed equally. To be published in the
  proceedings of MICCAI 2020
- **Journal**: None
- **Summary**: We propose and demonstrate a novel machine learning algorithm that assesses pulmonary edema severity from chest radiographs. While large publicly available datasets of chest radiographs and free-text radiology reports exist, only limited numerical edema severity labels can be extracted from radiology reports. This is a significant challenge in learning such models for image classification. To take advantage of the rich information present in the radiology reports, we develop a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs at inference time. Our experimental results suggest that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained on images only. We also show the use of the text for explaining the image classification by the joint model. To the best of our knowledge, our approach is the first to leverage free-text radiology reports for improving the image model performance in this application. Our code is available at https://github.com/RayRuizhiLiao/joint_chestxray.



### Supervision Levels Scale (SLS)
- **Arxiv ID**: http://arxiv.org/abs/2008.09890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09890v1)
- **Published**: 2020-08-22 18:03:20+00:00
- **Updated**: 2020-08-22 18:03:20+00:00
- **Authors**: Dima Damen, Michael Wray
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a three-dimensional discrete and incremental scale to encode a method's level of supervision - i.e. the data and labels used when training a model to achieve a given performance. We capture three aspects of supervision, that are known to give methods an advantage while requiring additional costs: pre-training, training labels and training data. The proposed three-dimensional scale can be included in result tables or leaderboards to handily compare methods not only by their performance, but also by the level of data supervision utilised by each method. The Supervision Levels Scale (SLS) is first presented generally fo any task/dataset/challenge. It is then applied to the EPIC-KITCHENS-100 dataset, to be used for the various leaderboards and challenges associated with this dataset.



### Online Visual Tracking with One-Shot Context-Aware Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.09891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09891v2)
- **Published**: 2020-08-22 18:13:12+00:00
- **Updated**: 2021-04-17 22:31:27+00:00
- **Authors**: Hossein Kashiani, Amir Abbas Hamidi Imani, Shahriar Baradaran Shokouhi, Ahmad Ayatollahi
- **Comment**: 36 pages, 1 algorithm, 8 figures, 1 table
- **Journal**: None
- **Summary**: Online learning policy makes visual trackers more robust against different distortions through learning domain-specific cues. However, the trackers adopting this policy fail to fully leverage the discriminative context of the background areas. Moreover, owing to the lack of sufficient data at each time step, the online learning approach can also make the trackers prone to over-fitting to the background regions. In this paper, we propose a domain adaptation approach to strengthen the contributions of the semantic background context. The domain adaptation approach is backboned with only an off-the-shelf deep model. The strength of the proposed approach comes from its discriminative ability to handle severe occlusion and background clutter challenges. We further introduce a cost-sensitive loss alleviating the dominance of non-semantic background candidates over the semantic candidates, thereby dealing with the data imbalance issue. Experimental results demonstrate that our tracker achieves competitive results at real-time speed compared to the state-of-the-art trackers.



### Few-Shot Learning with Intra-Class Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2008.09892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09892v1)
- **Published**: 2020-08-22 18:15:38+00:00
- **Updated**: 2020-08-22 18:15:38+00:00
- **Authors**: Vivek Roy, Yan Xu, Yu-Xiong Wang, Kris Kitani, Ruslan Salakhutdinov, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the few-shot classification task with an unbalanced dataset, in which some classes have sufficient training samples while other classes only have limited training samples. Recent works have proposed to solve this task by augmenting the training data of the few-shot classes using generative models with the few-shot training samples as the seeds. However, due to the limited number of the few-shot seeds, the generated samples usually have small diversity, making it difficult to train a discriminative classifier for the few-shot classes. To enrich the diversity of the generated samples, we propose to leverage the intra-class knowledge from the neighbor many-shot classes with the intuition that neighbor classes share similar statistical information. Such intra-class information is obtained with a two-step mechanism. First, a regressor trained only on the many-shot classes is used to evaluate the few-shot class means from only a few samples. Second, superclasses are clustered, and the statistical mean and feature variance of each superclass are used as transferable knowledge inherited by the children few-shot classes. Such knowledge is then used by a generator to augment the sparse training data to help the downstream classification tasks. Extensive experiments show that our method achieves state-of-the-art across different datasets and $n$-shot settings.



### Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space
- **Arxiv ID**: http://arxiv.org/abs/2009.05103v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2009.05103v1)
- **Published**: 2020-08-22 20:12:23+00:00
- **Updated**: 2020-08-22 20:12:23+00:00
- **Authors**: Sicheng Zhao, Yaxian Li, Xingxu Yao, Weizhi Nie, Pengfei Xu, Jufeng Yang, Kurt Keutzer
- **Comment**: Accepted by ACM Multimedia 2020
- **Journal**: None
- **Summary**: Both images and music can convey rich semantics and are widely used to induce specific emotions. Matching images and music with similar emotions might help to make emotion perceptions more vivid and stronger. Existing emotion-based image and music matching methods either employ limited categorical emotion states which cannot well reflect the complexity and subtlety of emotions, or train the matching model using an impractical multi-stage pipeline. In this paper, we study end-to-end matching between image and music based on emotions in the continuous valence-arousal (VA) space. First, we construct a large-scale dataset, termed Image-Music-Emotion-Matching-Net (IMEMNet), with over 140K image-music pairs. Second, we propose cross-modal deep continuous metric learning (CDCML) to learn a shared latent embedding space which preserves the cross-modal similarity relationship in the continuous matching space. Finally, we refine the embedding space by further preserving the single-modal emotion relationship in the VA spaces of both images and music. The metric learning in the embedding space and task regression in the label space are jointly optimized for both cross-modal matching and single-modal VA prediction. The extensive experiments conducted on IMEMNet demonstrate the superiority of CDCML for emotion-based image and music matching as compared to the state-of-the-art approaches.



### One Weight Bitwidth to Rule Them All
- **Arxiv ID**: http://arxiv.org/abs/2008.09916v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09916v2)
- **Published**: 2020-08-22 21:40:22+00:00
- **Updated**: 2020-08-28 18:49:48+00:00
- **Authors**: Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, Diana Marculescu
- **Comment**: Accepted at ECCV 2020 Embedded Vision Workshop (Best paper)
- **Journal**: None
- **Summary**: Weight quantization for deep ConvNets has shown promising results for applications such as image classification and semantic segmentation and is especially important for applications where memory storage is limited. However, when aiming for quantization without accuracy degradation, different tasks may end up with different bitwidths. This creates complexity for software and hardware support and the complexity accumulates when one considers mixed-precision quantization, in which case each layer's weights use a different bitwidth. Our key insight is that optimizing for the least bitwidth subject to no accuracy degradation is not necessarily an optimal strategy. This is because one cannot decide optimality between two bitwidths if one has a smaller model size while the other has better accuracy. In this work, we take the first step to understand if some weight bitwidth is better than others by aligning all to the same model size using a width-multiplier. Under this setting, somewhat surprisingly, we show that using a single bitwidth for the whole network can achieve better accuracy compared to mixed-precision quantization targeting zero accuracy degradation when both have the same model size. In particular, our results suggest that when the number of channels becomes a target hyperparameter, a single weight bitwidth throughout the network shows superior results for model compression.



### Quantitative Survey of the State of the Art in Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.09918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09918v2)
- **Published**: 2020-08-22 21:57:48+00:00
- **Updated**: 2020-08-29 10:49:47+00:00
- **Authors**: Oscar Koller
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a meta study covering around 300 published sign language recognition papers with over 400 experimental results. It includes most papers between the start of the field in 1983 and 2020. Additionally, it covers a fine-grained analysis on over 25 studies that have compared their recognition approaches on RWTH-PHOENIX-Weather 2014, the standard benchmark task of the field. Research in the domain of sign language recognition has progressed significantly in the last decade, reaching a point where the task attracts much more attention than ever before. This study compiles the state of the art in a concise way to help advance the field and reveal open questions. Moreover, all of this meta study's source data is made public, easing future work with it and further expansion. The analyzed papers have been manually labeled with a set of categories. The data reveals many insights, such as, among others, shifts in the field from intrusive to non-intrusive capturing, from local to global features and the lack of non-manual parameters included in medium and larger vocabulary recognition systems. Surprisingly, RWTH-PHOENIX-Weather with a vocabulary of 1080 signs represents the only resource for large vocabulary continuous sign language recognition benchmarking world wide.



