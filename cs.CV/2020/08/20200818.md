# Arxiv Papers in cs.CV on 2020-08-18
### One-pixel Signature: Characterizing CNN Models for Backdoor Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.07711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07711v1)
- **Published**: 2020-08-18 02:54:47+00:00
- **Updated**: 2020-08-18 02:54:47+00:00
- **Authors**: Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, Zhuowen Tu
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is to detect/classify if a CNN model has been maliciously inserted with an unknown Trojan trigger or not. Here, each CNN model is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choice of CNN architectures, and how they were trained. It can be computed efficiently for a black-box CNN model without accessing the network parameters. Our proposed one-pixel signature demonstrates a substantial improvement (by around 30% in the absolute detection accuracy) over the existing competing methods for backdoored CNN detection/classification. One-pixel signature is a general representation that can be used to characterize CNN models beyond backdoor detection.



### Contact Area Detector using Cross View Projection Consistency for COVID-19 Projects
- **Arxiv ID**: http://arxiv.org/abs/2008.07712v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07712v1)
- **Published**: 2020-08-18 02:57:26+00:00
- **Updated**: 2020-08-18 02:57:26+00:00
- **Authors**: Pan Zhang, Wilfredo Torres Calderon, Bokyung Lee, Alex Tessier, Jacky Bibliowicz, Liviu Calin, Michael Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to determine what parts of objects and surfaces people touch as they go about their daily lives would be useful in understanding how the COVID-19 virus spreads. To determine whether a person has touched an object or surface using visual data, images, or videos, is a hard problem. Computer vision 3D reconstruction approaches project objects and the human body from the 2D image domain to 3D and perform 3D space intersection directly. However, this solution would not meet the accuracy requirement in applications due to projection error. Another standard approach is to train a neural network to infer touch actions from the collected visual data. This strategy would require significant amounts of training data to generalize over scale and viewpoint variations. A different approach to this problem is to identify whether a person has touched a defined object. In this work, we show that the solution to this problem can be straightforward. Specifically, we show that the contact between an object and a static surface can be identified by projecting the object onto the static surface through two different viewpoints and analyzing their 2D intersection. The object contacts the surface when the projected points are close to each other; we call this cross view projection consistency. Instead of doing 3D scene reconstruction or transfer learning from deep networks, a mapping from the surface in the two camera views to the surface space is the only requirement. For planar space, this mapping is the Homography transformation. This simple method can be easily adapted to real-life applications. In this paper, we apply our method to do office occupancy detection for studying the COVID-19 transmission pattern from an office desk in a meeting room using the contact information.



### Multiple View Generation and Classification of Mid-wave Infrared Images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.07714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07714v1)
- **Published**: 2020-08-18 02:58:21+00:00
- **Updated**: 2020-08-18 02:58:21+00:00
- **Authors**: Maliha Arif, Abhijit Mahalanobis
- **Comment**: 5 pages, 5 figures, to be submitted in a journal
- **Journal**: None
- **Summary**: We propose a novel study of generating unseen arbitrary viewpoints for infrared imagery in the non-linear feature subspace . Current methods use synthetic images and often result in blurry and distorted outputs. Our approach on the contrary understands the semantic information in natural images and encapsulates it such that our predicted unseen views possess good 3D representations. We further explore the non-linear feature subspace and conclude that our network does not operate in the Euclidean subspace but rather in the Riemannian subspace. It does not learn the geometric transformation for predicting the position of the pixel in the new image but rather learns the manifold. To this end, we use t-SNE visualisations to conduct a detailed analysis of our network and perform classification of generated images as a low-shot learning task.



### Domain Generalizer: A Few-shot Meta Learning Framework for Domain Generalization in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2008.07724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07724v1)
- **Published**: 2020-08-18 03:35:56+00:00
- **Updated**: 2020-08-18 03:35:56+00:00
- **Authors**: Pulkit Khandelwal, Paul Yushkevich
- **Comment**: Medical Image Computing and Computer Assisted Interventions (MICCAI)
  2020 to be presented at DART 2020. Supplementary material and link to code
  included
- **Journal**: None
- **Summary**: Deep learning models perform best when tested on target (test) data domains whose distribution is similar to the set of source (train) domains. However, model generalization can be hindered when there is significant difference in the underlying statistics between the target and source domains. In this work, we adapt a domain generalization method based on a model-agnostic meta-learning framework to biomedical imaging. The method learns a domain-agnostic feature representation to improve generalization of models to the unseen test distribution. The method can be used for any imaging task, as it does not depend on the underlying model architecture. We validate the approach through a computed tomography (CT) vertebrae segmentation task across healthy and pathological cases on three datasets. Next, we employ few-shot learning, i.e. training the generalized model using very few examples from the unseen domain, to quickly adapt the model to new unseen data distribution. Our results suggest that the method could help generalize models across different medical centers, image acquisition protocols, anatomies, different regions in a given scan, healthy and diseased populations across varied imaging modalities.



### SoDA: Multi-Object Tracking with Soft Data Association
- **Arxiv ID**: http://arxiv.org/abs/2008.07725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07725v2)
- **Published**: 2020-08-18 03:40:25+00:00
- **Updated**: 2020-08-19 17:46:22+00:00
- **Authors**: Wei-Chih Hung, Henrik Kretzschmar, Tsung-Yi Lin, Yuning Chai, Ruichi Yu, Ming-Hsuan Yang, Dragomir Anguelov
- **Comment**: None
- **Journal**: None
- **Summary**: Robust multi-object tracking (MOT) is a prerequisite fora safe deployment of self-driving cars. Tracking objects, however, remains a highly challenging problem, especially in cluttered autonomous driving scenes in which objects tend to interact with each other in complex ways and frequently get occluded. We propose a novel approach to MOT that uses attention to compute track embeddings that encode the spatiotemporal dependencies between observed objects. This attention measurement encoding allows our model to relax hard data associations, which may lead to unrecoverable errors. Instead, our model aggregates information from all object detections via soft data associations. The resulting latent space representation allows our model to learn to reason about occlusions in a holistic data-driven way and maintain track estimates for objects even when they are occluded. Our experimental results on the Waymo OpenDataset suggest that our approach leverages modern large-scale datasets and performs favorably compared to the state of the art in visual multi-object tracking.



### Equivalent Classification Mapping for Weakly Supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.07728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07728v2)
- **Published**: 2020-08-18 03:54:56+00:00
- **Updated**: 2020-10-06 02:17:18+00:00
- **Authors**: Tao Zhao, Junwei Han, Le Yang, Dingwen Zhang
- **Comment**: 12 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: Weakly supervised temporal action localization is a newly emerging yet widely studied topic in recent years. The existing methods can be categorized into two localization-by-classification pipelines, i.e., the pre-classification pipeline and the post-classification pipeline. The pre-classification pipeline first performs classification on each video snippet and then aggregate the snippet-level classification scores to obtain the video-level classification score. In contrast, the post-classification pipeline aggregates the snippet-level features first and then predicts the video-level classification score based on the aggregated feature. Although the classifiers in these two pipelines are used in different ways, the role they play is exactly the same---to classify the given features to identify the corresponding action categories. To this end, an ideal classifier can make both pipelines work. This inspires us to simultaneously learn these two pipelines in a unified framework to obtain an effective classifier. Specifically, in the proposed learning framework, we implement two parallel network streams to model the two localization-by-classification pipelines simultaneously and make the two network streams share the same classifier. This achieves the novel Equivalent Classification Mapping (ECM) mechanism. Moreover, we discover that an ideal classifier may possess two characteristics: 1) The frame-level classification scores obtained from the pre-classification stream and the feature aggregation weights in the post-classification stream should be consistent; 2) The classification results of these two streams should be identical. Based on these two characteristics, we further introduce a weight-transition module and an equivalent training strategy into the proposed learning framework, which assists to thoroughly mine the equivalence mechanism.



### UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2008.07742v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07742v1)
- **Published**: 2020-08-18 04:48:39+00:00
- **Updated**: 2020-08-18 04:48:39+00:00
- **Authors**: Yuqian Zhou, Michael Kwan, Kyle Tolentino, Neil Emerton, Sehoon Lim, Tim Large, Lijiang Fu, Zhihong Pan, Baopu Li, Qirui Yang, Yihao Liu, Jigang Tang, Tao Ku, Shibin Ma, Bingnan Hu, Jiarong Wang, Densen Puthussery, Hrishikesh P S, Melvin Kuriakose, Jiji C V, Varun Sundar, Sumanth Hegde, Divya Kothandaraman, Kaushik Mitra, Akashdeep Jassal, Nisarg A. Shah, Sabari Nathan, Nagat Abdalla Esiad Rahel, Dafan Chen, Shichao Nie, Shuting Yin, Chengconghui Ma, Haoran Wang, Tongtong Zhao, Shanshan Zhao, Joshua Rego, Huaijin Chen, Shuai Li, Zhenhua Hu, Kin Wai Lau, Lai-Man Po, Dahai Yu, Yasar Abbas Ur Rehman, Yiqun Li, Lianping Xing
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: This paper is the report of the first Under-Display Camera (UDC) image restoration challenge in conjunction with the RLQ workshop at ECCV 2020. The challenge is based on a newly-collected database of Under-Display Camera. The challenge tracks correspond to two types of display: a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams registered the challenge, eight and nine teams submitted the results during the testing phase for each track. The results in the paper are state-of-the-art restoration performance of Under-Display Camera Restoration. Datasets and paper are available at https://yzhouas.github.io/projects/UDC/udc.html.



### Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images
- **Arxiv ID**: http://arxiv.org/abs/2008.07760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07760v1)
- **Published**: 2020-08-18 06:33:40+00:00
- **Updated**: 2020-08-18 06:33:40+00:00
- **Authors**: Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.



### Fully automated deep learning based segmentation of normal, infarcted and edema regions from multiple cardiac MRI sequences
- **Arxiv ID**: http://arxiv.org/abs/2008.07770v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07770v1)
- **Published**: 2020-08-18 07:01:24+00:00
- **Updated**: 2020-08-18 07:01:24+00:00
- **Authors**: Xiaoran Zhang, Michelle Noga, Kumaradevan Punithakumar
- **Comment**: Accepted by MyoPS 2020 Challenge in conjunction with MICCAI and
  STACOM 2020
- **Journal**: None
- **Summary**: Myocardial characterization is essential for patients with myocardial infarction and other myocardial diseases, and the assessment is often performed using cardiac magnetic resonance (CMR) sequences. In this study, we propose a fully automated approach using deep convolutional neural networks (CNN) for cardiac pathology segmentation, including left ventricular (LV) blood pool, right ventricular blood pool, LV normal myocardium, LV myocardial edema (ME) and LV myocardial scars (MS). The input to the network consists of three CMR sequences, namely, late gadolinium enhancement (LGE), T2 and balanced steady state free precession (bSSFP). The proposed approach utilized the data provided by the MyoPS challenge hosted by MICCAI 2020 in conjunction with STACOM. The training set for the CNN model consists of images acquired from 25 cases, and the gold standard labels are provided by trained raters and validated by radiologists. The proposed approach introduces a data augmentation module, linear encoder and decoder module and a network module to increase the number of training samples and improve the prediction accuracy for LV ME and MS. The proposed approach is evaluated by the challenge organizers with a test set including 20 cases and achieves a mean dice score of $46.8\%$ for LV MS and $55.7\%$ for LV ME+MS



### Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07783v2
- **DOI**: 10.1145/3394171.3413865
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07783v2)
- **Published**: 2020-08-18 07:41:40+00:00
- **Updated**: 2020-09-18 11:21:18+00:00
- **Authors**: Guangming Yao, Yi Yuan, Tianjia Shao, Kun Zhou
- **Comment**: 9 pages, 8 figures,accepted by ACM MM2020
- **Journal**: None
- **Summary**: Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.



### ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2008.07792v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.07792v2)
- **Published**: 2020-08-18 08:05:15+00:00
- **Updated**: 2021-03-26 04:44:22+00:00
- **Authors**: Fei Xia, Chengshu Li, Roberto Martín-Martín, Or Litany, Alexander Toshev, Silvio Savarese
- **Comment**: First two authors contributed equally. Access project website at
  http://svl.stanford.edu/projects/relmogen
- **Journal**: None
- **Summary**: Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator (a combination of motion planner and trajectory executor). We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen -- a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and Hierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots.



### Knowledge Transfer via Dense Cross-Layer Mutual-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.07816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07816v1)
- **Published**: 2020-08-18 09:25:08+00:00
- **Updated**: 2020-08-18 09:25:08+00:00
- **Authors**: Anbang Yao, Dawei Sun
- **Comment**: Accepted by ECCV 2020. The code is available at
  https://github.com/sundw2014/DCM, which is based on the implementation of our
  DKS work https://github.com/sundw2014/DKS
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) based methods adopt the one-way Knowledge Transfer (KT) scheme in which training a lower-capacity student network is guided by a pre-trained high-capacity teacher network. Recently, Deep Mutual Learning (DML) presented a two-way KT strategy, showing that the student network can be also helpful to improve the teacher network. In this paper, we propose Dense Cross-layer Mutual-distillation (DCM), an improved two-way KT method in which the teacher and student networks are trained collaboratively from scratch. To augment knowledge representation learning, well-designed auxiliary classifiers are added to certain hidden layers of both teacher and student networks. To boost KT performance, we introduce dense bidirectional KD operations between the layers appended with classifiers. After training, all auxiliary classifiers are discarded, and thus there are no extra parameters introduced to final models. We test our method on a variety of KT tasks, showing its superiorities over related methods. Code is available at https://github.com/sundw2014/DCM



### Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/2008.07817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07817v1)
- **Published**: 2020-08-18 09:25:55+00:00
- **Updated**: 2020-08-18 09:25:55+00:00
- **Authors**: Tomu Tahara, Takashi Seno, Gaku Narita, Tomoya Ishikawa
- **Comment**: 11 pages, 15 figures
- **Journal**: None
- **Summary**: In this paper, we present Retargetable AR, a novel AR framework that yields an AR experience that is aware of scene contexts set in various real environments, achieving natural interaction between the virtual and real worlds. To this end, we characterize scene contexts with relationships among objects in 3D space, not with coordinates transformations. A context assumed by an AR content and a context formed by a real environment where users experience AR are represented as abstract graph representations, i.e. scene graphs. From RGB-D streams, our framework generates a volumetric map in which geometric and semantic information of a scene are integrated. Moreover, using the semantic map, we abstract scene objects as oriented bounding boxes and estimate their orientations. With such a scene representation, our framework constructs, in an online fashion, a 3D scene graph characterizing the context of a real environment for AR. The correspondence between the constructed graph and an AR scene graph denoting the context of AR content provides a semantically registered content arrangement, which facilitates natural interaction between the virtual and real worlds. We performed extensive evaluations on our prototype system through quantitative evaluation of the performance of the oriented bounding box estimation, subjective evaluation of the AR content arrangement based on constructed 3D scene graphs, and an online AR demonstration. The results of these evaluations showed the effectiveness of our framework, demonstrating that it can provide a context-aware AR experience in a variety of real scenes.



### ConvGRU in Fine-grained Pitching Action Recognition for Action Outcome Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.07819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2008.07819v1)
- **Published**: 2020-08-18 09:27:17+00:00
- **Updated**: 2020-08-18 09:27:17+00:00
- **Authors**: Tianqi Ma, Lin Zhang, Xiumin Diao, Ou Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Prediction of the action outcome is a new challenge for a robot collaboratively working with humans. With the impressive progress in video action recognition in recent years, fine-grained action recognition from video data turns into a new concern. Fine-grained action recognition detects subtle differences of actions in more specific granularity and is significant in many fields such as human-robot interaction, intelligent traffic management, sports training, health caring. Considering that the different outcomes are closely connected to the subtle differences in actions, fine-grained action recognition is a practical method for action outcome prediction. In this paper, we explore the performance of convolutional gate recurrent unit (ConvGRU) method on a fine-grained action recognition tasks: predicting outcomes of ball-pitching. Based on sequences of RGB images of human actions, the proposed approach achieved the performance of 79.17% accuracy, which exceeds the current state-of-the-art result. We also compared different network implementations and showed the influence of different image sampling methods, different fusion methods and pre-training, etc. Finally, we discussed the advantages and limitations of ConvGRU in such action outcome prediction and fine-grained action recognition tasks.



### Mastering Large Scale Multi-label Image Recognition with high efficiency overCamera trap images
- **Arxiv ID**: http://arxiv.org/abs/2008.07828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07828v1)
- **Published**: 2020-08-18 09:51:34+00:00
- **Updated**: 2020-08-18 09:51:34+00:00
- **Authors**: Miroslav Valan, Lukáš Picek
- **Comment**: Hakuna Ma-data Challenge: 1st place submission description.
  (Fine-Grained Visual Categorization Workshop - CVPR2020)
- **Journal**: None
- **Summary**: Camera traps are crucial in biodiversity motivated studies, however dealing with large number of images while annotating these data sets is a tedious and time consuming task. To speed up this process, Machine Learning approaches are a reasonable asset. In this article we are proposing an easy, accessible, light-weight, fast and efficient approach based on our winning submission to the "Hakuna Ma-data - Serengeti Wildlife Identification challenge". Our system achieved an Accuracy of 97% and outperformed the human level performance. We show that, given relatively large data sets, it is effective to look at each image only once with little or no augmentation. By utilizing such a simple, yet effective baseline we were able to avoid over-fitting without extensive regularization techniques and to train a top scoring system on a very limited hardware featuring single GPU (1080Ti) despite the large training set (6.7M images and 6TB).



### Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.07831v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07831v1)
- **Published**: 2020-08-18 10:03:45+00:00
- **Updated**: 2020-08-18 10:03:45+00:00
- **Authors**: Malek Husseini, Anjany Sekuboyina, Maximilian Loeffler, Fernando Navarro, Bjoern H. Menze, Jan S. Kirschke
- **Comment**: To be presented at MICCAI 2020
- **Journal**: None
- **Summary**: Osteoporotic vertebral fractures have a severe impact on patients' overall well-being but are severely under-diagnosed. These fractures present themselves at various levels of severity measured using the Genant's grading scale. Insufficient annotated datasets, severe data-imbalance, and minor difference in appearances between fractured and healthy vertebrae make naive classification approaches result in poor discriminatory performance. Addressing this, we propose a representation learning-inspired approach for automated vertebral fracture detection, aimed at learning latent representations efficient for fracture detection. Building on state-of-art metric losses, we present a novel Grading Loss for learning representations that respect Genant's fracture grading scheme. On a publicly available spine dataset, the proposed loss function achieves a fracture detection F1 score of 81.5%, a 10% increase over a naive classification baseline.



### Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models
- **Arxiv ID**: http://arxiv.org/abs/2008.07832v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07832v1)
- **Published**: 2020-08-18 10:04:51+00:00
- **Updated**: 2020-08-18 10:04:51+00:00
- **Authors**: Tzu-Jui Julius Wang, Selen Pehlivan, Jorma Laaksonen
- **Comment**: accepted to BMVC2020
- **Journal**: None
- **Summary**: Predicting a scene graph that captures visual entities and their interactions in an image has been considered a crucial step towards full scene comprehension. Recent scene graph generation (SGG) models have shown their capability of capturing the most frequent relations among visual entities. However, the state-of-the-art results are still far from satisfactory, e.g. models can obtain 31% in overall recall R@100, whereas the likewise important mean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The discrepancy between R and mR results urges to shift the focus from pursuing a high R to a high mR with a still competitive R. We suspect that the observed discrepancy stems from both the annotation bias and sparse annotations in VG, in which many visual entity pairs are either not annotated at all or only with a single relation when multiple ones could be valid. To address this particular issue, we propose a novel SGG training scheme that capitalizes on self-learned knowledge. It involves two relation classifiers, one offering a less biased setting for the other to base on. The proposed scheme can be applied to most of the existing SGG models and is straightforward to implement. We observe significant relative improvements in mR (between +6.6% and +20.4%) and competitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.



### EASTER: Efficient and Scalable Text Recognizer
- **Arxiv ID**: http://arxiv.org/abs/2008.07839v2
- **DOI**: 10.21428/594757db.65eda33c
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.07839v2)
- **Published**: 2020-08-18 10:26:03+00:00
- **Updated**: 2020-08-19 14:02:50+00:00
- **Authors**: Kartik Chaudhary, Raghav Bali
- **Comment**: 9 pages, fixed typos and minor edits
- **Journal**: Proceedings of the Canadian Conference on Artificial Intelligence
  2021
- **Summary**: Recent progress in deep learning has led to the development of Optical Character Recognition (OCR) systems which perform remarkably well. Most research has been around recurrent networks as well as complex gated layers which make the overall solution complex and difficult to scale. In this paper, we present an Efficient And Scalable TExt Recognizer (EASTER) to perform optical character recognition on both machine printed and handwritten text. Our model utilises 1-D convolutional layers without any recurrence which enables parallel training with considerably less volume of data. We experimented with multiple variations of our architecture and one of the smallest variant (depth and number of parameter wise) performs comparably to RNN based complex choices. Our 20-layered deepest variant outperforms RNN architectures with a good margin on benchmarking datasets like IIIT-5k and SVT. We also showcase improvements over the current best results on offline handwritten text recognition task. We also present data generation pipelines with augmentation setup to generate synthetic datasets for both handwritten and machine printed text.



### Image Pre-processing on NumtaDB for Bengali Handwritten Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.07853v1
- **DOI**: 10.1109/ICBSLP.2018.8554910
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07853v1)
- **Published**: 2020-08-18 11:02:25+00:00
- **Updated**: 2020-08-18 11:02:25+00:00
- **Authors**: Ovi Paul
- **Comment**: 5 pages, 8 figures and 4 tables
- **Journal**: 2018 International Conference on Bangla Speech and Language
  Processing (ICBSLP), Sylhet, 2018, pp. 1-6
- **Summary**: NumtaDB is by far the largest data-set collection for handwritten digits in Bengali. This is a diverse dataset containing more than 85000 images. But this diversity also makes this dataset very difficult to work with. The goal of this paper is to find the benchmark for pre-processed images which gives good accuracy on any machine learning models. The reason being, there are no available pre-processed data for Bengali digit recognition to work with like the English digits for MNIST.



### Depth Completion with RGB Prior
- **Arxiv ID**: http://arxiv.org/abs/2008.07861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07861v1)
- **Published**: 2020-08-18 11:22:20+00:00
- **Updated**: 2020-08-18 11:22:20+00:00
- **Authors**: Yuri Feldman, Yoel Shapiro, Dotan Di Castro
- **Comment**: 17 pages, 4 figures
- **Journal**: None
- **Summary**: Depth cameras are a prominent perception system for robotics, especially when operating in natural unstructured environments. Industrial applications, however, typically involve reflective objects under harsh lighting conditions, a challenging scenario for depth cameras, as it induces numerous reflections and deflections, leading to loss of robustness and deteriorated accuracy. Here, we developed a deep model to correct the depth channel in RGBD images, aiming to restore the depth information to the required accuracy. To train the model, we created a novel industrial dataset that we now present to the public. The data was collected with low-end depth cameras and the ground truth depth was generated by multi-view fusion.



### Self-supervised Sparse to Dense Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.07872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07872v1)
- **Published**: 2020-08-18 11:40:18+00:00
- **Updated**: 2020-08-18 11:40:18+00:00
- **Authors**: Amirhossein Kardoost, Kalun Ho, Peter Ochs, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Observable motion in videos can give rise to the definition of objects moving with respect to the scene. The task of segmenting such moving objects is referred to as motion segmentation and is usually tackled either by aggregating motion information in long, sparse point trajectories, or by directly producing per frame dense segmentations relying on large amounts of training data. In this paper, we propose a self supervised method to learn the densification of sparse motion segmentations from single video frames. While previous approaches towards motion segmentation build upon pre-training on large surrogate datasets and use dense motion information as an essential cue for the pixelwise segmentation, our model does not require pre-training and operates at test time on single frames. It can be trained in a sequence specific way to produce high quality dense segmentations from sparse and noisy input. We evaluate our method on the well-known motion segmentation datasets FBMS59 and DAVIS16.



### Person image generation with semantic attention network for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2008.07884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07884v1)
- **Published**: 2020-08-18 12:18:51+00:00
- **Updated**: 2020-08-18 12:18:51+00:00
- **Authors**: Meichen Liu, Kejun Wang, Juihang Ji, Shuzhi Sam Ge
- **Comment**: None
- **Journal**: None
- **Summary**: Pose variation is one of the key factors which prevents the network from learning a robust person re-identification (Re-ID) model. To address this issue, we propose a novel person pose-guided image generation method, which is called the semantic attention network. The network consists of several semantic attention blocks, where each block attends to preserve and update the pose code and the clothing textures. The introduction of the binary segmentation mask and the semantic parsing is important for seamlessly stitching foreground and background in the pose-guided image generation. Compared with other methods, our network can characterize better body shape and keep clothing attributes, simultaneously. Our synthesized image can obtain better appearance and shape consistency related to the original image. Experimental results show that our approach is competitive with respect to both quantitative and qualitative results on Market-1501 and DeepFashion. Furthermore, we conduct extensive evaluations by using person re-identification (Re-ID) systems trained with the pose-transferred person based augmented data. The experiment shows that our approach can significantly enhance the person Re-ID accuracy.



### Visibility-aware Multi-view Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/2008.07928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07928v2)
- **Published**: 2020-08-18 13:47:36+00:00
- **Updated**: 2020-08-19 02:40:59+00:00
- **Authors**: Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, Tian Fang
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet significantly improves depth accuracies in the scenes with severe occlusion. Extensive experiments are performed on DTU, BlendedMVS, and Tanks and Temples datasets to justify the effectiveness of the proposed framework.



### Feature Products Yield Efficient Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07930v1)
- **Published**: 2020-08-18 13:47:56+00:00
- **Updated**: 2020-08-18 13:47:56+00:00
- **Authors**: Philipp Grüning, Thomas Martinetz, Erhardt Barth
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Feature-Product networks (FP-nets) as a novel deep-network architecture based on a new building block inspired by principles of biological vision. For each input feature map, a so-called FP-block learns two different filters, the outputs of which are then multiplied. Such FP-blocks are inspired by models of end-stopped neurons, which are common in cortical areas V1 and especially in V2. Convolutional neural networks can be transformed into parameter-efficient FP-nets by substituting conventional blocks of regular convolutions with FP-blocks. In this way, we create several novel FP-nets based on state-of-the-art networks and evaluate them on the Cifar-10 and ImageNet challenges. We show that the use of FP-blocks reduces the number of parameters significantly without decreasing generalization capability. Since so far heuristics and search algorithms have been used to find more efficient networks, it seems remarkable that we can obtain even more efficient networks based on a novel bio-inspired design principle.



### Motion Capture from Internet Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.07931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07931v2)
- **Published**: 2020-08-18 13:48:37+00:00
- **Updated**: 2020-08-19 02:56:48+00:00
- **Authors**: Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, Hujun Bao
- **Comment**: ECCV 2020 (Oral), project page: https://zju3dv.github.io/iMoCap/
- **Journal**: None
- **Summary**: Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a celebrity performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular motion capture methods.



### Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents
- **Arxiv ID**: http://arxiv.org/abs/2008.07935v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07935v2)
- **Published**: 2020-08-18 14:01:09+00:00
- **Updated**: 2020-08-22 12:58:39+00:00
- **Authors**: Ye Zhu, Yu Wu, Yi Yang, Yan Yan
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: With the arising concerns for the AI systems provided with direct access to abundant sensitive information, researchers seek to develop more reliable AI with implicit information sources. To this end, in this paper, we introduce a new task called video description via two multi-modal cooperative dialog agents, whose ultimate goal is for one conversational agent to describe an unseen video based on the dialog and two static frames. Specifically, one of the intelligent agents - Q-BOT - is given two static frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has already seen the entire video, assists Q-BOT to accomplish the goal by providing answers to those questions. We propose a QA-Cooperative Network with a dynamic dialog history update learning mechanism to transfer knowledge from A-BOT to Q-BOT, thus helping Q-BOT to better describe the video. Extensive experiments demonstrate that Q-BOT can effectively learn to describe an unseen video by the proposed model and the cooperative learning method, achieving the promising performance where Q-BOT is given the full ground truth history dialog.



### Dataset Bias in Few-shot Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.07960v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07960v3)
- **Published**: 2020-08-18 14:46:23+00:00
- **Updated**: 2021-03-16 03:23:18+00:00
- **Authors**: Shuqiang Jiang, Yaohui Zhu, Chenlong Liu, Xinhang Song, Xiangyang Li, Weiqing Min
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of few-shot image recognition (FSIR) is to identify novel categories with a small number of annotated samples by exploiting transferable knowledge from training data (base categories). Most current studies assume that the transferable knowledge can be well used to identify novel categories. However, such transferable capability may be impacted by the dataset bias, and this problem has rarely been investigated before. Besides, most of few-shot learning methods are biased to different datasets, which is also an important issue that needs to be investigated deeply. In this paper, we first investigate the impact of transferable capabilities learned from base categories. Specifically, we use the relevance to measure relationships between base categories and novel categories. Distributions of base categories are depicted via the instance density and category diversity. The FSIR model learns better transferable knowledge from relevant training data. In the relevant data, dense instances or diverse categories can further enrich the learned knowledge. Experimental results on different sub-datasets of ImagNet demonstrate category relevance, instance density and category diversity can depict transferable bias from base categories. Second, we investigate performance differences on different datasets from dataset structures and different few-shot learning methods. Specifically, we introduce image complexity, intra-concept visual consistency, and inter-concept visual similarity to quantify characteristics of dataset structures. We use these quantitative characteristics and four few-shot learning methods to analyze performance differences on five different datasets. Based on the experimental analysis, some insightful observations are obtained from the perspective of both dataset structures and few-shot learning methods. We hope these observations are useful to guide future FSIR research.



### Hierarchical HMM for Eye Movement Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.07961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07961v1)
- **Published**: 2020-08-18 14:47:23+00:00
- **Updated**: 2020-08-18 14:47:23+00:00
- **Authors**: Ye Zhu, Yan Yan, Oleg Komogortsev
- **Comment**: ECCV2020 Workshop, OpenEyes: Eye Gaze in AR, VR, and in the Wild
- **Journal**: None
- **Summary**: In this work, we tackle the problem of ternary eye movement classification, which aims to separate fixations, saccades and smooth pursuits from the raw eye positional data. The efficient classification of these different types of eye movements helps to better analyze and utilize the eye tracking data. Different from the existing methods that detect eye movement by several pre-defined threshold values, we propose a hierarchical Hidden Markov Model (HMM) statistical algorithm for detecting fixations, saccades and smooth pursuits. The proposed algorithm leverages different features from the recorded raw eye tracking data with a hierarchical classification strategy, separating one type of eye movement each time. Experimental results demonstrate the effectiveness and robustness of the proposed method by achieving competitive or better performance compared to the state-of-the-art methods.



### Comparison of Convolutional neural network training parameters for detecting Alzheimers disease and effect on visualization
- **Arxiv ID**: http://arxiv.org/abs/2008.07981v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2008.07981v1)
- **Published**: 2020-08-18 15:21:50+00:00
- **Updated**: 2020-08-18 15:21:50+00:00
- **Authors**: Arjun Haridas Pallath, Martin Dyrba
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have become a powerful tool for detecting patterns in image data. Recent papers report promising results in the domain of disease detection using brain MRI data. Despite the high accuracy obtained from CNN models for MRI data so far, almost no papers provided information on the features or image regions driving this accuracy as adequate methods were missing or challenging to apply. Recently, the toolbox iNNvestigate has become available, implementing various state of the art methods for deep learning visualizations. Currently, there is a great demand for a comparison of visualization algorithms to provide an overview of the practical usefulness and capability of these algorithms.   Therefore, this thesis has two goals: 1. To systematically evaluate the influence of CNN hyper-parameters on model accuracy. 2. To compare various visualization methods with respect to the quality (i.e. randomness/focus, soundness).



### Anomaly Detection with Convolutional Autoencoders for Fingerprint Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.07989v2
- **DOI**: 10.1109/TBIOM.2021.3050036
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07989v2)
- **Published**: 2020-08-18 15:33:41+00:00
- **Updated**: 2020-10-19 14:08:47+00:00
- **Authors**: Jascha Kolberg, Marcel Grimmer, Marta Gomez-Barrero, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the popularity of fingerprint-based biometric authentication systems significantly increased. However, together with many advantages, biometric systems are still vulnerable to presentation attacks (PAs). In particular, this applies for unsupervised applications, where new attacks unknown to the system operator may occur. Therefore, presentation attack detection (PAD) methods are used to determine whether samples stem from a bona fide subject or from a presentation attack instrument (PAI). In this context, most works are dedicated to solve PAD as a two-class classification problem, which includes training a model on both bona fide and PA samples. In spite of the good detection rates reported, these methods still face difficulties detecting PAIs from unknown materials. To address this issue, we propose a new PAD technique based on autoencoders (AEs) trained only on bona fide samples (i.e. one-class), which are captured in the short wave infrared domain. On the experimental evaluation over a database of 19,711 bona fide and 4,339 PA images including 45 different PAI species, a detection equal error rate (D-EER) of 2.00% was achieved. Additionally, our best performing AE model is compared to further one-class classifiers (support vector machine, Gaussian mixture model). The results show the effectiveness of the AE model as it significantly outperforms the previously proposed methods.



### Offloading Optimization in Edge Computing for Deep Learning Enabled Target Tracking by Internet-of-UAVs
- **Arxiv ID**: http://arxiv.org/abs/2008.08001v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.08001v1)
- **Published**: 2020-08-18 16:00:36+00:00
- **Updated**: 2020-08-18 16:00:36+00:00
- **Authors**: Bo Yang, Xuelin Cao, Chau Yuen, Lijun Qian
- **Comment**: Accepted by IEEE IoTJ
- **Journal**: None
- **Summary**: The empowering unmanned aerial vehicles (UAVs) have been extensively used in providing intelligence such as target tracking. In our field experiments, a pre-trained convolutional neural network (CNN) is deployed at the UAV to identify a target (a vehicle) from the captured video frames and enable the UAV to keep tracking. However, this kind of visual target tracking demands a lot of computational resources due to the desired high inference accuracy and stringent delay requirement. This motivates us to consider offloading this type of deep learning (DL) tasks to a mobile edge computing (MEC) server due to limited computational resource and energy budget of the UAV, and further improve the inference accuracy. Specifically, we propose a novel hierarchical DL tasks distribution framework, where the UAV is embedded with lower layers of the pre-trained CNN model, while the MEC server with rich computing resources will handle the higher layers of the CNN model. An optimization problem is formulated to minimize the weighted-sum cost including the tracking delay and energy consumption introduced by communication and computing of the UAVs, while taking into account the quality of data (e.g., video frames) input to the DL model and the inference errors. Analytical results are obtained and insights are provided to understand the tradeoff between the weighted-sum cost and inference error rate in the proposed framework. Numerical results demonstrate the effectiveness of the proposed offloading framework.



### Reinforcement Learning for Improving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.08005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08005v1)
- **Published**: 2020-08-18 16:20:04+00:00
- **Updated**: 2020-08-18 16:20:04+00:00
- **Authors**: Siddharth Nayak, Balaraman Ravindran
- **Comment**: 14 pages, 6 figures, 4 tables. Accepted in the RLQ-TOD workshop at
  ECCV 2020
- **Journal**: None
- **Summary**: The performance of a trained object detection neural network depends a lot on the image quality. Generally, images are pre-processed before feeding them into the neural network and domain knowledge about the image dataset is used to choose the pre-processing techniques. In this paper, we introduce an algorithm called ObjectRL to choose the amount of a particular pre-processing to be applied to improve the object detection performances of pre-trained networks. The main motivation for ObjectRL is that an image which looks good to a human eye may not necessarily be the optimal one for a pre-trained object detector to detect objects.



### Linguistically-aware Attention for Reducing the Semantic-Gap in Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2008.08012v1
- **DOI**: 10.1016/j.patcog.2020.107812
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08012v1)
- **Published**: 2020-08-18 16:29:49+00:00
- **Updated**: 2020-08-18 16:29:49+00:00
- **Authors**: Gouthaman KV, Athira Nambiar, Kancheti Sai Srinivas, Anurag Mittal
- **Comment**: None
- **Journal**: Pattern Recognition, 2021
- **Summary**: Attention models are widely used in Vision-language (V-L) tasks to perform the visual-textual correlation. Humans perform such a correlation with a strong linguistic understanding of the visual world. However, even the best performing attention model in V-L tasks lacks such a high-level linguistic understanding, thus creating a semantic gap between the modalities. In this paper, we propose an attention mechanism - Linguistically-aware Attention (LAT) - that leverages object attributes obtained from generic object detectors along with pre-trained language models to reduce this semantic gap. LAT represents visual and textual modalities in a common linguistically-rich space, thus providing linguistic awareness to the attention process. We apply and demonstrate the effectiveness of LAT in three V-L tasks: Counting-VQA, VQA, and Image captioning. In Counting-VQA, we propose a novel counting-specific VQA model to predict an intuitive count and achieve state-of-the-art results on five datasets. In VQA and Captioning, we show the generic nature and effectiveness of LAT by adapting it into various baselines and consistently improving their performance.



### MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images in the Context of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2008.08016v1
- **DOI**: 10.1016/j.smhl.2020.100144
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08016v1)
- **Published**: 2020-08-18 16:38:11+00:00
- **Updated**: 2020-08-18 16:38:11+00:00
- **Authors**: Adnane Cabani, Karim Hammoudi, Halim Benhabiles, Mahmoud Melkemi
- **Comment**: None
- **Journal**: None
- **Summary**: The wearing of the face masks appears as a solution for limiting the spread of COVID-19. In this context, efficient recognition systems are expected for checking that people faces are masked in regulated areas. To perform this task, a large dataset of masked faces is necessary for training deep learning models towards detecting people wearing masks and those not wearing masks. Some large datasets of masked faces are available in the literature. However, at the moment, there are no available large dataset of masked face images that permits to check if detected masked faces are correctly worn or not. Indeed, many people are not correctly wearing their masks due to bad practices, bad behaviors or vulnerability of individuals (e.g., children, old people). For these reasons, several mask wearing campaigns intend to sensitize people about this problem and good practices. In this sense, this work proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net). Realistic masked face datasets are proposed with a twofold objective: i) to detect people having their faces masked or not masked, ii) to detect faces having their masks correctly worn or incorrectly worn (e.g.; at airport portals or in crowds). To the best of our knowledge, no large dataset of masked faces provides such a granularity of classification towards permitting mask wearing analysis. Moreover, this work globally presents the applied mask-to-face deformable model for permitting the generation of other masked face images, notably with specific masks. Our datasets of masked face images (137,016 images) are available at https://github.com/cabani/MaskedFace-Net.



### Multilanguage Number Plate Detection using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.08023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08023v1)
- **Published**: 2020-08-18 16:50:47+00:00
- **Updated**: 2020-08-18 16:50:47+00:00
- **Authors**: Jatin Gupta, Vandana Saini, Kamaldeep Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Object Detection is a popular field of research for recent technologies. In recent years, profound learning performance attracts the researchers to use it in many applications. Number plate (NP) detection and classification is analyzed over decades however, it needs approaches which are more precise and state, language and design independent since cars are now moving from state to another easily. In this paperwe suggest a new strategy to detect NP and comprehend the nation, language and layout of NPs. YOLOv2 sensor with ResNet attribute extractor heart is proposed for NP detection and a brand new convolutional neural network architecture is suggested to classify NPs. The detector achieves average precision of 99.57% and country, language and layout classification precision of 99.33%. The results outperforms the majority of the previous works and can move the area forward toward international NP detection and recognition.



### Self-supervised Denoising via Diffeomorphic Template Estimation: Application to Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2008.08024v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08024v1)
- **Published**: 2020-08-18 16:52:10+00:00
- **Updated**: 2020-08-18 16:52:10+00:00
- **Authors**: Guillaume Gisbert, Neel Dey, Hiroshi Ishikawa, Joel Schuman, James Fishbaugh, Guido Gerig
- **Comment**: To be published in MICCAI Ophthalmic Medical Image Analysis 2020. 11
  pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) is pervasive in both the research and clinical practice of Ophthalmology. However, OCT images are strongly corrupted by noise, limiting their interpretation. Current OCT denoisers leverage assumptions on noise distributions or generate targets for training deep supervised denoisers via averaging of repeat acquisitions. However, recent self-supervised advances allow the training of deep denoising networks using only repeat acquisitions without clean targets as ground truth, reducing the burden of supervised learning. Despite the clear advantages of self-supervised methods, their use is precluded as OCT shows strong structural deformations even between sequential scans of the same subject due to involuntary eye motion. Further, direct nonlinear alignment of repeats induces correlation of the noise between images. In this paper, we propose a joint diffeomorphic template estimation and denoising framework which enables the use of self-supervised denoising for motion deformed repeat acquisitions, without empirically registering their noise realizations. Strong qualitative and quantitative improvements are achieved in denoising OCT images, with generic utility in any imaging modality amenable to multiple exposures.



### Gradients as a Measure of Uncertainty in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.08030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08030v2)
- **Published**: 2020-08-18 16:58:46+00:00
- **Updated**: 2020-09-03 19:47:36+00:00
- **Authors**: Jinsol Lee, Ghassan AlRegib
- **Comment**: IEEE International Conference on Image Processing (ICIP) 2020
- **Journal**: None
- **Summary**: Despite tremendous success of modern neural networks, they are known to be overconfident even when the model encounters inputs with unfamiliar conditions. Detecting such inputs is vital to preventing models from making naive predictions that may jeopardize real-world applications of neural networks. In this paper, we address the challenging problem of devising a simple yet effective measure of uncertainty in deep neural networks. Specifically, we propose to utilize backpropagated gradients to quantify the uncertainty of trained models. Gradients depict the required amount of change for a model to properly represent given inputs, thus providing a valuable insight into how familiar and certain the model is regarding the inputs. We demonstrate the effectiveness of gradients as a measure of model uncertainty in applications of detecting unfamiliar inputs, including out-of-distribution and corrupted samples. We show that our gradient-based method outperforms state-of-the-art methods by up to 4.8% of AUROC score in out-of-distribution detection and 35.7% in corrupted input detection.



### Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images
- **Arxiv ID**: http://arxiv.org/abs/2008.08055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08055v2)
- **Published**: 2020-08-18 17:36:56+00:00
- **Updated**: 2020-09-27 22:11:48+00:00
- **Authors**: Guy Leroy, Daniel Rueckert, Amir Alansary
- **Comment**: Accepted for the MLCN workshop, MICCAI 2020
- **Journal**: None
- **Summary**: Accurate detection of anatomical landmarks is an essential step in several medical imaging tasks. We propose a novel communicative multi-agent reinforcement learning (C-MARL) system to automatically detect landmarks in 3D brain images. C-MARL enables the agents to learn explicit communication channels, as well as implicit communication signals by sharing certain weights of the architecture among all the agents. The proposed approach is evaluated on two brain imaging datasets from adult magnetic resonance imaging (MRI) and fetal ultrasound scans. Our experiments show that involving multiple cooperating agents by learning their communication with each other outperforms previous approaches using single agents.



### AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation Metrics
- **Arxiv ID**: http://arxiv.org/abs/2008.08063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.08063v1)
- **Published**: 2020-08-18 17:45:56+00:00
- **Updated**: 2020-08-18 17:45:56+00:00
- **Authors**: Xinshuo Weng, Jianren Wang, David Held, Kris Kitani
- **Comment**: Extended abstract that will be presented at ECCV 2020 Workshops.
  Project website: http://www.xinshuoweng.com/projects/AB3DMOT
- **Journal**: None
- **Summary**: 3D multi-object tracking (MOT) is essential to applications such as autonomous driving. Recent work focuses on developing accurate systems giving less attention to computational cost and system complexity. In contrast, this work proposes a simple real-time 3D MOT system with strong performance. Our system first obtains 3D detections from a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is used for state estimation and data association. Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in 2D space and standardized 3D MOT evaluation tools are missing for a fair comparison of 3D MOT methods. We propose a new 3D MOT evaluation tool along with three new metrics to comprehensively evaluate 3D MOT methods. We show that, our proposed method achieves strong 3D MOT performance on KITTI and runs at a rate of $207.4$ FPS on the KITTI dataset, achieving the fastest speed among modern 3D MOT systems. Our code is publicly available at http://www.xinshuoweng.com/projects/AB3DMOT.



### AssembleNet++: Assembling Modality Representations via Attention Connections
- **Arxiv ID**: http://arxiv.org/abs/2008.08072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.08072v1)
- **Published**: 2020-08-18 17:54:08+00:00
- **Updated**: 2020-08-18 17:54:08+00:00
- **Authors**: Michael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, Anelia Angelova
- **Comment**: ECCV 2020 camera-ready version
- **Journal**: ECCV 2020
- **Summary**: We create a family of powerful video models which are able to: (i) learn interactions between semantic object information and raw appearance and motion features, and (ii) deploy attention in order to better learn the importance of features at each convolutional block of the network. A new network component named peer-attention is introduced, which dynamically learns the attention weights using another block or input modality. Even without pre-training, our models outperform the previous work on standard public activity recognition datasets with continuous videos, establishing new state-of-the-art. We also confirm that our findings of having neural connections from the object modality and the use of peer-attention is generally applicable for different existing architectures, improving their performances. We name our model explicitly as AssembleNet++. The code will be available at: https://sites.google.com/corp/view/assemblenet/



### TIDE: A General Toolbox for Identifying Object Detection Errors
- **Arxiv ID**: http://arxiv.org/abs/2008.08115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08115v2)
- **Published**: 2020-08-18 18:30:53+00:00
- **Updated**: 2020-08-31 19:06:51+00:00
- **Authors**: Daniel Bolya, Sean Foley, James Hays, Judy Hoffman
- **Comment**: Updated LVIS results with the v1.0.1 error calculation
- **Journal**: None
- **Summary**: We introduce TIDE, a framework and associated toolbox for analyzing the sources of error in object detection and instance segmentation algorithms. Importantly, our framework is applicable across datasets and can be applied directly to output prediction files without required knowledge of the underlying prediction system. Thus, our framework can be used as a drop-in replacement for the standard mAP computation while providing a comprehensive analysis of each model's strengths and weaknesses. We segment errors into six types and, crucially, are the first to introduce a technique for measuring the contribution of each error in a way that isolates its effect on overall performance. We show that such a representation is critical for drawing accurate, comprehensive conclusions through in-depth analysis across 4 datasets and 7 recognition models. Available at https://dbolya.github.io/tide/



### DeepLiDARFlow: A Deep Learning Architecture For Scene Flow Estimation Using Monocular Camera and Sparse LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2008.08136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.08136v1)
- **Published**: 2020-08-18 19:51:08+00:00
- **Updated**: 2020-08-18 19:51:08+00:00
- **Authors**: Rishav, Ramy Battrawy, René Schuster, Oliver Wasenmüller, Didier Stricker
- **Comment**: This paper is accepted to IROS2020
- **Journal**: None
- **Summary**: Scene flow is the dense 3D reconstruction of motion and geometry of a scene. Most state-of-the-art methods use a pair of stereo images as input for full scene reconstruction. These methods depend a lot on the quality of the RGB images and perform poorly in regions with reflective objects, shadows, ill-conditioned light environment and so on. LiDAR measurements are much less sensitive to the aforementioned conditions but LiDAR features are in general unsuitable for matching tasks due to their sparse nature. Hence, using both LiDAR and RGB can potentially overcome the individual disadvantages of each sensor by mutual improvement and yield robust features which can improve the matching process. In this paper, we present DeepLiDARFlow, a novel deep learning architecture which fuses high level RGB and LiDAR features at multiple scales in a monocular setup to predict dense scene flow. Its performance is much better in the critical regions where image-only and LiDAR-only methods are inaccurate. We verify our DeepLiDARFlow using the established data sets KITTI and FlyingThings3D and we show strong robustness compared to several state-of-the-art methods which used other input modalities. The code of our paper is available at https://github.com/dfki-av/DeepLiDARFlow.



### How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2008.08143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08143v2)
- **Published**: 2020-08-18 20:22:16+00:00
- **Updated**: 2021-04-01 16:54:42+00:00
- **Authors**: Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: Accepted at CVPR 2021. Dataset website: http://how2sign.github.io/
- **Journal**: None
- **Summary**: One of the factors that have hindered progress in the areas of sign language recognition, translation, and production is the absence of large annotated datasets. Towards this end, we introduce How2Sign, a multimodal and multiview continuous American Sign Language (ASL) dataset, consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation. To evaluate the potential of How2Sign for real-world impact, we conduct a study with ASL signers and show that synthesized videos using our dataset can indeed be understood. The study further gives insights on challenges that computer vision should address in order to make progress in this field.   Dataset website: http://how2sign.github.io/



### Category Level Object Pose Estimation via Neural Analysis-by-Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.08145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08145v1)
- **Published**: 2020-08-18 20:30:47+00:00
- **Updated**: 2020-08-18 20:30:47+00:00
- **Authors**: Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit CAD models per object instance unnecessary. The image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. At inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. Keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2D images alone. When provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6DOF pose successfully.



### Robust Handwriting Recognition with Limited and Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/2008.08148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08148v1)
- **Published**: 2020-08-18 20:33:23+00:00
- **Updated**: 2020-08-18 20:33:23+00:00
- **Authors**: Hai Pham, Amrith Setlur, Saket Dingliwal, Tzu-Hsiang Lin, Barnabas Poczos, Kang Huang, Zhuo Li, Jae Lim, Collin McCormack, Tam Vu
- **Comment**: icfhr2020
- **Journal**: None
- **Summary**: Despite the advent of deep learning in computer vision, the general handwriting recognition problem is far from solved. Most existing approaches focus on handwriting datasets that have clearly written text and carefully segmented labels. In this paper, we instead focus on learning handwritten characters from maintenance logs, a constrained setting where data is very limited and noisy. We break the problem into two consecutive stages of word segmentation and word recognition respectively and utilize data augmentation techniques to train both stages. Extensive comparisons with popular baselines for scene-text detection and word recognition show that our system achieves a lower error rate and is more suited to handle noisy and difficult documents



### Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization
- **Arxiv ID**: http://arxiv.org/abs/2008.08170v7
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08170v7)
- **Published**: 2020-08-18 22:19:29+00:00
- **Updated**: 2022-01-17 01:35:44+00:00
- **Authors**: Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang
- **Comment**: Published in Journal of Machine Learning Research (JMLR)
- **Journal**: None
- **Summary**: In the paper, we propose a class of accelerated zeroth-order and first-order momentum methods for both nonconvex mini-optimization and minimax-optimization. Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM) method for black-box mini-optimization where only function values can be obtained. Moreover, we prove that our Acc-ZOM method achieves a lower query complexity of $\tilde{O}(d^{3/4}\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which improves the best known result by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In particular, our Acc-ZOM does not need large batches required in the existing zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated zeroth-order momentum descent ascent (Acc-ZOMDA) method for black-box minimax optimization, where only function values can be obtained. Our Acc-ZOMDA obtains a low query complexity of $\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$ without requiring large batches for finding an $\epsilon$-stationary point, where $d_1$ and $d_2$ denote variable dimensions and $\kappa_y$ is condition number. Moreover, we propose an accelerated first-order momentum descent ascent (Acc-MDA) method for minimax optimization, whose explicit gradients are accessible. Our Acc-MDA achieves a low gradient complexity of $\tilde{O}(\kappa_y^{4.5}\epsilon^{-3})$ without requiring large batches for finding an $\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower gradient complexity of $\tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$ with a batch size $O(\kappa_y^4)$, which improves the best known result by a factor of $O(\kappa_y^{1/2})$. Extensive experimental results on black-box adversarial attack to deep neural networks and poisoning attack to logistic regression demonstrate efficiency of our algorithms.



### Learning to Generate Diverse Dance Motions with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2008.08171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.08171v1)
- **Published**: 2020-08-18 22:29:40+00:00
- **Updated**: 2020-08-18 22:29:40+00:00
- **Authors**: Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the ongoing pandemic, virtual concerts and live events using digitized performances of musicians are getting traction on massive multiplayer online worlds. However, well choreographed dance movements are extremely complex to animate and would involve an expensive and tedious production process. In addition to the use of complex motion capture systems, it typically requires a collaborative effort between animators, dancers, and choreographers. We introduce a complete system for dance motion synthesis, which can generate complex and highly diverse dance sequences given an input music sequence. As motion capture data is limited for the range of dance motions and styles, we introduce a massive dance motion data set that is created from YouTube videos. We also present a novel two-stream motion transformer generative model, which can generate motion sequences with high flexibility. We also introduce new evaluation metrics for the quality of synthesized dance motions, and demonstrate that our system can outperform state-of-the-art methods. Our system provides high-quality animations suitable for large crowds for virtual concerts and can also be used as reference for professional animation pipelines. Most importantly, we show that vast online videos can be effective in training dance motion models.



### Uncertainty-aware Self-supervised 3D Data Association
- **Arxiv ID**: http://arxiv.org/abs/2008.08173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.08173v1)
- **Published**: 2020-08-18 22:34:07+00:00
- **Updated**: 2020-08-18 22:34:07+00:00
- **Authors**: Jianren Wang, Siddharth Ancha, Yi-Ting Chen, David Held
- **Comment**: None
- **Journal**: 2020 IEEE/RSJ International Conference on Intelligent Robots and
  Systems
- **Summary**: 3D object trackers usually require training on large amounts of annotated data that is expensive and time-consuming to collect. Instead, we propose leveraging vast unlabeled datasets by self-supervised metric learning of 3D object trackers, with a focus on data association. Large scale annotations for unlabeled data are cheaply obtained by automatic object detection and association across frames. We show how these self-supervised annotations can be used in a principled manner to learn point-cloud embeddings that are effective for 3D tracking. We estimate and incorporate uncertainty in self-supervised tracking to learn more robust embeddings, without needing any labeled data. We design embeddings to differentiate objects across frames, and learn them using uncertainty-aware self-supervised training. Finally, we demonstrate their ability to perform accurate data association across frames, towards effective and accurate 3D tracking. Project videos and code are at https://jianrenw.github.io/Self-Supervised-3D-Data-Association.



### Discovering Multi-Hardware Mobile Models via Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2008.08178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08178v2)
- **Published**: 2020-08-18 22:58:17+00:00
- **Updated**: 2021-04-23 20:42:27+00:00
- **Authors**: Grace Chu, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, Andrew Howard
- **Comment**: CVPR Workshop 2021
- **Journal**: None
- **Summary**: Hardware-aware neural architecture designs have been predominantly focusing on optimizing model performance on single hardware and model development complexity, where another important factor, model deployment complexity, has been largely ignored. In this paper, we argue that, for applications that may be deployed on multiple hardware, having different single-hardware models across the deployed hardware makes it hard to guarantee consistent outputs across hardware and duplicates engineering work for debugging and fixing. To minimize such deployment cost, we propose an alternative solution, multi-hardware models, where a single architecture is developed for multiple hardware. With thoughtful search space design and incorporating the proposed multi-hardware metrics in neural architecture search, we discover multi-hardware models that give state-of-the-art (SoTA) performance across multiple hardware in both average and worse case scenarios. For performance on individual hardware, the single multi-hardware model yields similar or better results than SoTA performance on accelerators like GPU, DSP and EdgeTPU which was achieved by different models, while having similar performance with MobilenetV3 Large Minimalistic model on mobile CPU.



### Prevalence of Neural Collapse during the terminal phase of deep learning training
- **Arxiv ID**: http://arxiv.org/abs/2008.08186v2
- **DOI**: 10.1073/pnas.2015509117
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.08186v2)
- **Published**: 2020-08-18 23:12:54+00:00
- **Updated**: 2020-08-21 16:15:50+00:00
- **Authors**: Vardan Papyan, X. Y. Han, David L. Donoho
- **Comment**: None
- **Journal**: None
- **Summary**: Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.



### Learning Tuple Compatibility for Conditional OutfitRecommendation
- **Arxiv ID**: http://arxiv.org/abs/2008.08189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08189v1)
- **Published**: 2020-08-18 23:22:16+00:00
- **Updated**: 2020-08-18 23:22:16+00:00
- **Authors**: Xuewen Yang, Dongliang Xie, Xin Wang, Jiangbo Yuan, Wanying Ding, Pengyun Yan
- **Comment**: None
- **Journal**: ACM Multimedia 2020
- **Summary**: Outfit recommendation requires the answers of some challenging outfit compatibility questions such as 'Which pair of boots and school bag go well with my jeans and sweater?'. It is more complicated than conventional similarity search, and needs to consider not only visual aesthetics but also the intrinsic fine-grained and multi-category nature of fashion items. Some existing approaches solve the problem through sequential models or learning pair-wise distances between items. However, most of them only consider coarse category information in defining fashion compatibility while neglecting the fine-grained category information often desired in practical applications. To better define the fashion compatibility and more flexibly meet different needs, we propose a novel problem of learning compatibility among multiple tuples (each consisting of an item and category pair), and recommending fashion items following the category choices from customers. Our contributions include: 1) Designing a Mixed Category Attention Net (MCAN) which integrates both fine-grained and coarse category information into recommendation and learns the compatibility among fashion tuples. MCAN can explicitly and effectively generate diverse and controllable recommendations based on need. 2) Contributing a new dataset IQON, which follows eastern culture and can be used to test the generalization of recommendation systems. Our extensive experiments on a reference dataset Polyvore and our dataset IQON demonstrate that our method significantly outperforms state-of-the-art recommendation methods.



### PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data
- **Arxiv ID**: http://arxiv.org/abs/2008.08194v1
- **DOI**: 10.1007/978-3-030-68107-4_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08194v1)
- **Published**: 2020-08-18 23:37:05+00:00
- **Updated**: 2020-08-18 23:37:05+00:00
- **Authors**: Meng Ye, Qiaoying Huang, Dong Yang, Pengxiang Wu, Jingru Yi, Leon Axel, Dimitris Metaxas
- **Comment**: None
- **Journal**: STACOM 2020
- **Summary**: The 3D volumetric shape of the heart's left ventricle (LV) myocardium (MYO) wall provides important information for diagnosis of cardiac disease and invasive procedure navigation. Many cardiac image segmentation methods have relied on detection of region-of-interest as a pre-requisite for shape segmentation and modeling. With segmentation results, a 3D surface mesh and a corresponding point cloud of the segmented cardiac volume can be reconstructed for further analyses. Although state-of-the-art methods (e.g., U-Net) have achieved decent performance on cardiac image segmentation in terms of accuracy, these segmentation results can still suffer from imaging artifacts and noise, which will lead to inaccurate shape modeling results. In this paper, we propose a PC-U net that jointly reconstructs the point cloud of the LV MYO wall directly from volumes of 2D CT slices and generates its segmentation masks from the predicted 3D point cloud. Extensive experimental results show that by incorporating a shape prior from the point cloud, the segmentation masks are more accurate than the state-of-the-art U-Net results in terms of Dice's coefficient and Hausdorff distance.The proposed joint learning framework of our PC-U net is beneficial for automatic cardiac image analysis tasks because it can obtain simultaneously the 3D shape and segmentation of the LV MYO walls.



