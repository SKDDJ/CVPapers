# Arxiv Papers in cs.CV on 2020-08-12
### SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with Distractors
- **Arxiv ID**: http://arxiv.org/abs/2008.05955v1
- **DOI**: 10.1109/CVPRW.2019.00063
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05955v1)
- **Published**: 2020-08-12 00:14:19+00:00
- **Updated**: 2020-08-12 00:14:19+00:00
- **Authors**: Mona Jalal, Josef Spjut, Ben Boudaoud, Margrit Betke
- **Comment**: 3 pages, 4 figures, 1 table, Accepted at CVPR 2019 Workshop
- **Journal**: None
- **Summary**: We present a new, publicly-available image dataset generated by the NVIDIA Deep Learning Data Synthesizer intended for use in object detection, pose estimation, and tracking applications. This dataset contains 144k stereo image pairs that synthetically combine 18 camera viewpoints of three photorealistic virtual environments with up to 10 objects (chosen randomly from the 21 object models of the YCB dataset [1]) and flying distractors. Object and camera pose, scene lighting, and quantity of objects and distractors were randomized. Each provided view includes RGB, depth, segmentation, and surface normal images, all pixel level. We describe our approach for domain randomization and provide insight into the decisions that produced the dataset.



### Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05058v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.05058v4)
- **Published**: 2020-08-12 01:23:21+00:00
- **Updated**: 2022-01-03 23:32:01+00:00
- **Authors**: Borna Bešić, Abhinav Valada
- **Comment**: Dataset, code and models are available at
  http://rl.uni-freiburg.de/research/rgbd-inpainting
- **Journal**: IEEE Transactions on Intelligent Vehicles (T-IV), 2022
- **Summary**: Dynamic objects have a significant impact on the robot's perception of the environment which degrades the performance of essential tasks such as localization and mapping. In this work, we address this problem by synthesizing plausible color, texture and geometry in regions occluded by dynamic objects. We propose the novel geometry-aware DynaFill architecture that follows a coarse-to-fine topology and incorporates our gated recurrent feedback mechanism to adaptively fuse information from previous timesteps. We optimize our architecture using adversarial training to synthesize fine realistic textures which enables it to hallucinate color and depth structure in occluded regions online in a spatially and temporally coherent manner, without relying on future frame information. Casting our inpainting problem as an image-to-image translation task, our model also corrects regions correlated with the presence of dynamic objects in the scene, such as shadows or reflections. We introduce a large-scale hyperrealistic dataset with RGB-D images, semantic segmentation labels, camera poses as well as groundtruth RGB-D information of occluded regions. Extensive quantitative and qualitative evaluations show that our approach achieves state-of-the-art performance, even in challenging weather conditions. Furthermore, we present results for retrieval-based visual localization with the synthesized images that demonstrate the utility of our approach.



### Online Graph Completion: Multivariate Signal Recovery in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2008.05060v1
- **DOI**: 10.1109/CVPR.2017.533
- **Categories**: **cs.CV**, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05060v1)
- **Published**: 2020-08-12 01:34:21+00:00
- **Updated**: 2020-08-12 01:34:21+00:00
- **Authors**: Won Hwa Kim, Mona Jalal, Seongjae Hwang, Sterling C. Johnson, Vikas Singh
- **Comment**: 9 pages, 7 figures, CVPR 2017 Conference
- **Journal**: None
- **Summary**: The adoption of "human-in-the-loop" paradigms in computer vision and machine learning is leading to various applications where the actual data acquisition (e.g., human supervision) and the underlying inference algorithms are closely interwined. While classical work in active learning provides effective solutions when the learning module involves classification and regression tasks, many practical issues such as partially observed measurements, financial constraints and even additional distributional or structural aspects of the data typically fall outside the scope of this treatment. For instance, with sequential acquisition of partial measurements of data that manifest as a matrix (or tensor), novel strategies for completion (or collaborative filtering) of the remaining entries have only been studied recently. Motivated by vision problems where we seek to annotate a large dataset of images via a crowdsourced platform or alternatively, complement results from a state-of-the-art object detector using human feedback, we study the "completion" problem defined on graphs, where requests for additional measurements must be made sequentially. We design the optimization model in the Fourier domain of the graph describing how ideas based on adaptive submodularity provide algorithms that work well in practice. On a large set of images collected from Imgur, we see promising results on images that are otherwise difficult to categorize. We also show applications to an experimental design problem in neuroimaging.



### Select Good Regions for Deblurring based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.05065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05065v1)
- **Published**: 2020-08-12 01:58:35+00:00
- **Updated**: 2020-08-12 01:58:35+00:00
- **Authors**: Hang Yang, Xiaotian Wu, Xinglong Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of blind image deblurring is to recover sharp image from one input blurred image with an unknown blur kernel. Most of image deblurring approaches focus on developing image priors, however, there is not enough attention to the influence of image details and structures on the blur kernel estimation. What is the useful image structure and how to choose a good deblurring region? In this work, we propose a deep neural network model method for selecting good regions to estimate blur kernel. First we construct image patches with labels and train a deep neural networks, then the learned model is applied to determine which region of the image is most suitable to deblur. Experimental results illustrate that the proposed approach is effective, and could be able to select good regions for image deblurring.



### BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.05079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05079v1)
- **Published**: 2020-08-12 03:13:17+00:00
- **Updated**: 2020-08-12 03:13:17+00:00
- **Authors**: Lixin Yang, Jiasen Li, Wenqiang Xu, Yiqun Diao, Cewu Lu
- **Comment**: To appear on BMVC2020
- **Journal**: None
- **Summary**: 3D hand estimation has been a long-standing research topic in computer vision. A recent trend aims not only to estimate the 3D hand joint locations but also to recover the mesh model. However, achieving those goals from a single RGB image remains challenging. In this paper, we introduce an end-to-end learnable model, BiHand, which consists of three cascaded stages, namely 2D seeding stage, 3D lifting stage, and mesh generation stage. At the output of BiHand, the full hand mesh will be recovered using the joint rotations and shape parameters predicted from the network. Inside each stage, BiHand adopts a novel bisecting design which allows the networks to encapsulate two closely related information (e.g. 2D keypoints and silhouette in 2D seeding stage, 3D joints, and depth map in 3D lifting stage, joint rotations and shape parameters in the mesh generation stage) in a single forward pass. As the information represents different geometry or structure details, bisecting the data flow can facilitate optimization and increase robustness. For quantitative evaluation, we conduct experiments on two public benchmarks, namely the Rendered Hand Dataset (RHD) and the Stereo Hand Pose Tracking Benchmark (STB). Extensive experiments show that our model can achieve superior accuracy in comparison with state-of-the-art methods, and can produce appealing 3D hand meshes in several severe conditions.



### Self-supervised Light Field View Synthesis Using Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/2008.05084v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05084v1)
- **Published**: 2020-08-12 03:20:19+00:00
- **Updated**: 2020-08-12 03:20:19+00:00
- **Authors**: Yang Chen, Martin Alain, Aljosa Smolic
- **Comment**: Accepted at MMSP 2020
- **Journal**: None
- **Summary**: High angular resolution is advantageous for practical applications of light fields. In order to enhance the angular resolution of light fields, view synthesis methods can be utilized to generate dense intermediate views from sparse light field input. Most successful view synthesis methods are learning-based approaches which require a large amount of training data paired with ground truth. However, collecting such large datasets for light fields is challenging compared to natural images or videos. To tackle this problem, we propose a self-supervised light field view synthesis framework with cycle consistency. The proposed method aims to transfer prior knowledge learned from high quality natural video datasets to the light field view synthesis task, which reduces the need for labeled light field data. A cycle consistency constraint is used to build bidirectional mapping enforcing the generated views to be consistent with the input views. Derived from this key concept, two loss functions, cycle loss and reconstruction loss, are used to fine-tune the pre-trained model of a state-of-the-art video interpolation method. The proposed method is evaluated on various datasets to validate its robustness, and results show it not only achieves competitive performance compared to supervised fine-tuning, but also outperforms state-of-the-art light field view synthesis methods, especially when generating multiple intermediate views. Besides, our generic light field view synthesis framework can be adopted to any pre-trained model for advanced video interpolation.



### Learning to Caricature via Semantic Shape Transform
- **Arxiv ID**: http://arxiv.org/abs/2008.05090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05090v2)
- **Published**: 2020-08-12 03:41:49+00:00
- **Updated**: 2020-08-13 06:58:02+00:00
- **Authors**: Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Yu-Ting Chang, Yijun Li, Deng Cai, Ming-Hsuan Yang
- **Comment**: Submitted to IJCV, code and model are available at
  https://github.com/wenqingchu/Semantic-CariGANs/
- **Journal**: None
- **Summary**: Caricature is an artistic drawing created to abstract or exaggerate facial features of a person. Rendering visually pleasing caricatures is a difficult task that requires professional skills, and thus it is of great interest to design a method to automatically generate such drawings. To deal with large shape changes, we propose an algorithm based on a semantic shape transform to produce diverse and plausible shape exaggerations. Specifically, we predict pixel-wise semantic correspondences and perform image warping on the input photo to achieve dense shape transformation. We show that the proposed framework is able to render visually pleasing shape exaggerations while maintaining their facial structures. In addition, our model allows users to manipulate the shape via the semantic map. We demonstrate the effectiveness of our approach on a large photograph-caricature benchmark dataset with comparisons to the state-of-the-art methods.



### Inter-Image Communication for Weakly Supervised Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.05096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05096v1)
- **Published**: 2020-08-12 04:14:11+00:00
- **Updated**: 2020-08-12 04:14:11+00:00
- **Authors**: Xiaolin Zhang, Yunchao Wei, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised localization aims at finding target object regions using only image-level supervision. However, localization maps extracted from classification networks are often not accurate due to the lack of fine pixel-level supervision. In this paper, we propose to leverage pixel-level similarities across different objects for learning more accurate object locations in a complementary way. Particularly, two kinds of constraints are proposed to prompt the consistency of object features within the same categories. The first constraint is to learn the stochastic feature consistency among discriminative pixels that are randomly sampled from different images within a batch. The discriminative information embedded in one image can be leveraged to benefit its counterpart with inter-image communication. The second constraint is to learn the global consistency of object features throughout the entire dataset. We learn a feature center for each category and realize the global feature consistency by forcing the object features to approach class-specific centers. The global centers are actively updated with the training process. The two constraints can benefit each other to learn consistent pixel-level features within the same categories, and finally improve the quality of localization maps. We conduct extensive experiments on two popular benchmarks, i.e., ILSVRC and CUB-200-2011. Our method achieves the Top-1 localization error rate of 45.17% on the ILSVRC validation set, surpassing the current state-of-the-art method by a large margin. The code is available at https://github.com/xiaomengyc/I2C.



### FATNN: Fast and Accurate Ternary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.05101v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05101v4)
- **Published**: 2020-08-12 04:26:18+00:00
- **Updated**: 2021-07-29 11:50:10+00:00
- **Authors**: Peng Chen, Bohan Zhuang, Chunhua Shen
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision, ICCV 2021
- **Journal**: None
- **Summary**: Ternary Neural Networks (TNNs) have received much attention due to being potentially orders of magnitude faster in inference, as well as more power efficient, than full-precision counterparts. However, 2 bits are required to encode the ternary representation with only 3 quantization levels leveraged. As a result, conventional TNNs have similar memory consumption and speed compared with the standard 2-bit models, but have worse representational capability. Moreover, there is still a significant gap in accuracy between TNNs and full-precision networks, hampering their deployment to real applications. To tackle these two challenges, in this work, we first show that, under some mild constraints, computational complexity of the ternary inner product can be reduced by a factor of 2. Second, to mitigate the performance gap, we elaborately design an implementation-dependent ternary quantization algorithm. The proposed framework is termed Fast and Accurate Ternary Neural Networks (FATNN). Experiments on image classification demonstrate that our FATNN surpasses the state-of-the-arts by a significant margin in accuracy. More importantly, speedup evaluation compared with various precisions is analyzed on several platforms, which serves as a strong benchmark for further research.



### Local Temperature Scaling for Probability Calibration
- **Arxiv ID**: http://arxiv.org/abs/2008.05105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05105v2)
- **Published**: 2020-08-12 04:39:32+00:00
- **Updated**: 2021-07-27 10:30:12+00:00
- **Authors**: Zhipeng Ding, Xu Han, Peirong Liu, Marc Niethammer
- **Comment**: Accepted by ICCV-2021
- **Journal**: None
- **Summary**: For semantic segmentation, label probabilities are often uncalibrated as they are typically only the by-product of a segmentation task. Intersection over Union (IoU) and Dice score are often used as criteria for segmentation success, while metrics related to label probabilities are not often explored. However, probability calibration approaches have been studied, which match probability outputs with experimentally observed errors. These approaches mainly focus on classification tasks, but not on semantic segmentation. Thus, we propose a learning-based calibration method that focuses on multi-label semantic segmentation. Specifically, we adopt a convolutional neural network to predict local temperature values for probability calibration. One advantage of our approach is that it does not change prediction accuracy, hence allowing for calibration as a post-processing step. Experiments on the COCO, CamVid, and LPBA40 datasets demonstrate improved calibration performance for a range of different metrics. We also demonstrate the good performance of our method for multi-atlas brain segmentation from magnetic resonance images.



### Facial Expression Retargeting from Human to Avatar Made Easy
- **Arxiv ID**: http://arxiv.org/abs/2008.05110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.05110v1)
- **Published**: 2020-08-12 04:55:54+00:00
- **Updated**: 2020-08-12 04:55:54+00:00
- **Authors**: Juyong Zhang, Keyu Chen, Jianmin Zheng
- **Comment**: IEEE Transactions on Visualization and Computer Graphics (TVCG), to
  appear
- **Journal**: None
- **Summary**: Facial expression retargeting from humans to virtual characters is a useful technique in computer graphics and animation. Traditional methods use markers or blendshapes to construct a mapping between the human and avatar faces. However, these approaches require a tedious 3D modeling process, and the performance relies on the modelers' experience. In this paper, we propose a brand-new solution to this cross-domain expression transfer problem via nonlinear expression embedding and expression domain translation. We first build low-dimensional latent spaces for the human and avatar facial expressions with variational autoencoder. Then we construct correspondences between the two latent spaces guided by geometric and perceptual constraints. Specifically, we design geometric correspondences to reflect geometric matching and utilize a triplet data structure to express users' perceptual preference of avatar expressions. A user-friendly method is proposed to automatically generate triplets for a system allowing users to easily and efficiently annotate the correspondences. Using both geometric and perceptual correspondences, we trained a network for expression domain translation from human to avatar. Extensive experimental results and user studies demonstrate that even nonprofessional users can apply our method to generate high-quality facial expression retargeting results with less time and effort.



### A Longitudinal Method for Simultaneous Whole-Brain and Lesion Segmentation in Multiple Sclerosis
- **Arxiv ID**: http://arxiv.org/abs/2008.05117v2
- **DOI**: 10.1007/978-3-030-66843-3_12
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05117v2)
- **Published**: 2020-08-12 05:43:59+00:00
- **Updated**: 2020-09-15 13:03:48+00:00
- **Authors**: Stefano Cerri, Andrew Hoopes, Douglas N. Greve, Mark Mühlau, Koen Van Leemput
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel method for the segmentation of longitudinal brain MRI scans of patients suffering from Multiple Sclerosis. The method builds upon an existing cross-sectional method for simultaneous whole-brain and lesion segmentation, introducing subject-specific latent variables to encourage temporal consistency between longitudinal scans. It is very generally applicable, as it does not make any prior assumptions on the scanner, the MRI protocol, or the number and timing of longitudinal follow-up scans. Preliminary experiments on three longitudinal datasets indicate that the proposed method produces more reliable segmentations and detects disease effects better than the cross-sectional method it is based upon.



### Open Set Recognition with Conditional Probabilistic Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2008.05129v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05129v2)
- **Published**: 2020-08-12 06:23:49+00:00
- **Updated**: 2021-02-09 10:18:43+00:00
- **Authors**: Xin Sun, Chi Zhang, Guosheng Lin, Keck-Voon Ling
- **Comment**: Extended version of CGDL arXiv:2003.08823 in CVPR2020
- **Journal**: None
- **Summary**: Deep neural networks have made breakthroughs in a wide range of visual understanding tasks. A typical challenge that hinders their real-world applications is that unknown samples may be fed into the system during the testing phase, but traditional deep neural networks will wrongly recognize these unknown samples as one of the known classes. Open set recognition (OSR) is a potential solution to overcome this problem, where the open set classifier should have the flexibility to reject unknown samples and meanwhile maintain high classification accuracy in known classes. Probabilistic generative models, such as Variational Autoencoders (VAE) and Adversarial Autoencoders (AAE), are popular methods to detect unknowns, but they cannot provide discriminative representations for known classification. In this paper, we propose a novel framework, called Conditional Probabilistic Generative Models (CPGM), for open set recognition. The core insight of our work is to add discriminative information into the probabilistic generative models, such that the proposed models can not only detect unknown samples but also classify known classes by forcing different latent features to approximate conditional Gaussian distributions. We discuss many model variants and provide comprehensive experiments to study their characteristics. Experiment results on multiple benchmark datasets reveal that the proposed method significantly outperforms the baselines and achieves new state-of-the-art performance.



### Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?
- **Arxiv ID**: http://arxiv.org/abs/2008.05132v2
- **DOI**: 10.1145/3368089.3409691
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2008.05132v2)
- **Published**: 2020-08-12 06:36:33+00:00
- **Updated**: 2020-09-07 12:57:33+00:00
- **Authors**: Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming Zhu, Guoqiang Li
- **Comment**: 13 pages, accepted to ESEC/FSE '20
- **Journal**: None
- **Summary**: Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.



### An Inter- and Intra-Band Loss for Pansharpening Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.05133v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05133v1)
- **Published**: 2020-08-12 06:38:15+00:00
- **Updated**: 2020-08-12 06:38:15+00:00
- **Authors**: Jiajun Cai, Bo Huang
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Pansharpening aims to fuse panchromatic and multispectral images from the satellite to generate images with both high spatial and spectral resolution. With the successful applications of deep learning in the computer vision field, a lot of scholars have proposed many convolutional neural networks (CNNs) to solve the pansharpening task. These pansharpening networks focused on various distinctive structures of CNNs, and most of them are trained by L2 loss between fused images and simulated desired multispectral images. However, L2 loss is designed to directly minimize the difference of spectral information of each band, which does not consider the inter-band relations in the training process. In this letter, we propose a novel inter- and intra-band (IIB) loss to overcome the drawback of original L2 loss. Our proposed IIB loss can effectively preserve both inter- and intra-band relations and can be directly applied to different pansharpening CNNs.



### An Overview of Deep Learning Architectures in Few-Shot Learning Domain
- **Arxiv ID**: http://arxiv.org/abs/2008.06365v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06365v4)
- **Published**: 2020-08-12 06:58:45+00:00
- **Updated**: 2023-04-16 05:27:57+00:00
- **Authors**: Shruti Jadon, Aryan Jadon
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: Since 2012, Deep learning has revolutionized Artificial Intelligence and has achieved state-of-the-art outcomes in different domains, ranging from Image Classification to Speech Generation. Though it has many potentials, our current architectures come with the pre-requisite of large amounts of data. Few-Shot Learning (also known as one-shot learning) is a sub-field of machine learning that aims to create such models that can learn the desired objective with less data, similar to how humans learn. In this paper, we have reviewed some of the well-known deep learning-based approaches towards few-shot learning. We have discussed the recent achievements, challenges, and possibilities of improvement of few-shot learning based deep learning architectures. Our aim for this paper is threefold: (i) Give a brief introduction to deep learning architectures for few-shot learning with pointers to core references. (ii) Indicate how deep learning has been applied to the low-data regime, from data preparation to model training. and, (iii) Provide a starting point for people interested in experimenting and perhaps contributing to the field of few-shot learning by pointing out some useful resources and open-source code. Our code is available at Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.



### ASAP-Net: Attention and Structure Aware Point Cloud Sequence Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.05149v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2008.05149v1)
- **Published**: 2020-08-12 07:37:16+00:00
- **Updated**: 2020-08-12 07:37:16+00:00
- **Authors**: Hanwen Cao, Yongyi Lu, Cewu Lu, Bo Pang, Gongshen Liu, Alan Yuille
- **Comment**: The British Machine Vision Conference (BMVC)
- **Journal**: None
- **Summary**: Recent works of point clouds show that mulit-frame spatio-temporal modeling outperforms single-frame versions by utilizing cross-frame information. In this paper, we further improve spatio-temporal point cloud feature learning with a flexible module called ASAP considering both attention and structure information across frames, which we find as two important factors for successful segmentation in dynamic point clouds. Firstly, our ASAP module contains a novel attentive temporal embedding layer to fuse the relatively informative local features across frames in a recurrent fashion. Secondly, an efficient spatio-temporal correlation method is proposed to exploit more local structure for embedding, meanwhile enforcing temporal consistency and reducing computation complexity. Finally, we show the generalization ability of the proposed ASAP module with different backbone networks for point cloud sequence segmentation. Our ASAP-Net (backbone plus ASAP module) outperforms baselines and previous methods on both Synthia and SemanticKITTI datasets (+3.4 to +15.2 mIoU points with different backbones). Code is availabe at https://github.com/intrepidChw/ASAP-Net



### HOSE-Net: Higher Order Structure Embedded Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.05156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05156v1)
- **Published**: 2020-08-12 07:58:13+00:00
- **Updated**: 2020-08-12 07:58:13+00:00
- **Authors**: Meng Wei, Chun Yuan, Xiaoyu Yue, Kuo Zhong
- **Comment**: Accepted to ACM MM 2020
- **Journal**: None
- **Summary**: Scene graph generation aims to produce structured representations for images, which requires to understand the relations between objects. Due to the continuous nature of deep neural networks, the prediction of scene graphs is divided into object detection and relation classification. However, the independent relation classes cannot separate the visual features well. Although some methods organize the visual features into graph structures and use message passing to learn contextual information, they still suffer from drastic intra-class variations and unbalanced data distributions. One important factor is that they learn an unstructured output space that ignores the inherent structures of scene graphs. Accordingly, in this paper, we propose a Higher Order Structure Embedded Network (HOSE-Net) to mitigate this issue. First, we propose a novel structure-aware embedding-to-classifier(SEC) module to incorporate both local and global structural information of relationships into the output space. Specifically, a set of context embeddings are learned via local graph based message passing and then mapped to a global structure based classification space. Second, since learning too many context-specific classification subspaces can suffer from data sparsity issues, we propose a hierarchical semantic aggregation(HSA) module to reduces the number of subspaces by introducing higher order structural information. HSA is also a fast and flexible tool to automatically search a semantic object hierarchy based on relational knowledge graphs. Extensive experiments show that the proposed HOSE-Net achieves the state-of-the-art performance on two popular benchmarks of Visual Genome and VRD.



### Towards Geometry Guided Neural Relighting with Flash Photography
- **Arxiv ID**: http://arxiv.org/abs/2008.05157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05157v1)
- **Published**: 2020-08-12 08:03:28+00:00
- **Updated**: 2020-08-12 08:03:28+00:00
- **Authors**: Di Qiu, Jin Zeng, Zhanghan Ke, Wenxiu Sun, Chengxi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Previous image based relighting methods require capturing multiple images to acquire high frequency lighting effect under different lighting conditions, which needs nontrivial effort and may be unrealistic in certain practical use scenarios. While such approaches rely entirely on cleverly sampling the color images under different lighting conditions, little has been done to utilize geometric information that crucially influences the high-frequency features in the images, such as glossy highlight and cast shadow. We therefore propose a framework for image relighting from a single flash photograph with its corresponding depth map using deep learning. By incorporating the depth map, our approach is able to extrapolate realistic high-frequency effects under novel lighting via geometry guided image decomposition from the flashlight image, and predict the cast shadow map from the shadow-encoding transformed depth map. Moreover, the single-image based setup greatly simplifies the data capture process. We experimentally validate the advantage of our geometry guided approach over state-of-the-art image-based approaches in intrinsic image decomposition and image relighting, and also demonstrate our performance on real mobile phone photo examples.



### Balanced Depth Completion between Dense Depth Inference and Sparse Range Measurements via KISS-GP
- **Arxiv ID**: http://arxiv.org/abs/2008.05158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.05158v1)
- **Published**: 2020-08-12 08:07:55+00:00
- **Updated**: 2020-08-12 08:07:55+00:00
- **Authors**: Sungho Yoon, Ayoung Kim
- **Comment**: accepted to IROS 2020. 8 pages, 9 figures, 2 tables. Video at this
  https://www.youtube.com/watch?v=x8n0lvjvorg&t=33s
- **Journal**: None
- **Summary**: Estimating a dense and accurate depth map is the key requirement for autonomous driving and robotics. Recent advances in deep learning have allowed depth estimation in full resolution from a single image. Despite this impressive result, many deep-learning-based monocular depth estimation (MDE) algorithms have failed to keep their accuracy yielding a meter-level estimation error. In many robotics applications, accurate but sparse measurements are readily available from Light Detection and Ranging (LiDAR). Although they are highly accurate, the sparsity limits full resolution depth map reconstruction. Targeting the problem of dense and accurate depth map recovery, this paper introduces the fusion of these two modalities as a depth completion (DC) problem by dividing the role of depth inference and depth regression. Utilizing the state-of-the-art MDE and our Gaussian process (GP) based depth-regression method, we propose a general solution that can flexibly work with various MDE modules by enhancing its depth with sparse range measurements. To overcome the major limitation of GP, we adopt Kernel Interpolation for Scalable Structured (KISS)-GP and mitigate the computational complexity from O(N^3) to O(N). Our experiments demonstrate that the accuracy and robustness of our method outperform state-of-the-art unsupervised methods for sparse and biased measurements.



### An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.10400v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10400v2)
- **Published**: 2020-08-12 09:27:05+00:00
- **Updated**: 2020-10-05 03:49:48+00:00
- **Authors**: Sanghyeon An, Minjun Lee, Sanglee Park, Heerin Yang, Jungmin So
- **Comment**: 10 pages, 12 figures, 7 tables
- **Journal**: None
- **Summary**: We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3x3, 5x5, and 7x7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, and pooling is not used. Rotation and translation is used to augment training data, which is frequently used in most image classification tasks. A majority voting using the three models independently trained on the training data set can achieve up to 99.87% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91% test accuracy. The results can be reproduced by using the code at: https://github.com/ansh941/MnistSimpleCNN



### RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion Judgement and Objective AU Annotations
- **Arxiv ID**: http://arxiv.org/abs/2008.05196v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05196v3)
- **Published**: 2020-08-12 09:29:16+00:00
- **Updated**: 2020-09-28 07:20:14+00:00
- **Authors**: Wenjing Yan, Shan Li, Chengtao Que, JiQuan Pei, Weihong Deng
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: Much of the work on automatic facial expression recognition relies on databases containing a certain number of emotion classes and their exaggerated facial configurations (generally six prototypical facial expressions), based on Ekman's Basic Emotion Theory. However, recent studies have revealed that facial expressions in our human life can be blended with multiple basic emotions. And the emotion labels for these in-the-wild facial expressions cannot easily be annotated solely on pre-defined AU patterns. How to analyze the action units for such complex expressions is still an open question. To address this issue, we develop a RAF-AU database that employs a sign-based (i.e., AUs) and judgement-based (i.e., perceived emotion) approach to annotating blended facial expressions in the wild. We first reviewed the annotation methods in existing databases and identified crowdsourcing as a promising strategy for labeling in-the-wild facial expressions. Then, RAF-AU was finely annotated by experienced coders, on which we also conducted a preliminary investigation of which key AUs contribute most to a perceived emotion, and the relationship between AUs and facial expressions. Finally, we provided a baseline for AU recognition in RAF-AU using popular features and multi-label learning methods.



### Representative Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.05202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05202v1)
- **Published**: 2020-08-12 09:46:52+00:00
- **Updated**: 2020-08-12 09:46:52+00:00
- **Authors**: Changqian Yu, Yifan Liu, Changxin Gao, Chunhua Shen, Nong Sang
- **Comment**: Accepted to ECCV 2020. Code is available at https://git.io/RepGraph
- **Journal**: European Conference on Computer Vision 2020
- **Summary**: Non-local operation is widely explored to model the long-range dependencies. However, the redundant computation in this operation leads to a prohibitive complexity. In this paper, we present a Representative Graph (RepGraph) layer to dynamically sample a few representative features, which dramatically reduces redundancy. Instead of propagating the messages from all positions, our RepGraph layer computes the response of one node merely with a few representative nodes. The locations of representative nodes come from a learned spatial offset matrix. The RepGraph layer is flexible to integrate into many visual architectures and combine with other operations. With the application of semantic segmentation, without any bells and whistles, our RepGraph network can compete or perform favourably against the state-of-the-art methods on three challenging benchmarks: ADE20K, Cityscapes, and PASCAL-Context datasets. In the task of object detection, our RepGraph layer can also improve the performance on the COCO dataset compared to the non-local operation. Code is available at https://git.io/RepGraph.



### Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep Learning Semantic and Contour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.05204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07 (Primary) 68T45 (Secondary), I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2008.05204v1)
- **Published**: 2020-08-12 09:54:17+00:00
- **Updated**: 2020-08-12 09:54:17+00:00
- **Authors**: Iason Katsamenis, Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis, Athanasios Voulodimos
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Corrosion detection on metal constructions is a major challenge in civil engineering for quick, safe and effective inspection. Existing image analysis approaches tend to place bounding boxes around the defected region which is not adequate both for structural analysis and pre-fabrication, an innovative construction concept which reduces maintenance cost, time and improves safety. In this paper, we apply three semantic segmentation-oriented deep learning models (FCN, U-Net and Mask R-CNN) for corrosion detection, which perform better in terms of accuracy and time and require a smaller number of annotated samples compared to other deep models, e.g. CNN. However, the final images derived are still not sufficiently accurate for structural analysis and pre-fabrication. Thus, we adopt a novel data projection scheme that fuses the results of color segmentation, yielding accurate but over-segmented contours of a region, with a processed area of the deep masks, resulting in high-confidence corroded pixels.



### Large-Scale Analysis of Iliopsoas Muscle Volumes in the UK Biobank
- **Arxiv ID**: http://arxiv.org/abs/2008.05217v2
- **DOI**: 10.1038/s41598-020-77351-0
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05217v2)
- **Published**: 2020-08-12 10:28:39+00:00
- **Updated**: 2020-08-14 11:16:07+00:00
- **Authors**: Julie Fitzpatrick, Nicolas Basty, Madeleine Cule, Yi Liu, Jimmy D. Bell, E. Louise Thomas, Brandon Whitcher
- **Comment**: Julie Fitzpatrick and Nicolas Basty are joint first authors
- **Journal**: Scientific Reports 10 (1), 1-10, 2020
- **Summary**: Psoas muscle measurements are frequently used as markers of sarcopenia and predictors of health. Manually measured cross-sectional areas are most commonly used, but there is a lack of consistency regarding the position of the measurementand manual annotations are not practical for large population studies. We have developed a fully automated method to measure iliopsoas muscle volume (comprised of the psoas and iliacus muscles) using a convolutional neural network. Magnetic resonance images were obtained from the UK Biobank for 5,000 male and female participants, balanced for age, gender and BMI. Ninety manual annotations were available for model training and validation. The model showed excellent performance against out-of-sample data (dice score coefficient of 0.912 +/- 0.018). Iliopsoas muscle volumes were successfully measured in all 5,000 participants. Iliopsoas volume was greater in male compared with female subjects. There was a small but significant asymmetry between left and right iliopsoas muscle volumes. We also found that iliopsoas volume was significantly related to height, BMI and age, and that there was an acceleration in muscle volume decrease in men with age. Our method provides a robust technique for measuring iliopsoas muscle volume that can be applied to large cohorts.



### Compression of Deep Learning Models for Text: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2008.05221v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05221v4)
- **Published**: 2020-08-12 10:42:14+00:00
- **Updated**: 2021-06-13 17:47:28+00:00
- **Authors**: Manish Gupta, Puneet Agrawal
- **Comment**: Accepted at TKDD for publication. 53 pages
- **Journal**: None
- **Summary**: In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanksto deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like Bidirectional Encoder Representations from Transformers (BERT) [24], GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network (MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are humongous in size. On the other hand,real world applications demand small model size, low response times and low computational power wattage. In this survey, wediscuss six different types of methods (Pruning, Quantization, Knowledge Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic Transformer based methods) for compression of such models to enable their deployment in real industry NLP projects.Given the critical need of building applications with efficient and small models, and the large amount of recently published work inthis area, we believe that this survey organizes the plethora of work done by the 'deep learning for NLP' community in the past fewyears and presents it as a coherent story.



### A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2008.05225v1
- **DOI**: 10.1109/LGRS.2021.3056392
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05225v1)
- **Published**: 2020-08-12 10:51:24+00:00
- **Updated**: 2020-08-12 10:51:24+00:00
- **Authors**: Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional existing retrieval methods in remote sensing (RS) are often based on a uni-modal data retrieval framework. In this work, we propose a novel inter-modal triplet-based zero-shot retrieval scheme utilizing a sketch-based representation of RS data. The proposed scheme performs efficiently even when the sketch representations are marginally prototypical of the image. We conducted experiments on a new bi-modal image-sketch dataset called Earth on Canvas (EoC) conceived during this study. We perform a thorough bench-marking of this dataset and demonstrate that the proposed network outperforms other state-of-the-art methods for zero-shot sketch-based retrieval framework in remote sensing.



### Defending Adversarial Examples via DNN Bottleneck Reinforcement
- **Arxiv ID**: http://arxiv.org/abs/2008.05230v1
- **DOI**: 10.1145/3394171.3413604
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05230v1)
- **Published**: 2020-08-12 11:02:01+00:00
- **Updated**: 2020-08-12 11:02:01+00:00
- **Authors**: Wenqing Liu, Miaojing Shi, Teddy Furon, Li Li
- **Comment**: ACM MM 2020 - Full Paper
- **Journal**: None
- **Summary**: This paper presents a DNN bottleneck reinforcement scheme to alleviate the vulnerability of Deep Neural Networks (DNN) against adversarial attacks. Typical DNN classifiers encode the input image into a compressed latent representation more suitable for inference. This information bottleneck makes a trade-off between the image-specific structure and class-specific information in an image. By reinforcing the former while maintaining the latter, any redundant information, be it adversarial or not, should be removed from the latent representation. Hence, this paper proposes to jointly train an auto-encoder (AE) sharing the same encoding weights with the visual classifier. In order to reinforce the information bottleneck, we introduce the multi-scale low-pass objective and multi-scale high-frequency communication for better frequency steering in the network. Unlike existing approaches, our scheme is the first reforming defense per se which keeps the classifier structure untouched without appending any pre-processing head and is trained with clean images only. Extensive experiments on MNIST, CIFAR-10 and ImageNet demonstrate the strong defense of our method against various adversarial attacks.



### Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders
- **Arxiv ID**: http://arxiv.org/abs/2008.05231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05231v2)
- **Published**: 2020-08-12 11:02:40+00:00
- **Updated**: 2021-03-02 16:12:52+00:00
- **Authors**: Nicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio Falchi, Claudio Gennaro, Stéphane Marchand-Maillet
- **Comment**: Accepted in ACM Transactions on Multimedia Computing, Communications,
  and Applications (TOMM). arXiv admin note: text overlap with arXiv:2004.09144
- **Journal**: None
- **Summary**: Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences, i.e., image regions and words, respectively, in order to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.   Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way towards the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN.



### PAM:Point-wise Attention Module for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.05242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05242v1)
- **Published**: 2020-08-12 11:29:48+00:00
- **Updated**: 2020-08-12 11:29:48+00:00
- **Authors**: Myoungha Song, Jeongho Lee, Donghwan Kim
- **Comment**: 11 pages, 5figures
- **Journal**: None
- **Summary**: 6D pose estimation refers to object recognition and estimation of 3D rotation and 3D translation. The key technology for estimating 6D pose is to estimate pose by extracting enough features to find pose in any environment. Previous methods utilized depth information in the refinement process or were designed as a heterogeneous architecture for each data space to extract feature. However, these methods are limited in that they cannot extract sufficient feature. Therefore, this paper proposes a Point Attention Module that can efficiently extract powerful feature from RGB-D. In our Module, attention map is formed through a Geometric Attention Path(GAP) and Channel Attention Path(CAP). In GAP, it is designed to pay attention to important information in geometric information, and CAP is designed to pay attention to important information in Channel information. We show that the attention module efficiently creates feature representations without significantly increasing computational complexity. Experimental results show that the proposed method outperforms the existing methods in benchmarks, YCB Video and LineMod. In addition, the attention module was applied to the classification task, and it was confirmed that the performance significantly improved compared to the existing model.



### Learning to Learn from Mistakes: Robust Optimization for Adversarial Noise
- **Arxiv ID**: http://arxiv.org/abs/2008.05247v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.05247v1)
- **Published**: 2020-08-12 11:44:01+00:00
- **Updated**: 2020-08-12 11:44:01+00:00
- **Authors**: Alex Serban, Erik Poll, Joost Visser
- **Comment**: Published at ICANN 2020
- **Journal**: None
- **Summary**: Sensitivity to adversarial noise hinders deployment of machine learning algorithms in security-critical applications. Although many adversarial defenses have been proposed, robustness to adversarial noise remains an open problem. The most compelling defense, adversarial training, requires a substantial increase in processing time and it has been shown to overfit on the training data. In this paper, we aim to overcome these limitations by training robust models in low data regimes and transfer adversarial knowledge between different models. We train a meta-optimizer which learns to robustly optimize a model using adversarial examples and is able to transfer the knowledge learned to new models, without the need to generate new adversarial examples. Experimental results show the meta-optimizer is consistent across different architectures and data sets, suggesting it is possible to automatically patch adversarial vulnerabilities.



### Identity-Aware Attribute Recognition via Real-Time Distributed Inference in Mobile Edge Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.05255v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.MM, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2008.05255v1)
- **Published**: 2020-08-12 12:03:27+00:00
- **Updated**: 2020-08-12 12:03:27+00:00
- **Authors**: Zichuan Xu, Jiangkai Wu, Qiufen Xia, Pan Zhou, Jiankang Ren, Huizhi Liang
- **Comment**: 9 pages, 8 figures, Proceedings of the 28th ACM International
  Conference on Multimedia (ACM MM'20), Seattle, WA, USA
- **Journal**: None
- **Summary**: With the development of deep learning technologies, attribute recognition and person re-identification (re-ID) have attracted extensive attention and achieved continuous improvement via executing computing-intensive deep neural networks in cloud datacenters. However, the datacenter deployment cannot meet the real-time requirement of attribute recognition and person re-ID, due to the prohibitive delay of backhaul networks and large data transmissions from cameras to datacenters. A feasible solution thus is to employ mobile edge clouds (MEC) within the proximity of cameras and enable distributed inference. In this paper, we design novel models for pedestrian attribute recognition with re-ID in an MEC-enabled camera monitoring system. We also investigate the problem of distributed inference in the MEC-enabled camera network. To this end, we first propose a novel inference framework with a set of distributed modules, by jointly considering the attribute recognition and person re-ID. We then devise a learning-based algorithm for the distributions of the modules of the proposed distributed inference framework, considering the dynamic MEC-enabled camera network with uncertainties. We finally evaluate the performance of the proposed algorithm by both simulations with real datasets and system implementation in a real testbed. Evaluation results show that the performance of the proposed algorithm with distributed inference framework is promising, by reaching the accuracies of attribute recognition and person identification up to 92.9% and 96.6% respectively, and significantly reducing the inference delay by at least 40.6% compared with existing methods.



### Guided Collaborative Training for Pixel-wise Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05258v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05258v1)
- **Published**: 2020-08-12 12:08:25+00:00
- **Updated**: 2020-08-12 12:08:25+00:00
- **Authors**: Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, Rynson W. H. Lau
- **Comment**: 16th European Conference on Computer Vision (ECCV 2020)
- **Journal**: None
- **Summary**: We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collaborative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outperforms state-of-the-art SSL methods by a large margin. Our code available at: https://github.com/ZHKKKe/PixelSSL.



### Factor Graph based 3D Multi-Object Tracking in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.05309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.05309v1)
- **Published**: 2020-08-12 13:34:46+00:00
- **Updated**: 2020-08-12 13:34:46+00:00
- **Authors**: Johannes Pöschmann, Tim Pfeifer, Peter Protzel
- **Comment**: 8 pages, 4 figures, accepted by IEEE Intl. Conf. on Intelligent
  Robots and Systems (IROS) 2020, visualization of the results of our offline
  tracker available at https://www.youtube.com/watch?v=mvZmli4jrZQ
- **Journal**: None
- **Summary**: Accurate and reliable tracking of multiple moving objects in 3D space is an essential component of urban scene understanding. This is a challenging task because it requires the assignment of detections in the current frame to the predicted objects from the previous one. Existing filter-based approaches tend to struggle if this initial assignment is not correct, which can happen easily. We propose a novel optimization-based approach that does not rely on explicit and fixed assignments. Instead, we represent the result of an off-the-shelf 3D object detector as Gaussian mixture model, which is incorporated in a factor graph framework. This gives us the flexibility to assign all detections to all objects simultaneously. As a result, the assignment problem is solved implicitly and jointly with the 3D spatial multi-object state estimation using non-linear least squares optimization. Despite its simplicity, the proposed algorithm achieves robust and reliable tracking results and can be applied for offline as well as online tracking. We demonstrate its performance on the real world KITTI tracking dataset and achieve better results than many state-of-the-art algorithms. Especially the consistency of the estimated tracks is superior offline as well as online.



### TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2008.05314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05314v1)
- **Published**: 2020-08-12 13:44:20+00:00
- **Updated**: 2020-08-12 13:44:20+00:00
- **Authors**: Yibo Hu, Xiang Wu, Ran He
- **Comment**: Accepted by ECCV2020. Code is available at
  https://github.com/AberHu/TF-NAS
- **Journal**: None
- **Summary**: With the flourish of differentiable neural architecture search (NAS), automatically searching latency-constrained architectures gives a new perspective to reduce human labor and expertise. However, the searched architectures are usually suboptimal in accuracy and may have large jitters around the target latency. In this paper, we rethink three freedoms of differentiable NAS, i.e. operation-level, depth-level and width-level, and propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good classification accuracy and precise latency constraint. For the operation-level, we present a bi-sampling search algorithm to moderate the operation collapse. For the depth-level, we introduce a sink-connecting search space to ensure the mutual exclusion between skip and other candidate operations, as well as eliminate the architecture redundancy. For the width-level, we propose an elasticity-scaling strategy that achieves precise latency constraint in a progressively fine-grained manner. Experiments on ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched TF-NAS-A obtains 76.9% top-1 accuracy, achieving state-of-the-art results with less latency. The total search time is only 1.8 days on 1 Titan RTX GPU. Code is available at https://github.com/AberHu/TF-NAS.



### Renal Cell Carcinoma Detection and Subtyping with Minimal Point-Based Annotation in Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2008.05332v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05332v1)
- **Published**: 2020-08-12 14:12:07+00:00
- **Updated**: 2020-08-12 14:12:07+00:00
- **Authors**: Zeyu Gao, Pargorn Puttapirat, Jiangbo Shi, Chen Li
- **Comment**: 10 pages, 5 figure, 3 tables, accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Obtaining a large amount of labeled data in medical imaging is laborious and time-consuming, especially for histopathology. However, it is much easier and cheaper to get unlabeled data from whole-slide images (WSIs). Semi-supervised learning (SSL) is an effective way to utilize unlabeled data and alleviate the need for labeled data. For this reason, we proposed a framework that employs an SSL method to accurately detect cancerous regions with a novel annotation method called Minimal Point-Based annotation, and then utilize the predicted results with an innovative hybrid loss to train a classification model for subtyping. The annotator only needs to mark a few points and label them are cancer or not in each WSI. Experiments on three significant subtypes of renal cell carcinoma (RCC) proved that the performance of the classifier trained with the Min-Point annotated dataset is comparable to a classifier trained with the segmentation annotated dataset for cancer region detection. And the subtyping model outperforms a model trained with only diagnostic labels by 12% in terms of f1-score for testing WSIs.



### Image-based Portrait Engraving
- **Arxiv ID**: http://arxiv.org/abs/2008.05336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05336v1)
- **Published**: 2020-08-12 14:13:53+00:00
- **Updated**: 2020-08-12 14:13:53+00:00
- **Authors**: Paul L. Rosin, Yu-Kun Lai
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: This paper describes a simple image-based method that applies engraving stylisation to portraits using ordered dithering. Face detection is used to estimate a rough proxy geometry of the head consisting of a cylinder, which is used to warp the dither matrix, causing the engraving lines to curve around the face for better stylisation. Finally, an application of the approach to colour engraving is demonstrated.



### LogoDet-3K: A Large-Scale Image Dataset for Logo Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.05359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.05359v1)
- **Published**: 2020-08-12 14:57:53+00:00
- **Updated**: 2020-08-12 14:57:53+00:00
- **Authors**: Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng, Shuqiang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Logo detection has been gaining considerable attention because of its wide range of applications in the multimedia field, such as copyright infringement detection, brand visibility monitoring, and product brand management on social media. In this paper, we introduce LogoDet-3K, the largest logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. We describe the collection and annotation process of our dataset, analyze its scale and diversity in comparison to other datasets for logo detection. We further propose a strong baseline method Logo-Yolo, which incorporates Focal loss and CIoU loss into the state-of-the-art YOLOv3 framework for large-scale logo detection. Logo-Yolo can solve the problems of multi-scale objects, logo sample imbalance and inconsistent bounding-box regression. It obtains about 4% improvement on the average performance compared with YOLOv3, and greater improvements compared with reported several deep detection models on LogoDet-3K. The evaluations on other three existing datasets further verify the effectiveness of our method, and demonstrate better generalization ability of LogoDet-3K on logo detection and retrieval tasks. The LogoDet-3K dataset is used to promote large-scale logo-related research and it can be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.



### Anomaly localization by modeling perceptual features
- **Arxiv ID**: http://arxiv.org/abs/2008.05369v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05369v1)
- **Published**: 2020-08-12 15:09:13+00:00
- **Updated**: 2020-08-12 15:09:13+00:00
- **Authors**: David Dehaene, Pierre Eline
- **Comment**: None
- **Journal**: None
- **Summary**: Although unsupervised generative modeling of an image dataset using a Variational AutoEncoder (VAE) has been used to detect anomalous images, or anomalous regions in images, recent works have shown that this method often identifies images or regions that do not concur with human perception, even questioning the usability of generative models for robust anomaly detection. Here, we argue that those issues can emerge from having a simplistic model of the anomaly distribution and we propose a new VAE-based model expressing a more complex anomaly model that is also closer to human perception. This Feature-Augmented VAE is trained by not only reconstructing the input image in pixel space, but also in several different feature spaces, which are computed by a convolutional neural network trained beforehand on a large image dataset. It achieves clear improvement over state-of-the-art methods on the MVTec anomaly detection and localization datasets.



### Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text
- **Arxiv ID**: http://arxiv.org/abs/2008.05373v5
- **DOI**: 10.3390/jimaging6120141
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05373v5)
- **Published**: 2020-08-12 15:14:47+00:00
- **Updated**: 2020-08-20 13:59:37+00:00
- **Authors**: Abdelrahman Abdallah, Mohamed Hamada, Daniyar Nurseitov
- **Comment**: None
- **Journal**: J. Imaging 2020, 6, 141
- **Summary**: This research approaches the task of handwritten text with attention encoder-decoder networks that are trained on Kazakh and Russian language. We developed a novel deep neural network model based on Fully Gated CNN, supported by Multiple bidirectional GRU and Attention mechanisms to manipulate sophisticated features that achieve 0.045 Character Error Rate (CER), 0.192 Word Error Rate (WER) and 0.253 Sequence Error Rate (SER) for the first test dataset and 0.064 CER, 0.24 WER and 0.361 SER for the second test dataset. Also, we propose fully gated layers by taking the advantage of multiple the output feature from Tahn and input feature, this proposed work achieves better results and We experimented with our model on the Handwritten Kazakh & Russian Database (HKR). Our research is the first work on the HKR dataset and demonstrates state-of-the-art results to most of the other existing models.



### Improving the Performance of Fine-Grain Image Classifiers via Generative Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.05381v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05381v1)
- **Published**: 2020-08-12 15:29:11+00:00
- **Updated**: 2020-08-12 15:29:11+00:00
- **Authors**: Shashank Manjunath, Aitzaz Nathaniel, Jeff Druce, Stan German
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in machine learning (ML) and computer vision tools have enabled applications in a wide variety of arenas such as financial analytics, medical diagnostics, and even within the Department of Defense. However, their widespread implementation in real-world use cases poses several challenges: (1) many applications are highly specialized, and hence operate in a \emph{sparse data} domain; (2) ML tools are sensitive to their training sets and typically require cumbersome, labor-intensive data collection and data labelling processes; and (3) ML tools can be extremely "black box," offering users little to no insight into the decision-making process or how new data might affect prediction performance. To address these challenges, we have designed and developed Data Augmentation from Proficient Pre-Training of Robust Generative Adversarial Networks (DAPPER GAN), an ML analytics support tool that automatically generates novel views of training images in order to improve downstream classifier performance. DAPPER GAN leverages high-fidelity embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to create novel imagery for previously unseen classes. We experimentally evaluate this technique on the Stanford Cars dataset, demonstrating improved vehicle make and model classification accuracy and reduced requirements for real data using our GAN based data augmentation framework. The method's validity was supported through an analysis of classifier performance on both augmented and non-augmented datasets, achieving comparable or better accuracy with up to 30\% less real data across visually similar classes. To support this method, we developed a novel augmentation method that can manipulate semantically meaningful dimensions (e.g., orientation) of the target object in the embedding space.



### Towards Unsupervised Crowd Counting via Regression-Detection Bi-knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2008.05383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05383v2)
- **Published**: 2020-08-12 15:29:30+00:00
- **Updated**: 2020-09-27 14:36:33+00:00
- **Authors**: Yuting Liu, Zheng Wang, Miaojing Shi, Shin'ichi Satoh, Qijun Zhao, Hongyu Yang
- **Comment**: This paper has been accepted by ACM MM 2020(Oral)
- **Journal**: None
- **Summary**: Unsupervised crowd counting is a challenging yet not largely explored task. In this paper, we explore it in a transfer learning setting where we learn to detect and count persons in an unlabeled target set by transferring bi-knowledge learnt from regression- and detection-based models in a labeled source set. The dual source knowledge of the two models is heterogeneous and complementary as they capture different modalities of the crowd distribution. We formulate the mutual transformations between the outputs of regression- and detection-based models as two scene-agnostic transformers which enable knowledge distillation between the two models. Given the regression- and detection-based models and their mutual transformers learnt in the source, we introduce an iterative self-supervised learning scheme with regression-detection bi-knowledge transfer in the target. Extensive experiments on standard crowd counting benchmarks, ShanghaiTech, UCF\_CC\_50, and UCF\_QNRF demonstrate a substantial improvement of our method over other state-of-the-arts in the transfer learning setting.



### DAWN: Vehicle Detection in Adverse Weather Nature Dataset
- **Arxiv ID**: http://arxiv.org/abs/2008.05402v1
- **DOI**: 10.17632/766ygrbt8y.3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05402v1)
- **Published**: 2020-08-12 15:48:49+00:00
- **Updated**: 2020-08-12 15:48:49+00:00
- **Authors**: Mourad A. Kenk, Mahmoud Hassaballah
- **Comment**: Available at https://data.mendeley.com/datasets/766ygrbt8y/3 ,IEEE
  Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Recently, self-driving vehicles have been introduced with several automated features including lane-keep assistance, queuing assistance in traffic-jam, parking assistance and crash avoidance. These self-driving vehicles and intelligent visual traffic surveillance systems mainly depend on cameras and sensors fusion systems. Adverse weather conditions such as heavy fog, rain, snow, and sandstorms are considered dangerous restrictions of the functionality of cameras impacting seriously the performance of adopted computer vision algorithms for scene understanding (i.e., vehicle detection, tracking, and recognition in traffic scenes). For example, reflection coming from rain flow and ice over roads could cause massive detection errors which will affect the performance of intelligent visual traffic systems. Additionally, scene understanding and vehicle detection algorithms are mostly evaluated using datasets contain certain types of synthetic images plus a few real-world images. Thus, it is uncertain how these algorithms would perform on unclear images acquired in the wild and how the progress of these algorithms is standardized in the field. To this end, we present a new dataset (benchmark) consisting of real-world images collected under various adverse weather conditions called DAWN. This dataset emphasizes a diverse traffic environment (urban, highway and freeway) as well as a rich variety of traffic flow. The DAWN dataset comprises a collection of 1000 images from real-traffic environments, which are divided into four sets of weather conditions: fog, snow, rain and sandstorms. The dataset is annotated with object bounding boxes for autonomous driving and video surveillance scenarios. This data helps interpreting effects caused by the adverse weather conditions on the performance of vehicle detection systems.



### Look here! A parametric learning based approach to redirect visual attention
- **Arxiv ID**: http://arxiv.org/abs/2008.05413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05413v1)
- **Published**: 2020-08-12 16:08:36+00:00
- **Updated**: 2020-08-12 16:08:36+00:00
- **Authors**: Youssef Alami Mejjati, Celso F. Gomez, Kwang In Kim, Eli Shechtman, Zoya Bylinskii
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Across photography, marketing, and website design, being able to direct the viewer's attention is a powerful tool. Motivated by professional workflows, we introduce an automatic method to make an image region more attention-capturing via subtle image edits that maintain realism and fidelity to the original. From an input image and a user-provided mask, our GazeShiftNet model predicts a distinct set of global parametric transformations to be applied to the foreground and background image regions separately. We present the results of quantitative and qualitative experiments that demonstrate improvements over prior state-of-the-art. In contrast to existing attention shifting algorithms, our global parametric approach better preserves image semantics and avoids typical generative artifacts. Our edits enable inference at interactive rates on any image size, and easily generalize to videos. Extensions of our model allow for multi-style edits and the ability to both increase and attenuate attention in an image region. Furthermore, users can customize the edited images by dialing the edits up or down via interpolations in parameter space. This paper presents a practical tool that can simplify future image editing pipelines.



### DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features
- **Arxiv ID**: http://arxiv.org/abs/2008.05416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.05416v1)
- **Published**: 2020-08-12 16:14:46+00:00
- **Updated**: 2020-08-12 16:14:46+00:00
- **Authors**: Dongjiang Li, Xuesong Shi, Qiwei Long, Shenghui Liu, Wei Yang, Fangshi Wang, Qi Wei, Fei Qiao
- **Comment**: 8 pages, 5 figures, to be published in IROS 2020
- **Journal**: None
- **Summary**: A robust and efficient Simultaneous Localization and Mapping (SLAM) system is essential for robot autonomy. For visual SLAM algorithms, though the theoretical framework has been well established for most aspects, feature extraction and association is still empirically designed in most cases, and can be vulnerable in complex environments. This paper shows that feature extraction with deep convolutional neural networks (CNNs) can be seamlessly incorporated into a modern SLAM framework. The proposed SLAM system utilizes a state-of-the-art CNN to detect keypoints in each image frame, and to give not only keypoint descriptors, but also a global descriptor of the whole image. These local and global features are then used by different SLAM modules, resulting in much more robustness against environmental changes and viewpoint changes compared with using hand-crafted features. We also train a visual vocabulary of local features with a Bag of Words (BoW) method. Based on the local features, global features, and the vocabulary, a highly reliable loop closure detection method is built. Experimental results show that all the proposed modules significantly outperforms the baseline, and the full system achieves much lower trajectory errors and much higher correct rates on all evaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit and utilizing the Fast BoW library, the system benefits greatly from the SIMD (single-instruction-multiple-data) techniques in modern CPUs. The full system can run in real-time without any GPU or other accelerators. The code is public at https://github.com/ivipsourcecode/dxslam.



### Automatic assembly of aero engine low pressure turbine shaft based on 3D vision measurement
- **Arxiv ID**: http://arxiv.org/abs/2008.04903v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04903v1)
- **Published**: 2020-08-12 17:06:39+00:00
- **Updated**: 2020-08-12 17:06:39+00:00
- **Authors**: Jiaxiang Wang, Kunyong Chen
- **Comment**: 5pages,12figures
- **Journal**: None
- **Summary**: In order to solve the problem of low automation of Aero-engine Turbine shaft assembly and the difficulty of non-contact high-precision measurement, a structured light binocular measurement technology for key components of aero-engine is proposed in this paper. Combined with three-dimensional point cloud data processing and assembly position matching algorithm, the high-precision measurement of shaft hole assembly posture in the process of turbine shaft docking is realized. Firstly, the screw thread curve on the bolt surface is segmented based on PCA projection and edge point cloud clustering, and Hough transform is used to model fit the three-dimensional thread curve. Then the preprocessed two-dimensional convex hull is constructed to segment the key hole location features, and the mounting surface and hole location obtained by segmentation are fitted based on RANSAC method. Finally, the geometric feature matching is used the evaluation index of turbine shaft assembly is established to optimize the pose. The final measurement accuracy of mounting surface matching is less than 0.05mm, and the measurement accuracy of mounting hole matching based on minimum ance optimization is less than 0.1 degree. The measurement algorithm is implemented on the automatic assembly test-bed of a certain type of aero-engine low-pressure turbine rotor. In the narrow installation space, the assembly process of the turbine shaft assembly, such as the automatic alignment and docking of the shaft hole, the automatic heating and temperature measurement of the installation seam, and the automatic tightening of the two guns, are realized in the narrow installation space Guidance, real-time inspection and assembly result evaluation.



### DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.05440v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05440v4)
- **Published**: 2020-08-12 17:06:51+00:00
- **Updated**: 2022-05-28 17:40:15+00:00
- **Authors**: Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, Lin Gao
- **Comment**: Accept to ACM Transaction on Graphics 2022, 26 pages
- **Journal**: None
- **Summary**: D shape generation is a fundamental operation in computer graphics. While significant progress has been made, especially with recent deep generative models, it remains a challenge to synthesize high-quality shapes with rich geometric details and complex structure, in a controllable manner. To tackle this, we introduce DSG-Net, a deep neural network that learns a disentangled structured and geometric mesh representation for 3D shapes, where two key aspects of shapes, geometry, and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with disentangled control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged. To achieve this, we simultaneously learn structure and geometry through variational autoencoders (VAEs) in a hierarchical manner for both, with bijective mappings at each level. In this manner, we effectively encode geometry and structure in separate latent spaces, while ensuring their compatibility: the structure is used to guide the geometry and vice versa. At the leaf level, the part geometry is represented using a conditional part VAE, to encode high-quality geometric details, guided by the structure context as the condition. Our method not only supports controllable generation applications but also produces high-quality synthesized shapes, outperforming state-of-the-art methods. The code has been released at https://github.com/IGLICT/DSG-Net.



### Stable Low-rank Tensor Decomposition for Compression of Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.05441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05441v1)
- **Published**: 2020-08-12 17:10:12+00:00
- **Updated**: 2020-08-12 17:10:12+00:00
- **Authors**: Anh-Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov, Julia Gusak, Petr Tichavsky, Valeriy Glukhov, Ivan Oseledets, Andrzej Cichocki
- **Comment**: This paper is accepted to ECCV2020
- **Journal**: None
- **Summary**: Most state of the art deep neural networks are overparameterized and exhibit a high computational cost. A straightforward approach to this problem is to replace convolutional kernels with its low-rank tensor approximations, whereas the Canonical Polyadic tensor Decomposition is one of the most suited models. However, fitting the convolutional tensors by numerical optimization algorithms often encounters diverging components, i.e., extremely large rank-one tensors but canceling each other. Such degeneracy often causes the non-interpretable result and numerical instability for the neural network fine-tuning. This paper is the first study on degeneracy in the tensor decomposition of convolutional kernels. We present a novel method, which can stabilize the low-rank approximation of convolutional kernels and ensure efficient compression while preserving the high-quality performance of the neural networks. We evaluate our approach on popular CNN architectures for image classification and show that our method results in much lower accuracy degradation and provides consistent performance.



### More Diverse Means Better: Multimodal Deep Learning Meets Remote Sensing Imagery Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.05457v1
- **DOI**: 10.1109/TGRS.2020.3016820
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05457v1)
- **Published**: 2020-08-12 17:45:25+00:00
- **Updated**: 2020-08-12 17:45:25+00:00
- **Authors**: Danfeng Hong, Lianru Gao, Naoto Yokoya, Jing Yao, Jocelyn Chanussot, Qian Du, Bing Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Summary**: Classification and identification of the materials lying over or beneath the Earth's surface have long been a fundamental but challenging research topic in geoscience and remote sensing (RS) and have garnered a growing concern owing to the recent advancements of deep learning techniques. Although deep networks have been successfully applied in single-modality-dominated classification tasks, yet their performance inevitably meets the bottleneck in complex scenes that need to be finely classified, due to the limitation of information diversity. In this work, we provide a baseline solution to the aforementioned difficulty by developing a general multimodal deep learning (MDL) framework. In particular, we also investigate a special case of multi-modality learning (MML) -- cross-modality learning (CML) that exists widely in RS image classification applications. By focusing on "what", "where", and "how" to fuse, we show different fusion strategies as well as how to train deep networks and build the network architecture. Specifically, five fusion architectures are introduced and developed, further being unified in our MDL framework. More significantly, our framework is not only limited to pixel-wise classification tasks but also applicable to spatial information modeling with convolutional neural networks (CNNs). To validate the effectiveness and superiority of the MDL framework, extensive experiments related to the settings of MML and CML are conducted on two different multimodal RS datasets. Furthermore, the codes and datasets will be available at https://github.com/danfenghong/IEEE_TGRS_MDL-RS, contributing to the RS community.



### Multi-level Stress Assessment Using Multi-domain Fusion of ECG Signal
- **Arxiv ID**: http://arxiv.org/abs/2008.05503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05503v1)
- **Published**: 2020-08-12 18:08:35+00:00
- **Updated**: 2020-08-12 18:08:35+00:00
- **Authors**: Zeeshan Ahmad, Naimul Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Stress analysis and assessment of affective states of mind using ECG as a physiological signal is a burning research topic in biomedical signal processing. However, existing literature provides only binary assessment of stress, while multiple levels of assessment may be more beneficial for healthcare applications. Furthermore, in present research, ECG signal for stress analysis is examined independently in spatial domain or in transform domains but the advantage of fusing these domains has not been fully utilized. To get the maximum advantage of fusing diferent domains, we introduce a dataset with multiple stress levels and then classify these levels using a novel deep learning approach by converting ECG signal into signal images based on R-R peaks without any feature extraction. Moreover, We made signal images multimodal and multidomain by converting them into time-frequency and frequency domain using Gabor wavelet transform (GWT) and Discrete Fourier Transform (DFT) respectively. Convolutional Neural networks (CNNs) are used to extract features from different modalities and then decision level fusion is performed for improving the classification accuracy. The experimental results on an in-house dataset collected with 15 users show that with proposed fusion framework and using ECG signal to image conversion, we reach an average accuracy of 85.45%.



### Free View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.05511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05511v1)
- **Published**: 2020-08-12 18:16:08+00:00
- **Updated**: 2020-08-12 18:16:08+00:00
- **Authors**: Gernot Riegler, Vladlen Koltun
- **Comment**: published at ECCV 2020, https://youtu.be/JDJPn3ZtfZs
- **Journal**: None
- **Summary**: We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.



### Mitigating Dataset Imbalance via Joint Generation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.05524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05524v1)
- **Published**: 2020-08-12 18:40:38+00:00
- **Updated**: 2020-08-12 18:40:38+00:00
- **Authors**: Aadarsh Sahoo, Ankit Singh, Rameswar Panda, Rogerio Feris, Abir Das
- **Comment**: Accepted in ECCV2020 Workshop on Imbalance Problems in Computer
  Vision (IPCV)
- **Journal**: None
- **Summary**: Supervised deep learning methods are enjoying enormous success in many practical applications of computer vision and have the potential to revolutionize robotics. However, the marked performance degradation to biases and imbalanced data questions the reliability of these methods. In this work we address these questions from the perspective of dataset imbalance resulting out of severe under-representation of annotated training data for certain classes and its effect on both deep classification and generation methods. We introduce a joint dataset repairment strategy by combining a neural network classifier with Generative Adversarial Networks (GAN) that makes up for the deficit of training examples from the under-representated class by producing additional training examples. We show that the combined training helps to improve the robustness of both the classifier and the GAN against severe class imbalance. We show the effectiveness of our proposed approach on three very different datasets with different degrees of imbalance in them. The code is available at https://github.com/AadSah/ImbalanceCycleGAN .



### Polyth-Net: Classification of Polythene Bags for Garbage Segregation Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.07592v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07592v4)
- **Published**: 2020-08-12 19:00:56+00:00
- **Updated**: 2021-01-23 11:23:16+00:00
- **Authors**: Divyansh Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Polythene has always been a threat to the environment since its invention. It is non-biodegradable and very difficult to recycle. Even after many awareness campaigns and practices, Separation of polythene bags from waste has been a challenge for human civilization. The primary method of segregation deployed is manual handpicking, which causes a dangerous health hazards to the workers and is also highly inefficient due to human errors. In this paper I have designed and researched on image-based classification of polythene bags using a deep-learning model and its efficiency. This paper focuses on the architecture and statistical analysis of its performance on the data set as well as problems experienced in the classification. It also suggests a modified loss function to specifically detect polythene irrespective of its individual features. It aims to help the current environment protection endeavours and save countless lives lost to the hazards caused by current methods.



### Co-training for On-board Deep Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.05534v1
- **DOI**: 10.1109/ACCESS.2020.3032024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05534v1)
- **Published**: 2020-08-12 19:08:59+00:00
- **Updated**: 2020-08-12 19:08:59+00:00
- **Authors**: Gabriel Villalonga, Antonio M. Lopez
- **Comment**: None
- **Journal**: IEEE Access 8 (2020), 194441-194456
- **Summary**: Providing ground truth supervision to train visual models has been a bottleneck over the years, exacerbated by domain shifts which degenerate the performance of such models. This was the case when visual tasks relied on handcrafted features and shallow machine learning and, despite its unprecedented performance gains, the problem remains open within the deep learning paradigm due to its data-hungry nature. Best performing deep vision-based object detectors are trained in a supervised manner by relying on human-labeled bounding boxes which localize class instances (i.e.objects) within the training images.Thus, object detection is one of such tasks for which human labeling is a major bottleneck. In this paper, we assess co-training as a semi-supervised learning method for self-labeling objects in unlabeled images, so reducing the human-labeling effort for developing deep object detectors. Our study pays special attention to a scenario involving domain shift; in particular, when we have automatically generated virtual-world images with object bounding boxes and we have real-world images which are unlabeled. Moreover, we are particularly interested in using co-training for deep object detection in the context of driver assistance systems and/or self-driving vehicles. Thus, using well-established datasets and protocols for object detection in these application contexts, we will show how co-training is a paradigm worth to pursue for alleviating object labeling, working both alone and together with task-agnostic domain adaptation.



### Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.07588v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.07588v2)
- **Published**: 2020-08-12 20:08:04+00:00
- **Updated**: 2021-08-10 16:17:56+00:00
- **Authors**: Abhinav Sagar
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning motivated by convolutional neural networks has been highly successful in a range of medical imaging problems like image classification, image segmentation, image synthesis etc. However for validation and interpretability, not only do we need the predictions made by the model but also how confident it is while making those predictions. This is important in safety critical applications for the people to accept it. In this work, we used an encoder decoder architecture based on variational inference techniques for segmenting brain tumour images. We evaluate our work on the publicly available BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) as the evaluation metrics. Our model is able to segment brain tumours while taking into account both aleatoric uncertainty and epistemic uncertainty in a principled bayesian manner.



### Continual Class Incremental Learning for CT Thoracic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.05557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05557v1)
- **Published**: 2020-08-12 20:08:39+00:00
- **Updated**: 2020-08-12 20:08:39+00:00
- **Authors**: Abdelrahman Elskhawy, Aneta Lisowska, Matthias Keicher, Josep Henry, Paul Thomson, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning organ segmentation approaches require large amounts of annotated training data, which is limited in supply due to reasons of confidentiality and the time required for expert manual annotation. Therefore, being able to train models incrementally without having access to previously used data is desirable. A common form of sequential training is fine tuning (FT). In this setting, a model learns a new task effectively, but loses performance on previously learned tasks. The Learning without Forgetting (LwF) approach addresses this issue via replaying its own prediction for past tasks during model training. In this work, we evaluate FT and LwF for class incremental learning in multi-organ segmentation using the publicly available AAPM dataset. We show that LwF can successfully retain knowledge on previous segmentations, however, its ability to learn a new class decreases with the addition of each class. To address this problem we propose an adversarial continual learning segmentation approach (ACLSeg), which disentangles feature space into task-specific and task-invariant features. This enables preservation of performance on past tasks and effective acquisition of new knowledge.



### Generate High Resolution Images With Generative Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2008.10399v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10399v3)
- **Published**: 2020-08-12 20:15:34+00:00
- **Updated**: 2021-06-21 18:15:20+00:00
- **Authors**: Abhinav Sagar
- **Comment**: The network architecture used in this paper while training the model
  is not correct
- **Journal**: None
- **Summary**: In this work, we present a novel neural network to generate high resolution images. We replace the decoder of VAE with a discriminator while using the encoder as it is. The encoder is fed data from a normal distribution while the generator is fed from a gaussian distribution. The combination from both is given to a discriminator which tells whether the generated image is correct or not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA dataset. Our network beats the previous state of the art using MMD, SSIM, log likelihood, reconstruction error, ELBO and KL divergence as the evaluation metrics while generating much sharper images. This work is potentially very exciting as we are able to combine the advantages of generative models and inference models in a principled bayesian manner.



### Facial Expression Recognition Under Partial Occlusion from Virtual Reality Headsets based on Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.05563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05563v1)
- **Published**: 2020-08-12 20:25:07+00:00
- **Updated**: 2020-08-12 20:25:07+00:00
- **Authors**: Bita Houshmand, Naimul Khan
- **Comment**: To be presented at the IEEE BigMM 2020
- **Journal**: None
- **Summary**: Facial expressions of emotion are a major channel in our daily communications, and it has been subject of intense research in recent years. To automatically infer facial expressions, convolutional neural network based approaches has become widely adopted due to their proven applicability to Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR) has gained popularity as an immersive multimedia platform, where FER can provide enriched media experiences. However, recognizing facial expression while wearing a head-mounted VR headset is a challenging task due to the upper half of the face being completely occluded. In this paper we attempt to overcome these issues and focus on facial expression recognition in presence of a severe occlusion where the user is wearing a head-mounted display in a VR setting. We propose a geometric model to simulate occlusion resulting from a Samsung Gear VR headset that can be applied to existing FER datasets. Then, we adopt a transfer learning approach, starting from two pretrained networks, namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB datasets. Experimental results show that our approach achieves comparable results to existing methods while training on three modified benchmark datasets that adhere to realistic occlusion resulting from wearing a commodity VR headset. Code for this paper is available at: https://github.com/bita-github/MRP-FER



### Procedural Urban Forestry
- **Arxiv ID**: http://arxiv.org/abs/2008.05567v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05567v2)
- **Published**: 2020-08-12 20:44:56+00:00
- **Updated**: 2020-08-14 00:35:44+00:00
- **Authors**: Till Niese, Sören Pirk, Matthias Albrecht, Bedrich Benes, Oliver Deussen
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: The placement of vegetation plays a central role in the realism of virtual scenes. We introduce procedural placement models (PPMs) for vegetation in urban layouts. PPMs are environmentally sensitive to city geometry and allow identifying plausible plant positions based on structural and functional zones in an urban layout. PPMs can either be directly used by defining their parameters or can be learned from satellite images and land register data. Together with approaches for generating buildings and trees, this allows us to populate urban landscapes with complex 3D vegetation. The effectiveness of our framework is shown through examples of large-scale city scenes and close-ups of individually grown tree models; we also validate it by a perceptual user study.



### PLACE: Proximity Learning of Articulation and Contact in 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2008.05570v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05570v4)
- **Published**: 2020-08-12 21:00:10+00:00
- **Updated**: 2020-11-12 16:21:17+00:00
- **Authors**: Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, Siyu Tang
- **Comment**: Accepted by 3DV 2020, camera ready version with appendix
- **Journal**: None
- **Summary**: High fidelity digital 3D environments have been proposed in recent years, however, it remains extremely challenging to automatically equip such environment with realistic human bodies. Existing work utilizes images, depth or semantic maps to represent the scene, and parametric human models to represent 3D bodies. While being straightforward, their generated human-scene interactions are often lack of naturalness and physical plausibility. Our key observation is that humans interact with the world through body-scene contact. To synthesize realistic human-scene interactions, it is essential to effectively represent the physical contact and proximity between the body and the world. To that end, we propose a novel interaction generation method, named PLACE (Proximity Learning of Articulation and Contact in 3D Environments), which explicitly models the proximity between the human body and the 3D scene around it. Specifically, given a set of basis points on a scene mesh, we leverage a conditional variational autoencoder to synthesize the minimum distances from the basis points to the human body surface. The generated proximal relationship exhibits which region of the scene is in contact with the person. Furthermore, based on such synthesized proximity, we are able to effectively obtain expressive 3D human bodies that interact with the 3D scene naturally. Our perceptual study shows that PLACE significantly improves the state-of-the-art method, approaching the realism of real human-scene interaction. We believe our method makes an important step towards the fully automatic synthesis of realistic 3D human bodies in 3D scenes. The code and model are available for research at https://sanweiliti.github.io/PLACE/PLACE.html.



### Self-Path: Self-supervision for Classification of Pathology Images with Limited Annotations
- **Arxiv ID**: http://arxiv.org/abs/2008.05571v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05571v1)
- **Published**: 2020-08-12 21:02:32+00:00
- **Updated**: 2020-08-12 21:02:32+00:00
- **Authors**: Navid Alemi Koohbanani, Balagopal Unnikrishnan, Syed Ali Khurram, Pavitra Krishnaswamy, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: While high-resolution pathology images lend themselves well to `data hungry' deep learning algorithms, obtaining exhaustive annotations on these images is a major challenge. In this paper, we propose a self-supervised CNN approach to leverage unlabeled data for learning generalizable and domain invariant representations in pathology images. The proposed approach, which we term as Self-Path, is a multi-task learning approach where the main task is tissue classification and pretext tasks are a variety of self-supervised tasks with labels inherent to the input data. We introduce novel domain specific self-supervision tasks that leverage contextual, multi-resolution and semantic features in pathology images for semi-supervised learning and domain adaptation. We investigate the effectiveness of Self-Path on 3 different pathology datasets. Our results show that Self-Path with the domain-specific pretext tasks achieves state-of-the-art performance for semi-supervised learning when small amounts of labeled data are available. Further, we show that Self-Path improves domain adaptation for classification of histology image patches when there is no labeled data available for the target domain. This approach can potentially be employed for other applications in computational pathology, where annotation budget is often limited or large amount of unlabeled image data is available.



### We Have So Much In Common: Modeling Semantic Relational Set Abstractions in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.05596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05596v1)
- **Published**: 2020-08-12 22:57:44+00:00
- **Updated**: 2020-08-12 22:57:44+00:00
- **Authors**: Alex Andonian, Camilo Fosco, Mathew Monfort, Allen Lee, Rogerio Feris, Carl Vondrick, Aude Oliva
- **Comment**: European Conference on Computer Vision (ECCV) 2020, accepted
- **Journal**: None
- **Summary**: Identifying common patterns among events is a key ability in human and machine perception, as it underlies intelligent decision making. We propose an approach for learning semantic relational set abstractions on videos, inspired by human learning. We combine visual features with natural language supervision to generate high-level representations of similarities across a set of videos. This allows our model to perform cognitive tasks such as set abstraction (which general concept is in common among a set of videos?), set completion (which new video goes well with the set?), and odd one out detection (which video does not belong to the set?). Experiments on two video benchmarks, Kinetics and Multi-Moments in Time, show that robust and versatile representations emerge when learning to recognize commonalities among sets. We compare our model to several baseline algorithms and show that significant improvements result from explicitly learning relational abstractions with semantic supervision.



