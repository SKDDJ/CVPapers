# Arxiv Papers in cs.CV on 2020-08-24
### Bayesian Geodesic Regression on Riemannian Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2009.05108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05108v2)
- **Published**: 2020-08-24 00:42:58+00:00
- **Updated**: 2020-09-15 13:34:35+00:00
- **Authors**: Youshan Zhang
- **Comment**: BMVC 2020
- **Journal**: None
- **Summary**: Geodesic regression has been proposed for fitting the geodesic curve. However, it cannot automatically choose the dimensionality of data. In this paper, we develop a Bayesian geodesic regression model on Riemannian manifolds (BGRM) model. To avoid the overfitting problem, we add a regularization term to control the effectiveness of the model. To automatically select the dimensionality, we develop a prior for the geodesic regression model, which can automatically select the number of relevant dimensions by driving unnecessary tangent vectors to zero. To show the validation of our model, we first apply it in the 3D synthetic sphere and 2D pentagon data. We then demonstrate the effectiveness of our model in reducing the dimensionality and analyzing shape variations of human corpus callosum and mandible data.



### Hierarchical Style-based Networks for Motion Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.10162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10162v1)
- **Published**: 2020-08-24 02:11:02+00:00
- **Updated**: 2020-08-24 02:11:02+00:00
- **Authors**: Jingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Xiaolong Wang, Trevor Darrell
- **Comment**: ECCV 2020, Project Page:\<https://sites.google.com/view/hsnms>
- **Journal**: None
- **Summary**: Generating diverse and natural human motion is one of the long-standing goals for creating intelligent characters in the animated world. In this paper, we propose a self-supervised method for generating long-range, diverse and plausible behaviors to achieve a specific goal location. Our proposed method learns to model the motion of human by decomposing a long-range generation task in a hierarchical manner. Given the starting and ending states, a memory bank is used to retrieve motion references as source material for short-range clip generation. We first propose to explicitly disentangle the provided motion material into style and content counterparts via bi-linear transformation modelling, where diverse synthesis is achieved by free-form combination of these two components. The short-range clips are then connected to form a long-range motion sequence. Without ground truth annotation, we propose a parameterized bi-directional interpolation scheme to guarantee the physical validity and visual naturalness of generated results. On large-scale skeleton dataset, we show that the proposed method is able to synthesise long-range, diverse and plausible motion, which is also generalizable to unseen motion data during testing. Moreover, we demonstrate the generated sequences are useful as subgoals for actual physical execution in the animated world.



### Learning Kernel for Conditional Moment-Matching Discrepancy-based Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.10165v1
- **DOI**: 10.1109/TCYB.2019.2916198
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10165v1)
- **Published**: 2020-08-24 02:35:50+00:00
- **Updated**: 2020-08-24 02:35:50+00:00
- **Authors**: Chuan-Xian Ren, Pengfei Ge, Dao-Qing Dai, Hong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional Maximum Mean Discrepancy (CMMD) can capture the discrepancy between conditional distributions by drawing support from nonlinear kernel functions, thus it has been successfully used for pattern classification. However, CMMD does not work well on complex distributions, especially when the kernel function fails to correctly characterize the difference between intra-class similarity and inter-class similarity. In this paper, a new kernel learning method is proposed to improve the discrimination performance of CMMD. It can be operated with deep network features iteratively and thus denoted as KLN for abbreviation. The CMMD loss and an auto-encoder (AE) are used to learn an injective function. By considering the compound kernel, i.e., the injective function with a characteristic kernel, the effectiveness of CMMD for data category description is enhanced. KLN can simultaneously learn a more expressive kernel and label prediction distribution, thus, it can be used to improve the classification performance in both supervised and semi-supervised learning scenarios. In particular, the kernel-based similarities are iteratively learned on the deep network features, and the algorithm can be implemented in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets, including MNIST, SVHN, CIFAR-10 and CIFAR-100. The results indicate that KLN achieves state-of-the-art classification performance.



### Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars
- **Arxiv ID**: http://arxiv.org/abs/2008.10174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10174v1)
- **Published**: 2020-08-24 03:23:59+00:00
- **Updated**: 2020-08-24 03:23:59+00:00
- **Authors**: Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.



### HALO: Learning to Prune Neural Networks with Shrinkage
- **Arxiv ID**: http://arxiv.org/abs/2008.10183v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.10183v3)
- **Published**: 2020-08-24 04:08:48+00:00
- **Updated**: 2021-02-28 04:26:09+00:00
- **Authors**: Skyler Seto, Martin T. Wells, Wenyu Zhang
- **Comment**: Accepted at SDM 2021
- **Journal**: None
- **Summary**: Deep neural networks achieve state-of-the-art performance in a variety of tasks by extracting a rich set of features from unstructured data, however this performance is closely tied to model size. Modern techniques for inducing sparsity and reducing model size are (1) network pruning, (2) training with a sparsity inducing penalty, and (3) training a binary mask jointly with the weights of the network. We study different sparsity inducing penalties from the perspective of Bayesian hierarchical models and present a novel penalty called Hierarchical Adaptive Lasso (HALO) which learns to adaptively sparsify weights of a given network via trainable parameters. When used to train over-parametrized networks, our penalty yields small subnetworks with high accuracy without fine-tuning. Empirically, on image recognition tasks, we find that HALO is able to learn highly sparse network (only 5% of the parameters) with significant gains in performance over state-of-the-art magnitude pruning methods at the same level of sparsity. Code is available at https://github.com/skyler120/sparsity-halo.



### Affinity-aware Compression and Expansion Network for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2008.10191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10191v1)
- **Published**: 2020-08-24 05:16:08+00:00
- **Updated**: 2020-08-24 05:16:08+00:00
- **Authors**: Xinyan Zhang, Yunfeng Wang, Pengfei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: As a fine-grained segmentation task, human parsing is still faced with two challenges: inter-part indistinction and intra-part inconsistency, due to the ambiguous definitions and confusing relationships between similar human parts. To tackle these two problems, this paper proposes a novel \textit{Affinity-aware Compression and Expansion} Network (ACENet), which mainly consists of two modules: Local Compression Module (LCM) and Global Expansion Module (GEM). Specifically, LCM compresses parts-correlation information through structural skeleton points, obtained from an extra skeleton branch. It can decrease the inter-part interference, and strengthen structural relationships between ambiguous parts. Furthermore, GEM expands semantic information of each part into a complete piece by incorporating the spatial affinity with boundary guidance, which can effectively enhance the semantic consistency of intra-part as well. ACENet achieves new state-of-the-art performance on the challenging LIP and Pascal-Person-Part datasets. In particular, 58.1% mean IoU is achieved on the LIP benchmark.



### 1st Place Solution to Google Landmark Retrieval 2020
- **Arxiv ID**: http://arxiv.org/abs/2009.05132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05132v1)
- **Published**: 2020-08-24 05:45:20+00:00
- **Updated**: 2020-08-24 05:45:20+00:00
- **Authors**: SeungKee Jeon
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: This paper presents the 1st place solution to the Google Landmark Retrieval 2020 Competition on Kaggle. The solution is based on metric learning to classify numerous landmark classes, and uses transfer learning with two train datasets, fine-tuning on bigger images, adjusting loss weight for cleaner samples, and esemble to enhance the model's performance further. Finally, it scored 0.38677 mAP@100 on the private leaderboard.



### Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency
- **Arxiv ID**: http://arxiv.org/abs/2008.10208v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.5.3; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.10208v2)
- **Published**: 2020-08-24 06:11:29+00:00
- **Updated**: 2021-07-03 10:02:51+00:00
- **Authors**: Youwei Liang, Dong Huang, Chang-Dong Wang, Philip S. Yu
- **Comment**: Preprint, under review
- **Journal**: None
- **Summary**: Graph learning has emerged as a promising technique for multi-view clustering with its ability to learn a unified and robust graph from multiple views. However, existing graph learning methods mostly focus on the multi-view consistency issue, yet often neglect the inconsistency across multiple views, which makes them vulnerable to possibly low-quality or noisy datasets. To overcome this limitation, we propose a new multi-view graph learning framework, which for the first time simultaneously and explicitly models multi-view consistency and multi-view inconsistency in a unified objective function, through which the consistent and inconsistent parts of each single-view graph as well as the unified graph that fuses the consistent parts can be iteratively learned. Though optimizing the objective function is NP-hard, we design a highly efficient optimization algorithm which is able to obtain an approximate solution with linear time complexity in the number of edges in the unified graph. Furthermore, our multi-view graph learning approach can be applied to both similarity graphs and dissimilarity graphs, which lead to two graph fusion-based variants in our framework. Experiments on twelve multi-view datasets have demonstrated the robustness and efficiency of the proposed approach.



### Strawberry Detection using Mixed Training on Simulated and Real Data
- **Arxiv ID**: http://arxiv.org/abs/2008.10236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10236v1)
- **Published**: 2020-08-24 07:37:12+00:00
- **Updated**: 2020-08-24 07:37:12+00:00
- **Authors**: Sunny Goondram, Akansel Cosgun, Dana Kulic
- **Comment**: DICTA 2020 Short Paper Track
- **Journal**: None
- **Summary**: This paper demonstrates how simulated images can be useful for object detection tasks in the agricultural sector, where labeled data can be scarce and costly to collect. We consider training on mixed datasets with real and simulated data for strawberry detection in real images. Our results show that using the real dataset augmented by the simulated dataset resulted in slightly higher accuracy.



### VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.10238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10238v1)
- **Published**: 2020-08-24 07:54:59+00:00
- **Updated**: 2020-08-24 07:54:59+00:00
- **Authors**: Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, Chang D. Yoo
- **Comment**: 16 pages, 6 figures, European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: Video Moment Retrieval (VMR) is a task to localize the temporal moment in untrimmed video specified by natural language query. For VMR, several methods that require full supervision for training have been proposed. Unfortunately, acquiring a large number of training videos with labeled temporal boundaries for each query is a labor-intensive process. This paper explores methods for performing VMR in a weakly-supervised manner (wVMR): training is performed without temporal moment labels but only with the text query that describes a segment of the video. Existing methods on wVMR generate multi-scale proposals and apply query-guided attention mechanisms to highlight the most relevant proposal. To leverage the weak supervision, contrastive learning is used which predicts higher scores for the correct video-query pairs than for the incorrect pairs. It has been observed that a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention maps which limit the localization performance. To handle this issue, Video-Language Alignment Network (VLANet) is proposed that learns sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, and thus substantially reduces candidate proposals which leads to lower computation load and sharper attention. Next, the Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flow to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss which enforces semantically similar videos and queries to gather. The experiments show that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.



### A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research
- **Arxiv ID**: http://arxiv.org/abs/2008.10244v2
- **DOI**: 10.1038/s41597-020-00756-z
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.10244v2)
- **Published**: 2020-08-24 08:06:55+00:00
- **Updated**: 2020-11-27 11:49:32+00:00
- **Authors**: Marc Aubreville, Christof A. Bertram, Taryn A. Donovan, Christian Marzahl, Andreas Maier, Robert Klopfleisch
- **Comment**: 12 pages, 5 figures
- **Journal**: Sci Data 7, 417 (2020)
- **Summary**: Canine mammary carcinoma (CMC) has been used as a model to investigate the pathogenesis of human breast cancer and the same grading scheme is commonly used to assess tumor malignancy in both. One key component of this grading scheme is the density of mitotic figures (MF). Current publicly available datasets on human breast cancer only provide annotations for small subsets of whole slide images (WSIs). We present a novel dataset of 21 WSIs of CMC completely annotated for MF. For this, a pathologist screened all WSIs for potential MF and structures with a similar appearance. A second expert blindly assigned labels, and for non-matching labels, a third expert assigned the final labels. Additionally, we used machine learning to identify previously undetected MF. Finally, we performed representation learning and two-dimensional projection to further increase the consistency of the annotations. Our dataset consists of 13,907 MF and 36,379 hard negatives. We achieved a mean F1-score of 0.791 on the test set and of up to 0.696 on a human breast cancer dataset.



### Monocular Reconstruction of Neural Face Reflectance Fields
- **Arxiv ID**: http://arxiv.org/abs/2008.10247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10247v1)
- **Published**: 2020-08-24 08:19:05+00:00
- **Updated**: 2020-08-24 08:19:05+00:00
- **Authors**: Mallikarjun B R., Ayush Tewari, Tae-Hyun Oh, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, Christian Theobalt
- **Comment**: Project page -
  http://gvv.mpi-inf.mpg.de/projects/FaceReflectanceFields/
- **Journal**: None
- **Summary**: The reflectance field of a face describes the reflectance properties responsible for complex lighting effects including diffuse, specular, inter-reflection and self shadowing. Most existing methods for estimating the face reflectance from a monocular image assume faces to be diffuse with very few approaches adding a specular component. This still leaves out important perceptual aspects of reflectance as higher-order global illumination effects and self-shadowing are not modeled. We present a new neural representation for face reflectance where we can estimate all components of the reflectance responsible for the final appearance from a single monocular image. Instead of modeling each component of the reflectance separately using parametric models, our neural representation allows us to generate a basis set of faces in a geometric deformation-invariant space, parameterized by the input light direction, viewpoint and face geometry. We learn to reconstruct this reflectance field of a face just from a monocular image, which can be used to render the face from any viewpoint in any light condition. Our method is trained on a light-stage training dataset, which captures 300 people illuminated with 150 light conditions from 8 viewpoints. We show that our method outperforms existing monocular reflectance reconstruction methods, in terms of photorealism due to better capturing of physical premitives, such as sub-surface scattering, specularities, self-shadows and other higher-order effects.



### A Dataset for Evaluating Blood Detection in Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/2008.10254v2
- **DOI**: 10.1016/j.forsciint.2021.110701
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.10254v2)
- **Published**: 2020-08-24 08:38:00+00:00
- **Updated**: 2021-03-31 12:02:45+00:00
- **Authors**: Michał Romaszewski, Przemysław Głomb, Arkadiusz Sochan, Michał Cholewa
- **Comment**: None
- **Journal**: None
- **Summary**: The sensitivity of imaging spectroscopy to haemoglobin derivatives makes it a promising tool for detecting blood. However, due to complexity and high dimensionality of hyperspectral images, the development of hyperspectral blood detection algorithms is challenging. To facilitate their development, we present a new hyperspectral blood detection dataset. This dataset, published in accordance to open access mandate, consist of multiple detection scenarios with varying levels of complexity. It allows to test the performance of Machine Learning methods in relation to different acquisition environments, types of background, age of blood and presence of other blood-like substances. We explored the dataset with blood detection experiments. We used hyperspectral target detection algorithm based on the well-known Matched Filter detector. Our results and their discussion highlight the challenges of blood detection in hyperspectral data and form a reference for further works.



### Explainable Disease Classification via weakly-supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.10268v1
- **DOI**: 10.1007/978-3-030-61166-8_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10268v1)
- **Published**: 2020-08-24 09:00:30+00:00
- **Updated**: 2020-08-24 09:00:30+00:00
- **Authors**: Aniket Joshi, Gaurav Mishra, Jayanthi Sivaswamy
- **Comment**: None
- **Journal**: Interpretable and Annotation-Efficient Learning for Medical Image
  Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020. Lecture Notes in Computer
  Science, vol 12446. Springer, Cham
- **Summary**: Deep learning based approaches to Computer Aided Diagnosis (CAD) typically pose the problem as an image classification (Normal or Abnormal) problem. These systems achieve high to very high accuracy in specific disease detection for which they are trained but lack in terms of an explanation for the provided decision/classification result. The activation maps which correspond to decisions do not correlate well with regions of interest for specific diseases. This paper examines this problem and proposes an approach which mimics the clinical practice of looking for an evidence prior to diagnosis. A CAD model is learnt using a mixed set of information: class labels for the entire training set of images plus a rough localisation of suspect regions as an extra input for a smaller subset of training images for guiding the learning. The proposed approach is illustrated with detection of diabetic macular edema (DME) from OCT slices. Results of testing on on a large public dataset show that with just a third of images with roughly segmented fluid filled regions, the classification accuracy is on par with state of the art methods while providing a good explanation in the form of anatomically accurate heatmap /region of interest. The proposed solution is then adapted to Breast Cancer detection from mammographic images. Good evaluation results on public datasets underscores the generalisability of the proposed solution.



### Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels
- **Arxiv ID**: http://arxiv.org/abs/2008.10271v5
- **DOI**: 10.1109/JSTARS.2021.3066944
- **Categories**: **cs.CV**, cs.DC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10271v5)
- **Published**: 2020-08-24 09:03:31+00:00
- **Updated**: 2021-06-27 02:50:21+00:00
- **Authors**: Bharath Comandur, Avinash C. Kak
- **Comment**: This work has been accepted by the IEEE for publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: We present a novel multi-view training framework and CNN architecture for combining information from multiple overlapping satellite images and noisy training labels derived from OpenStreetMap (OSM) to semantically label buildings and roads across large geographic regions (100 km$^2$). Our approach to multi-view semantic segmentation yields a 4-7% improvement in the per-class IoU scores compared to the traditional approaches that use the views independently of one another. A unique (and, perhaps, surprising) property of our system is that modifications that are added to the tail-end of the CNN for learning from the multi-view data can be discarded at the time of inference with a relatively small penalty in the overall performance. This implies that the benefits of training using multiple views are absorbed by all the layers of the network. Additionally, our approach only adds a small overhead in terms of the GPU-memory consumption even when training with as many as 32 views per scene. The system we present is end-to-end automated, which facilitates comparing the classifiers trained directly on true orthophotos vis-a-vis first training them on the off-nadir images and subsequently translating the predicted labels to geographical coordinates. With no human supervision, our IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively which are better than state-of-the-art approaches that use OSM labels and that are not completely automated.



### Automated Search for Resource-Efficient Branched Multi-Task Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10292v2)
- **Published**: 2020-08-24 09:49:19+00:00
- **Updated**: 2021-05-11 17:58:18+00:00
- **Authors**: David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, Luc Van Gool
- **Comment**: British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: The multi-modal nature of many vision problems calls for neural network architectures that can perform multiple tasks concurrently. Typically, such architectures have been handcrafted in the literature. However, given the size and complexity of the problem, this manual architecture exploration likely exceeds human design abilities. In this paper, we propose a principled approach, rooted in differentiable neural architecture search, to automatically define branching (tree-like) structures in the encoding stage of a multi-task neural network. To allow flexibility within resource-constrained environments, we introduce a proxyless, resource-aware loss that dynamically controls the model size. Evaluations across a variety of dense prediction tasks show that our approach consistently finds high-performing branching structures within limited resource budgets.



### Bosch Deep Learning Hardware Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2008.10293v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.10293v1)
- **Published**: 2020-08-24 09:50:24+00:00
- **Updated**: 2020-08-24 09:50:24+00:00
- **Authors**: Armin Runge, Thomas Wenzel, Dimitrios Bariamis, Benedikt Sebastian Staffler, Lucas Rego Drumond, Michael Pfeiffer
- **Comment**: Presented in MLBench: Workshop on Benchmarking Machine Learning
  Workloads (https://sites.google.com/g.harvard.edu/mlbench/home)
- **Journal**: None
- **Summary**: The widespread use of Deep Learning (DL) applications in science and industry has created a large demand for efficient inference systems. This has resulted in a rapid increase of available Hardware Accelerators (HWAs) making comparison challenging and laborious. To address this, several DL hardware benchmarks have been proposed aiming at a comprehensive comparison for many models, tasks, and hardware platforms. Here, we present our DL hardware benchmark which has been specifically developed for inference on embedded HWAs and tasks required for autonomous driving. In addition to previous benchmarks, we propose a new granularity level to evaluate common submodules of DL models, a twofold benchmark procedure that accounts for hardware and model optimizations done by HWA manufacturers, and an extended set of performance indicators that can help to identify a mismatch between a HWA and the DL models used in our benchmark.



### CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/2008.10298v1
- **DOI**: 10.1007/978-3-030-67070-2_17
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10298v1)
- **Published**: 2020-08-24 10:11:17+00:00
- **Updated**: 2020-08-24 10:11:17+00:00
- **Authors**: Robin Kips, Pietro Gori, Matthieu Perrot, Isabelle Bloch
- **Comment**: None
- **Journal**: None
- **Summary**: While existing makeup style transfer models perform an image synthesis whose results cannot be explicitly controlled, the ability to modify makeup color continuously is a desirable property for virtual try-on applications. We propose a new formulation for the makeup style transfer task, with the objective to learn a color controllable makeup style synthesis. We introduce CA-GAN, a generative model that learns to modify the color of specific objects (e.g. lips or eyes) in the image to an arbitrary target color while preserving background. Since color labels are rare and costly to acquire, our method leverages weakly supervised learning for conditional GANs. This enables to learn a controllable synthesis of complex objects, and only requires a weak proxy of the image attribute that we desire to modify. Finally, we present for the first time a quantitative analysis of makeup style transfer and color control performance.



### LC-NAS: Latency Constrained Neural Architecture Search for Point Cloud Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.10309v1)
- **Published**: 2020-08-24 10:30:21+00:00
- **Updated**: 2020-08-24 10:30:21+00:00
- **Authors**: Guohao Li, Mengmeng Xu, Silvio Giancola, Ali Thabet, Bernard Ghanem
- **Comment**: Originally submitted to ECCV'2020 but rejected. This work was filed
  with the United States Patent and Trademark Office (USPTO) on May 19, 2020
  and assigned Serial No. 63/027,241
- **Journal**: None
- **Summary**: Point cloud architecture design has become a crucial problem for 3D deep learning. Several efforts exist to manually design architectures with high accuracy in point cloud tasks such as classification, segmentation, and detection. Recent progress in automatic Neural Architecture Search (NAS) minimizes the human effort in network design and optimizes high performing architectures. However, these efforts fail to consider important factors such as latency during inference. Latency is of high importance in time critical applications like self-driving cars, robot navigation, and mobile applications, that are generally bound by the available hardware. In this paper, we introduce a new NAS framework, dubbed LC-NAS, where we search for point cloud architectures that are constrained to a target latency. We implement a novel latency constraint formulation to trade-off between accuracy and latency in our architecture search. Contrary to previous works, our latency loss guarantees that the final network achieves latency under a specified target value. This is crucial when the end task is to be deployed in a limited hardware setting. Extensive experiments show that LC-NAS is able to find state-of-the-art architectures for point cloud classification in ModelNet40 with minimal computational cost. We also show how our searched architectures achieve any desired latency with a reasonably low drop in accuracy. Finally, we show how our searched architectures easily transfer to a different task, part segmentation on PartNet, where we achieve state-of-the-art results while lowering latency by a factor of 10.



### Self-Supervised Learning for Large-Scale Unsupervised Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2008.10312v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10312v2)
- **Published**: 2020-08-24 10:39:19+00:00
- **Updated**: 2020-11-09 16:14:04+00:00
- **Authors**: Evgenii Zheltonozhskii, Chaim Baskin, Alex M. Bronstein, Avi Mendelson
- **Comment**: accepted to NeurIPS 2020 Workshop: Self-Supervised Learning - Theory
  and Practice
- **Journal**: None
- **Summary**: Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at https://github.com/Randl/kmeans_selfsuper



### Improved Mutual Mean-Teaching for Unsupervised Domain Adaptive Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2008.10313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10313v1)
- **Published**: 2020-08-24 10:43:40+00:00
- **Updated**: 2020-08-24 10:43:40+00:00
- **Authors**: Yixiao Ge, Shijie Yu, Dapeng Chen
- **Comment**: 2nd place solution to VisDA-2020 Challenge (ECCVW). Code&Models are
  available at https://github.com/yxgeee/VisDA-ECCV20
- **Journal**: None
- **Summary**: In this technical report, we present our submission to the VisDA Challenge in ECCV 2020 and we achieved one of the top-performing results on the leaderboard. Our solution is based on Structured Domain Adaptation (SDA) and Mutual Mean-Teaching (MMT) frameworks. SDA, a domain-translation-based framework, focuses on carefully translating the source-domain images to the target domain. MMT, a pseudo-label-based framework, focuses on conducting pseudo label refinery with robust soft labels. Specifically, there are three main steps in our training pipeline. (i) We adopt SDA to generate source-to-target translated images, and (ii) such images serve as informative training samples to pre-train the network. (iii) The pre-trained network is further fine-tuned by MMT on the target domain. Note that we design an improved MMT (dubbed MMT+) to further mitigate the label noise by modeling inter-sample relations across two domains and maintaining the instance discrimination. Our proposed method achieved 74.78% accuracies in terms of mAP, ranked the 2nd place out of 153 teams.



### Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10314v1
- **DOI**: 10.1109/ICPR48806.2021.9412185
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10314v1)
- **Published**: 2020-08-24 10:45:19+00:00
- **Updated**: 2020-08-24 10:45:19+00:00
- **Authors**: Shoma Iwai, Tomo Miyazaki, Yoshihiro Sugaya, Shinichiro Omachi
- **Comment**: 8 pages, 11 figures
- **Journal**: ICPR, 2020
- **Summary**: We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. The code will be available.



### A Single Frame and Multi-Frame Joint Network for 360-degree Panorama Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.10320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.10320v1)
- **Published**: 2020-08-24 11:09:54+00:00
- **Updated**: 2020-08-24 11:09:54+00:00
- **Authors**: Hongying Liu, Zhubo Ruan, Chaowei Fang, Peng Zhao, Fanhua Shang, Yuanyuan Liu, Lijun Wang
- **Comment**: 10 pages, 5 figures, submitted to an international peer-review
  journal
- **Journal**: None
- **Summary**: Spherical videos, also known as \ang{360} (panorama) videos, can be viewed with various virtual reality devices such as computers and head-mounted displays. They attract large amount of interest since awesome immersion can be experienced when watching spherical videos. However, capturing, storing and transmitting high-resolution spherical videos are extremely expensive. In this paper, we propose a novel single frame and multi-frame joint network (SMFN) for recovering high-resolution spherical videos from low-resolution inputs. To take advantage of pixel-level inter-frame consistency, deformable convolutions are used to eliminate the motion difference between feature maps of the target frame and its neighboring frames. A mixed attention mechanism is devised to enhance the feature representation capability. The dual learning strategy is exerted to constrain the space of solution so that a better solution can be found. A novel loss function based on the weighted mean square error is proposed to emphasize on the super-resolution of the equatorial regions. This is the first attempt to settle the super-resolution of spherical videos, and we collect a novel dataset from the Internet, MiG Panorama Video, which includes 204 videos. Experimental results on 4 representative video clips demonstrate the efficacy of the proposed method. The dataset and code are available at https://github.com/lovepiano/SMFN_For_360VSR.



### LCA-Net: Light Convolutional Autoencoder for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2008.10325v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10325v1)
- **Published**: 2020-08-24 11:20:52+00:00
- **Updated**: 2020-08-24 11:20:52+00:00
- **Authors**: Pavan A, Adithya Bennur, Mohit Gaggar, Shylaja S S
- **Comment**: 4 pages, 7 figures in .pdf format and 1 figure in .jpg format
- **Journal**: None
- **Summary**: Image dehazing is a crucial image pre-processing task aimed at removing the incoherent noise generated by haze to improve the visual appeal of the image. The existing models use sophisticated networks and custom loss functions which are computationally inefficient and requires heavy hardware to run. Time is of the essence in image pre-processing since real time outputs can be obtained instantly. To overcome these problems, our proposed generic model uses a very light convolutional encoder-decoder network which does not depend on any atmospheric models. The network complexity-image quality trade off is handled well in this neural network and the performance of this network is not limited by low-spec systems. This network achieves optimum dehazing performance at a much faster rate, on several standard datasets, comparable to the state-of-the-art methods in terms of image quality.



### Cascade Convolutional Neural Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.10329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10329v2)
- **Published**: 2020-08-24 11:34:03+00:00
- **Updated**: 2020-08-25 09:31:23+00:00
- **Authors**: Jianwei Zhang, zhenxing Wang, yuhui Zheng, Guoqing Zhang
- **Comment**: 12 page,5 figures
- **Journal**: None
- **Summary**: With the development of the super-resolution convolutional neural network (SRCNN), deep learning technique has been widely applied in the field of image super-resolution. Previous works mainly focus on optimizing the structure of SRCNN, which have been achieved well performance in speed and restoration quality for image super-resolution. However, most of these approaches only consider a specific scale image during the training process, while ignoring the relationship between different scales of images. Motivated by this concern, in this paper, we propose a cascaded convolution neural network for image super-resolution (CSRCNN), which includes three cascaded Fast SRCNNs and each Fast SRCNN can process a specific scale image. Images of different scales can be trained simultaneously and the learned network can make full use of the information resided in different scales of images. Extensive experiments show that our network can achieve well performance for image SR.



### Global-local Enhancement Network for NMFs-aware Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.10428v2
- **DOI**: 10.1145/3436754
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10428v2)
- **Published**: 2020-08-24 13:28:55+00:00
- **Updated**: 2021-08-16 03:38:16+00:00
- **Authors**: Hezhen Hu, Wengang Zhou, Junfu Pu, Houqiang Li
- **Comment**: ACM Transactions on Multimedia Computing, Communications, and
  Applications
- **Journal**: None
- **Summary**: Sign language recognition (SLR) is a challenging problem, involving complex manual features, i.e., hand gestures, and fine-grained non-manual features (NMFs), i.e., facial expression, mouth shapes, etc. Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-local Enhancement Network (GLE-Net), including two mutually promoted streams towards different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of features, we introduce the first non-manual-features-aware isolated Chinese sign language dataset~(NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.



### FOCAL: A Forgery Localization Framework based on Video Coding Self-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2008.10454v2
- **DOI**: 10.1109/OJSP.2021.3074298
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10454v2)
- **Published**: 2020-08-24 13:55:14+00:00
- **Updated**: 2020-09-04 07:55:11+00:00
- **Authors**: Sebastiano Verde, Paolo Bestagini, Simone Milani, Giancarlo Calvagno, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: Forgery operations on video contents are nowadays within the reach of anyone, thanks to the availability of powerful and user-friendly editing software. Integrity verification and authentication of videos represent a major interest in both journalism (e.g., fake news debunking) and legal environments dealing with digital evidence (e.g., a court of law). While several strategies and different forensics traces have been proposed in recent years, latest solutions aim at increasing the accuracy by combining multiple detectors and features. This paper presents a video forgery localization framework that verifies the self-consistency of coding traces between and within video frames, by fusing the information derived from a set of independent feature descriptors. The feature extraction step is carried out by means of an explainable convolutional neural network architecture, specifically designed to look for and classify coding artifacts. The overall framework was validated in two typical forgery scenarios: temporal and spatial splicing. Experimental results show an improvement to the state-of-the-art on temporal splicing localization and also promising performance in the newly tackled case of spatial splicing, on both synthetic and real-world videos.



### CSCL: Critical Semantic-Consistent Learning for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.10464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10464v1)
- **Published**: 2020-08-24 14:12:04+00:00
- **Updated**: 2020-08-24 14:12:04+00:00
- **Authors**: Jiahua Dong, Yang Cong, Gan Sun, Yuyang Liu, Xiaowei Xu
- **Comment**: Accepted to Proceedings of the European Conference on Computer Vision
  2020 (ECCV 2020)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation without consuming annotation process for unlabeled target data attracts appealing interests in semantic segmentation. However, 1) existing methods neglect that not all semantic representations across domains are transferable, which cripples domain-wise transfer with untransferable knowledge; 2) they fail to narrow category-wise distribution shift due to category-agnostic feature alignment. To address above challenges, we develop a new Critical Semantic-Consistent Learning (CSCL) model, which mitigates the discrepancy of both domain-wise and category-wise distributions. Specifically, a critical transfer based adversarial framework is designed to highlight transferable domain-wise knowledge while neglecting untransferable knowledge. Transferability-critic guides transferability-quantizer to maximize positive transfer gain under reinforcement learning manner, although negative transfer of untransferable knowledge occurs. Meanwhile, with the help of confidence-guided pseudo labels generator of target samples, a symmetric soft divergence loss is presented to explore inter-class relationships and facilitate category-wise distribution alignment. Experiments on several datasets demonstrate the superiority of our model.



### 3rd Place Solution to "Google Landmark Retrieval 2020"
- **Arxiv ID**: http://arxiv.org/abs/2008.10480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10480v2)
- **Published**: 2020-08-24 14:39:51+00:00
- **Updated**: 2020-08-25 03:44:05+00:00
- **Authors**: Ke Mei, Lei li, Jinchang Xu, Yanhua Cheng, Yugeng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Image retrieval is a fundamental problem in computer vision. This paper presents our 3rd place detailed solution to the Google Landmark Retrieval 2020 challenge. We focus on the exploration of data cleaning and models with metric learning. We use a data cleaning strategy based on embedding clustering. Besides, we employ a data augmentation method called Corner-Cutmix, which improves the model's ability to recognize multi-scale and occluded landmark images. We show in detail the ablation experiments and results of our method.



### Lossy Image Compression with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2008.10486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10486v1)
- **Published**: 2020-08-24 14:46:23+00:00
- **Updated**: 2020-08-24 14:46:23+00:00
- **Authors**: Leonhard Helminger, Abdelaziz Djelouah, Markus Gross, Christopher Schroers
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based image compression has recently witnessed exciting progress and in some cases even managed to surpass transform coding based approaches that have been established and refined over many decades. However, state-of-the-art solutions for deep image compression typically employ autoencoders which map the input to a lower dimensional latent space and thus irreversibly discard information already before quantization. Due to that, they inherently limit the range of quality levels that can be covered. In contrast, traditional approaches in image compression allow for a larger range of quality levels. Interestingly, they employ an invertible transformation before performing the quantization step which explicitly discards information. Inspired by this, we propose a deep image compression method that is able to go from low bit-rates to near lossless quality by leveraging normalizing flows to learn a bijective mapping from the image space to a latent representation. In addition to this, we demonstrate further advantages unique to our solution, such as the ability to maintain constant quality results through re-encoding, even when performed multiple times. To the best of our knowledge, this is the first work to explore the opportunities for leveraging normalizing flows for lossy image compression.



### EfficientFCN: Holistically-guided Decoding for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.10487v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10487v2)
- **Published**: 2020-08-24 14:48:23+00:00
- **Updated**: 2020-08-27 02:42:38+00:00
- **Authors**: Jianbo Liu, Junjun He, Jiawei Zhang, Jimmy S. Ren, Hongsheng Li
- **Comment**: Accepted in ECCV 2020
- **Journal**: None
- **Summary**: Both performance and efficiency are important to semantic segmentation. State-of-the-art semantic segmentation algorithms are mostly based on dilated Fully Convolutional Networks (dilatedFCN), which adopt dilated convolutions in the backbone networks to extract high-resolution feature maps for achieving high-performance segmentation performance. However, due to many convolution operations are conducted on the high-resolution feature maps, such dilatedFCN-based methods result in large computational complexity and memory consumption. To balance the performance and efficiency, there also exist encoder-decoder structures that gradually recover the spatial information by combining multi-level feature maps from the encoder. However, the performances of existing encoder-decoder methods are far from comparable with the dilatedFCN-based methods. In this paper, we propose the EfficientFCN, whose backbone is a common ImageNet pre-trained network without any dilated convolution. A holistically-guided decoder is introduced to obtain the high-resolution semantic-rich feature maps via the multi-scale features from the encoder. The decoding task is converted to novel codebook generation and codeword assembly task, which takes advantages of the high-level and low-level features from the encoder. Such a framework achieves comparable or even better performance than state-of-the-art methods with only 1/3 of the computational cost. Extensive experiments on PASCAL Context, PASCAL VOC, ADE20K validate the effectiveness of the proposed EfficientFCN.



### Decision Support for Video-based Detection of Flu Symptoms
- **Arxiv ID**: http://arxiv.org/abs/2008.10534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10534v1)
- **Published**: 2020-08-24 16:16:38+00:00
- **Updated**: 2020-08-24 16:16:38+00:00
- **Authors**: Kenneth Lai, Svetlana N. Yanushkevich
- **Comment**: 8 pages, 7 figures, submitted to IEEE SMC
- **Journal**: None
- **Summary**: The development of decision support systems is a growing domain that can be applied in the area of disease control and diagnostics. Using video-based surveillance data, skeleton features are extracted to perform action recognition, specifically the detection and recognition of coughing and sneezing motions. Providing evidence of flu-like symptoms, a decision support system based on causal networks is capable of providing the operator with vital information for decision-making. A modified residual temporal convolutional network is proposed for action recognition using skeleton features. This paper addresses the capability of using results from a machine-learning model as evidence for a cognitive decision support system. We propose risk and trust measures as a metric to bridge between machine-learning and machine-reasoning. We provide experiments on evaluating the performance of the proposed network and how these performance measures can be combined with risk to generate trust.



### Automatic LiDAR Extrinsic Calibration System using Photodetector and Planar Board for Large-scale Applications
- **Arxiv ID**: http://arxiv.org/abs/2008.10542v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10542v1)
- **Published**: 2020-08-24 16:28:40+00:00
- **Updated**: 2020-08-24 16:28:40+00:00
- **Authors**: Ji-Hwan You, Seon Taek Oh, Jae-Eun Park, Azim Eskandarian, Young-Keun Kim
- **Comment**: prepost for IEEE journal
- **Journal**: None
- **Summary**: This paper presents a novel automatic calibration system to estimate the extrinsic parameters of LiDAR mounted on a mobile platform for sensor misalignment inspection in the large-scale production of highly automated vehicles. To obtain subdegree and subcentimeter accuracy levels of extrinsic calibration, this study proposed a new concept of a target board with embedded photodetector arrays, named the PD-target system, to find the precise position of the correspondence laser beams on the target surface. Furthermore, the proposed system requires only the simple design of the target board at the fixed pose in a close range to be readily applicable in the automobile manufacturing environment. The experimental evaluation of the proposed system on low-resolution LiDAR showed that the LiDAR offset pose can be estimated within 0.1 degree and 3 mm levels of precision. The high accuracy and simplicity of the proposed calibration system make it practical for large-scale applications for the reliability and safety of autonomous systems.



### TORNADO-Net: mulTiview tOtal vaRiatioN semAntic segmentation with Diamond inceptiOn module
- **Arxiv ID**: http://arxiv.org/abs/2008.10544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10544v1)
- **Published**: 2020-08-24 16:32:41+00:00
- **Updated**: 2020-08-24 16:32:41+00:00
- **Authors**: Martin Gerdzhev, Ryan Razani, Ehsan Taghavi, Bingbing Liu
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Semantic segmentation of point clouds is a key component of scene understanding for robotics and autonomous driving. In this paper, we introduce TORNADO-Net - a neural network for 3D LiDAR point cloud semantic segmentation. We incorporate a multi-view (bird-eye and range) projection feature extraction with an encoder-decoder ResNet architecture with a novel diamond context block. Current projection-based methods do not take into account that neighboring points usually belong to the same class. To better utilize this local neighbourhood information and reduce noisy predictions, we introduce a combination of Total Variation, Lovasz-Softmax, and Weighted Cross-Entropy losses. We also take advantage of the fact that the LiDAR data encompasses 360 degrees field of view and uses circular padding. We demonstrate state-of-the-art results on the SemanticKITTI dataset and also provide thorough quantitative evaluations and ablation results.



### Products-10K: A Large-scale Product Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2008.10545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10545v1)
- **Published**: 2020-08-24 16:33:37+00:00
- **Updated**: 2020-08-24 16:33:37+00:00
- **Authors**: Yalong Bai, Yuxiang Chen, Wei Yu, Linfang Wang, Wei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of electronic commerce, the way of shopping has experienced a revolutionary evolution. To fully meet customers' massive and diverse online shopping needs with quick response, the retailing AI system needs to automatically recognize products from images and videos at the stock-keeping unit (SKU) level with high accuracy. However, product recognition is still a challenging task, since many of SKU-level products are fine-grained and visually similar by a rough glimpse. Although there are already some products benchmarks available, these datasets are either too small (limited number of products) or noisy-labeled (lack of human labeling). In this paper, we construct a human-labeled product image dataset named "Products-10K", which contains 10,000 fine-grained SKU-level products frequently bought by online customers in JD.com. Based on our new database, we also introduced several useful tips and tricks for fine-grained product recognition. The products-10K dataset is available via https://products-10k.github.io/.



### Certainty Pooling for Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.10548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10548v1)
- **Published**: 2020-08-24 16:38:46+00:00
- **Updated**: 2020-08-24 16:38:46+00:00
- **Authors**: Jacob Gildenblat, Ido Ben-Shaul, Zvi Lapp, Eldad Klaiman
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Instance Learning is a form of weakly supervised learning in which the data is arranged in sets of instances called bags with one label assigned per bag. The bag level class prediction is derived from the multiple instances through application of a permutation invariant pooling operator on instance predictions or embeddings. We present a novel pooling operator called \textbf{Certainty Pooling} which incorporates the model certainty into bag predictions resulting in a more robust and explainable model. We compare our proposed method with other pooling operators in controlled experiments with low evidence ratio bags based on MNIST, as well as on a real life histopathology dataset - Camelyon16. Our method outperforms other methods in both bag level and instance level prediction, especially when only small training sets are available. We discuss the rationale behind our approach and the reasons for its superiority for these types of datasets.



### LMSCNet: Lightweight Multiscale 3D Semantic Completion
- **Arxiv ID**: http://arxiv.org/abs/2008.10559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10559v2)
- **Published**: 2020-08-24 16:52:16+00:00
- **Updated**: 2020-10-25 15:26:18+00:00
- **Authors**: Luis Roldão, Raoul de Charette, Anne Verroust-Blondet
- **Comment**: Accepted at 3DV 2020 (Oral). For a demo video, see
  http://tiny.cc/lmscnet. Code is available at
  https://github.com/cv-rits/LMSCNet
- **Journal**: None
- **Summary**: We introduce a new approach for multiscale 3Dsemantic scene completion from voxelized sparse 3D LiDAR scans. As opposed to the literature, we use a 2D UNet backbone with comprehensive multiscale skip connections to enhance feature flow, along with 3D segmentation heads. On the SemanticKITTI benchmark, our method performs on par on semantic completion and better on occupancy completion than all other published methods -- while being significantly lighter and faster. As such it provides a great performance/speed trade-off for mobile-robotics applications. The ablation studies demonstrate our method is robust to lower density inputs, and that it enables very high speed semantic completion at the coarsest level. Our code is available at https://github.com/cv-rits/LMSCNet.



### Classification of Noncoding RNA Elements Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10580v1)
- **Published**: 2020-08-24 17:43:50+00:00
- **Updated**: 2020-08-24 17:43:50+00:00
- **Authors**: Brian McClannahan, Krushi Patel, Usman Sajid, Cuncong Zhong, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The paper proposes to employ deep convolutional neural networks (CNNs) to classify noncoding RNA (ncRNA) sequences. To this end, we first propose an efficient approach to convert the RNA sequences into images characterizing their base-pairing probability. As a result, classifying RNA sequences is converted to an image classification problem that can be efficiently solved by available CNN-based classification models. The paper also considers the folding potential of the ncRNAs in addition to their primary sequence. Based on the proposed approach, a benchmark image classification dataset is generated from the RFAM database of ncRNA sequences. In addition, three classical CNN models have been implemented and compared to demonstrate the superior performance and efficiency of the proposed approach. Extensive experimental results show the great potential of using deep learning approaches for RNA classification.



### Accurate Alignment Inspection System for Low-resolution Automotive and Mobility LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2008.10584v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10584v1)
- **Published**: 2020-08-24 17:47:59+00:00
- **Updated**: 2020-08-24 17:47:59+00:00
- **Authors**: Seontake Oh, Ji-Hwan You, Azim Eskandarian, Young-Keun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: A misalignment of LiDAR as low as a few degrees could cause a significant error in obstacle detection and mapping that could cause safety and quality issues. In this paper, an accurate inspection system is proposed for estimating a LiDAR alignment error after sensor attachment on a mobility system such as a vehicle or robot. The proposed method uses only a single target board at the fixed position to estimate the three orientations (roll, tilt, and yaw) and the horizontal position of the LiDAR attachment with sub-degree and millimeter level accuracy. After the proposed preprocessing steps, the feature beam points that are the closest to each target corner are extracted and used to calculate the sensor attachment pose with respect to the target board frame using a nonlinear optimization method and with a low computational cost. The performance of the proposed method is evaluated using a test bench that can control the reference yaw and horizontal translation of LiDAR within ranges of 3 degrees and 30 millimeters, respectively. The experimental results for a low-resolution 16 channel LiDAR (Velodyne VLP-16) confirmed that misalignment could be estimated with accuracy within 0.2 degrees and 4 mm. The high accuracy and simplicity of the proposed system make it practical for large-scale industrial applications such as automobile or robot manufacturing process that inspects the sensor attachment for the safety quality control.



### What makes fake images detectable? Understanding properties that generalize
- **Arxiv ID**: http://arxiv.org/abs/2008.10588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10588v1)
- **Published**: 2020-08-24 17:50:28+00:00
- **Updated**: 2020-08-24 17:50:28+00:00
- **Authors**: Lucy Chai, David Bau, Ser-Nam Lim, Phillip Isola
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of image generation and manipulation is reaching impressive levels, making it increasingly difficult for a human to distinguish between what is real and what is fake. However, deep networks can still pick up on the subtle artifacts in these doctored images. We seek to understand what properties of fake images make them detectable and identify what generalizes across different model architectures, datasets, and variations in training. We use a patch-based classifier with limited receptive fields to visualize which regions of fake images are more easily detectable. We further show a technique to exaggerate these detectable properties and demonstrate that, even when the image generator is adversarially finetuned against a fake image classifier, it is still imperfect and leaves detectable artifacts in certain image patches. Code is available at https://chail.github.io/patch-forensics/.



### 3D for Free: Crossmodal Transfer Learning using HD Maps
- **Arxiv ID**: http://arxiv.org/abs/2008.10592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10592v1)
- **Published**: 2020-08-24 17:54:51+00:00
- **Updated**: 2020-08-24 17:54:51+00:00
- **Authors**: Benjamin Wilson, Zsolt Kira, James Hays
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is a core perceptual challenge for robotics and autonomous driving. However, the class-taxonomies in modern autonomous driving datasets are significantly smaller than many influential 2D detection datasets. In this work, we address the long-tail problem by leveraging both the large class-taxonomies of modern 2D datasets and the robustness of state-of-the-art 2D detection methods. We proceed to mine a large, unlabeled dataset of images and LiDAR, and estimate 3D object bounding cuboids, seeded from an off-the-shelf 2D instance segmentation model. Critically, we constrain this ill-posed 2D-to-3D mapping by using high-definition maps and object size priors. The result of the mining process is 3D cuboids with varying confidence. This mining process is itself a 3D object detector, although not especially accurate when evaluated as such. However, we then train a 3D object detection model on these cuboids, consistent with other recent observations in the deep learning literature, we find that the resulting model is fairly robust to the noisy supervision that our mining process provides. We mine a collection of 1151 unlabeled, multimodal driving logs from an autonomous vehicle and use the discovered objects to train a LiDAR-based object detector. We show that detector performance increases as we mine more unlabeled data. With our full, unlabeled dataset, our method performs competitively with fully supervised methods, even exceeding the performance for certain object categories, without any human 3D annotations.



### Semantic View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.10598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10598v1)
- **Published**: 2020-08-24 17:59:46+00:00
- **Updated**: 2020-08-24 17:59:46+00:00
- **Authors**: Hsin-Ping Huang, Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang
- **Comment**: ECCV 2020. Project: https://hhsinping.github.io/svs/index.html Colab:
  https://colab.research.google.com/drive/1iT5PfK7zl1quAOwC227GfBjieFMVHjI5
- **Journal**: None
- **Summary**: We tackle a new problem of semantic view synthesis -- generating free-viewpoint rendering of a synthesized scene using a semantic label map as input. We build upon recent advances in semantic image synthesis and view synthesis for handling photographic image content generation and view extrapolation. Direct application of existing image/view synthesis methods, however, results in severe ghosting/blurry artifacts. To address the drawbacks, we propose a two-step approach. First, we focus on synthesizing the color and depth of the visible surface of the 3D scene. We then use the synthesized color and depth to impose explicit constraints on the multiple-plane image (MPI) representation prediction process. Our method produces sharp contents at the original view and geometrically consistent renderings across novel viewpoints. The experiments on numerous indoor and outdoor images show favorable results against several strong baselines and validate the effectiveness of our approach.



### The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2008.10599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2008.10599v1)
- **Published**: 2020-08-24 17:59:56+00:00
- **Updated**: 2020-08-24 17:59:56+00:00
- **Authors**: William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio Torralba
- **Comment**: ECCV 2020 (Spotlight). Code available at
  https://github.com/wpeebles/hessian_penalty . Project page and videos
  available at https://www.wpeebles.com/hessian-penalty
- **Journal**: None
- **Summary**: Existing disentanglement methods for deep generative models rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization term that encourages the Hessian of a generative model with respect to its input to be diagonal. We introduce a model-agnostic, unbiased stochastic approximation of this term based on Hutchinson's estimator to compute it efficiently during training. Our method can be applied to a wide range of deep generators with just a few lines of code. We show that training with the Hessian Penalty often causes axis-aligned disentanglement to emerge in latent space when applied to ProGAN on several datasets. Additionally, we use our regularization term to identify interpretable directions in BigGAN's latent space in an unsupervised fashion. Finally, we provide empirical evidence that the Hessian Penalty encourages substantial shrinkage when applied to over-parameterized latent spaces.



### OpenBot: Turning Smartphones into Robots
- **Arxiv ID**: http://arxiv.org/abs/2008.10631v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10631v2)
- **Published**: 2020-08-24 18:04:50+00:00
- **Updated**: 2021-03-10 19:08:00+00:00
- **Authors**: Matthias Müller, Vladlen Koltun
- **Comment**: Accepted at ICRA'21. Documentation and code are available at
  www.openbot.org
- **Journal**: None
- **Summary**: Current robots are either expensive or make significant compromises on sensory richness, computational power, and communication capabilities. We propose to leverage smartphones to equip robots with extensive sensor suites, powerful computational abilities, state-of-the-art communication channels, and access to a thriving software ecosystem. We design a small electric vehicle that costs $50 and serves as a robot body for standard Android smartphones. We develop a software stack that allows smartphones to use this body for mobile operation and demonstrate that the system is sufficiently powerful to support advanced robotics workloads such as person following and real-time autonomous navigation in unstructured environments. Controlled experiments demonstrate that the presented approach is robust across different smartphones and robot bodies. A video of our work is available at https://www.youtube.com/watch?v=qc8hFLyWDOM



### DiverseNet: When One Right Answer is not Enough
- **Arxiv ID**: http://arxiv.org/abs/2008.10634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10634v1)
- **Published**: 2020-08-24 18:12:49+00:00
- **Updated**: 2020-08-24 18:12:49+00:00
- **Authors**: Michael Firman, Neill D. F. Campbell, Lourdes Agapito, Gabriel J. Brostow
- **Comment**: Presented at CVPR 2018
- **Journal**: None
- **Summary**: Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy.   We introduce a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. Our best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. We demonstrate that our method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction.



### Robust Pancreatic Ductal Adenocarcinoma Segmentation with Multi-Institutional Multi-Phase Partially-Annotated CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2008.10652v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10652v1)
- **Published**: 2020-08-24 18:50:30+00:00
- **Updated**: 2020-08-24 18:50:30+00:00
- **Authors**: Ling Zhang, Yu Shi, Jiawen Yao, Yun Bian, Kai Cao, Dakai Jin, Jing Xiao, Le Lu
- **Comment**: 10 pages, 2 figures; MICCAI 2020
- **Journal**: None
- **Summary**: Accurate and automated tumor segmentation is highly desired since it has the great potential to increase the efficiency and reproducibility of computing more complete tumor measurements and imaging biomarkers, comparing to (often partial) human measurements. This is probably the only viable means to enable the large-scale clinical oncology patient studies that utilize medical imaging. Deep learning approaches have shown robust segmentation performances for certain types of tumors, e.g., brain tumors in MRI imaging, when a training dataset with plenty of pixel-level fully-annotated tumor images is available. However, more than often, we are facing the challenge that only (very) limited annotations are feasible to acquire, especially for hard tumors. Pancreatic ductal adenocarcinoma (PDAC) segmentation is one of the most challenging tumor segmentation tasks, yet critically important for clinical needs. Previous work on PDAC segmentation is limited to the moderate amounts of annotated patient images (n<300) from venous or venous+arterial phase CT scans. Based on a new self-learning framework, we propose to train the PDAC segmentation model using a much larger quantity of patients (n~=1,000), with a mix of annotated and un-annotated venous or multi-phase CT images. Pseudo annotations are generated by combining two teacher models with different PDAC segmentation specialties on unannotated images, and can be further refined by a teaching assistant model that identifies associated vessels around the pancreas. A student model is trained on both manual and pseudo annotated multi-phase images. Experiment results show that our proposed method provides an absolute improvement of 6.3% Dice score over the strong baseline of nnUNet trained on annotated images, achieving the performance (Dice = 0.71) similar to the inter-observer variability between radiologists.



### Probabilistic Deep Learning for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.10678v2
- **DOI**: 10.1007/978-3-030-66415-2_29
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10678v2)
- **Published**: 2020-08-24 19:51:48+00:00
- **Updated**: 2020-12-17 11:38:42+00:00
- **Authors**: Josef Lorenz Rumberger, Lisa Mais, Dagmar Kainmueller
- **Comment**: ECCV 2020 BioImage Computing Workshop
- **Journal**: None
- **Summary**: Probabilistic convolutional neural networks, which predict distributions of predictions instead of point estimates, led to recent advances in many areas of computer vision, from image reconstruction to semantic segmentation. Besides state of the art benchmark results, these networks made it possible to quantify local uncertainties in the predictions. These were used in active learning frameworks to target the labeling efforts of specialist annotators or to assess the quality of a prediction in a safety-critical environment. However, for instance segmentation problems these methods are not frequently used so far. We seek to close this gap by proposing a generic method to obtain model-inherent uncertainty estimates within proposal-free instance segmentation models. Furthermore, we analyze the quality of the uncertainty estimates with a metric adapted from semantic segmentation. We evaluate our method on the BBBC010 C.\ elegans dataset, where it yields competitive performance while also predicting uncertainty estimates that carry information about object-level inaccuracies like false splits and false merges. We perform a simulation to show the potential use of such uncertainty estimates in guided proofreading.



### Video Frame Interpolation via Generalized Deformable Convolution
- **Arxiv ID**: http://arxiv.org/abs/2008.10680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10680v3)
- **Published**: 2020-08-24 20:00:39+00:00
- **Updated**: 2021-03-18 16:09:35+00:00
- **Authors**: Zhihao Shi, Xiaohong Liu, Kangdi Shi, Linhui Dai, Jun Chen
- **Comment**: 13pages, journal
- **Journal**: None
- **Summary**: Video frame interpolation aims at synthesizing intermediate frames from nearby source frames while maintaining spatial and temporal consistencies. The existing deep-learning-based video frame interpolation methods can be roughly divided into two categories: flow-based methods and kernel-based methods. The performance of flow-based methods is often jeopardized by the inaccuracy of flow map estimation due to oversimplified motion models, while that of kernel-based methods tends to be constrained by the rigidity of kernel shape. To address these performance-limiting issues, a novel mechanism named generalized deformable convolution is proposed, which can effectively learn motion information in a data-driven manner and freely select sampling points in space-time. We further develop a new video frame interpolation method based on this mechanism. Our extensive experiments demonstrate that the new method performs favorably against the state-of-the-art, especially when dealing with complex motions.



### Exploit Camera Raw Data for Video Super-Resolution via Hidden Markov Model Inference
- **Arxiv ID**: http://arxiv.org/abs/2008.10710v2
- **DOI**: 10.1109/TIP.2021.3049974
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10710v2)
- **Published**: 2020-08-24 21:14:13+00:00
- **Updated**: 2021-01-05 04:03:55+00:00
- **Authors**: Xiaohong Liu, Kangdi Shi, Zhe Wang, Jun Chen
- **Comment**: 13 pages, 14 figures, accepted in IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: To the best of our knowledge, the existing deep-learning-based Video Super-Resolution (VSR) methods exclusively make use of videos produced by the Image Signal Processor (ISP) of the camera system as inputs. Such methods are 1) inherently suboptimal due to information loss incurred by non-invertible operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR in fact serves as a pre-processing unit of ISP. To address this issue, we propose a new VSR method that can directly exploit camera sensor data, accompanied by a carefully built Raw Video Dataset (RawVD) for training, validation, and testing. This method consists of a Successive Deep Inference (SDI) module and a reconstruction module, among others. The SDI module is designed according to the architectural principle suggested by a canonical decomposition result for Hidden Markov Model (HMM) inference; it estimates the target high-resolution frame by repeatedly performing pairwise feature fusion using deformable convolutions. The reconstruction module, built with elaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the purpose of 1) refining the fused feature and 2) learning the color information needed to generate a spatial-specific transformation for accurate color correction. Extensive experiments demonstrate that owing to the informativeness of the camera raw data, the effectiveness of the network architecture, and the separation of super-resolution and color correction processes, the proposed method achieves superior VSR results compared to the state-of-the-art and can be adapted to any specific camera-ISP. Code and dataset are available at https://github.com/proteus1991/RawVSR.



### Interactive Annotation of 3D Object Geometry using 2D Scribbles
- **Arxiv ID**: http://arxiv.org/abs/2008.10719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10719v2)
- **Published**: 2020-08-24 21:51:29+00:00
- **Updated**: 2020-10-26 02:43:19+00:00
- **Authors**: Tianchang Shen, Jun Gao, Amlan Kar, Sanja Fidler
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Inferring detailed 3D geometry of the scene is crucial for robotics applications, simulation, and 3D content creation. However, such information is hard to obtain, and thus very few datasets support it. In this paper, we propose an interactive framework for annotating 3D object geometry from both point cloud data and RGB imagery. The key idea behind our approach is to exploit strong priors that humans have about the 3D world in order to interactively annotate complete 3D shapes. Our framework targets naive users without artistic or graphics expertise. We introduce two simple-to-use interaction modules. First, we make an automatic guess of the 3D shape and allow the user to provide feedback about large errors by drawing scribbles in desired 2D views. Next, we aim to correct minor errors, in which users drag and drop mesh vertices, assisted by a neural interactive module implemented as a Graph Convolutional Network. Experimentally, we show that only a few user interactions are needed to produce good quality 3D shapes on popular benchmarks such as ShapeNet, Pix3D and ScanNet. We implement our framework as a web service and conduct a user study, where we show that user annotated data using our method effectively facilitates real-world learning tasks. Web service: http://www.cs.toronto.edu/~shenti11/scribble3d.



### LULC Segmentation of RGB Satellite Image Using FCN-8
- **Arxiv ID**: http://arxiv.org/abs/2008.10736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.10736v1)
- **Published**: 2020-08-24 22:32:30+00:00
- **Updated**: 2020-08-24 22:32:30+00:00
- **Authors**: Abu Bakar Siddik Nayem, Anis Sarker, Ovi Paul, Amin Ali, Md. Ashraful Amin, AKM Mahbubur Rahman
- **Comment**: Accepted paper at 3rd SLAAI-International Conference on Artificial
  Intelligence; 13 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: This work presents use of Fully Convolutional Network (FCN-8) for semantic segmentation of high-resolution RGB earth surface satel-lite images into land use land cover (LULC) categories. Specically, we propose a non-overlapping grid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16 weights to segment satellite im-ages into four (forest, built-up, farmland and water) classes. The FCN-8 semantically projects the discriminating features in lower resolution learned by the encoder onto the pixel space in higher resolution to get a dense classi cation. We experimented the proposed system with Gaofen-2 image dataset, that contains 150 images of over 60 di erent cities in china. For comparison, we used available ground-truth along with images segmented using a widely used commeriial GIS software called eCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8 obtains signi cantly improved performance, than the eCognition soft-ware. Our model achieves average accuracy of 91.0% and average Inter-section over Union (IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is 0.60. This paper also reports a detail analysis of errors occurred at the LULC boundary.



