# Arxiv Papers in cs.CV on 2020-08-23
### Few-Shot Image Classification via Contrastive Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.09942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.09942v1)
- **Published**: 2020-08-23 02:24:31+00:00
- **Updated**: 2020-08-23 02:24:31+00:00
- **Authors**: Jianyi Li, Guizhong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most previous few-shot learning algorithms are based on meta-training with fake few-shot tasks as training samples, where large labeled base classes are required. The trained model is also limited by the type of tasks. In this paper we propose a new paradigm of unsupervised few-shot learning to repair the deficiencies. We solve the few-shot tasks in two phases: meta-training a transferable feature extractor via contrastive self-supervised learning and training a classifier using graph aggregation, self-distillation and manifold augmentation. Once meta-trained, the model can be used in any type of tasks with a task-dependent classifier training. Our method achieves state of-the-art performance in a variety of established few-shot tasks on the standard few-shot visual classification datasets, with an 8- 28% increase compared to the available unsupervised few-shot learning methods.



### Vulnerability of Face Recognition Systems Against Composite Face Reconstruction Attack
- **Arxiv ID**: http://arxiv.org/abs/2009.02286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02286v1)
- **Published**: 2020-08-23 03:37:51+00:00
- **Updated**: 2020-08-23 03:37:51+00:00
- **Authors**: Hadi Mansourifar, Weidong Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Rounding confidence score is considered trivial but a simple and effective countermeasure to stop gradient descent based image reconstruction attacks. However, its capability in the face of more sophisticated reconstruction attacks is an uninvestigated research area. In this paper, we prove that, the face reconstruction attacks based on composite faces can reveal the inefficiency of rounding policy as countermeasure. We assume that, the attacker takes advantage of face composite parts which helps the attacker to get access to the most important features of the face or decompose it to the independent segments. Afterwards, decomposed segments are exploited as search parameters to create a search path to reconstruct optimal face. Face composition parts enable the attacker to violate the privacy of face recognition models even with a blind search. However, we assume that, the attacker may take advantage of random search to reconstruct the target face faster. The algorithm is started with random composition of face parts as initial face and confidence score is considered as fitness value. Our experiments show that, since the rounding policy as countermeasure can't stop the random search process, current face recognition systems are extremely vulnerable against such sophisticated attacks. To address this problem, we successfully test Face Detection Score Filtering (FDSF) as a countermeasure to protect the privacy of training data against proposed attack.



### Matching Guided Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.09958v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.09958v2)
- **Published**: 2020-08-23 04:57:31+00:00
- **Updated**: 2020-10-13 02:58:25+00:00
- **Authors**: Kaiyu Yue, Jiangfan Deng, Feng Zhou
- **Comment**: ECCV 2020 Camera-Ready. Project: http://kaiyuyue.com/mgd
- **Journal**: None
- **Summary**: Feature distillation is an effective way to improve the performance for a smaller student model, which has fewer parameters and lower computation cost compared to the larger teacher model. Unfortunately, there is a common obstacle - the gap in semantic feature structure between the intermediate features of teacher and student. The classic scheme prefers to transform intermediate features by adding the adaptation module, such as naive convolutional, attention-based or more complicated one. However, this introduces two problems: a) The adaptation module brings more parameters into training. b) The adaptation module with random initialization or special transformation isn't friendly for distilling a pre-trained student. In this paper, we present Matching Guided Distillation (MGD) as an efficient and parameter-free manner to solve these problems. The key idea of MGD is to pose matching the teacher channels with students' as an assignment problem. We compare three solutions of the assignment problem to reduce channels from teacher features with partial distillation loss. The overall training takes a coordinate-descent approach between two optimization objects - assignments update and parameters update. Since MGD only contains normalization or pooling operations with negligible computation cost, it is flexible to plug into network with other distillation methods.



### Neighbourhood-Insensitive Point Cloud Normal Estimation Network
- **Arxiv ID**: http://arxiv.org/abs/2008.09965v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09965v3)
- **Published**: 2020-08-23 05:46:58+00:00
- **Updated**: 2021-01-15 11:01:58+00:00
- **Authors**: Zirui Wang, Victor Adrian Prisacariu
- **Comment**: Accepted in BMVC 2020 as oral presentation. Code available at
  https://code.active.vision and project page at http://ninormal.active.vision
- **Journal**: None
- **Summary**: We introduce a novel self-attention-based normal estimation network that is able to focus softly on relevant points and adjust the softness by learning a temperature parameter, making it able to work naturally and effectively within a large neighbourhood range. As a result, our model outperforms all existing normal estimation algorithms by a large margin, achieving 94.1% accuracy in comparison with the previous state of the art of 91.2%, with a 25x smaller model and 12x faster inference time. We also use point-to-plane Iterative Closest Point (ICP) as an application case to show that our normal estimations lead to faster convergence than normal estimations from other methods, without manually fine-tuning neighbourhood range parameters. Code available at https://code.active.vision.



### Vision at A Glance: Interplay between Fine and Coarse Information Processing Pathways
- **Arxiv ID**: http://arxiv.org/abs/2009.05101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05101v1)
- **Published**: 2020-08-23 06:46:26+00:00
- **Updated**: 2020-08-23 06:46:26+00:00
- **Authors**: Zilong Ji, Xiaolong Zou, Tiejun Huang, Si Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition is often viewed as a feedforward, bottom-up process in machine learning, but in real neural systems, object recognition is a complicated process which involves the interplay between two signal pathways. One is the parvocellular pathway (P-pathway), which is slow and extracts fine features of objects; the other is the magnocellular pathway (M-pathway), which is fast and extracts coarse features of objects. It has been suggested that the interplay between the two pathways endows the neural system with the capacity of processing visual information rapidly, adaptively, and robustly. However, the underlying computational mechanisms remain largely unknown. In this study, we build a computational model to elucidate the computational advantages associated with the interactions between two pathways. Our model consists of two convolution neural networks: one mimics the P-pathway, referred to as FineNet, which is deep, has small-size kernels, and receives detailed visual inputs; the other mimics the M-pathway, referred to as CoarseNet, which is shallow, has large-size kernels, and receives low-pass filtered or binarized visual inputs. The two pathways interact with each other via a Restricted Boltzmann Machine. We find that: 1) FineNet can teach CoarseNet through imitation and improve its performance considerably; 2) CoarseNet can improve the noise robustness of FineNet through association; 3) the output of CoarseNet can serve as a cognitive bias to improve the performance of FineNet. We hope that this study will provide insight into understanding visual information processing and inspire the development of new object recognition architectures.



### Visible Feature Guidance for Crowd Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.09993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09993v2)
- **Published**: 2020-08-23 08:52:52+00:00
- **Updated**: 2020-09-16 11:23:38+00:00
- **Authors**: Zhida Huang, Kaiyu Yue, Jiangfan Deng, Feng Zhou
- **Comment**: Technical report; To appear at ECCV 2020 RLQ Workshop
- **Journal**: None
- **Summary**: Heavy occlusion and dense gathering in crowd scene make pedestrian detection become a challenging problem, because it's difficult to guess a precise full bounding box according to the invisible human part. To crack this nut, we propose a mechanism called Visible Feature Guidance (VFG) for both training and inference. During training, we adopt visible feature to regress the simultaneous outputs of visible bounding box and full bounding box. Then we perform NMS only on visible bounding boxes to achieve the best fitting full box in inference. This manner can alleviate the incapable influence brought by NMS in crowd scene and make full bounding box more precisely. Furthermore, in order to ease feature association in the post application process, such as pedestrian tracking, we apply Hungarian algorithm to associate parts for a human instance. Our proposed method can stably bring about 2~3% improvements in mAP and AP50 for both two-stage and one-stage detector. It's also more effective for MR-2 especially with the stricter IoU. Experiments on Crowdhuman, Cityperson, Caltech and KITTI datasets show that visible feature guidance can help detector achieve promisingly better performances. Moreover, parts association produces a strong benchmark on Crowdhuman for the vision community.



### Discriminative Residual Analysis for Image Set Classification with Posture and Age Variations
- **Arxiv ID**: http://arxiv.org/abs/2008.09994v1
- **DOI**: 10.1109/TIP.2019.2954176
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09994v1)
- **Published**: 2020-08-23 08:53:06+00:00
- **Updated**: 2020-08-23 08:53:06+00:00
- **Authors**: Chuan-Xian Ren, You-Wei Luo, Xiao-Lin Xu, Dao-Qing Dai, Hong Yan
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 29, pp. 2875-2888,
  2020
- **Summary**: Image set recognition has been widely applied in many practical problems like real-time video retrieval and image caption tasks. Due to its superior performance, it has grown into a significant topic in recent years. However, images with complicated variations, e.g., postures and human ages, are difficult to address, as these variations are continuous and gradual with respect to image appearance. Consequently, the crucial point of image set recognition is to mine the intrinsic connection or structural information from the image batches with variations. In this work, a Discriminant Residual Analysis (DRA) method is proposed to improve the classification performance by discovering discriminant features in related and unrelated groups. Specifically, DRA attempts to obtain a powerful projection which casts the residual representations into a discriminant subspace. Such a projection subspace is expected to magnify the useful information of the input space as much as possible, then the relation between the training set and the test set described by the given metric or distance will be more precise in the discriminant subspace. We also propose a nonfeasance strategy by defining another approach to construct the unrelated groups, which help to reduce furthermore the cost of sampling errors. Two regularization approaches are used to deal with the probable small sample size problem. Extensive experiments are conducted on benchmark databases, and the results show superiority and efficiency of the new methods.



### A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild
- **Arxiv ID**: http://arxiv.org/abs/2008.10010v1
- **DOI**: 10.1145/3394171.3413532
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.10010v1)
- **Published**: 2020-08-23 11:01:25+00:00
- **Updated**: 2020-08-23 11:01:25+00:00
- **Authors**: K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar
- **Comment**: 9 pages (including references), 3 figures, Accepted in ACM
  Multimedia, 2020
- **Journal**: None
- **Summary**: In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: \url{cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild}. The code and models are released at this GitHub repository: \url{github.com/Rudrabha/Wav2Lip}. You can also try out the interactive demo at this link: \url{bhaasha.iiit.ac.in/lipsync}.



### Unsupervised Domain Adaptation via Discriminative Manifold Propagation
- **Arxiv ID**: http://arxiv.org/abs/2008.10030v1
- **DOI**: 10.1109/TPAMI.2020.3014218
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.10030v1)
- **Published**: 2020-08-23 12:31:37+00:00
- **Updated**: 2020-08-23 12:31:37+00:00
- **Authors**: You-Wei Luo, Chuan-Xian Ren, Dao-Qing Dai, Hong Yan
- **Comment**: To be published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is effective in leveraging rich information from a labeled source domain to an unlabeled target domain. Though deep learning and adversarial strategy made a significant breakthrough in the adaptability of features, there are two issues to be further studied. First, hard-assigned pseudo labels on the target domain are arbitrary and error-prone, and direct application of them may destroy the intrinsic data structure. Second, batch-wise training of deep learning limits the characterization of the global structure. In this paper, a Riemannian manifold learning framework is proposed to achieve transferability and discriminability simultaneously. For the first issue, this framework establishes a probabilistic discriminant criterion on the target domain via soft labels. Based on pre-built prototypes, this criterion is extended to a global approximation scheme for the second issue. Manifold metric alignment is adopted to be compatible with the embedding space. The theoretical error bounds of different alignment metrics are derived for constructive guidance. The proposed method can be used to tackle a series of variants of domain adaptation problems, including both vanilla and partial settings. Extensive experiments have been conducted to investigate the method and a comparative study shows the superiority of the discriminative manifold learning framework.



### Seesaw Loss for Long-Tailed Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.10032v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10032v4)
- **Published**: 2020-08-23 12:44:45+00:00
- **Updated**: 2021-06-17 15:13:10+00:00
- **Authors**: Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, Dahua Lin
- **Comment**: CVPR 2021 Camera Ready
- **Journal**: None
- **Summary**: Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection.



### Dual Adversarial Auto-Encoders for Clustering
- **Arxiv ID**: http://arxiv.org/abs/2008.10038v1
- **DOI**: 10.1109/TNNLS.2019.2919948
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10038v1)
- **Published**: 2020-08-23 13:16:34+00:00
- **Updated**: 2020-08-23 13:16:34+00:00
- **Authors**: Pengfei Ge, Chuan-Xian Ren, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: As a powerful approach for exploratory data analysis, unsupervised clustering is a fundamental task in computer vision and pattern recognition. Many clustering algorithms have been developed, but most of them perform unsatisfactorily on the data with complex structures. Recently, Adversarial Auto-Encoder (AAE) shows effectiveness on tackling such data by combining Auto-Encoder (AE) and adversarial training, but it cannot effectively extract classification information from the unlabeled data. In this work, we propose Dual Adversarial Auto-encoder (Dual-AAE) which simultaneously maximizes the likelihood function and mutual information between observed examples and a subset of latent variables. By performing variational inference on the objective function of Dual-AAE, we derive a new reconstruction loss which can be optimized by training a pair of Auto-encoders. Moreover, to avoid mode collapse, we introduce the clustering regularization term for the category variable. Experiments on four benchmarks show that Dual-AAE achieves superior performance over state-of-the-art clustering methods. Besides, by adding a reject option, the clustering accuracy of Dual-AAE can reach that of supervised CNN algorithms. Dual-AAE can also be used for disentangling style and content of images without using supervised information.



### Holistic Multi-View Building Analysis in the Wild with Projection Pooling
- **Arxiv ID**: http://arxiv.org/abs/2008.10041v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10041v3)
- **Published**: 2020-08-23 13:49:22+00:00
- **Updated**: 2020-12-19 20:59:07+00:00
- **Authors**: Zbigniew Wojna, Krzysztof Maziarz, Łukasz Jocz, Robert Pałuba, Robert Kozikowski, Iasonas Kokkinos
- **Comment**: Accepted for publication at the 35th AAAI Conference on Artificial
  Intelligence (AAAI 2021)
- **Journal**: None
- **Summary**: We address six different classification tasks related to fine-grained building attributes: construction type, number of floors, pitch and geometry of the roof, facade material, and occupancy class. Tackling such a remote building analysis problem became possible only recently due to growing large-scale datasets of urban scenes. To this end, we introduce a new benchmarking dataset, consisting of 49426 images (top-view and street-view) of 9674 buildings. These photos are further assembled, together with the geometric metadata. The dataset showcases various real-world challenges, such as occlusions, blur, partially visible objects, and a broad spectrum of buildings. We propose a new projection pooling layer, creating a unified, top-view representation of the top-view and the side views in a high-dimensional space. It allows us to utilize the building and imagery metadata seamlessly. Introducing this layer improves classification accuracy -- compared to highly tuned baseline models -- indicating its suitability for building analysis.



### Multi-Person Full Body Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.10060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10060v1)
- **Published**: 2020-08-23 15:47:13+00:00
- **Updated**: 2020-08-23 15:47:13+00:00
- **Authors**: Haoyi Zhu, Cheng Jie, Shaofei Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person pose estimation plays an important role in many fields. Although previous works have researched a lot on different parts of human pose estimation, full body pose estimation for multi-person still needs further research. Our work has developed an integrated model through knowledge distillation which can estimate full body poses. Trained based on the AlphaPose system and MSCOCO2017 dataset, our model achieves 51.5 mAP on the validation dataset annotated manually by ourselves. Related resources are available at https://esflfei.github.io/esflfei.gethub.io/website.html.



### Let me join you! Real-time F-formation recognition by a socially aware robot
- **Arxiv ID**: http://arxiv.org/abs/2008.10078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.10078v1)
- **Published**: 2020-08-23 17:46:08+00:00
- **Updated**: 2020-08-23 17:46:08+00:00
- **Authors**: Hrishav Bakul Barua, Pradip Pramanick, Chayan Sarkar, Theint Haythi Mg
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel architecture to detect social groups in real-time from a continuous image stream of an ego-vision camera. F-formation defines social orientations in space where two or more person tends to communicate in a social place. Thus, essentially, we detect F-formations in social gatherings such as meetings, discussions, etc. and predict the robot's approach angle if it wants to join the social group. Additionally, we also detect outliers, i.e., the persons who are not part of the group under consideration. Our proposed pipeline consists of -- a) a skeletal key points estimator (a total of 17) for the detected human in the scene, b) a learning model (using a feature vector based on the skeletal points) using CRF to detect groups of people and outlier person in a scene, and c) a separate learning model using a multi-class Support Vector Machine (SVM) to predict the exact F-formation of the group of people in the current scene and the angle of approach for the viewing robot. The system is evaluated using two data-sets. The results show that the group and outlier detection in a scene using our method establishes an accuracy of 91%. We have made rigorous comparisons of our systems with a state-of-the-art F-formation detection system and found that it outperforms the state-of-the-art by 29% for formation detection and 55% for combined detection of the formation and approach angle.



### Developing and Defeating Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2008.10106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10106v1)
- **Published**: 2020-08-23 21:00:33+00:00
- **Updated**: 2020-08-23 21:00:33+00:00
- **Authors**: Ian McDiarmid-Sterling, Allan Moser
- **Comment**: None
- **Journal**: None
- **Summary**: Breakthroughs in machine learning have resulted in state-of-the-art deep neural networks (DNNs) performing classification tasks in safety-critical applications. Recent research has demonstrated that DNNs can be attacked through adversarial examples, which are small perturbations to input data that cause the DNN to misclassify objects. The proliferation of DNNs raises important safety concerns about designing systems that are robust to adversarial examples. In this work we develop adversarial examples to attack the Yolo V3 object detector [1] and then study strategies to detect and neutralize these examples. Python code for this project is available at https://github.com/ianmcdiarmidsterling/adversarial



### Robust Vision Challenge 2020 -- 1st Place Report for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.10112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.10112v1)
- **Published**: 2020-08-23 21:41:43+00:00
- **Updated**: 2020-08-23 21:41:43+00:00
- **Authors**: Rohit Mohan, Abhinav Valada
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we present key details of our winning panoptic segmentation architecture EffPS_b1bs4_RVC. Our network is a lightweight version of our state-of-the-art EfficientPS architecture that consists of our proposed shared backbone with a modified EfficientNet-B5 model as the encoder, followed by the 2-way FPN to learn semantically rich multi-scale features. It consists of two task-specific heads, a modified Mask R-CNN instance head and our novel semantic segmentation head that processes features of different scales with specialized modules for coherent feature refinement. Finally, our proposed panoptic fusion module adaptively fuses logits from each of the heads to yield the panoptic segmentation output. The Robust Vision Challenge 2020 benchmarking results show that our model is ranked #1 on Microsoft COCO, VIPER and WildDash, and is ranked #2 on Cityscapes and Mapillary Vistas, thereby achieving the overall rank #1 for the panoptic segmentation task.



### Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment in Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2008.10123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.10123v1)
- **Published**: 2020-08-23 22:41:35+00:00
- **Updated**: 2020-08-23 22:41:35+00:00
- **Authors**: Yipu Zhao, Justin S. Smith, Patricio A. Vela
- **Comment**: 20 pages, 14 figures, 8 tables. Submitted to IEEE Transactions on
  Robotics, for the provided open-source software see
  https://github.com/ivalab/gf_orb_slam2
- **Journal**: None
- **Summary**: The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical characteristic of resource-limited applications. While hardware and algorithm advances have been significantly improved the cost-efficiency of VSLAM front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This paper describes a novel, rigorous method to improve the cost-efficiency of local BA in a BA-based VSLAM back-end. An efficient algorithm, called Good Graph, is developed to select size-reduced graphs optimized in local BA with condition preservation. To better suit BA-based VSLAM back-ends, the Good Graph predicts future estimation needs, dynamically assigns an appropriate size budget, and selects a condition-maximized subgraph for BA estimation. Evaluations are conducted on two scenarios: 1) VSLAM as standalone process, and 2) VSLAM as part of closed-loop navigation system. Results from the first scenario show Good Graph improves accuracy and robustness of VSLAM estimation, when computational limits exist. Results from the second scenario, indicate that Good Graph benefits the trajectory tracking performance of VSLAM-based closed-loop navigation systems, which is a primary application of VSLAM.



### m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.10134v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10134v2)
- **Published**: 2020-08-23 23:30:15+00:00
- **Updated**: 2020-12-10 21:34:59+00:00
- **Authors**: Salman Maqbool, Aqsa Riaz, Hasan Sajid, Osman Hasan
- **Comment**: 16 pages, 5 figures, Code available at:
  https://github.com/salmanmaq/segmentationNetworks, Dataset available at:
  https://www.kaggle.com/salmanmaq/m2caiseg
- **Journal**: None
- **Summary**: Autonomous surgical procedures, in particular minimal invasive surgeries, are the next frontier for Artificial Intelligence research. However, the existing challenges include precise identification of the human anatomy and the surgical settings, and modeling the environment for training of an autonomous agent. To address the identification of human anatomy and the surgical settings, we propose a deep learning based semantic segmentation algorithm to identify and label the tissues and organs in the endoscopic video feed of the human torso region. We present an annotated dataset, m2caiSeg, created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene. We propose and train a deep convolutional neural network for the semantic segmentation task. To cater for the low quantity of annotated data, we use unsupervised pre-training and data augmentation. The trained model is evaluated on an independent test set of the proposed dataset. We obtained a F1 score of 0.33 while using all the labeled categories for the semantic segmentation task. Secondly, we labeled all instruments into an 'Instruments' superclass to evaluate the model's performance on discerning the various organs and obtained a F1 score of 0.57. We propose a new dataset and a deep learning method for pixel level identification of various organs and instruments in a endoscopic surgical scene. Surgical scene understanding is one of the first steps towards automating surgical procedures.



