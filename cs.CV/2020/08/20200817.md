# Arxiv Papers in cs.CV on 2020-08-17
### Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors
- **Arxiv ID**: http://arxiv.org/abs/2008.07043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07043v2)
- **Published**: 2020-08-17 00:56:50+00:00
- **Updated**: 2020-08-29 06:38:18+00:00
- **Authors**: Jingru Yi, Pengxiang Wu, Bo Liu, Qiaoying Huang, Hui Qu, Dimitris Metaxas
- **Comment**: Accepted to WACV2021
- **Journal**: None
- **Summary**: Oriented object detection in aerial images is a challenging task as the objects in aerial images are displayed in arbitrary directions and are usually densely packed. Current oriented object detection methods mainly rely on two-stage anchor-based detectors. However, the anchor-based detectors typically suffer from a severe imbalance issue between the positive and negative anchor boxes. To address this issue, in this work we extend the horizontal keypoint-based object detector to the oriented object detection task. In particular, we first detect the center keypoints of the objects, based on which we then regress the box boundary-aware vectors (BBAVectors) to capture the oriented bounding boxes. The box boundary-aware vectors are distributed in the four quadrants of a Cartesian coordinate system for all arbitrarily oriented objects. To relieve the difficulty of learning the vectors in the corner cases, we further classify the oriented bounding boxes into horizontal and rotational bounding boxes. In the experiment, we show that learning the box boundary-aware vectors is superior to directly predicting the width, height, and angle of an oriented bounding box, as adopted in the baseline method. Besides, the proposed method competes favorably with state-of-the-art methods. Code is available at https://github.com/yijingru/BBAVectors-Oriented-Object-Detection.



### Video Region Annotation with Sparse Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/2008.07049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07049v1)
- **Published**: 2020-08-17 01:27:20+00:00
- **Updated**: 2020-08-17 01:27:20+00:00
- **Authors**: Yuzheng Xu, Yang Wu, Nur Sabrina binti Zuraimi, Shohei Nobuhara, Ko Nishino
- **Comment**: Accepted for publication in BMVC 2020 (Oral)
- **Journal**: None
- **Summary**: Video analysis has been moving towards more detailed interpretation (e.g. segmentation) with encouraging progresses. These tasks, however, increasingly rely on densely annotated training data both in space and time. Since such annotation is labour-intensive, few densely annotated video data with detailed region boundaries exist. This work aims to resolve this dilemma by learning to automatically generate region boundaries for all frames of a video from sparsely annotated bounding boxes of target regions. We achieve this with a Volumetric Graph Convolutional Network (VGCN), which learns to iteratively find keypoints on the region boundaries using the spatio-temporal volume of surrounding appearance and motion. The global optimization of VGCN makes it significantly stronger and generalize better than existing solutions. Experimental results using two latest datasets (one real and one synthetic), including ablation studies, demonstrate the effectiveness and superiority of our method.



### Progressively Guided Alternate Refinement Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.07064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07064v1)
- **Published**: 2020-08-17 02:55:06+00:00
- **Updated**: 2020-08-17 02:55:06+00:00
- **Authors**: Shuhan Chen, Yun Fu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we aim to develop an efficient and compact deep network for RGB-D salient object detection, where the depth image provides complementary information to boost performance in complex scenarios. Starting from a coarse initial prediction by a multi-scale residual block, we propose a progressively guided alternate refinement network to refine it. Instead of using ImageNet pre-trained backbone network, we first construct a lightweight depth stream by learning from scratch, which can extract complementary features more efficiently with less redundancy. Then, different from the existing fusion based methods, RGB and depth features are fed into proposed guided residual (GR) blocks alternately to reduce their mutual degradation. By assigning progressive guidance in the stacked GR blocks within each side-output, the false detection and missing parts can be well remedied. Extensive experiments on seven benchmark datasets demonstrate that our model outperforms existing state-of-the-art approaches by a large margin, and also shows superiority in efficiency (71 FPS) and model size (64.9 MB).



### Towards Cardiac Intervention Assistance: Hardware-aware Neural Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.07071v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07071v2)
- **Published**: 2020-08-17 03:22:57+00:00
- **Updated**: 2020-12-14 01:11:50+00:00
- **Authors**: Dewen Zeng, Weiwen Jiang, Tianchen Wang, Xiaowei Xu, Haiyun Yuan, Meiping Huang, Jian Zhuang, Jingtong Hu, Yiyu Shi
- **Comment**: 8 pages, conference
- **Journal**: None
- **Summary**: Real-time cardiac magnetic resonance imaging (MRI) plays an increasingly important role in guiding various cardiac interventions. In order to provide better visual assistance, the cine MRI frames need to be segmented on-the-fly to avoid noticeable visual lag. In addition, considering reliability and patient data privacy, the computation is preferably done on local hardware. State-of-the-art MRI segmentation methods mostly focus on accuracy only, and can hardly be adopted for real-time application or on local hardware. In this work, we present the first hardware-aware multi-scale neural architecture search (NAS) framework for real-time 3D cardiac cine MRI segmentation. The proposed framework incorporates a latency regularization term into the loss function to handle real-time constraints, with the consideration of underlying hardware. In addition, the formulation is fully differentiable with respect to the architecture parameters, so that stochastic gradient descent (SGD) can be used for optimization to reduce the computation cost while maintaining optimization quality. Experimental results on ACDC MICCAI 2017 dataset demonstrate that our hardware-aware multi-scale NAS framework can reduce the latency by up to 3.5 times and satisfy the real-time constraints, while still achieving competitive segmentation accuracy, compared with the state-of-the-art NAS segmentation framework.



### AlphaNet: Improving Long-Tail Classification By Combining Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2008.07073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07073v2)
- **Published**: 2020-08-17 03:31:39+00:00
- **Updated**: 2023-07-26 04:03:47+00:00
- **Authors**: Nadine Chang, Jayanth Koushik, Aarti Singh, Martial Hebert, Yu-Xiong Wang, Michael J. Tarr
- **Comment**: None
- **Journal**: None
- **Summary**: Methods in long-tail learning focus on improving performance for data-poor (rare) classes; however, performance for such classes remains much lower than performance for more data-rich (frequent) classes. Analyzing the predictions of long-tail methods for rare classes reveals that a large number of errors are due to misclassification of rare items as visually similar frequent classes. To address this problem, we introduce AlphaNet, a method that can be applied to existing models, performing post hoc correction on classifiers of rare classes. Starting with a pre-trained model, we find frequent classes that are closest to rare classes in the model's representation space and learn weights to update rare class classifiers with a linear combination of frequent class classifiers. AlphaNet, applied to several models, greatly improves test accuracy for rare classes in multiple long-tailed datasets, with very little change to overall accuracy. Our method also provides a way to control the trade-off between rare class and overall accuracy, making it practical for long-tail classification in the wild.



### Edge Network-Assisted Real-Time Object Detection Framework for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.07083v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07083v1)
- **Published**: 2020-08-17 04:35:20+00:00
- **Updated**: 2020-08-17 04:35:20+00:00
- **Authors**: Seung Wook Kim, Keunsoo Ko, Haneul Ko, Victor C. M. Leung
- **Comment**: This paper will be published in IEEE Network
- **Journal**: None
- **Summary**: Autonomous vehicles (AVs) can achieve the desired results within a short duration by offloading tasks even requiring high computational power (e.g., object detection (OD)) to edge clouds. However, although edge clouds are exploited, real-time OD cannot always be guaranteed due to dynamic channel quality. To mitigate this problem, we propose an edge network-assisted real-time OD framework~(EODF). In an EODF, AVs extract the region of interests~(RoIs) of the captured image when the channel quality is not sufficiently good for supporting real-time OD. Then, AVs compress the image data on the basis of the RoIs and transmit the compressed one to the edge cloud. In so doing, real-time OD can be achieved owing to the reduced transmission latency. To verify the feasibility of our framework, we evaluate the probability that the results of OD are not received within the inter-frame duration (i.e., outage probability) and their accuracy. From the evaluation, we demonstrate that the proposed EODF provides the results to AVs in real-time and achieves satisfactory accuracy.



### Spherical coordinates transformation pre-processing in Deep Convolution Neural Networks for brain tumor segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.07090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07090v1)
- **Published**: 2020-08-17 05:11:05+00:00
- **Updated**: 2020-08-17 05:11:05+00:00
- **Authors**: Carlo Russo, Sidong Liu, Antonio Di Ieva
- **Comment**: 26 pages, 5 figures. Submitted to Computer Methods and Programs in
  Biomedicine
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is used in everyday clinical practice to assess brain tumors. Several automatic or semi-automatic segmentation algorithms have been introduced to segment brain tumors and achieve an expert-like accuracy. Deep Convolutional Neural Networks (DCNN) have recently shown very promising results, however, DCNN models are still far from achieving clinically meaningful results mainly because of the lack of generalization of the models. DCNN models need large annotated datasets to achieve good performance. Models are often optimized on the domain dataset on which they have been trained, and then fail the task when the same model is applied to different datasets from different institutions. One of the reasons is due to the lack of data standardization to adjust for different models and MR machines. In this work, a 3D Spherical coordinates transform during the pre-processing phase has been hypothesized to improve DCNN models' accuracy and to allow more generalizable results even when the model is trained on small and heterogeneous datasets and translated into different domains. Indeed, the spherical coordinate system avoids several standardization issues since it works independently of resolution and imaging settings. Both Cartesian and spherical volumes were evaluated in two DCNN models with the same network structure using the BraTS 2019 dataset. The model trained on spherical transform pre-processed inputs resulted in superior performance over the Cartesian-input trained model on predicting gliomas' segmentation on tumor core and enhancing tumor classes (increase of 0.011 and 0.014 respectively on the validation dataset), achieving a further improvement in accuracy by merging the two models together. Furthermore, the spherical transform is not resolution-dependent and achieve same results on different input resolution.



### WSRNet: Joint Spotting and Recognition of Handwritten Words
- **Arxiv ID**: http://arxiv.org/abs/2008.07109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07109v1)
- **Published**: 2020-08-17 06:22:05+00:00
- **Updated**: 2020-08-17 06:22:05+00:00
- **Authors**: George Retsinas, Giorgos Sfikas, Petros Maragos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a unified model that can handle both Keyword Spotting and Word Recognition with the same network architecture. The proposed network is comprised of a non-recurrent CTC branch and a Seq2Seq branch that is further augmented with an Autoencoding module. The related joint loss leads to a boost in recognition performance, while the Seq2Seq branch is used to create efficient word representations. We show how to further process these representations with binarization and a retraining scheme to provide compact and highly efficient descriptors, suitable for keyword spotting. Numerical results validate the usefulness of the proposed architecture, as our method outperforms the previous state-of-the-art in keyword spotting, and provides results in the ballpark of the leading methods for word recognition.



### Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs
- **Arxiv ID**: http://arxiv.org/abs/2008.07119v2
- **DOI**: 10.1016/j.cad.2022.103225
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07119v2)
- **Published**: 2020-08-17 06:50:47+00:00
- **Updated**: 2021-02-16 07:04:49+00:00
- **Authors**: Seowoo Jang, Soyoung Yoo, Namwoo Kang
- **Comment**: None
- **Journal**: Computer-Aided Design, 146, 103225 (2022)
- **Summary**: Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.



### Reversing the cycle: self-supervised deep stereo through enhanced monocular distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.07130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07130v1)
- **Published**: 2020-08-17 07:40:22+00:00
- **Updated**: 2020-08-17 07:40:22+00:00
- **Authors**: Filippo Aleotti, Fabio Tosi, Li Zhang, Matteo Poggi, Stefano Mattoccia
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In many fields, self-supervised learning solutions are rapidly evolving and filling the gap with supervised approaches. This fact occurs for depth estimation based on either monocular or stereo, with the latter often providing a valid source of self-supervision for the former. In contrast, to soften typical stereo artefacts, we propose a novel self-supervised paradigm reversing the link between the two. Purposely, in order to train deep stereo networks, we distill knowledge through a monocular completion network. This architecture exploits single-image clues and few sparse points, sourced by traditional stereo algorithms, to estimate dense yet accurate disparity maps by means of a consensus mechanism over multiple estimations. We thoroughly evaluate with popular stereo datasets the impact of different supervisory signals showing how stereo networks trained with our paradigm outperform existing self-supervised frameworks. Finally, our proposal achieves notable generalization capabilities dealing with domain shift issues. Code available at https://github.com/FilippoAleotti/Reversing



### Fast and Robust Face-to-Parameter Translation for Game Character Auto-Creation
- **Arxiv ID**: http://arxiv.org/abs/2008.07132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07132v1)
- **Published**: 2020-08-17 07:45:31+00:00
- **Updated**: 2020-08-17 07:45:31+00:00
- **Authors**: Tianyang Shi, Zhengxia Zou, Yi Yuan, Changjie Fan
- **Comment**: Accepted by AAAI 2020 with supplementary material
- **Journal**: None
- **Summary**: With the rapid development of Role-Playing Games (RPGs), players are now allowed to edit the facial appearance of their in-game characters with their preferences rather than using default templates. This paper proposes a game character auto-creation framework that generates in-game characters according to a player's input face photo. Different from the previous methods that are designed based on neural style transfer or monocular 3D face reconstruction, we re-formulate the character auto-creation process in a different point of view: by predicting a large set of physically meaningful facial parameters under a self-supervised learning paradigm. Instead of updating facial parameters iteratively at the input end of the renderer as suggested by previous methods, which are time-consuming, we introduce a facial parameter translator so that the creation can be done efficiently through a single forward propagation from the face embeddings to parameters, with a considerable 1000x computational speedup. Despite its high efficiency, the interactivity is preserved in our method where users are allowed to optionally fine-tune the facial parameters on our creation according to their needs. Our approach also shows better robustness than previous methods, especially for those photos with head-pose variance. Comparison results and ablation analysis on seven public face verification datasets suggest the effectiveness of our method.



### AID: Pushing the Performance Boundary of Human Pose Estimation with Information Dropping Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.07139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07139v2)
- **Published**: 2020-08-17 08:05:16+00:00
- **Updated**: 2020-11-17 08:07:12+00:00
- **Authors**: Junjie Huang, Zheng Zhu, Guan Huang, Dalong Du
- **Comment**: None
- **Journal**: None
- **Summary**: Both appearance cue and constraint cue are vital for human pose estimation. However, there is a tendency in most existing works to overfitting the former and overlook the latter. In this paper, we propose Augmentation by Information Dropping (AID) to verify and tackle this dilemma. Alone with AID as a prerequisite for effectively exploiting its potential, we propose customized training schedules, which are designed by analyzing the pattern of loss and performance in training process from the perspective of information supplying. In experiments, as a model-agnostic approach, AID promotes various state-of-the-art methods in both bottom-up and top-down paradigms with different input sizes, frameworks, backbones, training and testing sets. On popular COCO human pose estimation test set, AID consistently boosts the performance of different configurations by around 0.6 AP in top-down paradigm and up to 1.5 AP in bottom-up paradigm. On more challenging CrowdPose dataset, the improvement is more than 1.5 AP. As AID successfully pushes the performance boundary of human pose estimation problem by considerable margin and sets a new state-of-the-art, we hope AID to be a regular configuration for training human pose estimators. The source code will be publicly available for further research.



### Multi-organ Segmentation via Co-training Weight-averaged Models from Few-organ Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.07149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07149v1)
- **Published**: 2020-08-17 08:39:16+00:00
- **Updated**: 2020-08-17 08:39:16+00:00
- **Authors**: Rui Huang, Yuanjie Zheng, Zhiqiang Hu, Shaoting Zhang, Hongsheng Li
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Multi-organ segmentation has extensive applications in many clinical applications. To segment multiple organs of interest, it is generally quite difficult to collect full annotations of all the organs on the same images, as some medical centers might only annotate a portion of the organs due to their own clinical practice. In most scenarios, one might obtain annotations of a single or a few organs from one training set, and obtain annotations of the the other organs from another set of training images. Existing approaches mostly train and deploy a single model for each subset of organs, which are memory intensive and also time inefficient. In this paper, we propose to co-train weight-averaged models for learning a unified multi-organ segmentation network from few-organ datasets. We collaboratively train two networks and let the coupled networks teach each other on un-annotated organs. To alleviate the noisy teaching supervisions between the networks, the weighted-averaged models are adopted to produce more reliable soft labels. In addition, a novel region mask is utilized to selectively apply the consistent constraint on the un-annotated organ regions that require collaborative teaching, which further boosts the performance. Extensive experiments on three public available single-organ datasets LiTS, KiTS, Pancreas and manually-constructed single-organ datasets from MOBA show that our method can better utilize the few-organ datasets and achieves superior performance with less inference computational cost.



### Neutral Face Game Character Auto-Creation via PokerFace-GAN
- **Arxiv ID**: http://arxiv.org/abs/2008.07154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07154v1)
- **Published**: 2020-08-17 08:43:48+00:00
- **Updated**: 2020-08-17 08:43:48+00:00
- **Authors**: Tianyang Shi, Zhengxia Zou, Xinhui Song, Zheng Song, Changjian Gu, Changjie Fan, Yi Yuan
- **Comment**: Accepted by ACMMM 2020
- **Journal**: None
- **Summary**: Game character customization is one of the core features of many recent Role-Playing Games (RPGs), where players can edit the appearance of their in-game characters with their preferences. This paper studies the problem of automatically creating in-game characters with a single photo. In recent literature on this topic, neural networks are introduced to make game engine differentiable and the self-supervised learning is used to predict facial customization parameters. However, in previous methods, the expression parameters and facial identity parameters are highly coupled with each other, making it difficult to model the intrinsic facial features of the character. Besides, the neural network based renderer used in previous methods is also difficult to be extended to multi-view rendering cases. In this paper, considering the above problems, we propose a novel method named "PokerFace-GAN" for neutral face game character auto-creation. We first build a differentiable character renderer which is more flexible than the previous methods in multi-view rendering cases. We then take advantage of the adversarial training to effectively disentangle the expression parameters from the identity parameters and thus generate player-preferred neutral face (expression-less) characters. Since all components of our method are differentiable, our method can be easily trained under a multi-task self-supervised learning paradigm. Experiment results show that our method can generate vivid neutral face game characters that are highly similar to the input photos. The effectiveness of our method is verified by comparison results and ablation studies.



### Deep Neural Networks for automatic extraction of features in time series satellite images
- **Arxiv ID**: http://arxiv.org/abs/2008.08432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08432v1)
- **Published**: 2020-08-17 09:26:52+00:00
- **Updated**: 2020-08-17 09:26:52+00:00
- **Authors**: Gael Kamdem De Teyou, Yuliya Tarabalka, Isabelle Manighetti, Rafael Almar, Sebastien Tripod
- **Comment**: None
- **Journal**: None
- **Summary**: Many earth observation programs such as Landsat, Sentinel, SPOT, and Pleiades produce huge volume of medium to high resolution multi-spectral images every day that can be organized in time series. In this work, we exploit both temporal and spatial information provided by these images to generate land cover maps. For this purpose, we combine a fully convolutional neural network with a convolutional long short-term memory. Implementation details of the proposed spatio-temporal neural network architecture are provided. Experimental results show that the temporal information provided by time series images allows increasing the accuracy of land cover classification, thus producing up-to-date maps that can help in identifying changes on earth.



### DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2008.07173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07173v1)
- **Published**: 2020-08-17 09:30:28+00:00
- **Updated**: 2020-08-17 09:30:28+00:00
- **Authors**: Chu-Tak Li, Wan-Chi Siu, Zhi-Song Liu, Li-Wen Wang, Daniel Pak-Kong Lun
- **Comment**: 17 pages (14 pages of main content + 3 pages references), 6 figures,
  2 tables, submitted to ECCV'20
- **Journal**: None
- **Summary**: The degree of difficulty in image inpainting depends on the types and sizes of the missing parts. Existing image inpainting approaches usually encounter difficulties in completing the missing parts in the wild with pleasing visual and contextual results as they are trained for either dealing with one specific type of missing patterns (mask) or unilaterally assuming the shapes and/or sizes of the masked areas. We propose a deep generative inpainting network, named DeepGIN, to handle various types of masked images. We design a Spatial Pyramid Dilation (SPD) ResNet block to enable the use of distant features for reconstruction. We also employ Multi-Scale Self-Attention (MSSA) mechanism and Back Projection (BP) technique to enhance our inpainting results. Our DeepGIN outperforms the state-of-the-art approaches generally, including two publicly available datasets (FFHQ and Oxford Buildings), both quantitatively and qualitatively. We also demonstrate that our model is capable of completing masked images in the wild.



### White blood cell classification
- **Arxiv ID**: http://arxiv.org/abs/2008.07181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07181v2)
- **Published**: 2020-08-17 09:48:12+00:00
- **Updated**: 2020-09-04 03:06:29+00:00
- **Authors**: Na Dong, Meng-die Zhai, Jian-fang Chang, Chun-ho Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel automatic classification framework for the recognition of five types of white blood cells. Segmenting complete white blood cells from blood smears images and extracting advantageous features from them remain challenging tasks in the classification of white blood cells. Therefore, we present an adaptive threshold segmentation method to deal with blood smears images with non-uniform color and uneven illumination, which is designed based on color space information and threshold segmentation. Subsequently, after successfully separating the white blood cell from the blood smear image, a large number of nonlinear features including geometrical, color and texture features are extracted. Nevertheless, redundant features can affect the classification speed and efficiency, and in view of that, a feature selection algorithm based on classification and regression trees (CART) is designed. Through in-depth analysis of the nonlinear relationship between features, the irrelevant and redundant features are successfully removed from the initial nonlinear features. Afterwards, the selected prominent features are fed into particle swarm optimization support vector machine (PSO-SVM) classifier to recognize the types of the white blood cells. Finally, to evaluate the performance of the proposed white blood cell classification methodology, we build a white blood cell data set containing 500 blood smear images for experiments. By comparing with the ground truth obtained manually, the proposed segmentation method achieves an average of 95.98% and 97.57% dice similarity for segmented nucleus and cell regions respectively. Furthermore, the proposed methodology achieves 99.76% classification accuracy, which well demonstrates its effectiveness.



### Category-Level 3D Non-Rigid Registration from Single-View RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2008.07203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.07203v1)
- **Published**: 2020-08-17 10:35:19+00:00
- **Updated**: 2020-08-17 10:35:19+00:00
- **Authors**: Diego Rodriguez, Florian Huber, Sven Behnke
- **Comment**: Accepted final version. In 2020 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach to solve the 3D non-rigid registration problem from RGB images using Convolutional Neural Networks (CNNs). Our objective is to find a deformation field (typically used for transferring knowledge between instances, e.g., grasping skills) that warps a given 3D canonical model into a novel instance observed by a single-view RGB image. This is done by training a CNN that infers a deformation field for the visible parts of the canonical model and by employing a learned shape (latent) space for inferring the deformations of the occluded parts. As result of the registration, the observed model is reconstructed. Because our method does not need depth information, it can register objects that are typically hard to perceive with RGB-D sensors, e.g. with transparent or shiny surfaces. Even without depth data, our approach outperforms the Coherent Point Drift (CPD) registration method for the evaluated object categories.



### Multi-label Learning with Missing Values using Combined Facial Action Unit Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.07234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07234v1)
- **Published**: 2020-08-17 11:58:06+00:00
- **Updated**: 2020-08-17 11:58:06+00:00
- **Authors**: Jaspar Pahl, Ines Rieger, Dominik Seuss
- **Comment**: Presented at the first Workshop on the Art of Learning with Missing
  Values (Artemiss) hosted by the 37th International Conference on Machine
  Learning (ICML) 2020
- **Journal**: None
- **Summary**: Facial action units allow an objective, standardized description of facial micro movements which can be used to describe emotions in human faces. Annotating data for action units is an expensive and time-consuming task, which leads to a scarce data situation. By combining multiple datasets from different studies, the amount of training data for a machine learning algorithm can be increased in order to create robust models for automated, multi-label action unit detection. However, every study annotates different action units, leading to a tremendous amount of missing labels in a combined database. In this work, we examine this challenge and present our approach to create a combined database and an algorithm capable of learning under the presence of missing labels without inferring their values. Our approach shows competitive performance compared to recent competitions in action unit detection.



### Self-Supervised Learning for Monocular Depth Estimation from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2008.07246v1
- **DOI**: 10.5194/isprs-annals-V-2-2020-357-2020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07246v1)
- **Published**: 2020-08-17 12:20:46+00:00
- **Updated**: 2020-08-17 12:20:46+00:00
- **Authors**: Max Hermann, Boitumelo Ruf, Martin Weinmann, Stefan Hinz
- **Comment**: None
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., V-2-2020,
  357-364, 2020
- **Summary**: Supervised learning based methods for monocular depth estimation usually require large amounts of extensively annotated training data. In the case of aerial imagery, this ground truth is particularly difficult to acquire. Therefore, in this paper, we present a method for self-supervised learning for monocular depth estimation from aerial imagery that does not require annotated training data. For this, we only use an image sequence from a single moving camera and learn to simultaneously estimate depth and pose information. By sharing the weights between pose and depth estimation, we achieve a relatively small model, which favors real-time application. We evaluate our approach on three diverse datasets and compare the results to conventional methods that estimate depth maps based on multi-view geometry. We achieve an accuracy {\delta}1.25 of up to 93.5 %. In addition, we have paid particular attention to the generalization of a trained model to unknown data and the self-improving capabilities of our approach. We conclude that, even though the results of monocular depth estimation are inferior to those achieved by conventional methods, they are well suited to provide a good initialization for methods that rely on image matching or to provide estimates in regions where image matching fails, e.g. occluded or texture-less regions.



### An Improved Dilated Convolutional Network for Herd Counting in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/2008.07254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07254v1)
- **Published**: 2020-08-17 12:31:10+00:00
- **Updated**: 2020-08-17 12:31:10+00:00
- **Authors**: Soufien Hamrouni, Hakim Ghazzai, Hamid Menouar, Yahya Massoud
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd management technologies that leverage computer vision are widespread in contemporary times. There exists many security-related applications of these methods, including, but not limited to: following the flow of an array of people and monitoring large gatherings. In this paper, we propose an accurate monitoring system composed of two concatenated convolutional deep learning architectures. The first part called Front-end, is responsible for converting bi-dimensional signals and delivering high-level features. The second part, called the Back-end, is a dilated Convolutional Neural Network (CNN) used to replace pooling layers. It is responsible for enlarging the receptive field of the whole network and converting the descriptors provided by the first network to a saliency map that will be utilized to estimate the number of people in highly congested images. We also propose to utilize a genetic algorithm in order to find an optimized dilation rate configuration in the back-end. The proposed model is shown to converge 30\% faster than state-of-the-art approaches. It is also shown that it achieves 20\% lower Mean Absolute Error (MAE) when applied to the Shanghai data~set.



### MLBF-Net: A Multi-Lead-Branch Fusion Network for Multi-Class Arrhythmia Classification Using 12-Lead ECG
- **Arxiv ID**: http://arxiv.org/abs/2008.07263v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07263v1)
- **Published**: 2020-08-17 12:51:39+00:00
- **Updated**: 2020-08-17 12:51:39+00:00
- **Authors**: Jing Zhang, Deng Liang, Aiping Liu, Min Gao, Xiang Chen, Xu Zhang, Xun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic arrhythmia detection using 12-lead electrocardiogram (ECG) signal plays a critical role in early prevention and diagnosis of cardiovascular diseases. In the previous studies on automatic arrhythmia detection, most methods concatenated 12 leads of ECG into a matrix, and then input the matrix to a variety of feature extractors or deep neural networks for extracting useful information. Under such frameworks, these methods had the ability to extract comprehensive features (known as integrity) of 12-lead ECG since the information of each lead interacts with each other during training. However, the diverse lead-specific features (known as diversity) among 12 leads were neglected, causing inadequate information learning for 12-lead ECG. To maximize the information learning of multi-lead ECG, the information fusion of comprehensive features with integrity and lead-specific features with diversity should be taken into account. In this paper, we propose a novel Multi-Lead-Branch Fusion Network (MLBF-Net) architecture for arrhythmia classification by integrating multi-loss optimization to jointly learning diversity and integrity of multi-lead ECG. MLBF-Net is composed of three components: 1) multiple lead-specific branches for learning the diversity of multi-lead ECG; 2) cross-lead features fusion by concatenating the output feature maps of all branches for learning the integrity of multi-lead ECG; 3) multi-loss co-optimization for all the individual branches and the concatenated network. We demonstrate our MLBF-Net on China Physiological Signal Challenge 2018 which is an open 12-lead ECG dataset. The experimental results show that MLBF-Net obtains an average $F_1$ score of 0.855, reaching the highest arrhythmia classification performance. The proposed method provides a promising solution for multi-lead ECG analysis from an information fusion perspective.



### AP-Loss for Accurate One-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.07294v1
- **DOI**: 10.1109/TPAMI.2020.2991457
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07294v1)
- **Published**: 2020-08-17 13:22:01+00:00
- **Updated**: 2020-08-17 13:22:01+00:00
- **Authors**: Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, Junni Zou
- **Comment**: Accepted to IEEE TPAMI. arXiv admin note: substantial text overlap
  with arXiv:1904.06373
- **Journal**: None
- **Summary**: One-stage object detectors are trained by optimizing classification-loss and localization-loss simultaneously, with the former suffering much from extreme foreground-background class imbalance issue due to the large number of anchors. This paper alleviates this issue by proposing a novel framework to replace the classification task in one-stage detectors with a ranking task, and adopting the Average-Precision loss (AP-loss) for the ranking problem. Due to its non-differentiability and non-convexity, the AP-loss cannot be optimized directly. For this purpose, we develop a novel optimization algorithm, which seamlessly combines the error-driven update scheme in perceptron learning and backpropagation algorithm in deep networks. We provide in-depth analyses on the good convergence property and computational complexity of the proposed algorithm, both theoretically and empirically. Experimental results demonstrate notable improvement in addressing the imbalance issue in object detection over existing AP-based optimization algorithms. An improved state-of-the-art performance is achieved in one-stage detectors based on AP-loss over detectors using classification-losses on various standard benchmarks. The proposed framework is also highly versatile in accommodating different network architectures. Code is available at https://github.com/cccorn/AP-loss .



### First U-Net Layers Contain More Domain Specific Information Than The Last Ones
- **Arxiv ID**: http://arxiv.org/abs/2008.07357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07357v1)
- **Published**: 2020-08-17 14:31:10+00:00
- **Updated**: 2020-08-17 14:31:10+00:00
- **Authors**: Boris Shirokikh, Ivan Zakazov, Alexey Chernyavskiy, Irina Fedulova, Mikhail Belyaev
- **Comment**: Accepted to DART workshop at MICCAI-2020
- **Journal**: None
- **Summary**: MRI scans appearance significantly depends on scanning protocols and, consequently, the data-collection institution. These variations between clinical sites result in dramatic drops of CNN segmentation quality on unseen domains. Many of the recently proposed MRI domain adaptation methods operate with the last CNN layers to suppress domain shift. At the same time, the core manifestation of MRI variability is a considerable diversity of image intensities. We hypothesize that these differences can be eliminated by modifying the first layers rather than the last ones. To validate this simple idea, we conducted a set of experiments with brain MRI scans from six domains. Our results demonstrate that 1) domain-shift may deteriorate the quality even for a simple brain extraction segmentation task (surface Dice Score drops from 0.85-0.89 even to 0.09); 2) fine-tuning of the first layers significantly outperforms fine-tuning of the last layers in almost all supervised domain adaptation setups. Moreover, fine-tuning of the first layers is a better strategy than fine-tuning of the whole network, if the amount of annotated data from the new domain is strictly limited.



### SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.07358v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07358v1)
- **Published**: 2020-08-17 14:32:35+00:00
- **Updated**: 2020-08-17 14:32:35+00:00
- **Authors**: Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari
- **Comment**: accepted in ECCV 2020 as oral
- **Journal**: None
- **Summary**: Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature -- points are stored in an unordered way -- makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling. For the decoder stage, we propose regional convolutions, a novel operator aimed at maximizing the global activation entropy. Furthermore, inspired by the local refining procedure in Point Completion Network (PCN), we also propose a patch-deforming operation to simulate deconvolutional operations for point clouds. This paper proves that our regional activation can be incorporated in many point cloud architectures like AtlasNet and PCN, leading to better performance for geometric completion. We evaluate our approach on different 3D tasks such as object completion and classification, achieving state-of-the-art accuracy.



### Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07404v4
- **DOI**: 10.1016/j.cviu.2021.103219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07404v4)
- **Published**: 2020-08-17 15:25:40+00:00
- **Updated**: 2021-06-22 15:29:28+00:00
- **Authors**: Chiara Plizzari, Marco Cannici, Matteo Matteucci
- **Comment**: Accepted at Computer Vision and Image Understanding (CVIU) 12 pages,
  8 figures
- **Journal**: Computer Vision and Image Understanding, Volumes 208-209 (2021),
  103219, ISSN 1077-3142
- **Summary**: Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints' coordinates as input, and results on-par with state-of-the-art when adding bones information.



### Improving Emergency Response during Hurricane Season using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2008.07418v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07418v2)
- **Published**: 2020-08-17 15:42:02+00:00
- **Updated**: 2020-09-08 19:51:37+00:00
- **Authors**: Marc Bosch, Christian Conroy, Benjamin Ortiz, Philip Bogden
- **Comment**: None
- **Journal**: None
- **Summary**: We have developed a framework for crisis response and management that incorporates the latest technologies in computer vision (CV), inland flood prediction, damage assessment and data visualization. The framework uses data collected before, during, and after the crisis to enable rapid and informed decision making during all phases of disaster response. Our computer-vision model analyzes spaceborne and airborne imagery to detect relevant features during and after a natural disaster and creates metadata that is transformed into actionable information through web-accessible mapping tools. In particular, we have designed an ensemble of models to identify features including water, roads, buildings, and vegetation from the imagery. We have investigated techniques to bootstrap and reduce dependency on large data annotation efforts by adding use of open source labels including OpenStreetMaps and adding complementary data sources including Height Above Nearest Drainage (HAND) as a side channel to the network's input to encourage it to learn other features orthogonal to visual characteristics. Modeling efforts include modification of connected U-Nets for (1) semantic segmentation, (2) flood line detection, and (3) for damage assessment. In particular for the case of damage assessment, we added a second encoder to U-Net so that it could learn pre-event and post-event image features simultaneously. Through this method, the network is able to learn the difference between the pre- and post-disaster images, and therefore more effectively classify the level of damage. We have validated our approaches using publicly available data from the National Oceanic and Atmospheric Administration (NOAA)'s Remote Sensing Division, which displays the city and street-level details as mosaic tile images as well as data released as part of the Xview2 challenge.



### Siloed Federated Learning for Multi-Centric Histopathology Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.07424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07424v1)
- **Published**: 2020-08-17 15:49:30+00:00
- **Updated**: 2020-08-17 15:49:30+00:00
- **Authors**: Mathieu Andreux, Jean Ogier du Terrail, Constance Beguier, Eric W. Tramel
- **Comment**: Accepted to MICCAI 2020 DCL workshop
- **Journal**: None
- **Summary**: While federated learning is a promising approach for training deep learning models over distributed sensitive datasets, it presents new challenges for machine learning, especially when applied in the medical domain where multi-centric data heterogeneity is common. Building on previous domain adaptation works, this paper proposes a novel federated learning approach for deep learning architectures via the introduction of local-statistic batch normalization (BN) layers, resulting in collaboratively-trained, yet center-specific models. This strategy improves robustness to data heterogeneity while also reducing the potential for information leaks by not sharing the center-specific layer activation statistics. We benchmark the proposed method on the classification of tumorous histopathology image patches extracted from the Camelyon16 and Camelyon17 datasets. We show that our approach compares favorably to previous state-of-the-art methods, especially for transfer learning across datasets.



### Hey Human, If your Facial Emotions are Uncertain, You Should Use Bayesian Neural Networks!
- **Arxiv ID**: http://arxiv.org/abs/2008.07426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.07426v1)
- **Published**: 2020-08-17 15:50:40+00:00
- **Updated**: 2020-08-17 15:50:40+00:00
- **Authors**: Maryam Matin, Matias Valdenegro-Toro
- **Comment**: 10 pages, 7 figures, Women in Computer Vision @ ECCV 2020 camera
  ready
- **Journal**: None
- **Summary**: Facial emotion recognition is the task to classify human emotions in face images. It is a difficult task due to high aleatoric uncertainty and visual ambiguity. A large part of the literature aims to show progress by increasing accuracy on this task, but this ignores the inherent uncertainty and ambiguity in the task. In this paper we show that Bayesian Neural Networks, as approximated using MC-Dropout, MC-DropConnect, or an Ensemble, are able to model the aleatoric uncertainty in facial emotion recognition, and produce output probabilities that are closer to what a human expects. We also show that calibration metrics show strange behaviors for this task, due to the multiple classes that can be considered correct, which motivates future work. We believe our work will motivate other researchers to move away from Classical and into Bayesian Neural Networks.



### Zero Shot Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2008.07443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07443v1)
- **Published**: 2020-08-17 16:04:03+00:00
- **Updated**: 2020-08-17 16:04:03+00:00
- **Authors**: Udit Maniyar, Joseph K J, Aniket Anand Deshmukh, Urun Dogan, Vineeth N Balasubramanian
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: Standard supervised learning setting assumes that training data and test data come from the same distribution (domain). Domain generalization (DG) methods try to learn a model that when trained on data from multiple domains, would generalize to a new unseen domain. We extend DG to an even more challenging setting, where the label space of the unseen domain could also change. We introduce this problem as Zero-Shot Domain Generalization (to the best of our knowledge, the first such effort), where the model generalizes across new domains and also across new classes in those domains. We propose a simple strategy which effectively exploits semantic information of classes, to adapt existing DG methods to meet the demands of Zero-Shot Domain Generalization. We evaluate the proposed methods on CIFAR-10, CIFAR-100, F-MNIST and PACS datasets, establishing a strong baseline to foster interest in this new research direction.



### Source Free Domain Adaptation with Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2008.07514v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07514v2)
- **Published**: 2020-08-17 17:57:33+00:00
- **Updated**: 2021-05-16 07:11:57+00:00
- **Authors**: Yunzhong Hou, Liang Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Effort in releasing large-scale datasets may be compromised by privacy and intellectual property considerations. A feasible alternative is to release pre-trained models instead. While these models are strong on their original task (source domain), their performance might degrade significantly when deployed directly in a new environment (target domain), which might not contain labels for training under realistic settings. Domain adaptation (DA) is a known solution to the domain gap problem, but usually requires labeled source data. In this paper, we study the problem of source free domain adaptation (SFDA), whose distinctive feature is that the source domain only provides a pre-trained model, but no source data. Being source free adds significant challenges to DA, especially when considering that the target dataset is unlabeled. To solve the SFDA problem, we propose an image translation approach that transfers the style of target images to that of unseen source images. To this end, we align the batch-wise feature statistics of generated images to that stored in batch normalization layers of the pre-trained model. Compared with directly classifying target images, higher accuracy is obtained with these style transferred images using the pre-trained model. On several image classification datasets, we show that the above-mentioned improvements are consistent and statistically significant.



### V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.07519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07519v1)
- **Published**: 2020-08-17 17:58:26+00:00
- **Updated**: 2020-08-17 17:58:26+00:00
- **Authors**: Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, James Tu, Raquel Urtasun
- **Comment**: ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from different viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements.



### Anatomy-Aware Cardiac Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.07579v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07579v1)
- **Published**: 2020-08-17 19:14:32+00:00
- **Updated**: 2020-08-17 19:14:32+00:00
- **Authors**: Pingjun Chen, Xiao Chen, Eric Z. Chen, Hanchao Yu, Terrence Chen, Shanhui Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac motion estimation is critical to the assessment of cardiac function. Myocardium feature tracking (FT) can directly estimate cardiac motion from cine MRI, which requires no special scanning procedure. However, current deep learning-based FT methods may result in unrealistic myocardium shapes since the learning is solely guided by image intensities without considering anatomy. On the other hand, motion estimation through learning is challenging because ground-truth motion fields are almost impossible to obtain. In this study, we propose a novel Anatomy-Aware Tracker (AATracker) for cardiac motion estimation that preserves anatomy by weak supervision. A convolutional variational autoencoder (VAE) is trained to encapsulate realistic myocardium shapes. A baseline dense motion tracker is trained to approximate the motion fields and then refined to estimate anatomy-aware motion fields under the weak supervision from the VAE. We evaluate the proposed method on long-axis cardiac cine MRI, which has more complex myocardium appearances and motions than short-axis. Compared with other methods, AATracker significantly improves the tracking performance and provides visually more realistic tracking results, demonstrating the effectiveness of the proposed weakly-supervision scheme in cardiac motion estimation.



### Automatic elimination of the pectoral muscle in mammograms based on anatomical features
- **Arxiv ID**: http://arxiv.org/abs/2009.06357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06357v1)
- **Published**: 2020-08-17 20:36:46+00:00
- **Updated**: 2020-08-17 20:36:46+00:00
- **Authors**: Jairo A. Ayala-Godoy, Rosa E. Lillo, Juan Romo
- **Comment**: None
- **Journal**: International Journal of Computer Science Issues; 2020
- **Summary**: Digital mammogram inspection is the most popular technique for early detection of abnormalities in human breast tissue. When mammograms are analyzed through a computational method, the presence of the pectoral muscle might affect the results of breast lesions detection. This problem is particularly evident in the mediolateral oblique view (MLO), where pectoral muscle occupies a large part of the mammography. Therefore, identifying and eliminating the pectoral muscle are essential steps for improving the automatic discrimination of breast tissue. In this paper, we propose an approach based on anatomical features to tackle this problem. Our method consists of two steps: (1) a process to remove the noisy elements such as labels, markers, scratches and wedges, and (2) application of an intensity transformation based on the Beta distribution. The novel methodology is tested with 322 digital mammograms from the Mammographic Image Analysis Society (mini-MIAS) database and with a set of 84 mammograms for which the area normalized error was previously calculated. The results show a very good performance of the method.



### HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN
- **Arxiv ID**: http://arxiv.org/abs/2008.09646v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09646v2)
- **Published**: 2020-08-17 20:45:59+00:00
- **Updated**: 2021-07-12 05:47:07+00:00
- **Authors**: Abhinav Sagar
- **Comment**: The design of neural network was based on assumptions which was found
  to be wrong
- **Journal**: None
- **Summary**: In this paper, we present a novel network for high resolution video generation. Our network uses ideas from Wasserstein GANs by enforcing k-Lipschitz constraint on the loss term and Conditional GANs using class labels for training and testing. We present Generator and Discriminator network layerwise details along with the combined network architecture, optimization details and algorithm used in this work. Our network uses a combination of two loss terms: mean square pixel loss and an adversarial loss. The datasets used for training and testing our network are UCF101, Golf and Aeroplane Datasets. Using Inception Score and Fr\'echet Inception Distance as the evaluation metrics, our network outperforms previous state of the art networks on unsupervised video generation.



### A Smartphone-based System for Real-time Early Childhood Caries Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2008.07623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07623v1)
- **Published**: 2020-08-17 21:11:19+00:00
- **Updated**: 2020-08-17 21:11:19+00:00
- **Authors**: Yipeng Zhang, Haofu Liao, Jin Xiao, Nisreen Al Jallad, Oriana Ly-Mapes, Jiebo Luo
- **Comment**: MICCAI 2020 Workshop
- **Journal**: None
- **Summary**: Early childhood caries (ECC) is the most common, yet preventable chronic disease in children under the age of 6. Treatments on severe ECC are extremely expensive and unaffordable for socioeconomically disadvantaged families. The identification of ECC in an early stage usually requires expertise in the field, and hence is often ignored by parents. Therefore, early prevention strategies and easy-to-adopt diagnosis techniques are desired. In this study, we propose a multistage deep learning-based system for cavity detection. We create a dataset containing RGB oral images labeled manually by dental practitioners. We then investigate the effectiveness of different deep learning models on the dataset. Furthermore, we integrate the deep learning system into an easy-to-use mobile application that can diagnose ECC from an early stage and provide real-time results to untrained users.



### A Deep Network for Joint Registration and Reconstruction of Images with Pathologies
- **Arxiv ID**: http://arxiv.org/abs/2008.07628v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.07628v1)
- **Published**: 2020-08-17 21:26:02+00:00
- **Updated**: 2020-08-17 21:26:02+00:00
- **Authors**: Xu Han, Zhengyang Shen, Zhenlin Xu, Spyridon Bakas, Hamed Akbari, Michel Bilello, Christos Davatzikos, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Registration of images with pathologies is challenging due to tissue appearance changes and missing correspondences caused by the pathologies. Moreover, mass effects as observed for brain tumors may displace tissue, creating larger deformations over time than what is observed in a healthy brain. Deep learning models have successfully been applied to image registration to offer dramatic speed up and to use surrogate information (e.g., segmentations) during training. However, existing approaches focus on learning registration models using images from healthy patients. They are therefore not designed for the registration of images with strong pathologies for example in the context of brain tumors, and traumatic brain injuries. In this work, we explore a deep learning approach to register images with brain tumors to an atlas. Our model learns an appearance mapping from images with tumors to the atlas, while simultaneously predicting the transformation to atlas space. Using separate decoders, the network disentangles the tumor mass effect from the reconstruction of quasi-normal images. Results on both synthetic and real brain tumor scans show that our approach outperforms cost function masking for registration to the atlas and that reconstructed quasi-normal images can be used for better longitudinal registrations.



### Learning Graph Edit Distance by Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.07641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07641v1)
- **Published**: 2020-08-17 21:49:59+00:00
- **Updated**: 2020-08-17 21:49:59+00:00
- **Authors**: Pau Riba, Andreas Fischer, Josep Lladós, Alicia Fornés
- **Comment**: None
- **Journal**: None
- **Summary**: The emergence of geometric deep learning as a novel framework to deal with graph-based representations has faded away traditional approaches in favor of completely new methodologies. In this paper, we propose a new framework able to combine the advances on deep metric learning with traditional approximations of the graph edit distance. Hence, we propose an efficient graph distance based on the novel field of geometric deep learning. Our method employs a message passing neural network to capture the graph structure, and thus, leveraging this information for its use on a distance computation. The performance of the proposed graph distance is validated on two different scenarios. On the one hand, in a graph retrieval of handwritten words~\ie~keyword spotting, showing its superior performance when compared with (approximate) graph edit distance benchmarks. On the other hand, demonstrating competitive results for graph similarity learning when compared with the current state-of-the-art on a recent benchmark dataset.



### Sequence-to-Sequence Predictive Model: From Prosody To Communicative Gestures
- **Arxiv ID**: http://arxiv.org/abs/2008.07643v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CL, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.07643v2)
- **Published**: 2020-08-17 21:55:22+00:00
- **Updated**: 2021-04-23 21:03:40+00:00
- **Authors**: Fajrian Yunus, Chloé Clavel, Catherine Pelachaud
- **Comment**: None
- **Journal**: None
- **Summary**: Communicative gestures and speech acoustic are tightly linked. Our objective is to predict the timing of gestures according to the acoustic. That is, we want to predict when a certain gesture occurs. We develop a model based on a recurrent neural network with attention mechanism. The model is trained on a corpus of natural dyadic interaction where the speech acoustic and the gesture phases and types have been annotated. The input of the model is a sequence of speech acoustic and the output is a sequence of gesture classes. The classes we are using for the model output is based on a combination of gesture phases and gesture types. We use a sequence comparison technique to evaluate the model performance. We find that the model can predict better certain gesture classes than others. We also perform ablation studies which reveal that fundamental frequency is a relevant feature for gesture prediction task. In another sub-experiment, we find that including eyebrow movements as acting as beat gesture improves the performance. Besides, we also find that a model trained on the data of one given speaker also works for the other speaker of the same conversation. We also perform a subjective experiment to measure how respondents judge the naturalness, the time consistency, and the semantic consistency of the generated gesture timing of a virtual agent. Our respondents rate the output of our model favorably.



### Pictorial and apictorial polygonal jigsaw puzzles: The lazy caterer model, properties, and solvers
- **Arxiv ID**: http://arxiv.org/abs/2008.07644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2008.07644v2)
- **Published**: 2020-08-17 22:07:40+00:00
- **Updated**: 2021-12-16 15:32:53+00:00
- **Authors**: Peleg Harel, Ohad Ben-Shahar
- **Comment**: None
- **Journal**: None
- **Summary**: Jigsaw puzzle solving, the problem of constructing a coherent whole from a set of non-overlapping unordered visual fragments, is fundamental to numerous applications and yet most of the literature of the last two decades has focused thus far on less realistic puzzles whose pieces are identical squares. Here we formalize a new type of jigsaw puzzle where the pieces are general convex polygons generated by cutting through a global polygonal shape/image with an arbitrary number of straight cuts, a generation model inspired by the celebrated Lazy caterer's sequence. We analyze the theoretical properties of such puzzles, including the inherent challenges in solving them once pieces are contaminated with geometrical noise. To cope with such difficulties and obtain tractable solutions, we abstract the problem as a multi-body spring-mass dynamical system endowed with hierarchical loop constraints and a layered reconstruction process. We define evaluation metrics and present experimental results on both apictorial and pictorial puzzles to show that they are solvable completely automatically.



### A Deep Dive into Adversarial Robustness in Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.07651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.07651v1)
- **Published**: 2020-08-17 22:26:06+00:00
- **Updated**: 2020-08-17 22:26:06+00:00
- **Authors**: Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu
- **Comment**: To appear in ECCV 2020, Workshop on Adversarial Robustness in the
  Real World
- **Journal**: None
- **Summary**: Machine learning (ML) systems have introduced significant advances in various fields, due to the introduction of highly complex models. Despite their success, it has been shown multiple times that machine learning models are prone to imperceptible perturbations that can severely degrade their accuracy. So far, existing studies have primarily focused on models where supervision across all classes were available. In constrast, Zero-shot Learning (ZSL) and Generalized Zero-shot Learning (GZSL) tasks inherently lack supervision across all classes. In this paper, we present a study aimed on evaluating the adversarial robustness of ZSL and GZSL models. We leverage the well-established label embedding model and subject it to a set of established adversarial attacks and defenses across multiple datasets. In addition to creating possibly the first benchmark on adversarial robustness of ZSL models, we also present analyses on important points that require attention for better interpretation of ZSL robustness results. We hope these points, along with the benchmark, will help researchers establish a better understanding what challenges lie ahead and help guide their work.



### Inverse Distance Aggregation for Federated Learning with Non-IID Data
- **Arxiv ID**: http://arxiv.org/abs/2008.07665v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.07665v1)
- **Published**: 2020-08-17 23:20:01+00:00
- **Updated**: 2020-08-17 23:20:01+00:00
- **Authors**: Yousef Yeganeh, Azade Farshad, Nassir Navab, Shadi Albarqouni
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) has been a promising approach in the field of medical imaging in recent years. A critical problem in FL, specifically in medical scenarios is to have a more accurate shared model which is robust to noisy and out-of distribution clients. In this work, we tackle the problem of statistical heterogeneity in data for FL which is highly plausible in medical data where for example the data comes from different sites with different scanner settings. We propose IDA (Inverse Distance Aggregation), a novel adaptive weighting approach for clients based on meta-information which handles unbalanced and non-iid data. We extensively analyze and evaluate our method against the well-known FL approach, Federated Averaging as a baseline.



### REFORM: Recognizing F-formations for Social Robots
- **Arxiv ID**: http://arxiv.org/abs/2008.07668v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2008.07668v1)
- **Published**: 2020-08-17 23:32:05+00:00
- **Updated**: 2020-08-17 23:32:05+00:00
- **Authors**: Hooman Hedayati, Annika Muehlbradt, Daniel J. Szafir, Sean Andrist
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: Recognizing and understanding conversational groups, or F-formations, is a critical task for situated agents designed to interact with humans. F-formations contain complex structures and dynamics, yet are used intuitively by people in everyday face-to-face conversations. Prior research exploring ways of identifying F-formations has largely relied on heuristic algorithms that may not capture the rich dynamic behaviors employed by humans. We introduce REFORM (REcognize F-FORmations with Machine learning), a data-driven approach for detecting F-formations given human and agent positions and orientations. REFORM decomposes the scene into all possible pairs and then reconstructs F-formations with a voting-based scheme. We evaluated our approach across three datasets: the SALSA dataset, a newly collected human-only dataset, and a new set of acted human-robot scenarios, and found that REFORM yielded improved accuracy over a state-of-the-art F-formation detection algorithm. We also introduce symmetry and tightness as quantitative measures to characterize F-formations. Supplementary video: https://youtu.be/Fp7ETdkKvdA , Dataset available at: github.com/cu-ironlab/Babble



