# Arxiv Papers in cs.CV on 2020-08-10
### Domain Private and Agnostic Feature for Modality Adaptive Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.03848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03848v1)
- **Published**: 2020-08-10 00:59:42+00:00
- **Updated**: 2020-08-10 00:59:42+00:00
- **Authors**: Yingguo Xu, Lei Zhang, Qingyan Duan
- **Comment**: Accepted by IJCB2020
- **Journal**: None
- **Summary**: Heterogeneous face recognition is a challenging task due to the large modality discrepancy and insufficient cross-modal samples. Most existing works focus on discriminative feature transformation, metric learning and cross-modal face synthesis. However, the fact that cross-modal faces are always coupled by domain (modality) and identity information has received little attention. Therefore, how to learn and utilize the domain-private feature and domain-agnostic feature for modality adaptive face recognition is the focus of this work. Specifically, this paper proposes a Feature Aggregation Network (FAN), which includes disentangled representation module (DRM), feature fusion module (FFM) and adaptive penalty metric (APM) learning session. First, in DRM, two subnetworks, i.e. domain-private network and domain-agnostic network are specially designed for learning modality features and identity features, respectively. Second, in FFM, the identity features are fused with domain features to achieve cross-modal bi-directional identity feature transformation, which, to a large extent, further disentangles the modality information and identity information. Third, considering that the distribution imbalance between easy and hard pairs exists in cross-modal datasets, which increases the risk of model bias, the identity preserving guided metric learning with adaptive hard pairs penalization is proposed in our FAN. The proposed APM also guarantees the cross-modality intra-class compactness and inter-class separation. Extensive experiments on benchmark cross-modal face datasets show that our FAN outperforms SOTA methods.



### Nighttime Dehazing with a Synthetic Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2008.03864v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03864v3)
- **Published**: 2020-08-10 02:16:46+00:00
- **Updated**: 2020-10-19 00:41:38+00:00
- **Authors**: Jing Zhang, Yang Cao, Zheng-Jun Zha, Dacheng Tao
- **Comment**: ACM MM 2020. Both the dataset and source code will be available at
  \url{https://github.com/chaimi2013/3R}
- **Journal**: None
- **Summary**: Increasing the visibility of nighttime hazy images is challenging because of uneven illumination from active artificial light sources and haze absorbing/scattering. The absence of large-scale benchmark datasets hampers progress in this area. To address this issue, we propose a novel synthetic method called 3R to simulate nighttime hazy images from daytime clear images, which first reconstructs the scene geometry, then simulates the light rays and object reflectance, and finally renders the haze effects. Based on it, we generate realistic nighttime hazy images by sampling real-world light colors from a prior empirical distribution. Experiments on the synthetic benchmark show that the degrading factors jointly reduce the image quality. To address this issue, we propose an optimal-scale maximum reflectance prior to disentangle the color correction from haze removal and address them sequentially. Besides, we also devise a simple but effective learning-based baseline which has an encoder-decoder structure based on the MobileNet-v2 backbone. Experiment results demonstrate their superiority over state-of-the-art methods in terms of both image quality and runtime. Both the dataset and source code will be available at https://github.com/chaimi2013/3R.



### RocNet: Recursive Octree Network for Efficient 3D Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/2008.03875v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03875v1)
- **Published**: 2020-08-10 03:02:10+00:00
- **Updated**: 2020-08-10 03:02:10+00:00
- **Authors**: Juncheng Liu, Steven Mills, Brendan McCane
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a deep recursive octree network for the compression of 3D voxel data. Our network compresses a voxel grid of any size down to a very small latent space in an autoencoder-like network. We show results for compressing 32, 64 and 128 grids down to just 80 floats in the latent space. We demonstrate the effectiveness and efficiency of our proposed method on several publicly available datasets with three experiments: 3D shape classification, 3D shape reconstruction, and shape generation. Experimental results show that our algorithm maintains accuracy while consuming less memory with shorter training times compared to existing methods, especially in 3D reconstruction tasks.



### Norm-in-Norm Loss with Faster Convergence and Better Performance for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2008.03889v1
- **DOI**: 10.1145/3394171.3413804
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.03889v1)
- **Published**: 2020-08-10 04:01:21+00:00
- **Updated**: 2020-08-10 04:01:21+00:00
- **Authors**: Dingquan Li, Tingting Jiang, Ming Jiang
- **Comment**: Accepted by ACM MM 2020, + supplemental materials
- **Journal**: None
- **Summary**: Currently, most image quality assessment (IQA) models are supervised by the MAE or MSE loss with empirically slow convergence. It is well-known that normalization can facilitate fast convergence. Therefore, we explore normalization in the design of loss functions for IQA. Specifically, we first normalize the predicted quality scores and the corresponding subjective quality scores. Then, the loss is defined based on the norm of the differences between these normalized values. The resulting "Norm-in-Norm'' loss encourages the IQA model to make linear predictions with respect to subjective quality scores. After training, the least squares regression is applied to determine the linear mapping from the predicted quality to the subjective quality. It is shown that the new loss is closely connected with two common IQA performance criteria (PLCC and RMSE). Through theoretical analysis, it is proved that the embedded normalization makes the gradients of the loss function more stable and more predictable, which is conducive to the faster convergence of the IQA model. Furthermore, to experimentally verify the effectiveness of the proposed loss, it is applied to solve a challenging problem: quality assessment of in-the-wild images. Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compared to MAE or MSE loss, the new loss enables the IQA model to converge about 10 times faster and the final model achieves better performance. The proposed model also achieves state-of-the-art prediction performance on this challenging problem. For reproducible scientific research, our code is publicly available at https://github.com/lidq92/LinearityIQA.



### IF-Net: An Illumination-invariant Feature Network
- **Arxiv ID**: http://arxiv.org/abs/2008.03897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03897v1)
- **Published**: 2020-08-10 04:32:32+00:00
- **Updated**: 2020-08-10 04:32:32+00:00
- **Authors**: Po-Heng Chen, Zhao-Xu Luo, Zu-Kuan Huang, Chun Yang, Kuan-Wen Chen
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Feature descriptor matching is a critical step is many computer vision applications such as image stitching, image retrieval and visual localization. However, it is often affected by many practical factors which will degrade its performance. Among these factors, illumination variations are the most influential one, and especially no previous descriptor learning works focus on dealing with this problem. In this paper, we propose IF-Net, aimed to generate a robust and generic descriptor under crucial illumination changes conditions. We find out not only the kind of training data important but also the order it is presented. To this end, we investigate several dataset scheduling methods and propose a separation training scheme to improve the matching accuracy. Further, we propose a ROI loss and hard-positive mining strategy along with the training scheme, which can strengthen the ability of generated descriptor dealing with large illumination change conditions. We evaluate our approach on public patch matching benchmark and achieve the best results compared with several state-of-the-arts methods. To show the practicality, we further evaluate IF-Net on the task of visual localization under large illumination changes scenes, and achieves the best localization accuracy.



### Prototype Mixture Models for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.03898v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03898v2)
- **Published**: 2020-08-10 04:33:17+00:00
- **Updated**: 2020-09-01 11:23:17+00:00
- **Authors**: Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Using a single prototype acquired directly from the support image to segment the query image causes semantic ambiguity. In this paper, we propose prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to enforce the prototype-based semantic representation. Estimated by an Expectation-Maximization algorithm, PMMs incorporate rich channel-wised and spatial semantics from limited support images. Utilized as representations as well as classifiers, PMMs fully leverage the semantics to activate objects in the query image while depressing background regions in a duplex manner. Extensive experiments on Pascal VOC and MS-COCO datasets show that PMMs significantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot segmentation performance on MS-COCO by up to 5.82\% with only a moderate cost for model size and inference speed.



### RARTS: An Efficient First-Order Relaxed Architecture Search Method
- **Arxiv ID**: http://arxiv.org/abs/2008.03901v2
- **DOI**: 10.1109/ACCESS.2022.3185095
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.03901v2)
- **Published**: 2020-08-10 04:55:51+00:00
- **Updated**: 2022-06-24 06:36:21+00:00
- **Authors**: Fanghui Xue, Yingyong Qi, Jack Xin
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) is an effective method for data-driven neural network design based on solving a bilevel optimization problem. Despite its success in many architecture search tasks, there are still some concerns about the accuracy of first-order DARTS and the efficiency of the second-order DARTS. In this paper, we formulate a single level alternative and a relaxed architecture search (RARTS) method that utilizes the whole dataset in architecture learning via both data and network splitting, without involving mixed second derivatives of the corresponding loss functions like DARTS. In our formulation of network splitting, two networks with different but related weights cooperate in search of a shared architecture. The advantage of RARTS over DARTS is justified by a convergence theorem and an analytically solvable model. Moreover, RARTS outperforms DARTS and its variants in accuracy and search efficiency, as shown in adequate experimental results. For the task of searching topological architecture, i.e., the edges and the operations, RARTS obtains a higher accuracy and 60\% reduction of computational cost than second-order DARTS on CIFAR-10. RARTS continues to out-perform DARTS upon transfer to ImageNet and is on par with recent variants of DARTS even though our innovation is purely on the training algorithm without modifying search space. For the task of searching width, i.e., the number of channels in convolutional layers, RARTS also outperforms the traditional network pruning benchmarks. Further experiments on the public architecture search benchmark like NATS-Bench also support the preeminence of RARTS.



### DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor Repressed Dynamic Regression
- **Arxiv ID**: http://arxiv.org/abs/2008.03912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T40, F.2.2; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.03912v1)
- **Published**: 2020-08-10 06:08:31+00:00
- **Updated**: 2020-08-10 06:08:31+00:00
- **Authors**: Changhong Fu, Fangqiang Ding, Yiming Li, Jin Jin, Chen Feng
- **Comment**: 8pages, 7 figures, accepted by 2020 IEEE/RJS International Conference
  on Intelligent Robots and Systems(IROS)
- **Journal**: None
- **Summary**: Visual tracking has yielded promising applications with unmanned aerial vehicle (UAV). In literature, the advanced discriminative correlation filter (DCF) type trackers generally distinguish the foreground from the background with a learned regressor which regresses the implicit circulated samples into a fixed target label. However, the predefined and unchanged regression target results in low robustness and adaptivity to uncertain aerial tracking scenarios. In this work, we exploit the local maximum points of the response map generated in the detection phase to automatically locate current distractors. By repressing the response of distractors in the regressor learning, we can dynamically and adaptively alter our regression target to leverage the tracking robustness as well as adaptivity. Substantial experiments conducted on three challenging UAV benchmarks demonstrate both excellent performance and extraordinary speed (~50fps on a cheap CPU) of our tracker.



### Automatic Failure Recovery and Re-Initialization for Online UAV Tracking with Joint Scale and Aspect Ratio Optimization
- **Arxiv ID**: http://arxiv.org/abs/2008.03915v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T40, F.2.2; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.03915v1)
- **Published**: 2020-08-10 06:31:30+00:00
- **Updated**: 2020-08-10 06:31:30+00:00
- **Authors**: Fangqiang Ding, Changhong Fu, Yiming Li, Jin Jin, Chen Feng
- **Comment**: 8pages, 8 figures, accepted by 2020 IEEE/RSJ International Conference
  on Intelligent Robots and Systems(IROS)
- **Journal**: None
- **Summary**: Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to: (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meets the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters: (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.



### Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional Gated Recurrent Units
- **Arxiv ID**: http://arxiv.org/abs/2008.03922v2
- **DOI**: 10.1109/TITS.2021.3060258
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03922v2)
- **Published**: 2020-08-10 06:50:48+00:00
- **Updated**: 2021-02-27 11:59:41+00:00
- **Authors**: Jiyong Zhang, Tao Deng, Fei Yan, Wenbo Liu
- **Comment**: 13 pages, 9 figures, 8 tables, Accepted by IEEE Transactions on
  Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Lane detection is one of the indispensable and key elements of self-driving environmental perception. Many lane detection models have been proposed, solving lane detection under challenging conditions, including intersection merging and splitting, curves, boundaries, occlusions and combinations of scene types. Nevertheless, lane detection will remain an open problem for some time to come. The ability to cope well with those challenging scenes impacts greatly the applications of lane detection on advanced driver assistance systems (ADASs). In this paper, a spatio-temporal network with double Convolutional Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in challenging scenes. Both of ConvGRUs have the same structures, but different locations and functions in our network. One is used to extract the information of the most likely low-level features of lane markings. The extracted features are input into the next layer of the end-to-end network after concatenating them with the outputs of some blocks. The other one takes some continuous frames as its input to process the spatio-temporal driving information. Extensive experiments on the large-scale TuSimple lane marking challenge dataset and Unsupervised LLAMAS dataset demonstrate that the proposed model can effectively detect lanes in the challenging driving scenes. Our model can outperform the state-of-the-art lane detection models.



### Rethinking of the Image Salient Object Detection: Object-level Semantic Saliency Re-ranking First, Pixel-wise Saliency Refinement Latter
- **Arxiv ID**: http://arxiv.org/abs/2008.05397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05397v1)
- **Published**: 2020-08-10 07:12:43+00:00
- **Updated**: 2020-08-10 07:12:43+00:00
- **Authors**: Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: The real human attention is an interactive activity between our visual system and our brain, using both low-level visual stimulus and high-level semantic information. Previous image salient object detection (SOD) works conduct their saliency predictions in a multi-task manner, i.e., performing pixel-wise saliency regression and segmentation-like saliency refinement at the same time, which degenerates their feature backbones in revealing semantic information. However, given an image, we tend to pay more attention to those regions which are semantically salient even in the case that these regions are perceptually not the most salient ones at first glance. In this paper, we divide the SOD problem into two sequential tasks: 1) we propose a lightweight, weakly supervised deep network to coarsely locate those semantically salient regions first; 2) then, as a post-processing procedure, we selectively fuse multiple off-the-shelf deep models on these semantically salient regions as the pixel-wise saliency refinement. In sharp contrast to the state-of-the-art (SOTA) methods that focus on learning pixel-wise saliency in "single image" using perceptual clues mainly, our method has investigated the "object-level semantic ranks between multiple images", of which the methodology is more consistent with the real human attention mechanism. Our method is simple yet effective, which is the first attempt to consider the salient object detection mainly as an object-level semantic re-ranking problem.



### Measuring shape relations using r-parallel sets
- **Arxiv ID**: http://arxiv.org/abs/2008.03927v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, 51K99 (Primary), 92F05 (Secondary), I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2008.03927v1)
- **Published**: 2020-08-10 07:30:51+00:00
- **Updated**: 2020-08-10 07:30:51+00:00
- **Authors**: Hans JT Stephensen, Anne Marie Svane, Carlos Benitez, Steven A. Goldman, Jon Sporring
- **Comment**: 10 pages 5 figures
- **Journal**: None
- **Summary**: Geometrical measurements of biological objects form the basis of many quantitative analyses. Hausdorff measures such as the volume and the area of objects are simple and popular descriptors of individual objects, however, for most biological processes, the interaction between objects cannot be ignored, and the shape and function of neighboring objects are mutually influential.   In this paper, we present a theory on the geometrical interaction between objects based on the theory of spatial point processes. Our theory is based on the relation between two objects: a reference and an observed object. We generate the $r$-parallel sets of the reference object, we calculate the intersection between the $r$-parallel sets and the observed object, and we define measures on these intersections. Our measures are simple like the volume and area of an object, but describe further details about the shape of individual objects and their pairwise geometrical relation. Finally, we propose a summary statistics for collections of shapes and their interaction.   We evaluate these measures on a publicly available FIB-SEM 3D data set of an adult rodent.



### Rethinking 3D LiDAR Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.03928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03928v2)
- **Published**: 2020-08-10 07:30:53+00:00
- **Updated**: 2021-12-02 19:55:27+00:00
- **Authors**: Shijie Li, Yun Liu, Juergen Gall
- **Comment**: TNNLS 2021
- **Journal**: None
- **Summary**: Many point-based semantic segmentation methods have been designed for indoor scenarios, but they struggle if they are applied to point clouds that are captured by a LiDAR sensor in an outdoor environment. In order to make these methods more efficient and robust such that they can handle LiDAR data, we introduce the general concept of reformulating 3D point-based operations such that they can operate in the projection space. While we show by means of three point-based methods that the reformulated versions are between 300 and 400 times faster and achieve a higher accuracy, we furthermore demonstrate that the concept of reformulating 3D point-based operations allows to design new architectures that unify the benefits of point-based and image-based methods. As an example, we introduce a network that integrates reformulated 3D point-based operations into a 2D encoder-decoder architecture that fuses the information from different 2D scales. We evaluate the approach on four challenging datasets for semantic LiDAR point cloud segmentation and show that leveraging reformulated 3D point-based operations with 2D image-based operations achieves very good results for all four datasets.



### Unsupervised Deep-Learning Based Deformable Image Registration: A Bayesian Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.03949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03949v1)
- **Published**: 2020-08-10 08:15:49+00:00
- **Updated**: 2020-08-10 08:15:49+00:00
- **Authors**: Samah Khawaled, Moti Freiman
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Unsupervised deep-learning (DL) models were recently proposed for deformable image registration tasks. In such models, a neural-network is trained to predict the best deformation field by minimizing some dissimilarity function between the moving and the target images. After training on a dataset without reference deformation fields available, such a model can be used to rapidly predict the deformation field between newly seen moving and target images. Currently, the training process effectively provides a point-estimate of the network weights rather than characterizing their entire posterior distribution. This may result in a potential over-fitting which may yield sub-optimal results at inference phase, especially for small-size datasets, frequently present in the medical imaging domain. We introduce a fully Bayesian framework for unsupervised DL-based deformable image registration. Our method provides a principled way to characterize the true posterior distribution, thus, avoiding potential over-fitting. We used stochastic gradient Langevin dynamics (SGLD) to conduct the posterior sampling, which is both theoretically well-founded and computationally efficient. We demonstrated the added-value of our Basyesian unsupervised DL-based registration framework on the MNIST and brain MRI (MGH10) datasets in comparison to the VoxelMorph unsupervised DL-based image registration framework. Our experiments show that our approach provided better estimates of the deformation field by means of improved mean-squared-error ($0.0063$ vs. $0.0065$) and Dice coefficient ($0.73$ vs. $0.71$) for the MNIST and the MGH10 datasets respectively. Further, our approach provides an estimate of the uncertainty in the deformation-field by characterizing the true posterior distribution.



### DQI: A Guide to Benchmark Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2008.03964v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2008.03964v1)
- **Published**: 2020-08-10 08:38:55+00:00
- **Updated**: 2020-08-10 08:38:55+00:00
- **Authors**: Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, Chitta Baral
- **Comment**: ICML UDL 2020
- **Journal**: None
- **Summary**: A `state of the art' model A surpasses humans in a benchmark B, but fails on similar benchmarks C, D, and E. What does B have that the other benchmarks do not? Recent research provides the answer: spurious bias. However, developing A to solve benchmarks B through E does not guarantee that it will solve future benchmarks. To progress towards a model that `truly learns' an underlying task, we need to quantify the differences between successive benchmarks, as opposed to existing binary and black-box approaches. We propose a novel approach to solve this underexplored task of quantifying benchmark quality by debuting a data quality metric: DQI.



### Deep Reinforcement Learning with Label Embedding Reward for Supervised Image Hashing
- **Arxiv ID**: http://arxiv.org/abs/2008.03973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03973v1)
- **Published**: 2020-08-10 09:17:20+00:00
- **Updated**: 2020-08-10 09:17:20+00:00
- **Authors**: Zhenzhen Wang, Weixiang Hong, Junsong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep hashing has shown promising results in image retrieval and recognition. Despite its success, most existing deep hashing approaches are rather similar: either multi-layer perceptron or CNN is applied to extract image feature, followed by different binarization activation functions such as sigmoid, tanh or autoencoder to generate binary code. In this work, we introduce a novel decision-making approach for deep supervised hashing. We formulate the hashing problem as travelling across the vertices in the binary code space, and learn a deep Q-network with a novel label embedding reward defined by Bose-Chaudhuri-Hocquenghem (BCH) codes to explore the best path. Extensive experiments and analysis on the CIFAR-10 and NUS-WIDE dataset show that our approach outperforms state-of-the-art supervised hashing methods under various code lengths.



### A model-guided deep network for limited-angle computed tomography
- **Arxiv ID**: http://arxiv.org/abs/2008.03988v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03988v1)
- **Published**: 2020-08-10 09:42:32+00:00
- **Updated**: 2020-08-10 09:42:32+00:00
- **Authors**: Wei Wang, Xiang-Gen Xia, Chuanjiang He, Zemin Ren, Jian Lu, Tianfu Wang, Baiying Lei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we first propose a variational model for the limited-angle computed tomography (CT) image reconstruction and then convert the model into an end-to-end deep network.We use the penalty method to solve the model and divide it into three iterative subproblems, where the first subproblem completes the sinograms by utilizing the prior information of sinograms in the frequency domain and the second refines the CT images by using the prior information of CT images in the spatial domain, and the last merges the outputs of the first two subproblems. In each iteration, we use the convolutional neural networks (CNNs) to approxiamte the solutions of the first two subproblems and, thus, obtain an end-to-end deep network for the limited-angle CT image reconstruction. Our network tackles both the sinograms and the CT images, and can simultaneously suppress the artifacts caused by the incomplete data and recover fine structural information in the CT images. Experimental results show that our method outperforms the existing algorithms for the limited-angle CT image reconstruction.



### 2nd Place Scheme on Action Recognition Track of ECCV 2020 VIPriors Challenges: An Efficient Optical Flow Stream Guided Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.03996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03996v1)
- **Published**: 2020-08-10 09:50:28+00:00
- **Updated**: 2020-08-10 09:50:28+00:00
- **Authors**: Haoyu Chen, Zitong Yu, Xin Liu, Wei Peng, Yoon Lee, Guoying Zhao
- **Comment**: Technical report for ECCV 2020 VIPrior Challenge, Action Recognition
  Track
- **Journal**: None
- **Summary**: To address the problem of training on small datasets for action recognition tasks, most prior works are either based on a large number of training samples or require pre-trained models transferred from other large datasets to tackle overfitting problems. However, it limits the research within organizations that have strong computational abilities. In this work, we try to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Specifically, by introducing a 3D central difference convolution operation, we proposed a novel C3D neural network-based two-stream (Rank Pooling RGB and Optical Flow) framework for the task. The method is validated on the action recognition track of the ECCV 2020 VIPriors challenges and got the 2nd place (88.31%). It is proved that our method can achieve a promising result even without a pre-trained model on large scale datasets. The code will be released soon.



### HAPI: Hardware-Aware Progressive Inference
- **Arxiv ID**: http://arxiv.org/abs/2008.03997v1
- **DOI**: 10.1145/3400302.3415698
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03997v1)
- **Published**: 2020-08-10 09:55:18+00:00
- **Updated**: 2020-08-10 09:55:18+00:00
- **Authors**: Stefanos Laskaridis, Stylianos I. Venieris, Hyeji Kim, Nicholas D. Lane
- **Comment**: Accepted at the 39th International Conference on Computer-Aided
  Design (ICCAD), 2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have recently become the state-of-the-art in a diversity of AI tasks. Despite their popularity, CNN inference still comes at a high computational cost. A growing body of work aims to alleviate this by exploiting the difference in the classification difficulty among samples and early-exiting at different stages of the network. Nevertheless, existing studies on early exiting have primarily focused on the training scheme, without considering the use-case requirements or the deployment platform. This work presents HAPI, a novel methodology for generating high-performance early-exit networks by co-optimising the placement of intermediate exits together with the early-exit strategy at inference time. Furthermore, we propose an efficient design space exploration algorithm which enables the faster traversal of a large number of alternative architectures and generates the highest-performing design, tailored to the use-case requirements and target hardware. Quantitative evaluation shows that our system consistently outperforms alternative search mechanisms and state-of-the-art early-exit schemes across various latency budgets. Moreover, it pushes further the performance of highly optimised hand-crafted early-exit CNNs, delivering up to 5.11x speedup over lightweight models on imposed latency-driven SLAs for embedded devices.



### Incomplete Descriptor Mining with Elastic Loss for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.04010v4
- **DOI**: 10.1109/TCSVT.2021.3061412
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04010v4)
- **Published**: 2020-08-10 10:29:15+00:00
- **Updated**: 2021-03-04 05:59:53+00:00
- **Authors**: Hongchen Tan, Yuhao Bian, Huasheng Wang, Xiuping Liu, Baocai Yin
- **Comment**: Acceped by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: In this paper, we propose a novel person Re-ID model, Consecutive Batch DropBlock Network (CBDB-Net), to capture the attentive and robust person descriptor for the person Re-ID task. The CBDB-Net contains two novel designs: the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL). In the Consecutive Batch DropBlock Module (CBDBM), we firstly conduct uniform partition on the feature maps. And then, we independently and continuously drop each patch from top to bottom on the feature maps, which can output multiple incomplete feature maps. In the training stage, these multiple incomplete features can better encourage the Re-ID model to capture the robust person descriptor for the Re-ID task. In the Elastic Loss (EL), we design a novel weight control item to help the Re-ID model adaptively balance hard sample pairs and easy sample pairs in the whole training process. Through an extensive set of ablation studies, we verify that the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL) each contribute to the performance boosts of CBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive performance on the three standard person Re-ID datasets (the Market-1501, the DukeMTMC-Re-ID, and the CUHK03 dataset), three occluded Person Re-ID datasets (the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and a general image retrieval dataset (In-Shop Clothes Retrieval dataset).



### MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.04015v4
- **DOI**: 10.1109/TNNLS.2022.3144163
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04015v4)
- **Published**: 2020-08-10 10:42:23+00:00
- **Updated**: 2022-04-16 10:20:26+00:00
- **Authors**: Hongchen Tan, Xiuping Liu, Baocai Yin, Xin Li
- **Comment**: Accepted by TNNLS
- **Journal**: None
- **Summary**: This paper presents a novel person re-identification model, named Multi-Head Self-Attention Network (MHSA-Net), to prune unimportant information and capture key local information from person images. MHSA-Net contains two main novel components: Multi-Head Self-Attention Branch (MHSAB) and Attention Competition Mechanism (ACM). The MHSAB adaptively captures key local person information, and then produces effective diversity embeddings of an image for the person matching. The ACM further helps filter out attention noise and non-key information. Through extensive ablation studies, we verified that the Multi-Head Self-Attention Branch (MHSAB) and Attention Competition Mechanism (ACM) both contribute to the performance improvement of the MHSA-Net. Our MHSA-Net achieves competitive performance in the standard and occluded person Re-ID tasks.



### SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.04017v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.04017v3)
- **Published**: 2020-08-10 10:52:47+00:00
- **Updated**: 2020-11-14 21:03:21+00:00
- **Authors**: Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim Fingscheidt, Patrick Maeder
- **Comment**: Camera ready version + supplementary. Accepted for presentation at
  Winter Conference on Applications of Computer Vision 2021
- **Journal**: None
- **Summary**: State-of-the-art self-supervised learning approaches for monocular depth estimation usually suffer from scale ambiguity. They do not generalize well when applied on distance estimation for complex projection models such as in fisheye and omnidirectional cameras. This paper introduces a novel multi-task learning strategy to improve self-supervised monocular distance estimation on fisheye and pinhole camera images. Our contribution to this work is threefold: Firstly, we introduce a novel distance estimation network architecture using a self-attention based encoder coupled with robust semantic feature guidance to the decoder that can be trained in a one-stage fashion. Secondly, we integrate a generalized robust loss function, which improves performance significantly while removing the need for hyperparameter tuning with the reprojection loss. Finally, we reduce the artifacts caused by dynamic objects violating static world assumptions using a semantic masking strategy. We significantly improve upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is little work on fisheye cameras, we evaluated the proposed method on KITTI using a pinhole model. We achieved state-of-the-art performance among self-supervised methods without requiring an external scale estimation.



### Road Segmentation for Remote Sensing Images using Adversarial Spatial Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04021v1
- **DOI**: 10.1109/TGRS.2020.3016086
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04021v1)
- **Published**: 2020-08-10 11:00:19+00:00
- **Updated**: 2020-08-10 11:00:19+00:00
- **Authors**: Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Ruili Wang, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Road extraction in remote sensing images is of great importance for a wide range of applications. Because of the complex background, and high density, most of the existing methods fail to accurately extract a road network that appears correct and complete. Moreover, they suffer from either insufficient training data or high costs of manual annotation. To address these problems, we introduce a new model to apply structured domain adaption for synthetic image generation and road segmentation. We incorporate a feature pyramid network into generative adversarial networks to minimize the difference between the source and target domains. A generator is learned to produce quality synthetic images, and the discriminator attempts to distinguish them. We also propose a feature pyramid network that improves the performance of the proposed model by extracting effective features from all the layers of the network for describing different scales objects. Indeed, a novel scale-wise architecture is introduced to learn from the multi-level feature maps and improve the semantics of the features. For optimization, the model is trained by a joint reconstruction loss function, which minimizes the difference between the fake images and the real ones. A wide range of experiments on three datasets prove the superior performance of the proposed approach in terms of accuracy and efficiency. In particular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts dataset with 14.89M parameters and 86.78B FLOPs, with 4x fewer FLOPs but higher accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches used in the evaluation.



### An Explainable 3D Residual Self-Attention Deep Neural Network FOR Joint Atrophy Localization and Alzheimer's Disease Diagnosis using Structural MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.04024v2
- **DOI**: 10.1109/JBHI.2021.3066832
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04024v2)
- **Published**: 2020-08-10 11:08:55+00:00
- **Updated**: 2021-04-20 23:44:53+00:00
- **Authors**: Xin Zhang, Liangxiu Han, Wenyong Zhu, Liang Sun, Daoqiang Zhang
- **Comment**: IEEE Journal of Biomedical and Health Informatics (2021)
- **Journal**: None
- **Summary**: Computer-aided early diagnosis of Alzheimer's disease (AD) and its prodromal form mild cognitive impairment (MCI) based on structure Magnetic Resonance Imaging (sMRI) has provided a cost-effective and objective way for early prevention and treatment of disease progression, leading to improved patient care. In this work, we have proposed a novel computer-aided approach for early diagnosis of AD by introducing an explainable 3D Residual Attention Deep Neural Network (3D ResAttNet) for end-to-end learning from sMRI scans. Different from the existing approaches, the novelty of our approach is three-fold: 1) A Residual Self-Attention Deep Neural Network has been proposed to capture local, global and spatial information of MR images to improve diagnostic performance; 2) An explanation method using Gradient-based Localization Class Activation mapping (Grad-CAM) has been introduced to improve the explainable of the proposed method; 3) This work has provided a full end-to-end learning solution for automated disease diagnosis. Our proposed 3D ResAttNet method has been evaluated on a large cohort of subjects from real datasets for two changeling classification tasks (i.e., Alzheimer's disease (AD) vs. Normal cohort (NC) and progressive MCI (pMCI) vs. stable MCI (sMCI)). The experimental results show that the proposed approach has a competitive advantage over the state-of-the-art models in terms of accuracy performance and generalizability. The explainable mechanism in our approach is able to identify and highlight the contribution of the important brain parts (e.g., hippocampus, lateral ventricle and most parts of the cortex) for transparent decisions.



### Invertible Neural BRDF for Object Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2008.04030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04030v2)
- **Published**: 2020-08-10 11:27:01+00:00
- **Updated**: 2020-08-11 04:15:00+00:00
- **Authors**: Zhe Chen, Shohei Nobuhara, Ko Nishino
- **Comment**: accepted to ECCV 2020 as spotlight
- **Journal**: None
- **Summary**: We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.



### Cooperative Bi-path Metric for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.04031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04031v1)
- **Published**: 2020-08-10 11:28:52+00:00
- **Updated**: 2020-08-10 11:28:52+00:00
- **Authors**: Zeyuan Wang, Yifan Zhao, Jia Li, Yonghong Tian
- **Comment**: 9 pages, 4 figures. Accepted by ACM MultiMedia 2020
- **Journal**: None
- **Summary**: Given base classes with sufficient labeled samples, the target of few-shot classification is to recognize unlabeled samples of novel classes with only a few labeled samples. Most existing methods only pay attention to the relationship between labeled and unlabeled samples of novel classes, which do not make full use of information within base classes. In this paper, we make two contributions to investigate the few-shot classification problem. First, we report a simple and effective baseline trained on base classes in the way of traditional supervised learning, which can achieve comparable results to the state of the art. Second, based on the baseline, we propose a cooperative bi-path metric for classification, which leverages the correlations between base classes and novel classes to further improve the accuracy. Experiments on two widely used benchmarks show that our method is a simple and effective framework, and a new state of the art is established in the few-shot classification field.



### Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning
- **Arxiv ID**: http://arxiv.org/abs/2008.04047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.04047v1)
- **Published**: 2020-08-10 12:16:44+00:00
- **Updated**: 2020-08-10 12:16:44+00:00
- **Authors**: Abdelhak Loukkal, Yves Grandvalet, Tom Drummond, You Li
- **Comment**: None
- **Journal**: None
- **Summary**: Camera-based end-to-end driving neural networks bring the promise of a low-cost system that maps camera images to driving control commands. These networks are appealing because they replace laborious hand engineered building blocks but their black-box nature makes them difficult to delve in case of failure. Recent works have shown the importance of using an explicit intermediate representation that has the benefits of increasing both the interpretability and the accuracy of networks' decisions. Nonetheless, these camera-based networks reason in camera view where scale is not homogeneous and hence not directly suitable for motion forecasting. In this paper, we introduce a novel monocular camera-only holistic end-to-end trajectory planning network with a Bird-Eye-View (BEV) intermediate representation that comes in the form of binary Occupancy Grid Maps (OGMs). To ease the prediction of OGMs in BEV from camera images, we introduce a novel scheme where the OGMs are first predicted as semantic masks in camera view and then warped in BEV using the homography between the two planes. The key element allowing this transformation to be applied to 3D objects such as vehicles, consists in predicting solely their footprint in camera-view, hence respecting the flat world hypothesis implied by the homography.



### Improved Adaptive Type-2 Fuzzy Filter with Exclusively Two Fuzzy Membership Function for Filtering Salt and Pepper Noise
- **Arxiv ID**: http://arxiv.org/abs/2008.04114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04114v1)
- **Published**: 2020-08-10 13:18:42+00:00
- **Updated**: 2020-08-10 13:18:42+00:00
- **Authors**: Vikas Singh, Pooja Agrawal, Teena Sharma, Nishchal K. Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is one of the preliminary steps in image processing methods in which the presence of noise can deteriorate the image quality. To overcome this limitation, in this paper a improved two-stage fuzzy filter is proposed for filtering salt and pepper noise from the images. In the first-stage, the pixels in the image are categorized as good or noisy based on adaptive thresholding using type-2 fuzzy logic with exclusively two different membership functions in the filter window. In the second-stage, the noisy pixels are denoised using modified ordinary fuzzy logic in the respective filter window. The proposed filter is validated on standard images with various noise levels. The proposed filter removes the noise and preserves useful image characteristics, i.e., edges and corners at higher noise level. The performance of the proposed filter is compared with the various state-of-the-art methods in terms of peak signal-to-noise ratio and computation time. To show the effectiveness of filter statistical tests, i.e., Friedman test and Bonferroni-Dunn (BD) test are also carried out which clearly ascertain that the proposed filter outperforms in comparison of various filtering approaches.



### T-GD: Transferable GAN-generated Images Detection Framework
- **Arxiv ID**: http://arxiv.org/abs/2008.04115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04115v1)
- **Published**: 2020-08-10 13:20:19+00:00
- **Updated**: 2020-08-10 13:20:19+00:00
- **Authors**: Hyeonseong Jeon, Youngoh Bang, Junyaup Kim, Simon S. Woo
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: Recent advancements in Generative Adversarial Networks (GANs) enable the generation of highly realistic images, raising concerns about their misuse for malicious purposes. Detecting these GAN-generated images (GAN-images) becomes increasingly challenging due to the significant reduction of underlying artifacts and specific patterns. The absence of such traces can hinder detection algorithms from identifying GAN-images and transferring knowledge to identify other types of GAN-images as well. In this work, we present the Transferable GAN-images Detection framework T-GD, a robust transferable framework for an effective detection of GAN-images. T-GD is composed of a teacher and a student model that can iteratively teach and evaluate each other to improve the detection performance. First, we train the teacher model on the source dataset and use it as a starting point for learning the target dataset. To train the student model, we inject noise by mixing up the source and target datasets, while constraining the weight variation to preserve the starting point. Our approach is a self-training method, but distinguishes itself from prior approaches by focusing on improving the transferability of GAN-image detection. T-GD achieves high performance on the source dataset by overcoming catastrophic forgetting and effectively detecting state-of-the-art GAN-images with only a small volume of data without any metadata information.



### Learning Bloch Simulations for MR Fingerprinting by Invertible Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04139v2
- **DOI**: 10.1007/978-3-030-61598-7_6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04139v2)
- **Published**: 2020-08-10 14:09:03+00:00
- **Updated**: 2021-03-10 12:32:49+00:00
- **Authors**: Fabian Balsiger, Alain Jungo, Olivier Scheidegger, Benjamin Marty, Mauricio Reyes
- **Comment**: Accepted at MICCAI MLMIR 2020
- **Journal**: Machine Learning for Medical Image Reconstruction. MLMIR 2020.
  Lecture Notes in Computer Science, vol 12450. Springer, Cham
- **Summary**: Magnetic resonance fingerprinting (MRF) enables fast and multiparametric MR imaging. Despite fast acquisition, the state-of-the-art reconstruction of MRF based on dictionary matching is slow and lacks scalability. To overcome these limitations, neural network (NN) approaches estimating MR parameters from fingerprints have been proposed recently. Here, we revisit NN-based MRF reconstruction to jointly learn the forward process from MR parameters to fingerprints and the backward process from fingerprints to MR parameters by leveraging invertible neural networks (INNs). As a proof-of-concept, we perform various experiments showing the benefit of learning the forward process, i.e., the Bloch simulations, for improved MR parameter estimation. The benefit especially accentuates when MR parameter estimation is difficult due to MR physical restrictions. Therefore, INNs might be a feasible alternative to the current solely backward-based NNs for MRF reconstruction.



### Vision Meets Wireless Positioning: Effective Person Re-identification with Recurrent Context Propagation
- **Arxiv ID**: http://arxiv.org/abs/2008.04146v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04146v2)
- **Published**: 2020-08-10 14:19:15+00:00
- **Updated**: 2020-09-04 09:10:28+00:00
- **Authors**: Yiheng Liu, Wengang Zhou, Mao Xi, Sanjing Shen, Houqiang Li
- **Comment**: Accepted by ACM MM 2020 as Oral paper
- **Journal**: None
- **Summary**: Existing person re-identification methods rely on the visual sensor to capture the pedestrians. The image or video data from visual sensor inevitably suffers the occlusion and dramatic variations of pedestrian postures, which degrades the re-identification performance and further limits its application to the open environment. On the other hand, for most people, one of the most important carry-on items is the mobile phone, which can be sensed by WiFi and cellular networks in the form of a wireless positioning signal. Such signal is robust to the pedestrian occlusion and visual appearance change, but suffers some positioning error. In this work, we approach person re-identification with the sensing data from both vision and wireless positioning. To take advantage of such cross-modality cues, we propose a novel recurrent context propagation module that enables information to propagate between visual data and wireless positioning data and finally improves the matching accuracy. To evaluate our approach, we contribute a new Wireless Positioning Person Re-identification (WP-ReID) dataset. Extensive experiments are conducted and demonstrate the effectiveness of the proposed algorithm. Code will be released at https://github.com/yolomax/WP-ReID.



### Deep Sketch-guided Cartoon Video Inbetweening
- **Arxiv ID**: http://arxiv.org/abs/2008.04149v2
- **DOI**: 10.1109/TVCG.2021.3049419
- **Categories**: **cs.CV**, I.2.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.04149v2)
- **Published**: 2020-08-10 14:22:04+00:00
- **Updated**: 2021-01-18 17:15:39+00:00
- **Authors**: Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander
- **Comment**: 15 pages, 16 figures
- **Journal**: None
- **Summary**: We propose a novel framework to produce cartoon videos by fetching the color information from two input keyframes while following the animated motion guided by a user sketch. The key idea of the proposed approach is to estimate the dense cross-domain correspondence between the sketch and cartoon video frames, and employ a blending module with occlusion estimation to synthesize the middle frame guided by the sketch. After that, the input frames and the synthetic frame equipped with established correspondence are fed into an arbitrary-time frame interpolation pipeline to generate and refine additional inbetween frames. Finally, a module to preserve temporal consistency is employed. Compared to common frame interpolation methods, our approach can address frames with relatively large motion and also has the flexibility to enable users to control the generated video sequences by editing the sketch guidance. By explicitly considering the correspondence between frames and the sketch, we can achieve higher quality results than other image synthesis methods. Our results show that our system generalizes well to different movie frames, achieving better results than existing solutions.



### Labels Are Not Perfect: Improving Probabilistic Object Detection via Label Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2008.04168v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04168v1)
- **Published**: 2020-08-10 14:49:49+00:00
- **Updated**: 2020-08-10 14:49:49+00:00
- **Authors**: Di Feng, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer
- **Comment**: A shorter version of this work is to appear at the Workshop on
  Perception for Autonomous Driving, 16th European Conference on Computer
  Vision (ECCV) 2020. Video to this work https://youtu.be/m05HnYietSk
- **Journal**: None
- **Summary**: Reliable uncertainty estimation is crucial for robust object detection in autonomous driving. However, previous works on probabilistic object detection either learn predictive probability for bounding box regression in an un-supervised manner, or use simple heuristics to do uncertainty regularization. This leads to unstable training or suboptimal detection performance. In this work, we leverage our previously proposed method for estimating uncertainty inherent in ground truth bounding box parameters (which we call label uncertainty) to improve the detection accuracy of a probabilistic LiDAR-based object detector. Experimental results on the KITTI dataset show that our method surpasses both the baseline model and the models based on simple heuristics by up to 3.6% in terms of Average Precision.



### Deep Learning-based Human Detection for UAVs with Optical and Infrared Cameras: System and Experiments
- **Arxiv ID**: http://arxiv.org/abs/2008.04197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.04197v1)
- **Published**: 2020-08-10 15:30:42+00:00
- **Updated**: 2020-08-10 15:30:42+00:00
- **Authors**: Timo Hinzmann, Tobias Stegemann, Cesar Cadena, Roland Siegwart
- **Comment**: Initial submission; 21 pages, 16 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we present our deep learning-based human detection system that uses optical (RGB) and long-wave infrared (LWIR) cameras to detect, track, localize, and re-identify humans from UAVs flying at high altitude. In each spectrum, a customized RetinaNet network with ResNet backbone provides human detections which are subsequently fused to minimize the overall false detection rate. We show that by optimizing the bounding box anchors and augmenting the image resolution the number of missed detections from high altitudes can be decreased by over 20 percent. Our proposed network is compared to different RetinaNet and YOLO variants, and to a classical optical-infrared human detection framework that uses hand-crafted features. Furthermore, along with the publication of this paper, we release a collection of annotated optical-infrared datasets recorded with different UAVs during search-and-rescue field tests and the source code of the implemented annotation tool.



### Describe What to Change: A Text-guided Unsupervised Image-to-Image Translation Approach
- **Arxiv ID**: http://arxiv.org/abs/2008.04200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2008.04200v1)
- **Published**: 2020-08-10 15:40:05+00:00
- **Updated**: 2020-08-10 15:40:05+00:00
- **Authors**: Yahui Liu, Marco De Nadai, Deng Cai, Huayang Li, Xavier Alameda-Pineda, Nicu Sebe, Bruno Lepri
- **Comment**: Submitted to ACM MM '20, October 12-16, 2020, Seattle, WA, USA
- **Journal**: None
- **Summary**: Manipulating visual attributes of images through human-written text is a very challenging task. On the one hand, models have to learn the manipulation without the ground truth of the desired output. On the other hand, models have to deal with the inherent ambiguity of natural language. Previous research usually requires either the user to describe all the characteristics of the desired image or to use richly-annotated image captioning datasets. In this work, we propose a novel unsupervised approach, based on image-to-image translation, that alters the attributes of a given image through a command-like sentence such as "change the hair color to black". Contrarily to state-of-the-art approaches, our model does not require a human-annotated dataset nor a textual description of all the attributes of the desired image, but only those that have to be modified. Our proposed model disentangles the image content from the visual attributes, and it learns to modify the latter using the textual description, before generating a new image from the content and the modified attribute representation. Because text might be inherently ambiguous (blond hair may refer to different shadows of blond, e.g. golden, icy, sandy), our method generates multiple stochastic versions of the same translation. Experiments show that the proposed model achieves promising performances on two large-scale public datasets: CelebA and CUB. We believe our approach will pave the way to new avenues of research combining textual and speech commands with visual attributes.



### Deep learning for photoacoustic imaging: a survey
- **Arxiv ID**: http://arxiv.org/abs/2008.04221v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04221v4)
- **Published**: 2020-08-10 15:53:30+00:00
- **Updated**: 2020-12-02 02:02:26+00:00
- **Authors**: Changchun Yang, Hengrong Lan, Feng Gao, Fei Gao
- **Comment**: A review of deep learning for photoacoustic imaging
- **Journal**: None
- **Summary**: Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging. The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.



### Artificial Intelligence to Assist in Exclusion of Coronary Atherosclerosis during CCTA Evaluation of Chest-Pain in the Emergency Department: Preparing an Application for Real-World Use
- **Arxiv ID**: http://arxiv.org/abs/2008.04802v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, I.5.4; I.5.2; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.04802v1)
- **Published**: 2020-08-10 16:07:04+00:00
- **Updated**: 2020-08-10 16:07:04+00:00
- **Authors**: Richard D. White, Barbaros S. Erdal, Mutlu Demirer, Vikash Gupta, Matthew T. Bigelow, Engin Dikici, Sema Candemir, Mauricio S. Galizia, Jessica L. Carpenter, Thomas P. O Donnell, Abdul H. Halabi, Luciano M. Prevedello
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Coronary Computed Tomography Angiography (CCTA) evaluation of chest-pain patients in an Emergency Department (ED) is considered appropriate. While a negative CCTA interpretation supports direct patient discharge from an ED, labor-intensive analyses are required, with accuracy in jeopardy from distractions. We describe the development of an Artificial Intelligence (AI) algorithm and workflow for assisting interpreting physicians in CCTA screening for the absence of coronary atherosclerosis. The two-phase approach consisted of (1) Phase 1 - focused on the development and preliminary testing of an algorithm for vessel-centerline extraction classification in a balanced study population (n = 500 with 50% disease prevalence) derived by retrospective random case selection; and (2) Phase 2 - concerned with simulated-clinical Trialing of the developed algorithm on a per-case basis in a more real-world study population (n = 100 with 28% disease prevalence) from an ED chest-pain series. This allowed pre-deployment evaluation of the AI-based CCTA screening application which provides a vessel-by-vessel graphic display of algorithm inference results integrated into a clinically capable viewer. Algorithm performance evaluation used Area Under the Receiver-Operating-Characteristic Curve (AUC-ROC); confusion matrices reflected ground-truth vs AI determinations. The vessel-based algorithm demonstrated strong performance with AUC-ROC = 0.96. In both Phase 1 and Phase 2, independent of disease prevalence differences, negative predictive values at the case level were very high at 95%. The rate of completion of the algorithm workflow process (96% with inference results in 55-80 seconds) in Phase 2 depended on adequate image quality. There is potential for this AI application to assist in CCTA interpretation to help extricate atherosclerosis from chest-pain presentations.



### Self-Supervised Learning of Audio-Visual Objects from Video
- **Arxiv ID**: http://arxiv.org/abs/2008.04237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.04237v1)
- **Published**: 2020-08-10 16:18:01+00:00
- **Updated**: 2020-08-10 16:18:01+00:00
- **Authors**: Triantafyllos Afouras, Andrew Owens, Joon Son Chung, Andrew Zisserman
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.



### Informative Dropout for Robust Representation Learning: A Shape-bias Perspective
- **Arxiv ID**: http://arxiv.org/abs/2008.04254v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.04254v1)
- **Published**: 2020-08-10 16:52:24+00:00
- **Updated**: 2020-08-10 16:52:24+00:00
- **Authors**: Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang
- **Comment**: Accepted to ICML2020. Code is available at
  https://github.com/bfshi/InfoDrop
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https://github.com/bfshi/InfoDrop.



### Predicting Risk of Developing Diabetic Retinopathy using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.04370v1
- **DOI**: 10.1016/S2589-7500(20)30250-8
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04370v1)
- **Published**: 2020-08-10 19:07:22+00:00
- **Updated**: 2020-08-10 19:07:22+00:00
- **Authors**: Ashish Bora, Siva Balasubramanian, Boris Babenko, Sunny Virmani, Subhashini Venugopalan, Akinori Mitani, Guilherme de Oliveira Marinho, Jorge Cuadros, Paisan Ruamviboonsuk, Greg S Corrado, Lily Peng, Dale R Webster, Avinash V Varadarajan, Naama Hammel, Yun Liu, Pinal Bavishi
- **Comment**: None
- **Journal**: The Lancet Digital Health (2021)
- **Summary**: Diabetic retinopathy (DR) screening is instrumental in preventing blindness, but faces a scaling challenge as the number of diabetic patients rises. Risk stratification for the development of DR may help optimize screening intervals to reduce costs while improving vision-related outcomes. We created and validated two versions of a deep learning system (DLS) to predict the development of mild-or-worse ("Mild+") DR in diabetic patients undergoing DR screening. The two versions used either three-fields or a single field of color fundus photographs (CFPs) as input. The training set was derived from 575,431 eyes, of which 28,899 had known 2-year outcome, and the remaining were used to augment the training process via multi-task learning. Validation was performed on both an internal validation set (set A; 7,976 eyes; 3,678 with known outcome) and an external validation set (set B; 4,762 eyes; 2,345 with known outcome). For predicting 2-year development of DR, the 3-field DLS had an area under the receiver operating characteristic curve (AUC) of 0.79 (95%CI, 0.78-0.81) on validation set A. On validation set B (which contained only a single field), the 1-field DLS's AUC was 0.70 (95%CI, 0.67-0.74). The DLS was prognostic even after adjusting for available risk factors (p<0.001). When added to the risk factors, the 3-field DLS improved the AUC from 0.72 (95%CI, 0.68-0.76) to 0.81 (95%CI, 0.77-0.84) in validation set A, and the 1-field DLS improved the AUC from 0.62 (95%CI, 0.58-0.66) to 0.71 (95%CI, 0.68-0.75) in validation set B. The DLSs in this study identified prognostic information for DR development from CFPs. This information is independent of and more informative than the available risk factors.



### Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss
- **Arxiv ID**: http://arxiv.org/abs/2008.04378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04378v1)
- **Published**: 2020-08-10 19:33:47+00:00
- **Updated**: 2020-08-10 19:33:47+00:00
- **Authors**: Yang Li, Shichao Kan, Zhihai He
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Existing approaches for unsupervised metric learning focus on exploring self-supervision information within the input image itself. We observe that, when analyzing images, human eyes often compare images against each other instead of examining images individually. In addition, they often pay attention to certain keypoints, image regions, or objects which are discriminative between image classes but highly consistent within classes. Even if the image is being transformed, the attention pattern will be consistent. Motivated by this observation, we develop a new approach to unsupervised deep metric learning where the network is learned based on self-supervision information across images instead of within one single image. To characterize the consistent pattern of human attention during image comparisons, we introduce the idea of transformed attention consistency. It assumes that visually similar images, even undergoing different image transforms, should share the same consistent visual attention map. This consistency leads to a pairwise self-supervision loss, allowing us to learn a Siamese deep neural network to encode and compare images against their transformed or matched pairs. To further enhance the inter-class discriminative power of the feature generated by this network, we adapt the concept of triplet loss from supervised metric learning to our unsupervised case and introduce the contrastive clustering loss. Our extensive experimental results on benchmark datasets demonstrate that our proposed method outperforms current state-of-the-art methods for unsupervised metric learning by a large margin.



### Bipartite Graph Reasoning GANs for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.04381v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04381v2)
- **Published**: 2020-08-10 19:37:10+00:00
- **Updated**: 2020-08-20 22:01:35+00:00
- **Authors**: Hao Tang, Song Bai, Philip H. S. Torr, Nicu Sebe
- **Comment**: 13 pages, 6 figures, accepted to BMVC 2020 as an oral paper, fix
  typos
- **Journal**: None
- **Summary**: We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets, i.e., Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN.



### GANBERT: Generative Adversarial Networks with Bidirectional Encoder Representations from Transformers for MRI to PET synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.04393v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04393v1)
- **Published**: 2020-08-10 20:07:33+00:00
- **Updated**: 2020-08-10 20:07:33+00:00
- **Authors**: Hoo-Chang Shin, Alvin Ihsani, Swetha Mandava, Sharath Turuvekere Sreenivas, Christopher Forster, Jiook Cha, Alzheimer's Disease Neuroimaging Initiative
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing medical images, such as PET, is a challenging task due to the fact that the intensity range is much wider and denser than those in photographs and digital renderings and are often heavily biased toward zero. Above all, intensity values in PET have absolute significance, and are used to compute parameters that are reproducible across the population. Yet, usually much manual adjustment has to be made in pre-/post- processing when synthesizing PET images, because its intensity ranges can vary a lot, e.g., between -100 to 1000 in floating point values. To overcome these challenges, we adopt the Bidirectional Encoder Representations from Transformers (BERT) algorithm that has had great success in natural language processing (NLP), where wide-range floating point intensity values are represented as integers ranging between 0 to 10000 that resemble a dictionary of natural language vocabularies. BERT is then trained to predict a proportion of masked values images, where its "next sentence prediction (NSP)" acts as GAN discriminator. Our proposed approach, is able to generate PET images from MRI images in wide intensity range, with no manual adjustments in pre-/post- processing. It is a method that can scale and ready to deploy.



### GANDALF: Generative Adversarial Networks with Discriminator-Adaptive Loss Fine-tuning for Alzheimer's Disease Diagnosis from MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.04396v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04396v1)
- **Published**: 2020-08-10 20:09:35+00:00
- **Updated**: 2020-08-10 20:09:35+00:00
- **Authors**: Hoo-Chang Shin, Alvin Ihsani, Ziyue Xu, Swetha Mandava, Sharath Turuvekere Sreenivas, Christopher Forster, Jiook Cha, Alzheimer's Disease Neuroimaging Initiative
- **Comment**: Accepted for publication at the MICCAI 2020 conference
- **Journal**: None
- **Summary**: Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.



### Locating Cephalometric X-Ray Landmarks with Foveated Pyramid Attention
- **Arxiv ID**: http://arxiv.org/abs/2008.04428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04428v1)
- **Published**: 2020-08-10 21:44:45+00:00
- **Updated**: 2020-08-10 21:44:45+00:00
- **Authors**: Logan Gilmour, Nilanjan Ray
- **Comment**: Presented at MIDL 2020
- **Journal**: None
- **Summary**: CNNs, initially inspired by human vision, differ in a key way: they sample uniformly, rather than with highest density in a focal point. For very large images, this makes training untenable, as the memory and computation required for activation maps scales quadratically with the side length of an image. We propose an image pyramid based approach that extracts narrow glimpses of the of the input image and iteratively refines them to accomplish regression tasks. To assist with high-accuracy regression, we introduce a novel intermediate representation we call 'spatialized features'. Our approach scales logarithmically with the side length, so it works with very large images. We apply our method to Cephalometric X-ray Landmark Detection and get state-of-the-art results.



### Measures of Complexity for Large Scale Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/2008.04431v1
- **DOI**: 10.1109/ICAIIC48513.2020.9065274
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04431v1)
- **Published**: 2020-08-10 21:54:23+00:00
- **Updated**: 2020-08-10 21:54:23+00:00
- **Authors**: Ameet Annasaheb Rahane, Anbumani Subramanian
- **Comment**: 6 pages, 3 tables, 4 figures
- **Journal**: None
- **Summary**: Large scale image datasets are a growing trend in the field of machine learning. However, it is hard to quantitatively understand or specify how various datasets compare to each other - i.e., if one dataset is more complex or harder to ``learn'' with respect to a deep-learning based network. In this work, we build a series of relatively computationally simple methods to measure the complexity of a dataset. Furthermore, we present an approach to demonstrate visualizations of high dimensional data, in order to assist with visual comparison of datasets. We present our analysis using four datasets from the autonomous driving research community - Cityscapes, IDD, BDD and Vistas. Using entropy based metrics, we present a rank-order complexity of these datasets, which we compare with an established rank-order with respect to deep learning.



### Distributed Multi-agent Video Fast-forwarding
- **Arxiv ID**: http://arxiv.org/abs/2008.04437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04437v1)
- **Published**: 2020-08-10 22:08:49+00:00
- **Updated**: 2020-08-10 22:08:49+00:00
- **Authors**: Shuyue Lan, Zhilu Wang, Amit K. Roy-Chowdhury, Ermin Wei, Qi Zhu
- **Comment**: To appear at ACM Multimedia 2020
- **Journal**: None
- **Summary**: In many intelligent systems, a network of agents collaboratively perceives the environment for better and more efficient situation awareness. As these agents often have limited resources, it could be greatly beneficial to identify the content overlapping among camera views from different agents and leverage it for reducing the processing, transmission and storage of redundant/unimportant video frames. This paper presents a consensus-based distributed multi-agent video fast-forwarding framework, named DMVF, that fast-forwards multi-view video streams collaboratively and adaptively. In our framework, each camera view is addressed by a reinforcement learning based fast-forwarding agent, which periodically chooses from multiple strategies to selectively process video frames and transmits the selected frames at adjustable paces. During every adaptation period, each agent communicates with a number of neighboring agents, evaluates the importance of the selected frames from itself and those from its neighbors, refines such evaluation together with other agents via a system-wide consensus algorithm, and uses such evaluation to decide their strategy for the next period. Compared with approaches in the literature on a real-world surveillance video dataset VideoWeb, our method significantly improves the coverage of important frames and also reduces the number of frames processed in the system.



### Spatio-temporal Attention Model for Tactile Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.04442v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04442v1)
- **Published**: 2020-08-10 22:32:34+00:00
- **Updated**: 2020-08-10 22:32:34+00:00
- **Authors**: Guanqun Cao, Yi Zhou, Danushka Bollegala, Shan Luo
- **Comment**: 7 pages, accepted by International Conference on Intelligent Robots
  and Systems 2020
- **Journal**: None
- **Summary**: Recently, tactile sensing has attracted great interest in robotics, especially for facilitating exploration of unstructured environments and effective manipulation. A detailed understanding of the surface textures via tactile sensing is essential for many of these tasks. Previous works on texture recognition using camera based tactile sensors have been limited to treating all regions in one tactile image or all samples in one tactile sequence equally, which includes much irrelevant or redundant information. In this paper, we propose a novel Spatio-Temporal Attention Model (STAM) for tactile texture recognition, which is the very first of its kind to our best knowledge. The proposed STAM pays attention to both spatial focus of each single tactile texture and the temporal correlation of a tactile sequence. In the experiments to discriminate 100 different fabric textures, the spatially and temporally selective attention has resulted in a significant improvement of the recognition accuracy, by up to 18.8%, compared to the non-attention based models. Specifically, after introducing noisy data that is collected before the contact happens, our proposed STAM can learn the salient features efficiently and the accuracy can increase by 15.23% on average compared with the CNN based baseline approach. The improved tactile texture perception can be applied to facilitate robot tasks like grasping and manipulation.



### Grasping Field: Learning Implicit Representations for Human Grasps
- **Arxiv ID**: http://arxiv.org/abs/2008.04451v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04451v3)
- **Published**: 2020-08-10 23:08:26+00:00
- **Updated**: 2020-11-26 16:07:13+00:00
- **Authors**: Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.



