# Arxiv Papers in cs.CV on 2020-08-07
### Predicting Visual Importance Across Graphic Design Types
- **Arxiv ID**: http://arxiv.org/abs/2008.02912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02912v1)
- **Published**: 2020-08-07 00:12:18+00:00
- **Updated**: 2020-08-07 00:12:18+00:00
- **Authors**: Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter O'Donovan, Aaron Hertzmann, Zoya Bylinskii
- **Comment**: None
- **Journal**: Proceedings of UIST 2020
- **Summary**: This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance. The model, code, and importance dataset are available at https://predimportance.mit.edu .



### An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local Shape Querying
- **Arxiv ID**: http://arxiv.org/abs/2008.02916v1
- **DOI**: 10.1016/j.cag.2020.09.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02916v1)
- **Published**: 2020-08-07 00:46:58+00:00
- **Updated**: 2020-08-07 00:46:58+00:00
- **Authors**: Bart Iver van Blokland, Theoharis Theoharis
- **Comment**: 13 pages, 13 figures, to be published in a Special Issue in Computers
  & Graphics
- **Journal**: Computers & Graphics Volume 92, November 2020, Pages 55-66
- **Summary**: A binary descriptor indexing scheme based on Hamming distance called the Hamming tree for local shape queries is presented. A new binary clutter resistant descriptor named Quick Intersection Count Change Image (QUICCI) is also introduced. This local shape descriptor is extremely small and fast to compare. Additionally, a novel distance function called Weighted Hamming applicable to QUICCI images is proposed for retrieval applications. The effectiveness of the indexing scheme and QUICCI is demonstrated on 828 million QUICCI images derived from the SHREC2017 dataset, while the clutter resistance of QUICCI is shown using the clutterbox experiment.



### Polysemy Deciphering Network for Robust Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.02918v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02918v3)
- **Published**: 2020-08-07 00:49:27+00:00
- **Updated**: 2021-03-24 01:13:06+00:00
- **Authors**: Xubin Zhong, Changxing Ding, Xian Qu, Dacheng Tao
- **Comment**: The IJCV version extended significantly from our ECCV2020 conference
  paper
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection is important to human-centric scene understanding tasks. Existing works tend to assume that the same verb has similar visual characteristics in different HOI categories, an approach that ignores the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net) that decodes the visual polysemy of verbs for HOI detection in three distinct ways. First, we refine features for HOI detection to be polysemyaware through the use of two novel modules: namely, Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). LPCA highlights important elements in human and object appearance features for each HOI category to be identified; moreover, LPFA augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce intra-class variation for the same verb. Second, we introduce a novel Polysemy-Aware Modal Fusion module (PAMF), which guides PD-Net to make decisions based on feature types deemed more important according to the language priors. Third, we propose to relieve the verb polysemy problem through sharing verb classifiers for semantically similar HOI categories. Furthermore, to expedite research on the verb polysemy problem, we build a new benchmark dataset named HOI-VerbPolysemy (HOIVP), which includes common verbs (predicates) that have diverse semantic meanings in the real world. Finally, through deciphering the visual polysemy of verbs, our approach is demonstrated to outperform state-of-the-art methods by significant margins on the HICO-DET, V-COCO, and HOI-VP databases. Code and data in this paper are available at https://github.com/MuchHair/PD-Net.



### A Deeper Look at Salient Object Detection: Bi-stream Network with a Small Training Dataset
- **Arxiv ID**: http://arxiv.org/abs/2008.02938v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02938v1)
- **Published**: 2020-08-07 01:24:33+00:00
- **Updated**: 2020-08-07 01:24:33+00:00
- **Authors**: Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with the conventional hand-crafted approaches, the deep learning based methods have achieved tremendous performance improvements by training exquisitely crafted fancy networks over large-scale training sets. However, do we really need large-scale training set for salient object detection (SOD)? In this paper, we provide a deeper insight into the interrelationship between the SOD performances and the training sets. To alleviate the conventional demands for large-scale training data, we provide a feasible way to construct a novel small-scale training set, which only contains 4K images. Moreover, we propose a novel bi-stream network to take full advantage of our proposed small training set, which is consisted of two feature backbones with different structures, achieving complementary semantical saliency fusion via the proposed gate control unit. To our best knowledge, this is the first attempt to use a small-scale training set to outperform state-of-the-art models which are trained on large-scale training sets; nevertheless, our method can still achieve the leading state-of-the-art performance on five benchmark datasets.



### Few Shot Learning Framework to Reduce Inter-observer Variability in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2008.02952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02952v1)
- **Published**: 2020-08-07 02:05:51+00:00
- **Updated**: 2020-08-07 02:05:51+00:00
- **Authors**: Sohini Roychowdhury
- **Comment**: 8 pages, 8 figures, 4 tables
- **Journal**: ICPR 2020
- **Summary**: Most computer aided pathology detection systems rely on large volumes of quality annotated data to aid diagnostics and follow up procedures. However, quality assuring large volumes of annotated medical image data can be subjective and expensive. In this work we present a novel standardization framework that implements three few-shot learning (FSL) models that can be iteratively trained by atmost 5 images per 3D stack to generate multiple regional proposals (RPs) per test image. These FSL models include a novel parallel echo state network (ParESN) framework and an augmented U-net model. Additionally, we propose a novel target label selection algorithm (TLSA) that measures relative agreeability between RPs and the manually annotated target labels to detect the "best" quality annotation per image. Using the FSL models, our system achieves 0.28-0.64 Dice coefficient across vendor image stacks for intra-retinal cyst segmentation. Additionally, the TLSA is capable of automatically classifying high quality target labels from their noisy counterparts for 60-97% of the images while ensuring manual supervision on remaining images. Also, the proposed framework with ParESN model minimizes manual annotation checking to 12-28% of the total number of images. The TLSA metrics further provide confidence scores for the automated annotation quality assurance. Thus, the proposed framework is flexible to extensions for quality image annotation curation of other image stacks as well.



### Location-aware Graph Convolutional Networks for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2008.09105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2008.09105v1)
- **Published**: 2020-08-07 02:12:56+00:00
- **Updated**: 2020-08-07 02:12:56+00:00
- **Authors**: Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, Chuang Gan
- **Comment**: None
- **Journal**: None
- **Summary**: We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning. In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action. As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering. Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets. Code and pre-trained models are publicly available at: https://github.com/SunDoge/L-GCN



### A Novel Video Salient Object Detection Method via Semi-supervised Motion Quality Perception
- **Arxiv ID**: http://arxiv.org/abs/2008.02966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02966v1)
- **Published**: 2020-08-07 02:58:51+00:00
- **Updated**: 2020-08-07 02:58:51+00:00
- **Authors**: Chenglizhao Chen, Jia Song, Chong Peng, Guodong Wang, Yuming Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Previous video salient object detection (VSOD) approaches have mainly focused on designing fancy networks to achieve their performance improvements. However, with the slow-down in development of deep learning techniques recently, it may become more and more difficult to anticipate another breakthrough via fancy networks solely. To this end, this paper proposes a universal learning scheme to get a further 3\% performance improvement for all state-of-the-art (SOTA) methods. The major highlight of our method is that we resort the "motion quality"---a brand new concept, to select a sub-group of video frames from the original testing set to construct a new training set. The selected frames in this new training set should all contain high-quality motions, in which the salient objects will have large probability to be successfully detected by the "target SOTA method"---the one we want to improve. Consequently, we can achieve a significant performance improvement by using this new training set to start a new round of network training. During this new round training, the VSOD results of the target SOTA method will be applied as the pseudo training objectives. Our novel learning scheme is simple yet effective, and its semi-supervised methodology may have large potential to inspire the VSOD community in the future.



### Exploring Rich and Efficient Spatial Temporal Interactions for Real Time Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.02973v1
- **DOI**: 10.1109/TIP.2021.3068644
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02973v1)
- **Published**: 2020-08-07 03:24:04+00:00
- **Updated**: 2020-08-07 03:24:04+00:00
- **Authors**: Chenglizhao Chen, Guotao Wang, Chong Peng, Dingwen Zhang, Yuming Fang, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: The current main stream methods formulate their video saliency mainly from two independent venues, i.e., the spatial and temporal branches. As a complementary component, the main task for the temporal branch is to intermittently focus the spatial branch on those regions with salient movements. In this way, even though the overall video saliency quality is heavily dependent on its spatial branch, however, the performance of the temporal branch still matter. Thus, the key factor to improve the overall video saliency is how to further boost the performance of these branches efficiently. In this paper, we propose a novel spatiotemporal network to achieve such improvement in a full interactive fashion. We integrate a lightweight temporal model into the spatial branch to coarsely locate those spatially salient regions which are correlated with trustworthy salient movements. Meanwhile, the spatial branch itself is able to recurrently refine the temporal model in a multi-scale manner. In this way, both the spatial and temporal branches are able to interact with each other, achieving the mutual performance improvement. Our method is easy to implement yet effective, achieving high quality video saliency detection in real-time speed with 50 FPS.



### Textual Description for Mathematical Equations
- **Arxiv ID**: http://arxiv.org/abs/2008.02980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02980v1)
- **Published**: 2020-08-07 03:46:32+00:00
- **Updated**: 2020-08-07 03:46:32+00:00
- **Authors**: Ajoy Mondal, C. V. Jawahar
- **Comment**: 8
- **Journal**: Published in ICDAR 2019
- **Summary**: Reading of mathematical expression or equation in the document images is very challenging due to the large variability of mathematical symbols and expressions. In this paper, we pose reading of mathematical equation as a task of generation of the textual description which interprets the internal meaning of this equation. Inspired by the natural image captioning problem in computer vision, we present a mathematical equation description (MED) model, a novel end-to-end trainable deep neural network based approach that learns to generate a textual description for reading mathematical equation images. Our MED model consists of a convolution neural network as an encoder that extracts features of input mathematical equation images and a recurrent neural network with attention mechanism which generates description related to the input mathematical equation images. Due to the unavailability of mathematical equation image data sets with their textual descriptions, we generate two data sets for experimental purpose. To validate the effectiveness of our MED model, we conduct a real-world experiment to see whether the students are able to write equations by only reading or listening their textual descriptions or not. Experiments conclude that the students are able to write most of the equations correctly by reading their textual descriptions only.



### NuI-Go: Recursive Non-Local Encoder-Decoder Network for Retinal Image Non-Uniform Illumination Removal
- **Arxiv ID**: http://arxiv.org/abs/2008.02984v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02984v1)
- **Published**: 2020-08-07 04:31:33+00:00
- **Updated**: 2020-08-07 04:31:33+00:00
- **Authors**: Chongyi Li, Huazhu Fu, Runmin Cong, Zechao Li, Qianqian Xu
- **Comment**: ACM MM2020
- **Journal**: None
- **Summary**: Retinal images have been widely used by clinicians for early diagnosis of ocular diseases. However, the quality of retinal images is often clinically unsatisfactory due to eye lesions and imperfect imaging process. One of the most challenging quality degradation issues in retinal images is non-uniform which hinders the pathological information and further impairs the diagnosis of ophthalmologists and computer-aided analysis.To address this issue, we propose a non-uniform illumination removal network for retinal image, called NuI-Go, which consists of three Recursive Non-local Encoder-Decoder Residual Blocks (NEDRBs) for enhancing the degraded retinal images in a progressive manner. Each NEDRB contains a feature encoder module that captures the hierarchical feature representations, a non-local context module that models the context information, and a feature decoder module that recovers the details and spatial dimension. Additionally, the symmetric skip-connections between the encoder module and the decoder module provide long-range information compensation and reuse. Extensive experiments demonstrate that the proposed method can effectively remove the non-uniform illumination on retinal images while well preserving the image details and color. We further demonstrate the advantages of the proposed method for improving the accuracy of retinal vessel segmentation.



### Global Context Aware Convolutions for 3D Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/2008.02986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02986v1)
- **Published**: 2020-08-07 04:33:27+00:00
- **Updated**: 2020-08-07 04:33:27+00:00
- **Authors**: Zhiyuan Zhang, Binh-Son Hua, Wei Chen, Yibin Tian, Sai-Kit Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning for 3D point clouds have shown great promises in scene understanding tasks thanks to the introduction of convolution operators to consume 3D point clouds directly in a neural network. Point cloud data, however, could have arbitrary rotations, especially those acquired from 3D scanning. Recent works show that it is possible to design point cloud convolutions with rotation invariance property, but such methods generally do not perform as well as translation-invariant only convolution. We found that a key reason is that compared to point coordinates, rotation-invariant features consumed by point cloud convolution are not as distinctive. To address this problem, we propose a novel convolution operator that enhances feature distinction by integrating global context information from the input point cloud to the convolution. To this end, a globally weighted local reference frame is constructed in each point neighborhood in which the local point set is decomposed into bins. Anchor points are generated in each bin to represent global shape features. A convolution can then be performed to transform the points and anchor features into final rotation-invariant features. We conduct several experiments on point cloud classification, part segmentation, shape retrieval, and normals estimation to evaluate our convolution, which achieves state-of-the-art accuracy under challenging rotations.



### Leveraging Localization for Multi-camera Association
- **Arxiv ID**: http://arxiv.org/abs/2008.02992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02992v1)
- **Published**: 2020-08-07 05:16:27+00:00
- **Updated**: 2020-08-07 05:16:27+00:00
- **Authors**: Zhongang Cai, Cunjun Yu, Junzhe Zhang, Jiawei Ren, Haiyu Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We present McAssoc, a deep learning approach to the as-sociation of detection bounding boxes in different views ofa multi-camera system. The vast majority of the academiahas been developing single-camera computer vision algo-rithms, however, little research attention has been directedto incorporating them into a multi-camera system. In thispaper, we designed a 3-branch architecture that leveragesdirect association and additional cross localization infor-mation. A new metric, image-pair association accuracy(IPAA) is designed specifically for performance evaluationof cross-camera detection association. We show in the ex-periments that localization information is critical to suc-cessful cross-camera association, especially when similar-looking objects are present. This paper is an experimentalwork prior to MessyTable, which is a large-scale bench-mark for instance association in mutliple cameras.



### Single-stage intake gesture detection using CTC loss and extended prefix beam search
- **Arxiv ID**: http://arxiv.org/abs/2008.02999v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02999v2)
- **Published**: 2020-08-07 06:04:25+00:00
- **Updated**: 2020-11-21 01:05:45+00:00
- **Authors**: Philipp V. Rouast, Marc T. P. Adam
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection of individual intake gestures is a key step towards automatic dietary monitoring. Both inertial sensor data of wrist movements and video data depicting the upper body have been used for this purpose. The most advanced approaches to date use a two-stage approach, in which (i) frame-level intake probabilities are learned from the sensor data using a deep neural network, and then (ii) sparse intake events are detected by finding the maxima of the frame-level probabilities. In this study, we propose a single-stage approach which directly decodes the probabilities learned from sensor data into sparse intake detections. This is achieved by weakly supervised training using Connectionist Temporal Classification (CTC) loss, and decoding using a novel extended prefix beam search decoding algorithm. Benefits of this approach include (i) end-to-end training for detections, (ii) simplified timing requirements for intake gesture labels, and (iii) improved detection performance compared to existing approaches. Across two separate datasets, we achieve relative $F_1$ score improvements between 1.9% and 6.2% over the two-stage approach for intake detection and eating/drinking detection tasks, for both video and inertial sensors.



### An Aggregate Method for Thorax Diseases Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.03008v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03008v5)
- **Published**: 2020-08-07 06:36:07+00:00
- **Updated**: 2020-12-24 12:35:14+00:00
- **Authors**: Bayu A. Nugroho
- **Comment**: code available: https://github.com/bayu-ladom-ipok/weOpen
- **Journal**: None
- **Summary**: A common problem found in real-word medical image classification is the inherent imbalance of the positive and negative patterns in the dataset where positive patterns are usually rare. Moreover, in the classification of multiple classes with neural network, a training pattern is treated as a positive pattern in one output node and negative in all the remaining output nodes. In this paper, the weights of a training pattern in the loss function are designed based not only on the number of the training patterns in the class but also on the different nodes where one of them treats this training pattern as positive and the others treat it as negative. We propose a combined approach of weights calculation algorithm for deep network training and the training optimization from the state-of-the-art deep network architecture for thorax diseases classification problem. Experimental results on the Chest X-Ray image dataset demonstrate that this new weighting scheme improves classification performances, also the training optimization from the EfficientNet improves the performance furthermore. We compare the aggregate method with several performances from the previous study of thorax diseases classifications to provide the fair comparisons against the proposed method.



### A Multi-Task Learning Approach for Human Activity Segmentation and Ergonomics Risk Assessment
- **Arxiv ID**: http://arxiv.org/abs/2008.03014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03014v2)
- **Published**: 2020-08-07 06:53:56+00:00
- **Updated**: 2020-12-02 00:03:53+00:00
- **Authors**: Behnoosh Parsa, Ashis G. Banerjee
- **Comment**: To appear at the 2021 Winter Conference on Applications of Computer
  Vision (WACV'21)
- **Journal**: None
- **Summary**: We propose a new approach to Human Activity Evaluation (HAE) in long videos using graph-based multi-task modeling. Previous works in activity evaluation either directly compute a metric using a detected skeleton or use the scene information to regress the activity score. These approaches are insufficient for accurate activity assessment since they only compute an average score over a clip, and do not consider the correlation between the joints and body dynamics. Moreover, they are highly scene-dependent which makes the generalizability of these methods questionable. We propose a novel multi-task framework for HAE that utilizes a Graph Convolutional Network backbone to embed the interconnections between human joints in the features. In this framework, we solve the Human Activity Segmentation (HAS) problem as an auxiliary task to improve activity assessment. The HAS head is powered by an Encoder-Decoder Temporal Convolutional Network to semantically segment long videos into distinct activity classes, whereas, HAE uses a Long-Short-Term-Memory-based architecture. We evaluate our method on the UW-IOM and TUM Kitchen datasets and discuss the success and failure cases in these two datasets.



### Deep Robust Clustering by Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.03030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03030v2)
- **Published**: 2020-08-07 08:05:53+00:00
- **Updated**: 2020-08-27 05:42:48+00:00
- **Authors**: Huasong Zhong, Chong Chen, Zhongming Jin, Xian-Sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many unsupervised deep learning methods have been proposed to learn clustering with unlabelled data. By introducing data augmentation, most of the latest methods look into deep clustering from the perspective that the original image and its transformation should share similar semantic clustering assignment. However, the representation features could be quite different even they are assigned to the same cluster since softmax function is only sensitive to the maximum value. This may result in high intra-class diversities in the representation feature space, which will lead to unstable local optimal and thus harm the clustering performance. To address this drawback, we proposed Deep Robust Clustering (DRC). Different from existing methods, DRC looks into deep clustering from two perspectives of both semantic clustering assignment and representation feature, which can increase inter-class diversities and decrease intra-class diversities simultaneously. Furthermore, we summarized a general framework that can turn any maximizing mutual information into minimizing contrastive loss by investigating the internal relationship between mutual information and contrastive learning. And we successfully applied it in DRC to learn invariant features and robust clusters. Extensive experiments on six widely-adopted deep clustering benchmarks demonstrate the superiority of DRC in both stability and accuracy. e.g., attaining 71.6% mean accuracy on CIFAR-10, which is 7.1% higher than state-of-the-art results.



### Fighting Deepfake by Exposing the Convolutional Traces on Images
- **Arxiv ID**: http://arxiv.org/abs/2008.04095v1
- **DOI**: 10.1109/ACCESS.2020.3023037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04095v1)
- **Published**: 2020-08-07 08:49:23+00:00
- **Updated**: 2020-08-07 08:49:23+00:00
- **Authors**: Luca Guarnera, Oliver Giudice, Sebastiano Battiato
- **Comment**: arXiv admin note: text overlap with arXiv:2004.10448
- **Journal**: IEEE Access 2020
- **Summary**: Advances in Artificial Intelligence and Image Processing are changing the way people interacts with digital images and video. Widespread mobile apps like FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to produce extreme transformations on human face photos such gender swap, aging, etc. The results are utterly realistic and extremely easy to be exploited even for non-experienced users. This kind of media object took the name of Deepfake and raised a new challenge in the multimedia forensics field: the Deepfake detection challenge. Indeed, discriminating a Deepfake from a real image could be a difficult task even for human eyes but recent works are trying to apply the same technology used for generating images for discriminating them with preliminary good results but with many limitations: employed Convolutional Neural Networks are not so robust, demonstrate to be specific to the context and tend to extract semantics from images. In this paper, a new approach aimed to extract a Deepfake fingerprint from images is proposed. The method is based on the Expectation-Maximization algorithm trained to detect and extract a fingerprint that represents the Convolutional Traces (CT) left by GANs during image generation. The CT demonstrates to have high discriminative power achieving better results than state-of-the-art in the Deepfake detection task also proving to be robust to different attacks. Achieving an overall classification accuracy of over 98%, considering Deepfakes from 10 different GAN architectures not only involved in images of faces, the CT demonstrates to be reliable and without any dependence on image semantic. Finally, tests carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the fake detection task, demonstrated the effectiveness of the proposed technique on a real-case scenario.



### Adversarial Examples on Object Recognition: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2008.04094v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04094v2)
- **Published**: 2020-08-07 08:51:21+00:00
- **Updated**: 2020-09-03 09:53:48+00:00
- **Authors**: Alex Serban, Erik Poll, Joost Visser
- **Comment**: Published in ACM CSUR. arXiv admin note: text overlap with
  arXiv:1810.01185
- **Journal**: None
- **Summary**: Deep neural networks are at the forefront of machine learning research. However, despite achieving impressive performance on complex tasks, they can be very sensitive: Small perturbations of inputs can be sufficient to induce incorrect behavior. Such perturbations, called adversarial examples, are intentionally designed to test the network's sensitivity to distribution drifts. Given their surprisingly small size, a wide body of literature conjectures on their existence and how this phenomenon can be mitigated. In this article we discuss the impact of adversarial examples on security, safety, and robustness of neural networks. We start by introducing the hypotheses behind their existence, the methods used to construct or protect against them, and the capacity to transfer adversarial examples between different machine learning models. Altogether, the goal is to provide a comprehensive and self-contained survey of this growing field of research.



### Improving Multispectral Pedestrian Detection by Addressing Modality Imbalance Problems
- **Arxiv ID**: http://arxiv.org/abs/2008.03043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03043v2)
- **Published**: 2020-08-07 08:58:46+00:00
- **Updated**: 2020-08-17 02:21:34+00:00
- **Authors**: Kailai Zhou, Linsen Chen, Xun Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral pedestrian detection is capable of adapting to insufficient illumination conditions by leveraging color-thermal modalities. On the other hand, it is still lacking of in-depth insights on how to fuse the two modalities effectively. Compared with traditional pedestrian detection, we find multispectral pedestrian detection suffers from modality imbalance problems which will hinder the optimization process of dual-modality network and depress the performance of detector. Inspired by this observation, we propose Modality Balance Network (MBNet) which facilitates the optimization process in a much more flexible and balanced manner. Firstly, we design a novel Differential Modality Aware Fusion (DMAF) module to make the two modalities complement each other. Secondly, an illumination aware feature alignment module selects complementary features according to the illumination conditions and aligns the two modality features adaptively. Extensive experimental results demonstrate MBNet outperforms the state-of-the-arts on both the challenging KAIST and CVC-14 multispectral pedestrian datasets in terms of the accuracy and the computational efficiency. Code is available at https://github.com/CalayZhou/MBNet.



### Depth Quality Aware Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04159v1
- **DOI**: 10.1109/TIP.2021.3052069
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04159v1)
- **Published**: 2020-08-07 09:54:39+00:00
- **Updated**: 2020-08-07 09:54:39+00:00
- **Authors**: Chenglizhao Chen, Jipeng Wei, Chong Peng, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: The existing fusion based RGB-D salient object detection methods usually adopt the bi-stream structure to strike the fusion trade-off between RGB and depth (D). The D quality usually varies from scene to scene, while the SOTA bi-stream approaches are depth quality unaware, which easily result in substantial difficulties in achieving complementary fusion status between RGB and D, leading to poor fusion results in facing of low-quality D. Thus, this paper attempts to integrate a novel depth quality aware subnet into the classic bi-stream structure, aiming to assess the depth quality before conducting the selective RGB-D fusion. Compared with the SOTA bi-stream methods, the major highlight of our method is its ability to lessen the importance of those low-quality, no-contribution, or even negative-contribution D regions during the RGB-D fusion, achieving a much improved complementary status between RGB and D.



### Evaluating Efficient Performance Estimators of Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/2008.03064v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03064v5)
- **Published**: 2020-08-07 09:56:54+00:00
- **Updated**: 2021-10-29 10:05:14+00:00
- **Authors**: Xuefei Ning, Changcheng Tang, Wenshuo Li, Zixuan Zhou, Shuang Liang, Huazhong Yang, Yu Wang
- **Comment**: accepted by NeurIPS 2021 (10 page main texts)
- **Journal**: None
- **Summary**: Conducting efficient performance estimations of neural architectures is a major challenge in neural architecture search (NAS). To reduce the architecture training costs in NAS, one-shot estimators (OSEs) amortize the architecture training costs by sharing the parameters of one "supernet" between all architectures. Recently, zero-shot estimators (ZSEs) that involve no training are proposed to further reduce the architecture evaluation cost. Despite the high efficiency of these estimators, the quality of such estimations has not been thoroughly studied. In this paper, we conduct an extensive and organized assessment of OSEs and ZSEs on five NAS benchmarks: NAS-Bench-101/201/301, and NDS ResNet/ResNeXt-A. Specifically, we employ a set of NAS-oriented criteria to study the behavior of OSEs and ZSEs and reveal that they have certain biases and variances. After analyzing how and why the OSE estimations are unsatisfying, we explore how to mitigate the correlation gap of OSEs from several perspectives. Through our analysis, we give out suggestions for future application and development of efficient architecture performance estimators. Furthermore, the analysis framework proposed in our work could be utilized in future research to give a more comprehensive understanding of newly designed architecture performance estimators. All codes are available at https://github.com/walkerning/aw_nas.



### Oversampling Adversarial Network for Class-Imbalanced Fault Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2008.03071v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03071v1)
- **Published**: 2020-08-07 10:12:07+00:00
- **Updated**: 2020-08-07 10:12:07+00:00
- **Authors**: Masoumeh Zareapoor, Pourya Shamsolmoali, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The collected data from industrial machines are often imbalanced, which poses a negative effect on learning algorithms. However, this problem becomes more challenging for a mixed type of data or while there is overlapping between classes. Class-imbalance problem requires a robust learning system which can timely predict and classify the data. We propose a new adversarial network for simultaneous classification and fault detection. In particular, we restore the balance in the imbalanced dataset by generating faulty samples from the proposed mixture of data distribution. We designed the discriminator of our model to handle the generated faulty samples to prevent outlier and overfitting. We empirically demonstrate that; (i) the discriminator trained with a generator to generates samples from a mixture of normal and faulty data distribution which can be considered as a fault detector; (ii), the quality of the generated faulty samples outperforms the other synthetic resampling techniques. Experimental results show that the proposed model performs well when comparing to other fault diagnosis methods across several evaluation metrics; in particular, coalescing of generative adversarial network (GAN) and feature matching function is effective at recognizing faulty samples.



### Data-Level Recombination and Lightweight Fusion Scheme for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.05102v1
- **DOI**: 10.1109/TIP.2020.3037470
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05102v1)
- **Published**: 2020-08-07 10:13:05+00:00
- **Updated**: 2020-08-07 10:13:05+00:00
- **Authors**: Xuehao Wang, Shuai Li, Chenglizhao Chen, Yuming Fang, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Existing RGB-D salient object detection methods treat depth information as an independent component to complement its RGB part, and widely follow the bi-stream parallel network architecture. To selectively fuse the CNNs features extracted from both RGB and depth as a final result, the state-of-the-art (SOTA) bi-stream networks usually consist of two independent subbranches; i.e., one subbranch is used for RGB saliency and the other aims for depth saliency. However, its depth saliency is persistently inferior to the RGB saliency because the RGB component is intrinsically more informative than the depth component. The bi-stream architecture easily biases its subsequent fusion procedure to the RGB subbranch, leading to a performance bottleneck. In this paper, we propose a novel data-level recombination strategy to fuse RGB with D (depth) before deep feature extraction, where we cyclically convert the original 4-dimensional RGB-D into \textbf{D}GB, R\textbf{D}B and RG\textbf{D}. Then, a newly lightweight designed triple-stream network is applied over these novel formulated data to achieve an optimal channel-wise complementary fusion status between the RGB and D, achieving a new SOTA performance.



### Full Reference Screen Content Image Quality Assessment by Fusing Multi-level Structure Similarity
- **Arxiv ID**: http://arxiv.org/abs/2008.05396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05396v1)
- **Published**: 2020-08-07 10:20:25+00:00
- **Updated**: 2020-08-07 10:20:25+00:00
- **Authors**: Chenglizhao Chen, Hongmeng Zhao, Huan Yang, Chong Peng, Teng Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The screen content images (SCIs) usually comprise various content types with sharp edges, in which the artifacts or distortions can be well sensed by the vanilla structure similarity measurement in a full reference manner. Nonetheless, almost all of the current SOTA structure similarity metrics are "locally" formulated in a single-level manner, while the true human visual system (HVS) follows the multi-level manner, and such mismatch could eventually prevent these metrics from achieving trustworthy quality assessment. To ameliorate, this paper advocates a novel solution to measure structure similarity "globally" from the perspective of sparse representation. To perform multi-level quality assessment in accordance with the real HVS, the above-mentioned global metric will be integrated with the conventional local ones by resorting to the newly devised selective deep fusion network. To validate its efficacy and effectiveness, we have compared our method with 12 SOTA methods over two widely-used large-scale public SCI datasets, and the quantitative results indicate that our method yields significantly higher consistency with subjective quality score than the currently leading works. Both the source code and data are also publicly available to gain widespread acceptance and facilitate new advancement and its validation.



### Recursive Multi-model Complementary Deep Fusion forRobust Salient Object Detection via Parallel Sub Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04158v1)
- **Published**: 2020-08-07 10:39:11+00:00
- **Updated**: 2020-08-07 10:39:11+00:00
- **Authors**: Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck. In sharp contrast to the conventional ``deeper'' schemes, this paper proposes a ``wider'' network architecture which consists of parallel sub networks with totally different network architectures. In this way, those deep features obtained via these two sub networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.



### Convolutional Ordinal Regression Forest for Image Ordinal Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.03077v2
- **DOI**: 10.1109/TNNLS.2021.3055816
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03077v2)
- **Published**: 2020-08-07 10:41:17+00:00
- **Updated**: 2021-01-27 05:20:02+00:00
- **Authors**: Haiping Zhu, Hongming Shan, Yuheng Zhang, Lingfu Che, Xiaoyang Xu, Junping Zhang, Jianbo Shi, Fei-Yue Wang
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2021
- **Summary**: Image ordinal estimation is to predict the ordinal label of a given image, which can be categorized as an ordinal regression problem. Recent methods formulate an ordinal regression problem as a series of binary classification problems. Such methods cannot ensure that the global ordinal relationship is preserved since the relationships among different binary classifiers are neglected. We propose a novel ordinal regression approach, termed Convolutional Ordinal Regression Forest or CORF, for image ordinal estimation, which can integrate ordinal regression and differentiable decision trees with a convolutional neural network for obtaining precise and stable global ordinal relationships. The advantages of the proposed CORF are twofold. First, instead of learning a series of binary classifiers \emph{independently}, the proposed method aims at learning an ordinal distribution for ordinal regression by optimizing those binary classifiers \emph{simultaneously}. Second, the differentiable decision trees in the proposed CORF can be trained together with the ordinal distribution in an end-to-end manner. The effectiveness of the proposed CORF is verified on two image ordinal estimation tasks, i.e. facial age estimation and image aesthetic assessment, showing significant improvements and better stability over the state-of-the-art ordinal regression methods.



### SimPatch: A Nearest Neighbor Similarity Match between Image Patches
- **Arxiv ID**: http://arxiv.org/abs/2008.03085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2008.03085v1)
- **Published**: 2020-08-07 10:51:10+00:00
- **Updated**: 2020-08-07 10:51:10+00:00
- **Authors**: Aritra Banerjee
- **Comment**: 12 pages, 13 figures, Submitted to International Journal of
  Artificial Intelligence and Soft Computing
- **Journal**: None
- **Summary**: Measuring the similarity between patches in images is a fundamental building block in various tasks. Naturally, the patch-size has a major impact on the matching quality, and on the consequent application performance. We try to use large patches instead of relatively small patches so that each patch contains more information. We use different feature extraction mechanisms to extract the features of each individual image patches which forms a feature matrix and find out the nearest neighbor patches in the image. The nearest patches are calculated using two different nearest neighbor algorithms in this paper for a query patch for a given image and the results have been demonstrated in this paper.



### Knowing Depth Quality In Advance: A Depth Quality Assessment Method For RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04157v1)
- **Published**: 2020-08-07 10:52:52+00:00
- **Updated**: 2020-08-07 10:52:52+00:00
- **Authors**: Xuehao Wang, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Previous RGB-D salient object detection (SOD) methods have widely adopted deep learning tools to automatically strike a trade-off between RGB and D (depth), whose key rationale is to take full advantage of their complementary nature, aiming for a much-improved SOD performance than that of using either of them solely. However, such fully automatic fusions may not always be helpful for the SOD task because the D quality itself usually varies from scene to scene. It may easily lead to a suboptimal fusion result if the D quality is not considered beforehand. Moreover, as an objective factor, the D quality has long been overlooked by previous work. As a result, it is becoming a clear performance bottleneck. Thus, we propose a simple yet effective scheme to measure D quality in advance, the key idea of which is to devise a series of features in accordance with the common attributes of high-quality D regions. To be more concrete, we conduct D quality assessments for each image region, following a multi-scale methodology that includes low-level edge consistency, mid-level regional uncertainty and high-level model variance. All these components will be computed independently and then be assembled with RGB and D features, applied as implicit indicators, to guide the selective fusion. Compared with the state-of-the-art fusion schemes, our method can achieve a more reasonable fusion status between RGB and D. Specifically, the proposed D quality measurement method achieves steady performance improvements for almost 2.0\% in general.



### Cascade Graph Neural Networks for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.03087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03087v1)
- **Published**: 2020-08-07 10:59:04+00:00
- **Updated**: 2020-08-07 10:59:04+00:00
- **Authors**: Ao Luo, Xin Li, Fan Yang, Zhicheng Jiao, Hong Cheng, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of salient object detection (SOD) for RGB-D images using both color and depth information.A major technical challenge in performing salient object detection fromRGB-D images is how to fully leverage the two complementary data sources. Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors.In this work, we introduceCascade Graph Neural Networks(Cas-Gnn),a unified framework which is capable of comprehensively distilling and reasoning the mutual benefits between these two data sources through a set of cascade graphs, to learn powerful representations for RGB-D salient object detection. Cas-Gnn processes the two data sources individually and employs a novelCascade Graph Reasoning(CGR) module to learn powerful dense feature embeddings, from which the saliency map can be easily inferred. Contrast to the previous approaches, the explicitly modeling and reasoning of high-level relations between complementary data sources allows us to better overcome challenges such as occlusions and ambiguities. Extensive experiments demonstrate that Cas-Gnn achieves significantly better performance than all existing RGB-DSOD approaches on several widely-used benchmarks.



### Associative Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.03111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03111v1)
- **Published**: 2020-08-07 12:15:38+00:00
- **Updated**: 2020-08-07 12:15:38+00:00
- **Authors**: Youngeun Kim, Sungeun Hong, Seunghan Yang, Sungil Kang, Yunho Jeon, Jiwon Kim
- **Comment**: 8 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Partial Adaptation (PDA) addresses a practical scenario in which the target domain contains only a subset of classes in the source domain. While PDA should take into account both class-level and sample-level to mitigate negative transfer, current approaches mostly rely on only one of them. In this paper, we propose a novel approach to fully exploit multi-level associations that can arise in PDA. Our Associative Partial Domain Adaptation (APDA) utilizes intra-domain association to actively select out non-trivial anomaly samples in each source-private class that sample-level weighting cannot handle. Additionally, our method considers inter-domain association to encourage positive transfer by mapping between nearby target samples and source samples with high label-commonness. For this, we exploit feature propagation in a proposed label space consisting of source ground-truth labels and target probabilistic labels. We further propose a geometric guidance loss based on the label commonness of each source class to encourage positive transfer. Our APDA consistently achieves state-of-the-art performance across public datasets.



### Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.03128v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03128v4)
- **Published**: 2020-08-07 12:45:39+00:00
- **Updated**: 2021-11-01 03:18:25+00:00
- **Authors**: Yixiong Zou, Shanghang Zhang, JianPeng Yu, Yonghong Tian, José M. F. Moura
- **Comment**: None
- **Journal**: None
- **Summary**: Existing few-shot learning (FSL) methods usually assume base classes and novel classes are from the same domain (in-domain setting). However, in practice, it may be infeasible to collect sufficient training samples for some special domains to construct base classes. To solve this problem, cross-domain FSL (CDFSL) is proposed very recently to transfer knowledge from general-domain base classes to special-domain novel classes. Existing CDFSL works mostly focus on transferring between near domains, while rarely consider transferring between distant domains, which is in practical need as any novel classes could appear in real-world applications, and is even more challenging. In this paper, we study a challenging subset of CDFSL where the novel classes are in distant domains from base classes, by revisiting the mid-level features, which are more transferable yet under-explored in main stream FSL work. To boost the discriminability of mid-level features, we propose a residual-prediction task to encourage mid-level features to learn discriminative information of each sample. Notably, such mechanism also benefits the in-domain FSL and CDFSL in near domains. Therefore, we provide two types of features for both cross- and in-domain FSL respectively, under the same training framework. Experiments under both settings on six public datasets, including two challenging medical datasets, validate the our rationale and demonstrate state-of-the-art performance. Code will be released.



### Image Transformation Network for Privacy-Preserving Deep Neural Networks and Its Security Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2008.03143v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03143v1)
- **Published**: 2020-08-07 12:58:45+00:00
- **Updated**: 2020-08-07 12:58:45+00:00
- **Authors**: Hiroki Ito, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: To appear in 2020 IEEE 9th Global Conference on Consumer Electronics
  (GCCE 2020)
- **Journal**: None
- **Summary**: We propose a transformation network for generating visually-protected images for privacy-preserving DNNs. The proposed transformation network is trained by using a plain image dataset so that plain images are transformed into visually protected ones. Conventional perceptual encryption methods have a weak visual-protection performance and some accuracy degradation in image classification. In contrast, the proposed network enables us not only to strongly protect visual information but also to maintain the image classification accuracy that using plain images achieves. In an image classification experiment, the proposed network is demonstrated to strongly protect visual information on plain images without any performance degradation under the use of CIFAR datasets. In addition, it is shown that the visually protected images are robust against a DNN-based attack, called inverse transformation network attack (ITN-Attack) in an experiment.



### A Study on Visual Perception of Light Field Content
- **Arxiv ID**: http://arxiv.org/abs/2008.03195v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2008.03195v1)
- **Published**: 2020-08-07 14:23:27+00:00
- **Updated**: 2020-08-07 14:23:27+00:00
- **Authors**: Ailbhe Gill, Emin Zerman, Cagri Ozcinar, Aljosa Smolic
- **Comment**: To appear in Irish Machine Vision and Image Processing (IMVIP) 2020
- **Journal**: None
- **Summary**: The effective design of visual computing systems depends heavily on the anticipation of visual attention, or saliency. While visual attention is well investigated for conventional 2D images and video, it is nevertheless a very active research area for emerging immersive media. In particular, visual attention of light fields (light rays of a scene captured by a grid of cameras or micro lenses) has only recently become a focus of research. As they may be rendered and consumed in various ways, a primary challenge that arises is the definition of what visual perception of light field content should be. In this work, we present a visual attention study on light field content. We conducted perception experiments displaying them to users in various ways and collected corresponding visual attention data. Our analysis highlights characteristics of user behaviour in light field imaging applications. The light field data set and attention data are provided with this paper.



### Convolutional neural network based deep-learning architecture for intraprostatic tumour contouring on PSMA PET images in patients with primary prostate cancer
- **Arxiv ID**: http://arxiv.org/abs/2008.03201v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2008.03201v1)
- **Published**: 2020-08-07 14:32:14+00:00
- **Updated**: 2020-08-07 14:32:14+00:00
- **Authors**: Dejan Kostyszyn, Tobias Fechter, Nico Bartl, Anca L. Grosu, Christian Gratzke, August Sigle, Michael Mix, Juri Ruf, Thomas F. Fassbender, Selina Kiefer, Alisa S. Bettermann, Nils H. Nicolay, Simon Spohn, Maria U. Kramer, Peter Bronsert, Hongqian Guo, Xuefeng Qiu, Feng Wang, Christoph Henkenberens, Rudolf A. Werner, Dimos Baltas, Philipp T. Meyer, Thorsten Derlin, Mengxia Chen, Constantinos Zamboglou
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate delineation of the intraprostatic gross tumour volume (GTV) is a prerequisite for treatment approaches in patients with primary prostate cancer (PCa). Prostate-specific membrane antigen positron emission tomography (PSMA-PET) may outperform MRI in GTV detection. However, visual GTV delineation underlies interobserver heterogeneity and is time consuming. The aim of this study was to develop a convolutional neural network (CNN) for automated segmentation of intraprostatic tumour (GTV-CNN) in PSMA-PET.   Methods: The CNN (3D U-Net) was trained on [68Ga]PSMA-PET images of 152 patients from two different institutions and the training labels were generated manually using a validated technique. The CNN was tested on two independent internal (cohort 1: [68Ga]PSMA-PET, n=18 and cohort 2: [18F]PSMA-PET, n=19) and one external (cohort 3: [68Ga]PSMA-PET, n=20) test-datasets. Accordance between manual contours and GTV-CNN was assessed with Dice-S{\o}rensen coefficient (DSC). Sensitivity and specificity were calculated for the two internal test-datasets by using whole-mount histology.   Results: Median DSCs for cohorts 1-3 were 0.84 (range: 0.32-0.95), 0.81 (range: 0.28-0.93) and 0.83 (range: 0.32-0.93), respectively. Sensitivities and specificities for GTV-CNN were comparable with manual expert contours: 0.98 and 0.76 (cohort 1) and 1 and 0.57 (cohort 2), respectively. Computation time was around 6 seconds for a standard dataset.   Conclusion: The application of a CNN for automated contouring of intraprostatic GTV in [68Ga]PSMA- and [18F]PSMA-PET images resulted in a high concordance with expert contours and in high sensitivities and specificities in comparison with histology reference. This robust, accurate and fast technique may be implemented for treatment concepts in primary PCa. The trained model and the study's source code are available in an open source repository.



### In-Depth DCT Coefficient Distribution Analysis for First Quantization Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.03206v1
- **DOI**: 10.1007/978-3-030-68780-9_45
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03206v1)
- **Published**: 2020-08-07 14:46:10+00:00
- **Updated**: 2020-08-07 14:46:10+00:00
- **Authors**: Sebastiano Battiato, Oliver Giudice, Francesco Guarnera, Giovanni Puglisi
- **Comment**: None
- **Journal**: None
- **Summary**: The exploitation of traces in JPEG double compressed images is of utter importance for investigations. Properly exploiting such insights, First Quantization Estimation (FQE) could be performed in order to obtain source camera model identification (CMI) and therefore reconstruct the history of a digital image. In this paper, a method able to estimate the first quantization factors for JPEG double compressed images is presented, employing a mixed statistical and Machine Learning approach. The presented solution is demonstrated to work without any a-priori assumptions about the quantization matrices. Experimental results and comparisons with the state-of-the-art show the goodness of the proposed technique.



### Visual Attack and Defense on Text
- **Arxiv ID**: http://arxiv.org/abs/2008.10356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.10356v1)
- **Published**: 2020-08-07 15:44:58+00:00
- **Updated**: 2020-08-07 15:44:58+00:00
- **Authors**: Shengjun Liu, Ningkang Jiang, Yuanbin Wu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Modifying characters of a piece of text to their visual similar ones often ap-pear in spam in order to fool inspection systems and other conditions, which we regard as a kind of adversarial attack to neural models. We pro-pose a way of generating such visual text attack and show that the attacked text are readable by humans but mislead a neural classifier greatly. We ap-ply a vision-based model and adversarial training to defense the attack without losing the ability to understand normal text. Our results also show that visual attack is extremely sophisticated and diverse, more work needs to be done to solve this.



### Investigation of Speaker-adaptation methods in Transformer based ASR
- **Arxiv ID**: http://arxiv.org/abs/2008.03247v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2008.03247v2)
- **Published**: 2020-08-07 16:09:03+00:00
- **Updated**: 2021-11-17 21:11:46+00:00
- **Authors**: Vishwas M. Shetty, Metilda Sagaya Mary N J, S. Umesh
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: End-to-end models are fast replacing the conventional hybrid models in automatic speech recognition. Transformer, a sequence-to-sequence model, based on self-attention popularly used in machine translation tasks, has given promising results when used for automatic speech recognition. This paper explores different ways of incorporating speaker information at the encoder input while training a transformer-based model to improve its speech recognition performance. We present speaker information in the form of speaker embeddings for each of the speakers. We experiment using two types of speaker embeddings: x-vectors and novel s-vectors proposed in our previous work. We report results on two datasets a) NPTEL lecture database and b) Librispeech 500-hour split. NPTEL is an open-source e-learning portal providing lectures from top Indian universities. We obtain improvements in the word error rate over the baseline through our approach of integrating speaker embeddings into the model.



### Multi-Level Temporal Pyramid Network for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.03270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03270v1)
- **Published**: 2020-08-07 17:08:24+00:00
- **Updated**: 2020-08-07 17:08:24+00:00
- **Authors**: Xiang Wang, Changxin Gao, Shiwei Zhang, Nong Sang
- **Comment**: Accepted by PRCV2020
- **Journal**: None
- **Summary**: Currently, one-stage frameworks have been widely applied for temporal action detection, but they still suffer from the challenge that the action instances span a wide range of time. The reason is that these one-stage detectors, e.g., Single Shot Multi-Box Detector (SSD), extract temporal features only applying a single-level layer for each head, which is not discriminative enough to perform classification and regression. In this paper, we propose a Multi-Level Temporal Pyramid Network (MLTPN) to improve the discrimination of the features. Specially, we first fuse the features from multiple layers with different temporal resolutions, to encode multi-layer temporal information. We then apply a multi-level feature pyramid architecture on the features to enhance their discriminative abilities. Finally, we design a simple yet effective feature fusion module to fuse the multi-level multi-scale features. By this means, the proposed MLTPN can learn rich and discriminative features for different action instances with different durations. We evaluate MLTPN on two challenging datasets: THUMOS'14 and Activitynet v1.3, and the experimental results show that MLTPN obtains competitive performance on Activitynet v1.3 and outperforms the state-of-the-art approaches on THUMOS'14 significantly.



### Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.03285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.03285v1)
- **Published**: 2020-08-07 17:34:28+00:00
- **Updated**: 2020-08-07 17:34:28+00:00
- **Authors**: Guillermo Garcia-Hernando, Edward Johns, Tae-Kyun Kim
- **Comment**: To appear in IROS2020
- **Journal**: None
- **Summary**: Dexterous manipulation of objects in virtual environments with our bare hands, by using only a depth sensor and a state-of-the-art 3D hand pose estimator (HPE), is challenging. While virtual environments are ruled by physics, e.g. object weights and surface frictions, the absence of force feedback makes the task challenging, as even slight inaccuracies on finger tips or contact points from HPE may make the interactions fail. Prior arts simply generate contact forces in the direction of the fingers' closures, when finger joints penetrate virtual objects. Although useful for simple grasping scenarios, they cannot be applied to dexterous manipulations such as in-hand manipulation. Existing reinforcement learning (RL) and imitation learning (IL) approaches train agents that learn skills by using task-specific rewards, without considering any online user input. In this work, we propose to learn a model that maps noisy input hand poses to target virtual poses, which introduces the needed contacts to accomplish the tasks on a physics simulator. The agent is trained in a residual setting by using a model-free hybrid RL+IL approach. A 3D hand pose estimation reward is introduced leading to an improvement on HPE accuracy when the physics-guided corrected target poses are remapped to the input space. As the model corrects HPE errors by applying minor but crucial joint displacements for contacts, this helps to keep the generated motion visually close to the user input. Since HPE sequences performing successful virtual interactions do not exist, a data generation scheme to train and evaluate the system is proposed. We test our framework in two applications that use hand pose estimates for dexterous manipulations: hand-object interactions in VR and hand-object motion reconstruction in-the-wild.



### HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures
- **Arxiv ID**: http://arxiv.org/abs/2008.03286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03286v2)
- **Published**: 2020-08-07 17:34:47+00:00
- **Updated**: 2021-03-25 05:06:19+00:00
- **Authors**: Yichao Zhou, Jingwei Huang, Xili Dai, Shichen Liu, Linjie Luo, Zhili Chen, Yi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: We present HoliCity, a city-scale 3D dataset with rich structural information. Currently, this dataset has 6,300 real-world panoramas of resolution $13312 \times 6656$ that are accurately aligned with the CAD model of downtown London with an area of more than 20 km$^2$, in which the median reprojection error of the alignment of an average image is less than half a degree. This dataset aims to be an all-in-one data platform for research of learning abstracted high-level holistic 3D structures that can be derived from city CAD models, e.g., corners, lines, wireframes, planes, and cuboids, with the ultimate goal of supporting real-world applications including city-scale reconstruction, localization, mapping, and augmented reality. The accurate alignment of the 3D CAD models and panoramas also benefits low-level 3D vision tasks such as surface normal estimation, as the surface normal extracted from previous LiDAR-based datasets is often noisy. We conduct experiments to demonstrate the applications of HoliCity, such as predicting surface segmentation, normal maps, depth maps, and vanishing points, as well as test the generalizability of methods trained on HoliCity and other related datasets. HoliCity is available at https://holicity.io.



### Generative Adversarial Network for Radar Signal Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.03346v1
- **DOI**: 10.1109/IJCNN.2019.8851887
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03346v1)
- **Published**: 2020-08-07 19:31:06+00:00
- **Updated**: 2020-08-07 19:31:06+00:00
- **Authors**: Thomas Truong, Svetlana Yanushkevich
- **Comment**: None
- **Journal**: None
- **Summary**: A major obstacle in radar based methods for concealed object detection on humans and seamless integration into security and access control system is the difficulty in collecting high quality radar signal data. Generative adversarial networks (GAN) have shown promise in data generation application in the fields of image and audio processing. As such, this paper proposes the design of a GAN for application in radar signal generation. Data collected using the Finite-Difference Time-Domain (FDTD) method on three concealed object classes (no object, large object, and small object) were used as training data to train a GAN to generate radar signal samples for each class. The proposed GAN generated radar signal data which was indistinguishable from the training data by qualitative human observers.



### Reliable Liver Fibrosis Assessment from Ultrasound using Global Hetero-Image Fusion and View-Specific Parameterization
- **Arxiv ID**: http://arxiv.org/abs/2008.03352v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03352v1)
- **Published**: 2020-08-07 19:50:15+00:00
- **Updated**: 2020-08-07 19:50:15+00:00
- **Authors**: Bowen Li, Ke Yan, Dar-In Tai, Yuankai Huo, Le Lu, Jing Xiao, Adam P. Harrison
- **Comment**: 10 pages, MICCAI 2020
- **Journal**: None
- **Summary**: Ultrasound (US) is a critical modality for diagnosing liver fibrosis. Unfortunately, assessment is very subjective, motivating automated approaches. We introduce a principled deep convolutional neural network (CNN) workflow that incorporates several innovations. First, to avoid overfitting on non-relevant image features, we force the network to focus on a clinical region of interest (ROI), encompassing the liver parenchyma and upper border. Second, we introduce global heteroimage fusion (GHIF), which allows the CNN to fuse features from any arbitrary number of images in a study, increasing its versatility and flexibility. Finally, we use 'style'-based view-specific parameterization (VSP) to tailor the CNN processing for different viewpoints of the liver, while keeping the majority of parameters the same across views. Experiments on a dataset of 610 patient studies (6979 images) demonstrate that our pipeline can contribute roughly 7% and 22% improvements in partial area under the curve and recall at 90% precision, respectively, over conventional classifiers, validating our approach to this crucial problem.



### Hybrid Score- and Rank-level Fusion for Person Identification using Face and ECG Data
- **Arxiv ID**: http://arxiv.org/abs/2008.03353v1
- **DOI**: 10.1109/EST.2019.8806206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03353v1)
- **Published**: 2020-08-07 19:54:59+00:00
- **Updated**: 2020-08-07 19:54:59+00:00
- **Authors**: Thomas Truong, Jonathan Graf, Svetlana Yanushkevich
- **Comment**: None
- **Journal**: None
- **Summary**: Uni-modal identification systems are vulnerable to errors in sensor data collection and are therefore more likely to misidentify subjects. For instance, relying on data solely from an RGB face camera can cause problems in poorly lit environments or if subjects do not face the camera. Other identification methods such as electrocardiograms (ECG) have issues with improper lead connections to the skin. Errors in identification are minimized through the fusion of information gathered from both of these models. This paper proposes a methodology for combining the identification results of face and ECG data using Part A of the BioVid Heat Pain Database containing synchronized RGB-video and ECG data on 87 subjects. Using 10-fold cross-validation, face identification was 98.8% accurate, while the ECG identification was 96.1% accurate. By using a fusion approach the identification accuracy improved to 99.8%. Our proposed methodology allows for identification accuracies to be significantly improved by using disparate face and ECG models that have non-overlapping modalities.



### X-Ray bone abnormalities detection using MURA dataset
- **Arxiv ID**: http://arxiv.org/abs/2008.03356v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 97-XX, J.3
- **Links**: [PDF](http://arxiv.org/pdf/2008.03356v1)
- **Published**: 2020-08-07 19:58:56+00:00
- **Updated**: 2020-08-07 19:58:56+00:00
- **Authors**: A. Solovyova, I. Solovyov
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: We introduce the deep network trained on the MURA dataset from the Stanford University released in 2017. Our system is able to detect bone abnormalities on the radiographs and visualise such zones. We found that our solution has the accuracy comparable to the best results that have been achieved by other development teams that used MURA dataset, in particular the overall Kappa score that was achieved by our team is about 0.942 on the wrist, 0.862 on the hand and o.735 on the shoulder (compared to the best available results to this moment on the official web-site 0.931, 0.851 and 0.729 accordingly). However, despite the good results there are a lot of directions for the future enhancement of the proposed technology. We see a big potential in the further development computer aided systems (CAD) for the radiographs as the one that will help practical specialists diagnose bone fractures as well as bone oncology cases faster and with the higher accuracy.



### Improving the Speed and Quality of GAN by Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2008.03364v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.03364v1)
- **Published**: 2020-08-07 20:21:31+00:00
- **Updated**: 2020-08-07 20:21:31+00:00
- **Authors**: Jiachen Zhong, Xuanqing Liu, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GAN) have shown remarkable results in image generation tasks. High fidelity class-conditional GAN methods often rely on stabilization techniques by constraining the global Lipschitz continuity. Such regularization leads to less expressive models and slower convergence speed; other techniques, such as the large batch training, require unconventional computing power and are not widely accessible. In this paper, we develop an efficient algorithm, namely FastGAN (Free AdverSarial Training), to improve the speed and quality of GAN training based on the adversarial training technique. We benchmark our method on CIFAR10, a subset of ImageNet, and the full ImageNet datasets. We choose strong baselines such as SNGAN and SAGAN; the results demonstrate that our training algorithm can achieve better generation quality (in terms of the Inception score and Frechet Inception distance) with less overall training time. Most notably, our training algorithm brings ImageNet training to the broader public by requiring 2-4 GPUs.



### A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context
- **Arxiv ID**: http://arxiv.org/abs/2008.07360v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2008.07360v1)
- **Published**: 2020-08-07 20:22:23+00:00
- **Updated**: 2020-08-07 20:22:23+00:00
- **Authors**: Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, Allan Halpern, Harald Kittler, Kivanc Kose, Steve Langer, Konstantinos Lioprys, Josep Malvehy, Shenara Musthaq, Jabpani Nanda, Ofer Reiter, George Shih, Alexander Stratigos, Philipp Tschandl, Jochen Weber, H. Peter Soyer
- **Comment**: Figures: 3, Tables: 2, Pages: 12
- **Journal**: None
- **Summary**: Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 histopathologically confirmed melanomas compared with benign melanoma mimickers.



