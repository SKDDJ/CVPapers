# Arxiv Papers in cs.CV on 2020-08-11
### Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2008.04469v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04469v2)
- **Published**: 2020-08-11 01:21:29+00:00
- **Updated**: 2020-09-11 13:50:29+00:00
- **Authors**: Jeffrey Byrne, Brian DeCann, Scott Bloom
- **Comment**: BMVC'20 (Best Paper - Runner up)
- **Journal**: None
- **Summary**: Modern cameras are not designed with computer vision or machine learning as the target application. There is a need for a new class of vision sensors that are privacy preserving by design, that do not leak private information and collect only the information necessary for a target machine learning task. In this paper, we introduce key-nets, which are convolutional networks paired with a custom vision sensor which applies an optical/analog transform such that the key-net can perform exact encrypted inference on this transformed image, but the image is not interpretable by a human or any other key-net. We provide five sufficient conditions for an optical transformation suitable for a key-net, and show that generalized stochastic matrices (e.g. scale, bias and fractional pixel shuffling) satisfy these conditions. We motivate the key-net by showing that without it there is a utility/privacy tradeoff for a network fine-tuned directly on optically transformed images for face identification and object detection. Finally, we show that a key-net is equivalent to homomorphic encryption using a Hill cipher, with an upper bound on memory and runtime that scales quadratically with a user specified privacy parameter. Therefore, the key-net is the first practical, efficient and privacy preserving vision sensor based on optical homomorphic encryption.



### ARPM-net: A novel CNN-based adversarial method with Markov Random Field enhancement for prostate and organs at risk segmentation in pelvic CT images
- **Arxiv ID**: http://arxiv.org/abs/2008.04488v4
- **DOI**: 10.1002/mp.14580
- **Categories**: **eess.IV**, cs.CV, 68T07(Primary), 68T45(Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2008.04488v4)
- **Published**: 2020-08-11 02:40:53+00:00
- **Updated**: 2020-09-17 21:28:26+00:00
- **Authors**: Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou Sun
- **Comment**: 21 pages, 8 figures; accepted as a journal article at Medical
  Physics; abstract presented at AAPM 2020
- **Journal**: None
- **Summary**: Purpose: The research is to develop a novel CNN-based adversarial deep learning method to improve and expedite the multi-organ semantic segmentation of CT images, and to generate accurate contours on pelvic CT images. Methods: Planning CT and structure datasets for 120 patients with intact prostate cancer were retrospectively selected and divided for 10-fold cross-validation. The proposed adversarial multi-residual multi-scale pooling Markov Random Field (MRF) enhanced network (ARPM-net) implements an adversarial training scheme. A segmentation network and a discriminator network were trained jointly, and only the segmentation network was used for prediction. The segmentation network integrates a newly designed MRF block into a variation of multi-residual U-net. The discriminator takes the product of the original CT and the prediction/ground-truth as input and classifies the input into fake/real. The segmentation network and discriminator network can be trained jointly as a whole, or the discriminator can be used for fine-tuning after the segmentation network is coarsely trained. Multi-scale pooling layers were introduced to preserve spatial resolution during pooling using less memory compared to atrous convolution layers. An adaptive loss function was proposed to enhance the training on small or low contrast organs. The accuracy of modeled contours was measured with the Dice similarity coefficient (DSC), Average Hausdorff Distance (AHD), Average Surface Hausdorff Distance (ASHD), and relative Volume Difference (VD) using clinical contours as references to the ground-truth. The proposed ARPM-net method was compared to several stateof-the-art deep learning methods.



### Keypoint Autoencoders: Learning Interest Points of Semantics
- **Arxiv ID**: http://arxiv.org/abs/2008.04502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04502v1)
- **Published**: 2020-08-11 03:43:18+00:00
- **Updated**: 2020-08-11 03:43:18+00:00
- **Authors**: Ruoxi Shi, Zhengrong Xue, Xinyang Li
- **Comment**: 4 Pages. Conference: IMVIP 2020
- **Journal**: None
- **Summary**: Understanding point clouds is of great importance. Many previous methods focus on detecting salient keypoints to identity structures of point clouds. However, existing methods neglect the semantics of points selected, leading to poor performance on downstream tasks. In this paper, we propose Keypoint Autoencoder, an unsupervised learning method for detecting keypoints. We encourage selecting sparse semantic keypoints by enforcing the reconstruction from keypoints to the original point cloud. To make sparse keypoint selection differentiable, Soft Keypoint Proposal is adopted by calculating weighted averages among input points. A downstream task of classifying shape with sparse keypoints is conducted to demonstrate the distinctiveness of our selected keypoints. Semantic Accuracy and Semantic Richness are proposed and our method gives competitive or even better performance than state of the arts on these two metrics.



### Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling
- **Arxiv ID**: http://arxiv.org/abs/2008.04504v1
- **DOI**: 10.1145/3394171.3413886
- **Categories**: **cs.CL**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04504v1)
- **Published**: 2020-08-11 03:55:11+00:00
- **Updated**: 2020-08-11 03:55:11+00:00
- **Authors**: Jiacheng Li, Siliang Tang, Juncheng Li, Jun Xiao, Fei Wu, Shiliang Pu, Yueting Zhuang
- **Comment**: ACM Multimedia 2020
- **Journal**: None
- **Summary**: Visual Storytelling~(VIST) is a task to tell a narrative story about a certain topic according to the given photo stream. The existing studies focus on designing complex models, which rely on a huge amount of human-annotated data. However, the annotation of VIST is extremely costly and many topics cannot be covered in the training dataset due to the long-tail topic distribution. In this paper, we focus on enhancing the generalization ability of the VIST model by considering the few-shot setting. Inspired by the way humans tell a story, we propose a topic adaptive storyteller to model the ability of inter-topic generalization. In practice, we apply the gradient-based meta-learning algorithm on multi-modal seq2seq models to endow the model the ability to adapt quickly from topic to topic. Besides, We further propose a prototype encoding structure to model the ability of intra-topic derivation. Specifically, we encode and restore the few training story text to serve as a reference to guide the generation at inference time. Experimental results show that topic adaptation and prototype encoding structure mutually bring benefit to the few-shot model on BLEU and METEOR metric. The further case study shows that the stories generated after few-shot adaptation are more relative and expressive.



### TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers
- **Arxiv ID**: http://arxiv.org/abs/2008.04509v3
- **DOI**: 10.1109/DAC18074.2021.9586266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04509v3)
- **Published**: 2020-08-11 04:20:27+00:00
- **Updated**: 2021-11-15 06:55:31+00:00
- **Authors**: Nguyen-Dong Ho, Ik-Joon Chang
- **Comment**: To appear in the 58th Design Automation Conference (DAC 2021)
- **Journal**: None
- **Summary**: Spiking-neural-networks (SNNs) are promising at edge devices since the event-driven operations of SNNs provides significantly lower power compared to analog-neural-networks (ANNs). Although it is difficult to efficiently train SNNs, many techniques to convert trained ANNs to SNNs have been developed. However, after the conversion, a trade-off relation between accuracy and latency exists in SNNs, causing considerable latency in large size datasets such as ImageNet. We present a technique, named as TCL, to alleviate the trade-off problem, enabling the accuracy of 73.87% (VGG-16) and 70.37% (ResNet-34) for ImageNet with the moderate latency of 250 cycles in SNNs.



### PneumoXttention: A CNN compensating for Human Fallibility when Detecting Pneumonia through CXR images with Attention
- **Arxiv ID**: http://arxiv.org/abs/2008.04907v1
- **DOI**: 10.1109/ISPA52656.2021.9552171
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04907v1)
- **Published**: 2020-08-11 05:11:16+00:00
- **Updated**: 2020-08-11 05:11:16+00:00
- **Authors**: Sanskriti Singh
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Automatic Chest Radiograph X-ray (CXR) interpretation by machines is an important research topic of Artificial Intelligence. As part of my journey through the California Science Fair, I have developed an algorithm that can detect pneumonia from a CXR image to compensate for human fallibility. My algorithm, PneumoXttention, is an ensemble of two 13 layer convolutional neural network trained on the RSNA dataset, a dataset provided by the Radiological Society of North America, containing 26,684 frontal X-ray images split into the categories of pneumonia and no pneumonia. The dataset was annotated by many professional radiologists in North America. It achieved an impressive F1 score, 0.82, on the test set (20% random split of RSNA dataset) and completely compensated Human Radiologists on a random set of 25 test images drawn from RSNA and NIH. I don't have a direct comparison but Stanford's Chexnet has a F1 score of 0.435 on the NIH dataset for category Pneumonia.



### SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2008.04526v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04526v2)
- **Published**: 2020-08-11 05:47:00+00:00
- **Updated**: 2021-03-26 16:08:45+00:00
- **Authors**: Srijay Deshpande, Fayyaz Minhas, Simon Graham, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic images can be used for the development and evaluation of deep learning algorithms in the context of limited availability of data. In the field of computational pathology, where histology images are large in size and visual context is crucial, synthesis of large high resolution images via generative modeling is a challenging task. This is due to memory and computational constraints hindering the generation of large images. To address this challenge, we propose a novel SAFRON (Stitching Across the FRONtiers) framework to construct realistic, large high resolution tissue image tiles from ground truth annotations while preserving morphological features and with minimal boundary artifacts. We show that the proposed method can generate realistic image tiles of arbitrarily large size after training it on relatively small image patches. We demonstrate that our model can generate high quality images, both visually and in terms of the Frechet Inception Distance. Compared to other existing approaches, our framework is efficient in terms of the memory requirements for training and also in terms of the number of computations to construct a large high-resolution image. We also show that training on synthetic data generated by SAFRON can significantly boost the performance of a state-of-the-art algorithm for gland segmentation in colorectal cancer histology images. Sample high resolution images generated using SAFRON are available at the URL: https://warwick.ac.uk/TIALab/SAFRON



### Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness and Sparsity-Regularized Tensor Optimization
- **Arxiv ID**: http://arxiv.org/abs/2008.04529v2
- **DOI**: 10.3390/rs12203446
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04529v2)
- **Published**: 2020-08-11 05:59:20+00:00
- **Updated**: 2020-09-01 04:28:23+00:00
- **Authors**: Chenxi Duan, Jun Pan, Rui Li
- **Comment**: None
- **Journal**: None
- **Summary**: In remote sensing images, the presence of thick cloud accompanying cloud shadow is a high probability event, which can affect the quality of subsequent processing and limit the scenarios of application. Hence, removing the thick cloud and cloud shadow as well as recovering the cloud-contaminated pixels is indispensable to make good use of remote sensing images. In this paper, a novel thick cloud removal method for remote sensing images based on temporal smoothness and sparsity-regularized tensor optimization (TSSTO) is proposed. The basic idea of TSSTO is that the thick cloud and cloud shadow are not only sparse but also smooth along the horizontal and vertical direction in images while the clean images are smooth along the temporal direction between images. Therefore, the sparsity norm is used to boost the sparsity of the cloud and cloud shadow, and unidirectional total variation (UTV) regularizers are applied to ensure the unidirectional smoothness. This paper utilizes alternation direction method of multipliers to solve the presented model and generate the cloud and cloud shadow element as well as the clean element. The cloud and cloud shadow element is purified to get the cloud area and cloud shadow area. Then, the clean area of the original cloud-contaminated images is replaced to the corresponding area of the clean element. Finally, the reference image is selected to reconstruct details of the cloud area and cloud shadow area using the information cloning method. A series of experiments are conducted both on simulated and real cloud-contaminated images from different sensors and with different resolutions, and the results demonstrate the potential of the proposed TSSTO method for removing cloud and cloud shadow from both qualitative and quantitative viewpoints.



### Text as Neural Operator: Image Manipulation by Text Instruction
- **Arxiv ID**: http://arxiv.org/abs/2008.04556v4
- **DOI**: 10.1145/3474085.3475343
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04556v4)
- **Published**: 2020-08-11 07:07:10+00:00
- **Updated**: 2021-11-29 16:48:56+00:00
- **Authors**: Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, text-guided image manipulation has gained increasing attention in the multimedia and computer vision community. The input to conditional image generation has evolved from image-only to multimodality. In this paper, we study a setting that allows users to edit an image with multiple objects using complex text instructions to add, remove, or change the objects. The inputs of the task are multimodal including (1) a reference image and (2) an instruction in natural language that describes desired modifications to the image. We propose a GAN-based method to tackle this problem. The key idea is to treat text as neural operators to locally modify the image feature. We show that the proposed model performs favorably against recent strong baselines on three public datasets. Specifically, it generates images of greater fidelity and semantic relevance, and when used as a image query, leads to better retrieval performance.



### Extension of JPEG XS for Two-Layer Lossless Coding
- **Arxiv ID**: http://arxiv.org/abs/2008.04558v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04558v1)
- **Published**: 2020-08-11 07:14:17+00:00
- **Updated**: 2020-08-11 07:14:17+00:00
- **Authors**: Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: to appear in 2020 IEEE 9th Global Conference on Consumer Electronics
- **Journal**: None
- **Summary**: A two-layer lossless image coding method compatible with JPEG XS is proposed. JPEG XS is a new international standard for still image coding that has the characteristics of very low latency and very low complexity. However, it does not support lossless coding, although it can achieve visual lossless coding. The proposed method has a two-layer structure similar to JPEG XT, which consists of JPEG XS coding and a lossless coding method. As a result, it enables us to losslessly restore original images, while maintaining compatibility with JPEG XS.



### PiNet: Attention Pooling for Graph Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.04575v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.04575v1)
- **Published**: 2020-08-11 08:17:14+00:00
- **Updated**: 2020-08-11 08:17:14+00:00
- **Authors**: Peter Meltzer, Marcelo Daniel Gutierrez Mallea, Peter J. Bentley
- **Comment**: 4 pages, 3 figures 1 table
- **Journal**: Neural Information Processing Systems (NIPS): Graph Representation
  Learning Workshop 2019
- **Summary**: We propose PiNet, a generalised differentiable attention-based pooling mechanism for utilising graph convolution operations for graph level classification. We demonstrate high sample efficiency and superior performance over other graph neural networks in distinguishing isomorphic graph classes, as well as competitive results with state of the art methods on standard chemo-informatics datasets.



### Rethinking Pseudo-LiDAR Representation
- **Arxiv ID**: http://arxiv.org/abs/2008.04582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04582v1)
- **Published**: 2020-08-11 08:44:18+00:00
- **Updated**: 2020-08-11 08:44:18+00:00
- **Authors**: Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, Wanli Ouyang
- **Comment**: ECCV2020. Supplemental Material attached
- **Journal**: None
- **Summary**: The recently proposed pseudo-LiDAR based 3D detectors greatly improve the benchmark of monocular/stereo 3D detection task. However, the underlying mechanism remains obscure to the research community. In this paper, we perform an in-depth investigation and observe that the efficacy of pseudo-LiDAR representation comes from the coordinate transformation, instead of data representation itself. Based on this observation, we design an image based CNN detector named Patch-Net, which is more generalized and can be instantiated as pseudo-LiDAR based 3D detectors. Moreover, the pseudo-LiDAR data in our PatchNet is organized as the image representation, which means existing 2D CNN designs can be easily utilized for extracting deep features from input data and boosting 3D detection performance. We conduct extensive experiments on the challenging KITTI dataset, where the proposed PatchNet outperforms all existing pseudo-LiDAR based counterparts. Code has been made available at: https://github.com/xinzhuma/patchnet.



### Sharp Multiple Instance Learning for DeepFake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04585v1
- **DOI**: 10.1145/3394171.3414034
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.04585v1)
- **Published**: 2020-08-11 08:52:17+00:00
- **Updated**: 2020-08-11 08:52:17+00:00
- **Authors**: Xiaodan Li, Yining Lang, Yuefeng Chen, Xiaofeng Mao, Yuan He, Shuhui Wang, Hui Xue, Quan Lu
- **Comment**: Accepted at ACM MM 2020. 11 pages, 8 figures, with appendix
- **Journal**: Proceedings of the 28th ACM International Conference on
  Multimedia, 2020
- **Summary**: With the rapid development of facial manipulation techniques, face forgery has received considerable attention in multimedia and computer vision community due to security concerns. Existing methods are mostly designed for single-frame detection trained with precise image-level labels or for video-level prediction by only modeling the inter-frame inconsistency, leaving potential high risks for DeepFake attackers. In this paper, we introduce a new problem of partial face attack in DeepFake video, where only video-level labels are provided but not all the faces in the fake videos are manipulated. We address this problem by multiple instance learning framework, treating faces and input video as instances and bag respectively. A sharp MIL (S-MIL) is proposed which builds direct mapping from instance embeddings to bag prediction, rather than from instance embeddings to instance prediction and then to bag prediction in traditional MIL. Theoretical analysis proves that the gradient vanishing in traditional MIL is relieved in S-MIL. To generate instances that can accurately incorporate the partially manipulated faces, spatial-temporal encoded instance is designed to fully model the intra-frame and inter-frame inconsistency, which further helps to promote the detection performance. We also construct a new dataset FFPMS for partially attacked DeepFake video detection, which can benefit the evaluation of different methods at both frame and video levels. Experiments on FFPMS and the widely used DFDC dataset verify that S-MIL is superior to other counterparts for partially attacked DeepFake video detection. In addition, S-MIL can also be adapted to traditional DeepFake image detection tasks and achieve state-of-the-art performance on single-frame datasets.



### Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms
- **Arxiv ID**: http://arxiv.org/abs/2008.04590v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2008.04590v1)
- **Published**: 2020-08-11 09:02:47+00:00
- **Updated**: 2020-08-11 09:02:47+00:00
- **Authors**: Steffen Illium, Robert Müller, Andreas Sedlmeier, Claudia Linnhoff-Popien
- **Comment**: 5 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: In many fields of research, labeled datasets are hard to acquire. This is where data augmentation promises to overcome the lack of training data in the context of neural network engineering and classification tasks. The idea here is to reduce model over-fitting to the feature distribution of a small under-descriptive training dataset. We try to evaluate such data augmentation techniques to gather insights in the performance boost they provide for several convolutional neural networks on mel-spectrogram representations of audio data. We show the impact of data augmentation on the binary classification task of surgical mask detection in samples of human voice (ComParE Challenge 2020). Also we consider four varying architectures to account for augmentation robustness. Results show that most of the baselines given by ComParE are outperformed.



### Multi-modal segmentation of 3D brain scans using neural networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04594v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04594v1)
- **Published**: 2020-08-11 09:13:54+00:00
- **Updated**: 2020-08-11 09:13:54+00:00
- **Authors**: Jonathan Zopes, Moritz Platscher, Silvio Paganucci, Christian Federau
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To implement a brain segmentation pipeline based on convolutional neural networks, which rapidly segments 3D volumes into 27 anatomical structures. To provide an extensive, comparative study of segmentation performance on various contrasts of magnetic resonance imaging (MRI) and computed tomography (CT) scans. Methods: Deep convolutional neural networks are trained to segment 3D MRI (MPRAGE, DWI, FLAIR) and CT scans. A large database of in total 851 MRI/CT scans is used for neural network training. Training labels are obtained on the MPRAGE contrast and coregistered to the other imaging modalities. The segmentation quality is quantified using the Dice metric for a total of 27 anatomical structures. Dropout sampling is implemented to identify corrupted input scans or low-quality segmentations. Full segmentation of 3D volumes with more than 2 million voxels is obtained in less than 1s of processing time on a graphical processing unit. Results: The best average Dice score is found on $T_1$-weighted MPRAGE ($85.3\pm4.6\,\%$). However, for FLAIR ($80.0\pm7.1\,\%$), DWI ($78.2\pm7.9\,\%$) and CT ($79.1\pm 7.9\,\%$), good-quality segmentation is feasible for most anatomical structures. Corrupted input volumes or low-quality segmentations can be detected using dropout sampling. Conclusion: The flexibility and performance of deep convolutional neural networks enables the direct, real-time segmentation of FLAIR, DWI and CT scans without requiring $T_1$-weighted scans.



### Left Ventricular Wall Motion Estimation by Active Polynomials for Acute Myocardial Infarction Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04615v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04615v1)
- **Published**: 2020-08-11 10:29:22+00:00
- **Updated**: 2020-08-11 10:29:22+00:00
- **Authors**: Serkan Kiranyaz, Aysen Degerli, Tahir Hamid, Rashid Mazhar, Rayyan Ahmed, Rayaan Abouhasera, Morteza Zabihi, Junaid Malik, Ridha Hamila, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Echocardiogram (echo) is the earliest and the primary tool for identifying regional wall motion abnormalities (RWMA) in order to diagnose myocardial infarction (MI) or commonly known as heart attack. This paper proposes a novel approach, Active Polynomials, which can accurately and robustly estimate the global motion of the Left Ventricular (LV) wall from any echo in a robust and accurate way. The proposed algorithm quantifies the true wall motion occurring in LV wall segments so as to assist cardiologists diagnose early signs of an acute MI. It further enables medical experts to gain an enhanced visualization capability of echo images through color-coded segments along with their "maximum motion displacement" plots helping them to better assess wall motion and LV Ejection-Fraction (LVEF). The outputs of the method can further help echo-technicians to assess and improve the quality of the echocardiogram recording. A major contribution of this study is the first public echo database collection composed by physicians at the Hamad Medical Corporation Hospital in Qatar. The so-called HMC-QU database will serve as the benchmark for the forthcoming relevant studies. The results over the HMC-QU dataset show that the proposed approach can achieve high accuracy, sensitivity and precision in MI detection even though the echo quality is quite poor, and the temporal resolution is low.



### Deep UAV Localization with Reference View Rendering
- **Arxiv ID**: http://arxiv.org/abs/2008.04619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.04619v1)
- **Published**: 2020-08-11 10:54:36+00:00
- **Updated**: 2020-08-11 10:54:36+00:00
- **Authors**: Timo Hinzmann, Roland Siegwart
- **Comment**: Initial submission; 15 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: This paper presents a framework for the localization of Unmanned Aerial Vehicles (UAVs) in unstructured environments with the help of deep learning. A real-time rendering engine is introduced that generates optical and depth images given a six Degrees-of-Freedom (DoF) camera pose, camera model, geo-referenced orthoimage, and elevation map. The rendering engine is embedded into a learning-based six-DoF Inverse Compositional Lucas-Kanade (ICLK) algorithm that is able to robustly align the rendered and real-world image taken by the UAV. To learn the alignment under environmental changes, the architecture is trained using maps spanning multiple years at high resolution. The evaluation shows that the deep 6DoF-ICLK algorithm outperforms its non-trainable counterparts by a large margin. To further support the research in this field, the real-time rendering engine and accompanying datasets are released along with this publication.



### Fully-Automated Packaging Structure Recognition in Logistics Environments
- **Arxiv ID**: http://arxiv.org/abs/2008.04620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04620v1)
- **Published**: 2020-08-11 10:57:23+00:00
- **Updated**: 2020-08-11 10:57:23+00:00
- **Authors**: Laura Dörr, Felix Brandt, Martin Pouls, Alexander Naumann
- **Comment**: Accepted for IEEE International Conference on Emerging Technologies
  and Factory Automation ETFA 2020
- **Journal**: None
- **Summary**: Within a logistics supply chain, a large variety of transported goods need to be handled, recognized and checked at many different network points. Often, huge manual effort is involved in recognizing or verifying packet identity or packaging structure, for instance to check the delivery for completeness. We propose a method for complete automation of packaging structure recognition: Based on a single image, one or multiple transport units are localized and, for each of these transport units, the characteristics, the total number and the arrangement of its packaging units is recognized. Our algorithm is based on deep learning models, more precisely convolutional neural networks for instance segmentation in images, as well as computer vision methods and heuristic components. We use a custom data set of realistic logistics images for training and evaluation of our method. We show that the solution is capable of correctly recognizing the packaging structure in approximately 85% of our test cases, and even more (91%) when focusing on most common package types.



### R-MNet: A Perceptual Adversarial Network for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2008.04621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04621v3)
- **Published**: 2020-08-11 10:58:10+00:00
- **Updated**: 2020-11-09 17:08:51+00:00
- **Authors**: Jireh Jam, Connah Kendrick, Vincent Drouard, Kevin Walker, Gee-Sern Hsu, Moi Hoon Yap
- **Comment**: 10 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: Facial image inpainting is a problem that is widely studied, and in recent years the introduction of Generative Adversarial Networks, has led to improvements in the field. Unfortunately some issues persists, in particular when blending the missing pixels with the visible ones. We address the problem by proposing a Wasserstein GAN combined with a new reverse mask operator, namely Reverse Masking Network (R-MNet), a perceptual adversarial network for image inpainting. The reverse mask operator transfers the reverse masked image to the end of the encoder-decoder network leaving only valid pixels to be inpainted. Additionally, we propose a new loss function computed in feature space to target only valid pixels combined with adversarial training. These then capture data distributions and generate images similar to those in the training data with achieved realism (realistic and coherent) on the output images. We evaluate our method on publicly available dataset, and compare with state-of-the-art methods. We show that our method is able to generalize to high-resolution inpainting task, and further show more realistic outputs that are plausible to the human visual system when compared with the state-of-the-art methods.



### Real-Time Sign Language Detection using Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.04637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2008.04637v2)
- **Published**: 2020-08-11 11:42:03+00:00
- **Updated**: 2020-09-13 11:40:20+00:00
- **Authors**: Amit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, Srini Narayanan
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.



### Learning to Cluster under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2008.04646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04646v1)
- **Published**: 2020-08-11 12:03:01+00:00
- **Updated**: 2020-08-11 12:03:01+00:00
- **Authors**: Willi Menapace, Stéphane Lathuilière, Elisa Ricci
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: While unsupervised domain adaptation methods based on deep architectures have achieved remarkable success in many computer vision tasks, they rely on a strong assumption, i.e. labeled source data must be available. In this work we overcome this assumption and we address the problem of transferring knowledge from a source to a target domain when both source and target data have no annotations. Inspired by recent works on deep clustering, our approach leverages information from data gathered from multiple source domains to build a domain-agnostic clustering model which is then refined at inference time when target data become available. Specifically, at training time we propose to optimize a novel information-theoretic loss which, coupled with domain-alignment layers, ensures that our model learns to correctly discover semantic labels while discarding domain-specific features. Importantly, our architecture design ensures that at inference time the resulting source model can be effectively adapted to the target domain without having access to source data, thanks to feature alignment and self-supervision. We evaluate the proposed approach in a variety of settings, considering several domain adaptation benchmarks and we show that our method is able to automatically discover relevant semantic information even in presence of few target samples and yields state-of-the-art results on multiple domain adaptation benchmarks.



### Fast and Accurate Optical Flow based Depth Map Estimation from Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2008.04673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04673v1)
- **Published**: 2020-08-11 12:53:31+00:00
- **Updated**: 2020-08-11 12:53:31+00:00
- **Authors**: Yang Chen, Martin Alain, Aljosa Smolic
- **Comment**: Accepted at IMVIP 2017
- **Journal**: None
- **Summary**: Depth map estimation is a crucial task in computer vision, and new approaches have recently emerged taking advantage of light fields, as this new imaging modality captures much more information about the angular direction of light rays compared to common approaches based on stereoscopic images or multi-view. In this paper, we propose a novel depth estimation method from light fields based on existing optical flow estimation methods. The optical flow estimator is applied on a sequence of images taken along an angular dimension of the light field, which produces several disparity map estimates. Considering both accuracy and efficiency, we choose the feature flow method as our optical flow estimator. Thanks to its spatio-temporal edge-aware filtering properties, the different disparity map estimates that we obtain are very consistent, which allows a fast and simple aggregation step to create a single disparity map, which can then converted into a depth map. Since the disparity map estimates are consistent, we can also create a depth map from each disparity estimate, and then aggregate the different depth maps in the 3D space to create a single dense depth map.



### ClimAlign: Unsupervised statistical downscaling of climate variables via normalizing flows
- **Arxiv ID**: http://arxiv.org/abs/2008.04679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.04679v1)
- **Published**: 2020-08-11 13:01:53+00:00
- **Updated**: 2020-08-11 13:01:53+00:00
- **Authors**: Brian Groenke, Luke Madaus, Claire Monteleoni
- **Comment**: 8 pages, submitted as journal paper to the 10th International
  Conference on Climate Informatics (2020)
- **Journal**: None
- **Summary**: Downscaling is a landmark task in climate science and meteorology in which the goal is to use coarse scale, spatio-temporal data to infer values at finer scales. Statistical downscaling aims to approximate this task using statistical patterns gleaned from an existing dataset of downscaled values, often obtained from observations or physical models. In this work, we investigate the application of deep latent variable learning to the task of statistical downscaling. We present ClimAlign, a novel method for unsupervised, generative downscaling using adaptations of recent work in normalizing flows for variational inference. We evaluate the viability of our method using several different metrics on two datasets consisting of daily temperature and precipitation values gridded at low (1 degree latitude/longitude) and high (1/4 and 1/8 degree) resolutions. We show that our method achieves comparable predictive performance to existing supervised statistical downscaling methods while simultaneously allowing for both conditional and unconditional sampling from the joint distribution over high and low resolution spatial fields. We provide publicly accessible implementations of our method, as well as the baselines used for comparison, on GitHub.



### Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in CT Exams
- **Arxiv ID**: http://arxiv.org/abs/2008.04690v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04690v1)
- **Published**: 2020-08-11 13:23:04+00:00
- **Updated**: 2020-08-11 13:23:04+00:00
- **Authors**: Dario Augusto Borges Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: The success of supervised lesion segmentation algorithms using Computed Tomography (CT) exams depends significantly on the quantity and variability of samples available for training. While annotating such data constitutes a challenge itself, the variability of lesions in the dataset also depends on the prevalence of different types of lesions. This phenomenon adds an inherent bias to lesion segmentation algorithms that can be diminished, among different possibilities, using aggressive data augmentation methods. In this paper, we present a method for implanting realistic lesions in CT slices to provide a rich and controllable set of training samples and ultimately improving semantic segmentation network performances for delineating lesions in CT exams. Our results show that implanting synthetic lesions not only improves (up to around 12\%) the segmentation performance considering different architectures but also that this improvement is consistent among different image synthesis networks. We conclude that increasing the variability of lesions synthetically in terms of size, density, shape, and position seems to improve the performance of segmentation models for liver lesion segmentation in CT slices.



### Transferring Inter-Class Correlation
- **Arxiv ID**: http://arxiv.org/abs/2008.10444v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.10444v1)
- **Published**: 2020-08-11 13:24:44+00:00
- **Updated**: 2020-08-11 13:24:44+00:00
- **Authors**: Hui Wen, Yue Wu, Chenming Yang, Jingjing Li, Yue Zhu, Xu Jiang, Hancong Duan
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: The Teacher-Student (T-S) framework is widely utilized in the classification tasks, through which the performance of one neural network (the student) can be improved by transferring knowledge from another trained neural network (the teacher). Since the transferring knowledge is related to the network capacities and structures between the teacher and the student, how to define efficient knowledge remains an open question. To address this issue, we design a novel transferring knowledge, the Self-Attention based Inter-Class Correlation (ICC) map in the output layer, and propose our T-S framework, Inter-Class Correlation Transfer (ICCT).



### PROFIT: A Novel Training Method for sub-4-bit MobileNet Models
- **Arxiv ID**: http://arxiv.org/abs/2008.04693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2008.04693v1)
- **Published**: 2020-08-11 13:29:50+00:00
- **Updated**: 2020-08-11 13:29:50+00:00
- **Authors**: Eunhyeok Park, Sungjoo Yoo
- **Comment**: Published at ECCV2020, spotlight paper
- **Journal**: None
- **Summary**: 4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86 % of top-1 accuracy.



### A Study of Efficient Light Field Subsampling and Reconstruction Strategies
- **Arxiv ID**: http://arxiv.org/abs/2008.04694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04694v1)
- **Published**: 2020-08-11 13:32:11+00:00
- **Updated**: 2020-08-11 13:32:11+00:00
- **Authors**: Yang Chen, Martin Alain, Aljosa Smolic
- **Comment**: Accepted at IMVIP 2020
- **Journal**: None
- **Summary**: Limited angular resolution is one of the main obstacles for practical applications of light fields. Although numerous approaches have been proposed to enhance angular resolution, view selection strategies have not been well explored in this area. In this paper, we study subsampling and reconstruction strategies for light fields. First, different subsampling strategies are studied with a fixed sampling ratio, such as row-wise sampling, column-wise sampling, or their combinations. Second, several strategies are explored to reconstruct intermediate views from four regularly sampled input views. The influence of the angular density of the input is also evaluated. We evaluate these strategies on both real-world and synthetic datasets, and optimal selection strategies are devised from our results. These can be applied in future light field research such as compression, angular super-resolution, and design of camera systems.



### Robust Long-Term Object Tracking via Improved Discriminative Model Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.04722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04722v2)
- **Published**: 2020-08-11 14:31:11+00:00
- **Updated**: 2020-08-25 15:37:50+00:00
- **Authors**: Seokeon Choi, Junhyun Lee, Yunsung Lee, Alexander Hauptmann
- **Comment**: Accepted to ECCV 2020 Workshop
- **Journal**: None
- **Summary**: We propose an improved discriminative model prediction method for robust long-term tracking based on a pre-trained short-term tracker. The baseline pre-trained short-term tracker is SuperDiMP which combines the bounding-box regressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP improves SuperDiMP in the following three aspects: (1) Uncertainty reduction using random erasing: To make our model robust, we exploit an agreement from multiple images after erasing random small rectangular areas as a certainty. And then, we correct the tracking state of our model accordingly. (2) Random search with spatio-temporal constraints: we propose a robust random search method with a score penalty applied to prevent the problem of sudden detection at a distance. (3) Background augmentation for more discriminative feature learning: We augment various backgrounds that are not included in the search area to train a more robust model in the background clutter. In experiments on the VOT-LT2020 benchmark dataset, the proposed method achieves comparable performance to the state-of-the-art long-term trackers. The source code is available at: https://github.com/bismex/RLT-DIMP.



### The Umbrella software suite for automated asteroid detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04724v2
- **DOI**: 10.1016/j.ascom.2021.100453
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04724v2)
- **Published**: 2020-08-11 14:37:58+00:00
- **Updated**: 2021-02-20 10:48:56+00:00
- **Authors**: Malin Stanescu, Ovidiu Vaduvescu
- **Comment**: Manuscript as accepted to Astronomy and Computing 15 Feb 2020
- **Journal**: None
- **Summary**: We present the Umbrella software suite for asteroid detection, validation, identification and reporting. The current core of Umbrella is an open-source modular library, called Umbrella2, that includes algorithms and interfaces for all steps of the processing pipeline, including a novel detection algorithm for faint trails. Building on the library, we have also implemented a detection pipeline accessible both as a desktop program (ViaNearby) and via a web server (Webrella), which we have successfully used in near real-time data reduction of a few asteroid surveys on the Wide Field Camera of the Isaac Newton Telescope. In this paper we describe the library, focusing on the interfaces and algorithms available, and we present the results obtained with the desktop version on a set of well-curated fields used by the EURONEAR project as an asteroid detection benchmark.



### AtrialJSQnet: A New Framework for Joint Segmentation and Quantification of Left Atrium and Scars Incorporating Spatial and Shape Information
- **Arxiv ID**: http://arxiv.org/abs/2008.04729v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04729v2)
- **Published**: 2020-08-11 14:44:19+00:00
- **Updated**: 2021-11-12 10:56:21+00:00
- **Authors**: Lei Li, Veronika A. Zimmer, Julia A. Schnabel, Xiahai Zhuang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Left atrial (LA) and atrial scar segmentation from late gadolinium enhanced magnetic resonance imaging (LGE MRI) is an important task in clinical practice. %, to guide ablation therapy and predict treatment results for atrial fibrillation (AF) patients. The automatic segmentation is however still challenging, due to the poor image quality, the various LA shapes, the thin wall, and the surrounding enhanced regions. Previous methods normally solved the two tasks independently and ignored the intrinsic spatial relationship between LA and scars. In this work, we develop a new framework, namely AtrialJSQnet, where LA segmentation, scar projection onto the LA surface, and scar quantification are performed simultaneously in an end-to-end style. We propose a mechanism of shape attention (SA) via an explicit surface projection, to utilize the inherent correlation between LA and LA scars. In specific, the SA scheme is embedded into a multi-task architecture to perform joint LA segmentation and scar quantification. Besides, a spatial encoding (SE) loss is introduced to incorporate continuous spatial information of the target, in order to reduce noisy patches in the predicted segmentation. We evaluated the proposed framework on 60 LGE MRIs from the MICCAI2018 LA challenge. Extensive experiments on a public dataset demonstrated the effect of the proposed AtrialJSQnet, which achieved competitive performance over the state-of-the-art. The relatedness between LA segmentation and scar quantification was explicitly explored and has shown significant performance improvements for both tasks. The code and results will be released publicly once the manuscript is accepted for publication via https://zmiclab.github.io/projects.html.



### Attention-based 3D Object Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2008.04738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04738v1)
- **Published**: 2020-08-11 14:51:18+00:00
- **Updated**: 2020-08-11 14:51:18+00:00
- **Authors**: Andrey Salvi, Nathan Gavenski, Eduardo Pooch, Felipe Tasoniero, Rodrigo Barros
- **Comment**: 8 pages, 4 figures, 3 tables
- **Journal**: International Joint Conference on Neural Networks (IJCNN) - 2020
- **Summary**: Recently, learning-based approaches for 3D reconstruction from 2D images have gained popularity due to its modern applications, e.g., 3D printers, autonomous robots, self-driving cars, virtual reality, and augmented reality. The computer vision community has applied a great effort in developing functions to reconstruct the full 3D geometry of objects and scenes. However, to extract image features, they rely on convolutional neural networks, which are ineffective in capturing long-range dependencies. In this paper, we propose to substantially improve Occupancy Networks, a state-of-the-art method for 3D object reconstruction. For such we apply the concept of self-attention within the network's encoder in order to leverage complementary input features rather than those based on local regions, helping the encoder to extract global information. With our approach, we were capable of improving the original work in 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10X the Chamfer-L1 distance. We also perform a qualitative study that shows that our approach was able to generate much more consistent meshes, confirming its increased generalization power over the current state-of-the-art.



### Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.04751v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.04751v1)
- **Published**: 2020-08-11 15:00:41+00:00
- **Updated**: 2020-08-11 15:00:41+00:00
- **Authors**: Xiaofeng Liu, Yimeng Zhang, Xiongchang Liu, Song Bai, Site Li, Jane You
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS)
- **Journal**: None
- **Summary**: Semantic segmentation is important for many real-world systems, e.g., autonomous vehicles, which predict the class of each pixel. Recently, deep networks achieved significant progress w.r.t. the mean Intersection-over Union (mIoU) with the cross-entropy loss. However, the cross-entropy loss can essentially ignore the difference of severity for an autonomous car with different wrong prediction mistakes. For example, predicting the car to the road is much more servery than recognize it as the bus. Targeting for this difficulty, we develop a Wasserstein training framework to explore the inter-class correlation by defining its ground metric as misclassification severity. The ground metric of Wasserstein distance can be pre-defined following the experience on a specific task. From the optimization perspective, we further propose to set the ground metric as an increasing function of the pre-defined ground metric. Furthermore, an adaptively learning scheme of the ground matrix is proposed to utilize the high-fidelity CARLA simulator. Specifically, we follow a reinforcement alternative learning scheme. The experiments on both CamVid and Cityscapes datasets evidenced the effectiveness of our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be adapted following a plug-in manner. We achieve significant improvements on the predefined important classes, and much longer continuous playtime in our simulator.



### HydraMix-Net: A Deep Multi-task Semi-supervised Learning Approach for Cell Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.04753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04753v1)
- **Published**: 2020-08-11 15:00:59+00:00
- **Updated**: 2020-08-11 15:00:59+00:00
- **Authors**: R. M. Saad Bashir, Talha Qaiser, Shan E Ahmed Raza, Nasir M. Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised techniques have removed the barriers of large scale labelled set by exploiting unlabelled data to improve the performance of a model. In this paper, we propose a semi-supervised deep multi-task classification and localization approach HydraMix-Net in the field of medical imagining where labelling is time consuming and costly. Firstly, the pseudo labels are generated using the model's prediction on the augmented set of unlabelled image with averaging. The high entropy predictions are further sharpened to reduced the entropy and are then mixed with the labelled set for training. The model is trained in multi-task learning manner with noise tolerant joint loss for classification localization and achieves better performance when given limited data in contrast to a simple deep model. On DLBCL data it achieves 80\% accuracy in contrast to simple CNN achieving 70\% accuracy when given only 100 labelled examples.



### Transfer Learning for Protein Structure Classification at Low Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.04757v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.BM, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.04757v4)
- **Published**: 2020-08-11 15:01:32+00:00
- **Updated**: 2020-08-31 17:02:33+00:00
- **Authors**: Alexander Hudson, Shaogang Gong
- **Comment**: 9 pages excluding references and appendices
- **Journal**: None
- **Summary**: Structure determination is key to understanding protein function at a molecular level. Whilst significant advances have been made in predicting structure and function from amino acid sequence, researchers must still rely on expensive, time-consuming analytical methods to visualise detailed protein conformation. In this study, we demonstrate that it is possible to make accurate ($\geq$80%) predictions of protein class and architecture from structures determined at low ($>$3A) resolution, using a deep convolutional neural network trained on high-resolution ($\leq$3A) structures represented as 2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein structure classification at low resolution, and a basis for extension to prediction of function. We investigate the impact of the input representation on classification performance, showing that side-chain information may not be necessary for fine-grained structure predictions. Finally, we confirm that high-resolution, low-resolution and NMR-determined structures inhabit a common feature space, and thus provide a theoretical foundation for boosting with single-image super-resolution.



### DTVNet+: A High-Resolution Scenic Dataset for Dynamic Time-lapse Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.04776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04776v2)
- **Published**: 2020-08-11 15:26:10+00:00
- **Updated**: 2021-12-17 15:39:34+00:00
- **Authors**: Jiangning Zhang, Chao Xu, Yong Liu, Yunliang Jiang
- **Comment**: This work extends the previous DTVNet in ECCV'20, and we further
  present a high-quality and high-resolution Quick-Sky-Time dataset and carry
  out more adequate experiments and analysis
- **Journal**: None
- **Summary**: This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: \emph{Optical Flow Encoder} (OFE) and \emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow maps to a \emph{normalized motion vector} that encodes the motion information of the generated video. The DVG contains motion and content streams to learn from the motion vector and the single landscape image. Besides, it contains an encoder to learn shared content features and a decoder to construct video frames with corresponding motion. Specifically, the \emph{motion stream} introduces multiple \emph{adaptive instance normalization} (AdaIN) layers to integrate multi-level motion information for controlling the object motion. In the testing stage, videos with the same content but various motion information can be generated by different \emph{normalized motion vectors} based on only one input image. Also, we propose a high-resolution scenic time-lapse video dataset, named Quick-Sky-Time, to evaluate different approaches, which can be viewed as a new benchmark for high-quality scenic image and video generation tasks. We further conduct experiments on Sky Time-lapse, Beach, and Quick-Sky-Time datasets. The results demonstrate the superiority of our approach over state-of-the-art methods for generating high-quality and various dynamic videos.



### Learning Stereo Matchability in Disparity Regression Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04800v1)
- **Published**: 2020-08-11 15:55:49+00:00
- **Updated**: 2020-08-11 15:55:49+00:00
- **Authors**: Jingyang Zhang, Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Learning-based stereo matching has recently achieved promising results, yet still suffers difficulties in establishing reliable matches in weakly matchable regions that are textureless, non-Lambertian, or occluded. In this paper, we address this challenge by proposing a stereo matching network that considers pixel-wise matchability. Specifically, the network jointly regresses disparity and matchability maps from 3D probability volume through expectation and entropy operations. Next, a learned attenuation is applied as the robust loss function to alleviate the influence of weakly matchable pixels in the training. Finally, a matchability-aware disparity refinement is introduced to improve the depth inference in weakly matchable regions. The proposed deep stereo matchability (DSM) framework can improve the matching result or accelerate the computation while still guaranteeing the quality. Moreover, the DSM framework is portable to many recent stereo networks. Extensive experiments are conducted on Scene Flow and KITTI stereo datasets to demonstrate the effectiveness of the proposed framework over the state-of-the-art learning-based stereo methods.



### 3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.04808v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04808v1)
- **Published**: 2020-08-11 16:03:51+00:00
- **Updated**: 2020-08-11 16:03:51+00:00
- **Authors**: Jonathan Alush-Aben, Linor Ackerman-Schraier, Tomer Weiss, Sanketh Vedula, Ortal Senouf, Alex Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) has long been considered to be among the gold standards of today's diagnostic imaging. The most significant drawback of MRI is long acquisition times, prohibiting its use in standard practice for some applications. Compressed sensing (CS) proposes to subsample the k-space (the Fourier domain dual to the physical space of spatial coordinates) leading to significantly accelerated acquisition. However, the benefit of compressed sensing has not been fully exploited; most of the sampling densities obtained through CS do not produce a trajectory that obeys the stringent constraints of the MRI machine imposed in practice. Inspired by recent success of deep learning based approaches for image reconstruction and ideas from computational imaging on learning-based design of imaging systems, we introduce 3D FLAT, a novel protocol for data-driven design of 3D non-Cartesian accelerated trajectories in MRI. Our proposal leverages the entire 3D k-space to simultaneously learn a physically feasible acquisition trajectory with a reconstruction method. Experimental results, performed as a proof-of-concept, suggest that 3D FLAT achieves higher image quality for a given readout time compared to standard trajectories such as radial, stack-of-stars, or 2D learned trajectories (trajectories that evolve only in the 2D plane while fully sampling along the third dimension). Furthermore, we demonstrate evidence supporting the significant benefit of performing MRI acquisitions using non-Cartesian 3D trajectories over 2D non-Cartesian trajectories acquired slice-wise.



### Unified Representation Learning for Cross Model Compatibility
- **Arxiv ID**: http://arxiv.org/abs/2008.04821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04821v1)
- **Published**: 2020-08-11 16:14:53+00:00
- **Updated**: 2020-08-11 16:14:53+00:00
- **Authors**: Chien-Yi Wang, Ya-Liang Chang, Shang-Ta Yang, Dong Chen, Shang-Hong Lai
- **Comment**: To appear in British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification.



### Detecting Urban Dynamics Using Deep Siamese Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04829v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04829v1)
- **Published**: 2020-08-11 16:20:11+00:00
- **Updated**: 2020-08-11 16:20:11+00:00
- **Authors**: Ephrem Admasu Yekun, Petros Reda Samsom
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is a fast-growing discipline in the areas of computer vision and remote sensing. In this work, we designed and developed a variant of convolutional neural network (CNN), known as Siamese CNN to extract features from pairs of Sentinel-2 temporal images of Mekelle city captured at different times and detect changes due to urbanization: buildings and roads. The detection capability of the proposed was measured in terms of overall accuracy (95.8), Kappa measure (72.5), recall (76.5), precision (77.7), F1 measure (77.1). The model has achieved a good performance in terms of most of these measures and can be used to detect changes in Mekelle and other cities at different time horizons undergoing urbanization.



### TransNet V2: An effective deep network architecture for fast shot transition detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04838v1)
- **Published**: 2020-08-11 16:37:59+00:00
- **Updated**: 2020-08-11 16:37:59+00:00
- **Authors**: Tomáš Souček, Jakub Lokoč
- **Comment**: None
- **Journal**: None
- **Summary**: Although automatic shot transition detection approaches are already investigated for more than two decades, an effective universal human-level model was not proposed yet. Even for common shot transitions like hard cuts or simple gradual changes, the potential diversity of analyzed video contents may still lead to both false hits and false dismissals. Recently, deep learning-based approaches significantly improved the accuracy of shot transition detection using 3D convolutional architectures and artificially created training data. Nevertheless, one hundred percent accuracy is still an unreachable ideal. In this paper, we share the current version of our deep network TransNet V2 that reaches state-of-the-art performance on respected benchmarks. A trained instance of the model is provided so it can be instantly utilized by the community for a highly efficient analysis of large video archives. Furthermore, the network architecture, as well as our experience with the training process, are detailed, including simple code snippets for convenient usage of the proposed model and visualization of results.



### Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04848v1)
- **Published**: 2020-08-11 16:47:02+00:00
- **Updated**: 2020-08-11 16:47:02+00:00
- **Authors**: Gengxing Wang, Jiahuan Zhou, Ying Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based video synthesis approaches, in particular with applications that can forge identities such as "DeepFake", have raised great security concerns. Therefore, corresponding deep forensic methods are proposed to tackle this problem. However, existing methods are either based on unexplainable deep networks which greatly degrades the principal interpretability factor to media forensic, or rely on fragile image statistics such as noise pattern, which in real-world scenarios can be easily deteriorated by data compression. In this paper, we propose an fully-interpretable video forensic method that is designed specifically to expose deep-faked videos. To enhance generalizability on videos with various content, we model the temporal motion of multiple specific spatial locations in the videos to extract a robust and reliable representation, called Co-Motion Pattern. Such kind of conjoint pattern is mined across local motion features which is independent of the video contents so that the instance-wise variation can also be largely alleviated. More importantly, our proposed co-motion pattern possesses both superior interpretability and sufficient robustness against data compression for deep-faked videos. We conduct extensive experiments to empirically demonstrate the superiority and effectiveness of our approach under both classification and anomaly detection evaluation settings against the state-of-the-art deep forensic methods.



### TextRay: Contour-based Geometric Modeling for Arbitrary-shaped Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.04851v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04851v2)
- **Published**: 2020-08-11 16:52:10+00:00
- **Updated**: 2020-08-12 07:29:25+00:00
- **Authors**: Fangfang Wang, Yifeng Chen, Fei Wu, Xi Li
- **Comment**: Accepted to ACM MM 2020
- **Journal**: None
- **Summary**: Arbitrary-shaped text detection is a challenging task due to the complex geometric layouts of texts such as large aspect ratios, various scales, random rotations and curve shapes. Most state-of-the-art methods solve this problem from bottom-up perspectives, seeking to model a text instance of complex geometric layouts with simple local units (e.g., local boxes or pixels) and generate detections with heuristic post-processings. In this work, we propose an arbitrary-shaped text detection method, namely TextRay, which conducts top-down contour-based geometric modeling and geometric parameter learning within a single-shot anchor-free framework. The geometric modeling is carried out under polar system with a bidirectional mapping scheme between shape space and parameter space, encoding complex geometric layouts into unified representations. For effective learning of the representations, we design a central-weighted training strategy and a content loss which builds propagation paths between geometric encodings and visual content. TextRay outputs simple polygon detections at one pass with only one NMS post-processing. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach. The code is available at https://github.com/LianaWang/TextRay.



### GeLaTO: Generative Latent Textured Objects
- **Arxiv ID**: http://arxiv.org/abs/2008.04852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04852v1)
- **Published**: 2020-08-11 16:55:26+00:00
- **Updated**: 2020-08-11 16:55:26+00:00
- **Authors**: Ricardo Martin-Brualla, Rohit Pandey, Sofien Bouaziz, Matthew Brown, Dan B Goldman
- **Comment**: ECCV 2020 Spotlight. Project website: https://gelato-paper.github.io
- **Journal**: European Conference on Computer Vision 2020
- **Summary**: Accurate modeling of 3D objects exhibiting transparency, reflections and thin structures is an extremely challenging problem. Inspired by billboards and geometric proxies used in computer graphics, this paper proposes Generative Latent Textured Objects (GeLaTO), a compact representation that combines a set of coarse shape proxies defining low frequency geometry with learned neural textures, to encode both medium and fine scale geometry as well as view-dependent appearance. To generate the proxies' textures, we learn a joint latent space allowing category-level appearance and geometry interpolation. The proxies are independently rasterized with their corresponding neural texture and composited using a U-Net, which generates an output photorealistic image including an alpha map. We demonstrate the effectiveness of our approach by reconstructing complex objects from a sparse set of views. We show results on a dataset of real images of eyeglasses frames, which are particularly challenging to reconstruct using classical methods. We also demonstrate that these coarse proxies can be handcrafted when the underlying object geometry is easy to model, like eyeglasses, or generated using a neural network for more complex categories, such as cars.



### KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2008.04858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04858v2)
- **Published**: 2020-08-11 17:03:06+00:00
- **Updated**: 2020-08-28 07:34:49+00:00
- **Authors**: Xiaoze Jiang, Siyi Du, Zengchang Qin, Yajing Sun, Jing Yu
- **Comment**: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020), Oral
- **Journal**: None
- **Summary**: Visual dialogue is a challenging task that needs to extract implicit information from both visual (image) and textual (dialogue history) contexts. Classical approaches pay more attention to the integration of the current question, vision knowledge and text knowledge, despising the heterogeneous semantic gaps between the cross-modal information. In the meantime, the concatenation operation has become de-facto standard to the cross-modal information fusion, which has a limited ability in information retrieval. In this paper, we propose a novel Knowledge-Bridge Graph Network (KBGN) model by using graph to bridge the cross-modal semantic relations between vision and text knowledge in fine granularity, as well as retrieving required knowledge via an adaptive information selection mode. Moreover, the reasoning clues for visual dialogue can be clearly drawn from intra-modal entities and inter-modal bridges. Experimental results on VisDial v1.0 and VisDial-Q datasets demonstrate that our model outperforms existing models with state-of-the-art results.



### BREEDS: Benchmarks for Subpopulation Shift
- **Arxiv ID**: http://arxiv.org/abs/2008.04859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.04859v1)
- **Published**: 2020-08-11 17:04:47+00:00
- **Updated**: 2020-08-11 17:04:47+00:00
- **Authors**: Shibani Santurkar, Dimitris Tsipras, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines for them. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of off-the-shelf train-time robustness interventions. Code and data available at https://github.com/MadryLab/BREEDS-Benchmarks .



### Hardware-Centric AutoML for Mixed-Precision Quantization
- **Arxiv ID**: http://arxiv.org/abs/2008.04878v1
- **DOI**: 10.1007/s11263-020-01339-6
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04878v1)
- **Published**: 2020-08-11 17:30:22+00:00
- **Updated**: 2020-08-11 17:30:22+00:00
- **Authors**: Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han
- **Comment**: Journal preprint of arXiv:1811.08886 (IJCV, 2020). The first three
  authors contributed equally to this work. Project page:
  https://hanlab.mit.edu/projects/haq/
- **Journal**: International Journal of Computer Vision (IJCV), Volume 128, Issue
  8-9, pp 2035-2048, 2020
- **Summary**: Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy, and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.



### Adversarial Generative Grammars for Human Activity Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.04888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04888v2)
- **Published**: 2020-08-11 17:47:53+00:00
- **Updated**: 2020-08-14 15:16:46+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo
- **Comment**: ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: In this paper we propose an adversarial generative grammar model for future prediction. The objective is to learn a model that explicitly captures temporal dependencies, providing a capability to forecast multiple, distinct future activities. Our adversarial grammar is designed so that it can learn stochastic production rules from the data distribution, jointly with its latent non-terminal representations. Being able to select multiple production rules during inference leads to different predicted outcomes, thus efficiently modeling many plausible futures. The adversarial generative grammar is evaluated on the Charades, MultiTHUMOS, Human3.6M, and 50 Salads datasets and on two activity prediction tasks: future 3D human pose prediction and future activity prediction. The proposed adversarial grammar outperforms the state-of-the-art approaches, being able to predict much more accurately and further in the future, than prior work.



### Visual Imitation Made Easy
- **Arxiv ID**: http://arxiv.org/abs/2008.04899v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04899v1)
- **Published**: 2020-08-11 17:58:50+00:00
- **Updated**: 2020-08-11 17:58:50+00:00
- **Authors**: Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter Abbeel, Lerrel Pinto
- **Comment**: None
- **Journal**: None
- **Summary**: Visual imitation learning provides a framework for learning complex manipulation behaviors by leveraging human demonstrations. However, current interfaces for imitation such as kinesthetic teaching or teleoperation prohibitively restrict our ability to efficiently collect large-scale data in the wild. Obtaining such diverse demonstration data is paramount for the generalization of learned skills to novel scenarios. In this work, we present an alternate interface for imitation that simplifies the data collection process while allowing for easy transfer to robots. We use commercially available reacher-grabber assistive tools both as a data collection device and as the robot's end-effector. To extract action information from these visual demonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in addition to training a finger detection network. We experimentally evaluate on two challenging tasks: non-prehensile pushing and prehensile stacking, with 1000 diverse demonstrations for each task. For both tasks, we use standard behavior cloning to learn executable policies from the previously collected offline demonstrations. To improve learning performance, we employ a variety of data augmentations and provide an extensive analysis of its effects. Finally, we demonstrate the utility of our interface by evaluating on real robotic scenarios with previously unseen objects and achieve a 87% success rate on pushing and a 62% success rate on stacking. Robot videos are available at https://dhiraj100892.github.io/Visual-Imitation-Made-Easy.



### Learning to See Through Obstructions with Layered Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2008.04902v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04902v3)
- **Published**: 2020-08-11 17:59:31+00:00
- **Updated**: 2021-07-25 06:47:11+00:00
- **Authors**: Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang
- **Comment**: Project page: https://alex04072000.github.io/SOLD/ Code:
  https://github.com/alex04072000/SOLD Extension of the CVPR 2020 paper:
  arXiv:2004.01180
- **Journal**: None
- **Summary**: We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.



### PX-NET: Simple and Efficient Pixel-Wise Training of Photometric Stereo Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.04933v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04933v3)
- **Published**: 2020-08-11 18:03:13+00:00
- **Updated**: 2021-10-12 12:55:11+00:00
- **Authors**: Fotios Logothetis, Ignas Budvytis, Roberto Mecca, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Retrieving accurate 3D reconstructions of objects from the way they reflect light is a very challenging task in computer vision. Despite more than four decades since the definition of the Photometric Stereo problem, most of the literature has had limited success when global illumination effects such as cast shadows, self-reflections and ambient light come into play, especially for specular surfaces.   Recent approaches have leveraged the power of deep learning in conjunction with computer graphics in order to cope with the need of a vast number of training data in order to invert the image irradiance equation and retrieve the geometry of the object. However, rendering global illumination effects is a slow process which can limit the amount of training data that can be generated.   In this work we propose a novel pixel-wise training procedure for normal prediction by replacing the training data (observation maps) of globally rendered images with independent per-pixel generated data. We show that global physical effects can be approximated on the observation map domain and this simplifies and speeds up the data creation procedure.   Our network, PX-NET, achieves the state-of-the-art performance compared to other pixelwise methods on synthetic datasets, as well as the Diligent real dataset on both dense and sparse light settings.



### Estimating the Magnitude and Phase of Automotive Radar Signals under Multiple Interference Sources with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.05948v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05948v2)
- **Published**: 2020-08-11 18:50:38+00:00
- **Updated**: 2021-11-06 08:37:37+00:00
- **Authors**: Nicolae-Cătălin Ristea, Andrei Anghel, Radu Tudor Ionescu
- **Comment**: Accepted in IEEE Access
- **Journal**: None
- **Summary**: Radar sensors are gradually becoming a wide-spread equipment for road vehicles, playing a crucial role in autonomous driving and road safety. The broad adoption of radar sensors increases the chance of interference among sensors from different vehicles, generating corrupted range profiles and range-Doppler maps. In order to extract distance and velocity of multiple targets from range-Doppler maps, the interference affecting each range profile needs to be mitigated. In this paper, we propose a fully convolutional neural network for automotive radar interference mitigation. In order to train our network in a real-world scenario, we introduce a new data set of realistic automotive radar signals with multiple targets and multiple interferers. To our knowledge, we are the first to apply weight pruning in the automotive radar domain, obtaining superior results compared to the widely-used dropout. While most previous works successfully estimated the magnitude of automotive radar signals, we propose a deep learning model that can accurately estimate the phase. For instance, our novel approach reduces the phase estimation error with respect to the commonly-adopted zeroing technique by half, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for automotive radar interference mitigation, we release as open source our large-scale data set that closely replicates the real-world automotive scenario for multiple interference cases, allowing others to objectively compare their future work in this domain. Our data set is available for download at: http://github.com/ristea/arim-v2.



### Image segmentation via Cellular Automata
- **Arxiv ID**: http://arxiv.org/abs/2008.04965v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04965v2)
- **Published**: 2020-08-11 19:04:09+00:00
- **Updated**: 2020-08-13 00:37:47+00:00
- **Authors**: Mark Sandler, Andrey Zhmoginov, Liangcheng Luo, Alexander Mordvintsev, Ettore Randazzo, Blaise Agúera y Arcas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new approach for building cellular automata to solve real-world segmentation problems. We design and train a cellular automaton that can successfully segment high-resolution images. We consider a colony that densely inhabits the pixel grid, and all cells are governed by a randomized update that uses the current state, the color, and the state of the $3\times 3$ neighborhood. The space of possible rules is defined by a small neural network. The update rule is applied repeatedly in parallel to a large random subset of cells and after convergence is used to produce segmentation masks that are then back-propagated to learn the optimal update rules using standard gradient descent methods. We demonstrate that such models can be learned efficiently with only limited trajectory length and that they show remarkable ability to organize the information to produce a globally consistent segmentation result, using only local information exchange. From a practical perspective, our approach allows us to build very efficient models -- our smallest automaton uses less than 10,000 parameters to solve complex segmentation tasks.



### Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical Understanding of Outdoor Scene
- **Arxiv ID**: http://arxiv.org/abs/2008.04968v1
- **DOI**: 10.1145/3394171.3413661
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04968v1)
- **Published**: 2020-08-11 19:10:32+00:00
- **Updated**: 2020-08-11 19:10:32+00:00
- **Authors**: Xinke Li, Chongshou Li, Zekun Tong, Andrew Lim, Junsong Yuan, Yuwei Wu, Jing Tang, Raymond Huang
- **Comment**: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
- **Journal**: Proceedings of the 28th ACM International Conference on Multimedia
  2020
- **Summary**: Learning on 3D scene-based point cloud has received extensive attention as its promising application in many fields, and well-annotated and multisource datasets can catalyze the development of those data-driven approaches. To facilitate the research of this area, we present a richly-annotated 3D point cloud dataset for multiple outdoor scene understanding tasks and also an effective learning framework for its hierarchical segmentation task. The dataset was generated via the photogrammetric processing on unmanned aerial vehicle (UAV) images of the National University of Singapore (NUS) campus, and has been point-wisely annotated with both hierarchical and instance-based labels. Based on it, we formulate a hierarchical learning problem for 3D point cloud segmentation and propose a measurement evaluating consistency across various hierarchies. To solve this problem, a two-stage method including multi-task (MT) learning and hierarchical ensemble (HE) with consistency consideration is proposed. Experimental results demonstrate the superiority of the proposed method and potential advantages of our hierarchical annotations. In addition, we benchmark results of semantic and instance segmentation, which is accessible online at https://3d.dataset.site with the dataset and all source codes.



### Retrieval Guided Unsupervised Multi-domain Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2008.04991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04991v1)
- **Published**: 2020-08-11 20:11:53+00:00
- **Updated**: 2020-08-11 20:11:53+00:00
- **Authors**: Raul Gomez, Yahui Liu, Marco De Nadai, Dimosthenis Karatzas, Bruno Lepri, Nicu Sebe
- **Comment**: Submitted to ACM MM '20, October 12-16, 2020, Seattle, WA, USA
- **Journal**: None
- **Summary**: Image to image translation aims to learn a mapping that transforms an image from one visual domain to another. Recent works assume that images descriptors can be disentangled into a domain-invariant content representation and a domain-specific style representation. Thus, translation models seek to preserve the content of source images while changing the style to a target visual domain. However, synthesizing new images is extremely challenging especially in multi-domain translations, as the network has to compose content and style to generate reliable and diverse images in multiple domains. In this paper we propose the use of an image retrieval system to assist the image-to-image translation task. First, we train an image-to-image translation model to map images to multiple domains. Then, we train an image retrieval model using real and generated images to find images similar to a query one in content but in a different domain. Finally, we exploit the image retrieval system to fine-tune the image-to-image translation model and generate higher quality images. Our experiments show the effectiveness of the proposed solution and highlight the contribution of the retrieval network, which can benefit from additional unlabeled data and help image-to-image translation models in the presence of scarce data.



### VI-Net: View-Invariant Quality of Human Movement Assessment
- **Arxiv ID**: http://arxiv.org/abs/2008.04999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04999v1)
- **Published**: 2020-08-11 20:51:42+00:00
- **Updated**: 2020-08-11 20:51:42+00:00
- **Authors**: Faegheh Sardari, Adeline Paiement, Sion Hannuna, Majid Mirmehdi
- **Comment**: 13 pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: We propose a view-invariant method towards the assessment of the quality of human movements which does not rely on skeleton data. Our end-to-end convolutional neural network consists of two stages, where at first a view-invariant trajectory descriptor for each body joint is generated from RGB images, and then the collection of trajectories for all joints are processed by an adapted, pre-trained 2D CNN (e.g. VGG-19 or ResNeXt-50) to learn the relationship amongst the different body parts and deliver a score for the movement quality. We release the only publicly-available, multi-view, non-skeleton, non-mocap, rehabilitation movement dataset (QMAR), and provide results for both cross-subject and cross-view scenarios on this dataset. We show that VI-Net achieves average rank correlation of 0.66 on cross-subject and 0.65 on unseen views when trained on only two views. We also evaluate the proposed method on the single-view rehabilitation dataset KIMORE and obtain 0.66 rank correlation against a baseline of 0.62.



### Audio- and Gaze-driven Facial Animation of Codec Avatars
- **Arxiv ID**: http://arxiv.org/abs/2008.05023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.05023v1)
- **Published**: 2020-08-11 22:28:48+00:00
- **Updated**: 2020-08-11 22:28:48+00:00
- **Authors**: Alexander Richard, Colin Lea, Shugao Ma, Juergen Gall, Fernando de la Torre, Yaser Sheikh
- **Comment**: None
- **Journal**: None
- **Summary**: Codec Avatars are a recent class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video. In this paper we describe the first approach to animate these parametric models in real-time which could be deployed on commodity virtual reality hardware using audio and/or eye tracking. Our goal is to display expressive conversations between individuals that exhibit important social signals such as laughter and excitement solely from latent cues in our lossy input signals. To this end we collected over 5 hours of high frame rate 3D face scans across three participants including traditional neutral speech as well as expressive and conversational speech. We investigate a multimodal fusion approach that dynamically identifies which sensor encoding should animate which parts of the face at any time. See the supplemental video which demonstrates our ability to generate full face motion far beyond the typically neutral lip articulations seen in competing work: https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/



### Learned Proximal Networks for Quantitative Susceptibility Mapping
- **Arxiv ID**: http://arxiv.org/abs/2008.05024v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.05024v1)
- **Published**: 2020-08-11 22:35:24+00:00
- **Updated**: 2020-08-11 22:35:24+00:00
- **Authors**: Kuo-Wei Lai, Manisha Aggarwal, Peter van Zijl, Xu Li, Jeremias Sulam
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Quantitative Susceptibility Mapping (QSM) estimates tissue magnetic susceptibility distributions from Magnetic Resonance (MR) phase measurements by solving an ill-posed dipole inversion problem. Conventional single orientation QSM methods usually employ regularization strategies to stabilize such inversion, but may suffer from streaking artifacts or over-smoothing. Multiple orientation QSM such as calculation of susceptibility through multiple orientation sampling (COSMOS) can give well-conditioned inversion and an artifact free solution but has expensive acquisition costs. On the other hand, Convolutional Neural Networks (CNN) show great potential for medical image reconstruction, albeit often with limited interpretability. Here, we present a Learned Proximal Convolutional Neural Network (LP-CNN) for solving the ill-posed QSM dipole inversion problem in an iterative proximal gradient descent fashion. This approach combines the strengths of data-driven restoration priors and the clear interpretability of iterative solvers that can take into account the physical model of dipole convolution. During training, our LP-CNN learns an implicit regularizer via its proximal, enabling the decoupling between the forward operator and the data-driven parameters in the reconstruction algorithm. More importantly, this framework is believed to be the first deep learning QSM approach that can naturally handle an arbitrary number of phase input measurements without the need for any ad-hoc rotation or re-training. We demonstrate that the LP-CNN provides state-of-the-art reconstruction results compared to both traditional and deep learning methods while allowing for more flexibility in the reconstruction process.



### End-to-End Rate-Distortion Optimization for Bi-Directional Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2008.05028v2
- **DOI**: 10.1109/ICIP40778.2020.9190881
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.05028v2)
- **Published**: 2020-08-11 22:50:06+00:00
- **Updated**: 2021-05-26 19:12:26+00:00
- **Authors**: M. Akin Yilmaz, A. Murat Tekalp
- **Comment**: This work is accepted for publication in IEEE ICIP 2020
- **Journal**: None
- **Summary**: Conventional video compression methods employ a linear transform and block motion model, and the steps of motion estimation, mode and quantization parameter selection, and entropy coding are optimized individually due to combinatorial nature of the end-to-end optimization problem. Learned video compression allows end-to-end rate-distortion optimized training of all nonlinear modules, quantization parameter and entropy model simultaneously. While previous work on learned video compression considered training a sequential video codec based on end-to-end optimization of cost averaged over pairs of successive frames, it is well-known in conventional video compression that hierarchical, bi-directional coding outperforms sequential compression. In this paper, we propose for the first time end-to-end optimization of a hierarchical, bi-directional motion compensated learned codec by accumulating cost function over fixed-size groups of pictures (GOP). Experimental results show that the rate-distortion performance of our proposed learned bi-directional {\it GOP coder} outperforms the state-of-the-art end-to-end optimized learned sequential compression as expected.



