# Arxiv Papers in cs.CV on 2020-08-02
### Looking in the Right place for Anomalies: Explainable AI through Automatic Location Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.00363v1
- **DOI**: 10.1109/ISBI45749.2020.9098370
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00363v1)
- **Published**: 2020-08-02 00:02:37+00:00
- **Updated**: 2020-08-02 00:02:37+00:00
- **Authors**: Satyananda Kashyap, Alexandros Karargyris, Joy Wu, Yaniv Gur, Arjun Sharma, Ken C. L. Wong, Mehdi Moradi, Tanveer Syeda-Mahmood
- **Comment**: 5 pages, Paper presented as a poster at the International Symposium
  on Biomedical Imaging, 2020, Paper Number 655
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI)
- **Summary**: Deep learning has now become the de facto approach to the recognition of anomalies in medical imaging. Their 'black box' way of classifying medical images into anomaly labels poses problems for their acceptance, particularly with clinicians. Current explainable AI methods offer justifications through visualizations such as heat maps but cannot guarantee that the network is focusing on the relevant image region fully containing the anomaly. In this paper, we develop an approach to explainable AI in which the anomaly is assured to be overlapping the expected location when present. This is made possible by automatically extracting location-specific labels from textual reports and learning the association of expected locations to labels using a hybrid combination of Bi-Directional Long Short-Term Memory Recurrent Neural Networks (Bi-LSTM) and DenseNet-121. Use of this expected location to bias the subsequent attention-guided inference network based on ResNet101 results in the isolation of the anomaly at the expected location when present. The method is evaluated on a large chest X-ray dataset.



### Vision and Inertial Sensing Fusion for Human Action Recognition : A Review
- **Arxiv ID**: http://arxiv.org/abs/2008.00380v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00380v1)
- **Published**: 2020-08-02 02:06:44+00:00
- **Updated**: 2020-08-02 02:06:44+00:00
- **Authors**: Sharmin Majumder, Nasser Kehtarnavaz
- **Comment**: 14 pages,4 figures,2 tables. Submitted to IEEE Sensors Journal
- **Journal**: None
- **Summary**: Human action recognition is used in many applications such as video surveillance, human computer interaction, assistive living, and gaming. Many papers have appeared in the literature showing that the fusion of vision and inertial sensing improves recognition accuracies compared to the situations when each sensing modality is used individually. This paper provides a survey of the papers in which vision and inertial sensing are used simultaneously within a fusion framework in order to perform human action recognition. The surveyed papers are categorized in terms of fusion approaches, features, classifiers, as well as multimodality datasets considered. Challenges as well as possible future directions are also stated for deploying the fusion of these two sensing modalities under realistic conditions.



### Point Cloud Completion by Learning Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2008.00394v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00394v2)
- **Published**: 2020-08-02 04:00:32+00:00
- **Updated**: 2021-07-15 08:07:03+00:00
- **Authors**: Xiaogang Wang, Marcelo H Ang Jr, Gim Hee Lee
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: In view of the difficulty in reconstructing object details in point cloud completion, we propose a shape prior learning method for object completion. The shape priors include geometric information in both complete and the partial point clouds. We design a feature alignment strategy to learn the shape prior from complete points, and a coarse to fine strategy to incorporate partial prior in the fine stage. To learn the complete objects prior, we first train a point cloud auto-encoder to extract the latent embeddings from complete points. Then we learn a mapping to transfer the point features from partial points to that of the complete points by optimizing feature alignment losses. The feature alignment losses consist of a L2 distance and an adversarial loss obtained by Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2 distance optimizes the partial features towards the complete ones in the feature space, and MMD-GAN decreases the statistical distance of two point features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art performances on the point cloud completion task. Our code is available at https://github.com/xiaogangw/point-cloud-completion-shape-prior.



### A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/2008.01645v3
- **DOI**: 10.1109/TVCG.2020.3028889
- **Categories**: **cs.HC**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01645v3)
- **Published**: 2020-08-02 04:22:43+00:00
- **Updated**: 2021-10-27 15:58:42+00:00
- **Authors**: Takanori Fujiwara, Shilpika, Naohisa Sakamoto, Jorji Nonaka, Keiji Yamamoto, Kwan-Liu Ma
- **Comment**: This is the author's version of the article that has been published
  in IEEE Transactions on Visualization and Computer Graphics. The final
  version of this record is available at: 10.1109/TVCG.2020.3028889
- **Journal**: None
- **Summary**: Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.



### Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.01652v1
- **DOI**: 10.1145/3394171.3413709
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01652v1)
- **Published**: 2020-08-02 04:38:59+00:00
- **Updated**: 2020-08-02 04:38:59+00:00
- **Authors**: Yanhui Guo, Xi Zhang, Xiaolin Wu
- **Comment**: Accepted by Proceedings of the 28th ACM International Conference on
  Multimedia(ACM MM),2020
- **Journal**: Proceedings of the 28th ACM International Conference on
  Multimedia,2020
- **Summary**: We propose a novel deep multi-modality neural network for restoring very low bit rate videos of talking heads. Such video contents are very common in social media, teleconferencing, distance education, tele-medicine, etc., and often need to be transmitted with limited bandwidth. The proposed CNN method exploits the correlations among three modalities, video, audio and emotion state of the speaker, to remove the video compression artifacts caused by spatial down sampling and quantization. The deep learning approach turns out to be ideally suited for the video restoration task, as the complex non-linear cross-modality correlations are very difficult to model analytically and explicitly. The new method is a video post processor that can significantly boost the perceptual quality of aggressively compressed talking head videos, while being fully compatible with all existing video compression standards.



### SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space
- **Arxiv ID**: http://arxiv.org/abs/2008.00397v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00397v2)
- **Published**: 2020-08-02 04:57:54+00:00
- **Updated**: 2022-04-27 14:03:01+00:00
- **Authors**: Liu Yang
- **Comment**: This submission has been withdrawn by arXiv administrators due to an
  unresolvable authorship dispute
- **Journal**: None
- **Summary**: In this work, we formulate a visual dialog as an information flow in which each piece of information is encoded with the joint visual-linguistic representation of a single dialog round. Based on this formulation, we consider the visual dialog task as a sequence problem consisting of ordered visual-linguistic vectors. For featurization, we use a Dense Symmetric Co-Attention network as a lightweight vison-language joint representation generator to fuse multimodal features (i.e., image and text), yielding better computation and data efficiencies. For inference, we propose two Sequential Dialog Networks (SeqDialN): the first uses LSTM for information propagation (IP) and the second uses a modified Transformer for multi-step reasoning (MR). Our architecture separates the complexity of multimodal feature fusion from that of inference, which allows simpler design of the inference engine. IP based SeqDialN is our baseline with a simple 2-layer LSTM design that achieves decent performance. MR based SeqDialN, on the other hand, recurrently refines the semantic question/history representations through the self-attention stack of Transformer and produces promising results on the visual dialog task. On VisDial v1.0 test-std dataset, our best single generative SeqDialN achieves 62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78% NDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog model. We fine-tune discriminative SeqDialN with dense annotations and boost the performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the extensive experiments we have conducted to demonstrate the effectiveness of our model components. We also provide visualization for the reasoning process from the relevant conversation rounds and discuss our fine-tuning methods. Our code is available at https://github.com/xiaoxiaoheimei/SeqDialN



### Removing Backdoor-Based Watermarks in Neural Networks with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2008.00407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00407v2)
- **Published**: 2020-08-02 06:25:26+00:00
- **Updated**: 2020-08-08 03:31:14+00:00
- **Authors**: Xuankai Liu, Fengting Li, Bihan Wen, Qi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been widely applied and achieved great success in various fields. As training deep models usually consumes massive data and computational resources, trading the trained deep models is highly demanded and lucrative nowadays. Unfortunately, the naive trading schemes typically involves potential risks related to copyright and trustworthiness issues, e.g., a sold model can be illegally resold to others without further authorization to reap huge profits. To tackle this problem, various watermarking techniques are proposed to protect the model intellectual property, amongst which the backdoor-based watermarking is the most commonly-used one. However, the robustness of these watermarking approaches is not well evaluated under realistic settings, such as limited in-distribution data availability and agnostic of watermarking patterns. In this paper, we benchmark the robustness of watermarking, and propose a novel backdoor-based watermark removal framework using limited data, dubbed WILD. The proposed WILD removes the watermarks of deep models with only a small portion of training data, and the output model can perform the same as models trained from scratch without watermarks injected. In particular, a novel data augmentation method is utilized to mimic the behavior of watermark triggers. Combining with the distribution alignment between the normal and perturbed (e.g., occluded) data in the feature space, our approach generalizes well on all typical types of trigger contents. The experimental results demonstrate that our approach can effectively remove the watermarks without compromising the deep model performance for the original task with the limited access to training data.



### Blind Face Restoration via Deep Multi-scale Component Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/2008.00418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00418v1)
- **Published**: 2020-08-02 07:02:07+00:00
- **Updated**: 2020-08-02 07:02:07+00:00
- **Authors**: Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang
- **Comment**: In ECCV 2020. Code is available at:
  https://github.com/csxmli2016/DFDNet
- **Journal**: None
- **Summary**: Recent reference-based face restoration methods have received considerable attention due to their great capability in recovering high-frequency details on real low-quality images. However, most of these methods require a high-quality reference image of the same identity, making them only applicable in limited scenes. To address this issue, this paper suggests a deep face dictionary network (termed as DFDNet) to guide the restoration process of degraded observations. To begin with, we use K-means to generate deep dictionaries for perceptually significant face components (\ie, left/right eyes, nose and mouth) from high-quality images. Next, with the degraded input, we match and select the most similar component features from their corresponding dictionaries and transfer the high-quality details to the input via the proposed dictionary feature transfer (DFT) block. In particular, component AdaIN is leveraged to eliminate the style diversity between the input and dictionary features (\eg, illumination), and a confidence score is proposed to adaptively fuse the dictionary feature to the input. Finally, multi-scale dictionaries are adopted in a progressive manner to enable the coarse-to-fine restoration. Experiments show that our proposed method can achieve plausible performance in both quantitative and qualitative evaluation, and more importantly, can generate realistic and promising results on real degraded images without requiring an identity-belonging reference. The source code and models are available at \url{https://github.com/csxmli2016/DFDNet}.



### Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.00446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00446v1)
- **Published**: 2020-08-02 10:26:09+00:00
- **Updated**: 2020-08-02 10:26:09+00:00
- **Authors**: Lei Zhou, Zixin Luo, Mingmin Zhen, Tianwei Shen, Shiwei Li, Zhuofei Huang, Tian Fang, Long Quan
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Current bundle adjustment solvers such as the Levenberg-Marquardt (LM) algorithm are limited by the bottleneck in solving the Reduced Camera System (RCS) whose dimension is proportional to the camera number. When the problem is scaled up, this step is neither efficient in computation nor manageable for a single compute node. In this work, we propose a stochastic bundle adjustment algorithm which seeks to decompose the RCS approximately inside the LM iterations to improve the efficiency and scalability. It first reformulates the quadratic programming problem of an LM iteration based on the clustering of the visibility graph by introducing the equality constraints across clusters. Then, we propose to relax it into a chance constrained problem and solve it through sampled convex program. The relaxation is intended to eliminate the interdependence between clusters embodied by the constraints, so that a large RCS can be decomposed into independent linear sub-problems. Numerical experiments on unordered Internet image sets and sequential SLAM image sets, as well as distributed experiments on large-scale datasets, have demonstrated the high efficiency and scalability of the proposed approach. Codes are released at https://github.com/zlthinker/STBA.



### Video Super-Resolution with Recurrent Structure-Detail Network
- **Arxiv ID**: http://arxiv.org/abs/2008.00455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00455v1)
- **Published**: 2020-08-02 11:01:19+00:00
- **Updated**: 2020-08-02 11:01:19+00:00
- **Authors**: Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, Qi Tian
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Most video super-resolution methods super-resolve a single reference frame with the help of neighboring frames in a temporal sliding window. They are less efficient compared to the recurrent-based methods. In this work, we propose a novel recurrent video super-resolution method which is both effective and efficient in exploiting previous frames to super-resolve the current frame. It divides the input into structure and detail components which are fed to a recurrent unit composed of several proposed two-stream structure-detail blocks. In addition, a hidden state adaptation module that allows the current frame to selectively use information from hidden state is introduced to enhance its robustness to appearance change and error accumulation. Extensive ablation study validate the effectiveness of the proposed modules. Experiments on several benchmark datasets demonstrate the superior performance of the proposed method compared to state-of-the-art methods on video super-resolution.



### Hindsight for Foresight: Unsupervised Structured Dynamics Models from Physical Interaction
- **Arxiv ID**: http://arxiv.org/abs/2008.00456v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00456v1)
- **Published**: 2020-08-02 11:04:49+00:00
- **Updated**: 2020-08-02 11:04:49+00:00
- **Authors**: Iman Nematollahi, Oier Mees, Lukas Hermann, Wolfram Burgard
- **Comment**: Accepted at the 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: A key challenge for an agent learning to interact with the world is to reason about physical properties of objects and to foresee their dynamics under the effect of applied forces. In order to scale learning through interaction to many objects and scenes, robots should be able to improve their own performance from real-world experience without requiring human supervision. To this end, we propose a novel approach for modeling the dynamics of a robot's interactions directly from unlabeled 3D point clouds and images. Unlike previous approaches, our method does not require ground-truth data associations provided by a tracker or any pre-trained perception network. To learn from unlabeled real-world interaction data, we enforce consistency of estimated 3D clouds, actions and 2D images with observed ones. Our joint forward and inverse network learns to segment a scene into salient object parts and predicts their 3D motion under the effect of applied actions. Moreover, our object-centric model outputs action-conditioned 3D scene flow, object masks and 2D optical flow as emergent properties. Our extensive evaluation both in simulation and with real-world data demonstrates that our formulation leads to effective, interpretable models that can be used for visuomotor control and planning. Videos, code and dataset are available at http://hind4sight.cs.uni-freiburg.de



### Joint Object Contour Points and Semantics for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00460v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00460v3)
- **Published**: 2020-08-02 11:11:28+00:00
- **Updated**: 2021-07-04 02:21:19+00:00
- **Authors**: Wenchao Zhang, Chong Fu, Mai Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The attributes of object contours has great significance for instance segmentation task. However, most of the current popular deep neural networks do not pay much attention to the object edge information. Inspired by the human annotation process when making instance segmentation datasets, in this paper, we propose Mask Point R-CNN aiming at promoting the neural network's attention to the object boundary. Specifically, we innovatively extend the original human keypoint detection task to the contour point detection of any object. Based on this analogy, we present an contour point detection auxiliary task to Mask R-CNN, which can boost the gradient flow between different tasks by effectively using feature fusion strategies and multi-task joint training. As a consequence, the model will be more sensitive to the edges of the object and can capture more geometric features. Quantitatively, the experimental results show that our approach outperforms vanilla Mask R-CNN by 3.8\% on Cityscapes dataset and 0.8\% on COCO dataset.



### A Plug-and-play Scheme to Adapt Image Saliency Deep Model for Video Data
- **Arxiv ID**: http://arxiv.org/abs/2008.09103v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.09103v1)
- **Published**: 2020-08-02 13:23:14+00:00
- **Updated**: 2020-08-02 13:23:14+00:00
- **Authors**: Yunxiao Li, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
- **Comment**: 12 pages, 10 figures, and, this paper is currently in peer review in
  IEEE TCSVT
- **Journal**: None
- **Summary**: With the rapid development of deep learning techniques, image saliency deep models trained solely by spatial information have occasionally achieved detection performance for video data comparable to that of the models trained by both spatial and temporal information. However, due to the lesser consideration of temporal information, the image saliency deep models may become fragile in the video sequences dominated by temporal information. Thus, the most recent video saliency detection approaches have adopted the network architecture starting with a spatial deep model that is followed by an elaborately designed temporal deep model. However, such methods easily encounter the performance bottleneck arising from the single stream learning methodology, so the overall detection performance is largely determined by the spatial deep model. In sharp contrast to the current mainstream methods, this paper proposes a novel plug-and-play scheme to weakly retrain a pretrained image saliency deep model for video data by using the newly sensed and coded temporal information. Thus, the retrained image saliency deep model will be able to maintain temporal saliency awareness, achieving much improved detection performance. Moreover, our method is simple yet effective for adapting any off-the-shelf pre-trained image saliency deep model to obtain high-quality video saliency detection. Additionally, both the data and source code of our method are publicly available.



### SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries of 3D Shapes from Single-View RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/2008.00485v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.00485v3)
- **Published**: 2020-08-02 14:10:09+00:00
- **Updated**: 2020-08-30 23:56:39+00:00
- **Authors**: Yifei Shi, Junwen Huang, Hongjia Zhang, Xin Xu, Szymon Rusinkiewicz, Kai Xu
- **Comment**: 15 pages
- **Journal**: ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020
- **Summary**: We study the problem of symmetry detection of 3D shapes from single-view RGB-D images, where severely missing data renders geometric detection approach infeasible. We propose an end-to-end deep neural network which is able to predict both reflectional and rotational symmetries of 3D objects present in the input RGB-D image. Directly training a deep model for symmetry prediction, however, can quickly run into the issue of overfitting. We adopt a multi-task learning approach. Aside from symmetry axis prediction, our network is also trained to predict symmetry correspondences. In particular, given the 3D points present in the RGB-D image, our network outputs for each 3D point its symmetric counterpart corresponding to a specific predicted symmetry. In addition, our network is able to detect for a given shape multiple symmetries of different types. We also contribute a benchmark of 3D symmetry detection based on single-view RGB-D images. Extensive evaluation on the benchmark demonstrates the strong generalization ability of our method, in terms of high accuracy of both symmetry axis prediction and counterpart estimation. In particular, our method is robust in handling unseen object instances with large variation in shape, multi-symmetry composition, as well as novel object categories.



### A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises
- **Arxiv ID**: http://arxiv.org/abs/2008.09104v2
- **DOI**: 10.1109/JPROC.2021.3054390
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09104v2)
- **Published**: 2020-08-02 14:26:13+00:00
- **Updated**: 2021-03-05 13:17:05+00:00
- **Authors**: S. Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S. Duncan, Bram van Ginneken, Anant Madabhushi, Jerry L. Prince, Daniel Rueckert, Ronald M. Summers
- **Comment**: 20 pages, 7 figures
- **Journal**: Proceedings of the IEEE (2021)
- **Summary**: Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first present traits of medical imaging, highlight both clinical needs and technical challenges in medical imaging, and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.



### Tensor Low-Rank Reconstruction for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00490v1)
- **Published**: 2020-08-02 14:29:28+00:00
- **Updated**: 2020-08-02 14:29:28+00:00
- **Authors**: Wanli Chen, Xinge Zhu, Ruoqi Sun, Junjun He, Ruiyu Li, Xiaoyong Shen, Bei Yu
- **Comment**: ECCV2020. Top-1 performance on PASCAL-VOC12; Source code at
  https://github.com/CWanli/RecoNet.git
- **Journal**: None
- **Summary**: Context information plays an indispensable role in the success of semantic segmentation. Recently, non-local self-attention based methods are proved to be effective for context information collection. Since the desired context consists of spatial-wise and channel-wise attentions, 3D representation is an appropriate formulation. However, these non-local methods describe 3D context information based on a 2D similarity matrix, where space compression may lead to channel-wise attention missing. An alternative is to model the contextual information directly without compression. However, this effort confronts a fundamental difficulty, namely the high-rank property of context information. In this paper, we propose a new approach to model the 3D context representations, which not only avoids the space compression but also tackles the high-rank difficulty. Here, inspired by tensor canonical-polyadic decomposition theory (i.e, a high-rank tensor can be expressed as a combination of rank-1 tensors.), we design a low-rank-to-high-rank context reconstruction framework (i.e, RecoNet). Specifically, we first introduce the tensor generation module (TGM), which generates a number of rank-1 tensors to capture fragments of context feature. Then we use these rank-1 tensors to recover the high-rank context features through our proposed tensor reconstruction module (TRM). Extensive experiments show that our method achieves state-of-the-art on various public datasets. Additionally, our proposed method has more than 100 times less computational cost compared with conventional non-local-based methods.



### HyperFaceNet: A Hyperspectral Face Recognition Method Based on Deep Fusion
- **Arxiv ID**: http://arxiv.org/abs/2008.00498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00498v2)
- **Published**: 2020-08-02 14:59:24+00:00
- **Updated**: 2020-09-12 09:46:33+00:00
- **Authors**: Zhicheng Cao, Xi Cen, Liaojun Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has already been well studied under the visible light and the infrared,in both intra-spectral and cross-spectral cases. However, how to fuse different light bands, i.e., hyperspectral face recognition, is still an open research problem, which has the advantages of richer information retaining and all-weather functionality over single band face recognition. Among the very few works for hyperspectral face recognition, traditional non-deep learning techniques are largely used. Thus, we in this paper bring deep learning into the topic of hyperspectral face recognition, and propose a new fusion model (termed HyperFaceNet) especially for hyperspectral faces. The proposed fusion model is characterized by residual dense learning, a feedback style encoder and a recognition-oriented loss function. During the experiments, our method is proved to be of higher recognition rates than face recognition using either visible light or the infrared. Moreover, our fusion model is shown to be superior to other general-purposed image fusion methods including state-of-the-arts, in terms of both image quality and recognition performance.



### Multi-level Wavelet-based Generative Adversarial Network for Perceptual Quality Enhancement of Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/2008.00499v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00499v1)
- **Published**: 2020-08-02 15:01:38+00:00
- **Updated**: 2020-08-02 15:01:38+00:00
- **Authors**: Jianyi Wang, Xin Deng, Mai Xu, Congyong Chen, Yuhang Song
- **Comment**: None
- **Journal**: 16th European conference on computer vision. 2020 Aug 23
- **Summary**: The past few years have witnessed fast development in video quality enhancement via deep learning. Existing methods mainly focus on enhancing the objective quality of compressed video while ignoring its perceptual quality. In this paper, we focus on enhancing the perceptual quality of compressed video. Our main observation is that enhancing the perceptual quality mostly relies on recovering high-frequency sub-bands in wavelet domain. Accordingly, we propose a novel generative adversarial network (GAN) based on multi-level wavelet packet transform (WPT) to enhance the perceptual quality of compressed video, which is called multi-level wavelet-based GAN (MW-GAN). In MW-GAN, we first apply motion compensation with a pyramid architecture to obtain temporal information. Then, we propose a wavelet reconstruction network with wavelet-dense residual blocks (WDRB) to recover the high-frequency details. In addition, the adversarial loss of MW-GAN is added via WPT to further encourage high-frequency details recovery for video frames. Experimental results demonstrate the superiority of our method.



### Differentiable Feature Aggregation Search for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2008.00506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00506v1)
- **Published**: 2020-08-02 15:42:29+00:00
- **Updated**: 2020-08-02 15:42:29+00:00
- **Authors**: Yushuo Guan, Pengyu Zhao, Bingxuan Wang, Yuanxing Zhang, Cong Yao, Kaigui Bian, Jian Tang
- **Comment**: A feature distillation method via differentiable architecture search
- **Journal**: None
- **Summary**: Knowledge distillation has become increasingly important in model compression. It boosts the performance of a miniaturized student network with the supervision of the output distribution and feature maps from a sophisticated teacher network. Some recent works introduce multi-teacher distillation to provide more supervision to the student network. However, the effectiveness of multi-teacher distillation methods are accompanied by costly computation resources. To tackle with both the efficiency and the effectiveness of knowledge distillation, we introduce the feature aggregation to imitate the multi-teacher distillation in the single-teacher distillation framework by extracting informative supervision from multiple teacher feature maps. Specifically, we introduce DFA, a two-stage Differentiable Feature Aggregation search method that motivated by DARTS in neural architecture search, to efficiently find the aggregations. In the first stage, DFA formulates the searching problem as a bi-level optimization and leverages a novel bridge loss, which consists of a student-to-teacher path and a teacher-to-student path, to find appropriate feature aggregations. The two paths act as two players against each other, trying to optimize the unified architecture parameters to the opposite directions while guaranteeing both expressivity and learnability of the feature aggregation simultaneously. In the second stage, DFA performs knowledge distillation with the derived feature aggregation. Experimental results show that DFA outperforms existing methods on CIFAR-100 and CINIC-10 datasets under various teacher-student settings, verifying the effectiveness and robustness of the design.



### Integrated monitoring of ice in selected Swiss lakes. Final project report
- **Arxiv ID**: http://arxiv.org/abs/2008.00512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00512v1)
- **Published**: 2020-08-02 16:18:51+00:00
- **Updated**: 2020-08-02 16:18:51+00:00
- **Authors**: Manu Tom, Melanie Suetterlin, Damien Bouffard, Mathias Rothermel, Stefan Wunderle, Emmanuel Baltsavias
- **Comment**: None
- **Journal**: None
- **Summary**: Various lake observables, including lake ice, are related to climate and climate change and provide a good opportunity for long-term monitoring. Lakes (and as part of them lake ice) is therefore considered an Essential Climate Variable (ECV) of the Global Climate Observing System (GCOS). Following the need for an integrated multi-temporal monitoring of lake ice in Switzerland, MeteoSwiss in the framework of GCOS Switzerland supported this 2-year project to explore not only the use of satellite images but also the possibilities of Webcams and in-situ measurements. The aim of this project is to monitor some target lakes and detect the extent of ice and especially the ice-on/off dates, with focus on the integration of various input data and processing methods. The target lakes are: St. Moritz, Silvaplana, Sils, Sihl, Greifen and Aegeri, whereby only the first four were mainly frozen during the observation period and thus processed. The observation period was mainly the winter 2016-17. During the project, various approaches were developed, implemented, tested and compared. Firstly, low spatial resolution (250 - 1000 m) but high temporal resolution (1 day) satellite images from the optical sensors MODIS and VIIRS were used. Secondly, and as a pilot project, the use of existing public Webcams was investigated for (a) validation of results from satellite data, and (b) independent estimation of lake ice, especially for small lakes like St. Moritz, that could not be possibly monitored in the satellite images. Thirdly, in-situ measurements were made in order to characterize the development of the temperature profiles and partly pressure before freezing and under the ice-cover until melting. This report presents the results of the project work.



### Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters
- **Arxiv ID**: http://arxiv.org/abs/2008.00528v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00528v2)
- **Published**: 2020-08-02 17:36:25+00:00
- **Updated**: 2020-08-30 13:39:21+00:00
- **Authors**: Yujie He, Changhong Fu, Fuling Lin, Yiming Li, Peng Lu
- **Comment**: IROS'20 accepted, 8 pages, 6 figures, and 2 tables
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2020), Las Vegas, USA
- **Summary**: Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a novel object tracking framework is proposed to leverage multi-level visual attention. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintaining high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmarks with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm favorably outperforms 12 state-of-the-art methods, yielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of $\sim$ 28 frames per second.



### Efficient Deep Learning of Non-local Features for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.00542v1
- **DOI**: 10.1109/TGRS.2020.3014286
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00542v1)
- **Published**: 2020-08-02 19:13:22+00:00
- **Updated**: 2020-08-02 19:13:22+00:00
- **Authors**: Yu Shen, Sijie Zhu, Chen Chen, Qian Du, Liang Xiao, Jianyu Chen, Delu Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based methods, such as Convolution Neural Network (CNN), have demonstrated their efficiency in hyperspectral image (HSI) classification. These methods can automatically learn spectral-spatial discriminative features within local patches. However, for each pixel in an HSI, it is not only related to its nearby pixels but also has connections to pixels far away from itself. Therefore, to incorporate the long-range contextual information, a deep fully convolutional network (FCN) with an efficient non-local module, named ENL-FCN, is proposed for HSI classification. In the proposed framework, a deep FCN considers an entire HSI as input and extracts spectral-spatial information in a local receptive field. The efficient non-local module is embedded in the network as a learning unit to capture the long-range contextual information. Different from the traditional non-local neural networks, the long-range contextual information is extracted in a specially designed criss-cross path for computation efficiency. Furthermore, by using a recurrent operation, each pixel's response is aggregated from all pixels of HSI. The benefits of our proposed ENL-FCN are threefold: 1) the long-range contextual information is incorporated effectively, 2) the efficient module can be freely embedded in a deep neural network in a plug-and-play fashion, and 3) it has much fewer learning parameters and requires less computational resources. The experiments conducted on three popular HSI datasets demonstrate that the proposed method achieves state-of-the-art classification performance with lower computational cost in comparison with several leading deep neural networks for HSI.



### Video Question Answering on Screencast Tutorials
- **Arxiv ID**: http://arxiv.org/abs/2008.00544v1
- **DOI**: 10.24963/ijcai.2020/148
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00544v1)
- **Published**: 2020-08-02 19:27:42+00:00
- **Updated**: 2020-08-02 19:27:42+00:00
- **Authors**: Wentian Zhao, Seokhwan Kim, Ning Xu, Hailin Jin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new video question answering task on screencast tutorials. We introduce a dataset including question, answer and context triples from the tutorial videos for a software. Unlike other video question answering works, all the answers in our dataset are grounded to the domain knowledge base. An one-shot recognition algorithm is designed to extract the visual cues, which helps enhance the performance of video question answering. We also propose several baseline neural network architectures based on various aspects of video contexts from the dataset. The experimental results demonstrate that our proposed models significantly improve the question answering performances by incorporating multi-modal contexts and domain knowledge.



### Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications
- **Arxiv ID**: http://arxiv.org/abs/2008.00549v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00549v3)
- **Published**: 2020-08-02 19:39:14+00:00
- **Updated**: 2021-08-27 05:48:36+00:00
- **Authors**: Ruimin Ke, Zhiyong Cui, Yanlong Chen, Meixin Zhu, Hao Yang, Yinhai Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic near-crash events serve as critical data sources for various smart transportation applications, such as being surrogate safety measures for traffic safety research and corner case data for automated vehicle testing. However, there are several key challenges for near-crash detection. First, extracting near-crashes from original data sources requires significant computing, communication, and storage resources. Also, existing methods lack efficiency and transferability, which bottlenecks prospective large-scale applications. To this end, this paper leverages the power of edge computing to address these challenges by processing the video streams from existing dashcams onboard in a real-time manner. We design a multi-thread system architecture that operates on edge devices and model the bounding boxes generated by object detection and tracking in linear complexity. The method is insensitive to camera parameters and backward compatible with different vehicles. The edge computing system has been evaluated with recorded videos and real-world tests on two cars and four buses for over ten thousand hours. It filters out irrelevant videos in real-time thereby saving labor cost, processing time, network bandwidth, and data storage. It collects not only event videos but also other valuable data such as road user type, event location, time to collision, vehicle trajectory, vehicle speed, brake switch, and throttle. The experiments demonstrate the promising performance of the system regarding efficiency, accuracy, reliability, and transferability. It is among the first efforts in applying edge computing for real-time traffic video analytics and is expected to benefit multiple sub-fields in smart transportation research and applications.



### Semi-supervised deep learning based on label propagation in a 2D embedded space
- **Arxiv ID**: http://arxiv.org/abs/2008.00558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, 68T09, 68T10, I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2008.00558v2)
- **Published**: 2020-08-02 20:08:54+00:00
- **Updated**: 2021-01-15 14:30:27+00:00
- **Authors**: Barbara Caroline Benato, Jancarlo Ferreira Gomes, Alexandru Cristian Telea, Alexandre Xavier Falcão
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: While convolutional neural networks need large labeled sets for training images, expert human supervision of such datasets can be very laborious. Proposed solutions propagate labels from a small set of supervised images to a large set of unsupervised ones to obtain sufficient truly-and-artificially labeled samples to train a deep neural network model. Yet, such solutions need many supervised images for validation. We present a loop in which a deep neural network (VGG-16) is trained from a set with more correctly labeled samples along iterations, created by using t-SNE to project the features of its last max-pooling layer into a 2D embedded space in which labels are propagated using the Optimum-Path Forest semi-supervised classifier. As the labeled set improves along iterations, it improves the features of the neural network. We show that this can significantly improve classification results on test data (using only 1\% to 5\% of supervised samples) of three private challenging datasets and two public ones.



