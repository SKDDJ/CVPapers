# Arxiv Papers in cs.CV on 2020-08-01
### White-Box Evaluation of Fingerprint Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2008.00128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00128v1)
- **Published**: 2020-08-01 00:14:37+00:00
- **Updated**: 2020-08-01 00:14:37+00:00
- **Authors**: Steven A. Grosz, Joshua J. Engelsma, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Typical evaluations of fingerprint recognition systems consist of end-to-end black-box evaluations, which assess performance in terms of overall identification or authentication accuracy. However, these black-box tests of system performance do not reveal insights into the performance of the individual modules, including image acquisition, feature extraction, and matching. On the other hand, white-box evaluations, the topic of this paper, measure the individual performance of each constituent module in isolation. While a few studies have conducted white-box evaluations of the fingerprint reader, feature extractor, and matching components, no existing study has provided a full system, white-box analysis of the uncertainty introduced at each stage of a fingerprint recognition system. In this work, we extend previous white-box evaluations of fingerprint recognition system components and provide a unified, in-depth analysis of fingerprint recognition system performance based on the aggregated white-box evaluation results. In particular, we analyze the uncertainty introduced at each stage of the fingerprint recognition system due to adverse capture conditions (i.e., varying illumination, moisture, and pressure) at the time of acquisition. Our experiments show that a system that performs better overall, in terms of black-box recognition performance, does not necessarily perform best at each module in the fingerprint recognition system pipeline, which can only be seen with white-box analysis of each sub-module. Findings such as these enable researchers to better focus their efforts in improving fingerprint recognition systems.



### Actor-Action Video Classification CSC 249/449 Spring 2020 Challenge Report
- **Arxiv ID**: http://arxiv.org/abs/2008.00141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00141v2)
- **Published**: 2020-08-01 01:07:41+00:00
- **Updated**: 2020-08-18 16:19:49+00:00
- **Authors**: Jing Shi, Zhiheng Li, Haitian Zheng, Yihang Xu, Tianyou Xiao, Weitao Tan, Xiaoning Guo, Sizhe Li, Bin Yang, Zhexin Xu, Ruitao Lin, Zhongkai Shangguan, Yue Zhao, Jingwen Wang, Rohan Sharma, Surya Iyer, Ajinkya Deshmukh, Raunak Mahalik, Srishti Singh, Jayant G Rohra, Yipeng Zhang, Tongyu Yang, Xuan Wen, Ethan Fahnestock, Bryce Ikeda, Ian Lawson, Alan Finkelstein, Kehao Guo, Richard Magnotti, Andrew Sexton, Jeet Ketan Thaker, Yiyang Su, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report summarizes submissions and compiles from Actor-Action video classification challenge held as a final project in CSC 249/449 Machine Vision course (Spring 2020) at University of Rochester



### Diabetic Retinopathy Diagnosis based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.00148v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00148v1)
- **Published**: 2020-08-01 01:56:04+00:00
- **Updated**: 2020-08-01 01:56:04+00:00
- **Authors**: Mohammed hamzah abed, Lamia Abed Noor Muhammed, Sarah Hussein Toman
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Diabetic Retinopathy DR is a popular disease for many people as a result of age or the diabetic, as a result, it can cause blindness. therefore, diagnosis of this disease especially in the early time can prevent its effect for a lot of patients. To achieve this diagnosis, eye retina must be examined continuously. Therefore, computer-aided tools can be used in the field based on computer vision techniques. Different works have been performed using various machine learning techniques. Convolutional Neural Network is one of the promise methods, so it was for Diabetic Retinopathy detection in this paper. Also, the proposed work contains visual enhancement in the pre-processing phase, then the CNN model is trained to be able for recognition and classification phase, to diagnosis the healthy and unhealthy retina image. Three public dataset DiaretDB0, DiaretDB1 and DrimDB were used in practical testing. The implementation of this work based on Matlab- R2019a, deep learning toolbox and deep network designer to design the architecture of the convolutional neural network and train it. The results were evaluated to different metrics; accuracy is one of them. The best accuracy that was achieved: for DiaretDB0 is 100%, DiaretDB1 is 99.495% and DrimDB is 97.55%.



### Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.01567v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01567v3)
- **Published**: 2020-08-01 02:32:43+00:00
- **Updated**: 2021-02-20 01:06:06+00:00
- **Authors**: Soumendu Majee, Thilo Balke, Craig A. J. Kemp, Gregery T. Buzzard, Charles A. Bouman
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1906.06601
- **Journal**: None
- **Summary**: Inverse problems spanning four or more dimensions such as space, time and other independent parameters have become increasingly important. State-of-the-art 4D reconstruction methods use model based iterative reconstruction (MBIR), but depend critically on the quality of the prior modeling. Recently, plug-and-play (PnP) methods have been shown to be an effective way to incorporate advanced prior models using state-of-the-art denoising algorithms. However, state-of-the-art denoisers such as BM4D and deep convolutional neural networks (CNNs) are primarily available for 2D or 3D images and extending them to higher dimensions is difficult due to algorithmic complexity and the increased difficulty of effective training.   In this paper, we present multi-slice fusion, a novel algorithm for 4D reconstruction, based on the fusion of multiple low-dimensional denoisers. Our approach uses multi-agent consensus equilibrium (MACE), an extension of plug-and-play, as a framework for integrating the multiple lower-dimensional models. We apply our method to 4D cone-beam X-ray CT reconstruction for non destructive evaluation (NDE) of samples that are dynamically moving during acquisition. We implement multi-slice fusion on distributed, heterogeneous clusters in order to reconstruct large 4D volumes in reasonable time and demonstrate the inherent parallelizable nature of the algorithm. We present simulated and real experimental results on sparse-view and limited-angle CT data to demonstrate that multi-slice fusion can substantially improve the quality of reconstructions relative to traditional methods, while also being practical to implement and train.



### L-CNN: A Lattice cross-fusion strategy for multistream convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2008.00157v1
- **DOI**: 10.1049/el.2019.2631
- **Categories**: **cs.CV**, cs.LG, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.00157v1)
- **Published**: 2020-08-01 03:08:28+00:00
- **Updated**: 2020-08-01 03:08:28+00:00
- **Authors**: Ana Paula G. S. de Almeida, Flavio de Barros Vidal
- **Comment**: 5 pages, 3 figures
- **Journal**: Electronics Letters, vol. 55, no. 22, pp. 1180-1182, 2029
- **Summary**: This paper proposes a fusion strategy for multistream convolutional networks, the Lattice Cross Fusion. This approach crosses signals from convolution layers performing mathematical operation-based fusions right before pooling layers. Results on a purposely worsened CIFAR-10, a popular image classification data set, with a modified AlexNet-LCNN version show that this novel method outperforms by 46% the baseline single stream network, with faster convergence, stability, and robustness.



### TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D Video
- **Arxiv ID**: http://arxiv.org/abs/2008.00158v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00158v3)
- **Published**: 2020-08-01 03:17:23+00:00
- **Updated**: 2020-09-21 03:14:17+00:00
- **Authors**: Tiancheng Zhi, Christoph Lassner, Tony Tung, Carsten Stoll, Srinivasa G. Narasimhan, Minh Vo
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present TexMesh, a novel approach to reconstruct detailed human meshes with high-resolution full-body texture from RGB-D video. TexMesh enables high quality free-viewpoint rendering of humans. Given the RGB frames, the captured environment map, and the coarse per-frame human mesh from RGB-D tracking, our method reconstructs spatiotemporally consistent and detailed per-frame meshes along with a high-resolution albedo texture. By using the incident illumination we are able to accurately estimate local surface geometry and albedo, which allows us to further use photometric constraints to adapt a synthetically trained model to real-world sequences in a self-supervised manner for detailed surface geometry and high-resolution texture estimation. In practice, we train our models on a short example sequence for self-adaptation and the model runs at interactive framerate afterwards. We validate TexMesh on synthetic and real-world data, and show it outperforms the state of art quantitatively and qualitatively.



### TactileSGNet: A Spiking Graph Neural Network for Event-based Tactile Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.08046v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.08046v1)
- **Published**: 2020-08-01 03:35:15+00:00
- **Updated**: 2020-08-01 03:35:15+00:00
- **Authors**: Fuqiang Gu, Weicong Sng, Tasbolat Taunyazov, Harold Soh
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: Tactile perception is crucial for a variety of robot tasks including grasping and in-hand manipulation. New advances in flexible, event-driven, electronic skins may soon endow robots with touch perception capabilities similar to humans. These electronic skins respond asynchronously to changes (e.g., in pressure, temperature), and can be laid out irregularly on the robot's body or end-effector. However, these unique features may render current deep learning approaches such as convolutional feature extractors unsuitable for tactile learning. In this paper, we propose a novel spiking graph neural network for event-based tactile object recognition. To make use of local connectivity of taxels, we present several methods for organizing the tactile data in a graph structure. Based on the constructed graphs, we develop a spiking graph convolutional network. The event-driven nature of spiking neural network makes it arguably more suitable for processing the event-based data. Experimental results on two tactile datasets show that the proposed method outperforms other state-of-the-art spiking methods, achieving high accuracies of approximately 90\% when classifying a variety of different household objects.



### Land Cover Classification from Remote Sensing Images Based on Multi-Scale Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2008.00168v2
- **DOI**: 10.1080/10095020.2021.2017237
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00168v2)
- **Published**: 2020-08-01 04:31:11+00:00
- **Updated**: 2020-10-21 11:10:31+00:00
- **Authors**: Rui Li, Shunyi Zheng, Chenxi Duan, Ce Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with multi-scale convolutional kernel is proposed to exploit discriminative representations from two-dimensional (2D) satellite images.



### State-of-The-Art Fuzzy Active Contour Models for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00175v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00175v1)
- **Published**: 2020-08-01 05:42:37+00:00
- **Updated**: 2020-08-01 05:42:37+00:00
- **Authors**: Ajoy Mondal, Kuntal Ghosh
- **Comment**: None
- **Journal**: Soft Computing, 1-17 (2020)
- **Summary**: Image segmentation is the initial step for every image analysis task. A large variety of segmentation algorithm has been proposed in the literature during several decades with some mixed success. Among them, the fuzzy energy based active contour models get attention to the researchers during last decade which results in development of various methods. A good segmentation algorithm should perform well in a large number of images containing noise, blur, low contrast, region in-homogeneity, etc. However, the performances of the most of the existing fuzzy energy based active contour models have been evaluated typically on the limited number of images. In this article, our aim is to review the existing fuzzy active contour models from the theoretical point of view and also evaluate them experimentally on a large set of images under the various conditions. The analysis under a large variety of images provides objective insight into the strengths and weaknesses of various fuzzy active contour models. Finally, we discuss several issues and future research direction on this particular topic.



### Contrastive Explanations in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.00178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00178v1)
- **Published**: 2020-08-01 05:50:01+00:00
- **Updated**: 2020-08-01 05:50:01+00:00
- **Authors**: Mohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: Visual explanations are logical arguments based on visual features that justify the predictions made by neural networks. Current modes of visual explanations answer questions of the form $`Why \text{ } P?'$. These $Why$ questions operate under broad contexts thereby providing answers that are irrelevant in some cases. We propose to constrain these $Why$ questions based on some context $Q$ so that our explanations answer contrastive questions of the form $`Why \text{ } P, \text{} rather \text{ } than \text{ } Q?'$. In this paper, we formalize the structure of contrastive visual explanations for neural networks. We define contrast based on neural networks and propose a methodology to extract defined contrasts. We then use the extracted contrasts as a plug-in on top of existing $`Why \text{ } P?'$ techniques, specifically Grad-CAM. We demonstrate their value in analyzing both networks and data in applications of large-scale recognition, fine-grained recognition, subsurface seismic analysis, and image quality assessment.



### Augmented Skeleton Based Contrastive Action Learning with Momentum LSTM for Unsupervised Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.00188v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00188v4)
- **Published**: 2020-08-01 06:37:57+00:00
- **Updated**: 2021-04-02 08:14:45+00:00
- **Authors**: Haocong Rao, Shihao Xu, Xiping Hu, Jun Cheng, Bin Hu
- **Comment**: Accepted by Information Sciences. Our codes are available at
  https://github.com/Mikexu007/AS-CAL
- **Journal**: None
- **Summary**: Action recognition via 3D skeleton data is an emerging important topic in these years. Most existing methods either extract hand-crafted descriptors or learn action representations by supervised learning paradigms that require massive labeled data. In this paper, we for the first time propose a contrastive action learning paradigm named AS-CAL that can leverage different augmentations of unlabeled skeleton data to learn action representations in an unsupervised manner. Specifically, we first propose to contrast similarity between augmented instances (query and key) of the input skeleton sequence, which are transformed by multiple novel augmentation strategies, to learn inherent action patterns ("pattern-invariance") of different skeleton transformations. Second, to encourage learning the pattern-invariance with more consistent action representations, we propose a momentum LSTM, which is implemented as the momentum-based moving average of LSTM based query encoder, to encode long-term action dynamics of the key sequence. Third, we introduce a queue to store the encoded keys, which allows our model to flexibly reuse proceeding keys and build a more consistent dictionary to improve contrastive learning. Last, by temporally averaging the hidden states of action learned by the query encoder, a novel representation named Contrastive Action Encoding (CAE) is proposed to represent human's action effectively. Extensive experiments show that our approach typically improves existing hand-crafted methods by 10-50% top-1 accuracy, and it can achieve comparable or even superior performance to numerous supervised learning methods.



### PanoNet: Real-time Panoptic Segmentation through Position-Sensitive Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/2008.00192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00192v1)
- **Published**: 2020-08-01 06:58:35+00:00
- **Updated**: 2020-08-01 06:58:35+00:00
- **Authors**: Xia Chen, Jianren Wang, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple, fast, and flexible framework to generate simultaneously semantic and instance masks for panoptic segmentation. Our method, called PanoNet, incorporates a clean and natural structure design that tackles the problem purely as a segmentation task without the time-consuming detection process. We also introduce position-sensitive embedding for instance grouping by accounting for both object's appearance and its spatial location. Overall, PanoNet yields high panoptic quality results of high-resolution Cityscapes images in real-time, significantly faster than all other methods with comparable performance. Our approach well satisfies the practical speed and memory requirement for many applications like autonomous driving and augmented reality.



### Joint Generative Learning and Super-Resolution For Real-World Camera-Screen Degradation
- **Arxiv ID**: http://arxiv.org/abs/2008.00195v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00195v3)
- **Published**: 2020-08-01 07:10:13+00:00
- **Updated**: 2020-09-14 09:22:34+00:00
- **Authors**: Guanghao Yin, Shouqian Sun, Chao Li, Xin Min
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world single image super-resolution (SISR) task, the low-resolution image suffers more complicated degradations, not only downsampled by unknown kernels. However, existing SISR methods are generally studied with the synthetic low-resolution generation such as bicubic interpolation (BI), which greatly limits their performance. Recently, some researchers investigate real-world SISR from the perspective of the camera and smartphone. However, except the acquisition equipment, the display device also involves more complicated degradations. In this paper, we focus on the camera-screen degradation and build a real-world dataset (Cam-ScreenSR), where HR images are original ground truths from the previous DIV2K dataset and corresponding LR images are camera-captured versions of HRs displayed on the screen. We conduct extensive experiments to demonstrate that involving more real degradations is positive to improve the generalization of SISR models. Moreover, we propose a joint two-stage model. Firstly, the downsampling degradation GAN(DD-GAN) is trained to model the degradation and produces more various of LR images, which is validated to be efficient for data augmentation. Then the dual residual channel attention network (DuRCAN) learns to recover the SR image. The weighted combination of L1 loss and proposed Laplacian loss are applied to sharpen the high-frequency edges. Extensive experimental results in both typical synthetic and complicated real-world degradations validate the proposed method outperforms than existing SOTA models with less parameters, faster speed and better visual results. Moreover, in real captured photographs, our model also delivers best visual quality with sharper edge, less artifacts, especially appropriate color enhancement, which has not been accomplished by previous methods.



### HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.00206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00206v2)
- **Published**: 2020-08-01 07:53:27+00:00
- **Updated**: 2020-08-10 11:55:12+00:00
- **Authors**: Jiefeng Li, Can Wang, Wentao Liu, Chen Qian, Cewu Lu
- **Comment**: To appear on ECCV2020
- **Journal**: None
- **Summary**: Remarkable progress has been made in 3D human pose estimation from a monocular RGB camera. However, only a few studies explored 3D multi-person cases. In this paper, we attempt to address the lack of a global perspective of the top-down approaches by introducing a novel form of supervision - Hierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes interaction information as the ordinal relations of depths and angles hierarchically, which captures the body-part and joint level semantic and maintains global consistency at the same time. In our approach, an integrated top-down model is designed to leverage these ordinal relations in the learning process. The integrated model estimates human bounding boxes, human depths, and root-relative 3D poses simultaneously, with a coarse-to-fine architecture to improve the accuracy of depth estimation. The proposed method significantly outperforms state-of-the-art methods on publicly available multi-person 3D pose datasets. In addition to superior performance, our method costs lower computation complexity and fewer model parameters.



### Efficient Adversarial Attacks for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.00217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00217v1)
- **Published**: 2020-08-01 08:47:58+00:00
- **Updated**: 2020-08-01 08:47:58+00:00
- **Authors**: Siyuan Liang, Xingxing Wei, Siyuan Yao, Xiaochun Cao
- **Comment**: None
- **Journal**: eccv 2020
- **Summary**: Visual object tracking is an important task that requires the tracker to find the objects quickly and accurately. The existing state-ofthe-art object trackers, i.e., Siamese based trackers, use DNNs to attain high accuracy. However, the robustness of visual tracking models is seldom explored. In this paper, we analyze the weakness of object trackers based on the Siamese network and then extend adversarial examples to visual object tracking. We present an end-to-end network FAN (Fast Attack Network) that uses a novel drift loss combined with the embedded feature loss to attack the Siamese network based trackers. Under a single GPU, FAN is efficient in the training speed and has a strong attack performance. The FAN can generate an adversarial example at 10ms, achieve effective targeted attack (at least 40% drop rate on OTB) and untargeted attack (at least 70% drop rate on OTB).



### Unsupervised Deep Cross-modality Spectral Hashing
- **Arxiv ID**: http://arxiv.org/abs/2008.00223v3
- **DOI**: 10.1109/TIP.2020.3014727
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00223v3)
- **Published**: 2020-08-01 09:20:11+00:00
- **Updated**: 2020-08-18 09:23:55+00:00
- **Authors**: Tuan Hoang, Thanh-Toan Do, Tam V. Nguyen, Ngai-Man Cheung
- **Comment**: Accepted to IEEE Transaction on Image Processing (TIP) Add
  Acknowledgement
- **Journal**: None
- **Summary**: This paper presents a novel framework, namely Deep Cross-modality Spectral Hashing (DCSH), to tackle the unsupervised learning problem of binary hash codes for efficient cross-modal retrieval. The framework is a two-step hashing approach which decouples the optimization into (1) binary optimization and (2) hashing function learning. In the first step, we propose a novel spectral embedding-based algorithm to simultaneously learn single-modality and binary cross-modality representations. While the former is capable of well preserving the local structure of each modality, the latter reveals the hidden patterns from all modalities. In the second step, to learn mapping functions from informative data inputs (images and word embeddings) to binary codes obtained from the first step, we leverage the powerful CNN for images and propose a CNN-based deep architecture to learn text modality. Quantitative evaluations on three standard benchmark datasets demonstrate that the proposed DCSH method consistently outperforms other state-of-the-art methods.



### Regularization by Denoising via Fixed-Point Projection (RED-PRO)
- **Arxiv ID**: http://arxiv.org/abs/2008.00226v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00226v2)
- **Published**: 2020-08-01 09:35:22+00:00
- **Updated**: 2020-10-28 20:22:33+00:00
- **Authors**: Regev Cohen, Michael Elad, Peyman Milanfar
- **Comment**: 33 Pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: Inverse problems in image processing are typically cast as optimization tasks, consisting of data-fidelity and stabilizing regularization terms. A recent regularization strategy of great interest utilizes the power of denoising engines. Two such methods are the Plug-and-Play Prior (PnP) and Regularization by Denoising (RED). While both have shown state-of-the-art results in various recovery tasks, their theoretical justification is incomplete. In this paper, we aim to bridge between RED and PnP, enriching the understanding of both frameworks. Towards that end, we reformulate RED as a convex optimization problem utilizing a projection (RED-PRO) onto the fixed-point set of demicontractive denoisers. We offer a simple iterative solution to this problem, by which we show that PnP proximal gradient method is a special case of RED-PRO, while providing guarantees for the convergence of both frameworks to globally optimal solutions. In addition, we present relaxations of RED-PRO that allow for handling denoisers with limited fixed-point sets. Finally, we demonstrate RED-PRO for the tasks of image deblurring and super-resolution, showing improved results with respect to the original RED framework.



### RGB-D Salient Object Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2008.00230v4
- **DOI**: 10.1007/s41095-020-0199-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00230v4)
- **Published**: 2020-08-01 10:01:32+00:00
- **Updated**: 2022-07-14 11:47:17+00:00
- **Authors**: Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, Ling Shao
- **Comment**: 25 pages, 14 figures
- **Journal**: Computational Visual Media, 2021, Vol. 7, No. 1, Pages: 37-69
- **Summary**: Salient object detection (SOD), which simulates the human visual perception system to locate the most attractive object(s) in a scene, has been widely applied to various computer vision tasks. Now, with the advent of depth sensors, depth maps with affluent spatial information that can be beneficial in boosting the performance of SOD, can easily be captured. Although various RGB-D based SOD models with promising performance have been proposed over the past several years, an in-depth understanding of these models and challenges in this topic remains lacking. In this paper, we provide a comprehensive survey of RGB-D based SOD models from various perspectives, and review related benchmark datasets in detail. Further, considering that the light field can also provide depth maps, we review SOD models and popular benchmark datasets from this domain as well. Moreover, to investigate the SOD ability of existing models, we carry out a comprehensive evaluation, as well as attribute-based evaluation of several representative RGB-D based SOD models. Finally, we discuss several challenges and open directions of RGB-D based SOD for future research. All collected models, benchmark datasets, source code links, datasets constructed for attribute-based evaluation, and codes for evaluation will be made publicly available at https://github.com/taozh2017/RGBDSODsurvey



### An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTscan Imagery
- **Arxiv ID**: http://arxiv.org/abs/2008.00238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00238v1)
- **Published**: 2020-08-01 10:44:03+00:00
- **Updated**: 2020-08-01 10:44:03+00:00
- **Authors**: Pavan Rajkumar Magesh, Richard Delwin Myloth, Rijo Jackson Tom
- **Comment**: None
- **Journal**: None
- **Summary**: Parkinson's disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTscan. In this study, we propose a machine learning model that accurately classifies any given DaTscan as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This is kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTscans were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a specificity of 90.9%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTscans. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease.



### Exploring Multi-Scale Feature Propagation and Communication for Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.00239v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00239v2)
- **Published**: 2020-08-01 10:44:06+00:00
- **Updated**: 2020-08-14 08:03:17+00:00
- **Authors**: Ruicheng Feng, Weipeng Guan, Yu Qiao, Chao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-scale techniques have achieved great success in a wide range of computer vision tasks. However, while this technique is incorporated in existing works, there still lacks a comprehensive investigation on variants of multi-scale convolution in image super resolution. In this work, we present a unified formulation over widely-used multi-scale structures. With this framework, we systematically explore the two factors of multi-scale convolution -- feature propagation and cross-scale communication. Based on the investigation, we propose a generic and efficient multi-scale convolution unit -- Multi-Scale cross-Scale Share-weights convolution (MS$^3$-Conv). Extensive experiments demonstrate that the proposed MS$^3$-Conv can achieve better SR performance than the standard convolution with less parameters and computational cost. Beyond quantitative analysis, we comprehensively study the visual quality, which shows that MS$^3$-Conv behave better to recover high-frequency details.



### Meta-DRN: Meta-Learning for 1-Shot Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00247v1)
- **Published**: 2020-08-01 11:23:37+00:00
- **Updated**: 2020-08-01 11:23:37+00:00
- **Authors**: Atmadeep Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning models have revolutionized the field of computer vision. But, a significant drawback of most of these models is that they require a large number of labelled examples to generalize properly. Recent developments in few-shot learning aim to alleviate this requirement. In this paper, we propose a novel lightweight CNN architecture for 1-shot image segmentation. The proposed model is created by taking inspiration from well-performing architectures for semantic segmentation and adapting it to the 1-shot domain. We train our model using 4 meta-learning algorithms that have worked well for image classification and compare the results. For the chosen dataset, our proposed model has a 70% lower parameter count than the benchmark, while having better or comparable mean IoU scores using all 4 of the meta-learning algorithms.



### Distilling Visual Priors from Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.00261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00261v1)
- **Published**: 2020-08-01 13:07:18+00:00
- **Updated**: 2020-08-01 13:07:18+00:00
- **Authors**: Bingchen Zhao, Xin Wen
- **Comment**: This is the 2nd place tech report for VIPriors Image Classification
  Challenge ECCVW2020
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are prone to overfit small training datasets. We present a novel two-phase pipeline that leverages self-supervised learning and knowledge distillation to improve the generalization ability of CNN models for image classification under the data-deficient setting. The first phase is to learn a teacher model which possesses rich and generalizable visual representations via self-supervised learning, and the second phase is to distill the representations into a student model in a self-distillation manner, and meanwhile fine-tune the student model for the image classification task. We also propose a novel margin loss for the self-supervised contrastive learning proxy task to better learn the representation under the data-deficient scenario. Together with other tricks, we achieve competitive performance in the VIPriors image classification challenge.



### From Shadow Segmentation to Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2008.00267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00267v1)
- **Published**: 2020-08-01 14:00:10+00:00
- **Updated**: 2020-08-01 14:00:10+00:00
- **Authors**: Hieu Le, Dimitris Samaras
- **Comment**: Accepted at ECCV 2020. All code, trained models, and data are
  available (soon) at: https://www3.cs.stonybrook.
  edu/~cvl/projects/FSS2SR/index.html
- **Journal**: None
- **Summary**: The requirement for paired shadow and shadow-free images limits the size and diversity of shadow removal datasets and hinders the possibility of training large-scale, robust shadow removal algorithms. We propose a shadow removal method that can be trained using only shadow and non-shadow patches cropped from the shadow images themselves. Our method is trained via an adversarial framework, following a physical model of shadow formation. Our central contribution is a set of physics-based constraints that enables this adversarial training. Our method achieves competitive shadow removal results compared to state-of-the-art methods that are trained with fully paired shadow and shadow-free images. The advantages of our training regime are even more pronounced in shadow removal for videos. Our method can be fine-tuned on a testing video with only the shadow masks generated by a pre-trained shadow detector and outperforms state-of-the-art methods on this challenging test. We illustrate the advantages of our method on our proposed video shadow removal dataset.



### Little Motion, Big Results: Using Motion Magnification to Reveal Subtle Tremors in Infants
- **Arxiv ID**: http://arxiv.org/abs/2008.04946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.04946v1)
- **Published**: 2020-08-01 15:35:55+00:00
- **Updated**: 2020-08-01 15:35:55+00:00
- **Authors**: Girik Malik, Ish K. Gulati
- **Comment**: None
- **Journal**: Proceedings of the First International AAI4H - Advances in
  Artificial Intelligence for Healthcare Workshop co-located with the 24th
  European Conference on Artificial Intelligence (ECAI 2020)
- **Summary**: Detecting tremors is challenging for both humans and machines. Infants exposed to opioids during pregnancy often show signs and symptoms of withdrawal after birth, which are easy to miss with the human eye. The constellation of clinical features, termed as Neonatal Abstinence Syndrome (NAS), include tremors, seizures, irritability, etc. The current standard of care uses Finnegan Neonatal Abstinence Syndrome Scoring System (FNASS), based on subjective evaluations. Monitoring with FNASS requires highly skilled nursing staff, making continuous monitoring difficult. In this paper we propose an automated tremor detection system using amplified motion signals. We demonstrate its applicability on bedside video of infant exhibiting signs of NAS. Further, we test different modes of deep convolutional network based motion magnification, and identify that dynamic mode works best in the clinical setting, being invariant to common orientational changes. We propose a strategy for discharge and follow up for NAS patients, using motion magnification to supplement the existing protocols. Overall our study suggests methods for bridging the gap in current practices, training and resource utilization.



### Eigen-CAM: Class Activation Map using Principal Components
- **Arxiv ID**: http://arxiv.org/abs/2008.00299v1
- **DOI**: 10.1109/IJCNN48605.2020.9206626
- **Categories**: **cs.CV**, cs.LG, 68T07 (Primary), 68T45 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2008.00299v1)
- **Published**: 2020-08-01 17:14:13+00:00
- **Updated**: 2020-08-01 17:14:13+00:00
- **Authors**: Mohammed Bany Muhammad, Mohammed Yeasin
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural networks are ubiquitous due to the ease of developing models and their influence on other domains. At the heart of this progress is convolutional neural networks (CNNs) that are capable of learning representations or features given a set of data. Making sense of such complex models (i.e., millions of parameters and hundreds of layers) remains challenging for developers as well as the end-users. This is partially due to the lack of tools or interfaces capable of providing interpretability and transparency. A growing body of literature, for example, class activation map (CAM), focuses on making sense of what a model learns from the data or why it behaves poorly in a given task. This paper builds on previous ideas to cope with the increasing demand for interpretable, robust, and transparent models. Our approach provides a simpler and intuitive (or familiar) way of generating CAM. The proposed Eigen-CAM computes and visualizes the principle components of the learned features/representations from the convolutional layers. Empirical studies were performed to compare the Eigen-CAM with the state-of-the-art methods (such as Grad-CAM, Grad-CAM++, CNN-fixations) by evaluating on benchmark datasets such as weakly-supervised localization and localizing objects in the presence of adversarial noise. Eigen-CAM was found to be robust against classification errors made by fully connected layers in CNNs, does not rely on the backpropagation of gradients, class relevance score, maximum activation locations, or any other form of weighting features. In addition, it works with all CNN models without the need to modify layers or retrain models. Empirical results show up to 12% improvement over the best method among the methods compared on weakly supervised object localization.



### Accurate and Efficient Intracranial Hemorrhage Detection and Subtype Classification in 3D CT Scans with Convolutional and Long Short-Term Memory Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.00302v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00302v3)
- **Published**: 2020-08-01 17:28:25+00:00
- **Updated**: 2020-09-29 14:55:07+00:00
- **Authors**: Mihail Burduja, Radu Tudor Ionescu, Nicolae Verga
- **Comment**: Accepted at Sensors
- **Journal**: None
- **Summary**: In this paper, we present our system for the RSNA Intracranial Hemorrhage Detection challenge. The proposed system is based on a lightweight deep neural network architecture composed of a convolutional neural network (CNN) that takes as input individual CT slices, and a Long Short-Term Memory (LSTM) network that takes as input feature embeddings provided by the CNN. For efficient processing, we consider various feature selection methods to produce a subset of useful CNN features for the LSTM. Furthermore, we reduce the CT slices by a factor of 2x, allowing ourselves to train the model faster. Even if our model is designed to balance speed and accuracy, we report a weighted mean log loss of 0.04989 on the final test set, which places us in the top 30 ranking (2%) from a total of 1345 participants. Although our computing infrastructure does not allow it, processing CT slices at their original scale is likely to improve performance. In order to enable others to reproduce our results, we provide our code as open source at https://github.com/warchildmd/ihd. After the challenge, we conducted a subjective intracranial hemorrhage detection assessment by radiologists, indicating that the performance of our deep model is on par with that of doctors specialized in reading CT scans. Another contribution of our work is to integrate Grad-CAM visualizations in our system, providing useful explanations for its predictions. We therefore consider our system as a viable option when a fast diagnosis or a second opinion on intracranial hemorrhage detection are needed.



### Self-supervised Learning of Point Clouds via Orientation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.00305v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00305v2)
- **Published**: 2020-08-01 17:49:45+00:00
- **Updated**: 2020-10-18 01:46:11+00:00
- **Authors**: Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir G. Kim
- **Comment**: 3DV 2020
- **Journal**: None
- **Summary**: Point clouds provide a compact and efficient representation of 3D shapes. While deep neural networks have achieved impressive results on point cloud learning tasks, they require massive amounts of manually labeled data, which can be costly and time-consuming to collect. In this paper, we leverage 3D self-supervision for learning downstream tasks on point clouds with fewer labels. A point cloud can be rotated in infinitely many ways, which provides a rich label-free source for self-supervision. We consider the auxiliary task of predicting rotations that in turn leads to useful features for other tasks such as shape classification and 3D keypoint prediction. Using experiments on ShapeNet and ModelNet, we demonstrate that our approach outperforms the state-of-the-art. Moreover, features learned by our model are complementary to other self-supervised methods and combining them leads to further performance improvement.



### Improving Skeleton-based Action Recognitionwith Robust Spatial and Temporal Features
- **Arxiv ID**: http://arxiv.org/abs/2008.00324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00324v1)
- **Published**: 2020-08-01 19:29:53+00:00
- **Updated**: 2020-08-01 19:29:53+00:00
- **Authors**: Zeshi Yang, Kangkang Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently skeleton-based action recognition has made signif-icant progresses in the computer vision community. Most state-of-the-art algorithms are based on Graph Convolutional Networks (GCN), andtarget at improving the network structure of the backbone GCN lay-ers. In this paper, we propose a novel mechanism to learn more robustdiscriminative features in space and time. More specifically, we add aDiscriminative Feature Learning (DFL) branch to the last layers of thenetwork to extract discriminative spatial and temporal features to helpregularize the learning. We also formally advocate the use of Direction-Invariant Features (DIF) as input to the neural networks. We show thataction recognition accuracy can be improved when these robust featuresare learned and used. We compare our results with those of ST-GCNand related methods on four datasets: NTU-RGBD60, NTU-RGBD120,SYSU 3DHOI and Skeleton-Kinetics.



### PERCH 2.0 : Fast and Accurate GPU-based Perception via Search for Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.00326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.00326v1)
- **Published**: 2020-08-01 19:42:56+00:00
- **Updated**: 2020-08-01 19:42:56+00:00
- **Authors**: Aditya Agarwal, Yupeng Han, Maxim Likhachev
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation of known objects is fundamental to tasks such as robotic grasping and manipulation. The need for reliable grasping imposes stringent accuracy requirements on pose estimation in cluttered, occluded scenes in dynamic environments. Modern methods employ large sets of training data to learn features in order to find correspondence between 3D models and observed data. However these methods require extensive annotation of ground truth poses. An alternative is to use algorithms that search for the best explanation of the observed scene in a space of possible rendered scenes. A recently developed algorithm, PERCH (PErception Via SeaRCH) does so by using depth data to converge to a globally optimum solution using a search over a specially constructed tree. While PERCH offers strong guarantees on accuracy, the current formulation suffers from low scalability owing to its high runtime. In addition, the sole reliance on depth data for pose estimation restricts the algorithm to scenes where no two objects have the same shape. In this work, we propose PERCH 2.0, a novel perception via search strategy that takes advantage of GPU acceleration and RGB data. We show that our approach can achieve a speedup of 100x over PERCH, as well as better accuracy than the state-of-the-art data-driven approaches on 6-DoF pose estimation without the need for annotating ground truth poses in the training data. Our code and video are available at https://sbpl-cruz.github.io/perception/.



### Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.00334v1
- **DOI**: 10.1145/3394171.3413827
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00334v1)
- **Published**: 2020-08-01 20:21:48+00:00
- **Updated**: 2020-08-01 20:21:48+00:00
- **Authors**: Wentao Bao, Qi Yu, Yu Kong
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Traffic accident anticipation aims to predict accidents from dashcam videos as early as possible, which is critical to safety-guaranteed self-driving systems. With cluttered traffic scenes and limited visual cues, it is of great challenge to predict how long there will be an accident from early observed frames. Most existing approaches are developed to learn features of accident-relevant agents for accident anticipation, while ignoring the features of their spatial and temporal relations. Besides, current deterministic deep neural networks could be overconfident in false predictions, leading to high risk of traffic accidents caused by self-driving systems. In this paper, we propose an uncertainty-based accident anticipation model with spatio-temporal relational learning. It sequentially predicts the probability of traffic accident occurrence with dashcam videos. Specifically, we propose to take advantage of graph convolution and recurrent networks for relational feature learning, and leverage Bayesian neural networks to address the intrinsic variability of latent relational representations. The derived uncertainty-based ranking loss is found to significantly boost model performance by improving the quality of relational features. In addition, we collect a new Car Crash Dataset (CCD) for traffic accident anticipation which contains environmental attributes and accident reasons annotations. Experimental results on both public and the newly-compiled datasets show state-of-the-art performance of our model. Our code and CCD dataset are available at https://github.com/Cogito2012/UString.



### Self-supervised Visual Attribute Learning for Fashion Compatibility
- **Arxiv ID**: http://arxiv.org/abs/2008.00348v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00348v2)
- **Published**: 2020-08-01 21:53:22+00:00
- **Updated**: 2021-08-12 01:22:33+00:00
- **Authors**: Donghyun Kim, Kuniaki Saito, Samarth Mishra, Stan Sclaroff, Kate Saenko, Bryan A Plummer
- **Comment**: Accepted to VIPriors Workshop ICCV 2021
- **Journal**: None
- **Summary**: Many self-supervised learning (SSL) methods have been successful in learning semantically meaningful visual representations by solving pretext tasks. However, prior work in SSL focuses on tasks like object recognition or detection, which aim to learn object shapes and assume that the features should be invariant to concepts like colors and textures. Thus, these SSL methods perform poorly on downstream tasks where these concepts provide critical information. In this paper, we present an SSL framework that enables us to learn color and texture-aware features without requiring any labels during training. Our approach consists of three self-supervised tasks designed to capture different concepts that are neglected in prior work that we can select from depending on the needs of our downstream tasks. Our tasks include learning to predict color histograms and discriminate shapeless local patches and textures from each instance. We evaluate our approach on fashion compatibility using Polyvore Outfits and In-Shop Clothing Retrieval using Deepfashion, improving upon prior SSL methods by 9.5-16%, and even outperforming some supervised approaches on Polyvore Outfits despite using no labels. We also show that our approach can be used for transfer learning, demonstrating that we can train on one dataset while achieving high performance on a different dataset.



### Animating Through Warping: an Efficient Method for High-Quality Facial Expression Animation
- **Arxiv ID**: http://arxiv.org/abs/2008.00362v1
- **DOI**: 10.1145/3394171.3413926
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.3.3; J.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.00362v1)
- **Published**: 2020-08-01 23:52:33+00:00
- **Updated**: 2020-08-01 23:52:33+00:00
- **Authors**: Zili Yi, Qiang Tang, Vishnu Sanjay Ramiya Srinivasan, Zhan Xu
- **Comment**: 18 pages, 13 figures, Accepted to ACM Multimedia 2020
- **Journal**: None
- **Summary**: Advances in deep neural networks have considerably improved the art of animating a still image without operating in 3D domain. Whereas, prior arts can only animate small images (typically no larger than 512x512) due to memory limitations, difficulty of training and lack of high-resolution (HD) training datasets, which significantly reduce their potential for applications in movie production and interactive systems. Motivated by the idea that HD images can be generated by adding high-frequency residuals to low-resolution results produced by a neural network, we propose a novel framework known as Animating Through Warping (ATW) to enable efficient animation of HD images.   Specifically, the proposed framework consists of two modules, a novel two-stage neural-network generator and a novel post-processing module known as Animating Through Warping (ATW). It only requires the generator to be trained on small images and can do inference on an image of any size. During inference, an HD input image is decomposed into a low-resolution component(128x128) and its corresponding high-frequency residuals. The generator predicts the low-resolution result as well as the motion field that warps the input face to the desired status (e.g., expressions categories or action units). Finally, the ResWarp module warps the residuals based on the motion field and adding the warped residuals to generates the final HD results from the naively up-sampled low-resolution results. Experiments show the effectiveness and efficiency of our method in generating high-resolution animations. Our proposed framework successfully animates a 4K facial image, which has never been achieved by prior neural models. In addition, our method generally guarantee the temporal coherency of the generated animations. Source codes will be made publicly available.



