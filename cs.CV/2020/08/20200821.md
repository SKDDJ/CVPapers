# Arxiv Papers in cs.CV on 2020-08-21
### Learning Affordance Landscapes for Interaction Exploration in 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2008.09241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09241v2)
- **Published**: 2020-08-21 00:29:36+00:00
- **Updated**: 2020-10-19 02:44:08+00:00
- **Authors**: Tushar Nagarajan, Kristen Grauman
- **Comment**: To be published in NeurIPS 2020
- **Journal**: None
- **Summary**: Embodied agents operating in human spaces must be able to master how their environment works: what objects can the agent use, and how can it use them? We introduce a reinforcement learning approach for exploration for interaction, whereby an embodied agent autonomously discovers the affordance landscape of a new unmapped 3D environment (such as an unfamiliar kitchen). Given an egocentric RGB-D camera and a high-level action space, the agent is rewarded for maximizing successful interactions while simultaneously training an image-based affordance segmentation model. The former yields a policy for acting efficiently in new environments to prepare for downstream interaction tasks, while the latter yields a convolutional neural network that maps image regions to the likelihood they permit each action, densifying the rewards for exploration. We demonstrate our idea with AI2-iTHOR. The results show agents can learn how to use new home environments intelligently and that it prepares them to rapidly address various downstream tasks like "find a knife and put it in the drawer." Project page: http://vision.cs.utexas.edu/projects/interaction-exploration/



### Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid
- **Arxiv ID**: http://arxiv.org/abs/2008.09269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09269v2)
- **Published**: 2020-08-21 02:22:06+00:00
- **Updated**: 2023-04-03 23:03:54+00:00
- **Authors**: Jun Gao, Zian Wang, Jinchen Xuan, Sanja Fidler
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In modern computer vision, images are typically represented as a fixed uniform grid with some stride and processed via a deep convolutional neural network. We argue that deforming the grid to better align with the high-frequency image content is a more effective strategy. We introduce \emph{Deformable Grid} DefGrid, a learnable neural network module that predicts location offsets of vertices of a 2-dimensional triangular grid, such that the edges of the deformed grid align with image boundaries. We showcase our DefGrid in a variety of use cases, i.e., by inserting it as a module at various levels of processing. We utilize DefGrid as an end-to-end \emph{learnable geometric downsampling} layer that replaces standard pooling methods for reducing feature resolution when feeding images into a deep CNN. We show significantly improved results at the same grid resolution compared to using CNNs on uniform grids for the task of semantic segmentation. We also utilize DefGrid at the output layers for the task of object mask annotation, and show that reasoning about object boundaries on our predicted polygonal grid leads to more accurate results over existing pixel-wise and curve-based approaches. We finally showcase DefGrid as a standalone module for unsupervised image partitioning, showing superior performance over existing approaches. Project website: http://www.cs.toronto.edu/~jungao/def-grid



### Occupancy Anticipation for Efficient Exploration and Navigation
- **Arxiv ID**: http://arxiv.org/abs/2008.09285v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09285v2)
- **Published**: 2020-08-21 03:16:51+00:00
- **Updated**: 2020-08-25 16:36:11+00:00
- **Authors**: Santhosh K. Ramakrishnan, Ziad Al-Halah, Kristen Grauman
- **Comment**: Accepted in ECCV 2020. 19 pages, 6 figures, appendix at end
- **Journal**: None
- **Summary**: State-of-the-art navigation methods leverage a spatial memory to generalize to new environments, but their occupancy maps are limited to capturing the geometric structures directly observed by the agent. We propose occupancy anticipation, where the agent uses its egocentric RGB-D observations to infer the occupancy state beyond the visible regions. In doing so, the agent builds its spatial awareness more rapidly, which facilitates efficient exploration and navigation in 3D environments. By exploiting context in both the egocentric views and top-down maps our model successfully anticipates a broader map of the environment, with performance significantly better than strong baselines. Furthermore, when deployed for the sequential decision-making tasks of exploration and navigation, our model outperforms state-of-the-art methods on the Gibson and Matterport3D datasets. Our approach is the winning entry in the 2020 Habitat PointNav Challenge. Project page: http://vision.cs.utexas.edu/projects/occupancy_anticipation/



### Automating the assessment of biofouling in images using expert agreement as a gold standard
- **Arxiv ID**: http://arxiv.org/abs/2008.09289v2
- **DOI**: 10.1038/s41598-021-81011-2
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.09289v2)
- **Published**: 2020-08-21 03:30:45+00:00
- **Updated**: 2020-12-17 03:59:09+00:00
- **Authors**: Nathaniel J. Bloomfield, Susan Wei, Bartholomew Woodham, Peter Wilkinson, Andrew Robinson
- **Comment**: 12 pages
- **Journal**: Sci Rep 11, 2739 (2021)
- **Summary**: Biofouling is the accumulation of organisms on surfaces immersed in water. It is of particular concern to the international shipping industry because it increases fuel costs and presents a biosecurity risk by providing a pathway for non-indigenous marine species to establish in new areas. There is growing interest within jurisdictions to strengthen biofouling risk-management regulations, but it is expensive to conduct in-water inspections and assess the collected data to determine the biofouling state of vessel hulls. Machine learning is well suited to tackle the latter challenge, and here we apply deep learning to automate the classification of images from in-water inspections to identify the presence and severity of fouling. We combined several datasets to obtain over 10,000 images collected from in-water surveys which were annotated by a group biofouling experts. We compared the annotations from three experts on a 120-sample subset of these images, and found that they showed 89% agreement (95% CI: 87-92%). Subsequent labelling of the whole dataset by one of these experts achieved similar levels of agreement with this group of experts, which we defined as performing at most 5% worse (p=0.009-0.054). Using these expert labels, we were able to train a deep learning model that also agreed similarly with the group of experts (p=0.001-0.014), demonstrating that automated analysis of biofouling in images is feasible and effective using this method.



### Graph Neural Networks for UnsupervisedDomain Adaptation of Histopathological ImageAnalytics
- **Arxiv ID**: http://arxiv.org/abs/2008.09304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09304v1)
- **Published**: 2020-08-21 04:53:44+00:00
- **Updated**: 2020-08-21 04:53:44+00:00
- **Authors**: Dou Xu, Chang Cai, Chaowei Fang, Bin Kong, Jihua Zhu, Zhongyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating histopathological images is a time-consuming andlabor-intensive process, which requires broad-certificated pathologistscarefully examining large-scale whole-slide images from cells to tissues.Recent frontiers of transfer learning techniques have been widely investi-gated for image understanding tasks with limited annotations. However,when applied for the analytics of histology images, few of them can effec-tively avoid the performance degradation caused by the domain discrep-ancy between the source training dataset and the target dataset, suchas different tissues, staining appearances, and imaging devices. To thisend, we present a novel method for the unsupervised domain adaptationin histopathological image analysis, based on a backbone for embeddinginput images into a feature space, and a graph neural layer for propa-gating the supervision signals of images with labels. The graph model isset up by connecting every image with its close neighbors in the embed-ded feature space. Then graph neural network is employed to synthesizenew feature representation from every image. During the training stage,target samples with confident inferences are dynamically allocated withpseudo labels. The cross-entropy loss function is used to constrain thepredictions of source samples with manually marked labels and targetsamples with pseudo labels. Furthermore, the maximum mean diversityis adopted to facilitate the extraction of domain-invariant feature repre-sentations, and contrastive learning is exploited to enhance the categorydiscrimination of learned features. In experiments of the unsupervised do-main adaptation for histopathological image classification, our methodachieves state-of-the-art performance on four public datasets



### ATG-PVD: Ticketing Parking Violations on A Drone
- **Arxiv ID**: http://arxiv.org/abs/2008.09305v1
- **DOI**: 10.1007/978-3-030-66823-5_32
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.09305v1)
- **Published**: 2020-08-21 05:07:21+00:00
- **Updated**: 2020-08-21 05:07:21+00:00
- **Authors**: Hengli Wang, Yuxuan Liu, Huaiyang Huang, Yuheng Pan, Wenbin Yu, Jialin Jiang, Dianbin Lyu, Mohammud J. Bocus, Ming Liu, Ioannis Pitas, Rui Fan
- **Comment**: 17 pages, 11 figures and 3 tables. This paper is accepted by ECCV
  Workshops 2020
- **Journal**: None
- **Summary**: In this paper, we introduce a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, our proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by our proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by our investigation module after drone re-localization.



### Robustness and Overfitting Behavior of Implicit Background Models
- **Arxiv ID**: http://arxiv.org/abs/2008.09306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09306v1)
- **Published**: 2020-08-21 05:08:33+00:00
- **Updated**: 2020-08-21 05:08:33+00:00
- **Authors**: Shirley Liu, Charles Lehman, Ghassan AlRegib
- **Comment**: 6 pages, 3 figures, accepted to IEEE International Conference on
  Image Processing (ICIP)
- **Journal**: None
- **Summary**: In this paper, we examine the overfitting behavior of image classification models modified with Implicit Background Estimation (SCrIBE), which transforms them into weakly supervised segmentation models that provide spatial domain visualizations without affecting performance. Using the segmentation masks, we derive an overfit detection criterion that does not require testing labels. In addition, we assess the change in model performance, calibration, and segmentation masks after applying data augmentations as overfitting reduction measures and testing on various types of distorted images.



### InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2008.09309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09309v1)
- **Published**: 2020-08-21 05:15:58+00:00
- **Updated**: 2020-08-21 05:15:58+00:00
- **Authors**: Gyeongsik Moon, Shoou-i Yu, He Wen, Takaaki Shiratori, Kyoung Mu Lee
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: Analysis of hand-hand interactions is a crucial step towards better understanding human behavior. However, most researches in 3D hand pose estimation have focused on the isolated single hand case. Therefore, we firstly propose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image. The proposed InterHand2.6M consists of \textbf{2.6M labeled single and interacting hand frames} under various poses from multiple subjects. Our InterNet simultaneously performs 3D single and interacting hand pose estimation. In our experiments, we demonstrate big gains in 3D interacting hand pose estimation accuracy when leveraging the interacting hand data in InterHand2.6M. We also report the accuracy of InterNet on InterHand2.6M, which serves as a strong baseline for this new dataset. Finally, we show 3D interacting hand pose estimation results from general images. Our code and dataset are available at https://mks0601.github.io/InterHand2.6M/.



### Domain Adaptation of Learned Features for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.09310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09310v1)
- **Published**: 2020-08-21 05:17:32+00:00
- **Updated**: 2020-08-21 05:17:32+00:00
- **Authors**: Sungyong Baik, Hyo Jin Kim, Tianwei Shen, Eddy Ilg, Kyoung Mu Lee, Chris Sweeney
- **Comment**: BMVC 2020
- **Journal**: None
- **Summary**: We tackle the problem of visual localization under changing conditions, such as time of day, weather, and seasons. Recent learned local features based on deep neural networks have shown superior performance over classical hand-crafted local features. However, in a real-world scenario, there often exists a large domain gap between training and target images, which can significantly degrade the localization accuracy. While existing methods utilize a large amount of data to tackle the problem, we present a novel and practical approach, where only a few examples are needed to reduce the domain gap. In particular, we propose a few-shot domain adaptation framework for learned local features that deals with varying conditions in visual localization. The experimental results demonstrate the superior performance over baselines, while using a scarce number of training examples from the target domain.



### Line-Circle-Square (LCS): A Multilayered Geometric Filter for Edge-Based Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.09315v3
- **DOI**: 10.1016/j.robot.2021.103732
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09315v3)
- **Published**: 2020-08-21 05:28:12+00:00
- **Updated**: 2021-01-13 04:30:04+00:00
- **Authors**: Seyed Amir Tafrishi, Xiaotian Dai, Vahid Esmaeilzadeh Kandjani
- **Comment**: 17 pages, 19 figures, accepted at the Elsevier's Robotics and
  Autonomous Systems journal
- **Journal**: Journal: Robotics and Autonomous Systems, publisher: Elsevier,
  volume number: 137, year: 2021, page number: 103732
- **Summary**: This paper presents a state-of-the-art filter that reduces the complexity in object detection, tracking and mapping applications. Existing edge detection and tracking methods are proposed to create suitable autonomy for mobile robots, however, many of them face overconfidence and large computations at the entrance to scenarios with an immense number of landmarks. The method in this work, the Line-Circle-Square (LCS) filter, claims that mobile robots without a large database for object recognition and highly advanced prediction methods can deal with incoming objects that the camera captures in real-time. The proposed filter applies detection, tracking and learning to each defined expert to extract higher level information for judging scenes without over-calculation. The interactive learning feed between each expert increases the consistency of detected landmarks that works against overwhelming detected features in crowded scenes. Our experts are dependent on trust factors' covariance under the geometric definitions to ignore, emerge and compare detected landmarks. The experiment validates the effectiveness of the proposed filter in terms of detection precision and resource usage in both experimental and real-world scenarios.



### DTDN: Dual-task De-raining Network
- **Arxiv ID**: http://arxiv.org/abs/2008.09326v1
- **DOI**: 10.1145/3343031.3350945
- **Categories**: **eess.IV**, cs.CV, cs.MM, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2008.09326v1)
- **Published**: 2020-08-21 06:32:42+00:00
- **Updated**: 2020-08-21 06:32:42+00:00
- **Authors**: Zheng Wang, Jianwu Li, Ge Song
- **Comment**: None
- **Journal**: None
- **Summary**: Removing rain streaks from rainy images is necessary for many tasks in computer vision, such as object detection and recognition. It needs to address two mutually exclusive objectives: removing rain streaks and reserving realistic details. Balancing them is critical for de-raining methods. We propose an end-to-end network, called dual-task de-raining network (DTDN), consisting of two sub-networks: generative adversarial network (GAN) and convolutional neural network (CNN), to remove rain streaks via coordinating the two mutually exclusive objectives self-adaptively. DTDN-GAN is mainly used to remove structural rain streaks, and DTDN-CNN is designed to recover details in original images. We also design a training algorithm to train these two sub-networks of DTDN alternatively, which share same weights but use different training sets. We further enrich two existing datasets to approximate the distribution of real rain streaks. Experimental results show that our method outperforms several recent state-of-the-art methods, based on both benchmark testing datasets and real rainy images.



### Kronecker CP Decomposition with Fast Multiplication for Compressing RNNs
- **Arxiv ID**: http://arxiv.org/abs/2008.09342v2
- **DOI**: 10.1109/TNNLS.2021.3105961
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09342v2)
- **Published**: 2020-08-21 07:29:45+00:00
- **Updated**: 2021-09-24 12:19:16+00:00
- **Authors**: Dingheng Wang, Bijiao Wu, Guangshe Zhao, Man Yao, Hengnu Chen, Lei Deng, Tianyi Yan, Guoqi Li
- **Comment**: Accepted by TNNLS
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) are powerful in the tasks oriented to sequential data, such as natural language processing and video recognition. However, since the modern RNNs, including long-short term memory (LSTM) and gated recurrent unit (GRU) networks, have complex topologies and expensive space/computation complexity, compressing them becomes a hot and promising topic in recent years. Among plenty of compression methods, tensor decomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and hierarchical Tucker (HT), appears to be the most amazing approach since a very high compression ratio might be obtained. Nevertheless, none of these tensor decomposition formats can provide both the space and computation efficiency. In this paper, we consider to compress RNNs based on a novel Kronecker CANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor (KT) decomposition, by proposing two fast algorithms of multiplication between the input and the tensor-decomposed weight. According to our experiments based on UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that the proposed KCP-RNNs have comparable performance of accuracy with those in other tensor-decomposed formats, and even 278,219x compression ratio could be obtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both space and computation complexity compared with other tensor-decomposed ones under similar ranks. Besides, we find KCP has the best potential for parallel computing to accelerate the calculations in neural networks.



### SSGP: Sparse Spatial Guided Propagation for Robust and Generic Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2008.09346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09346v2)
- **Published**: 2020-08-21 07:39:41+00:00
- **Updated**: 2020-11-04 09:06:51+00:00
- **Authors**: René Schuster, Oliver Wasenmüller, Christian Unger, Didier Stricker
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Interpolation of sparse pixel information towards a dense target resolution finds its application across multiple disciplines in computer vision. State-of-the-art interpolation of motion fields applies model-based interpolation that makes use of edge information extracted from the target image. For depth completion, data-driven learning approaches are widespread. Our work is inspired by latest trends in depth completion that tackle the problem of dense guidance for sparse information. We extend these ideas and create a generic cross-domain architecture that can be applied for a multitude of interpolation problems like optical flow, scene flow, or depth completion. In our experiments, we show that our proposed concept of Sparse Spatial Guided Propagation (SSGP) achieves improvements to robustness, accuracy, or speed compared to specialized algorithms.



### Deep Learning Methods for Lung Cancer Segmentation in Whole-slide Histopathology Images -- the ACDC@LungHP Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/2008.09352v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09352v1)
- **Published**: 2020-08-21 07:52:11+00:00
- **Updated**: 2020-08-21 07:52:11+00:00
- **Authors**: Zhang Li, Jiehua Zhang, Tao Tan, Xichao Teng, Xiaoliang Sun, Yang Li, Lihong Liu, Yang Xiao, Byungjae Lee, Yilong Li, Qianni Zhang, Shujiao Sun, Yushan Zheng, Junyu Yan, Ni Li, Yiyu Hong, Junsu Ko, Hyun Jung, Yanling Liu, Yu-cheng Chen, Ching-wei Wang, Vladimir Yurovskiy, Pavel Maevskikh, Vahid Khanagha, Yi Jiang, Xiangjun Feng, Zhihong Liu, Daiqiang Li, Peter J. Schüffler, Qifeng Yu, Hui Chen, Yuling Tang, Geert Litjens
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of lung cancer in pathology slides is a critical step in improving patient care. We proposed the ACDC@LungHP (Automatic Cancer Detection and Classification in Whole-slide Lung Histopathology) challenge for evaluating different computer-aided diagnosis (CADs) methods on the automatic diagnosis of lung cancer. The ACDC@LungHP 2019 focused on segmentation (pixel-wise detection) of cancer tissue in whole slide imaging (WSI), using an annotated dataset of 150 training images and 50 test images from 200 patients. This paper reviews this challenge and summarizes the top 10 submitted methods for lung cancer segmentation. All methods were evaluated using the false positive rate, false negative rate, and DICE coefficient (DC). The DC ranged from 0.7354$\pm$0.1149 to 0.8372$\pm$0.0858. The DC of the best method was close to the inter-observer agreement (0.8398$\pm$0.0890). All methods were based on deep learning and categorized into two groups: multi-model method and single model method. In general, multi-model methods were significantly better ($\textit{p}$<$0.01$) than single model methods, with mean DC of 0.7966 and 0.7544, respectively. Deep learning based methods could potentially help pathologists find suspicious regions for further analysis of lung cancer in WSI.



### Learning Domain-invariant Graph for Adaptive Semi-supervised Domain Adaptation with Few Labeled Source Samples
- **Arxiv ID**: http://arxiv.org/abs/2008.09359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09359v1)
- **Published**: 2020-08-21 08:13:25+00:00
- **Updated**: 2020-08-21 08:13:25+00:00
- **Authors**: Jinfeng Li, Weifeng Liu, Yicong Zhou, Jun Yu, Dapeng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to generalize a model from a source domain to tackle tasks in a related but different target domain. Traditional domain adaptation algorithms assume that enough labeled data, which are treated as the prior knowledge are available in the source domain. However, these algorithms will be infeasible when only a few labeled data exist in the source domain, and thus the performance decreases significantly. To address this challenge, we propose a Domain-invariant Graph Learning (DGL) approach for domain adaptation with only a few labeled source samples. Firstly, DGL introduces the Nystrom method to construct a plastic graph that shares similar geometric property as the target domain. And then, DGL flexibly employs the Nystrom approximation error to measure the divergence between plastic graph and source graph to formalize the distribution mismatch from the geometric perspective. Through minimizing the approximation error, DGL learns a domain-invariant geometric graph to bridge source and target domains. Finally, we integrate the learned domain-invariant graph with the semi-supervised learning and further propose an adaptive semi-supervised model to handle the cross-domain problems. The results of extensive experiments on popular datasets verify the superiority of DGL, especially when only a few labeled source samples are available.



### Learning Camera-Aware Noise Models
- **Arxiv ID**: http://arxiv.org/abs/2008.09370v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09370v1)
- **Published**: 2020-08-21 08:25:14+00:00
- **Updated**: 2020-08-21 08:25:14+00:00
- **Authors**: Ke-Chi Chang, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, Chia-Ping Chen, Yu-Lin Chang, Hwann-Tzong Chen
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Modeling imaging sensor noise is a fundamental problem for image processing and computer vision applications. While most previous works adopt statistical noise models, real-world noise is far more complicated and beyond what these models can describe. To tackle this issue, we propose a data-driven approach, where a generative noise model is learned from real-world noise. The proposed noise model is camera-aware, that is, different noise characteristics of different camera sensors can be learned simultaneously, and a single learned noise model can generate different noise for different camera sensors. Experimental results show that our method quantitatively and qualitatively outperforms existing statistical noise models and learning-based methods.



### A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples
- **Arxiv ID**: http://arxiv.org/abs/2008.09381v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.09381v4)
- **Published**: 2020-08-21 09:12:52+00:00
- **Updated**: 2021-09-06 17:24:12+00:00
- **Authors**: Julia Lust, Alexandru Paul Condurache
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous applications. However, it is difficult to tell beforehand if a DNN receiving an input will deliver the correct output since their decision criteria are usually nontransparent. A DNN delivers the correct output if the input is within the area enclosed by its generalization envelope. In this case, the information contained in the input sample is processed reasonably by the network. It is of large practical importance to assess at inference time if a DNN generalizes correctly. Currently, the approaches to achieve this goal are investigated in different problem set-ups rather independently from one another, leading to three main research and literature fields: predictive uncertainty, out-of-distribution detection and adversarial example detection. This survey connects the three fields within the larger framework of investigating the generalization performance of machine learning methods and in particular DNNs. We underline the common ground, point at the most promising approaches and give a structured overview of the methods that provide at inference time means to establish if the current input is within the generalization envelope of a DNN.



### CDE-GAN: Cooperative Dual Evolution Based Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2008.09388v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09388v2)
- **Published**: 2020-08-21 09:39:53+00:00
- **Updated**: 2021-03-23 13:06:29+00:00
- **Authors**: Shiming Chen, Wenjie Wang, Beihao Xia, Xinge You, Zehong Cao, Weiping Ding
- **Comment**: 15 pages,6 figures,4 tables. Accepted by IEEE Transactions on
  Evolutionary Computation
- **Journal**: IEEE Transactions on Evolutionary Computation, 2021
- **Summary**: Generative adversarial networks (GANs) have been a popular deep generative model for real-world applications. Despite many recent efforts on GANs that have been contributed, mode collapse and instability of GANs are still open problems caused by their adversarial optimization difficulties. In this paper, motivated by the cooperative co-evolutionary algorithm, we propose a Cooperative Dual Evolution based Generative Adversarial Network (CDE-GAN) to circumvent these drawbacks. In essence, CDE-GAN incorporates dual evolution with respect to the generator(s) and discriminators into a unified evolutionary adversarial framework to conduct effective adversarial multi-objective optimization. Thus it exploits the complementary properties and injects dual mutation diversity into training to steadily diversify the estimated density in capturing multi-modes and improve generative performance. Specifically, CDE-GAN decomposes the complex adversarial optimization problem into two subproblems (generation and discrimination), and each subproblem is solved with a separated subpopulation (E-Generator} and E-Discriminators), evolved by its own evolutionary algorithm. Additionally, we further propose a Soft Mechanism to balance the trade-off between E-Generators and E-Discriminators to conduct steady training for CDE-GAN. Extensive experiments on one synthetic dataset and three real-world benchmark image datasets demonstrate that the proposed CDE-GAN achieves a competitive and superior performance in generating good quality and diverse samples over baselines. The code and more generated results are available at our project homepage: https://shiming-chen.github.io/CDE-GAN-website/CDE-GAN.html.



### Align Deep Features for Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.09397v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09397v3)
- **Published**: 2020-08-21 09:55:13+00:00
- **Updated**: 2021-07-12 03:26:49+00:00
- **Authors**: Jiaming Han, Jian Ding, Jie Li, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The code is available at https://github.com/csuhan/s2anet.



### Exploiting Scene-specific Features for Object Goal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2008.09403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09403v1)
- **Published**: 2020-08-21 10:16:01+00:00
- **Updated**: 2020-08-21 10:16:01+00:00
- **Authors**: Tommaso Campari, Paolo Eccher, Luciano Serafini, Lamberto Ballan
- **Comment**: Accepted at ACVR2020 ECCV2020 Workshop
- **Journal**: None
- **Summary**: Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.



### Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.09412v1
- **DOI**: 10.1109/TIP.2021.3087348
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09412v1)
- **Published**: 2020-08-21 10:45:09+00:00
- **Updated**: 2020-08-21 10:45:09+00:00
- **Authors**: Zitong Yu, Benjia Zhou, Jun Wan, Pichao Wang, Haoyu Chen, Xin Liu, Stan Z. Li, Guoying Zhao
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Gesture recognition has attracted considerable attention owing to its great potential in applications. Although the great progress has been made recently in multi-modal learning methods, existing methods still lack effective integration to fully explore synergies among spatio-temporal modalities effectively for gesture recognition. The problems are partially due to the fact that the existing manually designed network architectures have low efficiency in the joint learning of multi-modalities. In this paper, we propose the first neural architecture search (NAS)-based method for RGB-D gesture recognition. The proposed method includes two key components: 1) enhanced temporal representation via the proposed 3D Central Difference Convolution (3D-CDC) family, which is able to capture rich temporal context via aggregating temporal difference information; and 2) optimized backbones for multi-sampling-rate branches and lateral connections among varied modalities. The resultant multi-modal multi-rate network provides a new perspective to understand the relationship between RGB and depth modalities and their temporal dynamics. Comprehensive experiments are performed on three benchmark datasets (IsoGD, NvGesture, and EgoGesture), demonstrating the state-of-the-art performance in both single- and multi-modality settings.The code is available at https://github.com/ZitongYu/3DCDC-NAS



### Automatic sleep stage classification with deep residual networks in a mixed-cohort setting
- **Arxiv ID**: http://arxiv.org/abs/2008.09416v1
- **DOI**: 10.1093/sleep/zsaa161
- **Categories**: **cs.CV**, eess.SP, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.09416v1)
- **Published**: 2020-08-21 10:48:35+00:00
- **Updated**: 2020-08-21 10:48:35+00:00
- **Authors**: Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B D Sorensen
- **Comment**: Author's original version. This article has been accepted for
  publication in SLEEP published by Oxford University Press
- **Journal**: None
- **Summary**: Study Objectives: Sleep stage scoring is performed manually by sleep experts and is prone to subjective interpretation of scoring rules with low intra- and interscorer reliability. Many automatic systems rely on few small-scale databases for developing models, and generalizability to new datasets is thus unknown. We investigated a novel deep neural network to assess the generalizability of several large-scale cohorts.   Methods: A deep neural network model was developed using 15684 polysomnography studies from five different cohorts. We applied four different scenarios: 1) impact of varying time-scales in the model; 2) performance of a single cohort on other cohorts of smaller, greater or equal size relative to the performance of other cohorts on a single cohort; 3) varying the fraction of mixed-cohort training data compared to using single-origin data; and 4) comparing models trained on combinations of data from 2, 3, and 4 cohorts.   Results: Overall classification accuracy improved with increasing fractions of training data (0.25$\%$: 0.782 $\pm$ 0.097, 95$\%$ CI [0.777-0.787]; 100$\%$: 0.869 $\pm$ 0.064, 95$\%$ CI [0.864-0.872]), and with increasing number of data sources (2: 0.788 $\pm$ 0.102, 95$\%$ CI [0.787-0.790]; 3: 0.808 $\pm$ 0.092, 95$\%$ CI [0.807-0.810]; 4: 0.821 $\pm$ 0.085, 95$\%$ CI [0.819-0.823]). Different cohorts show varying levels of generalization to other cohorts.   Conclusions: Automatic sleep stage scoring systems based on deep learning algorithms should consider as much data as possible from as many sources available to ensure proper generalization. Public datasets for benchmarking should be made available for future research.



### Action-Based Representation Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.09417v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.09417v2)
- **Published**: 2020-08-21 10:49:13+00:00
- **Updated**: 2020-11-09 15:45:26+00:00
- **Authors**: Yi Xiao, Felipe Codevilla, Christopher Pal, Antonio M. Lopez
- **Comment**: This paper has been accepted to the Conference on Robot Learning
  (CoRL 2020)
- **Journal**: None
- **Summary**: Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).



### Method to Classify Skin Lesions using Dermoscopic images
- **Arxiv ID**: http://arxiv.org/abs/2008.09418v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.09418v2)
- **Published**: 2020-08-21 10:58:33+00:00
- **Updated**: 2022-12-20 05:10:47+00:00
- **Authors**: Dusa Sai Charan, Hemanth Nadipineni, Subin Sahayam, Umarani Jayaraman
- **Comment**: 16 pages, 14 figures
- **Journal**: None
- **Summary**: Skin cancer is the most common cancer in the existing world constituting one-third of the cancer cases. Benign skin cancers are not fatal, can be cured with proper medication. But it is not the same as the malignant skin cancers. In the case of malignant melanoma, in its peak stage, the maximum life expectancy is less than or equal to 5 years. But, it can be cured if detected in early stages. Though there are numerous clinical procedures, the accuracy of diagnosis falls between 49% to 81% and is time-consuming. So, dermoscopy has been brought into the picture. It helped in increasing the accuracy of diagnosis but could not demolish the error-prone behaviour. A quick and less error-prone solution is needed to diagnose this majorly growing skin cancer. This project deals with the usage of deep learning in skin lesion classification. In this project, an automated model for skin lesion classification using dermoscopic images has been developed with CNN(Convolution Neural Networks) as a training model. Convolution neural networks are known for capturing features of an image. So, they are preferred in analyzing medical images to find the characteristics that drive the model towards success. Techniques like data augmentation for tackling class imbalance, segmentation for focusing on the region of interest and 10-fold cross-validation to make the model robust have been brought into the picture. This project also includes usage of certain preprocessing techniques like brightening the images using piece-wise linear transformation function, grayscale conversion of the image, resize the image. This project throws a set of valuable insights on how the accuracy of the model hikes with the bringing of new input strategies, preprocessing techniques. The best accuracy this model could achieve is 0.886.



### Self-Supervised Gait Encoding with Locality-Aware Attention for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.09435v1
- **DOI**: 10.24963/ijcai.2020/125
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09435v1)
- **Published**: 2020-08-21 12:03:17+00:00
- **Updated**: 2020-08-21 12:03:17+00:00
- **Authors**: Haocong Rao, Siqi Wang, Xiping Hu, Mingkui Tan, Huang Da, Jun Cheng, Bin Hu
- **Comment**: Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI.
  Codes are available at https://github.com/Kali-Hac/SGE-LA
- **Journal**: In IJCAI, pages 898-905, 2020
- **Summary**: Gait-based person re-identification (Re-ID) is valuable for safety-critical applications, and using only 3D skeleton data to extract discriminative gait features for person Re-ID is an emerging open topic. Existing methods either adopt hand-crafted features or learn gait features by traditional supervised learning paradigms. Unlike previous methods, we for the first time propose a generic gait encoding approach that can utilize unlabeled skeleton data to learn gait representations in a self-supervised manner. Specifically, we first propose to introduce self-supervision by learning to reconstruct input skeleton sequences in reverse order, which facilitates learning richer high-level semantics and better gait representations. Second, inspired by the fact that motion's continuity endows temporally adjacent skeletons with higher correlations ("locality"), we propose a locality-aware attention mechanism that encourages learning larger attention weights for temporally adjacent skeletons when reconstructing current skeleton, so as to learn locality when encoding gait. Finally, we propose Attention-based Gait Encodings (AGEs), which are built using context vectors learned by locality-aware attention, as final gait representations. AGEs are directly utilized to realize effective person Re-ID. Our approach typically improves existing skeleton-based methods by 10-20% Rank-1 accuracy, and it achieves comparable or even superior performance to multi-modal methods with extra RGB or depth information. Our codes are available at https://github.com/Kali-Hac/SGE-LA.



### An Improved Person Re-identification Method by light-weight convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2008.09448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09448v1)
- **Published**: 2020-08-21 12:34:15+00:00
- **Updated**: 2020-08-21 12:34:15+00:00
- **Authors**: Sajad Amouei Sheshkal, Kazim Fouladi-Ghaleh, Hossein Aghababa
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-identification is defined as a recognizing process where the person is observed by non-overlapping cameras at different places. In the last decade, the rise in the applications and importance of Person Re-identification for surveillance systems popularized this subject in different areas of computer vision. Person Re-identification is faced with challenges such as low resolution, varying poses, illumination, background clutter, and occlusion, which could affect the result of recognizing process. The present paper aims to improve Person Re-identification using transfer learning and application of verification loss function within the framework of Siamese network. The Siamese network receives image pairs as inputs and extract their features via a pre-trained model. EfficientNet was employed to obtain discriminative features and reduce the demands for data. The advantages of verification loss were used in the network learning. Experiments showed that the proposed model performs better than state-of-the-art methods on the CUHK01 dataset. For example, rank5 accuracies are 95.2% (+5.7) for the CUHK01 datasets. It also achieved an acceptable percentage in Rank 1. Because of the small size of the pre-trained model parameters, learning speeds up and there will be a need for less hardware and data.



### DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild
- **Arxiv ID**: http://arxiv.org/abs/2008.09457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09457v1)
- **Published**: 2020-08-21 12:54:26+00:00
- **Updated**: 2020-08-21 12:54:26+00:00
- **Authors**: Philippe Weinzaepfel, Romain Brégier, Hadrien Combaluzier, Vincent Leroy, Grégory Rogez
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We introduce DOPE, the first method to detect and estimate whole-body 3D human poses, including bodies, hands and faces, in the wild. Achieving this level of details is key for a number of applications that require understanding the interactions of the people with each other or with the environment. The main challenge is the lack of in-the-wild data with labeled whole-body 3D poses. In previous work, training data has been annotated or generated for simpler tasks focusing on bodies, hands or faces separately. In this work, we propose to take advantage of these datasets to train independent experts for each part, namely a body, a hand and a face expert, and distill their knowledge into a single deep network designed for whole-body 2D-3D pose detection. In practice, given a training image with partial or no annotation, each part expert detects its subset of keypoints in 2D and 3D and the resulting estimations are combined to obtain whole-body pseudo ground-truth poses. A distillation loss encourages the whole-body predictions to mimic the experts' outputs. Our results show that this approach significantly outperforms the same whole-body model trained without distillation while staying close to the performance of the experts. Importantly, DOPE is computationally less demanding than the ensemble of experts and can achieve real-time performance. Test code and models are available at https://europe.naverlabs.com/research/computer-vision/dope.



### INSIDE: Steering Spatial Attention with Non-Imaging Information in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2008.10418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.10418v1)
- **Published**: 2020-08-21 13:32:05+00:00
- **Updated**: 2020-08-21 13:32:05+00:00
- **Authors**: Grzegorz Jacenków, Alison Q. O'Neil, Brian Mohr, Sotirios A. Tsaftaris
- **Comment**: Accepted at International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI) 2020
- **Journal**: None
- **Summary**: We consider the problem of integrating non-imaging information into segmentation networks to improve performance. Conditioning layers such as FiLM provide the means to selectively amplify or suppress the contribution of different feature maps in a linear fashion. However, spatial dependency is difficult to learn within a convolutional paradigm. In this paper, we propose a mechanism to allow for spatial localisation conditioned on non-imaging information, using a feature-wise attention mechanism comprising a differentiable parametrised function (e.g. Gaussian), prior to applying the feature-wise modulation. We name our method INstance modulation with SpatIal DEpendency (INSIDE). The conditioning information might comprise any factors that relate to spatial or spatio-temporal information such as lesion location, size, and cardiac cycle phase. Our method can be trained end-to-end and does not require additional supervision. We evaluate the method on two datasets: a new CLEVR-Seg dataset where we segment objects based on location, and the ACDC dataset conditioned on cardiac phase and slice location within the volume. Code and the CLEVR-Seg dataset are available at https://github.com/jacenkow/inside.



### Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching
- **Arxiv ID**: http://arxiv.org/abs/2008.09474v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.09474v4)
- **Published**: 2020-08-21 13:42:25+00:00
- **Updated**: 2020-11-02 11:00:11+00:00
- **Authors**: Zexi Chen, Xuecheng Xu, Yue Wang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: The crucial step for localization is to match the current observation to the map. When the two sensor modalities are significantly different, matching becomes challenging. In this paper, we present an end-to-end deep phase correlation network (DPCN) to match heterogeneous sensor measurements. In DPCN, the primary component is a differentiable correlation-based estimator that back-propagates the pose error to learnable feature extractors, which addresses the problem that there are no direct common features for supervision. Also, it eliminates the exhaustive evaluation in some previous methods, improving efficiency. With the interpretable modeling, the network is light-weighted and promising for better generalization. We evaluate the system on both the simulation data and Aero-Ground Dataset which consists of heterogeneous sensor images and aerial images acquired by satellites or aerial robots. The results show that our method is able to match the heterogeneous sensor measurements, outperforming the comparative traditional phase correlation and other learning-based methods. Code is available at https://github.com/jessychen1016/DPCN .



### Single-Image Depth Prediction Makes Feature Matching Easier
- **Arxiv ID**: http://arxiv.org/abs/2008.09497v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2008.09497v1)
- **Published**: 2020-08-21 14:25:36+00:00
- **Updated**: 2020-08-21 14:25:36+00:00
- **Authors**: Carl Toft, Daniyar Turmukhambetov, Torsten Sattler, Fredrik Kahl, Gabriel Brostow
- **Comment**: 14 pages, 7 figures, accepted for publication at the European
  conference on computer vision (ECCV) 2020
- **Journal**: None
- **Summary**: Good local features improve the robustness of many 3D re-localization and multi-view reconstruction pipelines. The problem is that viewing angle and distance severely impact the recognizability of a local feature. Attempts to improve appearance invariance by choosing better local feature points or by leveraging outside information, have come with pre-requisites that made some of them impractical. In this paper, we propose a surprisingly effective enhancement to local feature extraction, which improves matching. We show that CNN-based depths inferred from single RGB images are quite helpful, despite their flaws. They allow us to pre-warp images and rectify perspective distortions, to significantly enhance SIFT and BRISK features, enabling more good matches, even when cameras are looking at the same scene but in opposite directions.



### PointNetLK Revisited
- **Arxiv ID**: http://arxiv.org/abs/2008.09527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09527v2)
- **Published**: 2020-08-21 15:09:28+00:00
- **Updated**: 2021-03-30 02:11:50+00:00
- **Authors**: Xueqian Li, Jhony Kaesemodel Pontes, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: We address the generalization ability of recent learning-based point cloud registration methods. Despite their success, these approaches tend to have poor performance when applied to mismatched conditions that are not well-represented in the training set, such as unseen object categories, different complex scenes, or unknown depth sensors. In these circumstances, it has often been better to rely on classical non-learning methods (e.g., Iterative Closest Point), which have better generalization ability. Hybrid learning methods, that use learning for predicting point correspondences and then a deterministic step for alignment, have offered some respite, but are still limited in their generalization abilities. We revisit a recent innovation -- PointNetLK -- and show that the inclusion of an analytical Jacobian can exhibit remarkable generalization properties while reaping the inherent fidelity benefits of a learning framework. Our approach not only outperforms the state-of-the-art in mismatched conditions but also produces results competitive with current learning methods when operating on real-world test data close to the training set.



### Behavioural pattern discovery from collections of egocentric photo-streams
- **Arxiv ID**: http://arxiv.org/abs/2008.09561v1
- **DOI**: 10.1007/978-3-030-66823-5_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09561v1)
- **Published**: 2020-08-21 16:05:45+00:00
- **Updated**: 2020-08-21 16:05:45+00:00
- **Authors**: Martin Menchon, Estefania Talavera, Jose M Massa, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic discovery of behaviour is of high importance when aiming to assess and improve the quality of life of people. Egocentric images offer a rich and objective description of the daily life of the camera wearer. This work proposes a new method to identify a person's patterns of behaviour from collected egocentric photo-streams. Our model characterizes time-frames based on the context (place, activities and environment objects) that define the images composition. Based on the similarity among the time-frames that describe the collected days for a user, we propose a new unsupervised greedy method to discover the behavioural pattern set based on a novel semantic clustering approach. Moreover, we present a new score metric to evaluate the performance of the proposed algorithm. We validate our method on 104 days and more than 100k images extracted from 7 users. Results show that behavioural patterns can be discovered to characterize the routine of individuals and consequently their lifestyle.



### TAnoGAN: Time Series Anomaly Detection with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.09567v2
- **DOI**: 10.1109/SSCI47803.2020.9308512
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.09567v2)
- **Published**: 2020-08-21 16:24:51+00:00
- **Updated**: 2020-09-25 01:50:33+00:00
- **Authors**: Md Abul Bashar, Richi Nayak
- **Comment**: Made some minor changes. This is the accepted version of the paper at
  AusDM'20
- **Journal**: 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Summary**: Anomaly detection in time series data is a significant problem faced in many application areas such as manufacturing, medical imaging and cyber-security. Recently, Generative Adversarial Networks (GAN) have gained attention for generation and anomaly detection in image domain. In this paper, we propose a novel GAN-based unsupervised method called TAnoGan for detecting anomalies in time series when a small number of data points are available. We evaluate TAnoGan with 46 real-world time series datasets that cover a variety of domains. Extensive experimental results show that TAnoGan performs better than traditional and neural network models.



### A persistent homology-based topological loss function for multi-class CNN segmentation of cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2008.09585v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09585v1)
- **Published**: 2020-08-21 17:09:13+00:00
- **Updated**: 2020-08-21 17:09:13+00:00
- **Authors**: Nick Byrne, James R. Clough, Giovanni Montana, Andrew P. King
- **Comment**: To be presented at the STACOM workshop at MICCAI 2020
- **Journal**: None
- **Summary**: With respect to spatial overlap, CNN-based segmentation of short axis cardiovascular magnetic resonance (CMR) images has achieved a level of performance consistent with inter observer variation. However, conventional training procedures frequently depend on pixel-wise loss functions, limiting optimisation with respect to extended or global features. As a result, inferred segmentations can lack spatial coherence, including spurious connected components or holes. Such results are implausible, violating the anticipated topology of image segments, which is frequently known a priori. Addressing this challenge, published work has employed persistent homology, constructing topological loss functions for the evaluation of image segments against an explicit prior. Building a richer description of segmentation topology by considering all possible labels and label pairs, we extend these losses to the task of multi-class segmentation. These topological priors allow us to resolve all topological errors in a subset of 150 examples from the ACDC short axis CMR training data set, without sacrificing overlap performance.



### Delving Deeper into Anti-aliasing in ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2008.09604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09604v1)
- **Published**: 2020-08-21 17:56:04+00:00
- **Updated**: 2020-08-21 17:56:04+00:00
- **Authors**: Xueyan Zou, Fanyi Xiao, Zhiding Yu, Yong Jae Lee
- **Comment**: [Accepted in BMVC2020] code: https://maureenzou.github.io/ddac/
- **Journal**: None
- **Summary**: Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling. However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks including ImageNet classification, COCO instance segmentation, and Cityscapes semantic segmentation. Qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition. Code is available at https://maureenzou.github.io/ddac/.



### Learning to Set Waypoints for Audio-Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2008.09622v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2008.09622v3)
- **Published**: 2020-08-21 18:00:33+00:00
- **Updated**: 2021-02-11 18:36:45+00:00
- **Authors**: Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh Kumar Ramakrishnan, Kristen Grauman
- **Comment**: Accepted to ICLR 2021
- **Journal**: None
- **Summary**: In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation. Project: http://vision.cs.utexas.edu/projects/audio_visual_waypoints.



### Orderly Disorder in Point Cloud Domain
- **Arxiv ID**: http://arxiv.org/abs/2008.09634v1
- **DOI**: 10.1007/978-3-030-58604-1_30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09634v1)
- **Published**: 2020-08-21 18:18:09+00:00
- **Updated**: 2020-08-21 18:18:09+00:00
- **Authors**: Morteza Ghahremani, Bernard Tiddeman, Yonghuai Liu, Ardhendu Behera
- **Comment**: None
- **Journal**: 16th European Conference on Computer Vision (ECCV2020)
- **Summary**: In the real world, out-of-distribution samples, noise and distortions exist in test data. Existing deep networks developed for point cloud data analysis are prone to overfitting and a partial change in test data leads to unpredictable behaviour of the networks. In this paper, we propose a smart yet simple deep network for analysis of 3D models using `orderly disorder' theory. Orderly disorder is a way of describing the complex structure of disorders within complex systems. Our method extracts the deep patterns inside a 3D object via creating a dynamic link to seek the most stable patterns and at once, throws away the unstable ones. Patterns are more robust to changes in data distribution, especially those that appear in the top layers. Features are extracted via an innovative cloning decomposition technique and then linked to each other to form stable complex patterns. Our model alleviates the vanishing-gradient problem, strengthens dynamic link propagation and substantially reduces the number of parameters. Extensive experiments on challenging benchmark datasets verify the superiority of our light network on the segmentation and classification tasks, especially in the presence of noise wherein our network's performance drops less than 10% while the state-of-the-art networks fail to work.



### Blending of Learning-based Tracking and Object Detection for Monocular Camera-based Target Following
- **Arxiv ID**: http://arxiv.org/abs/2008.09644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.09644v1)
- **Published**: 2020-08-21 18:44:35+00:00
- **Updated**: 2020-08-21 18:44:35+00:00
- **Authors**: Pranoy Panda, Martin Barczyk
- **Comment**: Accepted in 24th International Symposium on Mathematical Theory of
  Networks and Systems (MTNS 2020): Cambridge, UK (updated conference date:
  23-27 August 2021)
- **Journal**: None
- **Summary**: Deep learning has recently started being applied to visual tracking of generic objects in video streams. For the purposes of robotics applications, it is very important for a target tracker to recover its track if it is lost due to heavy or prolonged occlusions or motion blur of the target. We present a real-time approach which fuses a generic target tracker and object detection module with a target re-identification module. Our work focuses on improving the performance of Convolutional Recurrent Neural Network-based object trackers in cases where the object of interest belongs to the category of \emph{familiar} objects. Our proposed approach is sufficiently lightweight to track objects at 85-90 FPS while attaining competitive results on challenging benchmarks.



### Generating synthetic photogrammetric data for training deep learning based 3D point cloud segmentation models
- **Arxiv ID**: http://arxiv.org/abs/2008.09647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09647v1)
- **Published**: 2020-08-21 18:50:42+00:00
- **Updated**: 2020-08-21 18:50:42+00:00
- **Authors**: Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad, Ryan McAlinden, Lucio Soibelman
- **Comment**: None
- **Journal**: Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC) 2020
- **Summary**: At I/ITSEC 2019, the authors presented a fully-automated workflow to segment 3D photogrammetric point-clouds/meshes and extract object information, including individual tree locations and ground materials (Chen et al., 2019). The ultimate goal is to create realistic virtual environments and provide the necessary information for simulation. We tested the generalizability of the previously proposed framework using a database created under the U.S. Army's One World Terrain (OWT) project with a variety of landscapes (i.e., various buildings styles, types of vegetation, and urban density) and different data qualities (i.e., flight altitudes and overlap between images). Although the database is considerably larger than existing databases, it remains unknown whether deep-learning algorithms have truly achieved their full potential in terms of accuracy, as sizable data sets for training and validation are currently lacking. Obtaining large annotated 3D point-cloud databases is time-consuming and labor-intensive, not only from a data annotation perspective in which the data must be manually labeled by well-trained personnel, but also from a raw data collection and processing perspective. Furthermore, it is generally difficult for segmentation models to differentiate objects, such as buildings and tree masses, and these types of scenarios do not always exist in the collected data set. Thus, the objective of this study is to investigate using synthetic photogrammetric data to substitute real-world data in training deep-learning algorithms. We have investigated methods for generating synthetic UAV-based photogrammetric data to provide a sufficiently sized database for training a deep-learning algorithm with the ability to enlarge the data size for scenarios in which deep-learning models have difficulties.



### Semantic Segmentation and Data Fusion of Microsoft Bing 3D Cities and Small UAV-based Photogrammetric Data
- **Arxiv ID**: http://arxiv.org/abs/2008.09648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09648v1)
- **Published**: 2020-08-21 18:56:05+00:00
- **Updated**: 2020-08-21 18:56:05+00:00
- **Authors**: Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad, Ryan McAlinden, Lucio Soibelman
- **Comment**: None
- **Journal**: Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC) 2020
- **Summary**: With state-of-the-art sensing and photogrammetric techniques, Microsoft Bing Maps team has created over 125 highly detailed 3D cities from 11 different countries that cover hundreds of thousands of square kilometer areas. The 3D city models were created using the photogrammetric technique with high-resolution images that were captured from aircraft-mounted cameras. Such a large 3D city database has caught the attention of the US Army for creating virtual simulation environments to support military operations. However, the 3D city models do not have semantic information such as buildings, vegetation, and ground and cannot allow sophisticated user-level and system-level interaction. At I/ITSEC 2019, the authors presented a fully automated data segmentation and object information extraction framework for creating simulation terrain using UAV-based photogrammetric data. This paper discusses the next steps in extending our designed data segmentation framework for segmenting 3D city data. In this study, the authors first investigated the strengths and limitations of the existing framework when applied to the Bing data. The main differences between UAV-based and aircraft-based photogrammetric data are highlighted. The data quality issues in the aircraft-based photogrammetric data, which can negatively affect the segmentation performance, are identified. Based on the findings, a workflow was designed specifically for segmenting Bing data while considering its characteristics. In addition, since the ultimate goal is to combine the use of both small unmanned aerial vehicle (UAV) collected data and the Bing data in a virtual simulation environment, data from these two sources needed to be aligned and registered together. To this end, the authors also proposed a data registration workflow that utilized the traditional iterative closest point (ICP) with the extracted semantic information.



### DeepLandscape: Adversarial Modeling of Landscape Video
- **Arxiv ID**: http://arxiv.org/abs/2008.09655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09655v1)
- **Published**: 2020-08-21 19:14:19+00:00
- **Updated**: 2020-08-21 19:14:19+00:00
- **Authors**: Elizaveta Logacheva, Roman Suvorov, Oleg Khomenko, Anton Mashikhin, Victor Lempitsky
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: We build a new model of landscape videos that can be trained on a mixture of static landscape images as well as landscape animations. Our architecture extends StyleGAN model by augmenting it with parts that allow to model dynamic changes in a scene. Once trained, our model can be used to generate realistic time-lapse landscape videos with moving objects and time-of-the-day changes. Furthermore, by fitting the learned models to a static landscape image, the latter can be reenacted in a realistic way. We propose simple but necessary modifications to StyleGAN inversion procedure, which lead to in-domain latent codes and allow to manipulate real images. Quantitative comparisons and user studies suggest that our model produces more compelling animations of given photographs than previously proposed methods. The results of our approach including comparisons with prior art can be seen in supplementary materials and on the project page https://saic-mdal.github.io/deep-landscape



### Training Sparse Neural Networks using Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2008.09661v2
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.09661v2)
- **Published**: 2020-08-21 19:35:54+00:00
- **Updated**: 2021-04-07 04:14:08+00:00
- **Authors**: Jonathan W. Siegel, Jianhong Chen, Pengchuan Zhang, Jinchao Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning the weights of neural networks is an effective and widely-used technique for reducing model size and inference complexity. We develop and test a novel method based on compressed sensing which combines the pruning and training into a single step. Specifically, we utilize an adaptively weighted $\ell^1$ penalty on the weights during training, which we combine with a generalization of the regularized dual averaging (RDA) algorithm in order to train sparse neural networks. The adaptive weighting we introduce corresponds to a novel regularizer based on the logarithm of the absolute value of the weights. We perform a series of ablation studies demonstrating the improvement provided by the adaptive weighting and generalized RDA algorithm. Furthermore, numerical experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that our method 1) trains sparser, more accurate networks than existing state-of-the-art methods; 2) can be used to train sparse networks from scratch, i.e. from a random initialization, as opposed to initializing with a well-trained base model; 3) acts as an effective regularizer, improving generalization accuracy.



### Towards Autonomous Driving: a Multi-Modal 360$^{\circ}$ Perception Proposal
- **Arxiv ID**: http://arxiv.org/abs/2008.09672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.09672v1)
- **Published**: 2020-08-21 20:36:21+00:00
- **Updated**: 2020-08-21 20:36:21+00:00
- **Authors**: Jorge Beltrán, Carlos Guindel, Irene Cortés, Alejandro Barrera, Armando Astudillo, Jesús Urdiales, Mario Álvarez, Farid Bekka, Vicente Milanés, Fernando García
- **Comment**: Accepted for publication in IEEE ITSC 2020
- **Journal**: None
- **Summary**: In this paper, a multi-modal 360$^{\circ}$ framework for 3D object detection and tracking for autonomous vehicles is presented. The process is divided into four main stages. First, images are fed into a CNN network to obtain instance segmentation of the surrounding road participants. Second, LiDAR-to-image association is performed for the estimated mask proposals. Then, the isolated points of every object are processed by a PointNet ensemble to compute their corresponding 3D bounding boxes and poses. Lastly, a tracking stage based on Unscented Kalman Filter is used to track the agents along time. The solution, based on a novel sensor fusion configuration, provides accurate and reliable road environment detection. A wide variety of tests of the system, deployed in an autonomous vehicle, have successfully assessed the suitability of the proposed perception stack in a real autonomous driving application.



### Toward Quantifying Ambiguities in Artistic Images
- **Arxiv ID**: http://arxiv.org/abs/2008.09688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, I.4.0; I.3.8
- **Links**: [PDF](http://arxiv.org/pdf/2008.09688v1)
- **Published**: 2020-08-21 21:40:16+00:00
- **Updated**: 2020-08-21 21:40:16+00:00
- **Authors**: Xi Wang, Zoya Bylinskii, Aaron Hertzmann, Robert Pepperell
- **Comment**: None
- **Journal**: ACM Trans. Applied Perception, 2020
- **Summary**: It has long been hypothesized that perceptual ambiguities play an important role in aesthetic experience: a work with some ambiguity engages a viewer more than one that does not. However, current frameworks for testing this theory are limited by the availability of stimuli and data collection methods. This paper presents an approach to measuring the perceptual ambiguity of a collection of images. Crowdworkers are asked to describe image content, after different viewing durations. Experiments are performed using images created with Generative Adversarial Networks, using the Artbreeder website. We show that text processing of viewer responses can provide a fine-grained way to measure and describe image ambiguities.



### Many-shot from Low-shot: Learning to Annotate using Mixed Supervision for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.09694v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.09694v2)
- **Published**: 2020-08-21 22:06:43+00:00
- **Updated**: 2020-08-26 17:26:13+00:00
- **Authors**: Carlo Biffi, Steven McDonagh, Philip Torr, Ales Leonardis, Sarah Parisot
- **Comment**: Accepted at ECCV 2020. Camera-ready version and Appendices
- **Journal**: None
- **Summary**: Object detection has witnessed significant progress by relying on large, manually annotated datasets. Annotating such datasets is highly time consuming and expensive, which motivates the development of weakly supervised and few-shot object detection methods. However, these methods largely underperform with respect to their strongly supervised counterpart, as weak training signals \emph{often} result in partial or oversized detections. Towards solving this problem we introduce, for the first time, an online annotation module (OAM) that learns to generate a many-shot set of \emph{reliable} annotations from a larger volume of weakly labelled images. Our OAM can be jointly trained with any fully supervised two-stage object detection method, providing additional training annotations on the fly. This results in a fully end-to-end strategy that only requires a low-shot set of fully annotated images. The integration of the OAM with Fast(er) R-CNN improves their performance by $17\%$ mAP, $9\%$ AP50 on PASCAL VOC 2007 and MS-COCO benchmarks, and significantly outperforms competing methods using mixed supervision.



### A Unified Taylor Framework for Revisiting Attribution Methods
- **Arxiv ID**: http://arxiv.org/abs/2008.09695v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09695v3)
- **Published**: 2020-08-21 22:07:06+00:00
- **Updated**: 2021-04-13 09:00:51+00:00
- **Authors**: Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Attribution methods have been developed to understand the decision-making process of machine learning models, especially deep neural networks, by assigning importance scores to individual features. Existing attribution methods often built upon empirical intuitions and heuristics. There still lacks a general and theoretical framework that not only can unify these attribution methods, but also theoretically reveal their rationales, fidelity, and limitations. To bridge the gap, in this paper, we propose a Taylor attribution framework and reformulate seven mainstream attribution methods into the framework. Based on reformulations, we analyze the attribution methods in terms of rationale, fidelity, and limitation. Moreover, We establish three principles for a good attribution in the Taylor attribution framework, i.e., low approximation error, correct contribution assignment, and unbiased baseline selection. Finally, we empirically validate the Taylor reformulations and reveal a positive correlation between the attribution performance and the number of principles followed by the attribution method via benchmarking on real-world datasets.



### Perceptual underwater image enhancement with deep learning and physical priors
- **Arxiv ID**: http://arxiv.org/abs/2008.09697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.09697v2)
- **Published**: 2020-08-21 22:11:34+00:00
- **Updated**: 2020-09-26 21:30:53+00:00
- **Authors**: Long Chen, Zheheng Jiang, Lei Tong, Zhihua Liu, Aite Zhao, Qianni Zhang, Junyu Dong, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater image enhancement, as a pre-processing step to improve the accuracy of the following object detection task, has drawn considerable attention in the field of underwater navigation and ocean exploration. However, most of the existing underwater image enhancement strategies tend to consider enhancement and detection as two independent modules with no interaction, and the practice of separate optimization does not always help the underwater object detection task. In this paper, we propose two perceptual enhancement models, each of which uses a deep enhancement model with a detection perceptor. The detection perceptor provides coherent information in the form of gradients to the enhancement model, guiding the enhancement model to generate patch level visually pleasing images or detection favourable images. In addition, due to the lack of training data, a hybrid underwater image synthesis model, which fuses physical priors and data-driven cues, is proposed to synthesize training data and generalise our enhancement model for real-world underwater images. Experimental results show the superiority of our proposed method over several state-of-the-art methods on both real-world and synthetic underwater datasets.



### Comparative performance analysis of the ResNet backbones of Mask RCNN to segment the signs of COVID-19 in chest CT scans
- **Arxiv ID**: http://arxiv.org/abs/2008.09713v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09713v1)
- **Published**: 2020-08-21 23:42:08+00:00
- **Updated**: 2020-08-21 23:42:08+00:00
- **Authors**: Muhammad Aleem, Rahul Raj, Arshad Khan
- **Comment**: 11 pages, 10 figures and 4 tables
- **Journal**: None
- **Summary**: COVID-19 has been detrimental in terms of the number of fatalities and rising number of critical patients across the world. According to the UNDP (United National Development Programme) Socio-Economic programme, aimed at the COVID-19 crisis, the pandemic is far more than a health crisis: it is affecting societies and economies at their core. There has been greater developments recently in the chest X-ray-based imaging technique as part of the COVID-19 diagnosis especially using Convolution Neural Networks (CNN) for recognising and classifying images. However, given the limitation of supervised labelled imaging data, the classification and predictive risk modelling of medical diagnosis tend to compromise. This paper aims to identify and monitor the effects of COVID-19 on the human lungs by employing Deep Neural Networks on axial CT (Chest Computed Tomography) scan of lungs. We have adopted Mask RCNN, with ResNet50 and ResNet101 as its backbone, to segment the regions, affected by COVID-19 coronavirus. Using the regions of human lungs, where symptoms have manifested, the model classifies condition of the patient as either "Mild" or "Alarming". Moreover, the model is deployed on the Google Cloud Platform (GCP) to simulate the online usage of the model for performance evaluation and accuracy improvement. The ResNet101 backbone model produces an F1 score of 0.85 and faster prediction scores with an average time of 9.04 seconds per inference.



