# Arxiv Papers in cs.CV on 2020-08-28
### Regularized Densely-connected Pyramid Network for Salient Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.12416v2
- **DOI**: 10.1109/TIP.2021.3065822
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12416v2)
- **Published**: 2020-08-28 00:13:30+00:00
- **Updated**: 2021-03-12 03:21:50+00:00
- **Authors**: Yu-Huan Wu, Yun Liu, Le Zhang, Wang Gao, Ming-Ming Cheng
- **Comment**: Accepted in IEEE Transactions on Image Processing. Code:
  https://github.com/yuhuan-wu/RDPNet
- **Journal**: None
- **Summary**: Much of the recent efforts on salient object detection (SOD) have been devoted to producing accurate saliency maps without being aware of their instance labels. To this end, we propose a new pipeline for end-to-end salient instance segmentation (SIS) that predicts a class-agnostic mask for each detected salient instance. To better use the rich feature hierarchies in deep networks and enhance the side predictions, we propose the regularized dense connections, which attentively promote informative features and suppress non-informative ones from all feature pyramids. A novel multi-level RoIAlign based decoder is introduced to adaptively aggregate multi-level features for better mask predictions. Such strategies can be well-encapsulated into the Mask R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that our design significantly outperforms existing \sArt competitors by 6.3\% (58.6\% vs. 52.3\%) in terms of the AP metric.The code is available at https://github.com/yuhuan-wu/RDPNet.



### All About Knowledge Graphs for Actions
- **Arxiv ID**: http://arxiv.org/abs/2008.12432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12432v1)
- **Published**: 2020-08-28 01:44:01+00:00
- **Updated**: 2020-08-28 01:44:01+00:00
- **Authors**: Pallabi Ghosh, Nirat Saini, Larry S. Davis, Abhinav Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Current action recognition systems require large amounts of training data for recognizing an action. Recent works have explored the paradigm of zero-shot and few-shot learning to learn classifiers for unseen categories or categories with few labels. Following similar paradigms in object recognition, these approaches utilize external sources of knowledge (eg. knowledge graphs from language domains). However, unlike objects, it is unclear what is the best knowledge representation for actions. In this paper, we intend to gain a better understanding of knowledge graphs (KGs) that can be utilized for zero-shot and few-shot action recognition. In particular, we study three different construction mechanisms for KGs: action embeddings, action-object embeddings, visual embeddings. We present extensive analysis of the impact of different KGs in different experimental setups. Finally, to enable a systematic study of zero-shot and few-shot approaches, we propose an improved evaluation paradigm based on UCF101, HMDB51, and Charades datasets for knowledge transfer from models trained on Kinetics.



### Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.12444v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12444v3)
- **Published**: 2020-08-28 02:22:07+00:00
- **Updated**: 2020-09-03 02:33:25+00:00
- **Authors**: Jiangjing Lyu, Xiaobo Li, Xiangyu Zhu, Cheng Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: 3D face reconstruction is a fundamental task that can facilitate numerous applications such as robust facial analysis and augmented reality. It is also a challenging task due to the lack of high-quality datasets that can fuel current deep learning-based methods. However, existing datasets are limited in quantity, realisticity and diversity. To circumvent these hurdles, we introduce Pixel-Face, a large-scale, high-resolution and diverse 3D face dataset with massive annotations. Specifically, Pixel-Face contains 855 subjects aging from 18 to 80. Each subject has more than 20 samples with various expressions. Each sample is composed of high-resolution multi-view RGB images and 3D meshes with various expressions. Moreover, we collect precise landmarks annotation and 3D registration result for each data. To demonstrate the advantages of Pixel-Face, we re-parameterize the 3D Morphable Model (3DMM) into Pixel-3DM using the collected data. We show that the obtained Pixel-3DM is better in modeling a wide range of face shapes and expressions. We also carefully benchmark existing 3D face reconstruction methods on our dataset. Moreover, Pixel-Face serves as an effective training source. We observe that the performance of current face reconstruction models significantly improves both on existing benchmarks and Pixel-Face after being fine-tuned using our newly collected data. Extensive experiments demonstrate the effectiveness of Pixel-3DM and the usefulness of Pixel-Face.



### Fast Single-shot Ship Instance Segmentation Based on Polar Template Mask in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2008.12447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12447v1)
- **Published**: 2020-08-28 02:38:04+00:00
- **Updated**: 2020-08-28 02:38:04+00:00
- **Authors**: Zhenhang Huang, Shihao Sun, Ruirui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection and instance segmentation in remote sensing images is a fundamental and challenging task, due to the complexity of scenes and targets. The latest methods tried to take into account both the efficiency and the accuracy of instance segmentation. In order to improve both of them, in this paper, we propose a single-shot convolutional neural network structure, which is conceptually simple and straightforward, and meanwhile makes up for the problem of low accuracy of single-shot networks. Our method, termed with SSS-Net, detects targets based on the location of the object's center and the distances between the center and the points on the silhouette sampling with non-uniform angle intervals, thereby achieving abalanced sampling of lines in mask generation. In addition, we propose a non-uniform polar template IoU based on the contour template in polar coordinates. Experiments on both the Airbus Ship Detection Challenge dataset and the ISAIDships dataset show that SSS-Net has strong competitiveness in precision and speed for ship instance segmentation.



### Color and Edge-Aware Adversarial Image Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2008.12454v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML, 68T45, 62F35
- **Links**: [PDF](http://arxiv.org/pdf/2008.12454v2)
- **Published**: 2020-08-28 03:02:20+00:00
- **Updated**: 2021-03-22 19:25:48+00:00
- **Authors**: Robert Bassett, Mitchell Graves, Patrick Reilly
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial perturbation of images, in which a source image is deliberately modified with the intent of causing a classifier to misclassify the image, provides important insight into the robustness of image classifiers. In this work we develop two new methods for constructing adversarial perturbations, both of which are motivated by minimizing human ability to detect changes between the perturbed and source image. The first of these, the Edge-Aware method, reduces the magnitude of perturbations permitted in smooth regions of an image where changes are more easily detected. Our second method, the Color-Aware method, performs the perturbation in a color space which accurately captures human ability to distinguish differences in colors, thus reducing the perceived change. The Color-Aware and Edge-Aware methods can also be implemented simultaneously, resulting in image perturbations which account for both human color perception and sensitivity to changes in homogeneous regions. Because Edge-Aware and Color-Aware modifications exist for many image perturbations techniques, we also focus on computation to demonstrate their potential for use within more complex perturbation schemes. We empirically demonstrate that the Color-Aware and Edge-Aware perturbations we consider effectively cause misclassification, are less distinguishable to human perception, and are as easy to compute as the most efficient image perturbation techniques. Code and demo available at https://github.com/rbassett3/Color-and-Edge-Aware-Perturbations



### Accelerated WGAN update strategy with loss change rate balancing
- **Arxiv ID**: http://arxiv.org/abs/2008.12463v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12463v2)
- **Published**: 2020-08-28 03:29:09+00:00
- **Updated**: 2020-11-03 01:45:11+00:00
- **Authors**: Xu Ouyang, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: Optimizing the discriminator in Generative Adversarial Networks (GANs) to completion in the inner training loop is computationally prohibitive, and on finite datasets would result in overfitting. To address this, a common update strategy is to alternate between k optimization steps for the discriminator D and one optimization step for the generator G. This strategy is repeated in various GAN algorithms where k is selected empirically. In this paper, we show that this update strategy is not optimal in terms of accuracy and convergence speed, and propose a new update strategy for Wasserstein GANs (WGAN) and other GANs using the WGAN loss(e.g. WGAN-GP, Deblur GAN, and Super-resolution GAN). The proposed update strategy is based on a loss change ratio comparison of G and D. We demonstrate that the proposed strategy improves both convergence speed and accuracy.



### Counting from Sky: A Large-scale Dataset for Remote Sensing Object Counting and A Benchmark Method
- **Arxiv ID**: http://arxiv.org/abs/2008.12470v1
- **DOI**: 10.1109/TGRS.2020.3020555
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12470v1)
- **Published**: 2020-08-28 03:47:49+00:00
- **Updated**: 2020-08-28 03:47:49+00:00
- **Authors**: Guangshuai Gao, Qingjie Liu, Yunhong Wang
- **Comment**: To be prepared in TGRS
- **Journal**: None
- **Summary**: Object counting, whose aim is to estimate the number of objects from a given image, is an important and challenging computation task. Significant efforts have been devoted to addressing this problem and achieved great progress, yet counting the number of ground objects from remote sensing images is barely studied. In this paper, we are interested in counting dense objects from remote sensing images. Compared with object counting in a natural scene, this task is challenging in the following factors: large scale variation, complex cluttered background, and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting dataset with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, large-vehicles and small-vehicles in parking lots. We then benchmark the dataset by designing a novel neural network that can generate a density map of an input image. The proposed network consists of three parts namely attention module, scale pyramid module and deformable convolution module to attack the aforementioned challenging factors. Extensive experiments are performed on the proposed dataset and one crowd counting datset, which demonstrate the challenges of the proposed dataset and the superiority and effectiveness of our method compared with state-of-the-art methods.



### Human Blastocyst Classification after In Vitro Fertilization Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.12480v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12480v1)
- **Published**: 2020-08-28 04:40:55+00:00
- **Updated**: 2020-08-28 04:40:55+00:00
- **Authors**: Ali Akbar Septiandri, Ade Jamal, Pritta Ameilia Iffanolida, Oki Riayati, Budi Wiweko
- **Comment**: None
- **Journal**: None
- **Summary**: Embryo quality assessment after in vitro fertilization (IVF) is primarily done visually by embryologists. Variability among assessors, however, remains one of the main causes of the low success rate of IVF. This study aims to develop an automated embryo assessment based on a deep learning model. This study includes a total of 1084 images from 1226 embryos. The images were captured by an inverted microscope at day 3 after fertilization. The images were labelled based on Veeck criteria that differentiate embryos to grade 1 to 5 based on the size of the blastomere and the grade of fragmentation. Our deep learning grading results were compared to the grading results from trained embryologists to evaluate the model performance. Our best model from fine-tuning a pre-trained ResNet50 on the dataset results in 91.79% accuracy. The model presented could be developed into an automated embryo assessment method in point-of-care settings.



### DALE : Dark Region-Aware Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2008.12493v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12493v1)
- **Published**: 2020-08-28 06:14:21+00:00
- **Updated**: 2020-08-28 06:14:21+00:00
- **Authors**: Dokyeong Kwon, Guisik Kim, Junseok Kwon
- **Comment**: 12 pages, 7 figures, The 31st British Machine Vision Conference
- **Journal**: None
- **Summary**: In this paper, we present a novel low-light image enhancement method called dark region-aware low-light image enhancement (DALE), where dark regions are accurately recognized by the proposed visual attention module and their brightness are intensively enhanced. Our method can estimate the visual attention in an efficient manner using super-pixels without any complicated process. Thus, the method can preserve the color, tone, and brightness of original images and prevents normally illuminated areas of the images from being saturated and distorted. Experimental results show that our method accurately identifies dark regions via the proposed visual attention, and qualitatively and quantitatively outperforms state-of-the-art methods.



### Few-Shot Object Detection via Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2008.12496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12496v1)
- **Published**: 2020-08-28 06:35:27+00:00
- **Updated**: 2020-08-28 06:35:27+00:00
- **Authors**: Geonuk Kim, Hong-Gyu Jung, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional methods for object detection usually require substantial amounts of training data and annotated bounding boxes. If there are only a few training data and annotations, the object detectors easily overfit and fail to generalize. It exposes the practical weakness of the object detectors. On the other hand, human can easily master new reasoning rules with only a few demonstrations using previously learned knowledge. In this paper, we introduce a few-shot object detection via knowledge transfer, which aims to detect objects from a few training examples. Central to our method is prototypical knowledge transfer with an attached meta-learner. The meta-learner takes support set images that include the few examples of the novel categories and base categories, and predicts prototypes that represent each category as a vector. Then, the prototypes reweight each RoI (Region-of-Interest) feature vector from a query image to remodels R-CNN predictor heads. To facilitate the remodeling process, we predict the prototypes under a graph structure, which propagates information of the correlated base categories to the novel categories with explicit guidance of prior knowledge that represents correlations among categories. Extensive experiments on the PASCAL VOC dataset verifies the effectiveness of the proposed method.



### Nonlocal Adaptive Direction-Guided Structure Tensor Total Variation For Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2008.12505v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12505v1)
- **Published**: 2020-08-28 06:58:35+00:00
- **Updated**: 2020-08-28 06:58:35+00:00
- **Authors**: Ezgi Demircan-Tureyen, Mustafa E. Kamasak
- **Comment**: 9 pages, 4 figures, article
- **Journal**: None
- **Summary**: A common strategy in variational image recovery is utilizing the nonlocal self-similarity (NSS) property, when designing energy functionals. One such contribution is nonlocal structure tensor total variation (NLSTV), which lies at the core of this study. This paper is concerned with boosting the NLSTV regularization term through the use of directional priors. More specifically, NLSTV is leveraged so that, at each image point, it gains more sensitivity in the direction that is presumed to have the minimum local variation. The actual difficulty here is capturing this directional information from the corrupted image. In this regard, we propose a method that employs anisotropic Gaussian kernels to estimate directional features to be later used by our proposed model. The experiments validate that our entire two-stage framework achieves better results than the NLSTV model and two other competing local models, in terms of visual and quantitative evaluation.



### Distortion-Adaptive Grape Bunch Counting for Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/2008.12511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12511v1)
- **Published**: 2020-08-28 07:17:34+00:00
- **Updated**: 2020-08-28 07:17:34+00:00
- **Authors**: Ryota Akai, Yuzuko Utsumi, Yuka Miwa, Masakazu Iwamura, Koichi Kise
- **Comment**: Accepted 25th International Conference on Pattern Recognition (ICPR
  2020)
- **Journal**: None
- **Summary**: This paper proposes the first object counting method for omnidirectional images. Because conventional object counting methods cannot handle the distortion of omnidirectional images, we propose to process them using stereographic projection, which enables conventional methods to obtain a good approximation of the density function. However, the images obtained by stereographic projection are still distorted. Hence, to manage this distortion, we propose two methods. One is a new data augmentation method designed for the stereographic projection of omnidirectional images. The other is a distortion-adaptive Gaussian kernel that generates a density map ground truth while taking into account the distortion of stereographic projection. Using the counting of grape bunches as a case study, we constructed an original grape-bunch image dataset consisting of omnidirectional images and conducted experiments to evaluate the proposed method. The results show that the proposed method performs better than a direct application of the conventional method, improving mean absolute error by 14.7% and mean squared error by 10.5%.



### A Dataset and Baselines for Visual Question Answering on Art
- **Arxiv ID**: http://arxiv.org/abs/2008.12520v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2008.12520v1)
- **Published**: 2020-08-28 07:33:30+00:00
- **Updated**: 2020-08-28 07:33:30+00:00
- **Authors**: Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu Otani, Chenhui Chu, Yuta Nakashima, Teruko Mitamura
- **Comment**: None
- **Journal**: None
- **Summary**: Answering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acquired through the study of the history of art. In this work, we introduce our first attempt towards building a new dataset, coined AQUA (Art QUestion Answering). The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. Our dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. We also present a two-branch model as baseline, where the visual and knowledge questions are handled independently. We extensively compare our baseline model against the state-of-the-art models for question answering, and we provide a comprehensive study about the challenges and potential future directions for visual question answering on art.



### Soft Tissue Sarcoma Co-Segmentation in Combined MRI and PET/CT Data
- **Arxiv ID**: http://arxiv.org/abs/2008.12544v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12544v2)
- **Published**: 2020-08-28 09:15:42+00:00
- **Updated**: 2020-09-24 09:10:17+00:00
- **Authors**: Theresa Neubauer, Maria Wimmer, Astrid Berg, David Major, Dimitrios Lenis, Thomas Beyer, Jelena Saponjski, Katja Bühler
- **Comment**: Accepted for publication at Multimodal Learning for Clinical Decision
  Support Workshop at MICCAI 2020 (edit: corrected typos and model name in Fig.
  3, added missing circles in Table 1)
- **Journal**: None
- **Summary**: Tumor segmentation in multimodal medical images has seen a growing trend towards deep learning based methods. Typically, studies dealing with this topic fuse multimodal image data to improve the tumor segmentation contour for a single imaging modality. However, they do not take into account that tumor characteristics are emphasized differently by each modality, which affects the tumor delineation. Thus, the tumor segmentation is modality- and task-dependent. This is especially the case for soft tissue sarcomas, where, due to necrotic tumor tissue, the segmentation differs vastly. Closing this gap, we develop a modalityspecific sarcoma segmentation model that utilizes multimodal image data to improve the tumor delineation on each individual modality. We propose a simultaneous co-segmentation method, which enables multimodal feature learning through modality-specific encoder and decoder branches, and the use of resource-effcient densely connected convolutional layers. We further conduct experiments to analyze how different input modalities and encoder-decoder fusion strategies affect the segmentation result. We demonstrate the effectiveness of our approach on public soft tissue sarcoma data, which comprises MRI (T1 and T2 sequence) and PET/CT scans. The results show that our multimodal co-segmentation model provides better modality-specific tumor segmentation than models using only the PET or MRI (T1 and T2) scan as input.



### Hierarchical Deep Learning Ensemble to Automate the Classification of Breast Cancer Pathology Reports by ICD-O Topography
- **Arxiv ID**: http://arxiv.org/abs/2008.12571v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12571v1)
- **Published**: 2020-08-28 10:29:56+00:00
- **Updated**: 2020-08-28 10:29:56+00:00
- **Authors**: Waheeda Saib, David Sengeh, Gcininwe Dlamini, Elvira Singh
- **Comment**: Accepted to KDD workshop on Machine Learning for Medicine and
  Healthcare, August 2018, London UK
- **Journal**: None
- **Summary**: Like most global cancer registries, the National Cancer Registry in South Africa employs expert human coders to label pathology reports using appropriate International Classification of Disease for Oncology (ICD-O) codes spanning 42 different cancer types. The annotation is extensive for the large volume of cancer pathology reports the registry receives annually from public and private sector institutions. This manual process, coupled with other challenges results in a significant 4-year lag in reporting of annual cancer statistics in South Africa. We present a hierarchical deep learning ensemble method incorporating state of the art convolutional neural network models for the automatic labelling of 2201 de-identified, free text pathology reports, with appropriate ICD-O breast cancer topography codes across 8 classes. Our results show an improvement in primary site classification over the state of the art CNN model by greater than 14% for F1 micro and 55% for F1 macro scores. We demonstrate that the hierarchical deep learning ensemble improves on state-of-the-art models for ICD-O topography classification in comparison to a flat multiclass model for predicting ICD-O topography codes for pathology reports.



### Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2008.12577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12577v1)
- **Published**: 2020-08-28 10:49:28+00:00
- **Updated**: 2020-08-28 10:49:28+00:00
- **Authors**: Marco Rudolph, Bastian Wandt, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question. To this end, we propose DifferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing flows. Normalizing flows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing flow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the image enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD and Magnetic Tile Defects datasets.



### PV-RCNN: The Top-Performing LiDAR-only Solutions for 3D Detection / 3D Tracking / Domain Adaptation of Waymo Open Dataset Challenges
- **Arxiv ID**: http://arxiv.org/abs/2008.12599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12599v1)
- **Published**: 2020-08-28 12:05:26+00:00
- **Updated**: 2020-08-28 12:05:26+00:00
- **Authors**: Shaoshuai Shi, Chaoxu Guo, Jihan Yang, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we present the top-performing LiDAR-only solutions for 3D detection, 3D tracking and domain adaptation three tracks in Waymo Open Dataset Challenges 2020. Our solutions for the competition are built upon our recent proposed PV-RCNN 3D object detection framework. Several variants of our PV-RCNN are explored, including temporal information incorporation, dynamic voxelization, adaptive training sample selection, classification with RoI features, etc. A simple model ensemble strategy with non-maximum-suppression and box voting is adopted to generate the final results. By using only LiDAR point cloud data, our models finally achieve the 1st place among all LiDAR-only methods, and the 2nd place among all multi-modal methods, on the 3D Detection, 3D Tracking and Domain Adaptation three tracks of Waymo Open Dataset Challenges. Our solutions will be available at https://github.com/open-mmlab/OpenPCDet



### The Effects of Skin Lesion Segmentation on the Performance of Dermatoscopic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.12602v1
- **DOI**: 10.1016/j.cmpb.2020.105725
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12602v1)
- **Published**: 2020-08-28 12:17:25+00:00
- **Updated**: 2020-08-28 12:17:25+00:00
- **Authors**: Amirreza Mahbod, Philipp Tschandl, Georg Langs, Rupert Ecker, Isabella Ellinger
- **Comment**: Accepted to the Computer Methods and Programs in Biomedicine journal
- **Journal**: None
- **Summary**: Malignant melanoma (MM) is one of the deadliest types of skin cancer. Analysing dermatoscopic images plays an important role in the early detection of MM and other pigmented skin lesions. Among different computer-based methods, deep learning-based approaches and in particular convolutional neural networks have shown excellent classification and segmentation performances for dermatoscopic skin lesion images. These models can be trained end-to-end without requiring any hand-crafted features. However, the effect of using lesion segmentation information on classification performance has remained an open question. In this study, we explicitly investigated the impact of using skin lesion segmentation masks on the performance of dermatoscopic image classification. To do this, first, we developed a baseline classifier as the reference model without using any segmentation masks. Then, we used either manually or automatically created segmentation masks in both training and test phases in different scenarios and investigated the classification performances. Evaluated on the ISIC 2017 challenge dataset which contained two binary classification tasks (i.e. MM vs. all and seborrheic keratosis (SK) vs. all) and based on the derived area under the receiver operating characteristic curve scores, we observed four main outcomes. Our results show that 1) using segmentation masks did not significantly improve the MM classification performance in any scenario, 2) in one of the scenarios (using segmentation masks for dilated cropping), SK classification performance was significantly improved, 3) removing all background information by the segmentation masks significantly degraded the overall classification performance, and 4) in case of using the appropriate scenario (using segmentation for dilated cropping), there is no significant difference of using manually or automatically created segmentation masks.



### A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis
- **Arxiv ID**: http://arxiv.org/abs/2008.12603v1
- **DOI**: 10.1038/s41598-020-71639-x
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12603v1)
- **Published**: 2020-08-28 12:20:59+00:00
- **Updated**: 2020-08-28 12:20:59+00:00
- **Authors**: Alzayat Saleh, Issam H. Laradji, Dmitry A. Konovalov, Michael Bradley, David Vazquez, Marcus Sheaves
- **Comment**: 10 pages, 5 figures, 3 tables, Accepted for Publication in Scientific
  Reports (Nature) 14 August 2020
- **Journal**: None
- **Summary**: Visual analysis of complex fish habitats is an important step towards sustainable fisheries for human consumption and environmental protection. Deep Learning methods have shown great promise for scene analysis when trained on large-scale datasets. However, current datasets for fish analysis tend to focus on the classification task within constrained, plain environments which do not capture the complexity of underwater fish habitats. To address this limitation, we present DeepFish as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 \green{habitats in the} marine-environments of tropical Australia. The dataset originally contained only classification labels. Thus, we collected point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes. Our experiments provide an in-depth analysis of the dataset characteristics, and the performance evaluation of several state-of-the-art approaches based on our benchmark. Although models pre-trained on ImageNet have successfully performed on this benchmark, there is still room for improvement. Therefore, this benchmark serves as a testbed to motivate further development in this challenging domain of underwater computer vision. Code is available at: https://github.com/alzayats/DeepFish



### Next-Best View Policy for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.12664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.12664v2)
- **Published**: 2020-08-28 14:03:59+00:00
- **Updated**: 2020-09-06 15:30:45+00:00
- **Authors**: Daryl Peralta, Joel Casimiro, Aldrin Michael Nilles, Justine Aletta Aguilar, Rowel Atienza, Rhandley Cajote
- **Comment**: To be published in ECCV 2020 Workshops; typos in abstract corrected
- **Journal**: None
- **Summary**: Manually selecting viewpoints or using commonly available flight planners like circular path for large-scale 3D reconstruction using drones often results in incomplete 3D models. Recent works have relied on hand-engineered heuristics such as information gain to select the Next-Best Views. In this work, we present a learning-based algorithm called Scan-RL to learn a Next-Best View (NBV) Policy. To train and evaluate the agent, we created Houses3K, a dataset of 3D house models. Our experiments show that using Scan-RL, the agent can scan houses with fewer number of steps and a shorter distance compared to our baseline circular path. Experimental results also demonstrate that a single NBV policy can be used to scan multiple houses including those that were not seen during training. The link to Scan-RL is available at https://github.com/darylperalta/ScanRL and Houses3K dataset can be found at https://github.com/darylperalta/Houses3K.



### Person-in-Context Synthesiswith Compositional Structural Space
- **Arxiv ID**: http://arxiv.org/abs/2008.12679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12679v1)
- **Published**: 2020-08-28 14:33:28+00:00
- **Updated**: 2020-08-28 14:33:28+00:00
- **Authors**: Weidong Yin, Ziwei Liu, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant progress, controlled generation of complex images with interacting people remains difficult. Existing layout generation methods fall short of synthesizing realistic person instances; while pose-guided generation approaches focus on a single person and assume simple or known backgrounds. To tackle these limitations, we propose a new problem, \textbf{Persons in Context Synthesis}, which aims to synthesize diverse person instance(s) in consistent contexts, with user control over both. The context is specified by the bounding box object layout which lacks shape information, while pose of the person(s) by keypoints which are sparsely annotated. To handle the stark difference in input structures, we proposed two separate neural branches to attentively composite the respective (context/person) inputs into shared ``compositional structural space'', which encodes shape, location and appearance information for both context and person structures in a disentangled manner. This structural space is then decoded to the image space using multi-level feature modulation strategy, and learned in a self supervised manner from image collections and their corresponding inputs. Extensive experiments on two large-scale datasets (COCO-Stuff \cite{caesar2018cvpr} and Visual Genome \cite{krishna2017visual}) demonstrate that our framework outperforms state-of-the-art methods w.r.t. synthesis quality.



### Bayesian Neural Networks for Uncertainty Estimation of Imaging Biomarkers
- **Arxiv ID**: http://arxiv.org/abs/2008.12680v2
- **DOI**: 10.1007/978-3-030-59861-7_28
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12680v2)
- **Published**: 2020-08-28 14:34:12+00:00
- **Updated**: 2020-09-01 23:22:54+00:00
- **Authors**: J. Senapati, A. Guha Roy, S. Pölsterl, D. Gutmann, S. Gatidis, C. Schlett, A. Peters, F. Bamberg, C. Wachinger
- **Comment**: MICCAI-MLMI 2020 Workshop Paper (Accepted)
- **Journal**: None
- **Summary**: Image segmentation enables to extract quantitative measures from scans that can serve as imaging biomarkers for diseases. However, segmentation quality can vary substantially across scans, and therefore yield unfaithful estimates in the follow-up statistical analysis of biomarkers. The core problem is that segmentation and biomarker analysis are performed independently. We propose to propagate segmentation uncertainty to the statistical analysis to account for variations in segmentation confidence. To this end, we evaluate four Bayesian neural networks to sample from the posterior distribution and estimate the uncertainty. We then assign confidence measures to the biomarker and propose statistical models for its integration in group analysis and disease classification. Our results for segmenting the liver in patients with diabetes mellitus clearly demonstrate the improvement of integrating biomarker uncertainty in the statistical inference.



### On the Reliability of the PNU for Source Camera Identification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2008.12700v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.12700v1)
- **Published**: 2020-08-28 15:15:31+00:00
- **Updated**: 2020-08-28 15:15:31+00:00
- **Authors**: Andrea Bruno, Giuseppe Cattaneo, Paola Capasso
- **Comment**: 14 pages, 7 figures, to be presented to IWBDAF on 2021, 11 Jan
- **Journal**: None
- **Summary**: The PNU is an essential and reliable tool to perform SCI and, during the years, became a standard de-facto for this task in the forensic field. In this paper, we show that, although strategies exist that aim to cancel, modify, replace the PNU traces in a digital camera image, it is still possible, through our experimental method, to find residual traces of the noise produced by the sensor used to shoot the photo. Furthermore, we show that is possible to inject the PNU of a different camera in a target image and trace it back to the source camera, but only under the condition that the new camera is of the same model of the original one used to take the target image. Both cameras must fall within our availability.   For completeness, we carried out 2 experiments and, rather than using the popular public reference dataset, CASIA TIDE, we preferred to introduce a dataset that does not present any kind of statistical artifacts.   A preliminary experiment on a small dataset of smartphones showed that the injection of PNU from a different device makes it impossible to identify the source camera correctly.   For a second experiment, we built a large dataset of images taken with the same model DSLR. We extracted a denoised version of each image, injected each one with the RN of all the cameras in the dataset and compared all with a RP from each camera. The results of the experiments, clearly, show that either in the denoised images and the injected ones is possible to find residual traces of the original camera PNU.   The combined results of the experiments show that, even in theory is possible to remove or replace the \ac{PNU} from an image, this process can be, easily, detected and is possible, under some hard conditions, confirming the robustness of the \ac{PNU} under this type of attacks.



### Canonical 3D Deformer Maps: Unifying parametric and non-parametric methods for dense weakly-supervised category reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.12709v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.12709v2)
- **Published**: 2020-08-28 15:44:05+00:00
- **Updated**: 2020-12-06 11:59:06+00:00
- **Authors**: David Novotny, Roman Shapovalov, Andrea Vedaldi
- **Comment**: Published at NeurIPS 2020
- **Journal**: None
- **Summary**: We propose the Canonical 3D Deformer Map, a new representation of the 3D shape of common object categories that can be learned from a collection of 2D images of independent objects. Our method builds in a novel way on concepts from parametric deformation models, non-parametric 3D reconstruction, and canonical embeddings, combining their individual advantages. In particular, it learns to associate each image pixel with a deformation model of the corresponding 3D object point which is canonical, i.e. intrinsic to the identity of the point and shared across objects of the category. The result is a method that, given only sparse 2D supervision at training time, can, at test time, reconstruct the 3D shape and texture of objects from single views, while establishing meaningful dense correspondences between object instances. It also achieves state-of-the-art results in dense 3D reconstruction on public in-the-wild datasets of faces, cars, and birds.



### CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2008.12750v3
- **DOI**: 10.1109/TUFFC.2021.3131383
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12750v3)
- **Published**: 2020-08-28 17:15:37+00:00
- **Updated**: 2022-04-01 17:45:10+00:00
- **Authors**: Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi, Jean-Philippe Thiran
- **Comment**: Main: 14 pages (6 figures). Supplement: 9 pages (10 figures). Video
  provided as ancillary file. This version has been accepted for publication in
  the the IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency
  Control
- **Journal**: None
- **Summary**: Ultrafast ultrasound (US) revolutionized biomedical imaging with its capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough modalities such as shear-wave elastography and functional US neuroimaging. Yet, it suffers from strong diffraction artifacts, mainly caused by grating lobes, side lobes, or edge waves. Multiple acquisitions are typically required to obtain a sufficient image quality, at the cost of a reduced frame rate. To answer the increasing demand for high-quality imaging from single unfocused acquisitions, we propose a two-step convolutional neural network (CNN)-based image reconstruction method, compatible with real-time imaging. A low-quality estimate is obtained by means of a backprojection-based operation, akin to conventional delay-and-sum beamforming, from which a high-quality image is restored using a residual CNN with multiscale and multichannel filtering properties, trained specifically to remove the diffraction artifacts inherent to ultrafast US imaging. To account for both the high dynamic range and the oscillating properties of radio frequency US images, we introduce the mean signed logarithmic absolute error (MSLAE) as a training loss function. Experiments were conducted with a linear transducer array, in single plane-wave (PW) imaging. Trainings were performed on a simulated dataset, crafted to contain a wide diversity of structures and echogenicities. Extensive numerical evaluations demonstrate that the proposed approach can reconstruct images from single PWs with a quality similar to that of gold-standard synthetic aperture imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo experiments show that trainings carried out on simulated data perform well in experimental settings.



### AllenAct: A Framework for Embodied AI Research
- **Arxiv ID**: http://arxiv.org/abs/2008.12760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.12760v1)
- **Published**: 2020-08-28 17:35:22+00:00
- **Updated**: 2020-08-28 17:35:22+00:00
- **Authors**: Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, Aniruddha Kembhavi
- **Comment**: None
- **Journal**: None
- **Summary**: The domain of Embodied AI, in which agents learn to complete tasks through interaction with their environment from egocentric observations, has experienced substantial growth with the advent of deep reinforcement learning and increased interest from the computer vision, NLP, and robotics communities. This growth has been facilitated by the creation of a large number of simulated environments (such as AI2-THOR, Habitat and CARLA), tasks (like point navigation, instruction following, and embodied question answering), and associated leaderboards. While this diversity has been beneficial and organic, it has also fragmented the community: a huge amount of effort is required to do something as simple as taking a model trained in one environment and testing it in another. This discourages good science. We introduce AllenAct, a modular and flexible learning framework designed with a focus on the unique requirements of Embodied AI research. AllenAct provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. We hope that our framework makes Embodied AI more accessible and encourages new researchers to join this exciting area. The framework can be accessed at: https://allenact.org/



### ChildBot: Multi-Robot Perception and Interaction with Children
- **Arxiv ID**: http://arxiv.org/abs/2008.12818v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2008.12818v1)
- **Published**: 2020-08-28 19:07:28+00:00
- **Updated**: 2020-08-28 19:07:28+00:00
- **Authors**: Niki Efthymiou, Panagiotis P. Filntisis, Petros Koutras, Antigoni Tsiami, Jack Hadfield, Gerasimos Potamianos, Petros Maragos
- **Comment**: 19 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper we present an integrated robotic system capable of participating in and performing a wide range of educational and entertainment tasks, in collaboration with one or more children. The system, called ChildBot, features multimodal perception modules and multiple robotic agents that monitor the interaction environment, and can robustly coordinate complex Child-Robot Interaction use-cases. In order to validate the effectiveness of the system and its integrated modules, we have conducted multiple experiments with a total of 52 children. Our results show improved perception capabilities in comparison to our earlier works that ChildBot was based on. In addition, we have conducted a preliminary user experience study, employing some educational/entertainment tasks, that yields encouraging results regarding the technical validity of our system and initial insights on the user experience with it.



### Learning to Balance Specificity and Invariance for In and Out of Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2008.12839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12839v1)
- **Published**: 2020-08-28 20:39:51+00:00
- **Updated**: 2020-08-28 20:39:51+00:00
- **Authors**: Prithvijit Chattopadhyay, Yogesh Balaji, Judy Hoffman
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: We introduce Domain-specific Masks for Generalization, a model for improving both in-domain and out-of-domain generalization performance. For domain generalization, the goal is to learn from a set of source domains to produce a single model that will best generalize to an unseen target domain. As such, many prior approaches focus on learning representations which persist across all source domains with the assumption that these domain agnostic representations will generalize well. However, often individual domains contain characteristics which are unique and when leveraged can significantly aid in-domain recognition performance. To produce a model which best generalizes to both seen and unseen domains, we propose learning domain specific masks. The masks are encouraged to learn a balance of domain-invariant and domain-specific features, thus enabling a model which can benefit from the predictive power of specialized features while retaining the universal applicability of domain-invariant features. We demonstrate competitive performance compared to naive baselines and state-of-the-art methods on both PACS and DomainNet.



### Using Machine Learning for Particle Track Identification in the CLAS12 Detector
- **Arxiv ID**: http://arxiv.org/abs/2008.12860v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2008.12860v2)
- **Published**: 2020-08-28 21:57:38+00:00
- **Updated**: 2022-04-28 13:51:43+00:00
- **Authors**: Polykarpos Thomadakis, Angelos Angelopoulos, Gagik Gavalian, Nikos Chrisochoides
- **Comment**: None
- **Journal**: None
- **Summary**: Particle track reconstruction is the most computationally intensive process in nuclear physics experiments. Traditional algorithms use a combinatorial approach that exhaustively tests track measurements ("hits") to identify those that form an actual particle trajectory. In this article, we describe the development of four machine learning (ML) models that assist the tracking algorithm by identifying valid track candidates from the measurements in drift chambers. Several types of machine learning models were tested, including: Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Extremely Randomized Trees (ERT) and Recurrent Neural Networks (RNN). As a result of this work, an MLP network classifier was implemented as part of the CLAS12 reconstruction software to provide the tracking code with recommended track candidates. The resulting software achieved accuracy of greater than 99\% and resulted in an end-to-end speedup of 35\% compared to existing algorithms.



### Background Splitting: Finding Rare Classes in a Sea of Background
- **Arxiv ID**: http://arxiv.org/abs/2008.12873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.12873v1)
- **Published**: 2020-08-28 23:05:15+00:00
- **Updated**: 2020-08-28 23:05:15+00:00
- **Authors**: Ravi Teja Mullapudi, Fait Poms, William R. Mark, Deva Ramanan, Kayvon Fatahalian
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the real-world problem of training accurate deep models for image classification of a small number of rare categories. In these scenarios, almost all images belong to the background category in the dataset (>95% of the dataset is background). We demonstrate that both standard fine-tuning approaches and state-of-the-art approaches for training on imbalanced datasets do not produce accurate deep models in the presence of this extreme imbalance. Our key observation is that the extreme imbalance due to the background category can be drastically reduced by leveraging visual knowledge from an existing pre-trained model. Specifically, the background category is "split" into smaller and more coherent pseudo-categories during training using a pre-trained model. We incorporate background splitting into an image classification model by adding an auxiliary loss that learns to mimic the predictions of the existing, pre-trained image classification model. Note that this process is automatic and requires no additional manual labels. The auxiliary loss regularizes the feature representation of the shared network trunk by requiring it to discriminate between previously homogeneous background instances and reduces overfitting to the small number of rare category positives. We also show that BG splitting can be combined with other background imbalance methods to further improve performance. We evaluate our method on a modified version of the iNaturalist dataset where only a small subset of rare category labels are available during training (all other images are labeled as background). By jointly learning to recognize ImageNet categories and selected iNaturalist categories, our approach yields performance that is 42.3 mAP points higher than a fine-tuning baseline when 99.98% of the data is background, and 8.3 mAP points higher than SotA baselines when 98.30% of the data is background.



