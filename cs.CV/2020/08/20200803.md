# Arxiv Papers in cs.CV on 2020-08-03
### The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study
- **Arxiv ID**: http://arxiv.org/abs/2008.00605v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.2; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2008.00605v1)
- **Published**: 2020-08-03 01:39:01+00:00
- **Updated**: 2020-08-03 01:39:01+00:00
- **Authors**: Xiyang Luo, Hossein Talebi, Feng Yang, Michael Elad, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Handling digital images is almost always accompanied by a lossy compression in order to facilitate efficient transmission and storage. This introduces an unavoidable tension between the allocated bit-budget (rate) and the faithfulness of the resulting image to the original one (distortion). An additional complicating consideration is the effect of the compression on recognition performance by given classifiers (accuracy). This work aims to explore this rate-distortion-accuracy tradeoff. As a case study, we focus on the design of the quantization tables in the JPEG compression standard. We offer a novel optimal tuning of these tables via continuous optimization, leveraging a differential implementation of both the JPEG encoder-decoder and an entropy estimator. This enables us to offer a unified framework that considers the interplay between rate, distortion and classification accuracy. In all these fronts, we report a substantial boost in performance by a simple and easily implemented modification of these tables.



### Robust Collaborative Learning of Patch-level and Image-level Annotations for Diabetic Retinopathy Grading from Fundus Image
- **Arxiv ID**: http://arxiv.org/abs/2008.00610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00610v2)
- **Published**: 2020-08-03 02:17:42+00:00
- **Updated**: 2021-03-18 07:35:45+00:00
- **Authors**: Yehui Yang, Fangxin Shang, Binghong Wu, Dalu Yang, Lei Wang, Yanwu Xu, Wensheng Zhang, Tianzhu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) grading from fundus images has attracted increasing interest in both academic and industrial communities. Most convolutional neural network (CNN) based algorithms treat DR grading as a classification task via image-level annotations. However, these algorithms have not fully explored the valuable information in the DR-related lesions. In this paper, we present a robust framework, which collaboratively utilizes patch-level and image-level annotations, for DR severity grading. By an end-to-end optimization, this framework can bi-directionally exchange the fine-grained lesion and image-level grade information. As a result, it exploits more discriminative features for DR grading. The proposed framework shows better performance than the recent state-of-the-art algorithms and three clinical ophthalmologists with over nine years of experience. By testing on datasets of different distributions (such as label and camera), we prove that our algorithm is robust when facing image quality and distribution variations that commonly exist in real-world practice. We inspect the proposed framework through extensive ablation studies to indicate the effectiveness and necessity of each motivation. The code and some valuable annotations are now publicly available.



### Learning to Purify Noisy Labels via Meta Soft Label Corrector
- **Arxiv ID**: http://arxiv.org/abs/2008.00627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00627v1)
- **Published**: 2020-08-03 03:25:17+00:00
- **Updated**: 2020-08-03 03:25:17+00:00
- **Authors**: Yichen Wu, Jun Shu, Qi Xie, Qian Zhao, Deyu Meng
- **Comment**: 12 pages,6 figures
- **Journal**: AAAI 2021
- **Summary**: Recent deep neural networks (DNNs) can easily overfit to biased training data with noisy labels. Label correction strategy is commonly used to alleviate this issue by designing a method to identity suspected noisy labels and then correct them. Current approaches to correcting corrupted labels usually need certain pre-defined label correction rules or manually preset hyper-parameters. These fixed settings make it hard to apply in practice since the accurate label correction usually related with the concrete problem, training data and the temporal information hidden in dynamic iterations of training process. To address this issue, we propose a meta-learning model which could estimate soft labels through meta-gradient descent step under the guidance of noise-free meta data. By viewing the label correction procedure as a meta-process and using a meta-learner to automatically correct labels, we could adaptively obtain rectified soft labels iteratively according to current training problems without manually preset hyper-parameters. Besides, our method is model-agnostic and we can combine it with any other existing model with ease. Comprehensive experiments substantiate the superiority of our method in both synthetic and real-world problems with noisy labels compared with current SOTA label correction strategies.



### Deep Photo Cropper and Enhancer
- **Arxiv ID**: http://arxiv.org/abs/2008.00634v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00634v1)
- **Published**: 2020-08-03 03:50:20+00:00
- **Updated**: 2020-08-03 03:50:20+00:00
- **Authors**: Aaron Ott, Amir Mazaheri, Niels D. Lobo, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new type of image enhancement problem. Compared to traditional image enhancement methods, which mostly deal with pixel-wise modifications of a given photo, our proposed task is to crop an image which is embedded within a photo and enhance the quality of the cropped image. We split our proposed approach into two deep networks: deep photo cropper and deep image enhancer. In the photo cropper network, we employ a spatial transformer to extract the embedded image. In the photo enhancer, we employ super-resolution to increase the number of pixels in the embedded image and reduce the effect of stretching and distortion of pixels. We use cosine distance loss between image features and ground truth for the cropper and the mean square loss for the enhancer. Furthermore, we propose a new dataset to train and test the proposed method. Finally, we analyze the proposed method with respect to qualitative and quantitative evaluations.



### Self-supervised Object Tracking with Cycle-consistent Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.00637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00637v1)
- **Published**: 2020-08-03 04:10:38+00:00
- **Updated**: 2020-08-03 04:10:38+00:00
- **Authors**: Weihao Yuan, Michael Yu Wang, Qifeng Chen
- **Comment**: 2020 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: Self-supervised learning for visual object tracking possesses valuable advantages compared to supervised learning, such as the non-necessity of laborious human annotations and online training. In this work, we exploit an end-to-end Siamese network in a cycle-consistent self-supervised framework for object tracking. Self-supervision can be performed by taking advantage of the cycle consistency in the forward and backward tracking. To better leverage the end-to-end learning of deep networks, we propose to integrate a Siamese region proposal and mask regression network in our tracking framework so that a fast and more accurate tracker can be learned without the annotation of each frame. The experiments on the VOT dataset for visual object tracking and on the DAVIS dataset for video object segmentation propagation show that our method outperforms prior approaches on both tasks.



### High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands
- **Arxiv ID**: http://arxiv.org/abs/2008.00638v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.00638v1)
- **Published**: 2020-08-03 04:12:31+00:00
- **Updated**: 2020-08-03 04:12:31+00:00
- **Authors**: Dibakar Gope, Jesse Beu, Matthew Mattina
- **Comment**: None
- **Journal**: None
- **Summary**: Matrix multiplications between asymmetric bit-width operands, especially between 8- and 4-bit operands are likely to become a fundamental kernel of many important workloads including neural networks and machine learning. While existing SIMD matrix multiplication instructions for symmetric bit-width operands can support operands of mixed precision by zero- or sign-extending the narrow operand to match the size of the other operands, they cannot exploit the benefit of narrow bit-width of one of the operands. We propose a new SIMD matrix multiplication instruction that uses mixed precision on its inputs (8- and 4-bit operands) and accumulates product values into narrower 16-bit output accumulators, in turn allowing the SIMD operation at 128-bit vector width to process a greater number of data elements per instruction to improve processing throughput and memory bandwidth utilization without increasing the register read- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size SIMD instruction offers 2x improvement in throughput of matrix multiplication in comparison to throughput obtained using existing symmetric-operand-size instructions while causing negligible (0.05%) overflow from 16-bit accumulators for representative machine learning workloads. The asymmetric-operand-size instruction not only can improve matrix multiplication throughput in CPUs, but also can be effective to support multiply-and-accumulate (MAC) operation between 8- and 4-bit operands in state-of-the-art DNN hardware accelerators (e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar improvement in matrix multiply performance seamlessly without violating the various implementation constraints. We demonstrate how a systolic array architecture designed for symmetric-operand-size instructions could be modified to support an asymmetric-operand-sized instruction.



### PIC-Net: Point Cloud and Image Collaboration Network for Large-Scale Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.00658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00658v1)
- **Published**: 2020-08-03 05:58:00+00:00
- **Updated**: 2020-08-03 05:58:00+00:00
- **Authors**: Yuheng Lu, Fan Yang, Fangping Chen, Don Xie
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Place recognition is one of the hot research fields in automation technology and is still an open issue, Camera and Lidar are two mainstream sensors used in this task, Camera-based methods are easily affected by illumination and season changes, LIDAR cannot get the rich data as the image could , In this paper, we propose the PIC-Net (Point cloud and Image Collaboration Network), which use attention mechanism to fuse the features of image and point cloud, and mine the complementary information between the two. Furthermore, in order to improve the recognition performance at night, we transform the night image into the daytime style. Comparison results show that the collaboration of image and point cloud outperform both image-based and point cloud-based method, the attention strategy and day-night-transform could further improve the performance.



### The pursuit of beauty: Converting image labels to meaningful vectors
- **Arxiv ID**: http://arxiv.org/abs/2008.00665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, ASM-class: I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.00665v1)
- **Published**: 2020-08-03 06:33:11+00:00
- **Updated**: 2020-08-03 06:33:11+00:00
- **Authors**: Savvas Karatsiolis, Andreas Kamilaris
- **Comment**: 20 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: A challenge of the computer vision community is to understand the semantics of an image, in order to allow image reconstruction based on existing high-level features or to better analyze (semi-)labelled datasets. Towards addressing this challenge, this paper introduces a method, called Occlusion-based Latent Representations (OLR), for converting image labels to meaningful representations that capture a significant amount of data semantics. Besides being informational rich, these representations compose a disentangled low-dimensional latent space where each image label is encoded into a separate vector. We evaluate the quality of these representations in a series of experiments whose results suggest that the proposed model can capture data concepts and discover data interrelations.



### Adversarial Semantic Data Augmentation for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.00697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00697v1)
- **Published**: 2020-08-03 07:56:04+00:00
- **Updated**: 2020-08-03 07:56:04+00:00
- **Authors**: Yanrui Bin, Xuan Cao, Xinya Chen, Yanhao Ge, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Changxin Gao, Nong Sang
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation is the task of localizing body keypoints from still images. The state-of-the-art methods suffer from insufficient examples of challenging cases such as symmetric appearance, heavy occlusion and nearby person. To enlarge the amounts of challenging cases, previous methods augmented images by cropping and pasting image patches with weak semantics, which leads to unrealistic appearance and limited diversity. We instead propose Semantic Data Augmentation (SDA), a method that augments images by pasting segmented body parts with various semantic granularity. Furthermore, we propose Adversarial Semantic Data Augmentation (ASDA), which exploits a generative network to dynamiclly predict tailored pasting configuration. Given off-the-shelf pose estimation network as discriminator, the generator seeks the most confusing transformation to increase the loss of the discriminator while the discriminator takes the generated sample as input and learns from it. The whole pipeline is optimized in an adversarial manner. State-of-the-art results are achieved on challenging benchmarks.



### Anti-Bandit Neural Architecture Search for Model Defense
- **Arxiv ID**: http://arxiv.org/abs/2008.00698v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00698v2)
- **Published**: 2020-08-03 07:59:39+00:00
- **Updated**: 2020-08-05 08:33:48+00:00
- **Authors**: Hanlin Chen, Baochang Zhang, Song Xue, Xuan Gong, Hong Liu, Rongrong Ji, David Doermann
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have dominated as the best performers in machine learning, but can be challenged by adversarial attacks. In this paper, we defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters and convolutions. The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper confidence bounds (LCB and UCB). Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search efficiency and LCB for a fair competition between arms. Extensive experiments demonstrate that ABanditNAS is faster than other NAS methods, while achieving an $8.73\%$ improvement over prior arts on CIFAR-10 under PGD-$7$.



### Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2008.00710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00710v1)
- **Published**: 2020-08-03 08:25:59+00:00
- **Updated**: 2020-08-03 08:25:59+00:00
- **Authors**: Yuting He, Tiantian Li, Guanyu Yang, Youyong Kong, Yang Chen, Huazhong Shu, Jean-Louis Coatrieux, Jean-Louis Dillenseger, Shuo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions' disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge show great advantages of our DeepRS that outperforms the existing state-of-the-art models.



### AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2008.00714v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00714v5)
- **Published**: 2020-08-03 08:40:01+00:00
- **Updated**: 2021-07-06 14:06:06+00:00
- **Authors**: Wenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, Zhibo Yang, Tong Lu, Chunhua Shen, Ping Luo
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Scene text spotting aims to detect and recognize the entire word or sentence with multiple characters in natural images. It is still challenging because ambiguity often occurs when the spacing between characters is large or the characters are evenly spread in multiple rows and columns, making many visually plausible groupings of the characters (e.g. "BERLIN" is incorrectly detected as "BERL" and "IN" in Fig. 1(c)). Unlike previous works that merely employed visual features for text detection, this work proposes a novel text spotter, named Ambiguity Eliminating Text Spotter (AE TextSpotter), which learns both visual and linguistic features to significantly reduce ambiguity in text detection. The proposed AE TextSpotter has three important benefits. 1) The linguistic representation is learned together with the visual representation in a framework. To our knowledge, it is the first time to improve text detection by using a language model. 2) A carefully designed language module is utilized to reduce the detection confidence of incorrect text lines, making them easily pruned in the detection stage. 3) Extensive experiments show that AE TextSpotter outperforms other state-of-the-art methods by a large margin. For example, we carefully select a validation set of extremely ambiguous samples from the IC19-ReCTS dataset, where our approach surpasses other methods by more than 4%. The code has been released at https://github.com/whai362/AE_TextSpotter. The image list and evaluation scripts of the validation set have been released at https://github.com/whai362/TDA-ReCTS.



### The End-of-End-to-End: A Video Understanding Pentathlon Challenge (2020)
- **Arxiv ID**: http://arxiv.org/abs/2008.00744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00744v1)
- **Published**: 2020-08-03 09:55:26+00:00
- **Updated**: 2020-08-03 09:55:26+00:00
- **Authors**: Samuel Albanie, Yang Liu, Arsha Nagrani, Antoine Miech, Ernesto Coto, Ivan Laptev, Rahul Sukthankar, Bernard Ghanem, Andrew Zisserman, Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid, Shizhe Chen, Yida Zhao, Qin Jin, Kaixu Cui, Hui Liu, Chen Wang, Yudong Jiang, Xiaoshuai Hao
- **Comment**: Individual reports, dataset information, rules, and released source
  code can be found at the competition webpage
  (https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon)
- **Journal**: None
- **Summary**: We present a new video understanding pentathlon challenge, an open competition held in conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020. The objective of the challenge was to explore and evaluate new methods for text-to-video retrieval-the task of searching for content within a corpus of videos using natural language queries. This report summarizes the results of the first edition of the challenge together with the findings of the participants.



### GmFace: A Mathematical Model for Face Image Representation Using Multi-Gaussian
- **Arxiv ID**: http://arxiv.org/abs/2008.00752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00752v1)
- **Published**: 2020-08-03 10:11:10+00:00
- **Updated**: 2020-08-03 10:11:10+00:00
- **Authors**: Liping Zhang, Weijun Li, Lina Yu, Xiaoli Dong, Linjun Sun, Xin Ning, Jian Xu, Hong Qin
- **Comment**: 12 pages, 12 figures, 4 tables
- **Journal**: None
- **Summary**: Establishing mathematical models is a ubiquitous and effective method to understand the objective world. Due to complex physiological structures and dynamic behaviors, mathematical representation of the human face is an especially challenging task. A mathematical model for face image representation called GmFace is proposed in the form of a multi-Gaussian function in this paper. The model utilizes the advantages of two-dimensional Gaussian function which provides a symmetric bell surface with a shape that can be controlled by parameters. The GmNet is then designed using Gaussian functions as neurons, with parameters that correspond to each of the parameters of GmFace in order to transform the problem of GmFace parameter solving into a network optimization problem of GmNet. The face modeling process can be described by the following steps: (1) GmNet initialization; (2) feeding GmNet with face image(s); (3) training GmNet until convergence; (4) drawing out the parameters of GmNet (as the same as GmFace); (5) recording the face model GmFace. Furthermore, using GmFace, several face image transformation operations can be realized mathematically through simple parameter computation.



### IntroVAC: Introspective Variational Classifiers for Learning Interpretable Latent Subspaces
- **Arxiv ID**: http://arxiv.org/abs/2008.00760v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.00760v2)
- **Published**: 2020-08-03 10:21:41+00:00
- **Updated**: 2020-09-14 07:23:10+00:00
- **Authors**: Marco Maggipinto, Matteo Terzi, Gian Antonio Susto
- **Comment**: None
- **Journal**: None
- **Summary**: Learning useful representations of complex data has been the subject of extensive research for many years. With the diffusion of Deep Neural Networks, Variational Autoencoders have gained lots of attention since they provide an explicit model of the data distribution based on an encoder/decoder architecture which is able to both generate images and encode them in a low-dimensional subspace. However, the latent space is not easily interpretable and the generation capabilities show some limitations since images typically look blurry and lack details. In this paper, we propose the Introspective Variational Classifier (IntroVAC), a model that learns interpretable latent subspaces by exploiting information from an additional label and provides improved image quality thanks to an adversarial training strategy.We show that IntroVAC is able to learn meaningful directions in the latent space enabling fine-grained manipulation of image attributes. We validate our approach on the CelebA dataset.



### DCSFN: Deep Cross-scale Fusion Network for Single Image Rain Removal
- **Arxiv ID**: http://arxiv.org/abs/2008.00767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00767v1)
- **Published**: 2020-08-03 10:34:45+00:00
- **Updated**: 2020-08-03 10:34:45+00:00
- **Authors**: Cong Wang, Xiaoying Xing, Zhixun Su, Junyang Chen
- **Comment**: Accepted to ACM International Conference on Multimedia (MM'20)
- **Journal**: None
- **Summary**: Rain removal is an important but challenging computer vision task as rain streaks can severely degrade the visibility of images that may make other visions or multimedia tasks fail to work. Previous works mainly focused on feature extraction and processing or neural network structure, while the current rain removal methods can already achieve remarkable results, training based on single network structure without considering the cross-scale relationship may cause information drop-out. In this paper, we explore the cross-scale manner between networks and inner-scale fusion operation to solve the image rain removal task. Specifically, to learn features with different scales, we propose a multi-sub-networks structure, where these sub-networks are fused via a crossscale manner by Gate Recurrent Unit to inner-learn and make full use of information at different scales in these sub-networks. Further, we design an inner-scale connection block to utilize the multi-scale information and features fusion way between different scales to improve rain representation ability and we introduce the dense block with skip connection to inner-connect these blocks. Experimental results on both synthetic and real-world datasets have demonstrated the superiority of our proposed method, which outperforms over the state-of-the-art methods. The source code will be available at https://supercong94.wixsite.com/supercong94.



### Dynamic and Static Context-aware LSTM for Multi-agent Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.00777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00777v1)
- **Published**: 2020-08-03 11:03:57+00:00
- **Updated**: 2020-08-03 11:03:57+00:00
- **Authors**: Chaofan Tao, Qinhong Jiang, Lixin Duan, Ping Luo
- **Comment**: 17 pages, 6 figures
- **Journal**: ECCV 2020
- **Summary**: Multi-agent motion prediction is challenging because it aims to foresee the future trajectories of multiple agents (\textit{e.g.} pedestrians) simultaneously in a complicated scene. Existing work addressed this challenge by either learning social spatial interactions represented by the positions of a group of pedestrians, while ignoring their temporal coherence (\textit{i.e.} dependencies between different long trajectories), or by understanding the complicated scene layout (\textit{e.g.} scene segmentation) to ensure safe navigation. However, unlike previous work that isolated the spatial interaction, temporal coherence, and scene layout, this paper designs a new mechanism, \textit{i.e.}, Dynamic and Static Context-aware Motion Predictor (DSCMP), to integrates these rich information into the long-short-term-memory (LSTM). It has three appealing benefits. (1) DSCMP models the dynamic interactions between agents by learning both their spatial positions and temporal coherence, as well as understanding the contextual scene layout.(2) Different from previous LSTM models that predict motions by propagating hidden features frame by frame, limiting the capacity to learn correlations between long trajectories, we carefully design a differentiable queue mechanism in DSCMP, which is able to explicitly memorize and learn the correlations between long trajectories. (3) DSCMP captures the context of scene by inferring latent variable, which enables multimodal predictions with meaningful semantic scene layout. Extensive experiments show that DSCMP outperforms state-of-the-art methods by large margins, such as 9.05\% and 7.62\% relative improvements on the ETH-UCY and SDD datasets respectively.



### Ubicomp Digital 2020 -- Handwriting classification using a convolutional recurrent network
- **Arxiv ID**: http://arxiv.org/abs/2008.01078v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01078v1)
- **Published**: 2020-08-03 11:32:22+00:00
- **Updated**: 2020-08-03 11:32:22+00:00
- **Authors**: Wei-Cheng Lai, Hendrik Schröter
- **Comment**: CRNN, Handwriting recognition, Ubicomp, Stabilo
- **Journal**: None
- **Summary**: The Ubicomp Digital 2020 -- Time Series Classification Challenge from STABILO is a challenge about multi-variate time series classification. The data collected from 100 volunteer writers, and contains 15 features measured with multiple sensors on a pen. In this paper,we use a neural network to classify the data into 52 classes, that is lower and upper cases of Arabic letters. The proposed architecture of the neural network a is CNN-LSTM network. It combines convolutional neural network (CNN) for short term context with along short term memory layer (LSTM) for also long term dependencies. We reached an accuracy of 68% on our writer exclusive test set and64.6% on the blind challenge test set resulting in the second place.



### Multi-Scale Deep Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2008.00802v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00802v1)
- **Published**: 2020-08-03 12:01:47+00:00
- **Updated**: 2020-08-03 12:01:47+00:00
- **Authors**: Thuong Nguyen Canh, Byeungwoo Jeon
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Recently, deep learning-based compressive imaging (DCI) has surpassed the conventional compressive imaging in reconstruction quality and faster running time. While multi-scale has shown superior performance over single-scale, research in DCI has been limited to single-scale sampling. Despite training with single-scale images, DCI tends to favor low-frequency components similar to the conventional multi-scale sampling, especially at low subrate. From this perspective, it would be easier for the network to learn multi-scale features with a multi-scale sampling architecture. In this work, we proposed a multi-scale deep compressive imaging (MS-DCI) framework which jointly learns to decompose, sample, and reconstruct images at multi-scale. A three-phase end-to-end training scheme was introduced with an initial and two enhance reconstruction phases to demonstrate the efficiency of multi-scale sampling and further improve the reconstruction performance. We analyzed the decomposition methods (including Pyramid, Wavelet, and Scale-space), sampling matrices, and measurements and showed the empirical benefit of MS-DCI which consistently outperforms both conventional and deep learning-based approaches.



### Rethinking Image Deraining via Rain Streaks and Vapors
- **Arxiv ID**: http://arxiv.org/abs/2008.00823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00823v1)
- **Published**: 2020-08-03 12:15:07+00:00
- **Updated**: 2020-08-03 12:15:07+00:00
- **Authors**: Yinglong Wang, Yibing Song, Chao Ma, Bing Zeng
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Single image deraining regards an input image as a fusion of a background image, a transmission map, rain streaks, and atmosphere light. While advanced models are proposed for image restoration (i.e., background image generation), they regard rain streaks with the same properties as background rather than transmission medium. As vapors (i.e., rain streaks accumulation or fog-like rain) are conveyed in the transmission map to model the veiling effect, the fusion of rain streaks and vapors do not naturally reflect the rain image formation. In this work, we reformulate rain streaks as transmission medium together with vapors to model rain imaging. We propose an encoder-decoder CNN named as SNet to learn the transmission map of rain streaks. As rain streaks appear with various shapes and directions, we use ShuffleNet units within SNet to capture their anisotropic representations. As vapors are brought by rain streaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict the transmission map of vapors in multi-scales based on that of rain streaks. Meanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The SNet, VNet, and ANet are jointly trained to predict transmission maps and atmosphere light for rain image restoration. Extensive experiments on the benchmark datasets demonstrate the effectiveness of the proposed visual model to predict rain streaks and vapors. The proposed deraining method performs favorably against state-of-the-art deraining approaches.



### LSOTB-TIR:A Large-Scale High-Diversity Thermal Infrared Object Tracking Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2008.00836v1
- **DOI**: 10.1145/3394171.3413922
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.00836v1)
- **Published**: 2020-08-03 12:36:06+00:00
- **Updated**: 2020-08-03 12:36:06+00:00
- **Authors**: Qiao Liu, Xin Li, Zhenyu He, Chenglong Li, Jun Li, Zikun Zhou, Di Yuan, Jing Li, Kai Yang, Nana Fan, Feng Zheng
- **Comment**: accepted by ACM Mutlimedia Conference, 2020
- **Journal**: None
- **Summary**: In this paper, we present a Large-Scale and high-diversity general Thermal InfraRed (TIR) Object Tracking Benchmark, called LSOTBTIR, which consists of an evaluation dataset and a training dataset with a total of 1,400 TIR sequences and more than 600K frames. We annotate the bounding box of objects in every frame of all sequences and generate over 730K bounding boxes in total. To the best of our knowledge, LSOTB-TIR is the largest and most diverse TIR object tracking benchmark to date. To evaluate a tracker on different attributes, we define 4 scenario attributes and 12 challenge attributes in the evaluation dataset. By releasing LSOTB-TIR, we encourage the community to develop deep learning based TIR trackers and evaluate them fairly and comprehensively. We evaluate and analyze more than 30 trackers on LSOTB-TIR to provide a series of baselines, and the results show that deep trackers achieve promising performance. Furthermore, we re-train several representative deep trackers on LSOTB-TIR, and their results demonstrate that the proposed training dataset significantly improves the performance of deep TIR trackers. Codes and dataset are available at https://github.com/QiaoLiuHit/LSOTB-TIR.



### Multi-Task Driven Explainable Diagnosis of COVID-19 using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2008.03205v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03205v1)
- **Published**: 2020-08-03 12:52:23+00:00
- **Updated**: 2020-08-03 12:52:23+00:00
- **Authors**: Aakarsh Malhotra, Surbhi Mittal, Puspita Majumdar, Saheb Chhabra, Kartik Thakral, Mayank Vatsa, Richa Singh, Santanu Chaudhury, Ashwin Pudrod, Anjali Agrawal
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions of 9000 frontal chest radiographs taken from ChestXray-14, CheXpert and a consolidated COVID-19 dataset. Further, 200 chest radiographs pertaining to COVID-19 patients are also annotated for semantic segmentation. This database will be released to the research community.



### Adversarial Graph Representation Adaptation for Cross-Domain Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.00859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00859v2)
- **Published**: 2020-08-03 13:27:24+00:00
- **Updated**: 2020-08-04 09:50:05+00:00
- **Authors**: Yuan Xie, Tianshui Chen, Tao Pu, Hefeng Wu, Liang Lin
- **Comment**: Accepted at ACM MM 2020
- **Journal**: None
- **Summary**: Data inconsistency and bias are inevitable among different facial expression recognition (FER) datasets due to subjective annotating process and different collecting conditions. Recent works resort to adversarial mechanisms that learn domain-invariant features to mitigate domain shift. However, most of these works focus on holistic feature adaptation, and they ignore local features that are more transferable across different datasets. Moreover, local features carry more detailed and discriminative content for expression recognition, and thus integrating local features may enable fine-grained adaptation. In this work, we propose a novel Adversarial Graph Representation Adaptation (AGRA) framework that unifies graph representation propagation with adversarial learning for cross-domain holistic-local feature co-adaptation. To achieve this, we first build a graph to correlate holistic and local regions within each domain and another graph to correlate these regions across different domains. Then, we learn the per-class statistical distribution of each domain and extract holistic-local features from the input image to initialize the corresponding graph nodes. Finally, we introduce two stacked graph convolution networks to propagate holistic-local feature within each domain to explore their interaction and across different domains for holistic-local feature co-adaptation. In this way, the AGRA framework can adaptively learn fine-grained domain-invariant features and thus facilitate cross-domain expression recognition. We conduct extensive and fair experiments on several popular benchmarks and show that the proposed AGRA framework achieves superior performance over previous state-of-the-art methods.



### Fusion of Deep and Non-Deep Methods for Fast Super-Resolution of Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2008.00878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00878v1)
- **Published**: 2020-08-03 13:55:39+00:00
- **Updated**: 2020-08-03 13:55:39+00:00
- **Authors**: Gaurav Kumar Nayak, Saksham Jain, R Venkatesh Babu, Anirban Chakraborty
- **Comment**: Accepted in IEEE BigMM 2020
- **Journal**: None
- **Summary**: In the emerging commercial space industry there is a drastic increase in access to low cost satellite imagery. The price for satellite images depends on the sensor quality and revisit rate. This work proposes to bridge the gap between image quality and the price by improving the image quality via super-resolution (SR). Recently, a number of deep SR techniques have been proposed to enhance satellite images. However, none of these methods utilize the region-level context information, giving equal importance to each region in the image. This, along with the fact that most state-of-the-art SR methods are complex and cumbersome deep models, the time taken to process very large satellite images can be impractically high. We, propose to handle this challenge by designing an SR framework that analyzes the regional information content on each patch of the low-resolution image and judiciously chooses to use more computationally complex deep models to super-resolve more structure-rich regions on the image, while using less resource-intensive non-deep methods on non-salient regions. Through extensive experiments on a large satellite image, we show substantial decrease in inference time while achieving similar performance to that of existing deep SR methods over several evaluation measures like PSNR, MSE and SSIM.



### Shape Adaptor: A Learnable Resizing Module
- **Arxiv ID**: http://arxiv.org/abs/2008.00892v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.00892v2)
- **Published**: 2020-08-03 14:15:52+00:00
- **Updated**: 2020-08-10 13:10:50+00:00
- **Authors**: Shikun Liu, Zhe Lin, Yilin Wang, Jianming Zhang, Federico Perazzi, Edward Johns
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: We present a novel resizing module for neural networks: shape adaptor, a drop-in enhancement built on top of traditional resizing layers, such as pooling, bilinear sampling, and strided convolution. Whilst traditional resizing layers have fixed and deterministic reshaping factors, our module allows for a learnable reshaping factor. Our implementation enables shape adaptors to be trained end-to-end without any additional supervision, through which network architectures can be optimised for each individual task, in a fully automated way. We performed experiments across seven image classification datasets, and results show that by simply using a set of our shape adaptors instead of the original resizing layers, performance increases consistently over human-designed networks, across all datasets. Additionally, we show the effectiveness of shape adaptors on two other applications: network compression and transfer learning. The source code is available at: https://github.com/lorenmt/shape-adaptor.



### Automated Segmentation of Brain Gray Matter Nuclei on Quantitative Susceptibility Mapping Using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.00901v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00901v1)
- **Published**: 2020-08-03 14:32:30+00:00
- **Updated**: 2020-08-03 14:32:30+00:00
- **Authors**: Chao Chai, Pengchong Qiao, Bin Zhao, Huiying Wang, Guohua Liu, Hong Wu, E Mark Haacke, Wen Shen, Chen Cao, Xinchen Ye, Zhiyang Liu, Shuang Xia
- **Comment**: submitted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Abnormal iron accumulation in the brain subcortical nuclei has been reported to be correlated to various neurodegenerative diseases, which can be measured through the magnetic susceptibility from the quantitative susceptibility mapping (QSM). To quantitively measure the magnetic susceptibility, the nuclei should be accurately segmented, which is a tedious task for clinicians. In this paper, we proposed a double-branch residual-structured U-Net (DB-ResUNet) based on 3D convolutional neural network (CNN) to automatically segment such brain gray matter nuclei. To better tradeoff between segmentation accuracy and the memory efficiency, the proposed DB-ResUNet fed image patches with high resolution and the patches with low resolution but larger field of view into the local and global branches, respectively. Experimental results revealed that by jointly using QSM and T$_\text{1}$ weighted imaging (T$_\text{1}$WI) as inputs, the proposed method was able to achieve better segmentation accuracy over its single-branch counterpart, as well as the conventional atlas-based method and the classical 3D-UNet structure. The susceptibility values and the volumes were also measured, which indicated that the measurements from the proposed DB-ResUNet are able to present high correlation with values from the manually annotated regions of interest.



### Color Texture Image Retrieval Based on Copula Multivariate Modeling in the Shearlet Domain
- **Arxiv ID**: http://arxiv.org/abs/2008.00910v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T99, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2008.00910v1)
- **Published**: 2020-08-03 14:40:27+00:00
- **Updated**: 2020-08-03 14:40:27+00:00
- **Authors**: Sadegh Etemad, Maryam Amirmazlaghani
- **Comment**: 37 pages, 16 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, a color texture image retrieval framework is proposed based on Shearlet domain modeling using Copula multivariate model. In the proposed framework, Gaussian Copula is used to model the dependencies between different sub-bands of the Non Subsample Shearlet Transform (NSST) and non-Gaussian models are used for marginal modeling of the coefficients. Six different schemes are proposed for modeling NSST coefficients based on the four types of neighboring defined; moreover, Kullback Leibler Divergence(KLD) close form is calculated in different situations for the two Gaussian Copula and non Gaussian functions in order to investigate the similarities in the proposed retrieval framework. The Jeffery divergence (JD) criterion, which is a symmetrical version of KLD, is used for investigating similarities in the proposed framework. We have implemented our experiments on four texture image retrieval benchmark datasets, the results of which show the superiority of the proposed framework over the existing state-of-the-art methods. In addition, the retrieval time of the proposed framework is also analyzed in the two steps of feature extraction and similarity matching, which also shows that the proposed framework enjoys an appropriate retrieval time.



### Explainable Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.00916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00916v1)
- **Published**: 2020-08-03 14:47:51+00:00
- **Updated**: 2020-08-03 14:47:51+00:00
- **Authors**: Jonathan R. Williford, Brandon B. May, Jeffrey Byrne
- **Comment**: To appear in the Proceedings of ECCV 2020. Project page at
  https://stresearch.github.io/xfr
- **Journal**: None
- **Summary**: Explainable face recognition is the problem of explaining why a facial matcher matches faces. In this paper, we provide the first comprehensive benchmark and baseline evaluation for explainable face recognition. We define a new evaluation protocol called the ``inpainting game'', which is a curated set of 3648 triplets (probe, mate, nonmate) of 95 subjects, which differ by synthetically inpainting a chosen facial characteristic like the nose, eyebrows or mouth creating an inpainted nonmate. An explainable face matcher is tasked with generating a network attention map which best explains which regions in a probe image match with a mated image, and not with an inpainted nonmate for each triplet. This provides ground truth for quantifying what image regions contribute to face matching. Furthermore, we provide a comprehensive benchmark on this dataset comparing five state of the art methods for network attention in face recognition on three facial matchers. This benchmark includes two new algorithms for network attention called subtree EBP and Density-based Input Sampling for Explanation (DISE) which outperform the state of the art by a wide margin. Finally, we show qualitative visualization of these network attention techniques on novel images, and explore how these explainable face recognition models can improve transparency and trust for facial matchers.



### Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.00923v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00923v8)
- **Published**: 2020-08-03 15:00:31+00:00
- **Updated**: 2021-11-30 06:29:31+00:00
- **Authors**: Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, Lingbo Liu, Liang Lin
- **Comment**: Accepted at T-PAMI, 2021. arXiv admin note: text overlap with
  arXiv:2008.00859
- **Journal**: None
- **Summary**: To address the problem of data inconsistencies among different facial expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs) have been extensively devised in recent years. Although each declares to achieve superior performance, fair comparisons are lacking due to the inconsistent choices of the source/target datasets and feature extractors. In this work, we first analyze the performance effect caused by these inconsistent choices, and then re-implement some well-performing CD-FER and recently published domain adaptation algorithms. We ensure that all these algorithms adopt the same source datasets and feature extractors for fair CD-FER evaluations. We find that most of the current leading algorithms use adversarial learning to learn holistic domain-invariant features to mitigate domain shifts. However, these algorithms ignore local features, which are more transferable across different datasets and carry more detailed content for fine-grained adaptation. To address these issues, we integrate graph representation propagation with adversarial learning for cross-domain holistic-local feature co-adaptation by developing a novel adversarial graph representation adaptation (AGRA) framework. Specifically, it first builds two graphs to correlate holistic and local regions within each domain and across different domains, respectively. Then, it extracts holistic-local features from the input image and uses learnable per-class statistical distributions to initialize the corresponding graph nodes. Finally, two stacked graph convolution networks (GCNs) are adopted to propagate holistic-local features within each domain to explore their interaction and across different domains for holistic-local feature co-adaptation. We conduct extensive and fair evaluations on several popular benchmarks and show that the proposed AGRA framework outperforms previous state-of-the-art methods.



### AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and Baseline Methods
- **Arxiv ID**: http://arxiv.org/abs/2008.00932v2
- **DOI**: 10.1109/ACCESS.2020.3028072
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00932v2)
- **Published**: 2020-08-03 15:12:05+00:00
- **Updated**: 2020-10-19 10:31:48+00:00
- **Authors**: Ozge Mercanoglu Sincan, Hacer Yalim Keles
- **Comment**: Preprint of the accepted paper at IEEE Access Journal. The revised
  version contains empirical results with Montalbano dataset, in addition to
  AUTSL. The abstract is revised accordingly
- **Journal**: IEEE Access (2020), vol. 8, pp. 181340-181355
- **Summary**: Sign language recognition is a challenging problem where signs are identified by simultaneous local and global articulations of multiple sources, i.e. hand shape and orientation, hand movements, body posture, and facial expressions. Solving this problem computationally for a large vocabulary of signs in real life settings is still a challenge, even with the state-of-the-art models. In this study, we present a new largescale multi-modal Turkish Sign Language dataset (AUTSL) with a benchmark and provide baseline models for performance evaluations. Our dataset consists of 226 signs performed by 43 different signers and 38,336 isolated sign video samples in total. Samples contain a wide variety of backgrounds recorded in indoor and outdoor environments. Moreover, spatial positions and the postures of signers also vary in the recordings. Each sample is recorded with Microsoft Kinect v2 and contains RGB, depth, and skeleton modalities. We prepared benchmark training and test sets for user independent assessments of the models. We trained several deep learning based models and provide empirical evaluations using the benchmark; we used CNNs to extract features, unidirectional and bidirectional LSTM models to characterize temporal information. We also incorporated feature pooling modules and temporal attention to our models to improve the performances. We evaluated our baseline models on AUTSL and Montalbano datasets. Our models achieved competitive results with the state-of-the-art methods on Montalbano dataset, i.e. 96.11% accuracy. In AUTSL random train-test splits, our models performed up to 95.95% accuracy. In the proposed user-independent benchmark dataset our best baseline model achieved 62.02% accuracy. The gaps in the performances of the same baseline models show the challenges inherent in our benchmark dataset. AUTSL benchmark dataset is publicly available at https://cvml.ankara.edu.tr.



### Frame-To-Frame Consistent Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00948v3
- **DOI**: 10.3217/978-3-85125-752-6-18
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00948v3)
- **Published**: 2020-08-03 15:28:40+00:00
- **Updated**: 2020-08-27 18:14:38+00:00
- **Authors**: Manuel Rebol, Patrick Knöbelreiter
- **Comment**: ACVRW20
- **Journal**: None
- **Summary**: In this work, we aim for temporally consistent semantic segmentation throughout frames in a video. Many semantic segmentation algorithms process images individually which leads to an inconsistent scene interpretation due to illumination changes, occlusions and other variations over time. To achieve a temporally consistent prediction, we train a convolutional neural network (CNN) which propagates features through consecutive frames in a video using a convolutional long short term memory (ConvLSTM) cell. Besides the temporal feature propagation, we penalize inconsistencies in our loss function. We show in our experiments that the performance improves when utilizing video information compared to single frame prediction. The mean intersection over union (mIoU) metric on the Cityscapes validation set increases from 45.2 % for the single frames to 57.9 % for video data after implementing the ConvLSTM to propagate features trough time on the ESPNet. Most importantly, inconsistency decreases from 4.5 % to 1.3 % which is a reduction by 71.1 %. Our results indicate that the added temporal information produces a frame-to-frame consistent and more accurate image understanding compared to single frame processing. Code and videos are available at https://github.com/mrebol/f2f-consistent-semantic-segmentation



### Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2008.00951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00951v2)
- **Published**: 2020-08-03 15:30:38+00:00
- **Updated**: 2021-04-21 12:53:36+00:00
- **Authors**: Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or
- **Comment**: Accepted to CVPR 2021, project page available at
  https://eladrich.github.io/pixel2style2pixel/
- **Journal**: None
- **Summary**: We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.



### SeCo: Exploring Sequence Supervision for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.00975v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00975v2)
- **Published**: 2020-08-03 15:51:35+00:00
- **Updated**: 2021-01-27 17:25:22+00:00
- **Authors**: Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, Tao Mei
- **Comment**: AAAI 2021; Code is publicly available at:
  https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning
- **Journal**: None
- **Summary**: A steady momentum of innovations and breakthroughs has convincingly pushed the limits of unsupervised image representation learning. Compared to static 2D images, video has one more dimension (time). The inherent supervision existing in such sequential structure offers a fertile ground for building unsupervised learning models. In this paper, we compose a trilogy of exploring the basic and generic supervision in the sequence from spatial, spatiotemporal and sequential perspectives. We materialize the supervisory signals through determining whether a pair of samples is from one frame or from one video, and whether a triplet of samples is in the correct temporal order. We uniquely regard the signals as the foundation in contrastive learning and derive a particular form named Sequence Contrastive Learning (SeCo). SeCo shows superior results under the linear protocol on action recognition (Kinetics), untrimmed activity recognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo demonstrates considerable improvements over recent unsupervised pre-training techniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised ImageNet pre-training in action recognition task on UCF101 and HMDB51, respectively. Source code is available at \url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.



### An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers
- **Arxiv ID**: http://arxiv.org/abs/2008.00992v2
- **DOI**: 10.1007/978-3-030-68238-5_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00992v2)
- **Published**: 2020-08-03 16:21:18+00:00
- **Updated**: 2020-08-13 14:17:19+00:00
- **Authors**: Matteo Dunnhofer, Niki Martinel, Christian Micheloni
- **Comment**: European Conference on Computer Vision (ECCV) 2020, Visual Object
  Tracking Challenge VOT2020 workshop
- **Journal**: None
- **Summary**: Visual object tracking is the problem of predicting a target object's state in a video. Generally, bounding-boxes have been used to represent states, and a surge of effort has been spent by the community to produce efficient causal algorithms capable of locating targets with such representations. As the field is moving towards binary segmentation masks to define objects more precisely, in this paper we propose to extensively explore target-conditioned segmentation methods available in the computer vision community, in order to transform any bounding-box tracker into a segmentation tracker. Our analysis shows that such methods allow trackers to compete with recently proposed segmentation trackers, while performing quasi real-time.



### Improving concave point detection to better segment overlapped objects in images
- **Arxiv ID**: http://arxiv.org/abs/2008.00997v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00997v3)
- **Published**: 2020-08-03 16:32:49+00:00
- **Updated**: 2022-01-10 11:14:48+00:00
- **Authors**: Miquel Miró-Nicolau, Biel Moyà-Alcover, Manuel Gonzàlez-Hidalgo, Antoni Jaume-i-Capó
- **Comment**: Corrected typos. Improved results section
- **Journal**: None
- **Summary**: This paper presents a method that improve state-of-the-art of the concave point detection methods as a first step to segment overlapping objects on images. It is based on the analysis of the curvature of the objects contour. The method has three main steps. First, we pre-process the original image to obtain the value of the curvature on each contour point. Second, we select regions with higher curvature and we apply a recursive algorithm to refine the previous selected regions. Finally, we obtain a concave point from each region based on the analysis of the relative position of their neighbourhood We experimentally demonstrated that a better concave points detection implies a better cluster division. In order to evaluate the quality of the concave point detection algorithm, we constructed a synthetic dataset to simulate overlapping objects, providing the position of the concave points as a ground truth. As a case study, the performance of a well-known application is evaluated, such as the splitting of overlapped cells in images of peripheral blood smears samples of patients with sickle cell anaemia. We used the proposed method to detect the concave points in clusters of cells and then we separate this clusters by ellipse fitting.



### Teacher-Student Training and Triplet Loss for Facial Expression Recognition under Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2008.01003v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01003v2)
- **Published**: 2020-08-03 16:41:19+00:00
- **Updated**: 2021-02-25 18:54:30+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: In this paper, we study the task of facial expression recognition under strong occlusion. We are particularly interested in cases where 50% of the face is occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While previous studies show that pre-training convolutional neural networks (CNNs) on fully-visible (non-occluded) faces improves the accuracy, we propose to employ knowledge distillation to achieve further improvements. First of all, we employ the classic teacher-student training strategy, in which the teacher is a CNN trained on fully-visible faces and the student is a CNN trained on occluded faces. Second of all, we propose a new approach for knowledge distillation based on triplet loss. During training, the goal is to reduce the distance between an anchor embedding, produced by a student CNN that takes occluded faces as input, and a positive embedding (from the same class as the anchor), produced by a teacher CNN trained on fully-visible faces, so that it becomes smaller than the distance between the anchor and a negative embedding (from a different class than the anchor), produced by the student CNN. Third of all, we propose to combine the distilled embeddings obtained through the classic teacher-student strategy and our novel teacher-student strategy based on triplet loss into a single embedding vector. We conduct experiments on two benchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face, showing that knowledge distillation can bring significant improvements over the state-of-the-art methods designed for occluded faces in the VR setting.



### RareAct: A video dataset of unusual interactions
- **Arxiv ID**: http://arxiv.org/abs/2008.01018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01018v1)
- **Published**: 2020-08-03 16:53:35+00:00
- **Updated**: 2020-08-03 16:53:35+00:00
- **Authors**: Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a manually annotated video dataset of unusual actions, namely RareAct, including actions such as "blend phone", "cut keyboard" and "microwave shoes". RareAct aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately. We provide benchmarks using a state-of-the-art HowTo100M pretrained video and text model and show that zero-shot and few-shot compositionality of actions remains a challenging and unsolved task.



### From Design Draft to Real Attire: Unaligned Fashion Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2008.01023v3
- **DOI**: 10.1145/3394171.3413953
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.01023v3)
- **Published**: 2020-08-03 17:03:11+00:00
- **Updated**: 2020-09-16 12:27:01+00:00
- **Authors**: Yu Han, Shuai Yang, Wenjing Wang, Jiaying Liu
- **Comment**: Accepted by ACMMM 2020. Our project website is available at:
  https://victoriahy.github.io/MM2020/
- **Journal**: None
- **Summary**: Fashion manipulation has attracted growing interest due to its great application value, which inspires many researches towards fashion images. However, little attention has been paid to fashion design draft. In this paper, we study a new unaligned translation problem between design drafts and real fashion items, whose main challenge lies in the huge misalignment between the two modalities. We first collect paired design drafts and real fashion item images without pixel-wise alignment. To solve the misalignment problem, our main idea is to train a sampling network to adaptively adjust the input to an intermediate state with structure alignment to the output. Moreover, built upon the sampling network, we present design draft to real fashion item translation network (D2RNet), where two separate translation streams that focus on texture and shape, respectively, are combined tactfully to get both benefits. D2RNet is able to generate realistic garments with both texture and shape consistency to their design drafts. We show that this idea can be effectively applied to the reverse translation problem and present R2DNet accordingly. Extensive experiments on unaligned fashion design translation demonstrate the superiority of our method over state-of-the-art methods. Our project website is available at: https://victoriahy.github.io/MM2020/ .



### Project to Adapt: Domain Adaptation for Depth Completion from Noisy and Sparse Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/2008.01034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01034v2)
- **Published**: 2020-08-03 17:21:57+00:00
- **Updated**: 2020-08-05 14:46:40+00:00
- **Authors**: Adrian Lopez-Rodriguez, Benjamin Busam, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Depth completion aims to predict a dense depth map from a sparse depth input. The acquisition of dense ground truth annotations for depth completion settings can be difficult and, at the same time, a significant domain gap between real LiDAR measurements and synthetic data has prevented from successful training of models in virtual settings. We propose a domain adaptation approach for sparse-to-dense depth completion that is trained from synthetic data, without annotations in the real domain or additional sensors. Our approach simulates the real sensor noise in an RGB+LiDAR set-up, and consists of three modules: simulating the real LiDAR input in the synthetic domain via projections, filtering the real noisy LiDAR for supervision and adapting the synthetic RGB image using a CycleGAN approach. We extensively evaluate these modules against the state-of-the-art in the KITTI depth completion benchmark, showing significant improvements.



### Residual Frames with Efficient Pseudo-3D CNN for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.01057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01057v1)
- **Published**: 2020-08-03 17:40:17+00:00
- **Updated**: 2020-08-03 17:40:17+00:00
- **Authors**: Jiawei Chen, Jenson Hsiao, Chiu Man Ho
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition is regarded as a key cornerstone in domains such as surveillance or video understanding. Despite recent progress in the development of end-to-end solutions for video-based action recognition, achieving state-of-the-art performance still requires using auxiliary hand-crafted motion representations, e.g., optical flow, which are usually computationally demanding. In this work, we propose to use residual frames (i.e., differences between adjacent RGB frames) as an alternative "lightweight" motion representation, which carries salient motion information and is computationally efficient. In addition, we develop a new pseudo-3D convolution module which decouples 3D convolution into 2D and 1D convolution. The proposed module exploits residual information in the feature space to better structure motions, and is equipped with a self-attention mechanism that assists to recalibrate the appearance and motion features. Empirical results confirm the efficiency and effectiveness of residual frames as well as the proposed pseudo-3D convolution module.



### Improving One-stage Visual Grounding by Recursive Sub-query Construction
- **Arxiv ID**: http://arxiv.org/abs/2008.01059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01059v1)
- **Published**: 2020-08-03 17:43:30+00:00
- **Updated**: 2020-08-03 17:43:30+00:00
- **Authors**: Zhengyuan Yang, Tianlang Chen, Liwei Wang, Jiebo Luo
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We improve one-stage visual grounding by addressing current limitations on grounding long and complex queries. Existing one-stage methods encode the entire language query as a single sentence embedding vector, e.g., taking the embedding from BERT or the hidden state from LSTM. This single vector representation is prone to overlooking the detailed descriptions in the query. To address this query modeling deficiency, we propose a recursive sub-query construction framework, which reasons between image and query for multiple rounds and reduces the referring ambiguity step by step. We show our new one-stage method obtains 5.0%, 4.5%, 7.5%, 12.8% absolute improvements over the state-of-the-art one-stage baseline on ReferItGame, RefCOCO, RefCOCO+, and RefCOCOg, respectively. In particular, superior performances on longer and more complex queries validates the effectiveness of our query modeling.



### Memory-augmented Dense Predictive Coding for Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01065v1)
- **Published**: 2020-08-03 17:57:01+00:00
- **Updated**: 2020-08-03 17:57:01+00:00
- **Authors**: Tengda Han, Weidi Xie, Andrew Zisserman
- **Comment**: ECCV2020, Spotlight
- **Journal**: None
- **Summary**: The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.



### Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2008.01068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.01068v2)
- **Published**: 2020-08-03 17:58:46+00:00
- **Updated**: 2021-03-12 02:49:28+00:00
- **Authors**: Peng-Shuai Wang, Yu-Qi Yang, Qian-Fang Zou, Zhirong Wu, Yang Liu, Xin Tong
- **Comment**: Accepted by AAAI 2021. Code:
  https://github.com/microsoft/O-CNN/blob/master/docs/unsupervised.md
- **Journal**: None
- **Summary**: Although unsupervised feature learning has demonstrated its advantages to reducing the workload of data labeling and network design in many fields, existing unsupervised 3D learning methods still cannot offer a generic network for various shape analysis tasks with competitive performance to supervised methods. In this paper, we propose an unsupervised method for learning a generic and efficient shape encoding network for different shape analysis tasks. The key idea of our method is to jointly encode and learn shape and point features from unlabeled 3D point clouds. For this purpose, we adapt HR-Net to octree-based convolutional neural networks for jointly encoding shape and point features with fused multiresolution subnetworks and design a simple-yet-efficient Multiresolution Instance Discrimination (MID) loss for jointly learning the shape and point features. Our network takes a 3D point cloud as input and output both shape and point features. After training, the network is concatenated with simple task-specific back-end layers and fine-tuned for different shape analysis tasks. We evaluate the efficacy and generality of our method and validate our network and loss design with a set of shape analysis tasks, including shape classification, semantic shape segmentation, as well as shape registration tasks. With simple back-ends, our network demonstrates the best performance among all unsupervised methods and achieves competitive performance to supervised methods, especially in tasks with a small labeled dataset. For fine-grained shape segmentation, our method even surpasses existing supervised methods by a large margin.



### Sub-Pixel Back-Projection Network For Lightweight Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.01116v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01116v1)
- **Published**: 2020-08-03 18:15:16+00:00
- **Updated**: 2020-08-03 18:15:16+00:00
- **Authors**: Supratik Banerjee, Cagri Ozcinar, Aakanksha Rana, Aljosa Smolic, Michael Manzke
- **Comment**: To appear in IMVIP 2020
- **Journal**: None
- **Summary**: Convolutional neural network (CNN)-based methods have achieved great success for single-image superresolution (SISR). However, most models attempt to improve reconstruction accuracy while increasing the requirement of number of model parameters. To tackle this problem, in this paper, we study reducing the number of parameters and computational cost of CNN-based SISR methods while maintaining the accuracy of super-resolution reconstruction performance. To this end, we introduce a novel network architecture for SISR, which strikes a good trade-off between reconstruction quality and low computational complexity. Specifically, we propose an iterative back-projection architecture using sub-pixel convolution instead of deconvolution layers. We evaluate the performance of computational and reconstruction accuracy for our proposed model with extensive quantitative and qualitative evaluations. Experimental results reveal that our proposed method uses fewer parameters and reduces the computational cost while maintaining reconstruction accuracy against state-of-the-art SISR methods over well-known four SR benchmark datasets. Code is available at "https://github.com/supratikbanerjee/SubPixel-BackProjection_SuperResolution".



### AiRound and CV-BrCT: Novel Multi-View Datasets for Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.01133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01133v1)
- **Published**: 2020-08-03 18:55:46+00:00
- **Updated**: 2020-08-03 18:55:46+00:00
- **Authors**: Gabriel Machado, Edemir Ferreira, Keiller Nogueira, Hugo Oliveira, Pedro Gama, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: It is undeniable that aerial/satellite images can provide useful information for a large variety of tasks. But, since these images are always looking from above, some applications can benefit from complementary information provided by other perspective views of the scene, such as ground-level images. Despite a large number of public repositories for both georeferenced photographs and aerial images, there is a lack of benchmark datasets that allow the development of approaches that exploit the benefits and complementarity of aerial/ground imagery. In this paper, we present two new publicly available datasets named \thedataset~and CV-BrCT. The first one contains triplets of images from the same geographic coordinate with different perspectives of view extracted from various places around the world. Each triplet is composed of an aerial RGB image, a ground-level perspective image, and a Sentinel-2 sample. The second dataset contains pairs of aerial and street-level images extracted from southeast Brazil. We design an extensive set of experiments concerning multi-view scene classification, using early and late fusion. Such experiments were conducted to show that image classification can be enhanced using multi-view data.



### 3D B-mode ultrasound speckle reduction using deep learning for 3D registration applications
- **Arxiv ID**: http://arxiv.org/abs/2008.01147v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01147v1)
- **Published**: 2020-08-03 19:29:59+00:00
- **Updated**: 2020-08-03 19:29:59+00:00
- **Authors**: Hongliang Li, Tal Mezheritsky, Liset Vazquez Romaguera, Samuel Kadoury
- **Comment**: 10 pages, 3 figures and 3 tables
- **Journal**: None
- **Summary**: Ultrasound (US) speckles are granular patterns which can impede image post-processing tasks, such as image segmentation and registration. Conventional filtering approaches are commonly used to remove US speckles, while their main drawback is long run-time in a 3D scenario. Although a few studies were conducted to remove 2D US speckles using deep learning, to our knowledge, there is no study to perform speckle reduction of 3D B-mode US using deep learning. In this study, we propose a 3D dense U-Net model to process 3D US B-mode data from a clinical US system. The model's results were applied to 3D registration. We show that our deep learning framework can obtain similar suppression and mean preservation index (1.066) on speckle reduction when compared to conventional filtering approaches (0.978), while reducing the runtime by two orders of magnitude. Moreover, it is found that the speckle reduction using our deep learning model contributes to improving the 3D registration performance. The mean square error of 3D registration on 3D data using 3D U-Net speckle reduction is reduced by half compared to that with speckles.



### HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2008.01148v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01148v1)
- **Published**: 2020-08-03 19:34:48+00:00
- **Updated**: 2020-08-03 19:34:48+00:00
- **Authors**: Md Mofijul Islam, Tariq Iqbal
- **Comment**: To be published in the IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS) 2020
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS) 2020
- **Summary**: To fluently collaborate with people, robots need the ability to recognize human activities accurately. Although modern robots are equipped with various sensors, robust human activity recognition (HAR) still remains a challenging task for robots due to difficulties related to multimodal data fusion. To address these challenges, in this work, we introduce a deep neural network-based multimodal HAR algorithm, HAMLET. HAMLET incorporates a hierarchical architecture, where the lower layer encodes spatio-temporal features from unimodal data by adopting a multi-head self-attention mechanism. We develop a novel multimodal attention mechanism for disentangling and fusing the salient unimodal features to compute the multimodal features in the upper layer. Finally, multimodal features are used in a fully connect neural-network to recognize human activities. We evaluated our algorithm by comparing its performance to several state-of-the-art activity recognition algorithms on three human activity datasets. The results suggest that HAMLET outperformed all other evaluated baselines across all datasets and metrics tested, with the highest top-1 accuracy of 95.12% and 97.45% on the UTD-MHAD [1] and the UT-Kinect [2] datasets respectively, and F1-score of 81.52% on the UCSD-MIT [3] dataset. We further visualize the unimodal and multimodal attention maps, which provide us with a tool to interpret the impact of attention mechanisms concerning HAR.



### Action sequencing using visual permutations
- **Arxiv ID**: http://arxiv.org/abs/2008.01156v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01156v2)
- **Published**: 2020-08-03 19:49:06+00:00
- **Updated**: 2021-02-05 02:34:31+00:00
- **Authors**: Michael Burke, Kartic Subr, Subramanian Ramamoorthy
- **Comment**: This paper has been accepted for publication at IEEE RA-L
- **Journal**: None
- **Summary**: Humans can easily reason about the sequence of high level actions needed to complete tasks, but it is particularly difficult to instil this ability in robots trained from relatively few examples. This work considers the task of neural action sequencing conditioned on a single reference visual state. This task is extremely challenging as it is not only subject to the significant combinatorial complexity that arises from large action sets, but also requires a model that can perform some form of symbol grounding, mapping high dimensional input data to actions, while reasoning about action relationships. This paper takes a permutation perspective and argues that action sequencing benefits from the ability to reason about both permutations and ordering concepts. Empirical analysis shows that neural models trained with latent permutations outperform standard neural architectures in constrained action sequencing tasks. Results also show that action sequencing using visual permutations is an effective mechanism to initialise and speed up traditional planning techniques and successfully scales to far greater action set sizes than models considered previously.



### Classification of Multiple Diseases on Body CT Scans using Weakly Supervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.01158v3
- **DOI**: 10.1148/ryai.210026
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01158v3)
- **Published**: 2020-08-03 19:55:53+00:00
- **Updated**: 2021-11-17 02:42:07+00:00
- **Authors**: Fakrul Islam Tushar, Vincent M. D'Anniballe, Rui Hou, Maciej A. Mazurowski, Wanyi Fu, Ehsan Samei, Geoffrey D. Rubin, Joseph Y. Lo
- **Comment**: 22 pages, 6 figures, 2 tables; Accepted for publication at Radiology:
  Artificial Intelligence
- **Journal**: None
- **Summary**: Purpose: To design multi-disease classifiers for body CT scans for three different organ systems using automatically extracted labels from radiology text reports.Materials & Methods: This retrospective study included a total of 12,092 patients (mean age 57 +- 18; 6,172 women) for model development and testing (from 2012-2017). Rule-based algorithms were used to extract 19,225 disease labels from 13,667 body CT scans from 12,092 patients. Using a three-dimensional DenseVNet, three organ systems were segmented: lungs and pleura; liver and gallbladder; and kidneys and ureters. For each organ, a three-dimensional convolutional neural network classified no apparent disease versus four common diseases for a total of 15 different labels across all three models. Testing was performed on a subset of 2,158 CT volumes relative to 2,875 manually derived reference labels from 2133 patients (mean age 58 +- 18;1079 women). Performance was reported as receiver operating characteristic area under the curve (AUC) with 95% confidence intervals by the DeLong method. Results: Manual validation of the extracted labels confirmed 91% to 99% accuracy across the 15 different labels. AUCs for lungs and pleura labels were: atelectasis 0.77 (95% CI: 0.74, 0.81), nodule 0.65 (0.61, 0.69), emphysema 0.89 (0.86, 0.92), effusion 0.97 (0.96, 0.98), and no apparent disease 0.89 (0.87, 0.91). AUCs for liver and gallbladder were: hepatobiliary calcification 0.62 (95% CI: 0.56, 0.67), lesion 0.73 (0.69, 0.77), dilation 0.87 (0.84, 0.90), fatty 0.89 (0.86, 0.92), and no apparent disease 0.82 (0.78, 0.85). AUCs for kidneys and ureters were: stone 0.83 (95% CI: 0.79, 0.87), atrophy 0.92 (0.89, 0.94), lesion 0.68 (0.64, 0.72), cyst 0.70 (0.66, 0.73), and no apparent disease 0.79 (0.75, 0.83). Conclusion: Weakly-supervised deep learning models were able to classify diverse diseases in multiple organ systems.



### Recognition and 3D Localization of Pedestrian Actions from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2008.01162v1
- **DOI**: 10.1109/ITSC45102.2020.9294551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01162v1)
- **Published**: 2020-08-03 19:57:03+00:00
- **Updated**: 2020-08-03 19:57:03+00:00
- **Authors**: Jun Hayakawa, Behzad Dariush
- **Comment**: None
- **Journal**: IEEE Intelligent Transportation Systems Conference (ITSC) 2020
- **Summary**: Understanding and predicting pedestrian behavior is an important and challenging area of research for realizing safe and effective navigation strategies in automated and advanced driver assistance technologies in urban scenes. This paper focuses on monocular pedestrian action recognition and 3D localization from an egocentric view for the purpose of predicting intention and forecasting future trajectory. A challenge in addressing this problem in urban traffic scenes is attributed to the unpredictable behavior of pedestrians, whereby actions and intentions are constantly in flux and depend on the pedestrians pose, their 3D spatial relations, and their interaction with other agents as well as with the environment. To partially address these challenges, we consider the importance of pose toward recognition and 3D localization of pedestrian actions. In particular, we propose an action recognition framework using a two-stream temporal relation network with inputs corresponding to the raw RGB image sequence of the tracked pedestrian as well as the pedestrian pose. The proposed method outperforms methods using a single-stream temporal relation network based on evaluations using the JAAD public dataset. The estimated pose and associated body key-points are also used as input to a network that estimates the 3D location of the pedestrian using a unique loss function. The evaluation of our 3D localization method on the KITTI dataset indicates the improvement of the average localization error as compared to existing state-of-the-art methods. Finally, we conduct qualitative tests of action recognition and 3D localization on HRI's H3D driving dataset.



### Reducing Label Noise in Anchor-Free Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.01167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01167v2)
- **Published**: 2020-08-03 20:02:46+00:00
- **Updated**: 2020-08-13 19:12:23+00:00
- **Authors**: Nermin Samet, Samet Hicsonmez, Emre Akbas
- **Comment**: BMVC 2020 camera-ready version
- **Journal**: None
- **Summary**: Current anchor-free object detectors label all the features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach causes label noise during training, since some of these positively labeled features may be on the background or an occluder object, or they are simply not discriminative features. In this paper, we propose a new labeling strategy aimed to reduce the label noise in anchor-free detectors. We sum-pool predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We develop a new one-stage, anchor-free object detector, PPDet, to employ this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the best performance among anchor-free top-down detectors and performs on-par with the other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$). Code is available at https://github.com/nerminsamet/ppdet



### Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts
- **Arxiv ID**: http://arxiv.org/abs/2008.01178v5
- **DOI**: 10.1016/j.cviu.2021.103299
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2008.01178v5)
- **Published**: 2020-08-03 20:36:01+00:00
- **Updated**: 2021-11-12 10:23:11+00:00
- **Authors**: Nicolas Gonthier, Saïd Ladjal, Yann Gousseau
- **Comment**: 26 pages, 12 figures
- **Journal**: Computer Vision and Image Understanding 2022
- **Summary**: Weakly supervised object detection (WSOD) using only image-level annotations has attracted a growing attention over the past few years. Whereas such task is typically addressed with a domain-specific solution focused on natural images, we show that a simple multiple instance approach applied on pre-trained deep features yields excellent performances on non-photographic datasets, possibly including new classes. The approach does not include any fine-tuning or cross-domain learning and is therefore efficient and possibly applicable to arbitrary datasets and classes. We investigate several flavors of the proposed approach, some including multi-layers perceptron and polyhedral classifiers. Despite its simplicity, our method shows competitive results on a range of publicly available datasets, including paintings (People-Art, IconArt), watercolors, cliparts and comics and allows to quickly learn unseen visual categories.



### PillarFlow: End-to-end Birds-eye-view Flow Estimation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.01179v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.01179v3)
- **Published**: 2020-08-03 20:36:28+00:00
- **Updated**: 2020-08-29 13:35:09+00:00
- **Authors**: Kuan-Hui Lee, Matthew Kliemann, Adrien Gaidon, Jie Li, Chao Fang, Sudeep Pillai, Wolfram Burgard
- **Comment**: Accepted by IROS 2020
- **Journal**: None
- **Summary**: In autonomous driving, accurately estimating the state of surrounding obstacles is critical for safe and robust path planning. However, this perception task is difficult, particularly for generic obstacles/objects, due to appearance and occlusion changes. To tackle this problem, we propose an end-to-end deep learning framework for LIDAR-based flow estimation in bird's eye view (BeV). Our method takes consecutive point cloud pairs as input and produces a 2-D BeV flow grid describing the dynamic state of each cell. The experimental results show that the proposed method not only estimates 2-D BeV flow accurately but also improves tracking performance of both dynamic and static objects.



### Describing Textures using Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2008.01180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01180v1)
- **Published**: 2020-08-03 20:37:35+00:00
- **Updated**: 2020-08-03 20:37:35+00:00
- **Authors**: Chenyun Wu, Mikayla Timm, Subhransu Maji
- **Comment**: ECCV 2020 Oral. Code and dataset are released at:
  https://people.cs.umass.edu/~chenyun/texture
- **Journal**: None
- **Summary**: Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset.



### Weakly-Supervised Semantic Segmentation via Sub-category Exploration
- **Arxiv ID**: http://arxiv.org/abs/2008.01183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01183v1)
- **Published**: 2020-08-03 20:48:31+00:00
- **Updated**: 2020-08-03 20:48:31+00:00
- **Authors**: Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang
- **Comment**: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition. 2020
- **Journal**: None
- **Summary**: Existing weakly-supervised semantic segmentation methods using image-level annotations typically rely on initial responses to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts, due to the fact that the network does not need the entire object for optimizing the objective function. To enforce the network to pay attention to other parts of an object, we propose a simple yet effective approach that introduces a self-supervised task by exploiting the sub-category information. Specifically, we perform clustering on image features to generate pseudo sub-categories labels within each annotated parent class, and construct a sub-category objective to assign the network to a more challenging task. By iteratively clustering image features, the training process does not limit itself to the most discriminative object parts, hence improving the quality of the response maps. We conduct extensive analysis to validate the proposed method and show that our approach performs favorably against the state-of-the-art approaches.



### PhraseCut: Language-based Image Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2008.01187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01187v1)
- **Published**: 2020-08-03 20:58:53+00:00
- **Updated**: 2020-08-03 20:58:53+00:00
- **Authors**: Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, Subhransu Maji
- **Comment**: Published in CVPR 2020. Code and data are released at:
  https://people.cs.umass.edu/~chenyun/phrasecut
- **Journal**: None
- **Summary**: We consider the problem of segmenting image regions given a natural language phrase, and study it on a novel dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated. Phrases in our dataset correspond to multiple regions and describe a large number of object and stuff categories as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments show that the scale and diversity of concepts in our dataset poses significant challenges to the existing state-of-the-art. We systematically handle the long-tail nature of these concepts and present a modular approach to combine category, attribute, and relationship cues that outperforms existing approaches.



### Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization
- **Arxiv ID**: http://arxiv.org/abs/2008.01201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01201v1)
- **Published**: 2020-08-03 21:19:08+00:00
- **Updated**: 2020-08-03 21:19:08+00:00
- **Authors**: Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang
- **Comment**: Accepted at BMVC 2020
- **Journal**: None
- **Summary**: Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map. Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches.



### Generalized Zero-Shot Domain Adaptation via Coupled Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2008.01214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01214v1)
- **Published**: 2020-08-03 21:48:50+00:00
- **Updated**: 2020-08-03 21:48:50+00:00
- **Authors**: Qian Wang, Toby P. Breckon
- **Comment**: Durham University
- **Journal**: None
- **Summary**: Domain adaptation approaches aim to exploit useful information from the source domain where supervised learning examples are easier to obtain to address a learning problem in the target domain where there is no or limited availability of such examples. In classification problems, domain adaptation has been studied under varying supervised, unsupervised and semi-supervised conditions. However, a common situation when the labelled samples are available for a subset of target domain classes has been overlooked. In this paper, we formulate this particular domain adaptation problem within a generalized zero-shot learning framework by treating the labelled source domain samples as semantic representations for zero-shot learning. For this particular problem, neither conventional domain adaptation approaches nor zero-shot learning algorithms directly apply. To address this generalized zero-shot domain adaptation problem, we present a novel Coupled Conditional Variational Autoencoder (CCVAE) which can generate synthetic target domain features for unseen classes from their source domain counterparts. Extensive experiments have been conducted on three domain adaptation datasets including a bespoke X-ray security checkpoint dataset to simulate a real-world application in aviation security. The results demonstrate the effectiveness of our proposed approach both against established benchmarks and in terms of real-world applicability.



### Generalisable Cardiac Structure Segmentation via Attentional and Stacked Image Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.01216v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01216v2)
- **Published**: 2020-08-03 21:51:15+00:00
- **Updated**: 2020-09-15 20:00:16+00:00
- **Authors**: Hongwei Li, Jianguo Zhang, Bjoern Menze
- **Comment**: method description of our solution in M&M segmentation challenge,
  STACOM 2020
- **Journal**: None
- **Summary**: Tackling domain shifts in multi-centre and multi-vendor data sets remains challenging for cardiac image segmentation. In this paper, we propose a generalisable segmentation framework for cardiac image segmentation in which multi-centre, multi-vendor, multi-disease datasets are involved. A generative adversarial networks with an attention loss was proposed to translate the images from existing source domains to a target domain, thus to generate good-quality synthetic cardiac structure and enlarge the training set. A stack of data augmentation techniques was further used to simulate real-world transformation to boost the segmentation performance for unseen domains.We achieved an average Dice score of 90.3% for the left ventricle, 85.9% for the myocardium, and 86.5% for the right ventricle on the hidden validation set across four vendors. We show that the domain shifts in heterogeneous cardiac imaging datasets can be drastically reduced by two aspects: 1) good-quality synthetic data by learning the underlying target domain distribution, and 2) stacked classical image processing techniques for data augmentation.



### Multi-Class 3D Object Detection Within Volumetric 3D Computed Tomography Baggage Security Screening Imagery
- **Arxiv ID**: http://arxiv.org/abs/2008.01218v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01218v1)
- **Published**: 2020-08-03 21:54:14+00:00
- **Updated**: 2020-08-03 21:54:14+00:00
- **Authors**: Qian Wang, Neelanjan Bhowmik, Toby P. Breckon
- **Comment**: Durham University
- **Journal**: None
- **Summary**: Automatic detection of prohibited objects within passenger baggage is important for aviation security. X-ray Computed Tomography (CT) based 3D imaging is widely used in airports for aviation security screening whilst prior work on automatic prohibited item detection focus primarily on 2D X-ray imagery. These works have proven the possibility of extending deep convolutional neural networks (CNN) based automatic prohibited item detection from 2D X-ray imagery to volumetric 3D CT baggage security screening imagery. However, previous work on 3D object detection in baggage security screening imagery focused on the detection of one specific type of objects (e.g., either {\it bottles} or {\it handguns}). As a result, multiple models are needed if more than one type of prohibited item is required to be detected in practice. In this paper, we consider the detection of multiple object categories of interest using one unified framework. To this end, we formulate a more challenging multi-class 3D object detection problem within 3D CT imagery and propose a viable solution (3D RetinaNet) to tackle this problem. To enhance the performance of detection we investigate a variety of strategies including data augmentation and varying backbone networks. Experimentation carried out to provide both quantitative and qualitative evaluations of the proposed approach to multi-class 3D object detection within 3D CT baggage security screening imagery. Experimental results demonstrate the combination of the 3D RetinaNet and a series of favorable strategies can achieve a mean Average Precision (mAP) of 65.3\% over five object classes (i.e. {\it bottles, handguns, binoculars, glock frames, iPods}). The overall performance is affected by the poor performance on {\it glock frames} and {\it iPods} due to the lack of data and their resemblance with the baggage clutter.



### Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.01232v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01232v3)
- **Published**: 2020-08-03 22:57:22+00:00
- **Updated**: 2020-09-17 20:25:02+00:00
- **Authors**: M. Esat Kalfaoglu, Sinan Kalkan, A. Aydin Alatan
- **Comment**: Presented on the 2nd Workshop on Video Turing Test: Toward
  Human-Level Video Story Understanding, ECCV 2020
- **Journal**: None
- **Summary**: In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D convolutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT's attention mechanism. We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available.



