# Arxiv Papers in cs.CV on 2023-11-04
### OSM vs HD Maps: Map Representations for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2311.02305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2311.02305v1)
- **Published**: 2023-11-04 02:43:32+00:00
- **Updated**: 2023-11-04 02:43:32+00:00
- **Authors**: Jing-Yan Liao, Parth Doshi, Zihan Zhang, David Paz, Henrik Christensen
- **Comment**: None
- **Journal**: None
- **Summary**: While High Definition (HD) Maps have long been favored for their precise depictions of static road elements, their accessibility constraints and susceptibility to rapid environmental changes impede the widespread deployment of autonomous driving, especially in the motion forecasting task. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded receptive field and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a potential scalable solution within the domain of autonomous driving.



### LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2311.02313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02313v1)
- **Published**: 2023-11-04 03:55:38+00:00
- **Updated**: 2023-11-04 03:55:38+00:00
- **Authors**: Jianyuan Zhang, Zhiliu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale semantic mapping is crucial for outdoor autonomous agents to fulfill high-level tasks such as planning and navigation. This paper proposes a novel method for large-scale 3D semantic reconstruction through implicit representations from LiDAR measurements alone. We firstly leverages an octree-based and hierarchical structure to store implicit features, then these implicit features are decoded to semantic information and signed distance value through shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf algorithms to predict the semantic labels and instance IDs of point cloud. Then we jointly optimize the implicit features and MLPs parameters with self-supervision paradigm for point cloud geometry and pseudo-supervision pradigm for semantic and panoptic labels. Subsequently, Marching Cubes algorithm is exploited to subdivide and visualize the scenes in the inferring stage. For scenarios with memory constraints, a map stitching strategy is also developed to merge sub-maps into a complete map. As far as we know, our method is the first work to reconstruct semantic implicit scenes from LiDAR-only input. Experiments on three real-world datasets, SemanticKITTI, SemanticPOSS and nuScenes, demonstrate the effectiveness and efficiency of our framework compared to current state-of-the-art 3D mapping methods.



### Thermal Face Image Classification using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2311.02314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2311.02314v1)
- **Published**: 2023-11-04 03:56:40+00:00
- **Updated**: 2023-11-04 03:56:40+00:00
- **Authors**: Prosenjit Chatterjee, ANK Zaman
- **Comment**: 6 pages. Link of the Conference: https://american-cse.org/index.html/
- **Journal**: None
- **Summary**: Thermal images have various applications in security, medical and industrial domains. This paper proposes a practical deep-learning approach for thermal image classification. Accurate and efficient classification of thermal images poses a significant challenge across various fields due to the complex image content and the scarcity of annotated datasets. This work uses a convolutional neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to extract features from thermal images. This work also applied Kalman filter on thermal input images for image denoising. The experimental results demonstrate the effectiveness of the proposed approach in terms of accuracy and efficiency.



### Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel
- **Arxiv ID**: http://arxiv.org/abs/2311.02315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02315v1)
- **Published**: 2023-11-04 03:58:24+00:00
- **Updated**: 2023-11-04 03:58:24+00:00
- **Authors**: Zhiqiang Wang, Yiran Pang, Cihan Ulus, Xingquan Zhu
- **Comment**: 18 pages, 8 figures, 2 tables, 3 algorithms, and it has been accepted
  for publication in Scientific Reports
- **Journal**: None
- **Summary**: Manatees are aquatic mammals with voracious appetites. They rely on sea grass as the main food source, and often spend up to eight hours a day grazing. They move slow and frequently stay in group (i.e. aggregations) in shallow water to search for food, making them vulnerable to environment change and other risks. Accurate counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for human boaters, divers, etc., as well as scheduling nursing, intervention, and other plans. In this paper, we propose a deep learning based crowd counting approach to automatically count number of manatees within a region, by using low quality images as input. Because manatees have unique shape and they often stay in shallow water in groups, water surface reflection, occlusion, camouflage etc. making it difficult to accurately count manatee numbers. To address the challenges, we propose to use Anisotropic Gaussian Kernel (AGK), with tunable rotation and variances, to ensure that density functions can maximally capture shapes of individual manatees in different aggregations. After that, we apply AGK kernel to different types of deep neural networks primarily designed for crowd counting, including VGG, SANet, Congested Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and calculate number of manatees in the scene. By using generic low quality images extracted from surveillance videos, our experiment results and comparison show that AGK kernel based manatee counting achieves minimum Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The proposed method works particularly well for counting manatee aggregations in environments with complex background.



### Complex Organ Mask Guided Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2311.02329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2311.02329v1)
- **Published**: 2023-11-04 05:34:24+00:00
- **Updated**: 2023-11-04 05:34:24+00:00
- **Authors**: Gu Tiancheng, Liu Dongnan, Li Zhiyuan, Cai Weidong
- **Comment**: 12 pages, 7 images. Accepted by WACV 2024
- **Journal**: None
- **Summary**: The goal of automatic report generation is to generate a clinically accurate and coherent phrase from a single given X-ray image, which could alleviate the workload of traditional radiology reporting.However, in a real-world scenario, radiologists frequently face the challenge of producing extensive reports derived from numerous medical images, thereby medical report generation from multi-image perspective is needed.In this paper, we propose the Complex Organ Mask Guided (termed as COMG) report generation model, which incorporates masks from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide more detailed information and guide the model's attention to these crucial body regions. Specifically, we leverage prior knowledge of the disease corresponding to each organ in the fusion process to enhance the disease identification phase during the report generation process. Additionally, cosine similarity loss is introduced as target function to ensure the convergence of cross-modal consistency and facilitate model optimization.Experimental results on two public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.



### Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications
- **Arxiv ID**: http://arxiv.org/abs/2311.02332v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2311.02332v1)
- **Published**: 2023-11-04 05:42:51+00:00
- **Updated**: 2023-11-04 05:42:51+00:00
- **Authors**: Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles Kahn, Arvind Rao
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models and even more recently generative models. Recent years have seen a rise in the discovery of widely-available deep learning architectures that support multimodal data integration, particularly with images. The incorporation of multiple modalities into these models is a thriving research topic, presenting its own unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be elucidated further for successful clinical models and their translation to the clinical setting.



### STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots
- **Arxiv ID**: http://arxiv.org/abs/2311.02337v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2311.02337v1)
- **Published**: 2023-11-04 06:52:38+00:00
- **Updated**: 2023-11-04 06:52:38+00:00
- **Authors**: Yi Li, Muru Zhang, Markus Grotz, Kaichun Mo, Dieter Fox
- **Comment**: CoRL 2023, project page: https://sites.google.com/view/stow-corl23
- **Journal**: None
- **Summary**: Segmentation and tracking of unseen object instances in discrete frames pose a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.



### Potato Leaf Disease Classification using Deep Learning: A Convolutional Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/2311.02338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2311.02338v1)
- **Published**: 2023-11-04 07:16:37+00:00
- **Updated**: 2023-11-04 07:16:37+00:00
- **Authors**: Utkarsh Yashwant Tambe, A. Shobanadevi, A. Shanthini, Hsiu-Chun Hsu
- **Comment**: Accepted at the International Conference on Recent Trends in Data
  Science and its Applications (ICRTDA 2023), 6 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: In this study, a Convolutional Neural Network (CNN) is used to classify potato leaf illnesses using Deep Learning. The suggested approach entails preprocessing the leaf image data, training a CNN model on that data, and assessing the model's success on a test set. The experimental findings show that the CNN model, with an overall accuracy of 99.1%, is highly accurate in identifying two kinds of potato leaf diseases, including Early Blight, Late Blight, and Healthy. The suggested method may offer a trustworthy and effective remedy for identifying potato diseases, which is essential for maintaining food security and minimizing financial losses in agriculture. The model can accurately recognize the various disease types even when there are severe infections present. This work highlights the potential of deep learning methods for categorizing potato diseases, which can help with effective and automated disease management in potato farming.



### MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2311.02340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02340v1)
- **Published**: 2023-11-04 07:26:27+00:00
- **Updated**: 2023-11-04 07:26:27+00:00
- **Authors**: Miaojie Feng, Junda Cheng, Hao Jia, Longliang Liu, Gangwei Xu, Xin Yang
- **Comment**: Accepted to 3DV 2024
- **Journal**: None
- **Summary**: Stereo matching is a fundamental task in scene comprehension. In recent years, the method based on iterative optimization has shown promise in stereo matching. However, the current iteration framework employs a single-peak lookup, which struggles to handle the multi-peak problem effectively. Additionally, the fixed search range used during the iteration process limits the final convergence effects. To address these issues, we present a novel iterative optimization architecture called MC-Stereo. This architecture mitigates the multi-peak distribution problem in matching through the multi-peak lookup strategy, and integrates the coarse-to-fine concept into the iterative framework via the cascade search range. Furthermore, given that feature representation learning is crucial for successful learnbased stereo matching, we introduce a pre-trained network to serve as the feature extractor, enhancing the front end of the stereo matching pipeline. Based on these improvements, MC-Stereo ranks first among all publicly available methods on the KITTI-2012 and KITTI-2015 benchmarks, and also achieves state-of-the-art performance on ETH3D. The code will be open sourced after the publication of this paper.



### Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector
- **Arxiv ID**: http://arxiv.org/abs/2311.02342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02342v1)
- **Published**: 2023-11-04 07:46:45+00:00
- **Updated**: 2023-11-04 07:46:45+00:00
- **Authors**: Xuanyi Liu, Zhongqi Yue, Xian-Sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Open World Object Detection (OWOD) combines open-set object detection with incremental learning capabilities to handle the challenge of the open and dynamic visual world. Existing works assume that a foreground predictor trained on the seen categories can be directly transferred to identify the unseen categories' locations by selecting the top-k most confident foreground predictions. However, the assumption is hardly valid in practice. This is because the predictor is inevitably biased to the known categories, and fails under the shift in the appearance of the unseen categories. In this work, we aim to build an unbiased foreground predictor by re-formulating the task under Unsupervised Domain Adaptation, where the current biased predictor helps form the domains: the seen object locations and confident background locations as the source domain, and the rest ambiguous ones as the target domain. Then, we adopt the simple and effective self-training method to learn a predictor based on the domain-invariant foreground features, hence achieving unbiased prediction robust to the shift in appearance between the seen and unseen categories. Our approach's pipeline can adapt to various detection frameworks and UDA methods, empirically validated by OWOD evaluation, where we achieve state-of-the-art performance.



### Stable Diffusion Reference Only: Image Prompt and Blueprint Jointly Guided Multi-Condition Diffusion Model for Secondary Painting
- **Arxiv ID**: http://arxiv.org/abs/2311.02343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2311.02343v1)
- **Published**: 2023-11-04 07:53:59+00:00
- **Updated**: 2023-11-04 07:53:59+00:00
- **Authors**: Hao Ai, Lu Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: Stable Diffusion and ControlNet have achieved excellent results in the field of image generation and synthesis. However, due to the granularity and method of its control, the efficiency improvement is limited for professional artistic creations such as comics and animation production whose main work is secondary painting. In the current workflow, fixing characters and image styles often need lengthy text prompts, and even requires further training through TextualInversion, DreamBooth or other methods, which is very complicated and expensive for painters. Therefore, we present a new method in this paper, Stable Diffusion Reference Only, a images-to-image self-supervised model that uses only two types of conditional images for precise control generation to accelerate secondary painting. The first type of conditional image serves as an image prompt, supplying the necessary conceptual and color information for generation. The second type is blueprint image, which controls the visual structure of the generated image. It is natively embedded into the original UNet, eliminating the need for ControlNet. We released all the code for the module and pipeline, and trained a controllable character line art coloring model at https://github.com/aihao2000/stable-diffusion-reference-only, that achieved state-of-the-art results in this field. This verifies the effectiveness of the structure and greatly improves the production efficiency of animations, comics, and fanworks.



### Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution -- a Non-Denoising Model
- **Arxiv ID**: http://arxiv.org/abs/2311.02358v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2311.02358v1)
- **Published**: 2023-11-04 09:57:50+00:00
- **Updated**: 2023-11-04 09:57:50+00:00
- **Authors**: Chun-Chuen Hui, Wan-Chi Siu, Ngai-Fong Law
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale image super-resolution is a challenging computer vision task, since vast information is missing in a highly degraded image, say for example forscale x16 super-resolution. Diffusion models are used successfully in recent years in extreme super-resolution applications, in which Gaussian noise is used as a means to form a latent photo-realistic space, and acts as a link between the space of latent vectors and the latent photo-realistic space. There are quite a few sophisticated mathematical derivations on mapping the statistics of Gaussian noises making Diffusion Models successful. In this paper we propose a simple approach which gets away from using Gaussian noise but adopts some basic structures of diffusion models for efficient image super-resolution. Essentially, we propose a DNN to perform domain transfer between neighbor domains, which can learn the differences in statistical properties to facilitate gradual interpolation with results of reasonable quality. Further quality improvement is achieved by conditioning the domain transfer with reference to the input LR image. Experimental results show that our method outperforms not only state-of-the-art large scale super resolution models, but also the current diffusion models for image super-resolution. The approach can readily be extended to other image-to-image tasks, such as image enlightening, inpainting, denoising, etc.



### Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2311.02392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2311.02392v1)
- **Published**: 2023-11-04 12:28:04+00:00
- **Updated**: 2023-11-04 12:28:04+00:00
- **Authors**: Hao Zheng, Runqi Wang, Jianzhuang Liu, Asako Kanezaki
- **Comment**: None
- **Journal**: None
- **Summary**: The conventional few-shot classification aims at learning a model on a large labeled base dataset and rapidly adapting to a target dataset that is from the same distribution as the base dataset. However, in practice, the base and the target datasets of few-shot classification are usually from different domains, which is the problem of cross-domain few-shot classification. We tackle this problem by making a small proportion of unlabeled images in the target domain accessible in the training stage. In this setup, even though the base data are sufficient and labeled, the large domain shift still makes transferring the knowledge from the base dataset difficult. We meticulously design a cross-level knowledge distillation method, which can strengthen the ability of the model to extract more discriminative features in the target dataset by guiding the network's shallow layers to learn higher-level information. Furthermore, in order to alleviate the overfitting in the evaluation stage, we propose a feature denoising operation which can reduce the feature redundancy and mitigate overfitting. Our approach can surpass the previous state-of-the-art method, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot classification tasks on average in the BSCD-FSL benchmark. The implementation code will be available at https://github.com/jarucezh/cldfd.



### Continual Learning of Unsupervised Monocular Depth from Videos
- **Arxiv ID**: http://arxiv.org/abs/2311.02393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2311.02393v1)
- **Published**: 2023-11-04 12:36:07+00:00
- **Updated**: 2023-11-04 12:36:07+00:00
- **Authors**: Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV 2024)
- **Journal**: None
- **Summary**: Spatial scene understanding, including monocular depth estimation, is an important problem in various applications, such as robotics and autonomous driving. While improvements in unsupervised monocular depth estimation have potentially allowed models to be trained on diverse crowdsourced videos, this remains underexplored as most methods utilize the standard training protocol, wherein the models are trained from scratch on all data after new data is collected. Instead, continual training of models on sequentially collected data would significantly reduce computational and memory costs. Nevertheless, naive continual training leads to catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, highlighting the trade-off between model stability and plasticity. While several techniques have been proposed to address this issue in image classification, the high-dimensional and spatiotemporally correlated outputs of depth estimation make it a distinct challenge. To the best of our knowledge, no framework or method currently exists focusing on the problem of continual learning in depth estimation. Thus, we introduce a framework that captures the challenges of continual unsupervised depth estimation (CUDE), and define the necessary metrics to evaluate model performance. We propose a rehearsal-based dual-memory method, MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown.



### Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2311.02402v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2311.02402v1)
- **Published**: 2023-11-04 13:28:06+00:00
- **Updated**: 2023-11-04 13:28:06+00:00
- **Authors**: Luca Lusnig, Asel Sagingalieva, Mikhail Surmach, Tatjana Protasevich, Ovidiu Michiu, Joseph McLoughlin, Christopher Mansell, Graziano de' Petris, Deborah Bonazza, Fabrizio Zanconati, Alexey Melnikov, Fabio Cavalli
- **Comment**: 10 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: With the maturity achieved by deep learning techniques, intelligent systems that can assist physicians in the daily interpretation of clinical images can play a very important role. In addition, quantum techniques applied to deep learning can enhance this performance, and federated learning techniques can realize privacy-friendly collaborative learning among different participants, solving privacy issues due to the use of sensitive data and reducing the number of data to be collected for each individual participant. We present in this study a hybrid quantum neural network that can be used to quantify non-alcoholic liver steatosis and could be useful in the diagnostic process to determine a liver's suitability for transplantation; at the same time, we propose a federated learning approach based on a classical deep learning solution to solve the same problem, but using a reduced data set in each part. The liver steatosis image classification accuracy of the hybrid quantum neural network, the hybrid quantum ResNet model, consisted of 5 qubits and more than 100 variational gates, reaches 97%, which is 1.8% higher than its classical counterpart, ResNet. Crucially, that even with a reduced dataset, our hybrid approach consistently outperformed its classical counterpart, indicating superior generalization and less potential for overfitting in medical applications. In addition, a federated approach with multiple clients, up to 32, despite the lower accuracy, but still higher than 90%, would allow using, for each participant, a very small dataset, i.e., up to one-thirtieth. Our work, based over real-word clinical data can be regarded as a scalable and collaborative starting point, could thus fulfill the need for an effective and reliable computer-assisted system that facilitates the daily diagnostic work of the clinical pathologist.



### P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship
- **Arxiv ID**: http://arxiv.org/abs/2311.02413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02413v1)
- **Published**: 2023-11-04 14:32:55+00:00
- **Updated**: 2023-11-04 14:32:55+00:00
- **Authors**: Su Wang, Shini Zhang, Xuchong Qiu
- **Comment**: Accepted to IROS 2023. Presentation page:
  https://events.infovaya.com/presentation?id=103943
- **Journal**: 2023 IEEE/RSJ International Conference on Intelligent Robots and
  Systems
- **Summary**: The accurate and robust calibration result of sensors is considered as an important building block to the follow-up research in the autonomous driving and robotics domain. The current works involving extrinsic calibration between 3D LiDARs and monocular cameras mainly focus on target-based and target-less methods. The target-based methods are often utilized offline because of restrictions, such as additional target design and target placement limits. The current target-less methods suffer from feature indeterminacy and feature mismatching in various environments. To alleviate these limitations, we propose a novel target-less calibration approach which is based on the 2D-3D edge point extraction using the occlusion relationship in 3D space. Based on the extracted 2D-3D point pairs, we further propose an occlusion-guided point-matching method that improves the calibration accuracy and reduces computation costs. To validate the effectiveness of our approach, we evaluate the method performance qualitatively and quantitatively on real images from the KITTI dataset. The results demonstrate that our method outperforms the existing target-less methods and achieves low error and high robustness that can contribute to the practical applications relying on high-quality Camera-LiDAR calibration.



### Task Arithmetic with LoRA for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2311.02428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2311.02428v1)
- **Published**: 2023-11-04 15:12:24+00:00
- **Updated**: 2023-11-04 15:12:24+00:00
- **Authors**: Rajas Chitale, Ankit Vaidya, Aditya Kane, Archana Ghotkar
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning refers to the problem where the training data is available in sequential chunks, termed "tasks". The majority of progress in continual learning has been stunted by the problem of catastrophic forgetting, which is caused by sequential training of the model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.



### P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification
- **Arxiv ID**: http://arxiv.org/abs/2311.02432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02432v1)
- **Published**: 2023-11-04 15:23:21+00:00
- **Updated**: 2023-11-04 15:23:21+00:00
- **Authors**: Abid Ali, Ashish Marisetty, Francois Bremond
- **Comment**: None
- **Journal**: WACV 2024
- **Summary**: Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating face-based methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14.



### Extracting Network Structures from Corporate Organization Charts Using Heuristic Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2311.02460v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2311.02460v1)
- **Published**: 2023-11-04 17:32:50+00:00
- **Updated**: 2023-11-04 17:32:50+00:00
- **Authors**: Hiroki Sayama, Junichi Yamanoi
- **Comment**: 8 pages, 12 figures, 1 table; to be published in "Data Science and
  Security: Proceedings of IDSCS 2023" (Springer)
- **Journal**: None
- **Summary**: Organizational structure of corporations has potential to provide implications for dynamics and performance of corporate operations. However, this subject has remained unexplored because of the lack of readily available organization network datasets. To overcome the this gap, we developed a new heuristic image-processing method to extract and reconstruct organization network data from published organization charts. Our method analyzes a PDF file of a corporate organization chart and detects text labels, boxes, connecting lines, and other objects through multiple steps of heuristically implemented image processing. The detected components are reorganized together into a Python's NetworkX Graph object for visualization, validation and further network analysis. We applied the developed method to the organization charts of all the listed firms in Japan shown in the ``Organization Chart/System Diagram Handbook'' published by Diamond, Inc., from 2008 to 2011. Out of the 10,008 organization chart PDF files, our method was able to reconstruct 4,606 organization networks (data acquisition success rate: 46%). For each reconstructed organization network, we measured several network diagnostics, which will be used for further statistical analysis to investigate their potential correlations with corporate behavior and performance.



### SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling
- **Arxiv ID**: http://arxiv.org/abs/2311.02461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2311.02461v1)
- **Published**: 2023-11-04 17:38:20+00:00
- **Updated**: 2023-11-04 17:38:20+00:00
- **Authors**: Eduard Gabriel Bazavan, Andrei Zanfir, Thiemo Alldieck, Teodor Alexandru Szente, Mihai Zanfir, Cristian Sminchisescu
- **Comment**: To be published at the International Conference on 3D Vision 2024
- **Journal**: None
- **Summary**: We present \emph{SPHEAR}, an accurate, differentiable parametric statistical 3D human head model, enabled by a novel 3D registration method based on spherical embeddings. We shift the paradigm away from the classical Non-Rigid Registration methods, which operate under various surface priors, increasing reconstruction fidelity and minimizing required human intervention. Additionally, SPHEAR is a \emph{complete} model that allows not only to sample diverse synthetic head shapes and facial expressions, but also gaze directions, high-resolution color textures, surface normal maps, and hair cuts represented in detail, as strands. SPHEAR can be used for automatic realistic visual data generation, semantic annotation, and general reconstruction tasks. Compared to state-of-the-art approaches, our components are fast and memory efficient, and experiments support the validity of our design choices and the accuracy of registration, reconstruction and generation techniques.



### A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2311.02480v1
- **DOI**: 10.1109/SSP53291.2023.10207960
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2311.02480v1)
- **Published**: 2023-11-04 18:43:31+00:00
- **Updated**: 2023-11-04 18:43:31+00:00
- **Authors**: Swati Rai, Jignesh S. Bhatt, Sarat Kumar Patra
- **Comment**: None
- **Journal**: 2023 IEEE Statistical Signal Processing Workshop (SSP), Hanoi,
  Vietnam, 2023, pp. 61-65
- **Summary**: Medical image translation is an ill-posed problem. Unlike existing paired unbounded unidirectional translation networks, in this paper, we consider unpaired medical images and provide a strictly bounded network that yields a stable bidirectional translation. We propose a patch-level concatenated cyclic conditional generative adversarial network (pCCGAN) embedded with adaptive dictionary learning. It consists of two cyclically connected CGANs of 47 layers each; where both generators (each of 32 layers) are conditioned with concatenation of alternate unpaired patches from input and target modality images (not ground truth) of the same organ. The key idea is to exploit cross-neighborhood contextual feature information that bounds the translation space and boosts generalization. The generators are further equipped with adaptive dictionaries learned from the contextual patches to reduce possible degradation. Discriminators are 15-layer deep networks that employ minimax function to validate the translated imagery. A combined loss function is formulated with adversarial, non-adversarial, forward-backward cyclic, and identity losses that further minimize the variance of the proposed learning machine. Qualitative, quantitative, and ablation analysis show superior results on real CT and MRI.



### Neural Network Reconstruction of the Left Atrium using Sparse Catheter Paths
- **Arxiv ID**: http://arxiv.org/abs/2311.02488v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2311.02488v1)
- **Published**: 2023-11-04 19:21:24+00:00
- **Updated**: 2023-11-04 19:21:24+00:00
- **Authors**: Alon Baram, Moshe Safran, Tomer Noy, Naveh Geri, Hayit Greenspan
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: Catheter based radiofrequency ablation for pulmonary vein isolation has become the first line of treatment for atrial fibrillation in recent years. This requires a rather accurate map of the left atrial sub-endocardial surface including the ostia of the pulmonary veins, which requires dense sampling of the surface and takes more than 10 minutes. The focus of this work is to provide left atrial visualization early in the procedure to ease procedure complexity and enable further workflows, such as using catheters that have difficulty sampling the surface. We propose a dense encoder-decoder network with a novel regularization term to reconstruct the shape of the left atrium from partial data which is derived from simple catheter maneuvers. To train the network, we acquire a large dataset of 3D atria shapes and generate corresponding catheter trajectories. Once trained, we show that the suggested network can sufficiently approximate the atrium shape based on a given trajectory. We compare several network solutions for the 3D atrium reconstruction. We demonstrate that the solution proposed produces realistic visualization using partial acquisition within a 3-minute time interval. Synthetic and human clinical cases are shown.



### MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters
- **Arxiv ID**: http://arxiv.org/abs/2311.02502v1
- **DOI**: 10.1145/3606926
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO, 68U99, I.3.8; I.3.m
- **Links**: [PDF](http://arxiv.org/pdf/2311.02502v1)
- **Published**: 2023-11-04 20:40:39+00:00
- **Updated**: 2023-11-04 20:40:39+00:00
- **Authors**: Mohamed Younes, Ewa Kijak, Richard Kulpa, Simon Malinowski, Franck Multon
- **Comment**: SCA'23, Supplementary video: https://youtu.be/wQfIiw_rQ3w
- **Journal**: ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA),
  August 4-6, 2023, Los Angeles, CA, USA
- **Summary**: Simulating realistic interaction and motions for physics-based characters is of great interest for interactive applications, and automatic secondary character animation in the movie and video game industries. Recent works in reinforcement learning have proposed impressive results for single character simulation, especially the ones that use imitation learning based techniques. However, imitating multiple characters interactions and motions requires to also model their interactions. In this paper, we propose a novel Multi-Agent Generative Adversarial Imitation Learning based approach that generalizes the idea of motion imitation for one character to deal with both the interaction and the motions of the multiple physics-based characters. Two unstructured datasets are given as inputs: 1) a single-actor dataset containing motions of a single actor performing a set of motions linked to a specific application, and 2) an interaction dataset containing a few examples of interactions between multiple actors. Based on these datasets, our system trains control policies allowing each character to imitate the interactive skills associated with each actor, while preserving the intrinsic style. This approach has been tested on two different fighting styles, boxing and full-body martial art, to demonstrate the ability of the method to imitate different styles.



### Anthropomorphic Grasping with Neural Object Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/2311.02510v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2311.02510v1)
- **Published**: 2023-11-04 21:05:26+00:00
- **Updated**: 2023-11-04 21:05:26+00:00
- **Authors**: Diego Hidalgo-Carvajal, Hanzhi Chen, Gemma C. Bettelani, Jaesug Jung, Melissa Zavaglia, Laura Busse, Abdeldjallil Naceri, Stefan Leutenegger, Sami Haddadin
- **Comment**: Accepted to RA-L 2023
- **Journal**: None
- **Summary**: The progressive prevalence of robots in human-suited environments has given rise to a myriad of object manipulation techniques, in which dexterity plays a paramount role. It is well-established that humans exhibit extraordinary dexterity when handling objects. Such dexterity seems to derive from a robust understanding of object properties (such as weight, size, and shape), as well as a remarkable capacity to interact with them. Hand postures commonly demonstrate the influence of specific regions on objects that need to be grasped, especially when objects are partially visible. In this work, we leverage human-like object understanding by reconstructing and completing their full geometry from partial observations, and manipulating them using a 7-DoF anthropomorphic robot hand. Our approach has significantly improved the grasping success rates of baselines with only partial reconstruction by nearly 30% and achieved over 150 successful grasps with three different object categories. This demonstrates our approach's consistent ability to predict and execute grasping postures based on the completed object shapes from various directions and positions in real-world scenarios. Our work opens up new possibilities for enhancing robotic applications that require precise grasping and manipulation skills of real-world reconstructed objects.



### UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2311.02523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2311.02523v1)
- **Published**: 2023-11-04 23:00:40+00:00
- **Updated**: 2023-11-04 23:00:40+00:00
- **Authors**: Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan
- **Comment**: Accepted by Neurips 2023
- **Journal**: None
- **Summary**: Sample-to-class-based face recognition models can not fully explore the cross-sample relationship among large amounts of facial images, while sample-to-sample-based models require sophisticated pairing processes for training. Furthermore, neither method satisfies the requirements of real-world face verification applications, which expect a unified threshold separating positive from negative facial pairs. In this paper, we propose a unified threshold integrated sample-to-sample based loss (USS loss), which features an explicit unified threshold for distinguishing positive from negative pairs. Inspired by our USS loss, we also derive the sample-to-sample based softmax and BCE losses, and discuss their relationship. Extensive evaluation on multiple benchmark datasets, including MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace, demonstrates that the proposed USS loss is highly efficient and can work seamlessly with sample-to-class-based losses. The embedded loss (USS and sample-to-class Softmax loss) overcomes the pitfalls of previous approaches and the trained facial model UniTSFace exhibits exceptional performance, outperforming state-of-the-art methods, such as CosFace, ArcFace, VPL, AnchorFace, and UNPG. Our code is available.



