# Arxiv Papers in cs.CV on 2023-02-24
### On-Device Unsupervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.12753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.12753v1)
- **Published**: 2023-02-24 00:51:17+00:00
- **Updated**: 2023-02-24 00:51:17+00:00
- **Authors**: Junhuan Yang, Yi Sheng, Yuzhou Zhang, Weiwen Jiang, Lei Yang
- **Comment**: To be published in Design Automation Conference (DAC) 2023
- **Journal**: None
- **Summary**: Along with the breakthrough of convolutional neural networks, learning-based segmentation has emerged in many research works. Most of them are based on supervised learning, requiring plenty of annotated data; however, to support segmentation, a label for each pixel is required, which is obviously expensive. As a result, the issue of lacking annotated segmentation data commonly exists. Continuous learning is a promising way to deal with this issue; however, it still has high demands on human labor for annotation. What's more, privacy is highly required in segmentation data for real-world applications, which further calls for on-device learning. In this paper, we aim to resolve the above issue in an alternative way: Instead of supervised segmentation, we propose to develop efficient unsupervised segmentation that can be executed on edge devices. Based on our observation that segmentation can obtain high performance when pixels are mapped to a high-dimension space, we for the first time bring brain-inspired hyperdimensional computing (HDC) to the segmentation task. We build the HDC-based unsupervised segmentation framework, namely "SegHDC". In SegHDC, we devise a novel encoding approach that follows the Manhattan distance. A clustering algorithm is further developed on top of the encoded high-dimension vectors to obtain segmentation results. Experimental results show SegHDC can significantly surpass neural network-based unsupervised segmentation. On a standard segmentation dataset, DSB2018, SegHDC can achieve a 28.0% improvement in Intersection over Union (IoU) score; meanwhile, it achieves over 300x speedup on Raspberry PI. What's more, for a larger size image in the BBBC005 dataset, the existing approach cannot be accommodated to Raspberry PI due to out of memory; on the other hand, SegHDC can obtain segmentation results within 3 minutes while achieving a 0.9587 IoU score.



### TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.14611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14611v1)
- **Published**: 2023-02-24 01:45:29+00:00
- **Updated**: 2023-02-24 01:45:29+00:00
- **Authors**: Debasmit Das, Shubhankar Borse, Hyojin Park, Kambiz Azarian, Hong Cai, Risheek Garrepalli, Fatih Porikli
- **Comment**: ICASSP 2023
- **Journal**: None
- **Summary**: Test-time adaptive (TTA) semantic segmentation adapts a source pre-trained image semantic segmentation model to unlabeled batches of target domain test images, different from real-world, where samples arrive one-by-one in an online fashion. To tackle online settings, we propose TransAdapt, a framework that uses transformer and input transformations to improve segmentation performance. Specifically, we pre-train a transformer-based module on a segmentation network that transforms unsupervised segmentation output to a more reliable supervised output, without requiring test-time online training. To also facilitate test-time adaptation, we propose an unsupervised loss based on the transformed input that enforces the model to be invariant and equivariant to photometric and geometric perturbations, respectively. Overall, our framework produces higher quality segmentation masks with up to 17.6% and 2.8% mIOU improvement over no-adaptation and competitive baselines, respectively.



### Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics
- **Arxiv ID**: http://arxiv.org/abs/2302.12393v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12393v1)
- **Published**: 2023-02-24 01:47:13+00:00
- **Updated**: 2023-02-24 01:47:13+00:00
- **Authors**: Wei Zhou, Zhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\times$360$^{\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global omnidirectional image, respectively. A quality regression along with a weighting process is then followed that maps the extracted quality-aware features to a perceptual quality prediction. Experimental results demonstrate that the proposed S$^2$ method offers highly competitive performance against state-of-the-art methods.



### Towards Stable Test-Time Adaptation in Dynamic Wild World
- **Arxiv ID**: http://arxiv.org/abs/2302.12400v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12400v1)
- **Published**: 2023-02-24 02:03:41+00:00
- **Updated**: 2023-02-24 02:03:41+00:00
- **Authors**: Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, Mingkui Tan
- **Comment**: accepted by International Conference on Learning Representations
  (ICLR) 2023 as Notable-Top-5%; 27 pages, 10 figures, 18 tables
- **Journal**: None
- **Summary**: Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.



### A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data
- **Arxiv ID**: http://arxiv.org/abs/2302.12416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.4.6; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.12416v1)
- **Published**: 2023-02-24 02:44:39+00:00
- **Updated**: 2023-02-24 02:44:39+00:00
- **Authors**: Hayat Rajani, Nuno Gracias, Rafael Garcia
- **Comment**: Submitted to Ocean Engineering special issue "Autonomous Marine
  Robotics Operations"
- **Journal**: None
- **Summary**: Distinguishing among different marine benthic habitat characteristics is of key importance in a wide set of seabed operations ranging from installations of oil rigs to laying networks of cables and monitoring the impact of humans on marine ecosystems. The Side-Scan Sonar (SSS) is a widely used imaging sensor in this regard. It produces high-resolution seafloor maps by logging the intensities of sound waves reflected back from the seafloor. In this work, we leverage these acoustic intensity maps to produce pixel-wise categorization of different seafloor types. We propose a novel architecture adapted from the Vision Transformer (ViT) in an encoder-decoder framework. Further, in doing so, the applicability of ViTs is evaluated on smaller datasets. To overcome the lack of CNN-like inductive biases, thereby making ViTs more conducive to applications in low data regimes, we propose a novel feature extraction module to replace the Multi-layer Perceptron (MLP) block within transformer layers and a novel module to extract multiscale patch embeddings. A lightweight decoder is also proposed to complement this design in order to further boost multiscale feature extraction. With the modified architecture, we achieve state-of-the-art results and also meet real-time computational requirements. We make our code available at ~\url{https://github.com/hayatrajani/s3seg-vit



### An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2302.12420v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.12420v2)
- **Published**: 2023-02-24 02:51:09+00:00
- **Updated**: 2023-04-24 08:29:55+00:00
- **Authors**: Zili Lu, Yuexing Peng, Wei Li, Junchuan Yu, Daqing Ge, Wei Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Huge challenges exist for old landslide detection because their morphology features have been partially or strongly transformed over a long time and have little difference from their surrounding. Besides, small-sample problem also restrict in-depth learning.   In this paper, an iterative classification and semantic segmentation network (ICSSN) is developed, which can greatly enhance both object-level and pixel-level classification performance by iteratively upgrading the feature extractor shared by two network. An object-level contrastive learning (OCL) strategy is employed in the object classification sub-network featuring a siamese network to realize the global features extraction, and a sub-object-level contrastive learning (SOCL) paradigm is designed in the semantic segmentation sub-network to efficiently extract salient features from boundaries of landslides. Moreover, an iterative training strategy is elaborated to fuse features in semantic space such that both object-level and pixel-level classification performance are improved.   The proposed ICSSN is evaluated on the real landslide data set, and the experimental results show that ICSSN can greatly improve the classification and segmentation accuracy of old landslide detection. For the semantic segmentation task, compared to the baseline, the F1 score increases from 0.5054 to 0.5448, the mIoU improves from 0.6405 to 0.6610, the landslide IoU improved from 0.3381 to 0.3743, and the object-level detection accuracy of old landslides is enhanced from 0.55 to 0.9. For the object classification task, the F1 score increases from 0.8846 to 0.9230, and the accuracy score is up from 0.8375 to 0.8875.



### RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2302.12464v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.12464v1)
- **Published**: 2023-02-24 05:43:03+00:00
- **Updated**: 2023-02-24 05:43:03+00:00
- **Authors**: Shancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, Jianjun Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs), trained on a large-scale image dataset, can be a good approximator of the natural image manifold. GAN-inversion, using a pre-trained generator as a deep generative prior, is a promising tool for image restoration under corruptions. However, the performance of GAN-inversion can be limited by a lack of robustness to unknown gross corruptions, i.e., the restored image might easily deviate from the ground truth. In this paper, we propose a Robust GAN-inversion (RGI) method with a provable robustness guarantee to achieve image restoration under unknown \textit{gross} corruptions, where a small fraction of pixels are completely corrupted. Under mild assumptions, we show that the restored image and the identified corrupted region mask converge asymptotically to the ground truth. Moreover, we extend RGI to Relaxed-RGI (R-RGI) for generator fine-tuning to mitigate the gap between the GAN learned manifold and the true image manifold while avoiding trivial overfitting to the corrupted input image, which further improves the image restoration and corrupted region mask identification performance. The proposed RGI/R-RGI method unifies two important applications with state-of-the-art (SOTA) performance: (i) mask-free semantic inpainting, where the corruptions are unknown missing regions, the restored background can be used to restore the missing content; (ii) unsupervised pixel-wise anomaly detection, where the corruptions are unknown anomalous regions, the retrieved mask can be used as the anomalous region's segmentation mask.



### Unsupervised Discovery of Semantic Latent Directions in Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.12469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12469v1)
- **Published**: 2023-02-24 05:54:34+00:00
- **Updated**: 2023-02-24 05:54:34+00:00
- **Authors**: Yong-Hyun Park, Mingi Kwon, Junghyo Jo, Youngjung Uh
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. While image editing with GANs builds upon latent space, DMs rely on editing the conditions such as text prompts. We present an unsupervised method to discover interpretable editing directions for the latent variables $\mathbf{x}_t \in \mathcal{X}$ of DMs. Our method adopts Riemannian geometry between $\mathcal{X}$ and the intermediate feature maps $\mathcal{H}$ of the U-Nets to provide a deep understanding over the geometrical structure of $\mathcal{X}$. The discovered semantic latent directions mostly yield disentangled attribute changes, and they are globally consistent across different samples. Furthermore, editing in earlier timesteps edits coarse attributes, while ones in later timesteps focus on high-frequency details. We define the curvedness of a line segment between samples to show that $\mathcal{X}$ is a curved manifold. Experiments on different baselines and datasets demonstrate the effectiveness of our method even on Stable Diffusion. Our source code will be publicly available for the future researchers.



### Frequency and Scale Perspectives of Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2302.12477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.12477v1)
- **Published**: 2023-02-24 06:37:36+00:00
- **Updated**: 2023-02-24 06:37:36+00:00
- **Authors**: Liangqi Zhang, Yihao Luo, Xiang Cao, Haibo Shen, Tianjiang Wang
- **Comment**: 5 pages, 5 figures; ICASSP 2023
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved superior performance but still lack clarity about the nature and properties of feature extraction. In this paper, by analyzing the sensitivity of neural networks to frequencies and scales, we find that neural networks not only have low- and medium-frequency biases but also prefer different frequency bands for different classes, and the scale of objects influences the preferred frequency bands. These observations lead to the hypothesis that neural networks must learn the ability to extract features at various scales and frequencies. To corroborate this hypothesis, we propose a network architecture based on Gaussian derivatives, which extracts features by constructing scale space and employing partial derivatives as local feature extraction operators to separate high-frequency information. This manually designed method of extracting features from different scales allows our GSSDNets to achieve comparable accuracy with vanilla networks on various datasets.



### Disease Severity Regression with Continuous Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.12482v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12482v1)
- **Published**: 2023-02-24 06:48:29+00:00
- **Updated**: 2023-02-24 06:48:29+00:00
- **Authors**: Shumpei Takezaki, Kiyohito Tanaka, Seiichi Uchida, Takeaki Kadota
- **Comment**: Accepted at ISBI2023
- **Journal**: None
- **Summary**: Disease severity regression by a convolutional neural network (CNN) for medical images requires a sufficient number of image samples labeled with severity levels. Conditional generative adversarial network (cGAN)-based data augmentation (DA) is a possible solution, but it encounters two issues. The first issue is that existing cGANs cannot deal with real-valued severity levels as their conditions, and the second is that the severity of the generated images is not fully reliable. We propose continuous DA as a solution to the two issues. Our method uses continuous severity GAN to generate images at real-valued severity levels and dataset-disjoint multi-objective optimization to deal with the second issue. Our method was evaluated for estimating ulcerative colitis (UC) severity of endoscopic images and achieved higher classification performance than conventional DA methods.



### Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images
- **Arxiv ID**: http://arxiv.org/abs/2302.12491v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12491v2)
- **Published**: 2023-02-24 07:17:15+00:00
- **Updated**: 2023-02-27 01:24:52+00:00
- **Authors**: Yuki Kondo, Norimichi Ukita
- **Comment**: We have transferred this paper from "Automation in Construction" to
  "Advanced Engineering Informatics". The code used in this paper will be made
  public
- **Journal**: None
- **Summary**: This paper proposes crack segmentation augmented by super resolution (SR) with deep neural networks. In the proposed method, a SR network is jointly trained with a binary segmentation network in an end-to-end manner. This joint learning allows the SR network to be optimized for improving segmentation results. For realistic scenarios, the SR network is extended from non-blind to blind for processing a low-resolution image degraded by unknown blurs. The joint network is improved by our proposed two extra paths that further encourage the mutual optimization between SR and segmentation. Comparative experiments with SoTA segmentation methods demonstrate the superiority of our joint learning, and various ablation studies prove the effects of our contributions.



### Data fusion of satellite imagery for generation of daily cloud free images at high resolution level
- **Arxiv ID**: http://arxiv.org/abs/2302.12495v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, 49Q20 (Primary) 94A08, 49K20, 49J45 (Secondary), I.4.4; G.1.8; I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2302.12495v1)
- **Published**: 2023-02-24 07:29:13+00:00
- **Updated**: 2023-02-24 07:29:13+00:00
- **Authors**: Natalya Ivanchuk, Peter Kogut, Petro Martyniuk
- **Comment**: 29 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper we discuss a new variational approach to the Date Fusion problem of multi-spectral satellite images from Sentinel-2 and MODIS that have been captured at different resolution level and, arguably, on different days. The crucial point of our approach that the MODIS image is cloud-free whereas the images from Sentinel-2 can be corrupted by clouds or noise.



### Spatial Bias for Attention-free Non-local Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.12505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12505v1)
- **Published**: 2023-02-24 08:16:16+00:00
- **Updated**: 2023-02-24 08:16:16+00:00
- **Authors**: Junhyung Go, Jongbin Ryu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce the spatial bias to learn global knowledge without self-attention in convolutional neural networks. Owing to the limited receptive field, conventional convolutional neural networks suffer from learning long-range dependencies. Non-local neural networks have struggled to learn global knowledge, but unavoidably have too heavy a network design due to the self-attention operation. Therefore, we propose a fast and lightweight spatial bias that efficiently encodes global knowledge without self-attention on convolutional neural networks. Spatial bias is stacked on the feature map and convolved together to adjust the spatial structure of the convolutional features. Therefore, we learn the global knowledge on the convolution layer directly with very few additional resources. Our method is very fast and lightweight due to the attention-free non-local method while improving the performance of neural networks considerably. Compared to non-local neural networks, the spatial bias use about 10 times fewer parameters while achieving comparable performance with 1.6 ~ 3.3 times more throughput on a very little budget. Furthermore, the spatial bias can be used with conventional non-local neural networks to further improve the performance of the backbone model. We show that the spatial bias achieves competitive performance that improves the classification accuracy by +0.79% and +1.5% on ImageNet-1K and cifar100 datasets. Additionally, we validate our method on the MS-COCO and ADE20K datasets for downstream tasks involving object detection and semantic segmentation.



### Implicit neural representations for unsupervised super-resolution and denoising of 4D flow MRI
- **Arxiv ID**: http://arxiv.org/abs/2302.12835v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2302.12835v1)
- **Published**: 2023-02-24 08:42:04+00:00
- **Updated**: 2023-02-24 08:42:04+00:00
- **Authors**: Simone Saitta, Marcello Carioni, Subhadip Mukherjee, Carola-Bibiane Schönlieb, Alberto Redaelli
- **Comment**: None
- **Journal**: None
- **Summary**: 4D flow MRI is a non-invasive imaging method that can measure blood flow velocities over time. However, the velocity fields detected by this technique have limitations due to low resolution and measurement noise. Coordinate-based neural networks have been researched to improve accuracy, with SIRENs being suitable for super-resolution tasks. Our study investigates SIRENs for time-varying 3-directional velocity fields measured in the aorta by 4D flow MRI, achieving denoising and super-resolution. We trained our method on voxel coordinates and benchmarked our approach using synthetic measurements and a real 4D flow MRI scan. Our optimized SIREN architecture outperformed state-of-the-art techniques, producing denoised and super-resolved velocity fields from clinical data. Our approach is quick to execute and straightforward to implement for novel cases, achieving 4D super-resolution.



### Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention
- **Arxiv ID**: http://arxiv.org/abs/2302.12532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.12532v1)
- **Published**: 2023-02-24 09:36:31+00:00
- **Updated**: 2023-02-24 09:36:31+00:00
- **Authors**: Bin Liu, Xiaolin Wei, Bo Li, Junjie Cao, Yu-Kun Lai
- **Comment**: 15 pages, 12 figures
- **Journal**: None
- **Summary**: Most of the existing audio-driven 3D facial animation methods suffered from the lack of detailed facial expression and head pose, resulting in unsatisfactory experience of human-robot interaction. In this paper, a novel pose-controllable 3D facial animation synthesis method is proposed by utilizing hierarchical audio-vertex attention. To synthesize real and detailed expression, a hierarchical decomposition strategy is proposed to encode the audio signal into both a global latent feature and a local vertex-wise control feature. Then the local and global audio features combined with vertex spatial features are used to predict the final consistent facial animation via a graph convolutional neural network by fusing the intrinsic spatial topology structure of the face model and the corresponding semantic feature of the audio. To accomplish pose-controllable animation, we introduce a novel pose attribute augmentation method by utilizing the 2D talking face technique. Experimental results indicate that the proposed method can produce more realistic facial expressions and head posture movements. Qualitative and quantitative experiments show that the proposed method achieves competitive performance against state-of-the-art methods.



### Visual motion analysis of the player's finger
- **Arxiv ID**: http://arxiv.org/abs/2303.12697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2303.12697v1)
- **Published**: 2023-02-24 10:14:13+00:00
- **Updated**: 2023-02-24 10:14:13+00:00
- **Authors**: Marco Costanzo
- **Comment**: 34 pages, 49 figures
- **Journal**: None
- **Summary**: This work is about the extraction of the motion of fingers, in their three articulations, of a keyboard player from a video sequence. The relevance of the problem involves several aspects, in fact, the extraction of the movements of the fingers may be used to compute the keystroke efficiency and individual joint contributions, as showed by Werner Goebl and Caroline Palmer in the paper 'Temporal Control and Hand Movement Efficiency in Skilled Music Performance'. Those measures are directly related to the precision in timing and force measures. A very good approach to the hand gesture recognition problem has been presented in the paper ' Real-Time Hand Gesture Recognition Using Finger Segmentation'. Detecting the keys pressed on a keyboard is a task that can be complex because of the shadows that can degrade the quality of the result and possibly cause the detection of not pressed keys. Among the several approaches that already exist, a great amount of them is based on the subtraction of frames in order to detect the movements of the keys caused by their pressure. Detecting the keys that are pressed could be useful to automatically evaluate the performance of a pianist or to automatically write sheet music of the melody that is being played.



### Deep Learning for Video-Text Retrieval: a Review
- **Arxiv ID**: http://arxiv.org/abs/2302.12552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12552v1)
- **Published**: 2023-02-24 10:14:35+00:00
- **Updated**: 2023-02-24 10:14:35+00:00
- **Authors**: Cunjuan Zhu, Qi Jia, Wei Chen, Yanming Guo, Yu Liu
- **Comment**: International Journal of Multimedia Information Retrieval (IJMIR)
- **Journal**: None
- **Summary**: Video-Text Retrieval (VTR) aims to search for the most relevant video related to the semantics in a given sentence, and vice versa. In general, this retrieval task is composed of four successive steps: video and textual feature representation extraction, feature embedding and matching, and objective functions. In the last, a list of samples retrieved from the dataset is ranked based on their matching similarities to the query. In recent years, significant and flourishing progress has been achieved by deep learning techniques, however, VTR is still a challenging task due to the problems like how to learn an efficient spatial-temporal video feature and how to narrow the cross-modal gap. In this survey, we review and summarize over 100 research papers related to VTR, demonstrate state-of-the-art performance on several commonly benchmarked datasets, and discuss potential challenges and directions, with the expectation to provide some insights for researchers in the field of video-text retrieval.



### A Knowledge Distillation framework for Multi-Organ Segmentation of Medaka Fish in Tomographic Image
- **Arxiv ID**: http://arxiv.org/abs/2302.12562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12562v1)
- **Published**: 2023-02-24 10:31:29+00:00
- **Updated**: 2023-02-24 10:31:29+00:00
- **Authors**: Jwalin Bhatt, Yaroslav Zharov, Sungho Suh, Tilo Baumbach, Vincent Heuveline, Paul Lukowicz
- **Comment**: Accepted at IEEE International Symposium on Biomedical Imaging 2023
  (ISBI 2023)
- **Journal**: None
- **Summary**: Morphological atlases are an important tool in organismal studies, and modern high-throughput Computed Tomography (CT) facilities can produce hundreds of full-body high-resolution volumetric images of organisms. However, creating an atlas from these volumes requires accurate organ segmentation. In the last decade, machine learning approaches have achieved incredible results in image segmentation tasks, but they require large amounts of annotated data for training. In this paper, we propose a self-training framework for multi-organ segmentation in tomographic images of Medaka fish. We utilize the pseudo-labeled data from a pretrained Teacher model and adopt a Quality Classifier to refine the pseudo-labeled data. Then, we introduce a pixel-wise knowledge distillation method to prevent overfitting to the pseudo-labeled data and improve the segmentation performance. The experimental results demonstrate that our method improves mean Intersection over Union (IoU) by 5.9% on the full dataset and enables keeping the quality while using three times less markup.



### 3D PETCT Tumor Lesion Segmentation via GCN Refinement
- **Arxiv ID**: http://arxiv.org/abs/2302.12571v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2302.12571v1)
- **Published**: 2023-02-24 10:52:08+00:00
- **Updated**: 2023-02-24 10:52:08+00:00
- **Authors**: Hengzhi Xue, Qingqing Fang, Yudong Yao, Yueyang Teng
- **Comment**: 10 pages,5 figures,38 reference
- **Journal**: None
- **Summary**: Whole-body PET/CT scan is an important tool for diagnosing various malignancies (e.g., malignant melanoma, lymphoma, or lung cancer), and accurate segmentation of tumors is a key part for subsequent treatment. In recent years, CNN-based segmentation methods have been extensively investigated. However, these methods often give inaccurate segmentation results, such as over-segmentation and under-segmentation. Therefore, to address such issues, we propose a post-processing method based on a graph convolutional neural network (GCN) to refine inaccurate segmentation parts and improve the overall segmentation accuracy. Firstly, nnUNet is used as an initial segmentation framework, and the uncertainty in the segmentation results is analyzed. Certainty and uncertainty nodes establish the nodes of a graph neural network. Each node and its 6 neighbors form an edge, and 32 nodes are randomly selected for uncertain nodes to form edges. The highly uncertain nodes are taken as the subsequent refinement targets. Secondly, the nnUNet result of the certainty nodes is used as label to form a semi-supervised graph network problem, and the uncertainty part is optimized through training the GCN network to improve the segmentation performance. This describes our proposed nnUNet-GCN segmentation framework. We perform tumor segmentation experiments on the PET/CT dataset in the MICCIA2022 autoPET challenge. Among them, 30 cases are randomly selected for testing, and the experimental results show that the false positive rate is effectively reduced with nnUNet-GCN refinement. In quantitative analysis, there is an improvement of 2.12 % on the average Dice score, 6.34 on 95 % Hausdorff Distance (HD95), and 1.72 on average symmetric surface distance (ASSD). The quantitative and qualitative evaluation results show that GCN post-processing methods can effectively improve tumor segmentation performance.



### Revisiting Modality Imbalance In Multimodal Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.12589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.12589v2)
- **Published**: 2023-02-24 11:56:57+00:00
- **Updated**: 2023-07-07 08:22:17+00:00
- **Authors**: Arindam Das, Sudip Das, Ganesh Sistu, Jonathan Horgan, Ujjwal Bhattacharya, Edward Jones, Martin Glavin, Ciarán Eising
- **Comment**: 5 pages, 3 figure, 4 tables
- **Journal**: In proceedings of the IEEE 2023 International Conference on Image
  Processing
- **Summary**: Multimodal learning, particularly for pedestrian detection, has recently received emphasis due to its capability to function equally well in several critical autonomous driving scenarios such as low-light, night-time, and adverse weather conditions. However, in most cases, the training distribution largely emphasizes the contribution of one specific input that makes the network biased towards one modality. Hence, the generalization of such models becomes a significant problem where the non-dominant input modality during training could be contributing more to the course of inference. Here, we introduce a novel training setup with regularizer in the multimodal architecture to resolve the problem of this disparity between the modalities. Specifically, our regularizer term helps to make the feature fusion method more robust by considering both the feature extractors equivalently important during the training to extract the multimodal distribution which is referred to as removing the imbalance problem. Furthermore, our decoupling concept of output stream helps the detection task by sharing the spatial sensitive information mutually. Extensive experiments of the proposed method on KAIST and UTokyo datasets shows improvement of the respective state-of-the-art performance.



### Classification of structural building damage grades from multi-temporal photogrammetric point clouds using a machine learning model trained on virtual laser scanning data
- **Arxiv ID**: http://arxiv.org/abs/2302.12591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12591v1)
- **Published**: 2023-02-24 12:04:46+00:00
- **Updated**: 2023-02-24 12:04:46+00:00
- **Authors**: Vivien Zahs, Katharina Anders, Julia Kohns, Alexander Stark, Bernhard Höfle
- **Comment**: 29 pages, 12 figures
- **Journal**: None
- **Summary**: Automatic damage assessment based on UAV-derived 3D point clouds can provide fast information on the damage situation after an earthquake. However, the assessment of multiple damage grades is challenging due to the variety in damage patterns and limited transferability of existing methods to other geographic regions or data sources. We present a novel approach to automatically assess multi-class building damage from real-world multi-temporal point clouds using a machine learning model trained on virtual laser scanning (VLS) data. We (1) identify object-specific change features, (2) separate changed and unchanged building parts, (3) train a random forest machine learning model with VLS data based on object-specific change features, and (4) use the classifier to assess building damage in real-world point clouds from photogrammetry-based dense image matching (DIM). We evaluate classifiers trained on different input data with respect to their capacity to classify three damage grades (heavy, extreme, destruction) in pre- and post-event DIM point clouds of a real earthquake event. Our approach is transferable with respect to multi-source input point clouds used for training (VLS) and application (DIM) of the model. We further achieve geographic transferability of the model by training it on simulated data of geometric change which characterises relevant damage grades across different geographic regions. The model yields high multi-target classification accuracies (overall accuracy: 92.0% - 95.1%). Its performance improves only slightly when using real-world region-specific training data (< 3% higher overall accuracies) and when using real-world region-specific training data (< 2% higher overall accuracies). We consider our approach relevant for applications where timely information on the damage situation is required and sufficient real-world training data is not available.



### Effect of Lossy Compression Algorithms on Face Image Quality and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.12593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12593v1)
- **Published**: 2023-02-24 12:11:05+00:00
- **Updated**: 2023-02-24 12:11:05+00:00
- **Authors**: Torsten Schlett, Sebastian Schachner, Christian Rathgeb, Juan Tapia, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Lossy face image compression can degrade the image quality and the utility for the purpose of face recognition. This work investigates the effect of lossy image compression on a state-of-the-art face recognition model, and on multiple face image quality assessment models. The analysis is conducted over a range of specific image target sizes. Four compression types are considered, namely JPEG, JPEG 2000, downscaled PNG, and notably the new JPEG XL format. Frontal color images from the ColorFERET database were used in a Region Of Interest (ROI) variant and a portrait variant. We primarily conclude that JPEG XL allows for superior mean and worst case face recognition performance especially at lower target sizes, below approximately 5kB for the ROI variant, while there appears to be no critical advantage among the compression types at higher target sizes. Quality assessments from modern models correlate well overall with the compression effect on face recognition performance.



### COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.12656v2
- **DOI**: 10.1109/ETFA52439.2022.9921525
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.12656v2)
- **Published**: 2023-02-24 14:24:58+00:00
- **Updated**: 2023-04-04 09:06:52+00:00
- **Authors**: Charith Munasinghe, Fatemeh Mohammadi Amin, Davide Scaramuzza, Hans Wernher van de Venn
- **Comment**: None
- **Journal**: IEEE Conference on Emerging Technologies and Factory Automation
  (ETFA 2022)
- **Summary**: Safe human-robot collaboration (HRC) has recently gained a lot of interest with the emerging Industry 5.0 paradigm. Conventional robots are being replaced with more intelligent and flexible collaborative robots (cobots). Safe and efficient collaboration between cobots and humans largely relies on the cobot's comprehensive semantic understanding of the dynamic surrounding of industrial environments. Despite the importance of semantic understanding for such applications, 3D semantic segmentation of collaborative robot workspaces lacks sufficient research and dedicated datasets. The performance limitation caused by insufficient datasets is called 'data hunger' problem. To overcome this current limitation, this work develops a new dataset specifically designed for this use case, named "COVERED", which includes point-wise annotated point clouds of a robotic cell. Lastly, we also provide a benchmark of current state-of-the-art (SOTA) algorithm performance on the dataset and demonstrate a real-time semantic segmentation of a collaborative robot workspace using a multi-LiDAR system. The promising results from using the trained Deep Networks on a real-time dynamically changing situation shows that we are on the right track. Our perception pipeline achieves 20Hz throughput with a prediction point accuracy of $>$96\% and $>$92\% mean intersection over union (mIOU) while maintaining an 8Hz throughput.



### FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.12662v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12662v1)
- **Published**: 2023-02-24 14:27:41+00:00
- **Updated**: 2023-02-24 14:27:41+00:00
- **Authors**: Tianpeng Deng, Yanqi Huang, Zhenwei Shi, Jiatai Lin, Qi Dou, Ke Zhao, Fang-Fang Liu, Yu-Mian Jia, Jin Wang, Bingchao Zhao, Changhong Liang, Zaiyi Liu, Xiao-jing Guo, Guoqiang Han, Xin Chen, Chu Han
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological tissue classification is a fundamental task in computational pathology. Deep learning-based models have achieved superior performance but centralized training with data centralization suffers from the privacy leakage problem. Federated learning (FL) can safeguard privacy by keeping training samples locally, but existing FL-based frameworks require a large number of well-annotated training samples and numerous rounds of communication which hinder their practicability in the real-world clinical scenario. In this paper, we propose a universal and lightweight federated learning framework, named Federated Deep-Broad Learning (FedDBL), to achieve superior classification performance with limited training samples and only one-round communication. By simply associating a pre-trained deep learning feature extractor, a fast and lightweight broad learning inference system and a classical federated aggregation approach, FedDBL can dramatically reduce data dependency and improve communication efficiency. Five-fold cross-validation demonstrates that FedDBL greatly outperforms the competitors with only one-round communication and limited training samples, while it even achieves comparable performance with the ones under multiple-round communications. Furthermore, due to the lightweight design and one-round communication, FedDBL reduces the communication burden from 4.6GB to only 276.5KB per client using the ResNet-50 backbone at 50-round training. Since no data or deep model sharing across different clients, the privacy issue is well-solved and the model security is guaranteed with no model inversion attack risk. Code is available at https://github.com/tianpeng-deng/FedDBL.



### Video4MRI: An Empirical Study on Brain Magnetic Resonance Image Analytics with CNN-based Video Classification Frameworks
- **Arxiv ID**: http://arxiv.org/abs/2302.12688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12688v1)
- **Published**: 2023-02-24 15:26:31+00:00
- **Updated**: 2023-02-24 15:26:31+00:00
- **Authors**: Yuxuan Zhang, Qingzhong Wang, Jiang Bian, Yi Liu, Yanwu Xu, Dejing Dou, Haoyi Xiong
- **Comment**: Accepted by IEEE ISBI'23
- **Journal**: None
- **Summary**: To address the problem of medical image recognition, computer vision techniques like convolutional neural networks (CNN) are frequently used. Recently, 3D CNN-based models dominate the field of magnetic resonance image (MRI) analytics. Due to the high similarity between MRI data and videos, we conduct extensive empirical studies on video recognition techniques for MRI classification to answer the questions: (1) can we directly use video recognition models for MRI classification, (2) which model is more appropriate for MRI, (3) are the common tricks like data augmentation in video recognition still useful for MRI classification? Our work suggests that advanced video techniques benefit MRI classification. In this paper, four datasets of Alzheimer's and Parkinson's disease recognition are utilized in experiments, together with three alternative video recognition models and data augmentation techniques that are frequently applied to video tasks. In terms of efficiency, the results reveal that the video framework performs better than 3D-CNN models by 5% - 11% with 50% - 66% less trainable parameters. This report pushes forward the potential fusion of 3D medical imaging and video understanding research.



### Amortised Invariance Learning for Contrastive Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2302.12712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12712v2)
- **Published**: 2023-02-24 16:15:11+00:00
- **Updated**: 2023-04-03 18:08:18+00:00
- **Authors**: Ruchika Chavhan, Henry Gouk, Jan Stuehmer, Calum Heggan, Mehrdad Yaghoobi, Timothy Hospedales
- **Comment**: ICLR 2023, Code available here:
  https://github.com/ruchikachavhan/amortized-invariance-learning-ssl/
- **Journal**: None
- **Summary**: Contrastive self-supervised learning methods famously produce high quality transferable representations by learning invariances to different data augmentations. Invariances established during pre-training can be interpreted as strong inductive biases. However these may or may not be helpful, depending on if they match the invariance requirements of downstream tasks or not. This has led to several attempts to learn task-specific invariances during pre-training, however, these methods are highly compute intensive and tedious to train. We introduce the notion of amortised invariance learning for contrastive self supervision. In the pre-training stage, we parameterize the feature extractor by differentiable invariance hyper-parameters that control the invariances encoded by the representation. Then, for any downstream task, both linear readout and task-specific invariance requirements can be efficiently and effectively learned by gradient-descent. We evaluate the notion of amortised invariances for contrastive learning over two different modalities: vision and audio, on two widely-used contrastive learning methods in vision: SimCLR and MoCo-v2 with popular architectures like ResNets and Vision Transformers, and SimCLR with ResNet-18 for audio. We show that our amortised features provide a reliable way to learn diverse downstream tasks with different invariance requirements, while using a single feature and avoiding task-specific pre-training. This provides an exciting perspective that opens up new horizons in the field of general purpose representation learning.



### Modulating Pretrained Diffusion Models for Multimodal Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2302.12764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12764v2)
- **Published**: 2023-02-24 17:28:08+00:00
- **Updated**: 2023-05-18 22:30:48+00:00
- **Authors**: Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar Singh, Zhifei Zhang, Tobias Hinz
- **Comment**: SIGGRAPH Conference Proceedings 2023. Project page at
  https://mcm-diffusion.github.io
- **Journal**: None
- **Summary**: We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but \textit{does not require any updates to the diffusion network's parameters}. MCM is a small module trained to modulate the diffusion network's predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only $\sim$1$\%$ of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.



### Language-Driven Representation Learning for Robotics
- **Arxiv ID**: http://arxiv.org/abs/2302.12766v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12766v1)
- **Published**: 2023-02-24 17:29:31+00:00
- **Updated**: 2023-02-24 17:29:31+00:00
- **Authors**: Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang
- **Comment**: 30 Pages, 15 Figures
- **Journal**: None
- **Summary**: Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.



### FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets
- **Arxiv ID**: http://arxiv.org/abs/2302.12772v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12772v1)
- **Published**: 2023-02-24 17:39:53+00:00
- **Updated**: 2023-02-24 17:39:53+00:00
- **Authors**: Yelena Randall, Tali Treibitz
- **Comment**: None
- **Journal**: None
- **Summary**: Visibility underwater is challenging, and degrades as the distance between the subject and camera increases, making vision tasks in the forward-looking direction more difficult. We have collected underwater forward-looking stereo-vision and visual-inertial image sets in the Mediterranean and Red Sea. To our knowledge there are no other public datasets in the underwater environment acquired with this camera-sensor orientation published with ground-truth. These datasets are critical for the development of several underwater applications, including obstacle avoidance, visual odometry, 3D tracking, Simultaneous Localization and Mapping (SLAM) and depth estimation. The stereo datasets include synchronized stereo images in dynamic underwater environments with objects of known-size. The visual-inertial datasets contain monocular images and IMU measurements, aligned with millisecond resolution timestamps and objects of known size which were placed in the scene. Both sensor configurations allow for scale estimation, with the calibrated baseline in the stereo setup and the IMU in the visual-inertial setup. Ground truth depth maps were created offline for both dataset types using photogrammetry. The ground truth is validated with multiple known measurements placed throughout the imaged environment. There are 5 stereo and 8 visual-inertial datasets in total, each containing thousands of images, with a range of different underwater visibility and ambient light conditions, natural and man-made structures and dynamic camera motions. The forward-looking orientation of the camera makes these datasets unique and ideal for testing underwater obstacle-avoidance algorithms and for navigation close to the seafloor in dynamic environments. With our datasets, we hope to encourage the advancement of autonomous functionality for underwater vehicles in dynamic and/or shallow water environments.



### 3D Generative Model Latent Disentanglement via Local Eigenprojection
- **Arxiv ID**: http://arxiv.org/abs/2302.12798v2
- **DOI**: 10.1111/cgf.14793
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12798v2)
- **Published**: 2023-02-24 18:19:49+00:00
- **Updated**: 2023-04-04 14:17:15+00:00
- **Authors**: Simone Foti, Bongjin Koo, Danail Stoyanov, Matthew J. Clarkson
- **Comment**: Computer Graphics Forum 2023
- **Journal**: None
- **Summary**: Designing realistic digital humans is extremely complex. Most data-driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural-network-based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state-of-the-art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models.



### Decoupling Human and Camera Motion from Videos in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2302.12827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12827v2)
- **Published**: 2023-02-24 18:59:15+00:00
- **Updated**: 2023-03-20 22:11:45+00:00
- **Authors**: Vickie Ye, Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa
- **Comment**: Project site: https://vye16.github.io/slahmr. CVPR 2023
- **Journal**: None
- **Summary**: We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and video results can be found at https://vye16.github.io/slahmr.



### SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2302.12828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12828v1)
- **Published**: 2023-02-24 18:59:18+00:00
- **Updated**: 2023-02-24 18:59:18+00:00
- **Authors**: Ahmed Imtiaz Humayun, Randall Balestriero, Guha Balakrishnan, Richard Baraniuk
- **Comment**: 11 pages, 20 figures
- **Journal**: None
- **Summary**: Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN's mapping - including its decision boundary - over a specified region of the data space. By leveraging the theory of Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a DNs geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL nonlinearities, including (leaky-)ReLU, absolute value, maxout, and max-pooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability and sample from the decision boundary on or off the manifold. Project Website: bit.ly/splinecam.



### 3D Surface Reconstruction in the Wild by Deforming Shape Priors from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2302.12883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12883v1)
- **Published**: 2023-02-24 20:37:27+00:00
- **Updated**: 2023-02-24 20:37:27+00:00
- **Authors**: Nicolai Häni, Jun-Jee Chao, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing the underlying 3D surface of an object from a single image is a challenging problem that has received extensive attention from the computer vision community. Many learning-based approaches tackle this problem by learning a 3D shape prior from either ground truth 3D data or multi-view observations. To achieve state-of-the-art results, these methods assume that the objects are specified with respect to a fixed canonical coordinate frame, where instances of the same category are perfectly aligned. In this work, we present a new method for joint category-specific 3D reconstruction and object pose estimation from a single image. We show that one can leverage shape priors learned on purely synthetic 3D data together with a point cloud pose canonicalization method to achieve high-quality 3D reconstruction in the wild. Given a single depth image at test time, we first transform this partial point cloud into a learned canonical frame. Then, we use a neural deformation field to reconstruct the 3D surface of the object. Finally, we jointly optimize object pose and 3D shape to fit the partial depth observation. Our approach achieves state-of-the-art reconstruction performance across several real-world datasets, even when trained only on synthetic data. We further show that our method generalizes to different input modalities, from dense depth images to sparse and noisy LIDAR scans.



### Automatic Classification of Symmetry of Hemithoraces in Canine and Feline Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2302.12923v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.12923v1)
- **Published**: 2023-02-24 22:46:16+00:00
- **Updated**: 2023-02-24 22:46:16+00:00
- **Authors**: Peyman Tahghighi, Nicole Norena, Eran Ukwatta, Ryan B Appleby, Amin Komeili
- **Comment**: Submitted to SPIE Journal of Medical Imaging
- **Journal**: None
- **Summary**: Purpose: Thoracic radiographs are commonly used to evaluate patients with confirmed or suspected thoracic pathology. Proper patient positioning is more challenging in canine and feline radiography than in humans due to less patient cooperation and body shape variation. Improper patient positioning during radiograph acquisition has the potential to lead to a misdiagnosis. Asymmetrical hemithoraces are one of the indications of obliquity for which we propose an automatic classification method.   Approach: We propose a hemithoraces segmentation method based on Convolutional Neural Networks (CNNs) and active contours. We utilized the U-Net model to segment the ribs and spine and then utilized active contours to find left and right hemithoraces. We then extracted features from the left and right hemithoraces to train an ensemble classifier which includes Support Vector Machine, Gradient Boosting and Multi-Layer Perceptron. Five-fold cross-validation was used, thorax segmentation was evaluated by Intersection over Union (IoU), and symmetry classification was evaluated using Precision, Recall, Area under Curve and F1 score.   Results: Classification of symmetry for 900 radiographs reported an F1 score of 82.8% . To test the robustness of the proposed thorax segmentation method to underexposure and overexposure, we synthetically corrupted properly exposed radiographs and evaluated results using IoU. The results showed that the models IoU for underexposure and overexposure dropped by 2.1% and 1.2%, respectively.   Conclusions: Our results indicate that the proposed thorax segmentation method is robust to poor exposure radiographs. The proposed thorax segmentation method can be applied to human radiography with minimal changes.



### Visual Privacy: Current and Emerging Regulations Around Unconsented Video Analytics in Retail
- **Arxiv ID**: http://arxiv.org/abs/2302.12935v1
- **DOI**: 10.31219/osf.io/tfw96
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12935v1)
- **Published**: 2023-02-24 23:34:35+00:00
- **Updated**: 2023-02-24 23:34:35+00:00
- **Authors**: Scott Pletcher
- **Comment**: None
- **Journal**: None
- **Summary**: Video analytics is the practice of combining digital video data with machine learning models to infer various characteristics from that video. This capability has been used for years to detect objects, movement, and the number of customers in physical retail stores, but more complex machine learning models combined with more powerful computing power has unlocked new levels of possibility. Researchers claim it is now possible to infer a whole host of characteristics about an individual using video analytics, such as specific age, ethnicity, health status and emotional state. Moreover, an individuals visual identity can be augmented with information from other data providers to build out a detailed profile, all with the individual unknowingly contributing their physical presence in front of a retail store camera. Some retailers have begun to experiment with this new technology as a way to better know their customers. However, those same early adopters are caught in an evolving legal landscape around privacy and data ownership. This research looks into the current legal landscape and legislation currently in progress around the use of video analytics, specifically in the retail store setting. Because the ethical and legal norms around individualized video analytics are still heavily in flux, retailers are urged to adopt a wait and see approach or potentially incur costly legal expenses and risk damage to their brand.



