# Arxiv Papers in cs.CV on 2023-02-28
### ReorientDiff: Diffusion Model based Reorientation for Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2303.12700v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.12700v1)
- **Published**: 2023-02-28 00:08:38+00:00
- **Updated**: 2023-02-28 00:08:38+00:00
- **Authors**: Utkarsh A. Mishra, Yongxin Chen
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: The ability to manipulate objects in a desired configurations is a fundamental requirement for robots to complete various practical applications. While certain goals can be achieved by picking and placing the objects of interest directly, object reorientation is needed for precise placement in most of the tasks. In such scenarios, the object must be reoriented and re-positioned into intermediate poses that facilitate accurate placement at the target pose. To this end, we propose a reorientation planning method, ReorientDiff, that utilizes a diffusion model-based approach. The proposed method employs both visual inputs from the scene, and goal-specific language prompts to plan intermediate reorientation poses. Specifically, the scene and language-task information are mapped into a joint scene-task representation feature space, which is subsequently leveraged to condition the diffusion model. The diffusion model samples intermediate poses based on the representation using classifier-free guidance and then uses gradients of learned feasibility-score models for implicit iterative pose-refinement. The proposed method is evaluated using a set of YCB-objects and a suction gripper, demonstrating a success rate of 96.5\% in simulation. Overall, our study presents a promising approach to address the reorientation challenge in manipulation by learning a conditional distribution, which is an effective way to move towards more generalizable object manipulation. For more results, checkout our website: https://utkarshmishra04.github.io/ReorientDiff.



### Global Proxy-based Hard Mining for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.14217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14217v1)
- **Published**: 2023-02-28 00:43:13+00:00
- **Updated**: 2023-02-28 00:43:13+00:00
- **Authors**: Amar Ali-bey, Brahim Chaib-draa, Philippe Gigu√®re
- **Comment**: Accepted at BMVC 2022
- **Journal**: None
- **Summary**: Learning deep representations for visual place recognition is commonly performed using pairwise or triple loss functions that highly depend on the hardness of the examples sampled at each training iteration. Existing techniques address this by using computationally and memory expensive offline hard mining, which consists of identifying, at each iteration, the hardest samples from the training set. In this paper we introduce a new technique that performs global hard mini-batch sampling based on proxies. To do so, we add a new end-to-end trainable branch to the network, which generates efficient place descriptors (one proxy for each place). These proxy representations are thus used to construct a global index that encompasses the similarities between all places in the dataset, allowing for highly informative mini-batch sampling at each training iteration. Our method can be used in combination with all existing pairwise and triplet loss functions with negligible additional memory and computation cost. We run extensive ablation studies and show that our technique brings new state-of-the-art performance on multiple large-scale benchmarks such as Pittsburgh, Mapillary-SLS and SPED. In particular, our method provides more than 100% relative improvement on the challenging Nordland dataset. Our code is available at https://github.com/amaralibey/GPM



### Towards Surgical Context Inference and Translation to Gestures
- **Arxiv ID**: http://arxiv.org/abs/2302.14237v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, 65D19, 68T07, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/2302.14237v2)
- **Published**: 2023-02-28 01:39:36+00:00
- **Updated**: 2023-03-15 19:07:36+00:00
- **Authors**: Kay Hutchinson, Zongyu Li, Ian Reyes, Homa Alemzadeh
- **Comment**: accepted for the 2023 International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e.g., contact between graspers and objects in Suturing). We also find that the FSM models are more robust to poor segmentation and labeling performance than LSTMs. Our proposed method can significantly shorten the gesture labeling process (~2.8 times).



### Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images
- **Arxiv ID**: http://arxiv.org/abs/2302.14239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14239v1)
- **Published**: 2023-02-28 01:44:55+00:00
- **Updated**: 2023-02-28 01:44:55+00:00
- **Authors**: Zhongli Fan, Li Zhang, Yuxuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We present an effective method for the matching of multimodal images. Accurate image matching is the basis of various applications, such as image registration and structure from motion. Conventional matching methods fail when handling noisy multimodal image pairs with severe scale change, rotation, and nonlinear intensity distortion (NID). Toward this need, we introduce an image pyramid strategy to tackle scale change. We put forward an accurate primary orientation estimation approach to reduce the effect of image rotation at any angle. We utilize multi-scale and multi-orientation image filtering results and a feature-to-template matching scheme to ensure effective and accurate matching under large NID. Integrating these improvements significantly increases noise, scale, rotation, and NID invariant capability. Our experimental results confirm the excellent ability to achieve high-quality matches across various multimodal images. The proposed method outperforms the mainstream multimodal image matching methods in qualitative and quantitative evaluations. Our implementation is available at https://github.com/Zhongli-Fan/NISR.



### Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.14250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14250v2)
- **Published**: 2023-02-28 02:21:42+00:00
- **Updated**: 2023-04-20 08:12:44+00:00
- **Authors**: Chaohui Yu, Qiang Zhou, Jingliang Li, Jianlong Yuan, Zhibin Wang, Fan Wang
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Modern incremental learning for semantic segmentation methods usually learn new categories based on dense annotations. Although achieve promising results, pixel-by-pixel labeling is costly and time-consuming. Weakly incremental learning for semantic segmentation (WILSS) is a novel and attractive task, which aims at learning to segment new classes from cheap and widely available image-level labels. Despite the comparable results, the image-level labels can not provide details to locate each segment, which limits the performance of WILSS. This inspires us to think how to improve and effectively utilize the supervision of new classes given image-level labels while avoiding forgetting old ones. In this work, we propose a novel and data-efficient framework for WILSS, named FMWISS. Specifically, we propose pre-training based co-segmentation to distill the knowledge of complementary foundation models for generating dense pseudo labels. We further optimize the noisy pseudo masks with a teacher-student architecture, where a plug-in teacher is optimized with a proposed dense contrastive loss. Moreover, we introduce memory-based copy-paste augmentation to improve the catastrophic forgetting problem of old classes. Extensive experiments on Pascal VOC and COCO datasets demonstrate the superior performance of our framework, e.g., FMWISS achieves 70.7% and 73.3% in the 15-5 VOC setting, outperforming the state-of-the-art method by 3.4% and 6.1%, respectively.



### Remote Sensing Scene Classification with Masked Image Modeling (MIM)
- **Arxiv ID**: http://arxiv.org/abs/2302.14256v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14256v2)
- **Published**: 2023-02-28 02:27:36+00:00
- **Updated**: 2023-03-24 17:43:20+00:00
- **Authors**: Liya Wang, Alex Tien
- **Comment**: arXiv admin note: text overlap with arXiv:2301.12058
- **Journal**: None
- **Summary**: Remote sensing scene classification has been extensively studied for its critical roles in geological survey, oil exploration, traffic management, earthquake prediction, wildfire monitoring, and intelligence monitoring. In the past, the Machine Learning (ML) methods for performing the task mainly used the backbones pretrained in the manner of supervised learning (SL). As Masked Image Modeling (MIM), a self-supervised learning (SSL) technique, has been shown as a better way for learning visual feature representation, it presents a new opportunity for improving ML performance on the scene classification task. This research aims to explore the potential of MIM pretrained backbones on four well-known classification datasets: Merced, AID, NWPU-RESISC45, and Optimal-31. Compared to the published benchmarks, we show that the MIM pretrained Vision Transformer (ViTs) backbones outperform other alternatives (up to 18% on top 1 accuracy) and that the MIM technique can learn better feature representation than the supervised learning counterparts (up to 5% on top 1 accuracy). Moreover, we show that the general-purpose MIM-pretrained ViTs can achieve competitive performance as the specially designed yet complicated Transformer for Remote Sensing (TRS) framework. Our experiment results also provide a performance baseline for future studies.



### RGB-D Grasp Detection via Depth Guided Learning with Cross-modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2302.14264v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14264v1)
- **Published**: 2023-02-28 02:41:27+00:00
- **Updated**: 2023-02-28 02:41:27+00:00
- **Authors**: Ran Qin, Haoxiang Ma, Boyang Gao, Di Huang
- **Comment**: Accepted at ICRA 2023
- **Journal**: None
- **Summary**: Planar grasp detection is one of the most fundamental tasks to robotic manipulation, and the recent progress of consumer-grade RGB-D sensors enables delivering more comprehensive features from both the texture and shape modalities. However, depth maps are generally of a relatively lower quality with much stronger noise compared to RGB images, making it challenging to acquire grasp depth and fuse multi-modal clues. To address the two issues, this paper proposes a novel learning based approach to RGB-D grasp detection, namely Depth Guided Cross-modal Attention Network (DGCAN). To better leverage the geometry information recorded in the depth channel, a complete 6-dimensional rectangle representation is adopted with the grasp depth dedicatedly considered in addition to those defined in the common 5-dimensional one. The prediction of the extra grasp depth substantially strengthens feature learning, thereby leading to more accurate results. Moreover, to reduce the negative impact caused by the discrepancy of data quality in two modalities, a Local Cross-modal Attention (LCA) module is designed, where the depth features are refined according to cross-modal relations and concatenated to the RGB ones for more sufficient fusion. Extensive simulation and physical evaluations are conducted and the experimental results highlight the superiority of the proposed approach.



### Adversarial Attack with Raindrops
- **Arxiv ID**: http://arxiv.org/abs/2302.14267v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2302.14267v2)
- **Published**: 2023-02-28 03:01:58+00:00
- **Updated**: 2023-07-16 06:05:14+00:00
- **Authors**: Jiyuan Liu, Bingyi Lu, Mingkang Xiong, Tao Zhang, Huilin Xiong
- **Comment**: 10 pages, 7 figures, This manuscript was submitted to CVPR 2023
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to adversarial examples, which are usually designed artificially to fool DNNs, but rarely exist in real-world scenarios. In this paper, we study the adversarial examples caused by raindrops, to demonstrate that there exist plenty of natural phenomena being able to work as adversarial attackers to DNNs. Moreover, we present a new approach to generate adversarial raindrops, denoted as AdvRD, using the generative adversarial network (GAN) technique to simulate natural raindrops. The images crafted by our AdvRD look very similar to the real-world raindrop images, statistically close to the distribution of true raindrop images, and more importantly, can perform strong adversarial attack to the state-of-the-art DNN models. On the other side, we show that the adversarial training using our AdvRD images can significantly improve the robustness of DNNs to the real-world raindrop attacks. Extensive experiments are carried out to demonstrate that the images crafted by AdvRD are visually and statistically close to the natural raindrop images, can work as strong attackers to DNN models, and also help improve the robustness of DNNs to raindrop attacks.



### Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance
- **Arxiv ID**: http://arxiv.org/abs/2302.14268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14268v1)
- **Published**: 2023-02-28 03:02:11+00:00
- **Updated**: 2023-02-28 03:02:11+00:00
- **Authors**: Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, Li Yi
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Category-level articulated object pose estimation aims to estimate a hierarchy of articulation-aware object poses of an unseen articulated object from a known category. To reduce the heavy annotations needed for supervised learning methods, we present a novel self-supervised strategy that solves this problem without any human labels. Our key idea is to factorize canonical shapes and articulated object poses from input articulated shapes through part-level equivariant shape analysis. Specifically, we first introduce the concept of part-level SE(3) equivariance and devise a network to learn features of such property. Then, through a carefully designed fine-grained pose-shape disentanglement strategy, we expect that canonical spaces to support pose estimation could be induced automatically. Thus, we could further predict articulated object poses as per-part rigid transformations describing how parts transform from their canonical part spaces to the camera space. Extensive experiments demonstrate the effectiveness of our method on both complete and partial point clouds from synthetic and real articulated object datasets.



### DECOR-NET: A COVID-19 Lung Infection Segmentation Network Improved by Emphasizing Low-level Features and Decorrelating Features
- **Arxiv ID**: http://arxiv.org/abs/2302.14277v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14277v1)
- **Published**: 2023-02-28 03:23:36+00:00
- **Updated**: 2023-02-28 03:23:36+00:00
- **Authors**: Jiesi Hu, Yanwu Yang, Xutao Guo, Ting Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Since 2019, coronavirus Disease 2019 (COVID-19) has been widely spread and posed a serious threat to public health. Chest Computed Tomography (CT) holds great potential for screening and diagnosis of this disease. The segmentation of COVID-19 CT imaging can achieves quantitative evaluation of infections and tracks disease progression. COVID-19 infections are characterized by high heterogeneity and unclear boundaries, so capturing low-level features such as texture and intensity is critical for segmentation. However, segmentation networks that emphasize low-level features are still lacking. In this work, we propose a DECOR-Net capable of capturing more decorrelated low-level features. The channel re-weighting strategy is applied to obtain plenty of low-level features and the dependencies between channels are reduced by proposed decorrelation loss. Experiments show that DECOR-Net outperforms other cutting-edge methods and surpasses the baseline by 5.1% and 4.9% in terms of Dice coefficient and intersection over union. Moreover, the proposed decorrelation loss can improve the performance constantly under different settings. The Code is available at https://github.com/jiesihu/DECOR-Net.git.



### Rethink Long-tailed Recognition with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.14284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14284v2)
- **Published**: 2023-02-28 03:36:48+00:00
- **Updated**: 2023-04-17 08:35:02+00:00
- **Authors**: Zhengzhuo Xu, Shuo Yang, Xingjun Wang, Chun Yuan
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: In the real world, data tends to follow long-tailed distributions w.r.t. class or attribution, motivating the challenging Long-Tailed Recognition (LTR) problem. In this paper, we revisit recent LTR methods with promising Vision Transformers (ViT). We figure out that 1) ViT is hard to train with long-tailed data. 2) ViT learns generalized features in an unsupervised manner, like mask generative training, either on long-tailed or balanced datasets. Hence, we propose to adopt unsupervised learning to utilize long-tailed data. Furthermore, we propose the Predictive Distribution Calibration (PDC) as a novel metric for LTR, where the model tends to simply classify inputs into common classes. Our PDC can measure the model calibration of predictive preferences quantitatively. On this basis, we find many LTR approaches alleviate it slightly, despite the accuracy improvement. Extensive experiments on benchmark datasets validate that PDC reflects the model's predictive preference precisely, which is consistent with the visualization.



### Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2302.14290v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14290v1)
- **Published**: 2023-02-28 03:50:56+00:00
- **Updated**: 2023-02-28 03:50:56+00:00
- **Authors**: Gaurav Patel, Konda Reddy Mopuri, Qiang Qiu
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Data-free Knowledge Distillation (DFKD) has gained popularity recently, with the fundamental idea of carrying out knowledge transfer from a Teacher neural network to a Student neural network in the absence of training data. However, in the Adversarial DFKD framework, the student network's accuracy, suffers due to the non-stationary distribution of the pseudo-samples under multiple generator updates. To this end, at every generator update, we aim to maintain the student's performance on previously encountered examples while acquiring knowledge from samples of the current distribution. Thus, we propose a meta-learning inspired framework by treating the task of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously met samples) as meta-train and meta-test, respectively. Hence, we dub our method as Learning to Retain while Acquiring. Moreover, we identify an implicit aligning factor between the Knowledge-Retention and Knowledge-Acquisition tasks indicating that the proposed student update strategy enforces a common gradient direction for both tasks, alleviating interference between the two objectives. Finally, we support our hypothesis by exhibiting extensive evaluation and comparison of our method with prior arts on multiple datasets.



### A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking
- **Arxiv ID**: http://arxiv.org/abs/2302.14301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14301v1)
- **Published**: 2023-02-28 04:26:20+00:00
- **Updated**: 2023-02-28 04:26:20+00:00
- **Authors**: Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang, Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue, Shibao Zheng
- **Comment**: International Journal of Computer Vision (IJCV) [under review]
- **Journal**: None
- **Summary**: The robustness of deep neural networks is usually lacking under adversarial examples, common corruptions, and distribution shifts, which becomes an important research problem in the development of deep learning. Although new deep learning methods and robustness improvement techniques have been constantly proposed, the robustness evaluations of existing methods are often inadequate due to their rapid development, diverse noise patterns, and simple evaluation metrics. Without thorough robustness evaluations, it is hard to understand the advances in the field and identify the effective methods. In this paper, we establish a comprehensive robustness benchmark called \textbf{ARES-Bench} on the image classification task. In our benchmark, we evaluate the robustness of 55 typical deep learning models on ImageNet with diverse architectures (e.g., CNNs, Transformers) and learning algorithms (e.g., normal supervised training, pre-training, adversarial training) under numerous adversarial attacks and out-of-distribution (OOD) datasets. Using robustness curves as the major evaluation criteria, we conduct large-scale experiments and draw several important findings, including: 1) there is an inherent trade-off between adversarial and natural robustness for the same model architecture; 2) adversarial training effectively improves adversarial robustness, especially when performed on Transformer architectures; 3) pre-training significantly improves natural robustness based on more training data or self-supervised learning. Based on ARES-Bench, we further analyze the training tricks in large-scale adversarial training on ImageNet. By designing the training settings accordingly, we achieve the new state-of-the-art adversarial robustness. We have made the benchmarking results and code platform publicly available.



### Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/2302.14302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14302v2)
- **Published**: 2023-02-28 04:31:09+00:00
- **Updated**: 2023-07-11 06:04:58+00:00
- **Authors**: Chang Liu, Wenzhao Xiang, Yuan He, Hui Xue, Shibao Zheng, Hang Su
- **Comment**: Computer Vision and Image Understanding (CVIU) [under review]
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) may suffer from significantly degenerated performance when the training and test data are of different underlying distributions. Despite the importance of model generalization to out-of-distribution (OOD) data, the accuracy of state-of-the-art (SOTA) models on OOD data can plummet. Recent work has demonstrated that regular or off-manifold adversarial examples, as a special case of data augmentation, can be used to improve OOD generalization. Inspired by this, we theoretically prove that on-manifold adversarial examples can better benefit OOD generalization. Nevertheless, it is nontrivial to generate on-manifold adversarial examples because the real manifold is generally complex. To address this issue, we proposed a novel method of Augmenting data with Adversarial examples via a Wavelet module (AdvWavAug), an on-manifold adversarial data augmentation technique that is simple to implement. In particular, we project a benign image into a wavelet domain. With the assistance of the sparsity characteristic of wavelet transformation, we can modify an image on the estimated data manifold. We conduct adversarial augmentation based on AdvProp training framework. Extensive experiments on different models and different datasets, including ImageNet and its distorted versions, demonstrate that our method can improve model generalization, especially on OOD data. By integrating AdvWavAug into the training process, we have achieved SOTA results on some recent transformer-based models.



### CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping
- **Arxiv ID**: http://arxiv.org/abs/2302.14306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14306v1)
- **Published**: 2023-02-28 04:38:52+00:00
- **Updated**: 2023-02-28 04:38:52+00:00
- **Authors**: Srikanth Malla, Yi-Ting Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud data plays an essential role in robotics and self-driving applications. Yet, annotating point cloud data is time-consuming and nontrivial while they enable learning discriminative 3D representations that empower downstream tasks, such as classification and segmentation. Recently, contrastive learning-based frameworks have shown promising results for learning 3D representations in a self-supervised manner. However, existing contrastive learning methods cannot precisely encode and associate structural features and search the higher dimensional augmentation space efficiently. In this paper, we present CLR-GAM, a novel contrastive learning-based framework with Guided Augmentation (GA) for efficient dynamic exploration strategy and Guided Feature Mapping (GFM) for similar structural feature association between augmented point clouds. We empirically demonstrate that the proposed approach achieves state-of-the-art performance on both simulated and real-world 3D point cloud datasets for three different downstream tasks, i.e., 3D point cloud classification, few-shot learning, and object part segmentation.



### GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/2302.14307v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14307v3)
- **Published**: 2023-02-28 04:45:31+00:00
- **Updated**: 2023-03-15 11:15:41+00:00
- **Authors**: Kangyang Luo, Xiang Li, Yunshi Lan, Ming Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA achieves significant performance gains in accuracy and communication efficiency compared to SOTA baselines.



### Temporal Coherent Test-Time Optimization for Robust Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.14309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14309v1)
- **Published**: 2023-02-28 04:59:23+00:00
- **Updated**: 2023-02-28 04:59:23+00:00
- **Authors**: Chenyu Yi, Siyuan Yang, Yufei Wang, Haoliang Li, Yap-Peng Tan, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are likely to fail when the test data is corrupted in real-world deployment (e.g., blur, weather, etc.). Test-time optimization is an effective way that adapts models to generalize to corrupted data during testing, which has been shown in the image domain. However, the techniques for improving video classification corruption robustness remain few. In this work, we propose a Temporal Coherent Test-time Optimization framework (TeCo) to utilize spatio-temporal information in test-time optimization for robust video classification. To exploit information in video with self-supervised learning, TeCo uses global content from video clips and optimizes models for entropy minimization. TeCo minimizes the entropy of the prediction based on the global content from video clips. Meanwhile, it also feeds local content to regularize the temporal coherence at the feature level. TeCo retains the generalization ability of various video classification models and achieves significant improvements in corruption robustness across Mini Kinetics-C and Mini SSV2-C. Furthermore, TeCo sets a new baseline in video classification corruption robustness via test-time optimization.



### Read Pointer Meters in complex environments based on a Human-like Alignment and Recognition Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2302.14323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14323v2)
- **Published**: 2023-02-28 05:37:04+00:00
- **Updated**: 2023-07-30 15:36:36+00:00
- **Authors**: Yan Shu, Shaohui Liu, Honglei Xu, Feng Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, developing an automatic reading system for analog measuring instruments has gained increased attention, as it enables the collection of numerous state of equipment. Nonetheless, two major obstacles still obstruct its deployment to real-world applications. The first issue is that they rarely take the entire pipeline's speed into account. The second is that they are incapable of dealing with some low-quality images (i.e., meter breakage, blur, and uneven scale). In this paper, we propose a human-like alignment and recognition algorithm to overcome these problems. More specifically, a Spatial Transformed Module(STM) is proposed to obtain the front view of images in a self-autonomous way based on an improved Spatial Transformer Networks(STN). Meanwhile, a Value Acquisition Module(VAM) is proposed to infer accurate meter values by an end-to-end trained framework. In contrast to previous research, our model aligns and recognizes meters totally implemented by learnable processing, which mimics human's behaviours and thus achieves higher performances. Extensive results verify the good robustness of the proposed model in terms of the accuracy and efficiency.



### BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images
- **Arxiv ID**: http://arxiv.org/abs/2302.14325v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.14325v3)
- **Published**: 2023-02-28 05:37:45+00:00
- **Updated**: 2023-08-15 03:44:00+00:00
- **Authors**: Lun Luo, Shuhang Zheng, Yixuan Li, Yongzhi Fan, Beinan Yu, Siyuan Cao, Huiliang Shen
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: Place recognition is a key module for long-term SLAM systems. Current LiDAR-based place recognition methods usually use representations of point clouds such as unordered points or range images. These methods achieve high recall rates of retrieval, but their performance may degrade in the case of view variation or scene changes. In this work, we explore the potential of a different representation in place recognition, i.e. bird's eye view (BEV) images. We observe that the structural contents of BEV images are less influenced by rotations and translations of point clouds. We validate that, without any delicate design, a simple VGGNet trained on BEV images achieves comparable performance with the state-of-the-art place recognition methods in scenes of slight viewpoint changes. For more robust place recognition, we design a rotation-invariant network called BEVPlace. We use group convolution to extract rotation-equivariant local features from the images and NetVLAD for global feature aggregation. In addition, we observe that the distance between BEV features is correlated with the geometry distance of point clouds. Based on the observation, we develop a method to estimate the position of the query cloud, extending the usage of place recognition. The experiments conducted on large-scale public datasets show that our method 1) achieves state-of-the-art performance in terms of recall rates, 2) is robust to view changes, 3) shows strong generalization ability, and 4) can estimate the positions of query point clouds. Source codes are publicly available at https://github.com/zjuluolun/BEVPlace.



### Valid Information Guidance Network for Compressed Video Quality Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2303.00520v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.00520v1)
- **Published**: 2023-02-28 05:43:25+00:00
- **Updated**: 2023-02-28 05:43:25+00:00
- **Authors**: Xuan Sun, Ziyue Zhang, Guannan Chen, Dan Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years deep learning methods have shown great superiority in compressed video quality enhancement tasks. Existing methods generally take the raw video as the ground truth and extract practical information from consecutive frames containing various artifacts. However, they do not fully exploit the valid information of compressed and raw videos to guide the quality enhancement for compressed videos. In this paper, we propose a unique Valid Information Guidance scheme (VIG) to enhance the quality of compressed videos by mining valid information from both compressed videos and raw videos. Specifically, we propose an efficient framework, Compressed Redundancy Filtering (CRF) network, to balance speed and enhancement. After removing the redundancy by filtering the information, CRF can use the valid information of the compressed video to reconstruct the texture. Furthermore, we propose a progressive Truth Guidance Distillation (TGD) strategy, which does not need to design additional teacher models and distillation loss functions. By only using the ground truth as input to guide the model to aggregate the correct spatio-temporal correspondence across the raw frames, TGD can significantly improve the enhancement effect without increasing the extra training cost. Extensive experiments show that our method achieves the state-of-the-art performance of compressed video quality enhancement in terms of accuracy and efficiency.



### Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2302.14332v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14332v2)
- **Published**: 2023-02-28 05:55:42+00:00
- **Updated**: 2023-03-21 03:57:07+00:00
- **Authors**: Jingpei Lu, Florian Richter, Michael C. Yip
- **Comment**: 14 pages, 9 figures. Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Solving the camera-to-robot pose is a fundamental requirement for vision-based robot control, and is a process that takes considerable effort and cares to make accurate. Traditional approaches require modification of the robot via markers, and subsequent deep learning approaches enabled markerless feature extraction. Mainstream deep learning methods only use synthetic data and rely on Domain Randomization to fill the sim-to-real gap, because acquiring the 3D annotation is labor-intensive. In this work, we go beyond the limitation of 3D annotations for real-world data. We propose an end-to-end pose estimation framework that is capable of online camera-to-robot calibration and a self-supervised training method to scale the training to unlabeled real-world data. Our framework combines deep learning and geometric vision for solving the robot pose, and the pipeline is fully differentiable. To train the Camera-to-Robot Pose Estimation Network (CtRNet), we leverage foreground segmentation and differentiable rendering for image-level self-supervision. The pose prediction is visualized through a renderer and the image loss with the input image is back-propagated to train the neural network. Our experimental results on two public real datasets confirm the effectiveness of our approach over existing works. We also integrate our framework into a visual servoing system to demonstrate the promise of real-time precise robot pose estimation for automation tasks.



### DC-Former: Diverse and Compact Transformer for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2302.14335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14335v1)
- **Published**: 2023-02-28 06:03:42+00:00
- **Updated**: 2023-02-28 06:03:42+00:00
- **Authors**: Wen Li, Cheng Zou, Meng Wang, Furong Xu, Jianan Zhao, Ruobing Zheng, Yuan Cheng, Wei Chu
- **Comment**: Accepted by AAAI23
- **Journal**: None
- **Summary**: In person re-identification (re-ID) task, it is still challenging to learn discriminative representation by deep learning, due to limited data. Generally speaking, the model will get better performance when increasing the amount of data. The addition of similar classes strengthens the ability of the classifier to identify similar identities, thereby improving the discrimination of representation. In this paper, we propose a Diverse and Compact Transformer (DC-Former) that can achieve a similar effect by splitting embedding space into multiple diverse and compact subspaces. Compact embedding subspace helps model learn more robust and discriminative embedding to identify similar classes. And the fusion of these diverse embeddings containing more fine-grained information can further improve the effect of re-ID. Specifically, multiple class tokens are used in vision transformer to represent multiple embedding spaces. Then, a self-diverse constraint (SDC) is applied to these spaces to push them away from each other, which makes each embedding space diverse and compact. Further, a dynamic weight controller(DWC) is further designed for balancing the relative importance among them during training. The experimental results of our method are promising, which surpass previous state-of-the-art methods on several commonly used person re-ID benchmarks.



### UniFLG: Unified Facial Landmark Generator from Text or Speech
- **Arxiv ID**: http://arxiv.org/abs/2302.14337v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14337v2)
- **Published**: 2023-02-28 06:05:43+00:00
- **Updated**: 2023-05-19 02:43:32+00:00
- **Authors**: Kentaro Mitsui, Yukiya Hono, Kei Sawada
- **Comment**: 5 pages, 2 figures, 3 tables, accepted for INTERSPEECH 2023. Audio
  samples: https://rinnakk.github.io/research/publications/UniFLG
- **Journal**: None
- **Summary**: Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.



### Turning a CLIP Model into a Scene Text Detector
- **Arxiv ID**: http://arxiv.org/abs/2302.14338v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14338v3)
- **Published**: 2023-02-28 06:06:12+00:00
- **Updated**: 2023-03-26 12:58:19+00:00
- **Authors**: Wenwen Yu, Yuliang Liu, Wei Hua, Deqiang Jiang, Bo Ren, Xiang Bai
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: The recent large-scale Contrastive Language-Image Pretraining (CLIP) model has shown great potential in various downstream tasks via leveraging the pretrained vision and language knowledge. Scene text, which contains rich textual and visual information, has an inherent connection with a model like CLIP. Recently, pretraining approaches based on vision language models have made effective progresses in the field of text detection. In contrast to these works, this paper proposes a new method, termed TCM, focusing on Turning the CLIP Model directly for text detection without pretraining process. We demonstrate the advantages of the proposed TCM as follows: (1) The underlying principle of our framework can be applied to improve existing scene text detector. (2) It facilitates the few-shot training capability of existing methods, e.g., by using 10% of labeled data, we significantly improve the performance of the baseline method with an average of 22% in terms of the F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene text detection methods, we further achieve promising domain adaptation ability. The code will be publicly released at https://github.com/wenwenyu/TCM.



### HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization
- **Arxiv ID**: http://arxiv.org/abs/2302.14340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14340v2)
- **Published**: 2023-02-28 06:20:07+00:00
- **Updated**: 2023-03-01 12:24:02+00:00
- **Authors**: Zhihao Liang, Zhangjin Huang, Changxing Ding, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Recovery of an underlying scene geometry from multiview images stands as a long-time challenge in computer vision research. The recent promise leverages neural implicit surface learning and differentiable volume rendering, and achieves both the recovery of scene geometry and synthesis of novel views, where deep priors of neural models are used as an inductive smoothness bias. While promising for object-level surfaces, these methods suffer when coping with complex scene surfaces. In the meanwhile, traditional multi-view stereo can recover the geometry of scenes with rich textures, by globally optimizing the local, pixel-wise correspondences across multiple views. We are thus motivated to make use of the complementary benefits from the two strategies, and propose a method termed Helix-shaped neural implicit Surface learning or HelixSurf; HelixSurf uses the intermediate prediction from one strategy as the guidance to regularize the learning of the other one, and conducts such intertwined regularization iteratively during the learning process. We also propose an efficient scheme for differentiable volume rendering in HelixSurf. Experiments on surface reconstruction of indoor scenes show that our method compares favorably with existing methods and is orders of magnitude faster, even when some of existing methods are assisted with auxiliary training data. The source code is available at https://github.com/Gorilla-Lab-SCUT/HelixSurf.



### Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes
- **Arxiv ID**: http://arxiv.org/abs/2302.14348v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.14348v3)
- **Published**: 2023-02-28 06:38:25+00:00
- **Updated**: 2023-03-27 17:08:27+00:00
- **Authors**: Jihyun Lee, Minhyuk Sung, Honggyu Choi, Tae-Kyun Kim
- **Comment**: 6 figures, 14 pages, accepted to CVPR 2023, project page:
  https://jyunlee.github.io/projects/implicit-two-hands/
- **Journal**: None
- **Summary**: We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands - conditioned on an RGB image and coarse 3D keypoints - by two novel attention-based modules responsible for (1) initial occupancy estimation and (2) context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results. Our code is publicly available at https://github.com/jyunlee/Im2Hands.



### Knowledge Augmented Relation Inference for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.14350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14350v2)
- **Published**: 2023-02-28 06:59:05+00:00
- **Updated**: 2023-03-01 08:12:08+00:00
- **Authors**: Xianglong Lang, Zhuming Wang, Zun Li, Meng Tian, Ge Shi, Lifang Wu, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing group activity recognition methods construct spatial-temporal relations merely based on visual representation. Some methods introduce extra knowledge, such as action labels, to build semantic relations and use them to refine the visual presentation. However, the knowledge they explored just stay at the semantic-level, which is insufficient for pursing notable accuracy. In this paper, we propose to exploit knowledge concretization for the group activity recognition, and develop a novel Knowledge Augmented Relation Inference framework that can effectively use the concretized knowledge to improve the individual representations. Specifically, the framework consists of a Visual Representation Module to extract individual appearance features, a Knowledge Augmented Semantic Relation Module explore semantic representations of individual actions, and a Knowledge-Semantic-Visual Interaction Module aims to integrate visual and semantic information by the knowledge. Benefiting from these modules, the proposed framework can utilize knowledge to enhance the relation inference process and the individual representations, thus improving the performance of group activity recognition. Experimental results on two public datasets show that the proposed framework achieves competitive performance compared with state-of-the-art methods.



### Deep Learning for Identifying Iran's Cultural Heritage Buildings in Need of Conservation Using Image Classification and Grad-CAM
- **Arxiv ID**: http://arxiv.org/abs/2302.14354v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2302.14354v1)
- **Published**: 2023-02-28 07:14:15+00:00
- **Updated**: 2023-02-28 07:14:15+00:00
- **Authors**: Mahdi Bahrami, Amir Albadvi
- **Comment**: 16 pages, 4745 words, 11 figures, and 5 tables
- **Journal**: None
- **Summary**: The cultural heritage buildings (CHB), which are part of mankind's history and identity, are in constant danger of damage or in extreme situations total destruction. That being said, it's of utmost importance to preserve them by identifying the existent, or presumptive, defects using novel methods so that renovation processes can be done in a timely manner and with higher accuracy. The main goal of this research is to use new deep learning (DL) methods in the process of preserving CHBs (situated in Iran); a goal that has been neglected especially in developing countries such as Iran, as these countries still preserve their CHBs using manual, and even archaic, methods that need direct human supervision. Having proven their effectiveness and performance when it comes to processing images, the convolutional neural networks (CNN) are a staple in computer vision (CV) literacy and this paper is not exempt. When lacking enough CHB images, training a CNN from scratch would be very difficult and prone to overfitting; that's why we opted to use a technique called transfer learning (TL) in which we used pre-trained ResNet, MobileNet, and Inception networks, for classification. Even more, the Grad-CAM was utilized to localize the defects to some extent. The final results were very favorable based on those of similar research. The final proposed model can pave the way for moving from manual to unmanned CHB conservation, hence an increase in accuracy and a decrease in human-induced errors.



### One-Shot Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2302.14362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14362v1)
- **Published**: 2023-02-28 07:30:36+00:00
- **Updated**: 2023-02-28 07:30:36+00:00
- **Authors**: Sangjin Lee, Suhwan Cho, Sangyoun Lee
- **Comment**: AAAI2023 submitted
- **Journal**: None
- **Summary**: Recently, removing objects from videos and filling in the erased regions using deep video inpainting (VI) algorithms has attracted considerable attention. Usually, a video sequence and object segmentation masks for all frames are required as the input for this task. However, in real-world applications, providing segmentation masks for all frames is quite difficult and inefficient. Therefore, we deal with VI in a one-shot manner, which only takes the initial frame's object mask as its input. Although we can achieve that using naive combinations of video object segmentation (VOS) and VI methods, they are sub-optimal and generally cause critical errors. To address that, we propose a unified pipeline for one-shot video inpainting (OSVI). By jointly learning mask prediction and video completion in an end-to-end manner, the results can be optimal for the entire task instead of each separate module. Additionally, unlike the two stage methods that use the predicted masks as ground truth cues, our method is more reliable because the predicted masks can be used as the network's internal guidance. On the synthesized datasets for OSVI, our proposed method outperforms all others both quantitatively and qualitatively.



### Efficient Implicit Neural Reconstruction Using LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2302.14363v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14363v1)
- **Published**: 2023-02-28 07:31:48+00:00
- **Updated**: 2023-02-28 07:31:48+00:00
- **Authors**: Dongyu Yan, Xiaoyang Lyu, Jieqi Shi, Yi Lin
- **Comment**: 6+2 pages, 8 figures, Accepted for publication at IEEE International
  Conference on Robotics and Automation (ICRA) 2023
- **Journal**: None
- **Summary**: Modeling scene geometry using implicit neural representation has revealed its advantages in accuracy, flexibility, and low memory usage. Previous approaches have demonstrated impressive results using color or depth images but still have difficulty handling poor light conditions and large-scale scenes. Methods taking global point cloud as input require accurate registration and ground truth coordinate labels, which limits their application scenarios. In this paper, we propose a new method that uses sparse LiDAR point clouds and rough odometry to reconstruct fine-grained implicit occupancy field efficiently within a few minutes. We introduce a new loss function that supervises directly in 3D space without 2D rendering, avoiding information loss. We also manage to refine poses of input frames in an end-to-end manner, creating consistent geometry without global point cloud registration. As far as we know, our method is the first to reconstruct implicit scene representation from LiDAR-only input. Experiments on synthetic and real-world datasets, including indoor and outdoor scenes, prove that our method is effective, efficient, and accurate, obtaining comparable results with existing methods using dense input.



### RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch
- **Arxiv ID**: http://arxiv.org/abs/2302.14365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.14365v1)
- **Published**: 2023-02-28 07:37:53+00:00
- **Updated**: 2023-02-28 07:37:53+00:00
- **Authors**: Yizhong Zhang, Zhiqi Li, Sicheng Xu, Chong Li, Jiaolong Yang, Xin Tong, Baining Guo
- **Comment**: IEEE VR 2023
- **Journal**: None
- **Summary**: Recent research advance has significantly improved the visual realism of immersive 3D video communication. In this work we present a method to further enhance this immersive experience by adding the hand touch capability ("remote hand clapping"). In our system, each meeting participant sits in front of a large screen with haptic feedback. The local participant can reach his hand out to the screen and perform hand clapping with the remote participant as if the two participants were only separated by a virtual glass. A key challenge in emulating the remote hand touch is the realistic rendering of the participant's hand and arm as the hand touches the screen. When the hand is very close to the screen, the RGBD data required for realistic rendering is no longer available. To tackle this challenge, we present a dual representation of the user's hand. Our dual representation not only preserves the high-quality rendering usually found in recent image-based rendering systems but also allows the hand to reach the screen. This is possible because the dual representation includes both an image-based model and a 3D geometry-based model, with the latter driven by a hand skeleton tracked by a side view camera. In addition, the dual representation provides a distance-based fusion of the image-based and 3D geometry-based models as the hand moves closer to the screen. The result is that the image-based and 3D geometry-based models mutually enhance each other, leading to realistic and seamless rendering. Our experiments demonstrate that our method provides consistent hand contact experience between remote users and improves the immersive experience of 3D video communication.



### Towards Enhanced Controllability of Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.14368v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.14368v2)
- **Published**: 2023-02-28 07:43:00+00:00
- **Updated**: 2023-03-15 21:42:39+00:00
- **Authors**: Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale
- **Comment**: 28 pages, 28 figures
- **Journal**: None
- **Summary**: Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further improve the translations. We observe better controllability compared to existing methods and show that without explicit training objectives, diffusion models can be used for effective image manipulation and image translation.



### Linear Spaces of Meanings: Compositional Structures in Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2302.14383v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14383v2)
- **Published**: 2023-02-28 08:11:56+00:00
- **Updated**: 2023-03-27 01:02:17+00:00
- **Authors**: Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, Stefano Soatto
- **Comment**: 18 pages, 9 figures, 7 tables
- **Journal**: None
- **Summary**: We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic operations on embedding vectors can be used as compositional and interpretable methods for regulating the behavior of VLMs.



### Your time series is worth a binary image: machine vision assisted deep framework for time series forecasting
- **Arxiv ID**: http://arxiv.org/abs/2302.14390v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14390v1)
- **Published**: 2023-02-28 08:17:50+00:00
- **Updated**: 2023-02-28 08:17:50+00:00
- **Authors**: Luoxiao Yang, Xinqi Fan, Zijun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Time series forecasting (TSF) has been a challenging research area, and various models have been developed to address this task. However, almost all these models are trained with numerical time series data, which is not as effectively processed by the neural system as visual information. To address this challenge, this paper proposes a novel machine vision assisted deep time series analysis (MV-DTSA) framework. The MV-DTSA framework operates by analyzing time series data in a novel binary machine vision time series metric space, which includes a mapping and an inverse mapping function from the numerical time series space to the binary machine vision space, and a deep machine vision model designed to address the TSF task in the binary space. A comprehensive computational analysis demonstrates that the proposed MV-DTSA framework outperforms state-of-the-art deep TSF models, without requiring sophisticated data decomposition or model customization. The code for our framework is accessible at https://github.com/IkeYang/ machine-vision-assisted-deep-time-series-analysis-MV-DTSA-.



### Neural Video Compression with Diverse Contexts
- **Arxiv ID**: http://arxiv.org/abs/2302.14402v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.14402v3)
- **Published**: 2023-02-28 08:35:50+00:00
- **Updated**: 2023-03-14 01:41:44+00:00
- **Authors**: Jiahao Li, Bin Li, Yan Lu
- **Comment**: Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC
- **Journal**: None
- **Summary**: For any video codecs, the coding efficiency highly relies on whether the current signal to be encoded can find the relevant contexts from the previous reconstructed signals. Traditional codec has verified more contexts bring substantial coding gain, but in a time-consuming manner. However, for the emerging neural video codec (NVC), its contexts are still limited, leading to low compression ratio. To boost NVC, this paper proposes increasing the context diversity in both temporal and spatial dimensions. First, we guide the model to learn hierarchical quality patterns across frames, which enriches long-term and yet high-quality temporal contexts. Furthermore, to tap the potential of optical flow-based coding framework, we introduce a group-based offset diversity where the cross-group interaction is proposed for better context mining. In addition, this paper also adopts a quadtree-based partition to increase spatial context diversity when encoding the latent representation in parallel. Experiments show that our codec obtains 23.5% bitrate saving over previous SOTA NVC. Better yet, our codec has surpassed the under-developing next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in terms of PSNR. The codes are at https://github.com/microsoft/DCVC.



### An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections
- **Arxiv ID**: http://arxiv.org/abs/2302.14409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14409v1)
- **Published**: 2023-02-28 08:44:00+00:00
- **Updated**: 2023-02-28 08:44:00+00:00
- **Authors**: Andrea Montibeller, Fernando P√©rez-Gonz√°lez
- **Comment**: This paper was submitted to IEEE Transactions on Information
  Forensics & Security the July 28, 2022
- **Journal**: None
- **Summary**: Radial correction distortion, applied by in-camera or out-camera software/firmware alters the supporting grid of the image so as to hamper PRNU-based camera attribution. Existing solutions to deal with this problem try to invert/estimate the correction using radial transformations parameterized with few variables in order to restrain the computational load; however, with ever more prevalent complex distortion corrections their performance is unsatisfactory. In this paper we propose an adaptive algorithm that by dividing the image into concentric annuli is able to deal with sophisticated corrections like those applied out-camera by third party software like Adobe Lightroom, Photoshop, Gimp and PT-Lens. We also introduce a statistic called cumulative peak of correlation energy (CPCE) that allows for an efficient early stopping strategy. Experiments on a large dataset of in-camera and out-camera radially corrected images show that our solution improves the state of the art in terms of both accuracy and computational cost.



### Mesh-SORT: Simple and effective location-wise tracker with lost management strategies
- **Arxiv ID**: http://arxiv.org/abs/2302.14415v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.14415v3)
- **Published**: 2023-02-28 08:47:53+00:00
- **Updated**: 2023-03-12 13:07:15+00:00
- **Authors**: ZongTan Li
- **Comment**: 14 pages 18 figs
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) has gained extensive attention in recent years due to its potential applications in traffic and pedestrian detection. We note that tracking by detection may suffer from errors generated by noise detectors, such as an imprecise bounding box before the occlusions, and observed that in most tracking scenarios, objects tend to move and lost within specific locations. To counter this, we present a novel tracker to deal with the bad detector and occlusions. Firstly, we proposed a location-wise sub-region recognition method which equally divided the frame, which we called mesh. Then we proposed corresponding location-wise loss management strategies and different matching strategies. The resulting Mesh-SORT, ablation studies demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally, we analyze its limitation on the specific scene and discussed what future works can be extended.



### DREAM: Efficient Dataset Distillation by Representative Matching
- **Arxiv ID**: http://arxiv.org/abs/2302.14416v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14416v3)
- **Published**: 2023-02-28 08:48:45+00:00
- **Updated**: 2023-08-30 14:22:32+00:00
- **Authors**: Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, Yang You
- **Comment**: Efficient matching for dataset distillation
- **Journal**: None
- **Summary**: Dataset distillation aims to synthesize small datasets with little information loss from original large-scale ones for reducing storage and training costs. Recent state-of-the-art methods mainly constrain the sample synthesis process by matching synthetic images and the original ones regarding gradients, embedding distributions, or training trajectories. Although there are various matching objectives, currently the strategy for selecting original images is limited to naive random sampling.   We argue that random sampling overlooks the evenness of the selected sample distribution, which may result in noisy or biased matching targets.   Besides, the sample diversity is also not constrained by random sampling. These factors together lead to optimization instability in the distilling process and degrade the training efficiency. Accordingly, we propose a novel matching strategy named as \textbf{D}ataset distillation by \textbf{RE}present\textbf{A}tive \textbf{M}atching (DREAM), where only representative original images are selected for matching. DREAM is able to be easily plugged into popular dataset distillation frameworks and reduce the distilling iterations by more than 8 times without performance drop. Given sufficient training time, DREAM further provides significant improvements and achieves state-of-the-art performances.



### PCR-CG: Point Cloud Registration via Deep Color and Geometry
- **Arxiv ID**: http://arxiv.org/abs/2302.14418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14418v1)
- **Published**: 2023-02-28 08:50:17+00:00
- **Updated**: 2023-02-28 08:50:17+00:00
- **Authors**: Yu Zhang, Junle Yu, Xiaolin Huang, Wenhui Zhou, Ji Hou
- **Comment**: accepted to ECCV2022; code at https://github.com/Gardlin/PCR-CG
- **Journal**: None
- **Summary**: In this paper, we introduce PCR-CG: a novel 3D point cloud registration module explicitly embedding the color signals into the geometry representation. Different from previous methods that only use geometry representation, our module is specifically designed to effectively correlate color into geometry for the point cloud registration task. Our key contribution is a 2D-3D cross-modality learning algorithm that embeds the deep features learned from color signals to the geometry representation. With our designed 2D-3D projection module, the pixel features in a square region centered at correspondences perceived from images are effectively correlated with point clouds. In this way, the overlapped regions can be inferred not only from point cloud but also from the texture appearances. Adding color is non-trivial. We compare against a variety of baselines designed for adding color to 3D, such as exhaustively adding per-pixel features or RGB values in an implicit manner. We leverage Predator [25] as the baseline method and incorporate our proposed module onto it. To validate the effectiveness of 2D features, we ablate different 2D pre-trained networks and show a positive correlation between the pre-trained weights and the task performance. Our experimental results indicate a significant improvement of 6.5% registration recall over the baseline method on the 3DLoMatch benchmark. We additionally evaluate our approach on SOTA methods and observe consistent improvements, such as an improvement of 2.4% registration recall over GeoTransformer as well as 3.5% over CoFiNet. Our study reveals a significant advantages of correlating explicit deep color features to the point cloud in the registration task.



### Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain
- **Arxiv ID**: http://arxiv.org/abs/2302.14430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14430v1)
- **Published**: 2023-02-28 09:18:48+00:00
- **Updated**: 2023-02-28 09:18:48+00:00
- **Authors**: Chuanlin Lan, Ziyuan Yin, Arindam Basu, Rosa H. M. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: 3D hand tracking methods based on monocular RGB videos are easily affected by motion blur, while event camera, a sensor with high temporal resolution and dynamic range, is naturally suitable for this task with sparse output and low power consumption. However, obtaining 3D annotations of fast-moving hands is difficult for constructing event-based hand-tracking datasets. In this paper, we provided an event-based speed adaptive hand tracker (ESAHT) to solve the hand tracking problem based on event camera. We enabled a CNN model trained on a hand tracking dataset with slow motion, which enabled the model to leverage the knowledge of RGB-based hand tracking solutions, to work on fast hand tracking tasks. To realize our solution, we constructed the first 3D hand tracking dataset captured by an event camera in a real-world environment, figured out two data augment methods to narrow the domain gap between slow and fast motion data, developed a speed adaptive event stream segmentation method to handle hand movements in different moving speeds, and introduced a new event-to-frame representation method adaptive to event streams with different lengths. Experiments showed that our solution outperformed RGB-based as well as previous event-based solutions in fast hand tracking tasks, and our codes and dataset will be publicly available.



### Efficient Masked Autoencoders with Self-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2302.14431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14431v1)
- **Published**: 2023-02-28 09:21:12+00:00
- **Updated**: 2023-02-28 09:21:12+00:00
- **Authors**: Zhaowen Li, Yousong Zhu, Zhiyang Chen, Wei Li, Chaoyang Zhao, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by masked language modeling (MLM) in natural language processing, masked image modeling (MIM) has been recognized as a strong and popular self-supervised pre-training method in computer vision. However, its high random mask ratio would result in two serious problems: 1) the data are not efficiently exploited, which brings inefficient pre-training (\eg, 1600 epochs for MAE $vs.$ 300 epochs for the supervised), and 2) the high uncertainty and inconsistency of the pre-trained model, \ie, the prediction of the same patch may be inconsistent under different mask rounds. To tackle these problems, we propose efficient masked autoencoders with self-consistency (EMAE), to improve the pre-training efficiency and increase the consistency of MIM. In particular, we progressively divide the image into K non-overlapping parts, each of which is generated by a random mask and has the same mask ratio. Then the MIM task is conducted parallelly on all parts in an iteration and generates predictions. Besides, we design a self-consistency module to further maintain the consistency of predictions of overlapping masked patches among parts. Overall, the proposed method is able to exploit the data more efficiently and obtains reliable representations. Experiments on ImageNet show that EMAE achieves even higher results with only 300 pre-training epochs under ViT-Base than MAE (1600 epochs). EMAE also consistently obtains state-of-the-art transfer performance on various downstream tasks, like object detection, and semantic segmentation.



### A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images
- **Arxiv ID**: http://arxiv.org/abs/2302.14434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14434v2)
- **Published**: 2023-02-28 09:24:36+00:00
- **Updated**: 2023-03-28 06:27:14+00:00
- **Authors**: Biwen Lei, Jianqiang Ren, Mengyang Feng, Miaomiao Cui, Xuansong Xie
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Limited by the nature of the low-dimensional representational capacity of 3DMM, most of the 3DMM-based face reconstruction (FR) methods fail to recover high-frequency facial details, such as wrinkles, dimples, etc. Some attempt to solve the problem by introducing detail maps or non-linear operations, however, the results are still not vivid. To this end, we in this paper present a novel hierarchical representation network (HRN) to achieve accurate and detailed face reconstruction from a single image. Specifically, we implement the geometry disentanglement and introduce the hierarchical representation to fulfill detailed face modeling. Meanwhile, 3D priors of facial details are incorporated to enhance the accuracy and authenticity of the reconstruction results. We also propose a de-retouching module to achieve better decoupling of the geometry and appearance. It is noteworthy that our framework can be extended to a multi-view fashion by considering detail consistency of different views. Extensive experiments on two single-view and two multi-view FR benchmarks demonstrate that our method outperforms the existing methods in both reconstruction accuracy and visual effects. Finally, we introduce a high-quality 3D face dataset FaceHD-100 to boost the research of high-fidelity face reconstruction. The project homepage is at https://younglbw.github.io/HRN-homepage/.



### ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.14435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14435v1)
- **Published**: 2023-02-28 09:25:37+00:00
- **Updated**: 2023-02-28 09:25:37+00:00
- **Authors**: Shanshan Li, Pan Gao, Xiaoyang Tan, Mingqiang Wei
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Problems such as equipment defects or limited viewpoints will lead the captured point clouds to be incomplete. Therefore, recovering the complete point clouds from the partial ones plays an vital role in many practical tasks, and one of the keys lies in the prediction of the missing part. In this paper, we propose a novel point cloud completion approach namely ProxyFormer that divides point clouds into existing (input) and missing (to be predicted) parts and each part communicates information through its proxies. Specifically, we fuse information into point proxy via feature and position extractor, and generate features for missing point proxies from the features of existing point proxies. Then, in order to better perceive the position of missing points, we design a missing part sensitive transformer, which converts random normal distribution into reasonable position information, and uses proxy alignment to refine the missing proxies. It makes the predicted point proxies more sensitive to the features and positions of the missing part, and thus make these proxies more suitable for subsequent coarse-to-fine processes. Experimental results show that our method outperforms state-of-the-art completion networks on several benchmark datasets and has the fastest inference speed. Code is available at https://github.com/I2-Multimedia-Lab/ProxyFormer.



### Learning to Estimate Two Dense Depths from LiDAR and Event Data
- **Arxiv ID**: http://arxiv.org/abs/2302.14444v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2302.14444v1)
- **Published**: 2023-02-28 09:42:39+00:00
- **Updated**: 2023-02-28 09:42:39+00:00
- **Authors**: Vincent Brebion, Julien Moreau, Franck Davoine
- **Comment**: Accepted for SCIA 2023. For the project page, see
  https://vbrebion.github.io/ALED/
- **Journal**: None
- **Summary**: Event cameras do not produce images, but rather a continuous flow of events, which encode changes of illumination for each pixel independently and asynchronously. While they output temporally rich information, they lack any depth information which could facilitate their use with other sensors. LiDARs can provide this depth information, but are by nature very sparse, which makes the depth-to-event association more complex. Furthermore, as events represent changes of illumination, they might also represent changes of depth; associating them with a single depth is therefore inadequate. In this work, we propose to address these issues by fusing information from an event camera and a LiDAR using a learning-based approach to estimate accurate dense depth maps. To solve the "potential change of depth" problem, we propose here to estimate two depth maps at each step: one "before" the events happen, and one "after" the events happen. We further propose to use this pair of depths to compute a depth difference for each event, to give them more context. We train and evaluate our network, ALED, on both synthetic and real driving sequences, and show that it is able to predict dense depths with an error reduction of up to 61% compared to the current state of the art. We also demonstrate the quality of our 2-depths-to-event association, and the usefulness of the depth difference information. Finally, we release SLED, a novel synthetic dataset comprising events, LiDAR point clouds, RGB images, and dense depth maps.



### Swin Deformable Attention Hybrid U-Net for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.14450v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14450v1)
- **Published**: 2023-02-28 09:54:53+00:00
- **Updated**: 2023-02-28 09:54:53+00:00
- **Authors**: Lichao Wang, Jiahao Huang, Guang Yang
- **Comment**: 10 pages, 5 figures, conference
- **Journal**: None
- **Summary**: How to harmonize convolution and multi-head self-attention mechanisms has recently emerged as a significant area of research in the field of medical image segmentation. Various combination methods have been proposed. However, there is a common flaw in these works: failed to provide a direct explanation for their hybrid model, which is crucial in clinical scenarios. Deformable Attention can improve the segmentation performance and provide an explanation based on the deformation field. Incorporating Deformable Attention into a hybrid model could result in a synergistic effect to boost segmentation performance while enhancing the explainability. In this study, we propose the incorporation of Swin Deformable Attention with hybrid architecture to improve the segmentation performance while establishing explainability. In the experiment section, our proposed Swin Deformable Attention Hybrid UNet (SDAH-UNet) demonstrates state-of-the-art performance on both anatomical and lesion segmentation tasks.



### An Effective Crop-Paste Pipeline for Few-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.14452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14452v2)
- **Published**: 2023-02-28 09:56:45+00:00
- **Updated**: 2023-05-12 05:51:57+00:00
- **Authors**: Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao
- **Comment**: None
- **Journal**: CVPR2023 Workshop on Learning with Limited Labelled Data
- **Summary**: Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. However, detecting novel categories with only a few samples usually leads to the problem of misclassification. In FSOD, we notice the false positive (FP) of novel categories is prominent, in which the base categories are often recognized as novel ones. To address this issue, a novel data augmentation pipeline that Crops the Novel instances and Pastes them on the selected Base images, called CNPB, is proposed. There are two key questions to be answered: (1) How to select useful base images? and (2) How to combine novel and base data? We design a multi-step selection strategy to find useful base data. Specifically, we first discover the base images which contain the FP of novel categories and select a certain amount of samples from them for the base and novel categories balance. Then the bad cases, such as the base images that have unlabeled ground truth or easily confused base instances, are removed by using CLIP. Finally, the same category strategy is adopted, in which a novel instance with category n is pasted on the base image with the FP of n. During combination, a novel instance is cropped and randomly down-sized, and thus pasted at the assigned optimal location from the randomly generated candidates in a selected base image. Our method is simple yet effective and can be easy to plug into existing FSOD methods, demonstrating significant potential for use. Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of our method.



### Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis
- **Arxiv ID**: http://arxiv.org/abs/2302.14460v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2302.14460v2)
- **Published**: 2023-02-28 10:08:11+00:00
- **Updated**: 2023-07-14 14:24:45+00:00
- **Authors**: Riƒçards Marcinkeviƒçs, Patricia Reis Wolfertstetter, Ugne Klimiene, Kieran Chin-Cheong, Alyssia Paschke, Julia Zerres, Markus Denzinger, David Niederberger, Sven Wellmann, Ece Ozkan, Christian Knorr, Julia E. Vogt
- **Comment**: None
- **Journal**: None
- **Summary**: Appendicitis is among the most frequent reasons for pediatric abdominal surgeries. With recent advances in machine learning, data-driven decision support could help clinicians diagnose and manage patients while reducing the number of non-critical surgeries. Previous decision support systems for appendicitis focused on clinical, laboratory, scoring and computed tomography data, mainly ignoring abdominal ultrasound, a noninvasive and readily available diagnostic modality. To this end, we developed and validated interpretable machine learning models for predicting the diagnosis, management and severity of suspected appendicitis using ultrasound images. Our models were trained on a dataset comprising 579 pediatric patients with 1709 ultrasound images accompanied by clinical and laboratory data. Our methodological contribution is the generalization of concept bottleneck models to prediction problems with multiple views and incomplete concept sets. Notably, such models lend themselves to interpretation and interaction via high-level concepts understandable to clinicians without sacrificing performance or requiring time-consuming image annotation when deployed.



### Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/2302.14470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2302.14470v1)
- **Published**: 2023-02-28 10:26:02+00:00
- **Updated**: 2023-02-28 10:26:02+00:00
- **Authors**: Erik Franz, Barbara Solenthaler, Nils Thuerey
- **Comment**: ICLR 2023 poster, source code:
  https://github.com/tum-pbs/Neural-Global-Transport
- **Journal**: None
- **Summary**: We address the challenging problem of jointly inferring the 3D flow and volumetric densities moving in a fluid from a monocular input video with a deep neural network. Despite the complexity of this task, we show that it is possible to train the corresponding networks without requiring any 3D ground truth for training. In the absence of ground truth data we can train our model with observations from real-world capture setups instead of relying on synthetic reconstructions. We make this unsupervised training approach possible by first generating an initial prototype volume which is then moved and transported over time without the need for volumetric supervision. Our approach relies purely on image-based losses, an adversarial discriminator network, and regularization. Our method can estimate long-term sequences in a stable manner, while achieving closely matching targets for inputs such as rising smoke plumes.



### Benchmarking Deepart Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.14475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14475v1)
- **Published**: 2023-02-28 10:34:44+00:00
- **Updated**: 2023-02-28 10:34:44+00:00
- **Authors**: Yabin Wang, Zhiwu Huang, Xiaopeng Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake technologies have been blurring the boundaries between the real and unreal, likely resulting in malicious events. By leveraging newly emerged deepfake technologies, deepfake researchers have been making a great upending to create deepfake artworks (deeparts), which are further closing the gap between reality and fantasy. To address potentially appeared ethics questions, this paper establishes a deepart detection database (DDDB) that consists of a set of high-quality conventional art images (conarts) and five sets of deepart images generated by five state-of-the-art deepfake models. This database enables us to explore once-for-all deepart detection and continual deepart detection. For the two new problems, we suggest four benchmark evaluations and four families of solutions on the constructed DDDB. The comprehensive study demonstrates the effectiveness of the proposed solutions on the established benchmark dataset, which is capable of paving a way to more interesting directions of deepart detection. The constructed benchmark dataset and the source code will be made publicly available.



### RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data
- **Arxiv ID**: http://arxiv.org/abs/2302.14483v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.14483v1)
- **Published**: 2023-02-28 10:54:36+00:00
- **Updated**: 2023-02-28 10:54:36+00:00
- **Authors**: Sangwoo Mo, Jong-Chyi Su, Chih-Yao Ma, Mido Assran, Ishan Misra, Licheng Yu, Sean Bell
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Semi-supervised learning aims to train a model using limited labels. State-of-the-art semi-supervised methods for image classification such as PAWS rely on self-supervised representations learned with large-scale unlabeled but curated data. However, PAWS is often less effective when using real-world unlabeled data that is uncurated, e.g., contains out-of-class data. We propose RoPAWS, a robust extension of PAWS that can work with real-world unlabeled data. We first reinterpret PAWS as a generative classifier that models densities using kernel density estimation. From this probabilistic perspective, we calibrate its prediction based on the densities of labeled and unlabeled data, which leads to a simple closed-form solution from the Bayes' rule. We demonstrate that RoPAWS significantly improves PAWS for uncurated Semi-iNat by +5.3% and curated ImageNet by +0.4%.



### Memory-aided Contrastive Consensus Learning for Co-salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.14485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14485v2)
- **Published**: 2023-02-28 10:58:01+00:00
- **Updated**: 2023-03-11 16:38:07+00:00
- **Authors**: Peng Zheng, Jie Qin, Shuo Wang, Tian-Zhu Xiang, Huan Xiong
- **Comment**: AAAI 2023
- **Journal**: None
- **Summary**: Co-Salient Object Detection (CoSOD) aims at detecting common salient objects within a group of relevant source images. Most of the latest works employ the attention mechanism for finding common objects. To achieve accurate CoSOD results with high-quality maps and high efficiency, we propose a novel Memory-aided Contrastive Consensus Learning (MCCL) framework, which is capable of effectively detecting co-salient objects in real time (~150 fps). To learn better group consensus, we propose the Group Consensus Aggregation Module (GCAM) to abstract the common features of each image group; meanwhile, to make the consensus representation more discriminative, we introduce the Memory-based Contrastive Module (MCM), which saves and updates the consensus of images from different groups in a queue of memories. Finally, to improve the quality and integrity of the predicted maps, we develop an Adversarial Integrity Learning (AIL) strategy to make the segmented regions more likely composed of complete objects with less surrounding noise. Extensive experiments on all the latest CoSOD benchmarks demonstrate that our lite MCCL outperforms 13 cutting-edge models, achieving the new state of the art (~5.9% and ~6.2% improvement in S-measure on CoSOD3k and CoSal2015, respectively). Our source codes, saliency maps, and online demos are publicly available at https://github.com/ZhengPeng7/MCCL.



### TrainSim: A Railway Simulation Framework for LiDAR and Camera Dataset Generation
- **Arxiv ID**: http://arxiv.org/abs/2302.14486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2302.14486v1)
- **Published**: 2023-02-28 11:00:13+00:00
- **Updated**: 2023-02-28 11:00:13+00:00
- **Authors**: Gianluca D'Amico, Mauro Marinoni, Federico Nesti, Giulio Rossolini, Giorgio Buttazzo, Salvatore Sabina, Gianluigi Lauro
- **Comment**: Under review
- **Journal**: None
- **Summary**: The railway industry is searching for new ways to automate a number of complex train functions, such as object detection, track discrimination, and accurate train positioning, which require the artificial perception of the railway environment through different types of sensors, including cameras, LiDARs, wheel encoders, and inertial measurement units. A promising approach for processing such sensory data is the use of deep learning models, which proved to achieve excellent performance in other application domains, as robotics and self-driving cars. However, testing new algorithms and solutions requires the availability of a large amount of labeled data, acquired in different scenarios and operating conditions, which are difficult to obtain in a real railway setting due to strict regulations and practical constraints in accessing the trackside infrastructure and equipping a train with the required sensors. To address such difficulties, this paper presents a visual simulation framework able to generate realistic railway scenarios in a virtual environment and automatically produce inertial data and labeled datasets from emulated LiDARs and cameras useful for training deep neural networks or testing innovative algorithms. A set of experimental results are reported to show the effectiveness of the proposed approach.



### Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.14487v1
- **DOI**: 10.1109/ICCE56470.2023.10043496
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.8; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2302.14487v1)
- **Published**: 2023-02-28 11:00:55+00:00
- **Updated**: 2023-02-28 11:00:55+00:00
- **Authors**: Sudeep Kumar Sahoo, Sathish Chalasani, Abhishek Joshi, Kiran Nanjunda Iyer
- **Comment**: 6 pages, 7 figures Published in IEEE ICCE 2023
- **Journal**: 2023 IEEE International Conference on Consumer Electronics (ICCE),
  Las Vegas, NV, USA, 2023, pp. 1-6
- **Summary**: Real-world vision based applications require fine-grained classification for various area of interest like e-commerce, mobile applications, warehouse management, etc. where reducing the severity of mistakes and improving the classification accuracy is of utmost importance. This paper proposes a method to boost fine-grained classification through a hierarchical approach via learnable independent query embeddings. This is achieved through a classification network that uses coarse class predictions to improve the fine class accuracy in a stage-wise sequential manner. We exploit the idea of hierarchy to learn query embeddings that are scalable across all levels, thus making this a relevant approach even for extreme classification where we have a large number of classes. The query is initialized with a weighted Eigen image calculated from training samples to best represent and capture the variance of the object. We introduce transformer blocks to fuse intermediate layers at which query attention happens to enhance the spatial representation of feature maps at different scales. This multi-scale fusion helps improve the accuracy of small-size objects. We propose a two-fold approach for the unique representation of learnable queries. First, at each hierarchical level, we leverage cluster based loss that ensures maximum separation between inter-class query embeddings and helps learn a better (query) representation in higher dimensional spaces. Second, we fuse coarse level queries with finer level queries weighted by a learned scale factor. We additionally introduce a novel block called Cross Attention on Multi-level queries with Prior (CAMP) Block that helps reduce error propagation from coarse level to finer level, which is a common problem in all hierarchical classifiers. Our method is able to outperform the existing methods with an improvement of ~11% at the fine-grained classification.



### Estimating Head Motion from MR-Images
- **Arxiv ID**: http://arxiv.org/abs/2302.14490v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14490v1)
- **Published**: 2023-02-28 11:03:08+00:00
- **Updated**: 2023-02-28 11:03:08+00:00
- **Authors**: Clemens Pollak, David K√ºgler, Martin Reuter
- **Comment**: None
- **Journal**: None
- **Summary**: Head motion is an omnipresent confounder of magnetic resonance image (MRI) analyses as it systematically affects morphometric measurements, even when visual quality control is performed. In order to estimate subtle head motion, that remains undetected by experts, we introduce a deep learning method to predict in-scanner head motion directly from T1-weighted (T1w), T2-weighted (T2w) and fluid-attenuated inversion recovery (FLAIR) images using motion estimates from an in-scanner depth camera as ground truth. Since we work with data from compliant healthy participants of the Rhineland Study, head motion and resulting imaging artifacts are less prevalent than in most clinical cohorts and more difficult to detect. Our method demonstrates improved performance compared to state-of-the-art motion estimation methods and can quantify drift and respiration movement independently. Finally, on unseen data, our predictions preserve the known, significant correlation with age.



### Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?
- **Arxiv ID**: http://arxiv.org/abs/2302.14503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.14503v1)
- **Published**: 2023-02-28 11:34:55+00:00
- **Updated**: 2023-02-28 11:34:55+00:00
- **Authors**: Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee
- **Comment**: 7 pages, 3 figures, ICRA 2023
- **Journal**: None
- **Summary**: After many researchers observed fruitfulness from the recent diffusion probabilistic model, its effectiveness in image generation is actively studied these days. In this paper, our objective is to evaluate the potential of diffusion probabilistic models for 3D human motion-related tasks. To this end, this paper presents a study of employing diffusion probabilistic models to predict future 3D human motion(s) from the previously observed motion. Based on the Human 3.6M and HumanEva-I datasets, our results show that diffusion probabilistic models are competitive for both single (deterministic) and multiple (stochastic) 3D motion prediction tasks, after finishing a single training process. In addition, we find out that diffusion probabilistic models can offer an attractive compromise, since they can strike the right balance between the likelihood and diversity of the predicted future motions. Our code is publicly available on the project website: https://sites.google.com/view/diffusion-motion-prediction.



### A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.14511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14511v2)
- **Published**: 2023-02-28 12:01:16+00:00
- **Updated**: 2023-03-14 09:10:14+00:00
- **Authors**: Lin Li, Wendong Ding, Yongkun Wen, Yufei Liang, Yong Liu, Guowei Wan
- **Comment**: 8 pages. Accepted by ICRA-2023
- **Journal**: None
- **Summary**: Pairwise point cloud registration is a critical task for many applications, which heavily depends on finding correct correspondences from the two point clouds. However, the low overlap between input point clouds causes the registration to fail easily, leading to mistaken overlapping and mismatched correspondences, especially in scenes where non-overlapping regions contain similar structures. In this paper, we present a unified bird's-eye view (BEV) model for jointly learning of 3D local features and overlap estimation to fulfill pairwise registration and loop closure. Feature description is performed by a sparse UNet-like network based on BEV representation, and 3D keypoints are extracted by a detection head for 2D locations, and a regression head for heights. For overlap detection, a cross-attention module is applied for interacting contextual information of input point clouds, followed by a classification head to estimate the overlapping region. We evaluate our unified model extensively on the KITTI dataset and Apollo-SouthBay dataset. The experiments demonstrate that our method significantly outperforms existing methods on overlap estimation, especially in scenes with small overlaps. It also achieves top registration performance on both datasets in terms of translation and rotation errors.



### AdaptiveShape: Solving Shape Variability for 3D Object Detection with Geometry Aware Anchor Distributions
- **Arxiv ID**: http://arxiv.org/abs/2302.14522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14522v1)
- **Published**: 2023-02-28 12:31:31+00:00
- **Updated**: 2023-02-28 12:31:31+00:00
- **Authors**: Benjamin Sick, Michael Walter, Jochen Abhau
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection with point clouds and images plays an important role in perception tasks such as autonomous driving. Current methods show great performance on detection and pose estimation of standard-shaped vehicles but lack behind on more complex shapes as e.g. semi-trailer truck combinations. Determining the shape and motion of those special vehicles accurately is crucial in yard operation and maneuvering and industrial automation applications. This work introduces several new methods to improve and measure the performance for such classes. State-of-the-art methods are based on predefined anchor grids or heatmaps for ground truth targets. However, the underlying representations do not take the shape of different sized objects into account. Our main contribution, AdaptiveShape, uses shape aware anchor distributions and heatmaps to improve the detection capabilities. For large vehicles we achieve +10.9% AP in comparison to current shape agnostic methods. Furthermore we introduce a new fast LiDAR-camera fusion. It is based on 2D bounding box camera detections which are available in many processing pipelines. This fusion method does not rely on perfectly calibrated or temporally synchronized systems and is therefore applicable to a broad range of robotic applications. We extend a standard point pillar network to account for temporal data and improve learning of complex object movements. In addition we extended a ground truth augmentation to use grouped object pairs to further improve truck AP by +2.2% compared to conventional augmentation.



### DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2302.14533v1
- **DOI**: 10.5220/0011799600003417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14533v1)
- **Published**: 2023-02-28 12:43:52+00:00
- **Updated**: 2023-02-28 12:43:52+00:00
- **Authors**: Rajiv Kumar, G. Sivakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Requirements of large amounts of data is a difficulty in training many GANs. Data efficient GANs involve fitting a generators continuous target distribution with a limited discrete set of data samples, which is a difficult task. Single image methods have focused on modeling the internal distribution of a single image and generating its samples. While single image methods can synthesize image samples with diversity, they do not model multiple images or capture the inherent relationship possible between two images. Given only a handful of images, we are interested in generating samples and exploiting the commonalities in the input images. In this work, we extend the single-image GAN method to model multiple images for sample synthesis. We modify the discriminator with an auxiliary classifier branch, which helps to generate a wide variety of samples and to classify the input labels. Our Data-Efficient GAN (DEff-GAN) generates excellent results when similarities and correspondences can be drawn between the input images or classes.



### FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.14554v1
- **DOI**: 10.5220/0011797600003417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14554v1)
- **Published**: 2023-02-28 13:19:11+00:00
- **Updated**: 2023-02-28 13:19:11+00:00
- **Authors**: Chintan Tundia, Rajiv Kumar, Om Damani, G. Sivakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection for aerial imagery involves locating and identifying changes associated with the areas of interest between co-registered bi-temporal or multi-temporal images of a geographical location. Farm ponds are man-made structures belonging to the category of minor irrigation structures used to collect surface run-off water for future irrigation purposes. Detection of farm ponds from aerial imagery and their evolution over time helps in land surveying to analyze the agricultural shifts, policy implementation, seasonal effects and climate changes. In this paper, we introduce a publicly available object detection and instance segmentation (OD/IS) dataset for localizing farm ponds from aerial imagery. We also collected and annotated the bi-temporal data over a time-span of 14 years across 17 villages, resulting in a binary change detection dataset called \textbf{F}arm \textbf{P}ond \textbf{C}hange \textbf{D}etection Dataset (\textbf{FPCD}). We have benchmarked and analyzed the performance of various object detection and instance segmentation methods on our OD/IS dataset and the change detection methods over the FPCD dataset. The datasets are publicly accessible at this page: \textit{\url{https://huggingface.co/datasets/ctundia/FPCD}}



### GRAN: Ghost Residual Attention Network for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2302.14557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14557v2)
- **Published**: 2023-02-28 13:26:24+00:00
- **Updated**: 2023-03-02 02:01:19+00:00
- **Authors**: Axi Niu, Pei Wang, Yu Zhu, Jinqiu Sun, Qingsen Yan, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many works have designed wider and deeper networks to achieve higher image super-resolution performance. Despite their outstanding performance, they still suffer from high computational resources, preventing them from directly applying to embedded devices. To reduce the computation resources and maintain performance, we propose a novel Ghost Residual Attention Network (GRAN) for efficient super-resolution. This paper introduces Ghost Residual Attention Block (GRAB) groups to overcome the drawbacks of the standard convolutional operation, i.e., redundancy of the intermediate feature. GRAB consists of the Ghost Module and Channel and Spatial Attention Module (CSAM) to alleviate the generation of redundant features. Specifically, Ghost Module can reveal information underlying intrinsic features by employing linear operations to replace the standard convolutions. Reducing redundant features by the Ghost Module, our model decreases memory and computing resource requirements in the network. The CSAM pays more comprehensive attention to where and what the feature extraction is, which is critical to recovering the image details. Experiments conducted on the benchmark datasets demonstrate the superior performance of our method in both qualitative and quantitative. Compared to the baseline models, we achieve higher performance with lower computational resources, whose parameters and FLOPs have decreased by more than ten times.



### A Little Bit Attention Is All You Need for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2302.14574v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14574v1)
- **Published**: 2023-02-28 13:54:31+00:00
- **Updated**: 2023-02-28 13:54:31+00:00
- **Authors**: Markus Eisenbach, Jannik L√ºbberstedt, Dustin Aganian, Horst-Michael Gross
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2023
- **Journal**: None
- **Summary**: Person re-identification plays a key role in applications where a mobile robot needs to track its users over a long period of time, even if they are partially unobserved for some time, in order to follow them or be available on demand. In this context, deep-learning based real-time feature extraction on a mobile robot is often performed on special-purpose devices whose computational resources are shared for multiple tasks. Therefore, the inference speed has to be taken into account. In contrast, person re-identification is often improved by architectural changes that come at the cost of significantly slowing down inference. Attention blocks are one such example. We will show that some well-performing attention blocks used in the state of the art are subject to inference costs that are far too high to justify their use for mobile robotic applications. As a consequence, we propose an attention block that only slightly affects the inference speed while keeping up with much deeper networks or more complex attention blocks in terms of re-identification accuracy. We perform extensive neural architecture search to derive rules at which locations this attention block should be integrated into the architecture in order to achieve the best trade-off between speed and accuracy. Finally, we confirm that the best performing configuration on a re-identification benchmark also performs well on an indoor robotic dataset.



### Interactive Segmentation as Gaussian Process Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.14578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14578v1)
- **Published**: 2023-02-28 14:01:01+00:00
- **Updated**: 2023-02-28 14:01:01+00:00
- **Authors**: Minghao Zhou, Hong Wang, Qian Zhao, Yuexiang Li, Yawen Huang, Deyu Meng, Yefeng Zheng
- **Comment**: To appear in CVPR2023
- **Journal**: None
- **Summary**: Click-based interactive segmentation (IS) aims to extract the target objects under user interaction. For this task, most of the current deep learning (DL)-based methods mainly follow the general pipelines of semantic segmentation. Albeit achieving promising performance, they do not fully and explicitly utilize and propagate the click information, inevitably leading to unsatisfactory segmentation results, even at clicked points. Against this issue, in this paper, we propose to formulate the IS task as a Gaussian process (GP)-based pixel-wise binary classification model on each image. To solve this model, we utilize amortized variational inference to approximate the intractable GP posterior in a data-driven manner and then decouple the approximated GP posterior into double space forms for efficient sampling with linear complexity. Then, we correspondingly construct a GP classification framework, named GPCIS, which is integrated with the deep kernel learning mechanism for more flexibility. The main specificities of the proposed GPCIS lie in: 1) Under the explicit guidance of the derived GP posterior, the information contained in clicks can be finely propagated to the entire image and then boost the segmentation; 2) The accuracy of predictions at clicks has good theoretical support. These merits of GPCIS as well as its good generality and high efficiency are substantiated by comprehensive experiments on several benchmarks, as compared with representative methods both quantitatively and qualitatively.



### HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.14581v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14581v3)
- **Published**: 2023-02-28 14:03:40+00:00
- **Updated**: 2023-08-19 14:02:46+00:00
- **Authors**: Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, Shanlin Yang
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: 2D-to-3D human pose lifting is fundamental for 3D human pose estimation (HPE), for which graph convolutional networks (GCNs) have proven inherently suitable for modeling the human skeletal topology. However, the current GCN-based 3D HPE methods update the node features by aggregating their neighbors' information without considering the interaction of joints in different joint synergies. Although some studies have proposed importing limb information to learn the movement patterns, the latent synergies among joints, such as maintaining balance are seldom investigated. We propose the Hop-wise GraphFormer with Intragroup Joint Refinement (HopFIR) architecture to tackle the 3D HPE problem. HopFIR mainly consists of a novel hop-wise GraphFormer (HGF) module and an intragroup joint refinement (IJR) module. The HGF module groups the joints by k-hop neighbors and applies a hopwise transformer-like attention mechanism to these groups to discover latent joint synergies. The IJR module leverages the prior limb information for peripheral joint refinement. Extensive experimental results show that HopFIR outperforms the SOTA methods by a large margin, with a mean per-joint position error (MPJPE) on the Human3.6M dataset of 32.67 mm. We also demonstrate that the state-of-the-art GCN-based methods can benefit from the proposed hop-wise attention mechanism with a significant improvement in performance: SemGCN and MGCN are improved by 8.9% and 4.5%, respectively.



### Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation
- **Arxiv ID**: http://arxiv.org/abs/2302.14589v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14589v3)
- **Published**: 2023-02-28 14:16:32+00:00
- **Updated**: 2023-03-08 02:26:37+00:00
- **Authors**: Hao Ren, Shoudong Han, Huilin Ding, Ziwen Zhang, Hongwei Wang, Faquan Wang
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Discriminative representation is essential to keep a unique identifier for each target in Multiple object tracking (MOT). Some recent MOT methods extract features of the bounding box region or the center point as identity embeddings. However, when targets are occluded, these coarse-grained global representations become unreliable. To this end, we propose exploring diverse fine-grained representation, which describes appearance comprehensively from global and local perspectives. This fine-grained representation requires high feature resolution and precise semantic information. To effectively alleviate the semantic misalignment caused by indiscriminate contextual information aggregation, Flow Alignment FPN (FAFPN) is proposed for multi-scale feature alignment aggregation. It generates semantic flow among feature maps from different resolutions to transform their pixel positions. Furthermore, we present a Multi-head Part Mask Generator (MPMG) to extract fine-grained representation based on the aligned feature maps. Multiple parallel branches of MPMG allow it to focus on different parts of targets to generate local masks without label supervision. The diverse details in target masks facilitate fine-grained representation. Eventually, benefiting from a Shuffle-Group Sampling (SGS) training strategy with positive and negative samples balanced, we achieve state-of-the-art performance on MOT17 and MOT20 test sets. Even on DanceTrack, where the appearance of targets is extremely similar, our method significantly outperforms ByteTrack by 5.0% on HOTA and 5.6% on IDF1. Extensive experiments have proved that diverse fine-grained representation makes Re-ID great again in MOT.



### MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments
- **Arxiv ID**: http://arxiv.org/abs/2302.14595v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14595v1)
- **Published**: 2023-02-28 14:29:22+00:00
- **Updated**: 2023-02-28 14:29:22+00:00
- **Authors**: Junwei Zheng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen
- **Comment**: Code will be available at: https://github.com/JunweiZheng93/MATERobot
- **Journal**: None
- **Summary**: Wearable robotics can improve the lives of People with Visual Impairments (PVI) by providing additional sensory information. Blind people typically recognize objects through haptic perception. However, knowing materials before touching is under-explored in the field of assistive technology. To fill this gap, in this work, a wearable robotic system, MateRobot, is established for PVI to recognize materials before hand. Specially, the human-centric system can perform pixel-wise semantic segmentation of objects and materials. Considering both general object segmentation and material segmentation, an efficient MateViT architecture with Learnable Importance Sampling (LIS) and Multi-gate Mixture-of-Experts (MMoE) is proposed to wearable robots to achieve complementary gains from different target domains. Our methods achieve respective 40.2% and 51.1% of mIoU on COCOStuff and DMS datasets, surpassing previous method with +5.7% and +7.0% gains. Moreover, on the field test with participants, our wearable system obtains a score of 28 in NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MateRobot demonstrates the feasibility of recognizing material properties through visual cues, and offers a promising step towards improving the functionality of wearable robots for PVI. Code will be available at: https://github.com/JunweiZheng93/MATERobot.



### Fast as CHITA: Neural Network Pruning with Combinatorial Optimization
- **Arxiv ID**: http://arxiv.org/abs/2302.14623v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2302.14623v1)
- **Published**: 2023-02-28 15:03:18+00:00
- **Updated**: 2023-02-28 15:03:18+00:00
- **Authors**: Riade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder
- **Comment**: None
- **Journal**: None
- **Summary**: The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.



### Parametrizing Product Shape Manifolds by Composite Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.14665v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, math.DG, 68T07, 65D18, 65D15
- **Links**: [PDF](http://arxiv.org/pdf/2302.14665v1)
- **Published**: 2023-02-28 15:31:23+00:00
- **Updated**: 2023-02-28 15:31:23+00:00
- **Authors**: Josua Sassen, Klaus Hildebrandt, Martin Rumpf, Benedikt Wirth
- **Comment**: None
- **Journal**: None
- **Summary**: Parametrizations of data manifolds in shape spaces can be computed using the rich toolbox of Riemannian geometry. This, however, often comes with high computational costs, which raises the question if one can learn an efficient neural network approximation. We show that this is indeed possible for shape spaces with a special product structure, namely those smoothly approximable by a direct sum of low-dimensional manifolds. Our proposed architecture leverages this structure by separately learning approximations for the low-dimensional factors and a subsequent combination. After developing the approach as a general framework, we apply it to a shape space of triangular surfaces. Here, typical examples of data manifolds are given through datasets of articulated models and can be factorized, for example, by a Sparse Principal Geodesic Analysis (SPGA). We demonstrate the effectiveness of our proposed approach with experiments on synthetic data as well as manifolds extracted from data via SPGA.



### Double Dynamic Sparse Training for GANs
- **Arxiv ID**: http://arxiv.org/abs/2302.14670v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.14670v1)
- **Published**: 2023-02-28 15:34:01+00:00
- **Updated**: 2023-02-28 15:34:01+00:00
- **Authors**: Yite Wang, Jing Wu, Naira Hovakimyan, Ruoyu Sun
- **Comment**: Under review
- **Journal**: None
- **Summary**: The past decade has witnessed a drastic increase in modern deep neural networks (DNNs) size, especially for generative adversarial networks (GANs). Since GANs usually suffer from high computational complexity, researchers have shown an increased interest in applying pruning methods to reduce the training and inference costs of GANs. Among different pruning methods invented for supervised learning, dynamic sparse training (DST) has gained increasing attention recently as it enjoys excellent training efficiency with comparable performance to post-hoc pruning. Hence, applying DST on GANs, where we train a sparse GAN with a fixed parameter count throughout training, seems to be a good candidate for reducing GAN training costs. However, a few challenges, including the degrading training instability, emerge due to the adversarial nature of GANs. Hence, we introduce a quantity called balance ratio (BR) to quantify the balance of the generator and the discriminator. We conduct a series of experiments to show the importance of BR in understanding sparse GAN training. Building upon single dynamic sparse training (SDST), where only the generator is adjusted during training, we propose double dynamic sparse training (DDST) to control the BR during GAN training. Empirically, DDST automatically determines the density of the discriminator and greatly boosts the performance of sparse GANs on multiple datasets.



### Attention-based Point Cloud Edge Sampling
- **Arxiv ID**: http://arxiv.org/abs/2302.14673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14673v2)
- **Published**: 2023-02-28 15:36:17+00:00
- **Updated**: 2023-03-26 14:32:23+00:00
- **Authors**: Chengzhi Wu, Junwei Zheng, Julius Pfrommer, J√ºrgen Beyerer
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud sampling is a less explored research topic for this data representation. The most commonly used sampling methods are still classical random sampling and farthest point sampling. With the development of neural networks, various methods have been proposed to sample point clouds in a task-based learning manner. However, these methods are mostly generative-based, rather than selecting points directly using mathematical statistics. Inspired by the Canny edge detection algorithm for images and with the help of the attention mechanism, this paper proposes a non-generative Attention-based Point cloud Edge Sampling method (APES), which captures salient points in the point cloud outline. Both qualitative and quantitative experimental results show the superior performance of our sampling method on common benchmark tasks.



### Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger
- **Arxiv ID**: http://arxiv.org/abs/2302.14677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.14677v1)
- **Published**: 2023-02-28 15:39:31+00:00
- **Updated**: 2023-02-28 15:39:31+00:00
- **Authors**: Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex C. Kot
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Recent deep-learning-based compression methods have achieved superior performance compared with traditional approaches. However, deep learning models have proven to be vulnerable to backdoor attacks, where some specific trigger patterns added to the input can lead to malicious behavior of the models. In this paper, we present a novel backdoor attack with multiple triggers against learned image compression models. Motivated by the widely used discrete cosine transform (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives for various attacking scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as down-stream face recognition and semantic segmentation. Moreover, a novel simple dynamic loss is designed to balance the influence of different loss terms adaptively, which helps achieve more efficient training. Extensive experiments show that with our trained trigger injection models and simple modification of encoder parameters (of the compression model), the proposed attack can successfully inject several backdoors with corresponding triggers in a single image compression model.



### Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2302.14680v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14680v2)
- **Published**: 2023-02-28 15:45:20+00:00
- **Updated**: 2023-03-15 14:38:21+00:00
- **Authors**: Holy Lovenia, Samuel Cahyawijaya, Pascale Fung
- **Comment**: Accepted at EACL SRW 2023
- **Journal**: None
- **Summary**: The demand for multimodal dialogue systems has been rising in various domains, emphasizing the importance of interpreting multimodal inputs from conversational and situational contexts. We explore three methods to tackle this problem and evaluate them on the largest situated dialogue dataset, SIMMC 2.1. Our best method, scene-dialogue alignment, improves the performance by ~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and discussion regarding the limitation of our methods and the potential directions for future works. Our code is publicly available at https://github.com/holylovenia/multimodal-object-identification.



### IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF
- **Arxiv ID**: http://arxiv.org/abs/2302.14683v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14683v2)
- **Published**: 2023-02-28 15:51:19+00:00
- **Updated**: 2023-03-09 10:02:16+00:00
- **Authors**: Bo Peng, Jun Hu, Jingtao Zhou, Xuan Gao, Juyong Zhang
- **Comment**: Project page:https://ustc3dv.github.io/IntrinsicNGP/. arXiv admin
  note: substantial text overlap with arXiv:2210.01651
- **Journal**: None
- **Summary**: Recently, many works have been proposed to utilize the neural radiance field for novel view synthesis of human performers. However, most of these methods require hours of training, making them difficult for practical use. To address this challenging problem, we propose IntrinsicNGP, which can train from scratch and achieve high-fidelity results in few minutes with videos of a human performer. To achieve this target, we introduce a continuous and optimizable intrinsic coordinate rather than the original explicit Euclidean coordinate in the hash encoding module of instant-NGP. With this novel intrinsic coordinate, IntrinsicNGP can aggregate inter-frame information for dynamic objects with the help of proxy geometry shapes. Moreover, the results trained with the given rough geometry shapes can be further refined with an optimizable offset field based on the intrinsic coordinate.Extensive experimental results on several datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also illustrate our approach's ability to edit the shape of reconstructed subjects.



### DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.14685v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.14685v2)
- **Published**: 2023-02-28 15:54:47+00:00
- **Updated**: 2023-06-10 15:11:02+00:00
- **Authors**: Samyak Jain, Sravanti Addepalli, Pawan Sahu, Priyam Dey, R. Venkatesh Babu
- **Comment**: CVPR 2023. First two authors contributed equally
- **Journal**: None
- **Summary**: Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by casting it in the framework proposed by Shen et al. and theoretically show that it indeed generalizes better. In addition to improvements in In- Domain generalization, we demonstrate SOTA performance on the Domain Generalization benchmarks in the popular DomainBed framework as well. Our method is generic and can easily be integrated with several base training algorithms to achieve performance gains.



### Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.14696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14696v1)
- **Published**: 2023-02-28 16:09:06+00:00
- **Updated**: 2023-02-28 16:09:06+00:00
- **Authors**: Jian Shi, Pengyi Zhang, Ni Zhang, Hakim Ghazzai, Yehia Massoud
- **Comment**: None
- **Journal**: None
- **Summary**: Medical anomalous data normally contains fine-grained instance-wise additive feature patterns (e.g. tumor, hemorrhage), that are oftenly critical but insignificant. Interestingly, apart from the remarkable image generation abilities of diffusion models, we observed that diffusion models can dissolve image details for a given image, resulting in generalized feature representations. We hereby propose DIA, dissolving is amplifying, that amplifies fine-grained image features by contrasting an image against its feature dissolved counterpart. In particular, we show that diffusion models can serve as semantic preserving feature dissolvers that help learning fine-grained anomalous patterns for anomaly detection tasks, especially for medical domains with fine-grained feature differences. As a result, our method yields a novel fine-grained anomaly detection method, aims at amplifying instance-level feature patterns, that significantly improves medical anomaly detection accuracy in a large margin without any prior knowledge of explicit fine-grained anomalous feature patterns.



### Global Context-Aware Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2302.14728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.14728v1)
- **Published**: 2023-02-28 16:34:55+00:00
- **Updated**: 2023-02-28 16:34:55+00:00
- **Authors**: Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: We propose a data-driven approach for context-aware person image generation. Specifically, we attempt to generate a person image such that the synthesized instance can blend into a complex scene. In our method, the position, scale, and appearance of the generated person are semantically conditioned on the existing persons in the scene. The proposed technique is divided into three sequential steps. At first, we employ a Pix2PixHD model to infer a coarse semantic mask that represents the new person's spatial location, scale, and potential pose. Next, we use a data-centric approach to select the closest representation from a precomputed cluster of fine semantic masks. Finally, we adopt a multi-scale, attention-guided architecture to transfer the appearance attributes from an exemplar image. The proposed strategy enables us to synthesize semantically coherent realistic persons that can blend into an existing scene without altering the global context. We conclude our findings with relevant qualitative and quantitative evaluations.



### TextIR: A Simple Framework for Text-based Editable Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2302.14736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14736v1)
- **Published**: 2023-02-28 16:39:36+00:00
- **Updated**: 2023-02-28 16:39:36+00:00
- **Authors**: Yunpeng Bai, Cairong Wang, Shuzhao Xie, Chao Dong, Chun Yuan, Zhi Wang
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Most existing image restoration methods use neural networks to learn strong image-level priors from huge data to estimate the lost information. However, these works still struggle in cases when images have severe information deficits. Introducing external priors or using reference images to provide information also have limitations in the application domain. In contrast, text input is more readily available and provides information with higher flexibility. In this work, we design an effective framework that allows the user to control the restoration process of degraded images with text descriptions. We use the text-image feature compatibility of the CLIP to alleviate the difficulty of fusing text and image features. Our framework can be used for various image restoration tasks, including image inpainting, image super-resolution, and image colorization. Extensive experiments demonstrate the effectiveness of our method.



### Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors
- **Arxiv ID**: http://arxiv.org/abs/2302.14746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14746v1)
- **Published**: 2023-02-28 16:45:21+00:00
- **Updated**: 2023-02-28 16:45:21+00:00
- **Authors**: Ji Hou, Xiaoliang Dai, Zijian He, Angela Dai, Matthias Nie√üner
- **Comment**: accepted to CVPR2023
- **Journal**: None
- **Summary**: Current popular backbones in computer vision, such as Vision Transformers (ViT) and ResNets are trained to perceive the world from 2D images. However, to more effectively understand 3D structural priors in 2D backbones, we propose Mask3D to leverage existing large-scale RGB-D data in a self-supervised pre-training to embed these 3D priors into 2D learned feature representations. In contrast to traditional 3D contrastive learning paradigms requiring 3D reconstructions or multi-view correspondences, our approach is simple: we formulate a pre-text reconstruction task by masking RGB and depth patches in individual RGB-D frames. We demonstrate the Mask3D is particularly effective in embedding 3D priors into the powerful 2D ViT backbone, enabling improved representation learning for various scene understanding tasks, such as semantic segmentation, instance segmentation and object detection. Experiments show that Mask3D notably outperforms existing self-supervised 3D pre-training approaches on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image semantic segmentation.



### Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.14762v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2302.14762v1)
- **Published**: 2023-02-28 17:02:35+00:00
- **Updated**: 2023-02-28 17:02:35+00:00
- **Authors**: K√©vin Cortacero, Brienne McKenzie, Sabina M√ºller, Roxana Khazen, Fanny Lafouresse, Ga√´lle Corsaut, Nathalie Van Acker, Fran√ßois-Xavier Frenois, Laurence Lamant, Nicolas Meyer, B√©atrice Vergier, Dennis G. Wilson, Herv√© Luga, Oskar Staufer, Michael L. Dustin, Salvatore Valitutti, Sylvain Cussat-Blanc
- **Comment**: 42 pages, 6 main Figures, 3 Extended Data Figures, 5 Extended Data
  Tables, 1 Extended Data Movie. The Extended Data Movie is available at the
  following link:
  https://drive.google.com/file/d/1eNGhFC8gyu5xjVOhIZve894g3bBKXEgs/view?usp=sharing
- **Journal**: None
- **Summary**: An unresolved issue in contemporary biomedicine is the overwhelming number and diversity of complex images that require annotation, analysis and interpretation. Recent advances in Deep Learning have revolutionized the field of computer vision, creating algorithms that compete with human experts in image segmentation tasks. Crucially however, these frameworks require large human-annotated datasets for training and the resulting models are difficult to interpret. In this study, we introduce Kartezio, a modular Cartesian Genetic Programming based computational strategy that generates transparent and easily interpretable image processing pipelines by iteratively assembling and parameterizing computer vision functions. The pipelines thus generated exhibit comparable precision to state-of-the-art Deep Learning approaches on instance segmentation tasks, while requiring drastically smaller training datasets, a feature which confers tremendous flexibility, speed, and functionality to this approach. We also deployed Kartezio to solve semantic and instance segmentation problems in four real-world Use Cases, and showcase its utility in imaging contexts ranging from high-resolution microscopy to clinical pathology. By successfully implementing Kartezio on a portfolio of images ranging from subcellular structures to tumoral tissue, we demonstrated the flexibility, robustness and practical utility of this fully explicable evolutionary designer for semantic and instance segmentation.



### Membership Inference Attack for Beluga Whales Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2302.14769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14769v1)
- **Published**: 2023-02-28 17:10:32+00:00
- **Updated**: 2023-02-28 17:10:32+00:00
- **Authors**: Voncarlos Marcelo Ara√∫jo, S√©bastien Gambs, Cl√©ment Chion, Robert Michaud, L√©o Schneider, Hadrien Lautraite
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: To efficiently monitor the growth and evolution of a particular wildlife population, one of the main fundamental challenges to address in animal ecology is the re-identification of individuals that have been previously encountered but also the discrimination between known and unknown individuals (the so-called "open-set problem"), which is the first step to realize before re-identification. In particular, in this work, we are interested in the discrimination within digital photos of beluga whales, which are known to be among the most challenging marine species to discriminate due to their lack of distinctive features. To tackle this problem, we propose a novel approach based on the use of Membership Inference Attacks (MIAs), which are normally used to assess the privacy risks associated with releasing a particular machine learning model. More precisely, we demonstrate that the problem of discriminating between known and unknown individuals can be solved efficiently using state-of-the-art approaches for MIAs. Extensive experiments on three benchmark datasets related to whales, two different neural network architectures, and three MIA clearly demonstrate the performance of the approach. In addition, we have also designed a novel MIA strategy that we coined as ensemble MIA, which combines the outputs of different MIAs to increase the attack accuracy while diminishing the false positive rate. Overall, one of our main objectives is also to show that the research on privacy attacks can also be leveraged "for good" by helping to address practical challenges encountered in animal ecology.



### Generic-to-Specific Distillation of Masked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2302.14771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14771v1)
- **Published**: 2023-02-28 17:13:14+00:00
- **Updated**: 2023-02-28 17:13:14+00:00
- **Authors**: Wei Huang, Zhiliang Peng, Li Dong, Furu Wei, Jianbin Jiao, Qixiang Ye
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Large vision Transformers (ViTs) driven by self-supervised pre-training mechanisms achieved unprecedented progress. Lightweight ViT models limited by the model capacity, however, benefit little from those pre-training mechanisms. Knowledge distillation defines a paradigm to transfer representations from large (teacher) models to small (student) ones. However, the conventional single-stage distillation easily gets stuck on task-specific transfer, failing to retain the task-agnostic knowledge crucial for model generalization. In this study, we propose generic-to-specific distillation (G2SD), to tap the potential of small ViT models under the supervision of large models pre-trained by masked autoencoders. In generic distillation, decoder of the small model is encouraged to align feature predictions with hidden representations of the large model, so that task-agnostic knowledge can be transferred. In specific distillation, predictions of the small model are constrained to be consistent with those of the large model, to transfer task-specific features which guarantee task performance. With G2SD, the vanilla ViT-Small model respectively achieves 98.7%, 98.1% and 99.3% the performance of its teacher (ViT-Base) for image classification, object detection, and semantic segmentation, setting a solid baseline for two-stage vision distillation. Code will be available at https://github.com/pengzhiliang/G2SD.



### PA&DA: Jointly Sampling PAth and DAta for Consistent NAS
- **Arxiv ID**: http://arxiv.org/abs/2302.14772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14772v1)
- **Published**: 2023-02-28 17:14:24+00:00
- **Updated**: 2023-02-28 17:14:24+00:00
- **Authors**: Shun Lu, Yu Hu, Longxing Yang, Zihao Sun, Jilin Mei, Jianchao Tan, Chengru Song
- **Comment**: To appear in CVPR 2023; we will update the camera-ready version soon
- **Journal**: None
- **Summary**: Based on the weight-sharing mechanism, one-shot NAS methods train a supernet and then inherit the pre-trained weights to evaluate sub-models, largely reducing the search cost. However, several works have pointed out that the shared weights suffer from different gradient descent directions during training. And we further find that large gradient variance occurs during supernet training, which degrades the supernet ranking consistency. To mitigate this issue, we propose to explicitly minimize the gradient variance of the supernet training by jointly optimizing the sampling distributions of PAth and DAta (PA&DA). We theoretically derive the relationship between the gradient variance and the sampling distributions, and reveal that the optimal sampling probability is proportional to the normalized gradient norm of path and training data. Hence, we use the normalized gradient norm as the importance indicator for path and training data, and adopt an importance sampling strategy for the supernet training. Our method only requires negligible computation cost for optimizing the sampling distributions of path and data, but achieves lower gradient variance during supernet training and better generalization performance for the supernet, resulting in a more consistent NAS. We conduct comprehensive comparisons with other improved approaches in various search spaces. Results show that our method surpasses others with more reliable ranking performance and higher accuracy of searched architectures, showing the effectiveness of our method. Code is available at https://github.com/ShunLu91/PA-DA.



### VQA with Cascade of Self- and Co-Attention Blocks
- **Arxiv ID**: http://arxiv.org/abs/2302.14777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.14777v1)
- **Published**: 2023-02-28 17:20:40+00:00
- **Updated**: 2023-02-28 17:20:40+00:00
- **Authors**: Aakansha Mishra, Ashish Anand, Prithwijit Guha
- **Comment**: None
- **Journal**: None
- **Summary**: The use of complex attention modules has improved the performance of the Visual Question Answering (VQA) task. This work aims to learn an improved multi-modal representation through dense interaction of visual and textual modalities. The proposed model has an attention block containing both self-attention and co-attention on image and text. The self-attention modules provide the contextual information of objects (for an image) and words (for a question) that are crucial for inferring an answer. On the other hand, co-attention aids the interaction of image and text. Further, fine-grained information is obtained from two modalities by using a Cascade of Self- and Co-Attention blocks (CSCA). This proposal is benchmarked on the widely used VQA2.0 and TDIUC datasets. The efficacy of key components of the model and cascading of attention modules are demonstrated by experiments involving ablation analysis.



### FTSO: Effective NAS via First Topology Second Operator
- **Arxiv ID**: http://arxiv.org/abs/2303.12948v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.12948v1)
- **Published**: 2023-02-28 17:34:26+00:00
- **Updated**: 2023-02-28 17:34:26+00:00
- **Authors**: Likang Wang, Lei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Existing one-shot neural architecture search (NAS) methods have to conduct a search over a giant super-net, which leads to the huge computational cost. To reduce such cost, in this paper, we propose a method, called FTSO, to divide the whole architecture search into two sub-steps. Specifically, in the first step, we only search for the topology, and in the second step, we search for the operators. FTSO not only reduces NAS's search time from days to 0.68 seconds, but also significantly improves the found architecture's accuracy. Our extensive experiments on ImageNet show that within 18 seconds, FTSO can achieve a 76.4% testing accuracy, 1.5% higher than the SOTA, PC-DARTS. In addition, FTSO can reach a 97.77% testing accuracy, 0.27% higher than the SOTA, with nearly 100% (99.8%) search time saved, when searching on CIFAR10.



### Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.14794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14794v1)
- **Published**: 2023-02-28 17:46:18+00:00
- **Updated**: 2023-02-28 17:46:18+00:00
- **Authors**: Ivona Najdenkoska, Xiantong Zhen, Marcel Worring
- **Comment**: International Conference on Learning Representations 2023
- **Journal**: None
- **Summary**: Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.



### 3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.14795v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14795v1)
- **Published**: 2023-02-28 17:46:25+00:00
- **Updated**: 2023-02-28 17:46:25+00:00
- **Authors**: Kit Mills Bransby, Vincenzo Tufaro, Murat Cap, Greg Slabaugh, Christos Bourantas, Qianni Zhang
- **Comment**: Pre-print for IEEE International Symposium on Biomedical Imaging 2023
  (ISBI)
- **Journal**: None
- **Summary**: X-ray coronary angiography (XCA) is used to assess coronary artery disease and provides valuable information on lesion morphology and severity. However, XCA images are 2D and therefore limit visualisation of the vessel. 3D reconstruction of coronary vessels is possible using multiple views, however lumen border detection in current software is performed manually resulting in limited reproducibility and slow processing time. In this study we propose 3DAngioNet, a novel deep learning (DL) system that enables rapid 3D vessel mesh reconstruction using 2D XCA images from two views. Our approach learns a coarse mesh template using an EfficientB3-UNet segmentation network and projection geometries, and deforms it using a graph convolutional network. 3DAngioNet outperforms similar automated reconstruction methods, offers improved efficiency, and enables modelling of bifurcated vessels. The approach was validated using state-of-the-art software verified by skilled cardiologists.



### DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/2302.14807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.14807v1)
- **Published**: 2023-02-28 17:57:06+00:00
- **Updated**: 2023-02-28 17:57:06+00:00
- **Authors**: Mohamed Nagy, Majid Khonji, Jorge Dias, Sajid Javed
- **Comment**: \c{opyright} 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Persistent multi-object tracking (MOT) allows autonomous vehicles to navigate safely in highly dynamic environments. One of the well-known challenges in MOT is object occlusion when an object becomes unobservant for subsequent frames. The current MOT methods store objects information, like objects' trajectory, in internal memory to recover the objects after occlusions. However, they retain short-term memory to save computational time and avoid slowing down the MOT method. As a result, they lose track of objects in some occlusion scenarios, particularly long ones. In this paper, we propose DFR-FastMOT, a light MOT method that uses data from a camera and LiDAR sensors and relies on an algebraic formulation for object association and fusion. The formulation boosts the computational time and permits long-term memory that tackles more occlusion scenarios. Our method shows outstanding tracking performance over recent learning and non-learning benchmarks with about 3% and 4% margin in MOTA, respectively. Also, we conduct extensive experiments that simulate occlusion phenomena by employing detectors with various distortion levels. The proposed solution enables superior performance under various distortion levels in detection over current state-of-art methods. Our framework processes about 7,763 frames in 1.48 seconds, which is seven times faster than recent benchmarks. The framework will be available at https://github.com/MohamedNagyMostafa/DFR-FastMOT.



### Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2302.14808v2
- **DOI**: 10.1109/EUVIP53989.2022.9922769
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.14808v2)
- **Published**: 2023-02-28 17:58:54+00:00
- **Updated**: 2023-03-20 20:39:45+00:00
- **Authors**: Maryam Viqar, Violeta Madjarova, Vipul Baghel, Elena Stoykova
- **Comment**: None
- **Journal**: None
- **Summary**: Human veins are important for carrying the blood from the body-parts to the heart. The improper functioning of the human veins may arise from several venous diseases. Varicose vein is one such disease wherein back flow of blood can occur, often resulting in increased venous pressure or restricted blood flow due to changes in the structure of vein. To examine the functional characteristics of the varicose vein, it is crucial to study the physical and bio mechanical properties of the vein. This work proposes a segmentation model Opto-UNet, for segmenting the venous wall structure. Optical Coherence Tomography system is used to acquire images of varicose vein. As the extracted vein is not uniform in shape, hence adequate method of segmentation is required to segment the venous wall. Opto-UNet model is based on the U-Net architecture wherein a new block is integrated into the architecture, employing atrous and separable convolution to extract spatially wide-range and separable features maps for attaining advanced performance. Furthermore, the depth wise separable convolution significantly reduces the complexity of the network by optimizing the number of parameters. The model achieves accuracy of 0.9830, sensitivity of 0.8425 and specificity of 0.9980 using 8.54 million number of parameters. These results indicate that model is highly adequate in segmenting the varicose vein wall without deteriorating the segmentation quality along with reduced complexity



### Monocular Depth Estimation using Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.14816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14816v1)
- **Published**: 2023-02-28 18:08:21+00:00
- **Updated**: 2023-02-28 18:08:21+00:00
- **Authors**: Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, David J. Fleet
- **Comment**: None
- **Journal**: None
- **Summary**: We formulate monocular depth estimation using denoising diffusion models, inspired by their recent successes in high fidelity image generation. To that end, we introduce innovations to address problems arising due to noisy, incomplete depth maps in training data, including step-unrolled denoising diffusion, an $L_1$ loss, and depth infilling during training. To cope with the limited availability of data for supervised training, we leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, with a generic loss and architecture, our DepthGen model achieves SOTA performance on the indoor NYU dataset, and near SOTA results on the outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot performance combined with depth imputation, enable a simple but effective text-to-3D pipeline. Project page: https://depth-gen.github.io



### FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle
- **Arxiv ID**: http://arxiv.org/abs/2302.14831v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2302.14831v2)
- **Published**: 2023-02-28 18:28:35+00:00
- **Updated**: 2023-07-26 13:20:49+00:00
- **Authors**: Meshia C√©dric Oveneke, Rucha Vaishampayan, Deogratias Lukamba Nsadisa, Jenny Ambukiyenyi Onya
- **Comment**: 4 pages, 1 figure, 1 table, paper accepted at Black In AI at the 36th
  Conference on Neural Information Processing Systems (NeurIPS 2022), New
  Orleans, USA
- **Journal**: None
- **Summary**: This work proposes to solve the problem of few-shot biometric authentication by computing the Mahalanobis distance between testing embeddings and a multivariate Gaussian distribution of training embeddings obtained using pre-trained CNNs. Experimental results show that models pre-trained on the ImageNet dataset significantly outperform models pre-trained on human faces. With a VGG16 model, we obtain a FRR of 1.25% for a FAR of 1.18% on a dataset of 20 cattle identities.



### Novel Machine Learning Approach for Predicting Poverty using Temperature and Remote Sensing Data in Ethiopia
- **Arxiv ID**: http://arxiv.org/abs/2302.14835v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T99
- **Links**: [PDF](http://arxiv.org/pdf/2302.14835v1)
- **Published**: 2023-02-28 18:32:16+00:00
- **Updated**: 2023-02-28 18:32:16+00:00
- **Authors**: Om Shah, Krti Tallam
- **Comment**: 12 pages, 3 figures, title page included
- **Journal**: None
- **Summary**: In many developing nations, a lack of poverty data prevents critical humanitarian organizations from responding to large-scale crises. Currently, socioeconomic surveys are the only method implemented on a large scale for organizations and researchers to measure and track poverty. However, the inability to collect survey data efficiently and inexpensively leads to significant temporal gaps in poverty data; these gaps severely limit the ability of organizational entities to address poverty at its root cause. We propose a transfer learning model based on surface temperature change and remote sensing data to extract features useful for predicting poverty rates. Machine learning, supported by data sources of poverty indicators, has the potential to estimate poverty rates accurately and within strict time constraints. Higher temperatures, as a result of climate change, have caused numerous agricultural obstacles, socioeconomic issues, and environmental disruptions, trapping families in developing countries in cycles of poverty. To find patterns of poverty relating to temperature that have the highest influence on spatial poverty rates, we use remote sensing data. The two-step transfer model predicts the temperature delta from high resolution satellite imagery and then extracts image features useful for predicting poverty. The resulting model achieved 80% accuracy on temperature prediction. This method takes advantage of abundant satellite and temperature data to measure poverty in a manner comparable to the existing survey methods and exceeds similar models of poverty prediction.



### BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2302.14859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.14859v2)
- **Published**: 2023-02-28 18:58:03+00:00
- **Updated**: 2023-05-16 15:01:42+00:00
- **Authors**: Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, Ben Mildenhall
- **Comment**: Video and interactive web demo available at
  https://bakedsdf.github.io/
- **Journal**: None
- **Summary**: We present a method for reconstructing high-quality meshes of large unbounded real-world scenes suitable for photorealistic novel view synthesis. We first optimize a hybrid neural volume-surface scene representation designed to have well-behaved level sets that correspond to surfaces in the scene. We then bake this representation into a high-quality triangle mesh, which we equip with a simple and fast view-dependent appearance model based on spherical Gaussians. Finally, we optimize this baked representation to best reproduce the captured viewpoints, resulting in a model that can leverage accelerated polygon rasterization pipelines for real-time view synthesis on commodity hardware. Our approach outperforms previous scene representations for real-time rendering in terms of accuracy, speed, and power consumption, and produces high quality meshes that enable applications such as appearance editing and physical simulation.



### Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2303.00040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.00040v2)
- **Published**: 2023-02-28 19:29:05+00:00
- **Updated**: 2023-03-24 17:35:07+00:00
- **Authors**: Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: The correlation between the vision and text is essential for video moment retrieval (VMR), however, existing methods heavily rely on separate pre-training feature extractors for visual and textual understanding. Without sufficient temporal boundary annotations, it is non-trivial to learn universal video-text alignments. In this work, we explore multi-modal correlations derived from large-scale image-text data to facilitate generalisable VMR. To address the limitations of image-text pre-training models on capturing the video changes, we propose a generic method, referred to as Visual-Dynamic Injection (VDI), to empower the model's understanding of video moments. Whilst existing VMR methods are focusing on building temporal-aware video features, being aware of the text descriptions about the temporal changes is also critical but originally overlooked in pre-training by matching static images with sentences. Therefore, we extract visual context and spatial dynamic information from video frames and explicitly enforce their alignments with the phrases describing video changes (e.g. verb). By doing so, the potentially relevant visual and motion patterns in videos are encoded in the corresponding text embeddings (injected) so to enable more accurate video-text alignments. We conduct extensive experiments on two VMR benchmark datasets (Charades-STA and ActivityNet-Captions) and achieve state-of-the-art performances. Especially, VDI yields notable advantages when being tested on the out-of-distribution splits where the testing samples involve novel scenes and vocabulary.



### Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface
- **Arxiv ID**: http://arxiv.org/abs/2303.00050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.00050v1)
- **Published**: 2023-02-28 19:47:30+00:00
- **Updated**: 2023-02-28 19:47:30+00:00
- **Authors**: Decai Chen, Haofei Lu, Ingo Feldmann, Oliver Schreer, Peter Eisert
- **Comment**: 5 pages, accepted by ICASSP 2023
- **Journal**: None
- **Summary**: Reconstructing general dynamic scenes is important for many computer vision and graphics applications. Recent works represent the dynamic scene with neural radiance fields for photorealistic view synthesis, while their surface geometry is under-constrained and noisy. Other works introduce surface constraints to the implicit neural representation to disentangle the ambiguity of geometry and appearance field for static scene reconstruction. To bridge the gap between rendering dynamic scenes and recovering static surface geometry, we propose a template-free method to reconstruct surface geometry and appearance using neural implicit representations from multi-view videos. We leverage topology-aware deformation and the signed distance field to learn complex dynamic surfaces via differentiable volume rendering without scene-specific prior knowledge like template models. Furthermore, we propose a novel mask-based ray selection strategy to significantly boost the optimization on challenging time-varying regions. Experiments on different multi-view video datasets demonstrate that our method achieves high-fidelity surface reconstruction as well as photorealistic novel view synthesis.



### Applying Plain Transformers to Real-World Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2303.00086v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.00086v3)
- **Published**: 2023-02-28 21:06:36+00:00
- **Updated**: 2023-08-06 13:37:00+00:00
- **Authors**: Lanxiao Li, Michael Heizmann
- **Comment**: None
- **Journal**: None
- **Summary**: To apply transformer-based models to point cloud understanding, many previous works modify the architecture of transformers by using, e.g., local attention and down-sampling. Although they have achieved promising results, earlier works on transformers for point clouds have two issues. First, the power of plain transformers is still under-explored. Second, they focus on simple and small point clouds instead of complex real-world ones. This work revisits the plain transformers in real-world point cloud understanding. We first take a closer look at some fundamental components of plain transformers, e.g., patchifier and positional embedding, for both efficiency and performance. To close the performance gap due to the lack of inductive bias and annotated data, we investigate self-supervised pre-training with masked autoencoder (MAE). Specifically, we propose drop patch, which prevents information leakage and significantly improves the effectiveness of MAE. Our models achieve SOTA results in semantic segmentation on the S3DIS dataset and object detection on the ScanNet dataset with lower computational costs. Our work provides a new baseline for future research on transformers for point clouds.



### A study on the use of perceptual hashing to detect manipulation of embedded messages in images
- **Arxiv ID**: http://arxiv.org/abs/2303.00092v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.00092v1)
- **Published**: 2023-02-28 21:32:49+00:00
- **Updated**: 2023-02-28 21:32:49+00:00
- **Authors**: Sven-Jannik W√∂hnert, Kai Hendrik W√∂hnert, Eldar Almamedov, Carsten Frank, Volker Skwarek
- **Comment**: 12 pages, 3 figures submitted, accepted and presented at IPCV 2022,
  subconference of CSCE, https://american-cse.org/csce2022/conferences-IPCV as
  the publication of the proceedings is delayed, the permission for a
  (pre-)publication on arxiv was granted
  https://american-cse.org/csce2022/publisher
- **Journal**: None
- **Summary**: Typically, metadata of images are stored in a specific data segment of the image file. However, to securely detect changes, data can also be embedded within images. This follows the goal to invisibly and robustly embed as much information as possible to, ideally, even survive compression.   This work searches for embedding principles which allow to distinguish between unintended changes by lossy image compression and malicious manipulation of the embedded message based on the change of its perceptual or robust hash. Different embedding and compression algorithms are compared.   The study shows that embedding a message via integer wavelet transform and compression with Karhunen-Loeve-transform yields the best results. However, it was not possible to distinguish between manipulation and compression in all cases.



### PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.00111v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.00111v2)
- **Published**: 2023-02-28 22:26:18+00:00
- **Updated**: 2023-03-08 07:55:37+00:00
- **Authors**: Mevan Ekanayake, Kamlesh Pawar, Gary Egan, Zhaolin Chen
- **Comment**: 19 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Deep learning (DL) models are capable of successfully exploiting latent representations in MR data and have become state-of-the-art for accelerated MRI reconstruction. However, undersampling the measurements in k-space as well as the over- or under-parameterized and non-transparent nature of DL make these models exposed to uncertainty. Consequently, uncertainty estimation has become a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo (MC) inference techniques have become a common practice where multiple reconstructions are utilized to compute the variance in reconstruction as a measurement of uncertainty. However, these methods demand high computational costs as they require multiple inferences through the DL model. To this end, we introduce a method to estimate uncertainty during MRI reconstruction using a pixel classification framework. The proposed method, PixCUE (stands for Pixel Classification Uncertainty Estimation) produces the reconstructed image along with an uncertainty map during a single forward pass through the DL model. We demonstrate that this approach generates uncertainty maps that highly correlate with the reconstruction errors with respect to various MR imaging sequences and under numerous adversarial conditions. We also show that the estimated uncertainties are correlated to that of the conventional MC method. We further provide an empirical relationship between the uncertainty estimations using PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty in MRI reconstruction with a minimum additional computational cost.



### PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing
- **Arxiv ID**: http://arxiv.org/abs/2303.00137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.00137v1)
- **Published**: 2023-02-28 23:52:01+00:00
- **Updated**: 2023-02-28 23:52:01+00:00
- **Authors**: Yichen Sheng, Jianming Zhang, Julien Philip, Yannick Hold-Geoffroy, Xin Sun, HE Zhang, Lu Ling, Bedrich Benes
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Lighting effects such as shadows or reflections are key in making synthetic images realistic and visually appealing. To generate such effects, traditional computer graphics uses a physically-based renderer along with 3D geometry. To compensate for the lack of geometry in 2D Image compositing, recent deep learning-based approaches introduced a pixel height representation to generate soft shadows and reflections. However, the lack of geometry limits the quality of the generated soft shadows and constrain reflections to pure specular ones. We introduce PixHt-Lab, a system leveraging an explicit mapping from pixel height representation to 3D space. Using this mapping, PixHt-Lab reconstructs both the cutout and background geometry and renders realistic, diverse, lighting effects for image compositing. Given a surface with physically-based materials, we can render reflections with varying glossiness. To generate more realistic soft shadows, we further propose to use 3D-aware buffer channels to guide a neural renderer. Both quantitative and qualitative evaluations demonstrate that PixHt-Lab significantly improves soft shadow generation.



### Texture-Based Input Feature Selection for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.00138v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.00138v3)
- **Published**: 2023-02-28 23:56:31+00:00
- **Updated**: 2023-04-23 09:00:53+00:00
- **Authors**: Yalong Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of video action recognition has been significantly boosted by using motion representations within a two-stream Convolutional Neural Network (CNN) architecture. However, there are a few challenging problems in action recognition in real scenarios, e.g., the variations in viewpoints and poses, and the changes in backgrounds. The domain discrepancy between the training data and the test data causes the performance drop. To improve the model robustness, we propose a novel method to determine the task-irrelevant content in inputs which increases the domain discrepancy. The method is based on a human parsing model (HP model) which jointly conducts dense correspondence labelling and semantic part segmentation. The predictions from the HP model also function as re-rendering the human regions in each video using the same set of textures to make humans appearances in all classes be the same. A revised dataset is generated for training and testing and makes the action recognition model exhibit invariance to the irrelevant content in the inputs. Moreover, the predictions from the HP model are used to enrich the inputs to the AR model during both training and testing. Experimental results show that our proposed model is superior to existing models for action recognition on the HMDB-51 dataset and the Penn Action dataset.



