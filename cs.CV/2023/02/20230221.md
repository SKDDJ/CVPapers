# Arxiv Papers in cs.CV on 2023-02-21
### On Interpretable Approaches to Cluster, Classify and Represent Multi-Subspace Data via Minimum Lossy Coding Length based on Rate-Distortion Theory
- **Arxiv ID**: http://arxiv.org/abs/2302.10383v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2302.10383v1)
- **Published**: 2023-02-21 01:15:08+00:00
- **Updated**: 2023-02-21 01:15:08+00:00
- **Authors**: Kai-Liang Lu, Avraham Chapman
- **Comment**: None
- **Journal**: None
- **Summary**: To cluster, classify and represent are three fundamental objectives of learning from high-dimensional data with intrinsic structure. To this end, this paper introduces three interpretable approaches, i.e., segmentation (clustering) via the Minimum Lossy Coding Length criterion, classification via the Minimum Incremental Coding Length criterion and representation via the Maximal Coding Rate Reduction criterion. These are derived based on the lossy data coding and compression framework from the principle of rate distortion in information theory. These algorithms are particularly suitable for dealing with finite-sample data (allowed to be sparse or almost degenerate) of mixed Gaussian distributions or subspaces. The theoretical value and attractive features of these methods are summarized by comparison with other learning methods or evaluation criteria. This summary note aims to provide a theoretical guide to researchers (also engineers) interested in understanding 'white-box' machine (deep) learning methods.



### DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10390v2)
- **Published**: 2023-02-21 01:32:27+00:00
- **Updated**: 2023-03-15 15:44:37+00:00
- **Authors**: Ke Yu, Li Sun, Junxiang Chen, Max Reynolds, Tigmanshu Chaudhary, Kayhan Batmanghelich
- **Comment**: Added some recent references
- **Journal**: None
- **Summary**: Large-scale volumetric medical images with annotation are rare, costly, and time prohibitive to acquire. Self-supervised learning (SSL) offers a promising pre-training and feature extraction solution for many downstream tasks, as it only uses unlabeled data. Recently, SSL methods based on instance discrimination have gained popularity in the medical imaging domain. However, SSL pre-trained encoders may use many clues in the image to discriminate an instance that are not necessarily disease-related. Moreover, pathological patterns are often subtle and heterogeneous, requiring the ability of the desired method to represent anatomy-specific features that are sensitive to abnormal changes in different body parts. In this work, we present a novel SSL framework, named DrasCLR, for 3D medical imaging to overcome these challenges. We propose two domain-specific contrastive learning strategies: one aims to capture subtle disease patterns inside a local anatomical region, and the other aims to represent severe disease patterns that span larger regions. We formulate the encoder using conditional hyper-parameterized network, in which the parameters are dependant on the anatomical location, to extract anatomically sensitive features. Extensive experiments on large-scale computer tomography (CT) datasets of lung images show that our method improves the performance of many downstream prediction and segmentation tasks. The patient-level representation improves the performance of the patient survival prediction task. We show how our method can detect emphysema subtypes via dense prediction. We demonstrate that fine-tuning the pre-trained model can significantly reduce annotation efforts without sacrificing emphysema detection accuracy. Our ablation study highlights the importance of incorporating anatomical context into the SSL framework.



### Assessing Domain Gap for Continual Domain Adaptation in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.10396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10396v2)
- **Published**: 2023-02-21 02:07:13+00:00
- **Updated**: 2023-04-03 08:44:14+00:00
- **Authors**: Anh-Dzung Doan, Bach Long Nguyen, Surabhi Gupta, Ian Reid, Markus Wagner, Tat-Jun Chin
- **Comment**: Submitted to CVIU
- **Journal**: None
- **Summary**: To ensure reliable object detection in autonomous systems, the detector must be able to adapt to changes in appearance caused by environmental factors such as time of day, weather, and seasons. Continually adapting the detector to incorporate these changes is a promising solution, but it can be computationally costly. Our proposed approach is to selectively adapt the detector only when necessary, using new data that does not have the same distribution as the current training data. To this end, we investigate three popular metrics for domain gap evaluation and find that there is a correlation between the domain gap and detection accuracy. Therefore, we apply the domain gap as a criterion to decide when to adapt the detector. Our experiments show that our approach has the potential to improve the efficiency of the detector's operation in real-world scenarios, where environmental conditions change in a cyclical manner, without sacrificing the overall performance of the detector. Our code is publicly available at https://github.com/dadung/DGE-CDA.



### Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines
- **Arxiv ID**: http://arxiv.org/abs/2302.10406v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2302.10406v1)
- **Published**: 2023-02-21 02:42:03+00:00
- **Updated**: 2023-02-21 02:42:03+00:00
- **Authors**: Min Cen, Xingyu Li, Bangwei Guo, Jitendra Jonnagaddala, Hong Zhang, Xu Steven Xu
- **Comment**: None
- **Journal**: None
- **Summary**: NLP-based computer vision models, particularly vision transformers, have been shown to outperform CNN models in many imaging tasks. However, most digital pathology artificial-intelligence models are based on CNN architectures, probably owing to a lack of data regarding NLP models for pathology images. In this study, we developed digital pathology pipelines to benchmark the five most recently proposed NLP models (vision transformer (ViT), Swin Transformer, MobileViT, CMT, and Sequencer2D) and four popular CNN models (ResNet18, ResNet50, MobileNetV2, and EfficientNet) to predict biomarkers in colorectal cancer (microsatellite instability, CpG island methylator phenotype, and BRAF mutation). Hematoxylin and eosin-stained whole-slide images from Molecular and Cellular Oncology and The Cancer Genome Atlas were used as training and external validation datasets, respectively. Cross-study external validations revealed that the NLP-based models significantly outperformed the CNN-based models in biomarker prediction tasks, improving the overall prediction and precision up to approximately 10% and 26%, respectively. Notably, compared with existing models in the current literature using large training datasets, our NLP models achieved state-of-the-art predictions for all three biomarkers using a relatively small training dataset, suggesting that large training datasets are not a prerequisite for NLP models or transformers, and NLP may be more suitable for clinical studies in which small training datasets are commonly collected. The superior performance of Sequencer2D suggests that further research and innovation on both transformer and bidirectional long short-term memory architectures are warranted in the field of digital pathology. NLP models can replace classic CNN architectures and become the new workhorse backbone in the field of digital pathology.



### Non-pooling Network for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.10412v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10412v1)
- **Published**: 2023-02-21 02:49:16+00:00
- **Updated**: 2023-02-21 02:49:16+00:00
- **Authors**: Weihu Song, Heng Yu
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Existing studies tend tofocus onmodel modifications and integration with higher accuracy, which improve performance but also carry huge computational costs, resulting in longer detection times. Inmedical imaging, the use of time is extremely sensitive. And at present most of the semantic segmentation models have encoder-decoder structure or double branch structure. Their several times of the pooling use with high-level semantic information extraction operation cause information loss although there si a reverse pooling or other similar action to restore information loss of pooling operation. In addition, we notice that visual attention mechanism has superior performance on a variety of tasks. Given this, this paper proposes non-pooling network(NPNet), non-pooling commendably reduces the loss of information and attention enhancement m o d u l e ( A M ) effectively increases the weight of useful information. The method greatly reduces the number of parametersand computation costs by the shallow neural network structure. We evaluate the semantic segmentation model of our NPNet on three benchmark datasets comparing w i t h multiple current state-of-the-art(SOTA) models, and the implementation results show thatour NPNetachieves SOTA performance, with an excellent balance between accuracyand speed.



### CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization
- **Arxiv ID**: http://arxiv.org/abs/2302.10413v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10413v3)
- **Published**: 2023-02-21 02:53:37+00:00
- **Updated**: 2023-04-15 04:06:52+00:00
- **Authors**: Nang Hung Nguyen, Duc Long Nguyen, Trong Bang Nguyen, Thanh-Hung Nguyen, Huy Hieu Pham, Truong Thao Nguyen, Phi Le Nguyen
- **Comment**: Accepted for presentation at the 23rd International Symposium on
  Cluster, Cloud and Internet Computing (CCGrid 2023)
- **Journal**: None
- **Summary**: Federated learning enables edge devices to train a global model collaboratively without exposing their data. Despite achieving outstanding advantages in computing efficiency and privacy protection, federated learning faces a significant challenge when dealing with non-IID data, i.e., data generated by clients that are typically not independent and identically distributed. In this paper, we tackle a new type of Non-IID data, called cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed non-IID is a phenomenon in which clients can be grouped into clusters with similar data distributions. By performing an in-depth analysis of the behavior of a classification model's penultimate layer, we introduce a metric that quantifies the similarity between two clients' data distributions without violating their privacy. We then propose an aggregation scheme that guarantees equality between clusters. In addition, we offer a novel local training regularization based on the knowledge-distillation technique that reduces the overfitting problem at clients and dramatically boosts the training scheme's performance. We theoretically prove the superiority of the proposed aggregation over the benchmark FedAvg. Extensive experimental results on both standard public datasets and our in-house real-world dataset demonstrate that the proposed approach improves accuracy by up to 16% compared to the FedAvg algorithm.



### Improving Scene Text Image Super-Resolution via Dual Prior Modulation Network
- **Arxiv ID**: http://arxiv.org/abs/2302.10414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10414v1)
- **Published**: 2023-02-21 02:59:37+00:00
- **Updated**: 2023-02-21 02:59:37+00:00
- **Authors**: Shipeng Zhu, Zuoyan Zhao, Pengfei Fang, Hui Xue
- **Comment**: Accepted by AAAI-2023
- **Journal**: None
- **Summary**: Scene text image super-resolution (STISR) aims to simultaneously increase the resolution and legibility of the text images, and the resulting images will significantly affect the performance of downstream tasks. Although numerous progress has been made, existing approaches raise two crucial issues: (1) They neglect the global structure of the text, which bounds the semantic determinism of the scene text. (2) The priors, e.g., text prior or stroke prior, employed in existing works, are extracted from pre-trained text recognizers. That said, such priors suffer from the domain gap including low resolution and blurriness caused by poor imaging conditions, leading to incorrect guidance. Our work addresses these gaps and proposes a plug-and-play module dubbed Dual Prior Modulation Network (DPMN), which leverages dual image-level priors to bring performance gain over existing approaches. Specifically, two types of prior-guided refinement modules, each using the text mask or graphic recognition result of the low-quality SR image from the preceding layer, are designed to improve the structural clarity and semantic accuracy of the text, respectively. The following attention mechanism hence modulates two quality-enhanced images to attain a superior SR result. Extensive experiments validate that our method improves the image quality and boosts the performance of downstream tasks over five typical approaches on the benchmark. Substantial visualizations and ablation studies demonstrate the advantages of the proposed DPMN. Code is available at: https://github.com/jdfxzzy/DPMN.



### 'The Taurus': Cattle Breeds & Diseases Identification Mobile Application using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.10920v1
- **DOI**: 10.31033/ijemr.12.6.27
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10920v1)
- **Published**: 2023-02-21 03:11:11+00:00
- **Updated**: 2023-02-21 03:11:11+00:00
- **Authors**: R. M. D. S. M. Chandrarathna, T. W. M. S. A. Weerasinghe, N. S. Madhuranga, T. M. L. S. Thennakoon, Anjalie Gamage, Erandika Lakmali
- **Comment**: None
- **Journal**: International Journal of Engineering and Management Research, vol
  12, no 6, (December 2022), 198-205
- **Summary**: Dairy farming plays an important role in agriculture for thousands of years not only in Sri Lanka but also in so many other countries. When it comes to dairy farming cattle is an indispensable animal. According to the literature surveys almost 3.9 million cattle and calves die in a year due to different types of diseases. The causes of diseases are mainly bacteria, parasites, fungi, chemical poisons and etc. Infectious diseases can be a greatest threat to livestock health. The mortality rate of cattle causes a huge impact on social, economic and environmental damage. In order to decrease this negative impact, the proposal implements a cross-platform mobile application to easily analyze and identify the diseases which cattle suffer from and give them a solution and also to identify the cattle breeds. The mobile application is designed to identify the breeds by analyzing the images of the cattle and identify diseases after analyzing the videos and the images of affected areas. Then make a model to identify the weight and the age of a particular cow and suggest the best dose of the medicine to the identified disease. This will be a huge advantage to farmers as well as to dairy industry. The name of the proposed mobile application is 'The Taurus' and this paper address the selected machine learning and image processing models and the approaches taken to identify the diseases, breeds and suggest the prevention methods and medicine to the identified disease.



### HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.10420v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10420v2)
- **Published**: 2023-02-21 03:16:22+00:00
- **Updated**: 2023-03-13 11:00:33+00:00
- **Authors**: Chengxi Han, Chen Wu, Bo Du
- **Comment**: None
- **Journal**: None
- **Summary**: Very-high-resolution (VHR) remote sensing (RS) image change detection (CD) has been a challenging task for its very rich spatial information and sample imbalance problem. In this paper, we have proposed a hierarchical change guiding map network (HCGMNet) for change detection. The model uses hierarchical convolution operations to extract multiscale features, continuously merges multi-scale features layer by layer to improve the expression of global and local information, and guides the model to gradually refine edge features and comprehensive performance by a change guide module (CGM), which is a self-attention with changing guide map. Extensive experiments on two CD datasets show that the proposed HCGMNet architecture achieves better CD performance than existing state-of-the-art (SOTA) CD methods.



### Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2302.10425v2
- **DOI**: 10.1109/TCSVT.2023.3289885
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10425v2)
- **Published**: 2023-02-21 03:34:15+00:00
- **Updated**: 2023-08-28 07:06:35+00:00
- **Authors**: Chao Qi, Jianqin Yin, Jinghang Xu, Pengxiang Ding
- **Comment**: Accepted by IEEE TCSVT. The supplementary material is available in
  the media column of the journal version of the article
- **Journal**: None
- **Summary**: This work introduces a new task of instance-incremental scene graph generation: Given a scene of the point cloud, representing it as a graph and automatically increasing novel instances. A graph denoting the object layout of the scene is finally generated. It is an important task since it helps to guide the insertion of novel 3D objects into a real-world scene in vision-based applications like augmented reality. It is also challenging because the complexity of the real-world point cloud brings difficulties in learning object layout experiences from the observation data (non-empty rooms with labeled semantics). We model this task as a conditional generation problem and propose a 3D autoregressive framework based on normalizing flows (3D-ANF) to address it. First, we represent the point cloud as a graph by extracting the label semantics and contextual relationships. Next, a model based on normalizing flows is introduced to map the conditional generation of graphic elements into the Gaussian process. The mapping is invertible. Thus, the real-world experiences represented in the observation data can be modeled in the training phase, and novel instances can be autoregressively generated based on the Gaussian process in the testing phase. To evaluate the performance of our method sufficiently, we implement this new task on the indoor benchmark dataset 3DSSG-O27R16 and our newly proposed graphical dataset of outdoor scenes GPL3D. Experiments show that our method generates reliable novel graphs from the real-world point cloud and achieves state-of-the-art performance on the datasets.



### Interval Type-2 Fuzzy Neural Networks for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.10430v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10430v1)
- **Published**: 2023-02-21 04:00:44+00:00
- **Updated**: 2023-02-21 04:00:44+00:00
- **Authors**: Dayong Tian, Feifei Li, Yiwen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Prediction of multi-dimensional labels plays an important role in machine learning problems. We found that the classical binary labels could not reflect the contents and their relationships in an instance. Hence, we propose a multi-label classification model based on interval type-2 fuzzy logic. In the proposed model, we use a deep neural network to predict the type-1 fuzzy membership of an instance and another one to predict the fuzzifiers of the membership to generate interval type-2 fuzzy memberships. We also propose a loss function to measure the similarities between binary labels in datasets and interval type-2 fuzzy memberships generated by our model. The experiments validate that our approach outperforms baselines on multi-label classification benchmarks.



### Two-in-one Knowledge Distillation for Efficient Facial Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10437v1)
- **Published**: 2023-02-21 04:34:06+00:00
- **Updated**: 2023-02-21 04:34:06+00:00
- **Authors**: Chuyang Zhou, Jiajun Huang, Daochang Liu, Chengbin Du, Siqi Ma, Surya Nepal, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Facial forgery detection is a crucial but extremely challenging topic, with the fast development of forgery techniques making the synthetic artefact highly indistinguishable. Prior works show that by mining both spatial and frequency information the forgery detection performance of deep learning models can be vastly improved. However, leveraging multiple types of information usually requires more than one branch in the neural network, which makes the model heavy and cumbersome. Knowledge distillation, as an important technique for efficient modelling, could be a possible remedy. We find that existing knowledge distillation methods have difficulties distilling a dual-branch model into a single-branch model. More specifically, knowledge distillation on both the spatial and frequency branches has degraded performance than distillation only on the spatial branch. To handle such problem, we propose a novel two-in-one knowledge distillation framework which can smoothly merge the information from a large dual-branch network into a small single-branch network, with the help of different dedicated feature projectors and the gradient homogenization technique. Experimental analysis on two datasets, FaceForensics++ and Celeb-DF, shows that our proposed framework achieves superior performance for facial forgery detection with much fewer parameters.



### Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task
- **Arxiv ID**: http://arxiv.org/abs/2302.10445v1
- **DOI**: 10.1109/SMC53654.2022.9945180
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10445v1)
- **Published**: 2023-02-21 05:21:04+00:00
- **Updated**: 2023-02-21 05:21:04+00:00
- **Authors**: Yuhong Deng, Chongkun Xia, Xueqian Wang, Lipeng Chen
- **Comment**: has been accepted by IEEE International Conference on Systems, Man
  and Cybernetics 2022
- **Journal**: IEEE International Conference on Systems, Man and Cybernetics 2022
  (SMC 2022)
- **Summary**: Rearranging deformable objects is a long-standing challenge in robotic manipulation for the high dimensionality of configuration space and the complex dynamics of deformable objects. We present a novel framework, Graph-Transporter, for goal-conditioned deformable object rearranging tasks. To tackle the challenge of complex configuration space and dynamics, we represent the configuration space of a deformable object with a graph structure and the graph features are encoded by a graph convolution network. Our framework adopts an architecture based on Fully Convolutional Network (FCN) to output pixel-wise pick-and-place actions from only visual input. Extensive experiments have been conducted to validate the effectiveness of the graph representation of deformable object configuration. The experimental results also demonstrate that our framework is effective and general in handling goal-conditioned deformable object rearranging tasks.



### Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information
- **Arxiv ID**: http://arxiv.org/abs/2302.10450v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2302.10450v1)
- **Published**: 2023-02-21 05:32:28+00:00
- **Updated**: 2023-02-21 05:32:28+00:00
- **Authors**: Madhumitha Sakthi, Ahmed Tewfik, Marius Arvinte, Haris Vikalo
- **Comment**: None
- **Journal**: None
- **Summary**: Automotive radar has increasingly attracted attention due to growing interest in autonomous driving technologies. Acquiring situational awareness using multimodal data collected at high sampling rates by various sensing devices including cameras, LiDAR, and radar requires considerable power, memory and compute resources which are often limited at an edge device. In this paper, we present a novel adaptive radar sub-sampling algorithm designed to identify regions that require more detailed/accurate reconstruction based on prior environmental conditions' knowledge, enabling near-optimal performance at considerably lower effective sampling rates. Designed to robustly perform under variable weather conditions, the algorithm was shown on the Oxford raw radar and RADIATE dataset to achieve accurate reconstruction utilizing only 10% of the original samples in good weather and 20% in extreme (snow, fog) weather conditions. A further modification of the algorithm incorporates object motion to enable reliable identification of important regions. This includes monitoring possible future occlusions caused by objects detected in the present frame. Finally, we train a YOLO network on the RADIATE dataset to perform object detection directly on RADAR data and obtain a 6.6% AP50 improvement over the baseline Faster R-CNN network.



### Multimodal Trajectory Prediction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.10463v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10463v1)
- **Published**: 2023-02-21 06:11:08+00:00
- **Updated**: 2023-02-21 06:11:08+00:00
- **Authors**: Renhao Huang, Hao Xue, Maurice Pagnucco, Flora Salim, Yang Song
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction is an important task to support safe and intelligent behaviours in autonomous systems. Many advanced approaches have been proposed over the years with improved spatial and temporal feature extraction. However, human behaviour is naturally multimodal and uncertain: given the past trajectory and surrounding environment information, an agent can have multiple plausible trajectories in the future. To tackle this problem, an essential task named multimodal trajectory prediction (MTP) has recently been studied, which aims to generate a diverse, acceptable and explainable distribution of future predictions for each agent. In this paper, we present the first survey for MTP with our unique taxonomies and comprehensive analysis of frameworks, datasets and evaluation metrics. In addition, we discuss multiple future directions that can help researchers develop novel multimodal trajectory prediction systems.



### A Flexible Multi-view Multi-modal Imaging System for Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2302.10465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10465v1)
- **Published**: 2023-02-21 06:14:05+00:00
- **Updated**: 2023-02-21 06:14:05+00:00
- **Authors**: Meng Zhang, Wenxuan Guo, Bohao Fan, Yifan Chen, Jianjiang Feng, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view imaging systems enable uniform coverage of 3D space and reduce the impact of occlusion, which is beneficial for 3D object detection and tracking accuracy. However, existing imaging systems built with multi-view cameras or depth sensors are limited by the small applicable scene and complicated composition. In this paper, we propose a wireless multi-view multi-modal 3D imaging system generally applicable to large outdoor scenes, which consists of a master node and several slave nodes. Multiple spatially distributed slave nodes equipped with cameras and LiDARs are connected to form a wireless sensor network. While providing flexibility and scalability, the system applies automatic spatio-temporal calibration techniques to obtain accurate 3D multi-view multi-modal data. This system is the first imaging system that integrates mutli-view RGB cameras and LiDARs in large outdoor scenes among existing 3D imaging systems. We perform point clouds based 3D object detection and long-term tracking using the 3D imaging dataset collected by this system. The experimental results show that multi-view point clouds greatly improve 3D object detection and tracking accuracy regardless of complex and various outdoor environments.



### Oriented Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.10473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10473v3)
- **Published**: 2023-02-21 06:31:53+00:00
- **Updated**: 2023-07-08 08:19:18+00:00
- **Authors**: Kun Wang, Zi Wang, Zhang Li, Ang Su, Xichao Teng, Minhao Liu, Qifeng Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Oriented object detection is one of the most fundamental and challenging tasks in remote sensing, aiming at locating the oriented objects of numerous predefined object categories. Recently, deep learning based methods have achieved remarkable performance in detecting oriented objects in optical remote sensing imagery. However, a thorough review of the literature in remote sensing has not yet emerged. Therefore, we give a comprehensive survey of recent advances and cover many aspects of oriented object detection, including problem definition, commonly used datasets, evaluation protocols, detection frameworks, oriented object representations, and feature representations. Besides, the state-of-the-art methods are analyzed and discussed. We finally discuss future research directions to put forward some useful research guidance. We believe that this survey shall be valuable to researchers across academia and industry



### Climate Model Driven Seasonal Forecasting Approach with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.10480v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2302.10480v1)
- **Published**: 2023-02-21 07:00:32+00:00
- **Updated**: 2023-02-21 07:00:32+00:00
- **Authors**: Alper Unal, Busra Asan, Ismail Sezen, Bugra Yesilkaynak, Yusuf Aydin, Mehmet Ilicak, Gozde Unal
- **Comment**: This article is submitted to 12th International Conference on Climate
  Informatics 2023
- **Journal**: None
- **Summary**: Understanding seasonal climatic conditions is critical for better management of resources such as water, energy and agriculture. Recently, there has been a great interest in utilizing the power of artificial intelligence methods in climate studies. This paper presents a cutting-edge deep learning model (UNet++) trained by state-of-the-art global CMIP6 models to forecast global temperatures a month ahead using the ERA5 reanalysis dataset. ERA5 dataset was also used for finetuning as well performance analysis in the validation dataset. Three different setups (CMIP6; CMIP6 + elevation; CMIP6 + elevation + ERA5 finetuning) were used with both UNet and UNet++ algorithms resulting in six different models. For each model 14 different sequential and non-sequential temporal settings were used. The Mean Absolute Error (MAE) analysis revealed that UNet++ with CMIP6 with elevation and ERA5 finetuning model with "Year 3 Month 2" temporal case provided the best outcome with an MAE of 0.7. Regression analysis over the validation dataset between the ERA5 data values and the corresponding AI model predictions revealed slope and $R^2$ values close to 1 suggesting a very good agreement. The AI model predicts significantly better than the mean CMIP6 ensemble between 2016 and 2021. Both models predict the summer months more accurately than the winter months.



### LMPDNet: TOF-PET list-mode image reconstruction using model-based deep learning method
- **Arxiv ID**: http://arxiv.org/abs/2302.10481v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10481v1)
- **Published**: 2023-02-21 07:07:29+00:00
- **Updated**: 2023-02-21 07:07:29+00:00
- **Authors**: Chenxu Li, Rui Hu, Jianan Cui, Huafeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The integration of Time-of-Flight (TOF) information in the reconstruction process of Positron Emission Tomography (PET) yields improved image properties. However, implementing the cutting-edge model-based deep learning methods for TOF-PET reconstruction is challenging due to the substantial memory requirements. In this study, we present a novel model-based deep learning approach, LMPDNet, for TOF-PET reconstruction from list-mode data. We address the issue of real-time parallel computation of the projection matrix for list-mode data, and propose an iterative model-based module that utilizes a dedicated network model for list-mode data. Our experimental results indicate that the proposed LMPDNet outperforms traditional iteration-based TOF-PET list-mode reconstruction algorithms. Additionally, we compare the spatial and temporal consumption of list-mode data and sinogram data in model-based deep learning methods, demonstrating the superiority of list-mode data in model-based TOF-PET reconstruction.



### Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN
- **Arxiv ID**: http://arxiv.org/abs/2302.10484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10484v1)
- **Published**: 2023-02-21 07:16:53+00:00
- **Updated**: 2023-02-21 07:16:53+00:00
- **Authors**: Guoan Xu, Juncheng Li, Guangwei Gao, Huimin Lu, Jian Yang, Dong Yue
- **Comment**: IEEE Transactions on Intelligent Transportation Systems, 10 pages
- **Journal**: None
- **Summary**: In the past decade, convolutional neural networks (CNNs) have shown prominence for semantic segmentation. Although CNN models have very impressive performance, the ability to capture global representation is still insufficient, which results in suboptimal results. Recently, Transformer achieved huge success in NLP tasks, demonstrating its advantages in modeling long-range dependency. Recently, Transformer has also attracted tremendous attention from computer vision researchers who reformulate the image processing tasks as a sequence-to-sequence prediction but resulted in deteriorating local feature details. In this work, we propose a lightweight real-time semantic segmentation network called LETNet. LETNet combines a U-shaped CNN with Transformer effectively in a capsule embedding style to compensate for respective deficiencies. Meanwhile, the elaborately designed Lightweight Dilated Bottleneck (LDB) module and Feature Enhancement (FE) module cultivate a positive impact on training from scratch simultaneously. Extensive experiments performed on challenging datasets demonstrate that LETNet achieves superior performances in accuracy and efficiency balance. Specifically, It only contains 0.95M parameters and 13.6G FLOPs but yields 72.8\% mIoU at 120 FPS on the Cityscapes test set and 70.5\% mIoU at 250 FPS on the CamVid test dataset using a single RTX 3090 GPU. The source code will be available at https://github.com/IVIPLab/LETNet.



### MaskedKD: Efficient Distillation of Vision Transformers with Masked Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10494v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10494v2)
- **Published**: 2023-02-21 07:48:34+00:00
- **Updated**: 2023-05-31 04:50:46+00:00
- **Authors**: Seungwoo Son, Namhoon Lee, Jaeho Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model, which is already computed during the student forward pass, and thus incurs almost no additional computation. Without sacrificing the final student accuracy, MaskedKD dramatically reduces the amount of computations required for distilling ViTs. We demonstrate that MaskedKD can save up the distillation cost by $50\%$ without any student performance drop, leading to approximately $28\%$ drop in the overall training FLOPs.



### Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention
- **Arxiv ID**: http://arxiv.org/abs/2302.10501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10501v1)
- **Published**: 2023-02-21 07:59:31+00:00
- **Updated**: 2023-02-21 07:59:31+00:00
- **Authors**: Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: This paper presents an effective few-shot point cloud semantic segmentation approach for real-world applications. Existing few-shot segmentation methods on point cloud heavily rely on the fully-supervised pretrain with large annotated datasets, which causes the learned feature extraction bias to those pretrained classes. However, as the purpose of few-shot learning is to handle unknown/unseen classes, such class-specific feature extraction in pretrain is not ideal to generalize into new classes for few-shot learning. Moreover, point cloud datasets hardly have a large number of classes due to the annotation difficulty. To address these issues, we propose a contrastive self-supervision framework for few-shot learning pretrain, which aims to eliminate the feature extraction bias through class-agnostic contrastive supervision. Specifically, we implement a novel contrastive learning approach with a learnable augmentor for a 3D point cloud to achieve point-wise differentiation, so that to enhance the pretrain with managed overfitting through the self-supervision. Furthermore, we develop a multi-resolution attention module using both the nearest and farthest points to extract the local and global point information more effectively, and a center-concentrated multi-prototype is adopted to mitigate the intra-class sparsity. Comprehensive experiments are conducted to evaluate the proposed approach, which shows our approach achieves state-of-the-art performance. Moreover, a case study on practical CAM/CAD segmentation is presented to demonstrate the effectiveness of our approach for real-world applications.



### Learning Gradually Non-convex Image Priors Using Score Matching
- **Arxiv ID**: http://arxiv.org/abs/2302.10502v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2302.10502v1)
- **Published**: 2023-02-21 08:02:03+00:00
- **Updated**: 2023-02-21 08:02:03+00:00
- **Authors**: Erich Kobler, Thomas Pock
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we propose a unified framework of denoising score-based models in the context of graduated non-convex energy minimization. We show that for sufficiently large noise variance, the associated negative log density -- the energy -- becomes convex. Consequently, denoising score-based models essentially follow a graduated non-convexity heuristic. We apply this framework to learning generalized Fields of Experts image priors that approximate the joint density of noisy images and their associated variances. These priors can be easily incorporated into existing optimization algorithms for solving inverse problems and naturally implement a fast and robust graduated non-convexity mechanism.



### MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion
- **Arxiv ID**: http://arxiv.org/abs/2302.10511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10511v1)
- **Published**: 2023-02-21 08:25:50+00:00
- **Updated**: 2023-02-21 08:25:50+00:00
- **Authors**: Zizhang Wu, Guilian Chen, Yuanzhu Gan, Lei Wang, Jian Pu
- **Comment**: Accepted by ICRA 2023
- **Journal**: None
- **Summary**: Multi-view radar-camera fused 3D object detection provides a farther detection range and more helpful features for autonomous driving, especially under adverse weather. The current radar-camera fusion methods deliver kinds of designs to fuse radar information with camera data. However, these fusion approaches usually adopt the straightforward concatenation operation between multi-modal features, which ignores the semantic alignment with radar features and sufficient correlations across modals. In this paper, we present MVFusion, a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar features and enhance the cross-modal information interaction. To achieve so, we inject the semantic alignment into the radar features via the semantic-aligned radar encoder (SARE) to produce image-guided radar features. Then, we propose the radar-guided fusion transformer (RGFT) to fuse our radar and image features to strengthen the two modals' correlation from the global scope via the cross-attention mechanism. Extensive experiments show that MVFusion achieves state-of-the-art performance (51.7% NDS and 45.3% mAP) on the nuScenes dataset. We shall release our code and trained networks upon publication.



### USR: Unsupervised Separated 3D Garment and Human Reconstruction via Geometry and Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2302.10518v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10518v3)
- **Published**: 2023-02-21 08:48:27+00:00
- **Updated**: 2023-03-02 14:06:01+00:00
- **Authors**: Yue Shi, Yuxuan Xiong, Jingyi Chai, Bingbing Ni, Wenjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Dressed people reconstruction from images is a popular task with promising applications in the creative media and game industry. However, most existing methods reconstruct the human body and garments as a whole with the supervision of 3D models, which hinders the downstream interaction tasks and requires hard-to-obtain data. To address these issues, we propose an unsupervised separated 3D garments and human reconstruction model (USR), which reconstructs the human body and authentic textured clothes in layers without 3D models. More specifically, our method proposes a generalized surface-aware neural radiance field to learn the mapping between sparse multi-view images and geometries of the dressed people. Based on the full geometry, we introduce a Semantic and Confidence Guided Separation strategy (SCGS) to detect, segment, and reconstruct the clothes layer, leveraging the consistency between 2D semantic and 3D geometry. Moreover, we propose a Geometry Fine-tune Module to smooth edges. Extensive experiments on our dataset show that comparing with state-of-the-art methods, USR achieves improvements on both geometry and appearance reconstruction while supporting generalizing to unseen people in real time. Besides, we also introduce SMPL-D model to show the benefit of the separated modeling of clothes and the human body that allows swapping clothes and virtual try-on.



### PointFISH -- learning point cloud representations for RNA localization patterns
- **Arxiv ID**: http://arxiv.org/abs/2302.10923v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10923v1)
- **Published**: 2023-02-21 08:50:41+00:00
- **Updated**: 2023-02-21 08:50:41+00:00
- **Authors**: Arthur Imbert, Florian Mueller, Thomas Walter
- **Comment**: None
- **Journal**: None
- **Summary**: Subcellular RNA localization is a critical mechanism for the spatial control of gene expression. Its mechanism and precise functional role is not yet very well understood. Single Molecule Fluorescence in Situ Hybridization (smFISH) images allow for the detection of individual RNA molecules with subcellular accuracy. In return, smFISH requires robust methods to quantify and classify RNA spatial distribution. Here, we present PointFISH, a novel computational approach for the recognition of RNA localization patterns. PointFISH is an attention-based network for computing continuous vector representations of RNA point clouds. Trained on simulations only, it can directly process extracted coordinates from experimental smFISH images. The resulting embedding allows scalable and flexible spatial transcriptomics analysis and matches performance of hand-crafted pipelines.



### I2V: Towards Texture-Aware Self-Supervised Blind Denoising using Self-Residual Learning for Real-World Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10523v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10523v1)
- **Published**: 2023-02-21 08:51:17+00:00
- **Updated**: 2023-02-21 08:51:17+00:00
- **Authors**: Kanggeun Lee, Kyungryun Lee, Won-Ki Jeong
- **Comment**: 23 pages, 17 figures, 7 tables
- **Journal**: None
- **Summary**: Although the advances of self-supervised blind denoising are significantly superior to conventional approaches without clean supervision in synthetic noise scenarios, it shows poor quality in real-world images due to spatially correlated noise corruption. Recently, pixel-shuffle downsampling (PD) has been proposed to eliminate the spatial correlation of noise. A study combining a blind spot network (BSN) and asymmetric PD (AP) successfully demonstrated that self-supervised blind denoising is applicable to real-world noisy images. However, PD-based inference may degrade texture details in the testing phase because high-frequency details (e.g., edges) are destroyed in the downsampled images. To avoid such an issue, we propose self-residual learning without the PD process to maintain texture information. We also propose an order-variant PD constraint, noise prior loss, and an efficient inference scheme (progressive random-replacing refinement ($\text{PR}^3$)) to boost overall performance. The results of extensive experiments show that the proposed method outperforms state-of-the-art self-supervised blind denoising approaches, including several supervised learning methods, in terms of PSNR, SSIM, LPIPS, and DISTS in real-world sRGB images.



### EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10544v2
- **DOI**: 10.1109/TCSVT.2023.3285479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10544v2)
- **Published**: 2023-02-21 09:18:57+00:00
- **Updated**: 2023-08-06 09:51:02+00:00
- **Authors**: Zhichao Ye, Chong Bao, Xin Zhou, Haomin Liu, Hujun Bao, Guofeng Zhang
- **Comment**: Accepted 27 May 2023 (TCSVT)
- **Journal**: None
- **Summary**: Structure-from-Motion is a technology used to obtain scene structure through image collection, which is a fundamental problem in computer vision. For unordered Internet images, SfM is very slow due to the lack of prior knowledge about image overlap. For sequential images, knowing the large overlap between adjacent frames, SfM can adopt a variety of acceleration strategies, which are only applicable to sequential data. To further improve the reconstruction efficiency and break the gap of strategies between these two kinds of data, this paper presents an efficient covisibility-based incremental SfM. Different from previous methods, we exploit covisibility and registration dependency to describe the image connection which is suitable to any kind of data. Based on this general image connection, we propose a unified framework to efficiently reconstruct sequential images, unordered images, and the mixture of these two. Experiments on the unordered images and mixed data verify the effectiveness of the proposed method, which is three times faster than the state of the art on feature matching, and an order of magnitude faster on reconstruction without sacrificing the accuracy. The source code is publicly available at https://github.com/openxrlab/xrsfm



### MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts
- **Arxiv ID**: http://arxiv.org/abs/2302.10549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10549v1)
- **Published**: 2023-02-21 09:21:58+00:00
- **Updated**: 2023-02-21 09:21:58+00:00
- **Authors**: Zizhang Wu, Yuanzhu Gan, Lei Wang, Guilian Chen, Jian Pu
- **Comment**: Accepted by ICRA 2023
- **Journal**: None
- **Summary**: Monocular 3D object detection reveals an economical but challenging task in autonomous driving. Recently center-based monocular methods have developed rapidly with a great trade-off between speed and accuracy, where they usually depend on the object center's depth estimation via 2D features. However, the visual semantic features without sufficient pixel geometry information, may affect the performance of clues for spatial 3D detection tasks. To alleviate this, we propose MonoPGC, a novel end-to-end Monocular 3D object detection framework with rich Pixel Geometry Contexts. We introduce the pixel depth estimation as our auxiliary task and design depth cross-attention pyramid module (DCPM) to inject local and global depth geometry knowledge into visual features. In addition, we present the depth-space-aware transformer (DSAT) to integrate 3D space position and depth-aware features efficiently. Besides, we design a novel depth-gradient positional encoding (DGPE) to bring more distinct pixel geometry contexts into the transformer for better object detection. Extensive experiments demonstrate that our method achieves the state-of-the-art performance on the KITTI dataset.



### MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.10574v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.10574v3)
- **Published**: 2023-02-21 10:00:58+00:00
- **Updated**: 2023-03-30 08:51:05+00:00
- **Authors**: Weiqin Zhao, Shujun Wang, Maximus Yeung, Tianye Niu, Lequan Yu
- **Comment**: AAAI 2023
- **Journal**: None
- **Summary**: Whole slide image (WSI) has been widely used to assist automated diagnosis under the deep learning fields. However, most previous works only discuss the SINGLE task setting which is not aligned with real clinical setting, where pathologists often conduct multiple diagnosis tasks simultaneously. Also, it is commonly recognized that the multi-task learning paradigm can improve learning efficiency by exploiting commonalities and differences across multiple tasks. To this end, we present a novel multi-task framework (i.e., MulGT) for WSI analysis by the specially designed Graph-Transformer equipped with Task-aware Knowledge Injection and Domain Knowledge-driven Graph Pooling modules. Basically, with the Graph Neural Network and Transformer as the building commons, our framework is able to learn task-agnostic low-level local information as well as task-specific high-level global representation. Considering that different tasks in WSI analysis depend on different features and properties, we also design a novel Task-aware Knowledge Injection module to transfer the task-shared graph embedding into task-specific feature spaces to learn more accurate representation for different tasks. Further, we elaborately design a novel Domain Knowledge-driven Graph Pooling module for each task to improve both the accuracy and robustness of different tasks by leveraging different diagnosis patterns of multiple tasks. We evaluated our method on two public WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney carcinoma. Experimental results show that our method outperforms single-task counterparts and the state-of-theart methods on both tumor typing and staging tasks.



### Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels
- **Arxiv ID**: http://arxiv.org/abs/2302.10586v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10586v2)
- **Published**: 2023-02-21 10:24:53+00:00
- **Updated**: 2023-06-06 11:53:03+00:00
- **Authors**: Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion can generate realistic images with only a few labels (e.g., <0.1%) and generative augmentation remains viable for semi-supervised classification.



### Texturize a GAN Using a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2302.10600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10600v2)
- **Published**: 2023-02-21 11:07:58+00:00
- **Updated**: 2023-03-11 12:12:06+00:00
- **Authors**: Pengda Xiang, Sitao Xiang, Yajie Zhao
- **Comment**: This paper is still in the process of improving, a revised version
  may be released in the future
- **Journal**: None
- **Summary**: Can we customize a deep generative model which can generate images that can match the texture of some given image? When you see an image of a church, you may wonder if you can get similar pictures for that church. Here we present a method, for adapting GANs with one reference image, and then we can generate images that have similar textures to the given image. Specifically, we modify the weights of the pre-trained GAN model, guided by the reference image given by the user. We use a patch discriminator adversarial loss to encourage the output of the model to match the texture on the given image, also we use a laplacian adversarial loss to ensure diversity and realism, and alleviate the contradiction between the two losses. Experiments show that the proposed method can make the outputs of GANs match the texture of the given image as well as keep diversity and realism.



### SU-Net: Pose estimation network for non-cooperative spacecraft on-orbit
- **Arxiv ID**: http://arxiv.org/abs/2302.10602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10602v2)
- **Published**: 2023-02-21 11:14:01+00:00
- **Updated**: 2023-03-28 09:32:24+00:00
- **Authors**: Hu Gao, Zhihui Li, Depeng Dang, Ning Wang, Jingfan Yang
- **Comment**: We need to overhaul the paper and innovate
- **Journal**: None
- **Summary**: Spacecraft pose estimation plays a vital role in many on-orbit space missions, such as rendezvous and docking, debris removal, and on-orbit maintenance. At present, space images contain widely varying lighting conditions, high contrast and low resolution, pose estimation of space objects is more challenging than that of objects on earth. In this paper, we analyzing the radar image characteristics of spacecraft on-orbit, then propose a new deep learning neural Network structure named Dense Residual U-shaped Network (DR-U-Net) to extract image features. We further introduce a novel neural network based on DR-U-Net, namely Spacecraft U-shaped Network (SU-Net) to achieve end-to-end pose estimation for non-cooperative spacecraft. Specifically, the SU-Net first preprocess the image of non-cooperative spacecraft, then transfer learning was used for pre-training. Subsequently, in order to solve the problem of radar image blur and low ability of spacecraft contour recognition, we add residual connection and dense connection to the backbone network U-Net, and we named it DR-U-Net. In this way, the feature loss and the complexity of the model is reduced, and the degradation of deep neural network during training is avoided. Finally, a layer of feedforward neural network is used for pose estimation of non-cooperative spacecraft on-orbit. Experiments prove that the proposed method does not rely on the hand-made object specific features, and the model has robust robustness, and the calculation accuracy outperforms the state-of-the-art pose estimation methods. The absolute error is 0.1557 to 0.4491 , the mean error is about 0.302 , and the standard deviation is about 0.065 .



### Self-improving object detection via disagreement reconciliation
- **Arxiv ID**: http://arxiv.org/abs/2302.10624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10624v1)
- **Published**: 2023-02-21 12:20:46+00:00
- **Updated**: 2023-02-21 12:20:46+00:00
- **Authors**: Gianluca Scarpellini, Stefano Rosa, Pietro Morerio, Lorenzo Natale, Alessio Del Bue
- **Comment**: This article is a conference paper related to arXiv:2302.03566 and is
  currently under review
- **Journal**: None
- **Summary**: Object detectors often experience a drop in performance when new environmental conditions are insufficiently represented in the training data. This paper studies how to automatically fine-tune a pre-existing object detector while exploring and acquiring images in a new environment without relying on human intervention, i.e., in a self-supervised fashion. In our setting, an agent initially explores the environment using a pre-trained off-the-shelf detector to locate objects and associate pseudo-labels. By assuming that pseudo-labels for the same object must be consistent across different views, we devise a novel mechanism for producing refined predictions from the consensus among observations. Our approach improves the off-the-shelf object detector by 2.66% in terms of mAP and outperforms the current state of the art without relying on ground-truth annotations.



### LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2302.10630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2302.10630v1)
- **Published**: 2023-02-21 12:43:42+00:00
- **Updated**: 2023-02-21 12:43:42+00:00
- **Authors**: Zhihao Chen, Chuang Niu, Ge Wang, Hongming Shan
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: This paper studies 3D low-dose computed tomography (CT) imaging. Although various deep learning methods were developed in this context, typically they perform denoising due to low-dose and deblurring for super-resolution separately. Up to date, little work was done for simultaneous in-plane denoising and through-plane deblurring, which is important to improve clinical CT images. For this task, a straightforward method is to directly train an end-to-end 3D network. However, it demands much more training data and expensive computational costs. Here, we propose to link in-plane and through-plane transformers for simultaneous in-plane denoising and through-plane deblurring, termed as LIT-Former, which can efficiently synergize in-plane and through-plane sub-tasks for 3D CT imaging and enjoy the advantages of both convolution and transformer networks. LIT-Former has two novel designs: efficient multi-head self-attention modules (eMSM) and efficient convolutional feed-forward networks (eCFN). First, eMSM integrates in-plane 2D self-attention and through-plane 1D self-attention to efficiently capture global interactions of 3D self-attention, the core unit of transformer networks. Second, eCFN integrates 2D convolution and 1D convolution to extract local information of 3D convolution in the same fashion. As a result, the proposed LIT-Former synergizes these two sub-tasks, significantly reducing the computational complexity as compared to 3D counterparts and enabling rapid convergence. Extensive experimental results on simulated and clinical datasets demonstrate superior performance over state-of-the-art models.



### A Deep Learning-Based and Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis from 3D Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2302.10634v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2302.10634v1)
- **Published**: 2023-02-21 12:48:44+00:00
- **Updated**: 2023-02-21 12:48:44+00:00
- **Authors**: Riccardo Munafò, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta
- **Comment**: None
- **Journal**: None
- **Summary**: 3D transesophageal echocardiography (3DTEE), is the recommended method for diagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of the mitral valve (MV), allowing for precise segmentation and measurement of the regurgitant valve anatomy. However, manual TEE segmentations are time-consuming and prone to intra-operator variability, affecting the reliability of the measurements. To address this, we developed a fully automated pipeline using a 3D convolutional neural network (CNN) to segment MV substructures (annulus, anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN, based on a multi-decoder residual U-Net architecture, was trained and tested on a dataset comprising 100 3DTEE images with corresponding segmentations. Within the pipeline, a custom algorithm refines the CNN-based segmentations and extracts MV models, from which anatomical landmarks and features are quantified. The accuracy of the proposed method was assessed using Dice score and mean surface distance (MSD) against ground truth segmentations, and the extracted anatomical parameters were compared against a semiautomated commercial software TomTec Image Arena. The trained 3D CNN achieved an average Dice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the annulus, anterior and posterior leaflet. The proposed CNN architecture outperformed a baseline residual U-Net architecture in MV substructure segmentation, and the refinement of the predicted annulus segmentation improved MSD by 8.36%. The annular and leaflet linear measurements differed by less than 7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained with TomTec Image Arena. The proposed pipeline was faster than the commercial software, with a modeling time of 12.54 s and a quantification time of 54.42 s.



### Semantic Segmentation of Urban Textured Meshes Through Point Sampling
- **Arxiv ID**: http://arxiv.org/abs/2302.10635v1
- **DOI**: 10.5194/isprs-annals-V-2-2022-177-2022
- **Categories**: **cs.CV**, 68T10 (Primary) 68T20, 68T07, I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.10635v1)
- **Published**: 2023-02-21 12:49:31+00:00
- **Updated**: 2023-02-21 12:49:31+00:00
- **Authors**: Grégoire Grzeczkowicz, Bruno Vallet
- **Comment**: 9 pages, 6 figures, conference, presented at XXIV ISPRS Congress
- **Journal**: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
  Information Sciences, V-2-2022, 2022, 177-184
- **Summary**: Textured meshes are becoming an increasingly popular representation combining the 3D geometry and radiometry of real scenes. However, semantic segmentation algorithms for urban mesh have been little investigated and do not exploit all radiometric information. To address this problem, we adopt an approach consisting in sampling a point cloud from the textured mesh, then using a point cloud semantic segmentation algorithm on this cloud, and finally using the obtained semantic to segment the initial mesh. In this paper, we study the influence of different parameters such as the sampling method, the density of the extracted cloud, the features selected (color, normal, elevation) as well as the number of points used at each training period. Our result outperforms the state-of-the-art on the SUM dataset, earning about 4 points in OA and 18 points in mIoU.



### A3S: Adversarial learning of semantic representations for Scene-Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2302.10641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10641v1)
- **Published**: 2023-02-21 12:59:18+00:00
- **Updated**: 2023-02-21 12:59:18+00:00
- **Authors**: Masato Fujitake
- **Comment**: Accepted to ICASSP 2023
- **Journal**: None
- **Summary**: Scene-text spotting is a task that predicts a text area on natural scene images and recognizes its text characters simultaneously. It has attracted much attention in recent years due to its wide applications. Existing research has mainly focused on improving text region detection, not text recognition. Thus, while detection accuracy is improved, the end-to-end accuracy is insufficient. Texts in natural scene images tend to not be a random string of characters but a meaningful string of characters, a word. Therefore, we propose adversarial learning of semantic representations for scene text spotting (A3S) to improve end-to-end accuracy, including text recognition. A3S simultaneously predicts semantic features in the detected text area instead of only performing text recognition based on existing visual features. Experimental results on publicly available datasets show that the proposed method achieves better accuracy than other methods.



### BrackishMOT: The Brackish Multi-Object Tracking Dataset
- **Arxiv ID**: http://arxiv.org/abs/2302.10645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10645v1)
- **Published**: 2023-02-21 13:02:36+00:00
- **Updated**: 2023-02-21 13:02:36+00:00
- **Authors**: Malte Pedersen, Daniel Lehotský, Ivan Nikolov, Thomas B. Moeslund
- **Comment**: None
- **Journal**: None
- **Summary**: There exist no publicly available annotated underwater multi-object tracking (MOT) datasets captured in turbid environments. To remedy this we propose the BrackishMOT dataset with focus on tracking schools of small fish, which is a notoriously difficult MOT task. BrackishMOT consists of 98 sequences captured in the wild. Alongside the novel dataset, we present baseline results by training a state-of-the-art tracker. Additionally, we propose a framework for creating synthetic sequences in order to expand the dataset. The framework consists of animated fish models and realistic underwater environments. We analyse the effects of including synthetic data during training and show that a combination of real and synthetic underwater training data can enhance tracking performance. Links to code and data can be found at https://www.vap.aau.dk/brackishmot



### Clinically Acceptable Segmentation of Organs at Risk in Cervical Cancer Radiation Treatment from Clinically Available Annotations
- **Arxiv ID**: http://arxiv.org/abs/2302.10661v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10661v1)
- **Published**: 2023-02-21 13:24:40+00:00
- **Updated**: 2023-02-21 13:24:40+00:00
- **Authors**: Monika Grewal, Dustin van Weersel, Henrike Westerveld, Peter A. N. Bosman, Tanja Alderliesten
- **Comment**: submitted to MIDL 2023 conference
- **Journal**: None
- **Summary**: Deep learning models benefit from training with a large dataset (labeled or unlabeled). Following this motivation, we present an approach to learn a deep learning model for the automatic segmentation of Organs at Risk (OARs) in cervical cancer radiation treatment from a large clinically available dataset of Computed Tomography (CT) scans containing data inhomogeneity, label noise, and missing annotations. We employ simple heuristics for automatic data cleaning to minimize data inhomogeneity and label noise. Further, we develop a semi-supervised learning approach utilizing a teacher-student setup, annotation imputation, and uncertainty-guided training to learn in presence of missing annotations. Our experimental results show that learning from a large dataset with our approach yields a significant improvement in the test performance despite missing annotations in the data. Further, the contours generated from the segmentation masks predicted by our model are found to be equally clinically acceptable as manually generated contours.



### RealFusion: 360° Reconstruction of Any Object from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2302.10663v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10663v2)
- **Published**: 2023-02-21 13:25:35+00:00
- **Updated**: 2023-02-23 15:18:23+00:00
- **Authors**: Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi
- **Comment**: Project page: https://lukemelas.github.io/realfusion
- **Journal**: None
- **Summary**: We consider the problem of reconstructing a full 360{\deg} photographic model of an object from a single image of it. We do so by fitting a neural radiance field to the image, but find this problem to be severely ill-posed. We thus take an off-the-self conditional image generator based on diffusion and engineer a prompt that encourages it to "dream up" novel views of the object. Using an approach inspired by DreamFields and DreamFusion, we fuse the given input view, the conditional prior, and other regularizers in a final, consistent reconstruction. We demonstrate state-of-the-art reconstruction results on benchmark images when compared to prior methods for monocular 3D reconstruction of objects. Qualitatively, our reconstructions provide a faithful match of the input view and a plausible extrapolation of its appearance and 3D shape, including to the side of the object not visible in the image.



### $PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2302.10668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10668v2)
- **Published**: 2023-02-21 13:37:07+00:00
- **Updated**: 2023-02-23 16:03:05+00:00
- **Authors**: Luke Melas-Kyriazi, Christian Rupprecht, Andrea Vedaldi
- **Comment**: Project page:
  https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion
- **Journal**: None
- **Summary**: Reconstructing the 3D shape of an object from a single RGB image is a long-standing and highly challenging problem in computer vision. In this paper, we propose a novel method for single-image 3D reconstruction which generates a sparse point cloud via a conditional denoising diffusion process. Our method takes as input a single RGB image along with its camera pose and gradually denoises a set of 3D points, whose positions are initially sampled randomly from a three-dimensional Gaussian distribution, into the shape of an object. The key to our method is a geometrically-consistent conditioning process which we call projection conditioning: at each step in the diffusion process, we project local image features onto the partially-denoised point cloud from the given camera pose. This projection conditioning process enables us to generate high-resolution sparse geometries that are well-aligned with the input image, and can additionally be used to predict point colors after shape reconstruction. Moreover, due to the probabilistic nature of the diffusion process, our method is naturally capable of generating multiple different shapes consistent with a single input image. In contrast to prior work, our approach not only performs well on synthetic benchmarks, but also gives large qualitative improvements on complex real-world data.



### Evaluating the effect of data augmentation and BALD heuristics on distillation of Semantic-KITTI dataset
- **Arxiv ID**: http://arxiv.org/abs/2302.10679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10679v1)
- **Published**: 2023-02-21 13:56:47+00:00
- **Updated**: 2023-02-21 13:56:47+00:00
- **Authors**: Anh Duong, Alexandre Almin, Léo Lemarié, B Ravi Kiran
- **Comment**: Submitted to VISAPP Springer book extension. arXiv admin note:
  substantial text overlap with arXiv:2202.02661
- **Journal**: None
- **Summary**: Active Learning (AL) has remained relatively unexplored for LiDAR perception tasks in autonomous driving datasets. In this study we evaluate Bayesian active learning methods applied to the task of dataset distillation or core subset selection (subset with near equivalent performance as full dataset). We also study the effect of application of data augmentation (DA) within Bayesian AL based dataset distillation. We perform these experiments on the full Semantic-KITTI dataset. We extend our study over our existing work only on 1/4th of the same dataset. Addition of DA and BALD have a negative impact over the labeling efficiency and thus the capacity to distill datasets. We demonstrate key issues in designing a functional AL framework and finally conclude with a review of challenges in real world active learning.



### Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes
- **Arxiv ID**: http://arxiv.org/abs/2302.10685v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10685v1)
- **Published**: 2023-02-21 14:10:56+00:00
- **Updated**: 2023-02-21 14:10:56+00:00
- **Authors**: Zecheng Hao, Jianhao Ding, Tong Bu, Tiejun Huang, Zhaofei Yu
- **Comment**: Accepted by ICLR 2023
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have attracted great attention due to their distinctive characteristics of low power consumption and temporal information processing. ANN-SNN conversion, as the most commonly used training method for applying SNNs, can ensure that converted SNNs achieve comparable performance to ANNs on large-scale datasets. However, the performance degrades severely under low quantities of time-steps, which hampers the practical applications of SNNs to neuromorphic chips. In this paper, instead of evaluating different conversion errors and then eliminating these errors, we define an offset spike to measure the degree of deviation between actual and desired SNN firing rates. We perform a detailed analysis of offset spike and note that the firing of one additional (or one less) spike is the main cause of conversion errors. Based on this, we propose an optimization strategy based on shifting the initial membrane potential and we theoretically prove the corresponding optimal shifting distance for calibrating the spike. In addition, we also note that our method has a unique iterative property that enables further reduction of conversion errors. The experimental results show that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets. For example, we reach a top-1 accuracy of 67.12% on ImageNet when using 6 time-steps. To the best of our knowledge, this is the first time an ANN-SNN conversion has been shown to simultaneously achieve high accuracy and ultralow latency on complex datasets. Code is available at https://github.com/hzc1208/ANN2SNN_COS.



### On Calibrating Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2302.10688v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.10688v2)
- **Published**: 2023-02-21 14:14:40+00:00
- **Updated**: 2023-05-26 12:55:29+00:00
- **Authors**: Tianyu Pang, Cheng Lu, Chao Du, Min Lin, Shuicheng Yan, Zhijie Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Calibrated-DPMs.



### A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.10697v2
- **DOI**: 10.1109/TCSVT.2023.3284076
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10697v2)
- **Published**: 2023-02-21 14:31:57+00:00
- **Updated**: 2023-06-09 01:30:00+00:00
- **Authors**: Binwei Xu, Haoran Liang, Weihua Gong, Ronghua Liang, Peng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Fully supervised salient object detection (SOD) methods have made considerable progress in performance, yet these models rely heavily on expensive pixel-wise labels. Recently, to achieve a trade-off between labeling burden and performance, scribble-based SOD methods have attracted increasing attention. Previous scribble-based models directly implement the SOD task only based on SOD training data with limited information, it is extremely difficult for them to understand the image and further achieve a superior SOD task. In this paper, we propose a simple yet effective framework guided by general visual representations with rich contextual semantic knowledge for scribble-based SOD. These general visual representations are generated by self-supervised learning based on large-scale unlabeled datasets. Our framework consists of a task-related encoder, a general visual module, and an information integration module to efficiently combine the general visual representations with task-related features to perform the SOD task based on understanding the contextual connections of images. Meanwhile, we propose a novel global semantic affinity loss to guide the model to perceive the global structure of the salient objects. Experimental results on five public benchmark datasets demonstrate that our method, which only utilizes scribble annotations without introducing any extra label, outperforms the state-of-the-art weakly supervised SOD methods. Specifically, it outperforms the previous best scribble-based method on all datasets with an average gain of 5.5% for max f-measure, 5.8% for mean f-measure, 24% for MAE, and 3.1% for E-measure. Moreover, our method achieves comparable or even superior performance to the state-of-the-art fully supervised models.



### Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations
- **Arxiv ID**: http://arxiv.org/abs/2302.10698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10698v1)
- **Published**: 2023-02-21 14:36:18+00:00
- **Updated**: 2023-02-21 14:36:18+00:00
- **Authors**: Lin Zhang, Tiziano Portenier, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Photorealistic image generation from simulated label maps are necessitated in several contexts, such as for medical training in virtual reality. With conventional deep learning methods, this task requires images that are paired with semantic annotations, which typically are unavailable. We introduce a contrastive learning framework for generating photorealistic images from simulated label maps, by learning from unpaired sets of both. Due to potentially large scene differences between real images and label maps, existing unpaired image translation methods lead to artifacts of scene modification in synthesized images. We utilize simulated images as surrogate targets for a contrastive loss, while ensuring consistency by utilizing features from a reverse translation network. Our method enables bidirectional label-image translations, which is demonstrated in a variety of scenarios and datasets, including laparoscopy, ultrasound, and driving scenes. By comparing with state-of-the-art unpaired translation methods, our proposed method is shown to generate realistic and scene-accurate translations.



### Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment
- **Arxiv ID**: http://arxiv.org/abs/2302.10717v1
- **DOI**: 10.1109/IROS40897.2019.8967899
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10717v1)
- **Published**: 2023-02-21 15:10:35+00:00
- **Updated**: 2023-02-21 15:10:35+00:00
- **Authors**: Yuhong Deng, Xiaofeng Guo, Yixuan Wei, Kai Lu, Bin Fang, Di Guo, Huaping Liu, Fuchun Sun
- **Comment**: has been accepted by IEEE/RSJ International Conference on Intelligent
  Robots and Systems 2019
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems 2019 (IROS 2019)
- **Summary**: In this paper, a novel robotic grasping system is established to automatically pick up objects in cluttered scenes. A composite robotic hand composed of a suction cup and a gripper is designed for grasping the object stably. The suction cup is used for lifting the object from the clutter first and the gripper for grasping the object accordingly. We utilize the affordance map to provide pixel-wise lifting point candidates for the suction cup. To obtain a good affordance map, the active exploration mechanism is introduced to the system. An effective metric is designed to calculate the reward for the current affordance map, and a deep Q-Network (DQN) is employed to guide the robotic hand to actively explore the environment until the generated affordance map is suitable for grasping. Experimental results have demonstrated that the proposed robotic grasping system is able to greatly increase the success rate of the robotic grasping in cluttered scenes.



### Effects of Architectures on Continual Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.10718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10718v1)
- **Published**: 2023-02-21 15:12:01+00:00
- **Updated**: 2023-02-21 15:12:01+00:00
- **Authors**: Tobias Kalb, Niket Ahuja, Jingxing Zhou, Jürgen Beyerer
- **Comment**: Currently under Review
- **Journal**: None
- **Summary**: Research in the field of Continual Semantic Segmentation is mainly investigating novel learning algorithms to overcome catastrophic forgetting of neural networks. Most recent publications have focused on improving learning algorithms without distinguishing effects caused by the choice of neural architecture.Therefore, we study how the choice of neural network architecture affects catastrophic forgetting in class- and domain-incremental semantic segmentation. Specifically, we compare the well-researched CNNs to recently proposed Transformers and Hybrid architectures, as well as the impact of the choice of novel normalization layers and different decoder heads. We find that traditional CNNs like ResNet have high plasticity but low stability, while transformer architectures are much more stable. When the inductive biases of CNN architectures are combined with transformers in hybrid architectures, it leads to higher plasticity and stability. The stability of these models can be explained by their ability to learn general features that are robust against distribution shifts. Experiments with different normalization layers show that Continual Normalization achieves the best trade-off in terms of adaptability and stability of the model. In the class-incremental setting, the choice of the normalization layer has much less impact. Our experiments suggest that the right choice of architecture can significantly reduce forgetting even with naive fine-tuning and confirm that for real-world applications, the architecture is an important factor in designing a continual learning model.



### Memory-augmented Online Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.10719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68-02, 68-04, 68-06, 68T07, 68T10, 68T45, F.1.1
- **Links**: [PDF](http://arxiv.org/pdf/2302.10719v1)
- **Published**: 2023-02-21 15:14:27+00:00
- **Updated**: 2023-02-21 15:14:27+00:00
- **Authors**: Leonardo Rossi, Vittorio Bernuzzi, Tomaso Fontanini, Massimo Bertozzi, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to understand the surrounding scene is of paramount importance for Autonomous Vehicles (AVs). This paper presents a system capable to work in a real time guaranteed response times and online fashion, giving an immediate response to the arise of anomalies surrounding the AV, exploiting only the videos captured by a dash-mounted camera. Our architecture, called MOVAD, relies on two main modules: a short-term memory to extract information related to the ongoing action, implemented by a Video Swin Transformer adapted to work in an online scenario, and a long-term memory module that considers also remote past information thanks to the use of a Long-Short Term Memory (LSTM) network. We evaluated the performance of our method on Detection of Traffic Anomaly (DoTA) dataset, a challenging collection of dash-mounted camera videos of accidents. After an extensive ablation study, MOVAD is able to reach an AUC score of 82.11%, surpassing the current state-of-the-art by +2.81 AUC. Our code will be available on https://github.com/IMPLabUniPr/movad/tree/icip



### Depth Estimation and Image Restoration by Deep Learning from Defocused Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10730v2
- **DOI**: 10.1109/TCI.2023.3288335
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10730v2)
- **Published**: 2023-02-21 15:28:42+00:00
- **Updated**: 2023-07-27 19:29:15+00:00
- **Authors**: Saqib Nazir, Lorenzo Vaquero, Manuel Mucientes, Víctor M. Brea, Daniela Coltuc
- **Comment**: None
- **Journal**: IEEE Transactions on Computational Imaging, vol. 9, pp. 607-619,
  2023
- **Summary**: Monocular depth estimation and image deblurring are two fundamental tasks in computer vision, given their crucial role in understanding 3D scenes. Performing any of them by relying on a single image is an ill-posed problem. The recent advances in the field of Deep Convolutional Neural Networks (DNNs) have revolutionized many tasks in computer vision, including depth estimation and image deblurring. When it comes to using defocused images, the depth estimation and the recovery of the All-in-Focus (Aif) image become related problems due to defocus physics. Despite this, most of the existing models treat them separately. There are, however, recent models that solve these problems simultaneously by concatenating two networks in a sequence to first estimate the depth or defocus map and then reconstruct the focused image based on it. We propose a DNN that solves the depth estimation and image deblurring in parallel. Our Two-headed Depth Estimation and Deblurring Network (2HDED:NET) extends a conventional Depth from Defocus (DFD) networks with a deblurring branch that shares the same encoder as the depth branch. The proposed method has been successfully tested on two benchmarks, one for indoor and the other for outdoor scenes: NYU-v2 and Make3D. Extensive experiments with 2HDED:NET on these benchmarks have demonstrated superior or close performances to those of the state-of-the-art models for depth estimation and image deblurring.



### Quantifying Jump Height Using Markerless Motion Capture with a Single Smartphone
- **Arxiv ID**: http://arxiv.org/abs/2302.10749v2
- **DOI**: 10.1109/OJEMB.2023.3280127
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10749v2)
- **Published**: 2023-02-21 15:59:00+00:00
- **Updated**: 2023-06-16 16:10:16+00:00
- **Authors**: Timilehin B. Aderinola, Hananeh Younesian, Darragh Whelan, Brian Caulfield, Georgiana Ifrim
- **Comment**: None
- **Journal**: in IEEE Open Journal of Engineering in Medicine and Biology, vol.
  4, pp. 109-115, 2023
- **Summary**: Goal: The countermovement jump (CMJ) is commonly used to measure lower-body explosive power. This study evaluates how accurately markerless motion capture (MMC) with a single smartphone can measure bilateral and unilateral CMJ jump height. Methods: First, three repetitions each of bilateral and unilateral CMJ were performed by sixteen healthy adults (mean age: 30.87$\pm$7.24 years; mean BMI: 23.14$\pm$2.55 $kg/m^2$) on force plates and simultaneously captured using optical motion capture (OMC) and one smartphone camera. Next, MMC was performed on the smartphone videos using OpenPose. Then, we evaluated MMC in quantifying jump height using the force plate and OMC as ground truths. Results: MMC quantifies jump heights with ICC between 0.84 and 0.99 without manual segmentation and camera calibration. Conclusions: Our results suggest that using a single smartphone for markerless motion capture is promising.   Index Terms - Countermovement jump, Markerless motion capture, Optical motion capture, Jump height.   Impact Statement - Countermovement jump height can be accurately quantified using markerless motion capture with a single smartphone, with a simple setup that requires neither camera calibration nor manual segmentation.



### Learning 3D Photography Videos via Self-supervised Diffusion on Single Images
- **Arxiv ID**: http://arxiv.org/abs/2302.10781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10781v1)
- **Published**: 2023-02-21 16:18:40+00:00
- **Updated**: 2023-02-21 16:18:40+00:00
- **Authors**: Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: 3D photography renders a static image into a video with appealing 3D visual effects. Existing approaches typically first conduct monocular depth estimation, then render the input frame to subsequent frames with various viewpoints, and finally use an inpainting model to fill those missing/occluded regions. The inpainting model plays a crucial role in rendering quality, but it is normally trained on out-of-domain data. To reduce the training and inference gap, we propose a novel self-supervised diffusion model as the inpainting module. Given a single input image, we automatically construct a training pair of the masked occluded image and the ground-truth image with random cycle-rendering. The constructed training samples are closely aligned to the testing instances, without the need of data annotation. To make full use of the masked images, we design a Masked Enhanced Block (MEB), which can be easily plugged into the UNet and enhance the semantic conditions. Towards real-world animation, we present a novel task: out-animation, which extends the space and time of input objects. Extensive experiments on real datasets show that our method achieves competitive results with existing SOTA methods.



### Bokeh Rendering Based on Adaptive Depth Calibration Network
- **Arxiv ID**: http://arxiv.org/abs/2302.10808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10808v1)
- **Published**: 2023-02-21 16:33:51+00:00
- **Updated**: 2023-02-21 16:33:51+00:00
- **Authors**: Lu Liu, Lei Zhou, Yuhan Dong
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Bokeh rendering is a popular and effective technique used in photography to create an aesthetically pleasing effect. It is widely used to blur the background and highlight the subject in the foreground, thereby drawing the viewer's attention to the main focus of the image. In traditional digital single-lens reflex cameras (DSLRs), this effect is achieved through the use of a large aperture lens. This allows the camera to capture images with shallow depth-of-field, in which only a small area of the image is in sharp focus, while the rest of the image is blurred. However, the hardware embedded in mobile phones is typically much smaller and more limited than that found in DSLRs. Consequently, mobile phones are not able to capture natural shallow depth-of-field photos, which can be a significant limitation for mobile photography. To address this challenge, in this paper, we propose a novel method for bokeh rendering using the Vision Transformer, a recent and powerful deep learning architecture. Our approach employs an adaptive depth calibration network that acts as a confidence level to compensate for errors in monocular depth estimation. This network is used to supervise the rendering process in conjunction with depth information, allowing for the generation of high-quality bokeh images at high resolutions. Our experiments demonstrate that our proposed method outperforms state-of-the-art methods, achieving about 24.7% improvements on LPIPS and obtaining higher PSNR scores.



### Tracking Objects and Activities with Attention for Temporal Sentence Grounding
- **Arxiv ID**: http://arxiv.org/abs/2302.10813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10813v1)
- **Published**: 2023-02-21 16:42:52+00:00
- **Updated**: 2023-02-21 16:42:52+00:00
- **Authors**: Zeyu Xiong, Daizong Liu, Pan Zhou, Jiahao Zhu
- **Comment**: accepted by ICASSP2023
- **Journal**: None
- **Summary**: Temporal sentence grounding (TSG) aims to localize the temporal segment which is semantically aligned with a natural language query in an untrimmed video.Most existing methods extract frame-grained features or object-grained features by 3D ConvNet or detection network under a conventional TSG framework, failing to capture the subtle differences between frames or to model the spatio-temporal behavior of core persons/objects. In this paper, we introduce a new perspective to address the TSG task by tracking pivotal objects and activities to learn more fine-grained spatio-temporal behaviors. Specifically, we propose a novel Temporal Sentence Tracking Network (TSTNet), which contains (A) a Cross-modal Targets Generator to generate multi-modal templates and search space, filtering objects and activities, and (B) a Temporal Sentence Tracker to track multi-modal targets for modeling the targets' behavior and to predict query-related segment. Extensive experiments and comparisons with state-of-the-arts are conducted on challenging benchmarks: Charades-STA and TACoS. And our TSTNet achieves the leading performance with a considerable real-time speed.



### Device Tuning for Multi-Task Large Model
- **Arxiv ID**: http://arxiv.org/abs/2302.10820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10820v1)
- **Published**: 2023-02-21 16:55:48+00:00
- **Updated**: 2023-02-21 16:55:48+00:00
- **Authors**: Penghao Jiang, Xuanchen Hou, Yinsi Zhou
- **Comment**: AAAI Conference on Artificial Intelligence Deployable AI (DAI) 2023
- **Journal**: None
- **Summary**: Unsupervised pre-training approaches have achieved great success in many fields such as Computer Vision (CV), Natural Language Processing (NLP) and so on. However, compared to typical deep learning models, pre-training or even fine-tuning the state-of-the-art self-attention models is extremely expensive, as they require much more computational and memory resources. It severely limits their applications and success in a variety of domains, especially for multi-task learning. To improve the efficiency, we propose Device Tuning for the efficient multi-task model, which is a massively multitask framework across the cloud and device and is designed to encourage learning of representations that generalize better to many different tasks. Specifically, we design Device Tuning architecture of a multi-task model that benefits both cloud modelling and device modelling, which reduces the communication between device and cloud by representation compression. Experimental results demonstrate the effectiveness of our proposed method.



### Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.10834v2
- **DOI**: 10.1109/TMI.2023.3262847
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10834v2)
- **Published**: 2023-02-21 17:26:49+00:00
- **Updated**: 2023-04-11 13:55:28+00:00
- **Authors**: Sanat Ramesh, Diego Dall'Alba, Cristians Gonzalez, Tong Yu, Pietro Mascagni, Didier Mutter, Jacques Marescaux, Paolo Fiorini, Nicolas Padoy
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic recognition of fine-grained surgical activities, called steps, is a challenging but crucial task for intelligent intra-operative computer assistance. The development of current vision-based activity recognition methods relies heavily on a high volume of manually annotated data. This data is difficult and time-consuming to generate and requires domain-specific knowledge. In this work, we propose to use coarser and easier-to-annotate activity labels, namely phases, as weak supervision to learn step recognition with fewer step annotated videos. We introduce a step-phase dependency loss to exploit the weak supervision signal. We then employ a Single-Stage Temporal Convolutional Network (SS-TCN) with a ResNet-50 backbone, trained in an end-to-end fashion from weakly annotated videos, for temporal activity segmentation and recognition. We extensively evaluate and show the effectiveness of the proposed method on a large video dataset consisting of 40 laparoscopic gastric bypass procedures and the public benchmark CATARACTS containing 50 cataract surgeries.



### Spatial gradient consistency for unsupervised learning of hyperspectral demosaicking: Application to surgical imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.10927v1
- **DOI**: 10.1007/s11548-023-02865-7
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10927v1)
- **Published**: 2023-02-21 18:07:14+00:00
- **Updated**: 2023-02-21 18:07:14+00:00
- **Authors**: Peichao Li, Muhammad Asad, Conor Horgan, Oscar MacCormac, Jonathan Shapey, Tom Vercauteren
- **Comment**: None
- **Journal**: International Journal of Computer Assisted Radiology and Surgery,
  2023
- **Summary**: Hyperspectral imaging has the potential to improve intraoperative decision making if tissue characterisation is performed in real-time and with high-resolution. Hyperspectral snapshot mosaic sensors offer a promising approach due to their fast acquisition speed and compact size. However, a demosaicking algorithm is required to fully recover the spatial and spectral information of the snapshot images. Most state-of-the-art demosaicking algorithms require ground-truth training data with paired snapshot and high-resolution hyperspectral images, but such imagery pairs with the exact same scene are physically impossible to acquire in intraoperative settings. In this work, we present a fully unsupervised hyperspectral image demosaicking algorithm which only requires exemplar snapshot images for training purposes. We regard hyperspectral demosaicking as an ill-posed linear inverse problem which we solve using a deep neural network. We take advantage of the spectral correlation occurring in natural scenes to design a novel inter spectral band regularisation term based on spatial gradient consistency. By combining our proposed term with standard regularisation techniques and exploiting a standard data fidelity term, we obtain an unsupervised loss function for training deep neural networks, which allows us to achieve real-time hyperspectral image demosaicking. Quantitative results on hyperspetral image datasets show that our unsupervised demosaicking approach can achieve similar performance to its supervised counter-part, and significantly outperform linear demosaicking. A qualitative user study on real snapshot hyperspectral surgical images confirms the results from the quantitative analysis. Our results suggest that the proposed unsupervised algorithm can achieve promising hyperspectral demosaicking in real-time thus advancing the suitability of the modality for intraoperative use.



### SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.10859v2
- **DOI**: 10.1016/j.compmedimag.2023.102279
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10859v2)
- **Published**: 2023-02-21 18:16:20+00:00
- **Updated**: 2023-02-28 20:33:45+00:00
- **Authors**: Rafsanjany Kushol, Collin C. Luk, Avyarthana Dey, Michael Benatar, Hannah Briemberg, Annie Dionne, Nicolas Dupré, Richard Frayne, Angela Genge, Summer Gibson, Simon J. Graham, Lawrence Korngut, Peter Seres, Robert C. Welsh, Alan Wilman, Lorne Zinman, Sanjay Kalra, Yee-Hong Yang
- **Comment**: 17 pages, 8 figures
- **Journal**: Computerized Medical Imaging and Graphics Volume 108, September
  2023, 102279
- **Summary**: Amyotrophic Lateral Sclerosis (ALS) is a complex neurodegenerative disorder involving motor neuron degeneration. Significant research has begun to establish brain magnetic resonance imaging (MRI) as a potential biomarker to diagnose and monitor the state of the disease. Deep learning has turned into a prominent class of machine learning programs in computer vision and has been successfully employed to solve diverse medical image analysis tasks. However, deep learning-based methods applied to neuroimaging have not achieved superior performance in ALS patients classification from healthy controls due to having insignificant structural changes correlated with pathological features. Therefore, the critical challenge in deep models is to determine useful discriminative features with limited training data. By exploiting the long-range relationship of image features, this study introduces a framework named SF2Former that leverages vision transformer architecture's power to distinguish the ALS subjects from the control group. To further improve the network's performance, spatial and frequency domain information are combined because MRI scans are captured in the frequency domain before being converted to the spatial domain. The proposed framework is trained with a set of consecutive coronal 2D slices, which uses the pre-trained weights on ImageNet by leveraging transfer learning. Finally, a majority voting scheme has been employed to those coronal slices of a particular subject to produce the final classification decision. Our proposed architecture has been thoroughly assessed with multi-modal neuroimaging data using two well-organized versions of the Canadian ALS Neuroimaging Consortium (CALSNIC) multi-center datasets. The experimental results demonstrate the superiority of our proposed strategy in terms of classification accuracy compared with several popular deep learning-based techniques.



### Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2302.10873v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10873v3)
- **Published**: 2023-02-21 18:42:24+00:00
- **Updated**: 2023-07-11 18:15:18+00:00
- **Authors**: Pei Xu, Jean-Bernard Hayet, Ioannis Karamouzas
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time, accurate prediction of human steering behaviors has wide applications, from developing intelligent traffic systems to deploying autonomous driving systems in both real and simulated worlds. In this paper, we present ContextVAE, a context-aware approach for multi-modal vehicle trajectory prediction. Built upon the backbone architecture of a timewise variational autoencoder, ContextVAE observation encoding employs a dual attention mechanism that accounts for the environmental context and the dynamic agents' states, in a unified way. By utilizing features extracted from semantic maps during agent state encoding, our approach takes into account both the social features exhibited by agents on the scene and the physical environment constraints to generate map-compliant and socially-aware trajectories. We perform extensive testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo Open Motion Dataset to show the effectiveness of our approach and its state-of-the-art performance. In all tested datasets, ContextVAE models are fast to train and provide high-quality multi-modal predictions in real-time. Our code is available at: https://github.com/xupei0610/ContextVAE.



### Combining Blockchain and Biometrics: A Survey on Technical Aspects and a First Legal Analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.10883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10883v1)
- **Published**: 2023-02-21 18:58:32+00:00
- **Updated**: 2023-02-21 18:58:32+00:00
- **Authors**: Mahdi Ghafourian, Bilgesu Sumer, Ruben Vera-Rodriguez, Julian Fierrez, Ruben Tolosana, Aythami Moralez, Els Kindt
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric recognition as a unique, hard-to-forge, and efficient way of identification and verification has become an indispensable part of the current digital world. The fast evolution of this technology has been a strong incentive for integrating it into many applications. Meanwhile, blockchain, the very attractive decentralized ledger technology, has been widely received both by the research and industry in the past years and it is being increasingly deployed nowadays in many different applications, such as money transfer, IoT, healthcare, or logistics. Recently, researchers have started to speculate what would be the pros and cons and what would be the best applications when these two technologies cross paths. This paper provides a survey of technical literature research on the combination of blockchain and biometrics and includes a first legal analysis of this integration to shed light on challenges and potentials. While this combination is still in its infancy and a growing body of literature discusses specific blockchain applications and solutions in an advanced technological set-up, this paper presents a holistic understanding of blockchains applicability in the biometric sector. This study demonstrates that combining blockchain and biometrics would be beneficial for novel applications in biometrics such as the PKI mechanism, distributed trusted service, and identity management. However, blockchain networks at their current stage are not efficient and economical for real-time applications. From a legal point of view, the allocation of accountability remains a main issue, while other difficulties remain, such as conducting a proper Data Protection Impact Assessment. Finally, it supplies technical and legal recommendations to reap the benefits and mitigate the risks of the combination.



### Differentiable Rendering with Reparameterized Volume Sampling
- **Arxiv ID**: http://arxiv.org/abs/2302.10970v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10970v2)
- **Published**: 2023-02-21 19:56:50+00:00
- **Updated**: 2023-05-10 14:15:16+00:00
- **Authors**: Nikita Morozov, Denis Rakitin, Oleg Desheulin, Dmitry Vetrov, Kirill Struminsky
- **Comment**: Preprint
- **Journal**: None
- **Summary**: In view synthesis, a neural radiance field approximates underlying density and radiance fields based on a sparse set of scene pictures. To generate a pixel of a novel view, it marches a ray through the pixel and computes a weighted sum of radiance emitted from a dense set of ray points. This rendering algorithm is fully differentiable and facilitates gradient-based optimization of the fields. However, in practice, only a tiny opaque portion of the ray contributes most of the radiance to the sum. We propose a simple end-to-end differentiable sampling algorithm based on inverse transform sampling. It generates samples according to the probability distribution induced by the density field and picks non-transparent points on the ray. We utilize the algorithm in two ways. First, we propose a novel rendering approach based on Monte Carlo estimates. This approach allows for evaluating and optimizing a neural radiance field with just a few radiance field calls per ray. Second, we use the sampling algorithm to modify the hierarchical scheme proposed in the original NeRF work. We show that our modification improves reconstruction quality of hierarchical models, at the same time simplifying the training procedure by removing the need for auxiliary proposal network losses.



### Likelihood Annealing: Fast Calibrated Uncertainty for Regression
- **Arxiv ID**: http://arxiv.org/abs/2302.11012v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11012v2)
- **Published**: 2023-02-21 21:24:35+00:00
- **Updated**: 2023-07-02 13:01:05+00:00
- **Authors**: Uddeshya Upadhyay, Jae Myung Kim, Cordelia Schmidt, Bernhard Schölkopf, Zeynep Akata
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have shown that uncertainty estimation is becoming increasingly important in applications such as medical imaging, natural language processing, and autonomous systems. However, accurately quantifying uncertainty remains a challenging problem, especially in regression tasks where the output space is continuous. Deep learning approaches that allow uncertainty estimation for regression problems often converge slowly and yield poorly calibrated uncertainty estimates that can not be effectively used for quantification. Recently proposed post hoc calibration techniques are seldom applicable to regression problems and often add overhead to an already slow model training phase. This work presents a fast calibrated uncertainty estimation method for regression tasks called Likelihood Annealing, that consistently improves the convergence of deep regression models and yields calibrated uncertainty without any post hoc calibration phase. Unlike previous methods for calibrated uncertainty in regression that focus only on low-dimensional regression problems, our method works well on a broad spectrum of regression problems, including high-dimensional regression.Our empirical analysis shows that our approach is generalizable to various network architectures, including multilayer perceptrons, 1D/2D convolutional networks, and graph neural networks, on five vastly diverse tasks, i.e., chaotic particle trajectory denoising, physical property prediction of molecules using 3D atomistic representation, natural image super-resolution, and medical image translation using MRI.



### Using Semantic Information for Defining and Detecting OOD Inputs
- **Arxiv ID**: http://arxiv.org/abs/2302.11019v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11019v1)
- **Published**: 2023-02-21 21:31:20+00:00
- **Updated**: 2023-02-21 21:31:20+00:00
- **Authors**: Ramneet Kaur, Xiayan Ji, Souradeep Dutta, Michele Caprio, Yahan Yang, Elena Bernardis, Oleg Sokolsky, Insup Lee
- **Comment**: None
- **Journal**: None
- **Summary**: As machine learning models continue to achieve impressive performance across different tasks, the importance of effective anomaly detection for such models has increased as well. It is common knowledge that even well-trained models lose their ability to function effectively on out-of-distribution inputs. Thus, out-of-distribution (OOD) detection has received some attention recently. In the vast majority of cases, it uses the distribution estimated by the training dataset for OOD detection. We demonstrate that the current detectors inherit the biases in the training dataset, unfortunately. This is a serious impediment, and can potentially restrict the utility of the trained model. This can render the current OOD detectors impermeable to inputs lying outside the training distribution but with the same semantic information (e.g. training class labels). To remedy this situation, we begin by defining what should ideally be treated as an OOD, by connecting inputs with their semantic information content. We perform OOD detection on semantic information extracted from the training data of MNIST and COCO datasets and show that it not only reduces false alarms but also significantly improves the detection of OOD inputs with spurious features from the training data.



### Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2302.11027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11027v1)
- **Published**: 2023-02-21 22:02:39+00:00
- **Updated**: 2023-02-21 22:02:39+00:00
- **Authors**: Labib Ahmed Siddique, Rabita Junhai, Tanzim Reza, Salman Sayeed Khan, Tanvir Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time video surveillance, through CCTV camera systems has become essential for ensuring public safety which is a priority today. Although CCTV cameras help a lot in increasing security, these systems require constant human interaction and monitoring. To eradicate this issue, intelligent surveillance systems can be built using deep learning video classification techniques that can help us automate surveillance systems to detect violence as it happens. In this research, we explore deep learning video classification techniques to detect violence as they are happening. Traditional image classification techniques fall short when it comes to classifying videos as they attempt to classify each frame separately for which the predictions start to flicker. Therefore, many researchers are coming up with video classification techniques that consider spatiotemporal features while classifying. However, deploying these deep learning models with methods such as skeleton points obtained through pose estimation and optical flow obtained through depth sensors, are not always practical in an IoT environment. Although these techniques ensure a higher accuracy score, they are computationally heavier. Keeping these constraints in mind, we experimented with various video classification and action recognition techniques such as ConvLSTM, LRCN (with both custom CNN layers and VGG-16 as feature extractor) CNNTransformer and C3D. We achieved a test accuracy of 80% on ConvLSTM, 83.33% on CNN-BiLSTM, 70% on VGG16-BiLstm ,76.76% on CNN-Transformer and 80% on C3D.



### Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching
- **Arxiv ID**: http://arxiv.org/abs/2302.11046v1
- **DOI**: 10.1145/3544548.3581449
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11046v1)
- **Published**: 2023-02-21 23:03:49+00:00
- **Updated**: 2023-02-21 23:03:49+00:00
- **Authors**: Kyzyl Monteiro, Ritik Vatsal, Neil Chulpongsatorn, Aman Parnami, Ryo Suzuki
- **Comment**: CHI 2023
- **Journal**: None
- **Summary**: This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.



### Framework for Certification of AI-Based Systems
- **Arxiv ID**: http://arxiv.org/abs/2302.11049v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2302.11049v1)
- **Published**: 2023-02-21 23:08:37+00:00
- **Updated**: 2023-02-21 23:08:37+00:00
- **Authors**: Maxime Gariel, Brian Shimanuki, Rob Timpe, Evan Wilson
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: The current certification process for aerospace software is not adapted to "AI-based" algorithms such as deep neural networks. Unlike traditional aerospace software, the precise parameters optimized during neural network training are as important as (or more than) the code processing the network and they are not directly mathematically understandable. Despite their lack of explainability such algorithms are appealing because for some applications they can exhibit high performance unattainable with any traditional explicit line-by-line software methods.   This paper proposes a framework and principles that could be used to establish certification methods for neural network models for which the current certification processes such as DO-178 cannot be applied. While it is not a magic recipe, it is a set of common sense steps that will allow the applicant and the regulator increase their confidence in the developed software, by demonstrating the capabilities to bring together, trace, and track the requirements, data, software, training process, and test results.



