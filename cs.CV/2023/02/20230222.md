# Arxiv Papers in cs.CV on 2023-02-22
### BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.11082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11082v1)
- **Published**: 2023-02-22 01:03:53+00:00
- **Updated**: 2023-02-22 01:03:53+00:00
- **Authors**: Guoli Wang, Pingping Wang, Jinyu Cong, Kunmeng Liu, Benzheng Wei
- **Comment**: under Computers in Biology and Medicine submission
- **Journal**: None
- **Summary**: Multi-label chest X-ray (CXR) recognition involves simultaneously diagnosing and identifying multiple labels for different pathologies. Since pathological labels have rich information about their relationship to each other, modeling the co-occurrence dependencies between pathological labels is essential to improve recognition performance. However, previous methods rely on state variable coding and attention mechanisms-oriented to model local label information, and lack learning of global co-occurrence relationships between labels. Furthermore, these methods roughly integrate image features and label embedding, ignoring the alignment and compactness problems in cross-modal vector fusion.To solve these problems, a Bi-modal Bridged Graph Convolutional Network (BB-GCN) model is proposed. This model mainly consists of a backbone module, a pathology Label Co-occurrence relationship Embedding (LCE) module, and a Transformer Bridge Graph (TBG) module. Specifically, the backbone module obtains image visual feature representation. The LCE module utilizes a graph to model the global co-occurrence relationship between multiple labels and employs graph convolutional networks for learning inference. The TBG module bridges the cross-modal vectors more compactly and efficiently through the GroupSum method.We have evaluated the effectiveness of the proposed BB-GCN in two large-scale CXR datasets (ChestX-Ray14 and CheXpert). Our model achieved state-of-the-art performance: the mean AUC scores for the 14 pathologies were 0.835 and 0.813, respectively.The proposed LCE and TBG modules can jointly effectively improve the recognition performance of BB-GCN. Our model also achieves satisfactory results in multi-label chest X-ray recognition and exhibits highly competitive generalization performance.



### Distribution Normalization: An "Effortless" Test-Time Augmentation for Contrastively Learned Visual-language Models
- **Arxiv ID**: http://arxiv.org/abs/2302.11084v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11084v1)
- **Published**: 2023-02-22 01:14:30+00:00
- **Updated**: 2023-02-22 01:14:30+00:00
- **Authors**: Yifei Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, Ser-Nam Lim
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Advances in the field of visual-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has quickly garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper however reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should ideally also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference. We propose Distribution Normalization (DN), where we approximate the mean representation of a batch of test samples and use such a mean to represent what would be analogous to negative samples in the InfoNCE loss. DN requires no retraining or fine-tuning and can be effortlessly applied during inference. Extensive experiments on a wide variety of downstream tasks exhibit a clear advantage of DN over the dot product.



### MM-SFENet: Multi-scale Multi-task Localization and Classification of Bladder Cancer in MRI with Spatial Feature Encoder Network
- **Arxiv ID**: http://arxiv.org/abs/2302.11095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11095v1)
- **Published**: 2023-02-22 02:28:14+00:00
- **Updated**: 2023-02-22 02:28:14+00:00
- **Authors**: Yu Ren, Guoli Wang, Pingping Wang, Kunmeng Liu, Quanjin Liu, Hongfu Sun, Xiang Li, Benzheng Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Objective: Bladder cancer is a common malignant urinary carcinoma, with muscle-invasive and non-muscle-invasive as its two major subtypes. This paper aims to achieve automated bladder cancer invasiveness localization and classification based on MRI. Method: Different from previous efforts that segment bladder wall and tumor, we propose a novel end-to-end multi-scale multi-task spatial feature encoder network (MM-SFENet) for locating and classifying bladder cancer, according to the classification criteria of the spatial relationship between the tumor and bladder wall. First, we built a backbone with residual blocks to distinguish bladder wall and tumor; then, a spatial feature encoder is designed to encode the multi-level features of the backbone to learn the criteria. Results: We substitute Smooth-L1 Loss with IoU Loss for multi-task learning, to improve the accuracy of the classification task. By testing a total of 1287 MRIs collected from 98 patients at the hospital, the mAP and IoU are used as the evaluation metrics. The experimental result could reach 93.34\% and 83.16\% on test set. Conclusions: The experimental result demonstrates the effectiveness of the proposed MM-SFENet on the localization and classification of bladder cancer. It may provide an effective supplementary diagnosis method for bladder cancer staging.



### A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram
- **Arxiv ID**: http://arxiv.org/abs/2302.11097v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11097v2)
- **Published**: 2023-02-22 02:38:25+00:00
- **Updated**: 2023-04-28 10:04:17+00:00
- **Authors**: Ming-Liang Zhang, Fei Yin, Cheng-Lin Liu
- **Comment**: Accepted to IJCAI 2023
- **Journal**: None
- **Summary**: Geometry problem solving (GPS) is a high-level mathematical reasoning requiring the capacities of multi-modal fusion and geometric knowledge application. Recently, neural solvers have shown great potential in GPS but still be short in diagram presentation and modal fusion. In this work, we convert diagrams into basic textual clauses to describe diagram features effectively, and propose a new neural solver called PGPSNet to fuse multi-modal information efficiently. Combining structural and semantic pre-training, data augmentation and self-limited decoding, PGPSNet is endowed with rich knowledge of geometry theorems and geometric representation, and therefore promotes geometric understanding and reasoning. In addition, to facilitate the research of GPS, we build a new large-scale and fine-annotated GPS dataset named PGPS9K, labeled with both fine-grained diagram annotation and interpretable solution program. Experiments on PGPS9K and an existing dataset Geometry3K validate the superiority of our method over the state-of-the-art neural solvers. Our code, dataset and appendix material are available at \url{https://github.com/mingliangzhang2018/PGPS}.



### Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.11102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11102v2)
- **Published**: 2023-02-22 02:49:21+00:00
- **Updated**: 2023-04-16 14:04:19+00:00
- **Authors**: Haiyu Wu, Grace Bezold, Aman Bhatta, Kevin W. Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: Face attribute research has so far used only simple binary attributes for facial hair; e.g., beard / no beard. We have created a new, more descriptive facial hair annotation scheme and applied it to create a new facial hair attribute dataset, FH37K. Face attribute research also so far has not dealt with logical consistency and completeness. For example, in prior research, an image might be classified as both having no beard and also having a goatee (a type of beard). We show that the test accuracy of previous classification methods on facial hair attribute classification drops significantly if logical consistency of classifications is enforced. We propose a logically consistent prediction loss, LCPLoss, to aid learning of logical consistency across attributes, and also a label compensation training strategy to eliminate the problem of no positive prediction across a set of related attributes. Using an attribute classifier trained on FH37K, we investigate how facial hair affects face recognition accuracy, including variation across demographics. Results show that similarity and difference in facial hairstyle have important effects on the impostor and genuine score distributions in face recognition. The code is at https:// github.com/ HaiyuWu/ LogicalConsistency.



### Multi-Head Feature Pyramid Networks for Breast Mass Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.11106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11106v1)
- **Published**: 2023-02-22 03:02:52+00:00
- **Updated**: 2023-02-22 03:02:52+00:00
- **Authors**: Hexiang Zhang, Zhenghua Xu, Dan Yao, Shuo Zhang, Junyang Chen, Thomas Lukasiewicz
- **Comment**: 7 pages, 3 figures,Received by the ICASSP2023
- **Journal**: None
- **Summary**: Analysis of X-ray images is one of the main tools to diagnose breast cancer. The ability to quickly and accurately detect the location of masses from the huge amount of image data is the key to reducing the morbidity and mortality of breast cancer. Currently, the main factor limiting the accuracy of breast mass detection is the unequal focus on the mass boxes, leading the network to focus too much on larger masses at the expense of smaller ones. In the paper, we propose the multi-head feature pyramid module (MHFPN) to solve the problem of unbalanced focus of target boxes during feature map fusion and design a multi-head breast mass detection network (MBMDnet). Experimental studies show that, comparing to the SOTA detection baselines, our method improves by 6.58% (in AP@50) and 5.4% (in TPR@50) on the commonly used INbreast dataset, while about 6-8% improvements (in AP@20) are also observed on the public MIAS and BCS-DBT datasets.



### Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities
- **Arxiv ID**: http://arxiv.org/abs/2302.11154v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2302.11154v2)
- **Published**: 2023-02-22 05:31:26+00:00
- **Updated**: 2023-02-24 00:50:25+00:00
- **Authors**: Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, Ming-Wei Chang
- **Comment**: Dataset available at https://open-vision-language.github.io/oven
- **Journal**: None
- **Summary**: Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit strong generalization on various visual domains and tasks. However, existing image classification benchmarks often evaluate recognition on a specific domain (e.g., outdoor images) or a specific task (e.g., classifying plant species), which falls short of evaluating whether pre-trained foundational models are universal visual recognizers. To address this, we formally present the task of Open-domain Visual Entity recognitioN (OVEN), where a model need to link an image onto a Wikipedia entity with respect to a text query. We construct OVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto one single label space: Wikipedia entities. OVEN challenges models to select among six million possible Wikipedia entities, making it a general visual recognition benchmark with the largest number of labels. Our study on state-of-the-art pre-trained models reveals large headroom in generalizing to the massive-scale label space. We show that a PaLI-based auto-regressive visual recognition model performs surprisingly well, even on Wikipedia entities that have never been seen during fine-tuning. We also find existing pretrained models yield different strengths: while PaLI-based models obtain higher overall performance, CLIP-based models are better at recognizing tail entities.



### DISCO: Distributed Inference with Sparse Communications
- **Arxiv ID**: http://arxiv.org/abs/2302.11180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11180v1)
- **Published**: 2023-02-22 07:20:34+00:00
- **Updated**: 2023-02-22 07:20:34+00:00
- **Authors**: Minghai Qin, Chao Sun, Jaco Hofmann, Dejan Vucinic
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have great potential to solve many real-world problems, but they usually require an extensive amount of computation and memory. It is of great difficulty to deploy a large DNN model to a single resource-limited device with small memory capacity. Distributed computing is a common approach to reduce single-node memory consumption and to accelerate the inference of DNN models. In this paper, we explore the "within-layer model parallelism", which distributes the inference of each layer into multiple nodes. In this way, the memory requirement can be distributed to many nodes, making it possible to use several edge devices to infer a large DNN model. Due to the dependency within each layer, data communications between nodes during this parallel inference can be a bottleneck when the communication bandwidth is limited. We propose a framework to train DNN models for Distributed Inference with Sparse Communications (DISCO). We convert the problem of selecting which subset of data to transmit between nodes into a model optimization problem, and derive models with both computation and communication reduction when each layer is inferred on multiple nodes. We show the benefit of the DISCO framework on a variety of CV tasks such as image classification, object detection, semantic segmentation, and image super resolution. The corresponding models include important DNN building blocks such as convolutions and transformers. For example, each layer of a ResNet-50 model can be distributively inferred across two nodes with five times less data communications, almost half overall computations and half memory requirement for a single node, and achieve comparable accuracy to the original ResNet-50 model. This also results in 4.7 times overall inference speedup.



### A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2302.11184v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11184v2)
- **Published**: 2023-02-22 07:39:09+00:00
- **Updated**: 2023-03-03 06:51:27+00:00
- **Authors**: Jin Zhu, Guang Yang, Pietro Lio
- **Comment**: Preprint submitted to Medical Image Analysis and under review
- **Journal**: None
- **Summary**: Super-resolution plays an essential role in medical imaging because it provides an alternative way to achieve high spatial resolutions and image quality with no extra acquisition costs. In the past few decades, the rapid development of deep neural networks has promoted super-resolution performance with novel network architectures, loss functions and evaluation metrics. Specifically, vision transformers dominate a broad range of computer vision tasks, but challenges still exist when applying them to low-level medical image processing tasks. This paper proposes an efficient vision transformer with residual dense connections and local feature fusion to achieve efficient single-image super-resolution (SISR) of medical modalities. Moreover, we implement a general-purpose perceptual loss with manual control for image quality improvements of desired aspects by incorporating prior knowledge of medical image segmentation. Compared with state-of-the-art methods on four public medical image datasets, the proposed method achieves the best PSNR scores of 6 modalities among seven modalities. It leads to an average improvement of $+0.09$ dB PSNR with only 38\% parameters of SwinIR. On the other hand, the segmentation-based perceptual loss increases $+0.14$ dB PSNR on average for SOTA methods, including CNNs and vision transformers. Additionally, we conduct comprehensive ablation studies to discuss potential factors for the superior performance of vision transformers over CNNs and the impacts of network and loss function components. The code will be released on GitHub with the paper published.



### Invariant Target Detection in Images through the Normalized 2-D Correlation Technique
- **Arxiv ID**: http://arxiv.org/abs/2302.11196v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11196v1)
- **Published**: 2023-02-22 08:13:34+00:00
- **Updated**: 2023-02-22 08:13:34+00:00
- **Authors**: Fatin E. M. Al-Obaidi, Anwar H. Al-Saleh, Shaymaa H. Kafi, Ali J. Karam, Ali A. D. Al-Zuky
- **Comment**: None
- **Journal**: None
- **Summary**: The normalized 2-D correlation technique is a robust method for detecting targets in images due to its ability to remain invariant under rotation, translation, and scaling. This paper examines the impact of translation, and scaling on target identification in images. The results indicate a high level of accuracy in detecting targets, even when they are exhibit variations in location and size. The results indicate that the similarity between the image and the two used targets improves as the resize ratio increases. All statistical estimators demonstrate a strong similarity between the original and extracted targets. The elapsed time for all scenarios falls within the range (44.75-44.85), (37.48-37.73) seconds for bird and children targets respectively, and the correlation coefficient displays stable relationships with values that fall within the range of (0.90-0.98) and (0.87-0.93) for bird and children targets respectively.



### Semi-Supervised Segmentation of Multi-vendor and Multi-center Cardiac MRI using Histogram Matching
- **Arxiv ID**: http://arxiv.org/abs/2302.11200v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11200v1)
- **Published**: 2023-02-22 08:23:19+00:00
- **Updated**: 2023-02-22 08:23:19+00:00
- **Authors**: Mahyar Bolhassani, Ilkay Oksuz
- **Comment**: 5 pages, 8 figures, IEEE conference published paper
- **Journal**: IEEE Conference on Signal Processing and Communications
  applications (SIU 2021)
- **Summary**: Automatic segmentation of the heart cavity is an essential task for the diagnosis of cardiac diseases. In this paper, we propose a semi-supervised segmentation setup for leveraging unlabeled data to segment Left-ventricle, Right-ventricle, and Myocardium. We utilize an enhanced version of residual U-Net architecture on a large-scale cardiac MRI dataset. Handling the class imbalanced data issue using dice loss, the enhanced supervised model is able to achieve better dice scores in comparison with a vanilla U-Net model. We applied several augmentation techniques including histogram matching to increase the performance of our model in other domains. Also, we introduce a simple but efficient semi-supervised segmentation method to improve segmentation results without the need for large labeled data. Finally, we applied our method on two benchmark datasets, STACOM2018, and M\&Ms 2020 challenges, to show the potency of the proposed model. The effectiveness of our proposed model is demonstrated by the quantitative results. The model achieves average dice scores of 0.921, 0.926, and 0.891 for Left-ventricle, Right-ventricle, and Myocardium respectively.



### KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.11208v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11208v2)
- **Published**: 2023-02-22 08:48:08+00:00
- **Updated**: 2023-03-16 04:54:05+00:00
- **Authors**: Kaikai Zhao, Norimichi Ukita
- **Comment**: None
- **Journal**: None
- **Summary**: Scaled dot-product attention applies a softmax function on the scaled dot-product of queries and keys to calculate weights and then multiplies the weights and values. In this work, we study how to improve the learning of scaled dot-product attention to improve the accuracy of DETR. Our method is based on the following observations: using ground truth foreground-background mask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables learning much better weights/values; with better weights/values, better values/weights can be learned. We propose a triple-attention module in which the first attention is a plain scaled dot-product attention, the second/third attention generates high-quality weights/values (with the assistance of GT Fg-Bg Mask) and shares the values/weights with the first attention to improve the quality of values/weights. The second and third attentions are removed during inference. We call our method knowledge-sharing DETR (KS-DETR), which is an extension of knowledge distillation (KD) in the way that the improved weights and values of the teachers (the second and third attentions) are directly shared, instead of mimicked, by the student (the first attention) to enable more efficient knowledge transfer from the teachers to the student. Experiments on various DETR-like methods show consistent improvements over the baseline methods on the MS COCO benchmark. Code is available at https://github.com/edocanonymous/KS-DETR.



### DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.12007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.12007v1)
- **Published**: 2023-02-22 08:53:11+00:00
- **Updated**: 2023-02-22 08:53:11+00:00
- **Authors**: Shannan Guan, Xin Yu, Wei Huang, Gengfa Fang, Haiyan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a new Dual Min-Max Games (DMMG) based self-supervised skeleton action recognition method by augmenting unlabeled data in a contrastive learning framework. Our DMMG consists of a viewpoint variation min-max game and an edge perturbation min-max game. These two min-max games adopt an adversarial paradigm to perform data augmentation on the skeleton sequences and graph-structured body joints, respectively. Our viewpoint variation min-max game focuses on constructing various hard contrastive pairs by generating skeleton sequences from various viewpoints. These hard contrastive pairs help our model learn representative action features, thus facilitating model transfer to downstream tasks. Moreover, our edge perturbation min-max game specializes in building diverse hard contrastive samples through perturbing connectivity strength among graph-based body joints. The connectivity-strength varying contrastive pairs enable the model to capture minimal sufficient information of different actions, such as representative gestures for an action while preventing the model from overfitting. By fully exploiting the proposed DMMG, we can generate sufficient challenging contrastive pairs and thus achieve discriminative action feature representations from unlabeled skeleton data in a self-supervised manner. Extensive experiments demonstrate that our method achieves superior results under various evaluation protocols on widely-used NTU-RGB+D and NTU120-RGB+D datasets.



### Connecting Vision and Language with Video Localized Narratives
- **Arxiv ID**: http://arxiv.org/abs/2302.11217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11217v2)
- **Published**: 2023-02-22 09:04:00+00:00
- **Updated**: 2023-03-15 10:30:18+00:00
- **Authors**: Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, Vittorio Ferrari
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and language. In the original Localized Narratives, annotators speak and move their mouse simultaneously on an image, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Localized Narratives, capturing even complex events involving multiple actors interacting with each other and with several passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google.github.io/video-localized-narratives/.



### Considering Layerwise Importance in the Lottery Ticket Hypothesis
- **Arxiv ID**: http://arxiv.org/abs/2302.11244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11244v1)
- **Published**: 2023-02-22 09:51:00+00:00
- **Updated**: 2023-02-22 09:51:00+00:00
- **Authors**: Benjamin Vandersmissen, Jose Oramas
- **Comment**: None
- **Journal**: None
- **Summary**: The Lottery Ticket Hypothesis (LTH) showed that by iteratively training a model, removing connections with the lowest global weight magnitude and rewinding the remaining connections, sparse networks can be extracted.   This global comparison removes context information between connections within a layer. Here we study means for recovering some of this layer distributional context and generalise the LTH to consider weight importance values rather than global weight magnitudes.   We find that given a repeatable training procedure, applying different importance metrics leads to distinct performant lottery tickets with little overlapping connections. This strongly suggests that lottery tickets are not unique



### Focusing On Targets For Improving Weakly Supervised Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2302.11252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11252v1)
- **Published**: 2023-02-22 10:02:21+00:00
- **Updated**: 2023-02-22 10:02:21+00:00
- **Authors**: Viet-Quoc Pham, Nao Mishima
- **Comment**: accepted by ICASSP2023
- **Journal**: None
- **Summary**: Weakly supervised visual grounding aims to predict the region in an image that corresponds to a specific linguistic query, where the mapping between the target object and query is unknown in the training stage. The state-of-the-art method uses a vision language pre-training model to acquire heatmaps from Grad-CAM, which matches every query word with an image region, and uses the combined heatmap to rank the region proposals. In this paper, we propose two simple but efficient methods for improving this approach. First, we propose a target-aware cropping approach to encourage the model to learn both object and scene level semantic representations. Second, we apply dependency parsing to extract words related to the target object, and then put emphasis on these words in the heatmap combination. Our method surpasses the previous SOTA methods on RefCOCO, RefCOCO+, and RefCOCOg by a notable margin.



### Cross-modal Audio-visual Co-learning for Text-independent Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/2302.11254v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11254v1)
- **Published**: 2023-02-22 10:06:37+00:00
- **Updated**: 2023-02-22 10:06:37+00:00
- **Authors**: Meng Liu, Kong Aik Lee, Longbiao Wang, Hanyi Zhang, Chang Zeng, Jianwu Dang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual speech (i.e., lip motion) is highly related to auditory speech due to the co-occurrence and synchronization in speech production. This paper investigates this correlation and proposes a cross-modal speech co-learning paradigm. The primary motivation of our cross-modal co-learning method is modeling one modality aided by exploiting knowledge from another modality. Specifically, two cross-modal boosters are introduced based on an audio-visual pseudo-siamese structure to learn the modality-transformed correlation. Inside each booster, a max-feature-map embedded Transformer variant is proposed for modality alignment and enhanced feature generation. The network is co-learned both from scratch and with pretrained models. Experimental results on the LRSLip3, GridLip, LomGridLip, and VoxLip datasets demonstrate that our proposed method achieves 60% and 20% average relative performance improvement over independently trained audio-only/visual-only and baseline fusion systems, respectively.



### Asynchronous Trajectory Matching-Based Multimodal Maritime Data Fusion for Vessel Traffic Surveillance in Inland Waterways
- **Arxiv ID**: http://arxiv.org/abs/2302.11283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11283v1)
- **Published**: 2023-02-22 11:00:34+00:00
- **Updated**: 2023-02-22 11:00:34+00:00
- **Authors**: Yu Guo, Ryan Wen Liu, Jingxiang Qu, Yuxu Lu, Fenghua Zhu, Yisheng Lv
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic identification system (AIS) and video cameras have been widely exploited for vessel traffic surveillance in inland waterways. The AIS data could provide the vessel identity and dynamic information on vessel position and movements. In contrast, the video data could describe the visual appearances of moving vessels, but without knowing the information on identity, position and movements, etc. To further improve vessel traffic surveillance, it becomes necessary to fuse the AIS and video data to simultaneously capture the visual features, identity and dynamic information for the vessels of interest. However, traditional data fusion methods easily suffer from several potential limitations, e.g., asynchronous messages, missing data, random outliers, etc. In this work, we first extract the AIS- and video-based vessel trajectories, and then propose a deep learning-enabled asynchronous trajectory matching method (named DeepSORVF) to fuse the AIS-based vessel information with the corresponding visual targets. In addition, by combining the AIS- and video-based movement features, we also present a prior knowledge-driven anti-occlusion method to yield accurate and robust vessel tracking results under occlusion conditions. To validate the efficacy of our DeepSORVF, we have also constructed a new benchmark dataset (termed FVessel) for vessel detection, tracking, and data fusion. It consists of many videos and the corresponding AIS data collected in various weather conditions and locations. The experimental results have demonstrated that our method is capable of guaranteeing high-reliable data fusion and anti-occlusion vessel tracking.



### Towards End-to-end Semi-supervised Learning for One-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.11299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11299v1)
- **Published**: 2023-02-22 11:35:40+00:00
- **Updated**: 2023-02-22 11:35:40+00:00
- **Authors**: Gen Luo, Yiyi Zhou, Lei Jin, Xiaoshuai Sun, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised object detection (SSOD) is a research hot spot in computer vision, which can greatly reduce the requirement for expensive bounding-box annotations. Despite great success, existing progress mainly focuses on two-stage detection networks like FasterRCNN, while the research on one-stage detectors is often ignored. In this paper, we focus on the semi-supervised learning for the advanced and popular one-stage detection network YOLOv5. Compared with Faster-RCNN, the implementation of YOLOv5 is much more complex, and the various training techniques used in YOLOv5 can also reduce the benefit of SSOD. In addition to this challenge, we also reveal two key issues in one-stage SSOD, which are low-quality pseudo-labeling and multi-task optimization conflict, respectively. To address these issues, we propose a novel teacher-student learning recipe called OneTeacher with two innovative designs, namely Multi-view Pseudo-label Refinement (MPR) and Decoupled Semi-supervised Optimization (DSO). In particular, MPR improves the quality of pseudo-labels via augmented-view refinement and global-view filtering, and DSO handles the joint optimization conflicts via structure tweaks and task-specific pseudo-labeling. In addition, we also carefully revise the implementation of YOLOv5 to maximize the benefits of SSOD, which is also shared with the existing SSOD methods for fair comparison. To validate OneTeacher, we conduct extensive experiments on COCO and Pascal VOC. The extensive experiments show that OneTeacher can not only achieve superior performance than the compared methods, e.g., 15.0% relative AP gains over Unbiased Teacher, but also well handle the key issues in one-stage SSOD. Our source code is available at: https://github.com/luogen1996/OneTeacher.



### View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.11301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11301v2)
- **Published**: 2023-02-22 11:36:40+00:00
- **Updated**: 2023-02-23 02:01:36+00:00
- **Authors**: Xiaoyue Wan, Zhuo Chen, Xu Zhao
- **Comment**: 8 pages, 6 figures, currently under review at Computer Vision and
  Image Understanding (CVIU) journal
- **Journal**: None
- **Summary**: The rapid development of multi-view 3D human pose estimation (HPE) is attributed to the maturation of monocular 2D HPE and the geometry of 3D reconstruction. However, 2D detection outliers in occluded views due to neglect of view consistency, and 3D implausible poses due to lack of pose coherence, remain challenges. To solve this, we introduce a Multi-View Fusion module to refine 2D results by establishing view correlations. Then, Holistic Triangulation is proposed to infer the whole pose as an entirety, and anatomy prior is injected to maintain the pose coherence and improve the plausibility. Anatomy prior is extracted by PCA whose input is skeletal structure features, which can factor out global context and joint-by-joint relationship from abstract to concrete. Benefiting from the closed-form solution, the whole framework is trained end-to-end. Our method outperforms the state of the art in both precision and plausibility which is assessed by a new metric.



### Human MotionFormer: Transferring Human Motions with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.11306v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11306v2)
- **Published**: 2023-02-22 11:42:44+00:00
- **Updated**: 2023-02-25 14:59:45+00:00
- **Authors**: Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, Qifeng Chen
- **Comment**: Accepted by ICLR2023
- **Journal**: None
- **Summary**: Human motion transfer aims to transfer motions from a target dynamic person to a source static one for motion synthesis. An accurate matching between the source person and the target motion in both large and subtle motion changes is vital for improving the transferred motion quality. In this paper, we propose Human MotionFormer, a hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively. It consists of two ViT encoders to extract input features (i.e., a target motion image and a source human image) and a ViT decoder with several cascaded blocks for feature matching and motion transfer. In each block, we set the target motion feature as Query and the source person as Key and Value, calculating the cross-attention maps to conduct a global feature matching. Further, we introduce a convolutional layer to improve the local perception after the global cross-attention computations. This matching process is implemented in both warping and generation branches to guide the motion transfer. During training, we propose a mutual learning loss to enable the co-supervision between warping and generation branches for better motion representations. Experiments show that our Human MotionFormer sets the new state-of-the-art performance both qualitatively and quantitatively. Project page: \url{https://github.com/KumapowerLIU/Human-MotionFormer}



### Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.11325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.11325v2)
- **Published**: 2023-02-22 12:09:39+00:00
- **Updated**: 2023-07-04 15:51:23+00:00
- **Authors**: Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto M Gambaruto, Tilo Burghardt
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a deep learning framework for medical video segmentation. Convolution neural network (CNN) and transformer-based methods have achieved great milestones in medical image segmentation tasks due to their incredible semantic feature encoding and global information comprehension abilities. However, most existing approaches ignore a salient aspect of medical video data - the temporal dimension. Our proposed framework explicitly extracts features from neighbouring frames across the temporal dimension and incorporates them with a temporal feature blender, which then tokenises the high-level spatio-temporal feature to form a strong global feature encoded via a Swin Transformer. The final segmentation results are produced via a UNet-like encoder-decoder architecture. Our model outperforms other approaches by a significant margin and improves the segmentation benchmarks on the VFSS2022 dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets tested. Our studies also show the efficacy of the temporal feature blending scheme and cross-dataset transferability of learned capabilities. Code and models are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.



### A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.11327v2
- **DOI**: 10.1109/OJSP.2023.3279011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11327v2)
- **Published**: 2023-02-22 12:17:32+00:00
- **Updated**: 2023-02-23 09:13:03+00:00
- **Authors**: Seyedsaman Emami, Gonzalo Martínez-Muñoz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has revolutionized the computer vision and image classification domains. In this context Convolutional Neural Networks (CNNs) based architectures are the most widely applied models. In this article, we introduced two procedures for training Convolutional Neural Networks (CNNs) and Deep Neural Network based on Gradient Boosting (GB), namely GB-CNN and GB-DNN. These models are trained to fit the gradient of the loss function or pseudo-residuals of previous models. At each iteration, the proposed method adds one dense layer to an exact copy of the previous deep NN model. The weights of the dense layers trained on previous iterations are frozen to prevent over-fitting, permitting the model to fit the new dense as well as to fine-tune the convolutional layers (for GB-CNN) while still utilizing the information already learned. Through extensive experimentation on different 2D-image classification and tabular datasets, the presented models show superior performance in terms of classification accuracy with respect to standard CNN and Deep-NN with the same architectures.



### Steerable Equivariant Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.11349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11349v1)
- **Published**: 2023-02-22 12:42:45+00:00
- **Updated**: 2023-02-22 12:42:45+00:00
- **Authors**: Sangnie Bhardwaj, Willie McClinton, Tongzhou Wang, Guillaume Lajoie, Chen Sun, Phillip Isola, Dilip Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-trained deep image representations are useful for post-training tasks such as classification through transfer learning, image retrieval, and object detection. Data augmentations are a crucial aspect of pre-training robust representations in both supervised and self-supervised settings. Data augmentations explicitly or implicitly promote invariance in the embedding space to the input image transformations. This invariance reduces generalization to those downstream tasks which rely on sensitivity to these particular data augmentations. In this paper, we propose a method of learning representations that are instead equivariant to data augmentations. We achieve this equivariance through the use of steerable representations. Our representations can be manipulated directly in embedding space via learned linear maps. We demonstrate that our resulting steerable and equivariant representations lead to better performance on transfer learning and robustness: e.g. we improve linear probe top-1 accuracy by between 1% to 3% for transfer; and ImageNet-C accuracy by upto 3.4%. We further show that the steerability of our representations provides significant speedup (nearly 50x) for test-time augmentations; by applying a large number of augmentations for out-of-distribution detection, we significantly improve OOD AUC on the ImageNet-C dataset over an invariant representation.



### X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.11352v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2302.11352v1)
- **Published**: 2023-02-22 12:53:33+00:00
- **Updated**: 2023-02-22 12:53:33+00:00
- **Authors**: Tom van Sonsbeek, Marcel Worring
- **Comment**: IPMI 2023
- **Journal**: None
- **Summary**: An important component of human analysis of medical images and their context is the ability to relate newly seen things to related instances in our memory. In this paper we mimic this ability by using multi-modal retrieval augmentation and apply it to several tasks in chest X-ray analysis. By retrieving similar images and/or radiology reports we expand and regularize the case at hand with additional knowledge, while maintaining factual knowledge consistency. The method consists of two components. First, vision and language modalities are aligned using a pre-trained CLIP model. To enforce that the retrieval focus will be on detailed disease-related content instead of global visual appearance it is fine-tuned using disease class information. Subsequently, we construct a non-parametric retrieval index, which reaches state-of-the-art retrieval levels. We use this index in our downstream tasks to augment image representations through multi-head attention for disease classification and report retrieval. We show that retrieval augmentation gives considerable improvements on these tasks. Our downstream report retrieval even shows to be competitive with dedicated report generation methods, paving the path for this method in medical imaging.



### Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems
- **Arxiv ID**: http://arxiv.org/abs/2302.11356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11356v1)
- **Published**: 2023-02-22 13:03:31+00:00
- **Updated**: 2023-02-22 13:03:31+00:00
- **Authors**: Haiyi Mao, Cong Peng, Yue Liu, Jinping Tang, Hua Peng, Wei Yi
- **Comment**: in 2023 IEEE Radar Conference (RadarConf2023), 2023
- **Journal**: None
- **Summary**: A variety of filters with track-before-detect (TBD) strategies have been developed and applied to low signal-to-noise ratio (SNR) scenarios, including the probability hypothesis density (PHD) filter. Assumptions of the standard point measurement model based on detect-before-track (DBT) strategies are not suitable for the amplitude echo model based on TBD strategies. However, based on different models and unmatched assumptions, the measurement update formulas for DBT-PHD filter are just mechanically applied to existing TBD-PHD filters. In this paper, based on the Kullback-Leibler divergence minimization criterion, finite set statistics theory and rigorous Bayes rule, a principled closed-form solution of TBD-PHD filter is derived. Furthermore, we emphasize that PHD filter is conjugated to the Poisson prior based on TBD strategies. Next, a capping operation is devised to handle the divergence of target number estimation as SNR increases. Moreover, the sequential Monte Carlo implementations of dynamic and amplitude echo models are proposed for the radar system. Finally, Monte Carlo experiments exhibit good performance in Rayleigh noise and low SNR scenarios.



### HDR image watermarking using saliency detection and quantization index modulation
- **Arxiv ID**: http://arxiv.org/abs/2302.11361v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.11361v2)
- **Published**: 2023-02-22 13:25:29+00:00
- **Updated**: 2023-02-23 08:55:04+00:00
- **Authors**: Ahmed Khan, Minoru Kuribayashi, KokSheik Wong, Vishnu Monn Baskaran
- **Comment**: None
- **Journal**: None
- **Summary**: High-dynamic range (HDR) images are circulated rapidly over the internet with risks of being exploited for unauthorized usage. To protect these images, some HDR image based watermarking (HDR-IW) methods were put forward. However, they inherited the same problem faced by conventional IW methods for standard dynamic range (SDR) images, where only trade-offs among conflicting requirements are managed instead of simultaneous improvement. In this paper, a novel saliency (eye-catching object) detection based trade-off independent HDR-IW is proposed, to simultaneously improve robustness, imperceptibility and payload. First, the host image goes through our proposed salient object detection model to produce a saliency map, which is, in turn, exploited to segment the foreground and background of the host image. Next, the binary watermark is partitioned into the foregrounds and backgrounds using the same mask and scrambled using a random permutation algorithm. Finally, the watermark segments are embedded into selected bit-plane of the corresponding host segments using quantized indexed modulation. Experimental results suggest that the proposed work outperforms state-of-the-art methods in terms of improving the conflicting requirements.



### Vision-Based Estimation of Small Body Rotational State during the Approach Phase
- **Arxiv ID**: http://arxiv.org/abs/2302.11364v2
- **DOI**: None
- **Categories**: **astro-ph.EP**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11364v2)
- **Published**: 2023-02-22 13:32:58+00:00
- **Updated**: 2023-08-06 15:36:48+00:00
- **Authors**: Paolo Panicucci, Jérémy Lebreton, Roland Brochard, Emmanuel Zenou, Michel Delpech
- **Comment**: None
- **Journal**: None
- **Summary**: The heterogeneity of the small body population complicates the prediction of small body properties before the spacecraft's arrival. In the context of autonomous small body exploration, it is crucial to develop algorithms that estimate the small body characteristics before orbit insertion and close proximity operations. This paper develops a vision-based estimation of the small-body rotational state (i.e., the center of rotation and rotation axis direction) during the approach phase. In this mission phase, the spacecraft observes the rotating celestial body and tracks features in images. As feature tracks are the projection of the landmarks' circular movement, the possible rotation axes are computed. Then, the rotation axis solution is chosen among the possible candidates by exploiting feature motion and a heuristic approach. Finally, the center of rotation is estimated from the center of brightness. The algorithm is tested on more than 800 test cases with two different asteroids (i.e., Bennu and Itokawa), three different lighting conditions, and more than 100 different rotation axis orientations. Each test case is composed of about 250 synthetic images of the asteroid which are used to track features and determine the rotational state. Results show that the error between the true rotation axis and its estimation is below $10^{\circ}$ for $80\%$ of the considered test cases, implying that the proposed algorithm is a suitable method for autonomous small body characterization.



### Entity-Level Text-Guided Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2302.11383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11383v1)
- **Published**: 2023-02-22 13:56:23+00:00
- **Updated**: 2023-02-22 13:56:23+00:00
- **Authors**: Yikai Wang, Jianan Wang, Guansong Lu, Hang Xu, Zhenguo Li, Wei Zhang, Yanwei Fu
- **Comment**: Extension of our CVPR 2022 oral paper: 2204.04428. Yikai Wang and
  Jianan Wang contribute equally. The arxiv version uses small size figures for
  fast preview, the full size pdf version can be found in our project page:
  https://yikai-wang.github.io/semani/. arXiv admin note: substantial text
  overlap with arXiv:2204.04428
- **Journal**: None
- **Summary**: Existing text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical applications. In this work, we study a novel task on text-guided image manipulation on the entity level in the real world (eL-TGIM). The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the entity-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose an elegant framework, dubbed as SeMani, forming the Semantic Manipulation of real-world images that can not only edit the appearance of entities but also generate new entities corresponding to the text guidance. To solve eL-TGIM, SeMani decomposes the task into two phases: the semantic alignment phase and the image manipulation phase. In the semantic alignment phase, SeMani incorporates a semantic alignment module to locate the entity-relevant region to be manipulated. In the image manipulation phase, SeMani adopts a generative model to synthesize new images conditioned on the entity-irrelevant regions and target text descriptions. We discuss and propose two popular generation processes that can be utilized in SeMani, the discrete auto-regressive generation with transformers and the continuous denoising generation with diffusion models, yielding SeMani-Trans and SeMani-Diff, respectively. We conduct extensive experiments on the real datasets CUB, Oxford, and COCO datasets to verify that SeMani can distinguish the entity-relevant and -irrelevant regions and achieve more precise and flexible manipulation in a zero-shot manner compared with baseline methods. Our codes and models will be released at https://github.com/Yikai-Wang/SeMani.



### ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms
- **Arxiv ID**: http://arxiv.org/abs/2302.11408v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11408v2)
- **Published**: 2023-02-22 14:43:33+00:00
- **Updated**: 2023-08-06 17:24:21+00:00
- **Authors**: Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia
- **Comment**: 18 pages, with 13 pages of main text
- **Journal**: None
- **Summary**: Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. We also provide procedures to adaptively select the number of suspicious points to remove. In the end-to-end SL setting, ASSET is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios; in particular, it is the only method that can detect the state-of-the-art clean-label attack. Moreover, ASSET's average detection rates are higher than the best existing methods in SSL and TL, respectively, by 69.3% and 33.2%, thus providing the first practical backdoor defense for these new DL settings. We open-source the project to drive further development and encourage engagement: https://github.com/ruoxi-jia-group/ASSET.



### Gradient Adjusting Networks for Domain Inversion
- **Arxiv ID**: http://arxiv.org/abs/2302.11413v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11413v1)
- **Published**: 2023-02-22 14:47:57+00:00
- **Updated**: 2023-02-22 14:47:57+00:00
- **Authors**: Erez Sheffi, Michael Rotman, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: StyleGAN2 was demonstrated to be a powerful image generation engine that supports semantic editing. However, in order to manipulate a real-world image, one first needs to be able to retrieve its corresponding latent representation in StyleGAN's latent space that is decoded to an image as close as possible to the desired image. For many real-world images, a latent representation does not exist, which necessitates the tuning of the generator network. We present a per-image optimization method that tunes a StyleGAN2 generator such that it achieves a local edit to the generator's weights, resulting in almost perfect inversion, while still allowing image editing, by keeping the rest of the mapping between an input latent representation tensor and an output image relatively intact. The method is based on a one-shot training of a set of shallow update networks (aka. Gradient Modification Modules) that modify the layers of the generator. After training the Gradient Modification Modules, a modified generator is obtained by a single application of these networks to the original parameters, and the previous editing capabilities of the generator are maintained. Our experiments show a sizable gap in performance over the current state of the art in this very active domain. Our code is available at \url{https://github.com/sheffier/gani}.



### Structure Embedded Nucleus Classification for Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2302.11416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11416v1)
- **Published**: 2023-02-22 14:52:06+00:00
- **Updated**: 2023-02-22 14:52:06+00:00
- **Authors**: Wei Lou, Xiang Wan, Guanbin Li, Xiaoying Lou, Chenghang Li, Feng Gao, Haofeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclei classification provides valuable information for histopathology image analysis. However, the large variations in the appearance of different nuclei types cause difficulties in identifying nuclei. Most neural network based methods are affected by the local receptive field of convolutions, and pay less attention to the spatial distribution of nuclei or the irregular contour shape of a nucleus. In this paper, we first propose a novel polygon-structure feature learning mechanism that transforms a nucleus contour into a sequence of points sampled in order, and employ a recurrent neural network that aggregates the sequential change in distance between key points to obtain learnable shape features. Next, we convert a histopathology image into a graph structure with nuclei as nodes, and build a graph neural network to embed the spatial distribution of nuclei into their representations. To capture the correlations between the categories of nuclei and their surrounding tissue patterns, we further introduce edge features that are defined as the background textures between adjacent nuclei. Lastly, we integrate both polygon and graph structure learning mechanisms into a whole framework that can extract intra and inter-nucleus structural characteristics for nuclei classification. Experimental results show that the proposed framework achieves significant improvements compared to the state-of-the-art methods.



### Enhanced Face Authentication With Separate Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/2302.11427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11427v1)
- **Published**: 2023-02-22 15:07:29+00:00
- **Updated**: 2023-02-22 15:07:29+00:00
- **Authors**: Anh-Kiet Duong, Hoang-Lan Nguyen, Toan-Thinh Truong
- **Comment**: in Vietnamese language
- **Journal**: None
- **Summary**: The overall objective of the main project is to propose and develop a system of facial authentication in unlocking phones or applications in phones using facial recognition. The system will include four separate architectures: face detection, face recognition, face spoofing, and classification of closed eyes. In which, we consider the problem of face recognition to be the most important, determining the true identity of the person standing in front of the screen with absolute accuracy is what facial recognition systems need to achieve. Along with the development of the face recognition problem, the problem of the anti-fake face is also gradually becoming popular and equally important. Our goal is to propose and develop two loss functions: LMCot and Double Loss. Then apply them to the face authentication process.



### Singular value decomposition based matrix surgery
- **Arxiv ID**: http://arxiv.org/abs/2302.11446v1
- **DOI**: None
- **Categories**: **math.AT**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11446v1)
- **Published**: 2023-02-22 15:30:08+00:00
- **Updated**: 2023-02-22 15:30:08+00:00
- **Authors**: Jehan Ghafuri, Sabah Jassim
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: This paper aims to develop a simple procedure to reduce and control the condition number of random matrices, and investigate the effect on the persistent homology (PH) of point clouds of well- and ill-conditioned matrices. For a square matrix generated randomly using Gaussian/Uniform distribution, the SVD-Surgery procedure works by: (1) computing its singular value decomposition (SVD), (2) replacing the diagonal factor by changing a list of the smaller singular values by a convex linear combination of the entries in the list, and (3) compute the new matrix by reversing the SVD. Applying SVD-Surgery on a matrix often results in having different diagonal factor to those of the input matrix. The spatial distribution of random square matrices are known to be correlated to the distribution of their condition numbers. The persistent homology (PH) investigations, therefore, are focused on comparing the effect of SVD-Surgery on point clouds of large datasets of randomly generated well-conditioned and ill-conditioned matrices, as well as that of the point clouds formed by their inverses. This work is motivated by the desire to stabilise the impact of Deep Learning (DL) training on medical images in terms of the condition numbers of their sets of convolution filters as a mean of reducing overfitting and improving robustness against tolerable amounts of image noise. When applied to convolution filters during training, the SVD-Surgery acts as a spectral regularisation of the DL model without the need for learning extra parameters. We shall demonstrate that for several point clouds of sufficiently large convolution filters our simple strategy preserve filters norm and reduces the norm of its inverse depending on the chosen linear combination parameters. Moreover, our approach showed significant improvements towards the well-conditioning of matrices and stable topological behaviour.



### Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2302.11458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11458v1)
- **Published**: 2023-02-22 15:53:00+00:00
- **Updated**: 2023-02-22 15:53:00+00:00
- **Authors**: Manuel Stoiber, Mariam Elsayed, Anne E. Reichert, Florian Steidle, Dongheui Lee, Rudolph Triebel
- **Comment**: Submitted to IEEE/RSJ International Conference on Intelligent Robots
- **Journal**: None
- **Summary**: In many applications of advanced robotic manipulation, six degrees of freedom (6DoF) object pose estimates are continuously required. In this work, we develop a multi-modality tracker that fuses information from visual appearance and geometry to estimate object poses. The algorithm extends our previous method ICG, which uses geometry, to additionally consider surface appearance. In general, object surfaces contain local characteristics from text, graphics, and patterns, as well as global differences from distinct materials and colors. To incorporate this visual information, two modalities are developed. For local characteristics, keypoint features are used to minimize distances between points from keyframes and the current image. For global differences, a novel region approach is developed that considers multiple regions on the object surface. In addition, it allows the modeling of external geometries. Experiments on the YCB-Video and OPT datasets demonstrate that our approach ICG+ performs best on both datasets, outperforming both conventional and deep learning-based methods. At the same time, the algorithm is highly efficient and runs at more than 300 Hz. The source code of our tracker is publicly available.



### Saliency Guided Contrastive Learning on Scene Images
- **Arxiv ID**: http://arxiv.org/abs/2302.11461v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11461v2)
- **Published**: 2023-02-22 15:54:07+00:00
- **Updated**: 2023-02-23 05:46:53+00:00
- **Authors**: Meilin Chen, Yizhou Wang, Shixiang Tang, Feng Zhu, Haiyang Yang, Lei Bai, Rui Zhao, Donglian Qi, Wanli Ouyang
- **Comment**: 12 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:2106.11952 by other authors
- **Journal**: None
- **Summary**: Self-supervised learning holds promise in leveraging large numbers of unlabeled data. However, its success heavily relies on the highly-curated dataset, e.g., ImageNet, which still needs human cleaning. Directly learning representations from less-curated scene images is essential for pushing self-supervised learning to a higher level. Different from curated images which include simple and clear semantic information, scene images are more complex and mosaic because they often include complex scenes and multiple objects. Despite being feasible, recent works largely overlooked discovering the most discriminative regions for contrastive learning to object representations in scene images. In this work, we leverage the saliency map derived from the model's output during learning to highlight these discriminative regions and guide the whole contrastive learning. Specifically, the saliency map first guides the method to crop its discriminative regions as positive pairs and then reweighs the contrastive losses among different crops by its saliency scores. Our method significantly improves the performance of self-supervised learning on scene images by +1.1, +4.3, +2.2 Top1 accuracy in ImageNet linear evaluation, Semi-supervised learning with 1% and 10% ImageNet labels, respectively. We hope our insights on saliency maps can motivate future research on more general-purpose unsupervised representation learning from scene data.



### Debiased Mapping for Full-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2302.11464v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11464v2)
- **Published**: 2023-02-22 15:57:03+00:00
- **Updated**: 2023-03-16 14:20:14+00:00
- **Authors**: Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mapping images to deep feature space for comparisons has been wildly adopted in recent learning-based full-reference image quality assessment (FR-IQA) models. Analogous to the classical classification task, the ideal mapping space for quality regression should possess both inter-class separability and intra-class compactness. The inter-class separability that focuses on the discrimination of images with different quality levels has been highly emphasized in existing models. However, the intra-class compactness that maintains small objective quality variance of images with the same or indistinguishable quality escapes the research attention, potentially leading to the perception-biased measures. In this paper, we reveal that such bias is mainly caused by the unsuitable subspace that the features are projected and compared in. To account for this, we develop the Debiased Mapping based quality Measure (DMM), which relies on the orthonormal bases of deep learning features formed by singular value decomposition (SVD). The SVD in deep learning feature domain, which overwhelmingly separates the quality variations with singular values and projection bases, facilitates the quality inference with dedicatedly designed distance measure. Experiments on different IQA databases demonstrate the mapping method is able to mitigate the perception bias efficiently, and the superior performance on quality prediction verifies the effectiveness of our method. The implementation will be publicly available.



### Distilling Calibrated Student from an Uncalibrated Teacher
- **Arxiv ID**: http://arxiv.org/abs/2302.11472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11472v1)
- **Published**: 2023-02-22 16:18:38+00:00
- **Updated**: 2023-02-22 16:18:38+00:00
- **Authors**: Ishan Mishra, Sethu Vamsi Krishna, Deepak Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation is a common technique for improving the performance of a shallow student network by transferring information from a teacher network, which in general, is comparatively large and deep. These teacher networks are pre-trained and often uncalibrated, as no calibration technique is applied to the teacher model while training. Calibration of a network measures the probability of correctness for any of its predictions, which is critical in high-risk domains. In this paper, we study how to obtain a calibrated student from an uncalibrated teacher. Our approach relies on the fusion of the data-augmentation techniques, including but not limited to cutout, mixup, and CutMix, with knowledge distillation. We extend our approach beyond traditional knowledge distillation and find it suitable for Relational Knowledge Distillation and Contrastive Representation Distillation as well. The novelty of the work is that it provides a framework to distill a calibrated student from an uncalibrated teacher model without compromising the accuracy of the distilled student. We perform extensive experiments to validate our approach on various datasets, including CIFAR-10, CIFAR-100, CINIC-10 and TinyImageNet, and obtained calibrated student models. We also observe robust performance of our approach while evaluating it on corrupted CIFAR-100C data.



### Transformer-Based Sensor Fusion for Autonomous Driving: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.11481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11481v1)
- **Published**: 2023-02-22 16:28:20+00:00
- **Updated**: 2023-02-22 16:28:20+00:00
- **Authors**: Apoorv Singh
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Transformers-based detection head and CNN-based feature encoder to extract features from raw sensor-data has emerged as one of the best performing sensor-fusion 3D-detection-framework, according to the dataset leaderboards. In this work we provide an in-depth literature survey of transformer based 3D-object detection task in the recent past, primarily focusing on the sensor fusion. We also briefly go through the Vision transformers (ViT) basics, so that readers can easily follow through the paper. Moreover, we also briefly go through few of the non-transformer based less-dominant methods for sensor fusion for autonomous driving. In conclusion we summarize with sensor-fusion trends to follow and provoke future research. More updated summary can be found at: https://github.com/ApoorvRoboticist/Transformers-Sensor-Fusion



### Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers
- **Arxiv ID**: http://arxiv.org/abs/2302.11488v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.1; I.4.0; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; I.5.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2302.11488v1)
- **Published**: 2023-02-22 16:44:41+00:00
- **Updated**: 2023-02-22 16:44:41+00:00
- **Authors**: Pranav Jeevan, Nikhil Cherian Kurian, Amit Sethi
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Convolution Neural Networks (CNNs) are widely used in medical image analysis, but their performance degrade when the magnification of testing images differ from the training images. The inability of CNNs to generalize across magnification scales can result in sub-optimal performance on external datasets. This study aims to evaluate the robustness of various deep learning architectures in the analysis of breast cancer histopathological images with varying magnification scales at training and testing stages. Here we explore and compare the performance of multiple deep learning architectures, including CNN-based ResNet and MobileNet, self-attention-based Vision Transformers and Swin Transformers, and token-mixing models, such as FNet, ConvMixer, MLP-Mixer, and WaveMix. The experiments are conducted using the BreakHis dataset, which contains breast cancer histopathological images at varying magnification levels. We show that performance of WaveMix is invariant to the magnification of training and testing data and can provide stable and good classification accuracy. These evaluations are critical in identifying deep learning architectures that can robustly handle changes in magnification scale, ensuring that scale changes across anatomical structures do not disturb the inference results.



### On The Role of Alias and Band-Shift for Sentinel-2 Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2302.11494v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11494v2)
- **Published**: 2023-02-22 17:08:45+00:00
- **Updated**: 2023-04-17 16:24:05+00:00
- **Authors**: Ngoc Long Nguyen, Jérémy Anger, Lara Raad, Bruno Galerne, Gabriele Facciolo
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: In this work, we study the problem of single-image super-resolution (SISR) of Sentinel-2 imagery. We show that thanks to its unique sensor specification, namely the inter-band shift and alias, that deep-learning methods are able to recover fine details. By training a model using a simple $L_1$ loss, results are free of hallucinated details. For this study, we build a dataset of pairs of images Sentinel-2/PlanetScope to train and evaluate our super-resolution (SR) model.



### S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.11506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11506v1)
- **Published**: 2023-02-22 17:23:33+00:00
- **Updated**: 2023-02-22 17:23:33+00:00
- **Authors**: Pranav Kadam, Hardik Prajapati, Min Zhang, Jintang Xue, Shan Liu, C. -C. Jay Kuo
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Many point cloud classification methods are developed under the assumption that all point clouds in the dataset are well aligned with the canonical axes so that the 3D Cartesian point coordinates can be employed to learn features. When input point clouds are not aligned, the classification performance drops significantly. In this work, we focus on a mathematically transparent point cloud classification method called PointHop, analyze its reason for failure due to pose variations, and solve the problem by replacing its pose dependent modules with rotation invariant counterparts. The proposed method is named SO(3)-Invariant PointHop (or S3I-PointHop in short). We also significantly simplify the PointHop pipeline using only one single hop along with multiple spatial aggregation techniques. The idea of exploiting more spatial information is novel. Experiments on the ModelNet40 dataset demonstrate the superiority of S3I-PointHop over traditional PointHop-like methods.



### Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.11510v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11510v4)
- **Published**: 2023-02-22 17:27:03+00:00
- **Updated**: 2023-03-30 15:00:52+00:00
- **Authors**: Guangyao Zheng, Samson Zhou, Vladimir Braverman, Michael A. Jacobs, Vishwa S. Parekh
- **Comment**: None
- **Journal**: None
- **Summary**: Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.   We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifelong learning models trained on a sequence of 10 different brain MR imaging environments demonstrated excellent performance localizing the ventricle with a mean pixel error distance of 12.93 for the compression ratio of 10x. In comparison, the conventional lifelong learning model localized the ventricle with a mean pixel distance of 10.87. Similarly, the coreset lifelong learning models trained on whole-body MRI demonstrated no significant difference (p=0.28) between the 10x compressed coreset lifelong learning models and conventional lifelong learning models for all the landmarks. The mean pixel distance for the 10x compressed models across all the landmarks was 25.30, compared to 19.24 for the conventional lifelong learning models. Our results demonstrate that the potential of the coreset-based ERB compression method for compressing experiences without a significant drop in performance.



### A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.11517v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11517v1)
- **Published**: 2023-02-22 17:39:00+00:00
- **Updated**: 2023-02-22 17:39:00+00:00
- **Authors**: Wei Tang, Yinxiao Wang, Kangning Cui, Raymond H. Chan
- **Comment**: 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a leading cause of blindness worldwide. Early diagnosis is essential in the treatment of diabetes and can assist in preventing vision impairment. Since manual annotation of medical images is time-consuming, costly, and prone to subjectivity that leads to inconsistent diagnoses, several deep learning segmentation approaches have been proposed to address these challenges. However, these networks often rely on simple loss functions, such as binary cross entropy (BCE), which may not be sophisticated enough to effectively segment lesions such as those present in DR. In this paper, we propose a loss function that incorporates a global segmentation loss, a patch-wise density loss, and a patch-wise edge-aware loss to improve the performance of these networks on the detection and segmentation of hard exudates. Comparing our proposed loss function against the BCE loss on several state-of-the-art networks, our experimental results reveal substantial improvement in network performance achieved by incorporating the patch-wise contrastive loss.



### Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.11522v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11522v1)
- **Published**: 2023-02-22 17:47:37+00:00
- **Updated**: 2023-02-22 17:47:37+00:00
- **Authors**: Olivier Rukundo
- **Comment**: 4 pages, 10 figure, 7 tables. arXiv admin note: text overlap with
  arXiv:2101.11508
- **Journal**: None
- **Summary**: In this study, the author evaluated the use of an extra pixel interpolation algorithm with mask processing versus non-extra pixel interpolation algorithm when interpolating training dataset images and masks for medical image segmentation with deep learning. The author also examined scenarios of interpolating dataset images and masks using different algorithms: extra pixel for interpolating dataset images and non-extra pixel for interpolating dataset masks. The evaluation outcomes revealed that training on datasets consisting of images and masks both interpolated using the extra pixel bicubic interpolation (BIC) resulted in better segmentation accuracy compared to using either the non-extra pixel nearest neighbor interpolation (NN) or BIC for dataset images and NN for dataset masks. Specifically, the evaluation revealed that the BIC-BIC network was a 8.9578 % (with image size 256 x 256) and a 1.0496 % (with image size 384 x 384) increase of NN-NN network compared to the NN-BIC network which was a 8.3127 % (with image size 256 x 256) and a 0.2887 % (with image size 384 x 384) increase of NN-NN network.



### Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.11524v1
- **DOI**: 10.1145/3574198.3574205
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11524v1)
- **Published**: 2023-02-22 17:54:39+00:00
- **Updated**: 2023-02-22 17:54:39+00:00
- **Authors**: Deepak Raina, Kashish Verma, SH Chandrashekhara, Subir Kumar Saha
- **Comment**: Accepted in 9th ACM International Conference on Biomedical and
  Bioinformatics Engineering (ICBBE) 2022 http://www.icbbe.com/
- **Journal**: Proceedings of the 2022 9th International Conference on Biomedical
  and Bioinformatics Engineering, pp. 41-48
- **Summary**: We investigate the applicability of U-Net based models for segmenting Urinary Bladder (UB) in male pelvic view UltraSound (US) images. The segmentation of UB in the US image aids radiologists in diagnosing the UB. However, UB in US images has arbitrary shapes, indistinct boundaries and considerably large inter- and intra-subject variability, making segmentation a quite challenging task. Our study of the state-of-the-art (SOTA) segmentation network, U-Net, for the problem reveals that it often fails to capture the salient characteristics of UB due to the varying shape and scales of anatomy in the noisy US image. Also, U-net has an excessive number of trainable parameters, reporting poor computational efficiency during training. We propose a Slim U-Net to address the challenges of UB segmentation. Slim U-Net proposes to efficiently preserve the salient features of UB by reshaping the structure of U-Net using a less number of 2D convolution layers in the contracting path, in order to preserve and impose them on expanding path. To effectively distinguish the blurred boundaries, we propose a novel annotation methodology, which includes the background area of the image at the boundary of a marked region of interest (RoI), thereby steering the model's attention towards boundaries. In addition, we suggested a combination of loss functions for network training in the complex segmentation of UB. The experimental results demonstrate that Slim U-net is statistically superior to U-net for UB segmentation. The Slim U-net further decreases the number of trainable parameters and training time by 54% and 57.7%, respectively, compared to the standard U-Net, without compromising the segmentation accuracy.



### A study on the invariance in security whatever the dimension of images for the steganalysis by deep-learning
- **Arxiv ID**: http://arxiv.org/abs/2302.11527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2302.11527v1)
- **Published**: 2023-02-22 18:09:15+00:00
- **Updated**: 2023-02-22 18:09:15+00:00
- **Authors**: Kévin Planolles, Marc Chaumont, Frédéric Comby
- **Comment**: PAPER ACCEPTED TO IEEE ICASSP'2023, GREEK ISLAND OF RHODES, 4-10 JUNE
  2023. FAQ can be found there
  https://www.lirmm.fr/~chaumont/publications/QA-ICASSP2023.pdf
- **Journal**: None
- **Summary**: In this paper, we study the performance invariance of convolutional neural networks when confronted with variable image sizes in the context of a more "wild steganalysis". First, we propose two algorithms and definitions for a fine experimental protocol with datasets owning "similar difficulty" and "similar security". The "smart crop 2" algorithm allows the introduction of the Nearly Nested Image Datasets (NNID) that ensure "a similar difficulty" between various datasets, and a dichotomous research algorithm allows a "similar security". Second, we show that invariance does not exist in state-of-the-art architectures. We also exhibit a difference in behavior depending on whether we test on images larger or smaller than the training images. Finally, based on the experiments, we propose to use the dilated convolution which leads to an improvement of a state-of-the-art architecture.



### Scaling Robot Learning with Semantically Imagined Experience
- **Arxiv ID**: http://arxiv.org/abs/2302.11550v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11550v1)
- **Published**: 2023-02-22 18:47:51+00:00
- **Updated**: 2023-02-22 18:47:51+00:00
- **Authors**: Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Dee M, Jodilyn Peralta, Brian Ichter, Karol Hausman, Fei Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which are challenging to scale. To mitigate this issue, we propose an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. We term our method Robot Learning with Semantically Imagened Experience (ROSIE). Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting various unseen objects for manipulation, backgrounds, and distractors with text guidance. Through extensive real-world experiments, we show that manipulation policies trained on data augmented this way are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation. The project's website and videos can be found at diffusion-rosie.github.io



### Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC
- **Arxiv ID**: http://arxiv.org/abs/2302.11552v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.11552v3)
- **Published**: 2023-02-22 18:48:46+00:00
- **Updated**: 2023-06-08 17:39:01+00:00
- **Authors**: Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl
- **Comment**: ICML 2023, Project Webpage:
  https://energy-based-model.github.io/reduce-reuse-recycle/
- **Journal**: None
- **Summary**: Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide set of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.



### K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.11557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11557v2)
- **Published**: 2023-02-22 18:53:57+00:00
- **Updated**: 2023-02-26 00:34:58+00:00
- **Authors**: Chaoyi Wu, Xiaoman Zhang, Yanfeng Wang, Ya Zhang, Weidi Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge. In particular, we make the following contributions: First, to explicitly incorporate experts' knowledge, we propose to learn a neural representation for the medical knowledge graph via contrastive learning, implicitly establishing relations between different medical concepts. Second, while training the visual encoder, we keep the parameters of the knowledge encoder frozen and propose to learn a set of prompt vectors for efficient adaptation. Third, we adopt a Transformer-based disease-query module for cross-model fusion, which naturally enables explainable diagnosis results via cross attention. To validate the effectiveness of our proposed framework, we conduct thorough experiments on three x-ray imaging datasets across different anatomy structures, showing our model is able to exploit the implicit relations between diseases/findings, thus is beneficial to the commonly encountered problem in the medical domain, namely, long-tailed and zero-shot recognition, which conventional methods either struggle or completely fail to realize.



### Word level Bangla Sign Language Dataset for Continuous BSL Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.11559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11559v2)
- **Published**: 2023-02-22 18:55:54+00:00
- **Updated**: 2023-04-09 18:48:21+00:00
- **Authors**: Md Shamimul Islam, A. J. M. Akhtarujjaman Joha, Md Nur Hossain, Sohaib Abdullah, Ibrahim Elwarfalli, Md Mahedi Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: An robust sign language recognition system can greatly alleviate communication barriers, particularly for people who struggle with verbal communication. This is crucial for human growth and progress as it enables the expression of thoughts, feelings, and ideas. However, sign recognition is a complex task that faces numerous challenges such as same gesture patterns for multiple signs, lighting, clothing, carrying conditions, and the presence of large poses, as well as illumination discrepancies across different views. Additionally, the absence of an extensive Bangla sign language video dataset makes it even more challenging to operate recognition systems, particularly when utilizing deep learning techniques. In order to address this issue, firstly, we created a large-scale dataset called the MVBSL-W50, which comprises 50 isolated words across 13 categories. Secondly, we developed an attention-based Bi-GRU model that captures the temporal dynamics of pose information for individuals communicating through sign language. The proposed model utilizes human pose information, which has shown to be successful in analyzing sign language patterns. By focusing solely on movement information and disregarding body appearance and environmental factors, the model is simplified and can achieve a speedier performance. The accuracy of the model is reported to be 85.64%.



### Uncovering Bias in Face Generation Models
- **Arxiv ID**: http://arxiv.org/abs/2302.11562v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.11562v1)
- **Published**: 2023-02-22 18:57:35+00:00
- **Updated**: 2023-02-22 18:57:35+00:00
- **Authors**: Cristian Muñoz, Sara Zannone, Umar Mohammed, Adriano Koshiyama
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in GANs and diffusion models have enabled the creation of high-resolution, hyper-realistic images. However, these models may misrepresent certain social groups and present bias. Understanding bias in these models remains an important research question, especially for tasks that support critical decision-making and could affect minorities. The contribution of this work is a novel analysis covering architectures and embedding spaces for fine-grained understanding of bias over three approaches: generators, attribute modifier, and post-processing bias mitigators. This work shows that generators suffer from bias across all social groups with attribute preferences such as between 75%-85% for whiteness and 60%-80% for the female gender (for all trained CelebA models) and low probabilities of generating children and older men. Modifier and mitigators work as post-processor and change the generator performance. For instance, attribute channel perturbation strategies modify the embedding spaces. We quantify the influence of this change on group fairness by measuring the impact on image quality and group features. Specifically, we use the Fr\'echet Inception Distance (FID), the Face Matching Error and the Self-Similarity score. For Interfacegan, we analyze one and two attribute channel perturbations and examine the effect on the fairness distribution and the quality of the image. Finally, we analyzed the post-processing bias mitigators, which are the fastest and most computationally efficient way to mitigate bias. We find that these mitigation techniques show similar results on KL divergence and FID score, however, self-similarity scores show a different feature concentration on the new groups of the data distribution. The weaknesses and ongoing challenges described in this work must be considered in the pursuit of creating fair and unbiased face generation models.



### Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2302.11566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11566v1)
- **Published**: 2023-02-22 18:59:17+00:00
- **Updated**: 2023-02-22 18:59:17+00:00
- **Authors**: Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, Otmar Hilliges
- **Comment**: Project page: https://moygcc.github.io/vid2avatar/
- **Journal**: None
- **Summary**: We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.



### MVTrans: Multi-View Perception of Transparent Objects
- **Arxiv ID**: http://arxiv.org/abs/2302.11683v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.11683v1)
- **Published**: 2023-02-22 22:45:28+00:00
- **Updated**: 2023-02-22 22:45:28+00:00
- **Authors**: Yi Ru Wang, Yuchi Zhao, Haoping Xu, Saggi Eppel, Alan Aspuru-Guzik, Florian Shkurti, Animesh Garg
- **Comment**: Accepted to ICRA 2023; 6 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Transparent object perception is a crucial skill for applications such as robot manipulation in household and laboratory settings. Existing methods utilize RGB-D or stereo inputs to handle a subset of perception tasks including depth and pose estimation. However, transparent object perception remains to be an open problem. In this paper, we forgo the unreliable depth map from RGB-D sensors and extend the stereo based method. Our proposed method, MVTrans, is an end-to-end multi-view architecture with multiple perception capabilities, including depth estimation, segmentation, and pose estimation. Additionally, we establish a novel procedural photo-realistic dataset generation pipeline and create a large-scale transparent object detection dataset, Syn-TODD, which is suitable for training networks with all three modalities, RGB-D, stereo and multi-view RGB. Project Site: https://ac-rad.github.io/MVTrans/



### fAIlureNotes: Supporting Designers in Understanding the Limits of AI Models for Computer Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2302.11703v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.11703v1)
- **Published**: 2023-02-22 23:41:36+00:00
- **Updated**: 2023-02-22 23:41:36+00:00
- **Authors**: Steven Moore, Q. Vera Liao, Hariharan Subramonyam
- **Comment**: None
- **Journal**: None
- **Summary**: To design with AI models, user experience (UX) designers must assess the fit between the model and user needs. Based on user research, they need to contextualize the model's behavior and potential failures within their product-specific data instances and user scenarios. However, our formative interviews with ten UX professionals revealed that such a proactive discovery of model limitations is challenging and time-intensive. Furthermore, designers often lack technical knowledge of AI and accessible exploration tools, which challenges their understanding of model capabilities and limitations. In this work, we introduced a failure-driven design approach to AI, a workflow that encourages designers to explore model behavior and failure patterns early in the design process. The implementation of fAIlureNotes, a designer-centered failure exploration and analysis tool, supports designers in evaluating models and identifying failures across diverse user groups and scenarios. Our evaluation with UX practitioners shows that fAIlureNotes outperforms today's interactive model cards in assessing context-specific model performance.



### ACE: Zero-Shot Image to Image Translation via Pretrained Auto-Contrastive-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2302.11705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.11705v1)
- **Published**: 2023-02-22 23:52:23+00:00
- **Updated**: 2023-02-22 23:52:23+00:00
- **Authors**: Sihan Xu, Zelong Jiang, Ruisi Liu, Kaikai Yang, Zhijie Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation is a fundamental task in computer vision. It transforms images from one domain to images in another domain so that they have particular domain-specific characteristics. Most prior works train a generative model to learn the mapping from a source domain to a target domain. However, learning such mapping between domains is challenging because data from different domains can be highly unbalanced in terms of both quality and quantity. To address this problem, we propose a new approach to extract image features by learning the similarities and differences of samples within the same data distribution via a novel contrastive learning framework, which we call Auto-Contrastive-Encoder (ACE). ACE learns the content code as the similarity between samples with the same content information and different style perturbations. The design of ACE enables us to achieve zero-shot image-to-image translation with no training on image translation tasks for the first time.   Moreover, our learning method can learn the style features of images on different domains effectively. Consequently, our model achieves competitive results on multimodal image translation tasks with zero-shot learning as well. Additionally, we demonstrate the potential of our method in transfer learning. With fine-tuning, the quality of translated images improves in unseen domains. Even though we use contrastive learning, all of our training can be performed on a single GPU with the batch size of 8.



