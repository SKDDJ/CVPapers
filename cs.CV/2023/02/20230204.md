# Arxiv Papers in cs.CV on 2023-02-04
### Semantic Diffusion Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02057v1)
- **Published**: 2023-02-04 01:39:16+00:00
- **Updated**: 2023-02-04 01:39:16+00:00
- **Authors**: Haoru Tan, Sitong Wu, Jimin Pi
- **Comment**: Accepted by NeurIPS2022
- **Journal**: None
- **Summary**: Precise and accurate predictions over boundary areas are essential for semantic segmentation. However, the commonly-used convolutional operators tend to smooth and blur local detail cues, making it difficult for deep models to generate accurate boundary predictions. In this paper, we introduce an operator-level approach to enhance semantic boundary awareness, so as to improve the prediction of the deep semantic segmentation model. Specifically, we first formulate the boundary feature enhancement as an anisotropic diffusion process. We then propose a novel learnable approach called semantic diffusion network (SDN) to approximate the diffusion process, which contains a parameterized semantic difference convolution operator followed by a feature fusion module. Our SDN aims to construct a differentiable mapping from the original feature to the inter-class boundary-enhanced feature. The proposed SDN is an efficient and flexible module that can be easily plugged into existing encoder-decoder segmentation models. Extensive experiments show that our approach can achieve consistent improvements over several typical and state-of-the-art segmentation baseline models on challenging public benchmarks. The code will be released soon.



### Semantic-Guided Image Augmentation with Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2302.02070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02070v1)
- **Published**: 2023-02-04 02:47:41+00:00
- **Updated**: 2023-02-04 02:47:41+00:00
- **Authors**: Bohan Li, Xinghao Wang, Xiao Xu, Yutai Hou, Yunlong Feng, Feng Wang, Wanxiang Che
- **Comment**: 15 pages, 13 figures, 7 tables
- **Journal**: None
- **Summary**: Image augmentation is a common mechanism to alleviate data scarcity in computer vision. Existing image augmentation methods often apply pre-defined transformations or mixup to augment the original image, but only locally vary the image. This makes them struggle to find a balance between maintaining semantic information and improving the diversity of augmented images. In this paper, we propose a Semantic-guided Image augmentation method with Pre-trained models (SIP). Specifically, SIP constructs prompts with image labels and captions to better guide the image-to-image generation process of the pre-trained Stable Diffusion model. The semantic information contained in the original images can be well preserved, and the augmented images still maintain diversity. Experimental results show that SIP can improve two commonly used backbones, i.e., ResNet-50 and ViT, by 12.60% and 2.07% on average over seven datasets, respectively. Moreover, SIP not only outperforms the best image augmentation baseline RandAugment by 4.46% and 1.23% on two backbones, but also further improves the performance by integrating naturally with the baseline. A detailed analysis of SIP is presented, including the diversity of augmented images, an ablation study on textual prompts, and a case study on the generated images.



### GDB: Gated convolutions-based Document Binarization
- **Arxiv ID**: http://arxiv.org/abs/2302.02073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02073v1)
- **Published**: 2023-02-04 02:56:40+00:00
- **Updated**: 2023-02-04 02:56:40+00:00
- **Authors**: Zongyuan Yang, Yongping Xiong, Guibin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Document binarization is a key pre-processing step for many document analysis tasks. However, existing methods can not extract stroke edges finely, mainly due to the fair-treatment nature of vanilla convolutions and the extraction of stroke edges without adequate supervision by boundary-related information. In this paper, we formulate text extraction as the learning of gating values and propose an end-to-end gated convolutions-based network (GDB) to solve the problem of imprecise stroke edge extraction. The gated convolutions are applied to selectively extract the features of strokes with different attention. Our proposed framework consists of two stages. Firstly, a coarse sub-network with an extra edge branch is trained to get more precise feature maps by feeding a priori mask and edge. Secondly, a refinement sub-network is cascaded to refine the output of the first stage by gated convolutions based on the sharp edge. For global information, GDB also contains a multi-scale operation to combine local and global features. We conduct comprehensive experiments on ten Document Image Binarization Contest (DIBCO) datasets from 2009 to 2019. Experimental results show that our proposed methods outperform the state-of-the-art methods in terms of all metrics on average and achieve top ranking on six benchmark datasets.



### X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2302.02075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02075v1)
- **Published**: 2023-02-04 03:16:18+00:00
- **Updated**: 2023-02-04 03:16:18+00:00
- **Authors**: Leqi Shen, Tao He, Yuchen Guo, Guiguang Ding
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Currently, most existing person re-identification methods use Instance-Level features, which are extracted only from a single image. However, these Instance-Level features can easily ignore the discriminative information due to the appearance of each identity varies greatly in different images. Thus, it is necessary to exploit Identity-Level features, which can be shared across different images of each identity. In this paper, we propose to promote Instance-Level features to Identity-Level features by employing cross-attention to incorporate information from one image to another of the same identity, thus more unified and discriminative pedestrian information can be obtained. We propose a novel training framework named X-ReID. Specifically, a Cross Intra-Identity Instances module (IntraX) fuses different intra-identity instances to transfer Identity-Level knowledge and make Instance-Level features more compact. A Cross Inter-Identity Instances module (InterX) involves hard positive and hard negative instances to improve the attention response to the same identity instead of different identity, which minimizes intra-identity variation and maximizes inter-identity variation. Extensive experiments on benchmark datasets show the superiority of our method over existing works. Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP improvements when compared to the second place.



### AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2302.02088v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2302.02088v2)
- **Published**: 2023-02-04 04:17:19+00:00
- **Updated**: 2023-02-07 17:38:18+00:00
- **Authors**: Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Human perception of the complex world relies on a comprehensive analysis of multi-modal signals, and the co-occurrences of audio and video signals provide humans with rich cues. This paper focuses on novel audio-visual scene synthesis in the real world. Given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that audio-visual scene. Directly using a NeRF-based model for audio synthesis is insufficient due to its lack of prior knowledge and acoustic supervision. To tackle the challenges, we first propose an acoustic-aware audio generation module that integrates our prior knowledge of audio propagation into NeRF, in which we associate audio generation with the 3D geometry of the visual environment. In addition, we propose a coordinate transformation module that expresses a viewing direction relative to the sound source. Such a direction transformation helps the model learn sound source-centric acoustic fields. Moreover, we utilize a head-related impulse response function to synthesize pseudo binaural audio for data augmentation that strengthens training. We qualitatively and quantitatively demonstrate the advantage of our model on real-world audio-visual scenes. We refer interested readers to view our video results for convincing comparisons.



### MOMA:Distill from Self-Supervised Teachers
- **Arxiv ID**: http://arxiv.org/abs/2302.02089v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02089v1)
- **Published**: 2023-02-04 04:23:52+00:00
- **Updated**: 2023-02-04 04:23:52+00:00
- **Authors**: Yuchong Yao, Nandakishor Desai, Marimuthu Palaniswami
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive Learning and Masked Image Modelling have demonstrated exceptional performance on self-supervised representation learning, where Momentum Contrast (i.e., MoCo) and Masked AutoEncoder (i.e., MAE) are the state-of-the-art, respectively. In this work, we propose MOMA to distill from pre-trained MoCo and MAE in a self-supervised manner to collaborate the knowledge from both paradigms. We introduce three different mechanisms of knowledge transfer in the propsoed MOMA framework. : (1) Distill pre-trained MoCo to MAE. (2) Distill pre-trained MAE to MoCo (3) Distill pre-trained MoCo and MAE to a random initialized student. During the distillation, the teacher and the student are fed with original inputs and masked inputs, respectively. The learning is enabled by aligning the normalized representations from the teacher and the projected representations from the student. This simple design leads to efficient computation with extremely high mask ratio and dramatically reduced training epochs, and does not require extra considerations on the distillation target. The experiments show MOMA delivers compact student models with comparable performance to existing state-of-the-art methods, combining the power of both self-supervised learning paradigms. It presents competitive results against different benchmarks in computer vision. We hope our method provides an insight on transferring and adapting the knowledge from large-scale pre-trained models in a computationally efficient way.



### Reducing ANN-SNN Conversion Error through Residual Membrane Potential
- **Arxiv ID**: http://arxiv.org/abs/2302.02091v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02091v1)
- **Published**: 2023-02-04 04:44:31+00:00
- **Updated**: 2023-02-04 04:44:31+00:00
- **Authors**: Zecheng Hao, Tong Bu, Jianhao Ding, Tiejun Huang, Zhaofei Yu
- **Comment**: Accepted as a AAAI 2023 Oral Paper
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have received extensive academic attention due to the unique properties of low power consumption and high-speed computing on neuromorphic chips. Among various training methods of SNNs, ANN-SNN conversion has shown the equivalent level of performance as ANNs on large-scale datasets. However, unevenness error, which refers to the deviation caused by different temporal sequences of spike arrival on activation layers, has not been effectively resolved and seriously suffers the performance of SNNs under the condition of short time-steps. In this paper, we make a detailed analysis of unevenness error and divide it into four categories. We point out that the case of the ANN output being zero while the SNN output being larger than zero accounts for the largest percentage. Based on this, we theoretically prove the sufficient and necessary conditions of this case and propose an optimization strategy based on residual membrane potential to reduce unevenness error. The experimental results show that the proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets. For example, we reach top-1 accuracy of 64.32\% on ImageNet with 10-steps. To the best of our knowledge, this is the first time ANN-SNN conversion can simultaneously achieve high accuracy and ultra-low-latency on the complex dataset. Code is available at https://github.com/hzc1208/ANN2SNN\_SRP.



### PartitionVAE -- a human-interpretable VAE
- **Arxiv ID**: http://arxiv.org/abs/2302.03689v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.03689v1)
- **Published**: 2023-02-04 05:22:19+00:00
- **Updated**: 2023-02-04 05:22:19+00:00
- **Authors**: Fareed Sheriff, Sameer Pai
- **Comment**: 13 pages, 18 figures
- **Journal**: None
- **Summary**: VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space feature represented by one unit of the representation layer does not necessarily have high correlation with the feature represented by a neighboring unit of the representation layer. This makes it difficult to interpret VAEs since the representation layer is not structured in a way that is easy for humans to parse. We aim to make a more interpretable VAE by partitioning the representation layer into disjoint sets of units. Partitioning the representation layer into disjoint sets of interconnected units yields a prior that features of the input space to this new VAE, which we call a partition VAE or PVAE, are grouped together by correlation -- for example, if our image space were the space of all ping ping game images (a somewhat complex image space we use to test our architecture) then we would hope the partitions in the representation layer each learned some large feature of the image like the characteristics of the ping pong table or the characteristics and position of the players or the ball. We also add to the PVAE a cost-saving measure: subresolution. Because we do not have access to GPU training environments for long periods of time and Google Colab Pro costs money, we attempt to decrease the complexity of the PVAE by outputting an image with dimensions scaled down from the input image by a constant factor, thus forcing the model to output a smaller version of the image. We then increase the resolution to calculate loss and train by interpolating through neighboring pixels. We train a tuned PVAE on MNIST and Sports10 to test its effectiveness.



### Knowledge Distillation in Vision Transformers: A Critical Review
- **Arxiv ID**: http://arxiv.org/abs/2302.02108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02108v1)
- **Published**: 2023-02-04 06:30:57+00:00
- **Updated**: 2023-02-04 06:30:57+00:00
- **Authors**: Gousia Habib, Tausifa Jan Saleem, Brejesh Lall
- **Comment**: 28pages, 16 figures
- **Journal**: None
- **Summary**: In Natural Language Processing (NLP), Transformers have already revolutionized the field by utilizing an attention-based encoder-decoder model. Recently, some pioneering works have employed Transformer-like architectures in Computer Vision (CV) and they have reported outstanding performance of these architectures in tasks such as image classification, object detection, and semantic segmentation. Vision Transformers (ViTs) have demonstrated impressive performance improvements over Convolutional Neural Networks (CNNs) due to their competitive modelling capabilities. However, these architectures demand massive computational resources which makes these models difficult to be deployed in the resource-constrained applications. Many solutions have been developed to combat this issue, such as compressive transformers and compression functions such as dilated convolution, min-max pooling, 1D convolution, etc. Model compression has recently attracted considerable research attention as a potential remedy. A number of model compression methods have been proposed in the literature such as weight quantization, weight multiplexing, pruning and Knowledge Distillation (KD). However, techniques like weight quantization, pruning and weight multiplexing typically involve complex pipelines for performing the compression. KD has been found to be a simple and much effective model compression technique that allows a relatively simple model to perform tasks almost as accurately as a complex model. This paper discusses various approaches based upon KD for effective compression of ViT models. The paper elucidates the role played by KD in reducing the computational and memory requirements of these models. The paper also presents the various challenges faced by ViTs that are yet to be resolved.



### Learning to Agree on Vision Attention for Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2302.02117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02117v2)
- **Published**: 2023-02-04 07:02:29+00:00
- **Updated**: 2023-02-19 06:44:39+00:00
- **Authors**: Zhenyang Li, Yangyang Guo, Kejie Wang, Fan Liu, Liqiang Nie, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Commonsense Reasoning (VCR) remains a significant yet challenging research problem in the realm of visual reasoning. A VCR model generally aims at answering a textual question regarding an image, followed by the rationale prediction for the preceding answering process. Though these two processes are sequential and intertwined, existing methods always consider them as two independent matching-based instances. They, therefore, ignore the pivotal relationship between the two processes, leading to sub-optimal model performance. This paper presents a novel visual attention alignment method to efficaciously handle these two processes in a unified framework. To achieve this, we first design a re-attention module for aggregating the vision attention map produced in each process. Thereafter, the resultant two sets of attention maps are carefully aligned to guide the two processes to make decisions based on the same image regions. We apply this method to both conventional attention and the recent Transformer models and carry out extensive experiments on the VCR benchmark dataset. The results demonstrate that with the attention alignment module, our method achieves a considerable improvement over the baseline methods, evidently revealing the feasibility of the coupling of the two processes as well as the effectiveness of the proposed method.



### Transform, Contrast and Tell: Coherent Entity-Aware Multi-Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2302.02124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02124v1)
- **Published**: 2023-02-04 07:50:31+00:00
- **Updated**: 2023-02-04 07:50:31+00:00
- **Authors**: Jingqiang Chen
- **Comment**: 28 pages, 9 tables, 3 figures
- **Journal**: None
- **Summary**: Coherent entity-aware multi-image captioning aims to generate coherent captions for multiple adjacent images in a news document. There are coherence relationships among adjacent images because they often describe same entities or events. These relationships are important for entity-aware multi-image captioning, but are neglected in entity-aware single-image captioning. Most existing work focuses on single-image captioning, while multi-image captioning has not been explored before. Hence, this paper proposes a coherent entity-aware multi-image captioning model by making use of coherence relationships. The model consists of a Transformer-based caption generation model and two types of contrastive learning-based coherence mechanisms. The generation model generates the caption by paying attention to the image and the accompanying text. The horizontal coherence mechanism aims to the make the caption coherent with captions of adjacent images. The vertical coherence mechanism aims to make the caption coherent with the image and the accompanying text. To evaluate coherence between captions, two coherence evaluation metrics are proposed. The new dataset DM800K is constructed that has more images per document than two existing datasets GoodNews and NYT800K, and are more suitable for multi-image captioning. Experiments on three datasets show the proposed captioning model outperforms 6 baselines according to single-image captioning evaluations, and the generated captions are more coherent than that of baselines according to coherence evaluations and human evaluations.



### Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity
- **Arxiv ID**: http://arxiv.org/abs/2302.02125v1
- **DOI**: 10.1109/TMI.2023.3269523
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02125v1)
- **Published**: 2023-02-04 07:55:30+00:00
- **Updated**: 2023-02-04 07:55:30+00:00
- **Authors**: Hao Du, Qihua Dong, Yan Xu, Jing Liao
- **Comment**: Weakly-supervised Segmentation, Medical Image Segmentation,
  Contrastive Similarity, Geometric Prior, Point Cloud
- **Journal**: IEEE Trans. Med. Imaging, Early Access, pp. 1-1, April 24, 2023
- **Summary**: Medical image segmentation is almost the most important pre-processing procedure in computer-aided diagnosis but is also a very challenging task due to the complex shapes of segments and various artifacts caused by medical imaging, (i.e., low-contrast tissues, and non-homogenous textures). In this paper, we propose a simple yet effective segmentation framework that incorporates the geometric prior and contrastive similarity into the weakly-supervised segmentation framework in a loss-based fashion. The proposed geometric prior built on point cloud provides meticulous geometry to the weakly-supervised segmentation proposal, which serves as better supervision than the inherent property of the bounding-box annotation (i.e., height and width). Furthermore, we propose contrastive similarity to encourage organ pixels to gather around in the contrastive embedding space, which helps better distinguish low-contrast tissues. The proposed contrastive embedding space can make up for the poor representation of the conventionally-used gray space. Extensive experiments are conducted to verify the effectiveness and the robustness of the proposed weakly-supervised segmentation framework. The proposed framework is superior to state-of-the-art weakly-supervised methods on the following publicly accessible datasets: LiTS 2017 Challenge, KiTS 2021 Challenge, and LPBA40. We also dissect our method and evaluate the performance of each component.



### Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.02136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02136v2)
- **Published**: 2023-02-04 09:14:18+00:00
- **Updated**: 2023-03-05 10:09:11+00:00
- **Authors**: Min Peng, Chongyang Wang, Yu Shi, Xiang-Dong Zhou
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: This paper presents a new method for end-to-end Video Question Answering (VideoQA), aside from the current popularity of using large-scale pre-training with huge feature extractors. We achieve this with a pyramidal multimodal transformer (PMT) model, which simply incorporates a learnable word embedding layer, a few convolutional and transformer layers. We use the anisotropic pyramid to fulfill video-language interactions across different spatio-temporal scales. In addition to the canonical pyramid, which includes both bottom-up and top-down pathways with lateral connections, novel strategies are proposed to decompose the visual feature stream into spatial and temporal sub-streams at different scales and implement their interactions with the linguistic semantics while preserving the integrity of local and global semantics. We demonstrate better or on-par performances with high computational efficiency against state-of-the-art methods on five VideoQA benchmarks. Our ablation study shows the scalability of our model that achieves competitive results for text-to-video retrieval by leveraging feature extractors with reusable pre-trained weights, and also the effectiveness of the pyramid.



### LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.02141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.02141v1)
- **Published**: 2023-02-04 10:22:18+00:00
- **Updated**: 2023-02-04 10:22:18+00:00
- **Authors**: Feng Xue, Yu Li, Deyin Liu, Yincen Xie, Lin Wu, Richang Hong
- **Comment**: Under review
- **Journal**: None
- **Summary**: Lipreading refers to understanding and further translating the speech of a speaker in the video into natural language. State-of-the-art lipreading methods excel in interpreting overlap speakers, i.e., speakers appear in both training and inference sets. However, generalizing these methods to unseen speakers incurs catastrophic performance degradation due to the limited number of speakers in training bank and the evident visual variations caused by the shape/color of lips for different speakers. Therefore, merely depending on the visible changes of lips tends to cause model overfitting. To address this problem, we propose to use multi-modal features across visual and landmarks, which can describe the lip motion irrespective to the speaker identities. Then, we develop a sentence-level lipreading framework based on visual-landmark transformers, namely LipFormer. Specifically, LipFormer consists of a lip motion stream, a facial landmark stream, and a cross-modal fusion. The embeddings from the two streams are produced by self-attention, which are fed to the cross-attention module to achieve the alignment between visuals and landmarks. Finally, the resulting fused features can be decoded to output texts by a cascade seq2seq model. Experiments demonstrate that our method can effectively enhance the model generalization to unseen speakers.



### This Intestine Does Not Exist: Multiscale Residual Variational Autoencoder for Realistic Wireless Capsule Endoscopy Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2302.02150v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02150v2)
- **Published**: 2023-02-04 11:49:38+00:00
- **Updated**: 2023-02-07 03:50:25+00:00
- **Authors**: Dimitrios E. Diamantis, Panagiota Gatoula, Anastasios Koulaouzidis, Dimitris K. Iakovidis
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Medical image synthesis has emerged as a promising solution to address the limited availability of annotated medical data needed for training machine learning algorithms in the context of image-based Clinical Decision Support (CDS) systems. To this end, Generative Adversarial Networks (GANs) have been mainly applied to support the algorithm training process by generating synthetic images for data augmentation. However, in the field of Wireless Capsule Endoscopy (WCE), the limited content diversity and size of existing publicly available annotated datasets, adversely affect both the training stability and synthesis performance of GANs. Aiming to a viable solution for WCE image synthesis, a novel Variational Autoencoder architecture is proposed, namely "This Intestine Does not Exist" (TIDE). The proposed architecture comprises multiscale feature extraction convolutional blocks and residual connections, which enable the generation of high-quality and diverse datasets even with a limited number of training images. Contrary to the current approaches, which are oriented towards the augmentation of the available datasets, this study demonstrates that using TIDE, real WCE datasets can be fully substituted by artificially generated ones, without compromising classification performance. Furthermore, qualitative and user evaluation studies by experienced WCE specialists, validate from a medical viewpoint that both the normal and abnormal WCE images synthesized by TIDE are sufficiently realistic.



### Guaranteed Tensor Recovery Fused Low-rankness and Smoothness
- **Arxiv ID**: http://arxiv.org/abs/2302.02155v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.02155v1)
- **Published**: 2023-02-04 12:20:32+00:00
- **Updated**: 2023-02-04 12:20:32+00:00
- **Authors**: Hailin Wang, Jiangjun Peng, Wenjin Qin, Jianjun Wang, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: The tensor data recovery task has thus attracted much research attention in recent years. Solving such an ill-posed problem generally requires to explore intrinsic prior structures underlying tensor data, and formulate them as certain forms of regularization terms for guiding a sound estimate of the restored tensor. Recent research have made significant progress by adopting two insightful tensor priors, i.e., global low-rankness (L) and local smoothness (S) across different tensor modes, which are always encoded as a sum of two separate regularization terms into the recovery models. However, unlike the primary theoretical developments on low-rank tensor recovery, these joint L+S models have no theoretical exact-recovery guarantees yet, making the methods lack reliability in real practice. To this crucial issue, in this work, we build a unique regularization term, which essentially encodes both L and S priors of a tensor simultaneously. Especially, by equipping this single regularizer into the recovery models, we can rigorously prove the exact recovery guarantees for two typical tensor recovery tasks, i.e., tensor completion (TC) and tensor robust principal component analysis (TRPCA). To the best of our knowledge, this should be the first exact-recovery results among all related L+S methods for tensor recovery. Significant recovery accuracy improvements over many other SOTA methods in several TC and TRPCA tasks with various kinds of visual tensor data are observed in extensive experiments. Typically, our method achieves a workable performance when the missing rate is extremely large, e.g., 99.5%, for the color image inpainting task, while all its peers totally fail in such challenging case.



### Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/2302.02181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02181v1)
- **Published**: 2023-02-04 15:28:49+00:00
- **Updated**: 2023-02-04 15:28:49+00:00
- **Authors**: Rudolf Herdt, Maximilian Schmidt, Daniel Otero Baguer, Jean Le'Clerc Arrastia, Peter Maass
- **Comment**: None
- **Journal**: None
- **Summary**: Critical applications, such as in the medical field, require the rapid provision of additional information to interpret decisions made by deep learning methods. In this work, we propose a fast and accurate method to visualize activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing convolutions. We test our approach on images of animals from the AFHQ wild dataset and real-world digital pathology scans of stained tissue samples. Our method provides comparable results to established gradient descent methods on these datasets while running about two orders of magnitude faster.



### Real-Time Image Demoireing on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2302.02184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02184v1)
- **Published**: 2023-02-04 15:42:42+00:00
- **Updated**: 2023-02-04 15:42:42+00:00
- **Authors**: Yuxin Zhang, Mingbao Lin, Xunchao Li, Han Liu, Guozhi Wang, Fei Chao, Shuai Ren, Yafei Wen, Xiaoxin Chen, Rongrong Ji
- **Comment**: To appear in the eleventh International Conference on Learning
  Representations (ICLR 2023)
- **Journal**: None
- **Summary**: Moire patterns appear frequently when taking photos of digital screens, drastically degrading the image quality. Despite the advance of CNNs in image demoireing, existing networks are with heavy design, causing redundant computation burden for mobile devices. In this paper, we launch the first study on accelerating demoireing networks and propose a dynamic demoireing acceleration method (DDA) towards a real-time deployment on mobile devices. Our stimulus stems from a simple-yet-universal fact that moire patterns often unbalancedly distribute across an image. Consequently, excessive computation is wasted upon non-moire areas. Therefore, we reallocate computation costs in proportion to the complexity of image patches. In order to achieve this aim, we measure the complexity of an image patch by designing a novel moire prior that considers both colorfulness and frequency information of moire patterns. Then, we restore image patches with higher-complexity using larger networks and the ones with lower-complexity are assigned with smaller networks to relieve the computation burden. At last, we train all networks in a parameter-shared supernet paradigm to avoid additional parameter burden. Extensive experiments on several benchmarks demonstrate the efficacy of our proposed DDA. In addition, the acceleration evaluated on the VIVO X80 Pro smartphone equipped with a chip of Snapdragon 8 Gen 1 shows that our method can drastically reduce the inference time, leading to a real-time image demoireing on mobile devices. Source codes and models are released at https://github.com/zyxxmu/DDA



### Referential communication in heterogeneous communities of pre-trained visual deep networks
- **Arxiv ID**: http://arxiv.org/abs/2302.08913v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08913v3)
- **Published**: 2023-02-04 15:55:23+00:00
- **Updated**: 2023-07-31 14:49:06+00:00
- **Authors**: Matéo Mahaut, Francesca Franzon, Roberto Dessì, Marco Baroni
- **Comment**: None
- **Journal**: None
- **Summary**: As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evidence that it is capturing high-level semantic features of objects.



### Laplacian ICP for Progressive Registration of 3D Human Head Meshes
- **Arxiv ID**: http://arxiv.org/abs/2302.02194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02194v1)
- **Published**: 2023-02-04 16:39:38+00:00
- **Updated**: 2023-02-04 16:39:38+00:00
- **Authors**: Nick Pears, Hang Dai, Will Smith, Hao Sun
- **Comment**: 7 pages, 6 figures
- **Journal**: 17th IEEE International Conference on Automatic Face and Gesture
  Recognition, Jan 5th-8th 2023
- **Summary**: We present a progressive 3D registration framework that is a highly-efficient variant of classical non-rigid Iterative Closest Points (N-ICP). Since it uses the Laplace-Beltrami operator for deformation regularisation, we view the overall process as Laplacian ICP (L-ICP). This exploits a `small deformation per iteration' assumption and is progressively coarse-to-fine, employing an increasingly flexible deformation model, an increasing number of correspondence sets, and increasingly sophisticated correspondence estimation. Correspondence matching is only permitted within predefined vertex subsets derived from domain-specific feature extractors. Additionally, we present a new benchmark and a pair of evaluation metrics for 3D non-rigid registration, based on annotation transfer. We use this to evaluate our framework on a publicly-available dataset of 3D human head scans (Headspace). The method is robust and only requires a small fraction of the computation time compared to the most popular classical approach, yet has comparable registration performance.



### Oscillation-free Quantization for Low-bit Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.02210v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02210v3)
- **Published**: 2023-02-04 17:40:39+00:00
- **Updated**: 2023-06-02 05:04:43+00:00
- **Authors**: Shih-Yang Liu, Zechun Liu, Kwang-Ting Cheng
- **Comment**: Proceedings of the 40 th International Conference on Machine
  Learning, Honolulu, Hawaii, USA. PMLR 202, 2023
- **Journal**: ICML 2023
- **Summary**: Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confidence}$ and calms the oscillating weights; and $\textit{query}$-$\textit{key}$ reparameterization ($\rm QKR$) to resolve the query-key intertwined oscillation and mitigate the resulting gradient misestimation. Extensive experiments demonstrate that these proposed techniques successfully abate weight oscillation and consistently achieve substantial accuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-S algorithms outperform the previous state-of-the-art by 9.8% and 7.7%, respectively. Code and models are available at: https://github.com/nbasyl/OFQ.



### CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks
- **Arxiv ID**: http://arxiv.org/abs/2302.02213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02213v2)
- **Published**: 2023-02-04 17:59:30+00:00
- **Updated**: 2023-06-19 20:24:28+00:00
- **Authors**: Shashank Agnihotri, Steffen Jung, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: While neural networks allow highly accurate predictions in many tasks, their lack of robustness towards even slight input perturbations hampers their deployment in many real-world applications. Recent research towards evaluating the robustness of neural networks such as the seminal projected gradient descent(PGD) attack and subsequent works have drawn significant attention, as they provide an effective insight into the quality of representations learned by the network. However, these methods predominantly focus on image classification tasks, while only a few approaches specifically address the analysis of pixel-wise prediction tasks such as semantic segmentation, optical flow, disparity estimation, and others, respectively. Thus, there is a lack of a unified adversarial robustness benchmarking tool(algorithm) that is applicable to all such pixel-wise prediction tasks. In this work, we close this gap and propose CosPGD, a novel white-box adversarial attack that allows optimizing dedicated attacks for any pixel-wise prediction task in a unified setting. It leverages the cosine similarity between the distributions over the predictions and ground truth (or target) to extend directly from classification tasks to regression settings. We outperform the SotA on semantic segmentation attacks in our experiments on PASCAL VOC2012 and CityScapes. Further, we set a new benchmark for adversarial attacks on optical flow, and image restoration displaying the ability to extend to any pixel-wise prediction task.



### Variational multichannel multiclass segmentation using unsupervised lifting with CNNs
- **Arxiv ID**: http://arxiv.org/abs/2302.02214v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.FA, math.NA, 65K10, 68U10, 68T10,, I.4.6; I.2.10; I.5.4; G.1.6; G.1.10; G.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.02214v2)
- **Published**: 2023-02-04 18:01:47+00:00
- **Updated**: 2023-06-16 16:28:21+00:00
- **Authors**: Nadja Gruber, Johannes Schwab, Sebastien Court, Elke Gizewski, Markus Haltmeier
- **Comment**: 20th INTERNATIONAL CONFERENCE OF NUMERICAL ANALYSIS AND APPLIED
  MATHEMATICS
- **Journal**: None
- **Summary**: We propose an unsupervised image segmentation approach, that combines a variational energy functional and deep convolutional neural networks. The variational part is based on a recent multichannel multiphase Chan-Vese model, which is capable to extract useful information from multiple input images simultaneously. We implement a flexible multiclass segmentation method that divides a given image into $K$ different regions. We use convolutional neural networks (CNNs) targeting a pre-decomposition of the image. By subsequently minimising the segmentation functional, the final segmentation is obtained in a fully unsupervised manner. Special emphasis is given to the extraction of informative feature maps serving as a starting point for the segmentation. The initial results indicate that the proposed method is able to decompose and segment the different regions of various types of images, such as texture and medical images and compare its performance with another multiphase segmentation method.



### A Minimax Approach Against Multi-Armed Adversarial Attacks Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02216v1)
- **Published**: 2023-02-04 18:21:22+00:00
- **Updated**: 2023-02-04 18:21:22+00:00
- **Authors**: Federica Granese, Marco Romanelli, Siddharth Garg, Pablo Piantanida
- **Comment**: 10 pages, 13 figures, 14 tables
- **Journal**: None
- **Summary**: Multi-armed adversarial attacks, in which multiple algorithms and objective loss functions are simultaneously used at evaluation time, have been shown to be highly successful in fooling state-of-the-art adversarial examples detectors while requiring no specific side information about the detection mechanism. By formalizing the problem at hand, we can propose a solution that aggregates the soft-probability outputs of multiple pre-trained detectors according to a minimax approach. The proposed framework is mathematically sound, easy to implement, and modular, allowing for integrating existing or future detectors. Through extensive evaluation on popular datasets (e.g., CIFAR10 and SVHN), we show that our aggregation consistently outperforms individual state-of-the-art detectors against multi-armed adversarial attacks, making it an effective solution to improve the resilience of available methods.



### Revisiting Image Deblurring with an Efficient ConvNet
- **Arxiv ID**: http://arxiv.org/abs/2302.02234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02234v1)
- **Published**: 2023-02-04 20:42:46+00:00
- **Updated**: 2023-02-04 20:42:46+00:00
- **Authors**: Lingyan Ruan, Mojtaba Bemana, Hans-peter Seidel, Karol Myszkowski, Bin Chen
- **Comment**: 30 pages (12 pages for the main manuscript and 18 for the
  supplementary materials)
- **Journal**: None
- **Summary**: Image deblurring aims to recover the latent sharp image from its blurry counterpart and has a wide range of applications in computer vision. The Convolution Neural Networks (CNNs) have performed well in this domain for many years, and until recently an alternative network architecture, namely Transformer, has demonstrated even stronger performance. One can attribute its superiority to the multi-head self-attention (MHSA) mechanism, which offers a larger receptive field and better input content adaptability than CNNs. However, as MHSA demands high computational costs that grow quadratically with respect to the input resolution, it becomes impractical for high-resolution image deblurring tasks. In this work, we propose a unified lightweight CNN network that features a large effective receptive field (ERF) and demonstrates comparable or even better performance than Transformers while bearing less computational costs. Our key design is an efficient CNN block dubbed LaKD, equipped with a large kernel depth-wise convolution and spatial-channel mixing structure, attaining comparable or larger ERF than Transformers but with a smaller parameter scale. Specifically, we achieve +0.17dB / +0.43dB PSNR over the state-of-the-art Restormer on defocus / motion deblurring benchmark datasets with 32% fewer parameters and 39% fewer MACs. Extensive experiments demonstrate the superior performance of our network and the effectiveness of each module. Furthermore, we propose a compact and intuitive ERFMeter metric that quantitatively characterizes ERF, and shows a high correlation to the network performance. We hope this work can inspire the research community to further explore the pros and cons of CNN and Transformer architectures beyond image deblurring tasks.



### Self-supervised Multi-view Disentanglement for Expansion of Visual Collections
- **Arxiv ID**: http://arxiv.org/abs/2302.02249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.02249v1)
- **Published**: 2023-02-04 22:09:17+00:00
- **Updated**: 2023-02-04 22:09:17+00:00
- **Authors**: Nihal Jain, Praneetha Vaddamanu, Paridhi Maheshwari, Vishwa Vinay, Kuldeep Kulkarni
- **Comment**: A version of this paper has been accepted at WSDM 2023
- **Journal**: None
- **Summary**: Image search engines enable the retrieval of images relevant to a query image. In this work, we consider the setting where a query for similar images is derived from a collection of images. For visual search, the similarity measurements may be made along multiple axes, or views, such as style and color. We assume access to a set of feature extractors, each of which computes representations for a specific view. Our objective is to design a retrieval algorithm that effectively combines similarities computed over representations from multiple views. To this end, we propose a self-supervised learning method for extracting disentangled view-specific representations for images such that the inter-view overlap is minimized. We show how this allows us to compute the intent of a collection as a distribution over views. We show how effective retrieval can be performed by prioritizing candidate expansion images that match the intent of a query collection. Finally, we present a new querying mechanism for image search enabled by composing multiple collections and perform retrieval under this setting using the techniques presented in this paper.



### Thermal Analysis of Malignant Brain Tumors by Employing a Morphological Differentiation-Based Method in Conjunction with Artificial Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2302.10271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10271v1)
- **Published**: 2023-02-04 22:41:04+00:00
- **Updated**: 2023-02-04 22:41:04+00:00
- **Authors**: Hamed Hani, Afsaneh Mojra
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, a morphological differentiation-based method has been introduced which employs temperature distribution on the tissue surface to detect brain tumor's malignancy. According to the common tumor CT scans, two different scenarios have been implemented to describe irregular shape of the malignant tumor. In the first scenario, tumor has been considered as a polygon base prism and in the second one, it has been considered as a star-shaped base prism. By increasing the number of sides of the polygon or wings of the star, degree of the malignancy has been increased. Constant heat generation has been considered for the tumor and finite element analysis has been conducted by the ABAQUS software linked with a PYTHON script on both tumor models to study temperature variations on the top tissue surface. This temperature distribution has been characterized by 10 parameters. In each scenario, 98 sets of these parameters has been used as inputs of a radial basis function neural network (RBFNN) and number of sides or wings has been selected to be the output. The RBFNN has been trained to identify malignancy of tumor based on its morphology. According to the RBFNN results, the proposed method has been capable of differentiating between benign and malignant tumors and estimating the degree of malignancy with high accuracy



### Human-Imperceptible Identification with Learnable Lensless Imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.02255v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02255v1)
- **Published**: 2023-02-04 22:58:46+00:00
- **Updated**: 2023-02-04 22:58:46+00:00
- **Authors**: Thuong Nguyen Canh, Trung Thanh Ngo, Hajime Nagahara
- **Comment**: None
- **Journal**: None
- **Summary**: Lensless imaging protects visual privacy by capturing heavily blurred images that are imperceptible for humans to recognize the subject but contain enough information for machines to infer information. Unfortunately, protecting visual privacy comes with a reduction in recognition accuracy and vice versa. We propose a learnable lensless imaging framework that protects visual privacy while maintaining recognition accuracy. To make captured images imperceptible to humans, we designed several loss functions based on total variation, invertibility, and the restricted isometry property. We studied the effect of privacy protection with blurriness on the identification of personal identity via a quantitative method based on a subjective evaluation. Moreover, we validate our simulation by implementing a hardware realization of lensless imaging with photo-lithographically printed masks.



### CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D
- **Arxiv ID**: http://arxiv.org/abs/2302.02259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02259v1)
- **Published**: 2023-02-04 23:30:04+00:00
- **Updated**: 2023-02-04 23:30:04+00:00
- **Authors**: David Paz, Srinidhi Kalgundi Srinivas, Yunchao Yao, Henrik I. Christensen
- **Comment**: 5 pages, 4 figures, 1 table. Under review at IEEE Intelligent
  Vehicles Symposium 2023
- **Journal**: None
- **Summary**: This work introduces a new approach for joint detection of centerlines based on image data by localizing the features jointly in 2D and 3D. In contrast to existing work that focuses on detection of visual cues, we explore feature extraction methods that are directly amenable to the urban driving task. To develop and evaluate our approach, a large urban driving dataset dubbed AV Breadcrumbs is automatically labeled by leveraging vector map representations and projective geometry to annotate over 900,000 images. Our results demonstrate potential for dynamic scene modeling across various urban driving scenarios. Our model achieves an F1 score of 0.684 and an average normalized depth error of 2.083. The code and data annotations are publicly available.



