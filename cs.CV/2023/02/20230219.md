# Arxiv Papers in cs.CV on 2023-02-19
### Liveness score-based regression neural networks for face anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2302.09461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.09461v2)
- **Published**: 2023-02-19 02:45:35+00:00
- **Updated**: 2023-03-21 00:14:41+00:00
- **Authors**: Youngjun Kwak, Minyoung Jung, Hunjae Yoo, JinHo Shin, Changick Kim
- **Comment**: Submission to ICASSP 2023
- **Journal**: None
- **Summary**: Previous anti-spoofing methods have used either pseudo maps or user-defined labels, and the performance of each approach depends on the accuracy of the third party networks generating pseudo maps and the way in which the users define the labels. In this paper, we propose a liveness score-based regression network for overcoming the dependency on third party networks and users. First, we introduce a new labeling technique, called pseudo-discretized label encoding for generating discretized labels indicating the amount of information related to real images. Secondly, we suggest the expected liveness score based on a regression network for training the difference between the proposed supervision and the expected liveness score. Finally, extensive experiments were conducted on four face anti-spoofing benchmarks to verify our proposed method on both intra-and cross-dataset tests. The experimental results show our approach outperforms previous methods.



### MedViT: A Robust Vision Transformer for Generalized Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.09462v1
- **DOI**: 10.1016/j.compbiomed.2023.106791
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09462v1)
- **Published**: 2023-02-19 02:55:45+00:00
- **Updated**: 2023-02-19 02:55:45+00:00
- **Authors**: Omid Nejati Manzari, Hamid Ahmadabadi, Hossein Kashiani, Shahriar B. Shokouhi, Ahmad Ayatollahi
- **Comment**: None
- **Journal**: Computers in Biology and Medicine 2023
- **Summary**: Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, there are still concerns about the reliability of deep medical diagnosis systems against the potential threats of adversarial attacks since inaccurate diagnosis could lead to disastrous consequences in the safety realm. In this study, we propose a highly robust yet efficient CNN-Transformer hybrid model which is equipped with the locality of CNNs as well as the global connectivity of vision Transformers. To mitigate the high quadratic complexity of the self-attention mechanism while jointly attending to information in various representation subspaces, we construct our attention mechanism by means of an efficient convolution operation. Moreover, to alleviate the fragility of our Transformer model against adversarial attacks, we attempt to learn smoother decision boundaries. To this end, we augment the shape information of an image in the high-level feature space by permuting the feature mean and variance within mini-batches. With less computational complexity, our proposed hybrid model demonstrates its high robustness and generalization ability compared to the state-of-the-art studies on a large-scale collection of standardized MedMNIST-2D datasets.



### Designing a 3D-Aware StyleNeRF Encoder for Face Editing
- **Arxiv ID**: http://arxiv.org/abs/2302.09467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09467v1)
- **Published**: 2023-02-19 03:32:28+00:00
- **Updated**: 2023-02-19 03:32:28+00:00
- **Authors**: Songlin Yang, Wei Wang, Bo Peng, Jing Dong
- **Comment**: Accepted by 2023 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)
- **Journal**: None
- **Summary**: GAN inversion has been exploited in many face manipulation tasks, but 2D GANs often fail to generate multi-view 3D consistent images. The encoders designed for 2D GANs are not able to provide sufficient 3D information for the inversion and editing. Therefore, 3D-aware GAN inversion is proposed to increase the 3D editing capability of GANs. However, the 3D-aware GAN inversion remains under-explored. To tackle this problem, we propose a 3D-aware (3Da) encoder for GAN inversion and face editing based on the powerful StyleNeRF model. Our proposed 3Da encoder combines a parametric 3D face model with a learnable detail representation model to generate geometry, texture and view direction codes. For more flexible face manipulation, we then design a dual-branch StyleFlow module to transfer the StyleNeRF codes with disentangled geometry and texture flows. Extensive experiments demonstrate that we realize 3D consistent face manipulation in both facial attribute editing and texture transfer. Furthermore, for video editing, we make the sequence of frame codes share a common canonical manifold, which improves the temporal consistency of the edited attributes.



### Few-shot Multimodal Multitask Multilingual Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.12489v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.12489v1)
- **Published**: 2023-02-19 03:48:46+00:00
- **Updated**: 2023-02-19 03:48:46+00:00
- **Authors**: Aman Chadha, Vinija Jain
- **Comment**: None
- **Journal**: None
- **Summary**: While few-shot learning as a transfer learning paradigm has gained significant traction for scenarios with limited data, it has primarily been explored in the context of building unimodal and unilingual models. Furthermore, a significant part of the existing literature in the domain of few-shot multitask learning perform in-context learning which requires manually generated prompts as the input, yielding varying outcomes depending on the level of manual prompt-engineering. In addition, in-context learning suffers from substantial computational, memory, and storage costs which eventually leads to high inference latency because it involves running all of the prompt's examples through the model every time a prediction is made. In contrast, methods based on the transfer learning via the fine-tuning paradigm avoid the aforementioned issues at a one-time cost of fine-tuning weights on a per-task basis. However, such methods lack exposure to few-shot multimodal multitask learning. In this paper, we propose few-shot learning for a multimodal multitask multilingual (FM3) setting by adapting pre-trained vision and language models using task-specific hypernetworks and contrastively fine-tuning them to enable few-shot learning. FM3's architecture combines the best of both worlds of in-context and fine-tuning based learning and consists of three major components: (i) multimodal contrastive fine-tuning to enable few-shot learning, (ii) hypernetwork task adaptation to perform multitask learning, and (iii) task-specific output heads to cater to a plethora of diverse tasks. FM3 learns the most prominent tasks in the vision and language domains along with their intersections, namely visual entailment (VE), visual question answering (VQA), and natural language understanding (NLU) tasks such as neural entity recognition (NER) and the GLUE benchmark including QNLI, MNLI, QQP, and SST-2.



### Video-Text Retrieval by Supervised Multi-Space Multi-Grained Alignment
- **Arxiv ID**: http://arxiv.org/abs/2302.09473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.09473v1)
- **Published**: 2023-02-19 04:03:22+00:00
- **Updated**: 2023-02-19 04:03:22+00:00
- **Authors**: Yimu Wang, Peng Shi
- **Comment**: None
- **Journal**: None
- **Summary**: While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-space multi-grained supervised learning framework, SUMA, to learn an aligned representation space shared between the video and the text for video-text retrieval. The shared aligned space is initialized with a finite number of concept clusters, each of which refers to a number of basic concepts (words). With the text data at hand, we are able to update the shared aligned space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarity. Benefiting from learned shared aligned space and multi-grained similarity, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of SUMA over existing methods.



### LC-NeRF: Local Controllable Face Generation in Neural Randiance Field
- **Arxiv ID**: http://arxiv.org/abs/2302.09486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09486v1)
- **Published**: 2023-02-19 05:50:08+00:00
- **Updated**: 2023-02-19 05:50:08+00:00
- **Authors**: Wenyang Zhou, Lu Yuan, Shuyu Chen, Lin Gao, Shimin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D face generation has achieved high visual quality and 3D consistency thanks to the development of neural radiance fields (NeRF). Recently, to generate and edit 3D faces with NeRF representation, some methods are proposed and achieve good results in decoupling geometry and texture. The latent codes of these generative models affect the whole face, and hence modifications to these codes cause the entire face to change. However, users usually edit a local region when editing faces and do not want other regions to be affected. Since changes to the latent code affect global generation results, these methods do not allow for fine-grained control of local facial regions. To improve local controllability in NeRF-based face editing, we propose LC-NeRF, which is composed of a Local Region Generators Module and a Spatial-Aware Fusion Module, allowing for local geometry and texture control of local facial regions. Qualitative and quantitative evaluations show that our method provides better local editing than state-of-the-art face editing methods. Our method also performs well in downstream tasks, such as text-driven facial image editing.



### A Picture May Be Worth a Thousand Lives: An Interpretable Artificial Intelligence Strategy for Predictions of Suicide Risk from Social Media Images
- **Arxiv ID**: http://arxiv.org/abs/2302.09488v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2302.09488v1)
- **Published**: 2023-02-19 06:18:23+00:00
- **Updated**: 2023-02-19 06:18:23+00:00
- **Authors**: Yael Badian, Yaakov Ophir, Refael Tikochinski, Nitay Calderon, Anat Brunstein Klomek, Roi Reichart
- **Comment**: 33 pages, 1 figure, 4 tables
- **Journal**: None
- **Summary**: The promising research on Artificial Intelligence usages in suicide prevention has principal gaps, including black box methodologies, inadequate outcome measures, and scarce research on non-verbal inputs, such as social media images (despite their popularity today, in our digital era). This study addresses these gaps and combines theory-driven and bottom-up strategies to construct a hybrid and interpretable prediction model of valid suicide risk from images. The lead hypothesis was that images contain valuable information about emotions and interpersonal relationships, two central concepts in suicide-related treatments and theories. The dataset included 177,220 images by 841 Facebook users who completed a gold-standard suicide scale. The images were represented with CLIP, a state-of-the-art algorithm, which was utilized, unconventionally, to extract predefined features that served as inputs to a simple logistic-regression prediction model (in contrast to complex neural networks). The features addressed basic and theory-driven visual elements using everyday language (e.g., bright photo, photo of sad people). The results of the hybrid model (that integrated theory-driven and bottom-up methods) indicated high prediction performance that surpassed common bottom-up algorithms, thus providing a first proof that images (alone) can be leveraged to predict validated suicide risk. Corresponding with the lead hypothesis, at-risk users had images with increased negative emotions and decreased belonginess. The results are discussed in the context of non-verbal warning signs of suicide. Notably, the study illustrates the advantages of hybrid models in such complicated tasks and provides simple and flexible prediction strategies that could be utilized to develop real-life monitoring tools of suicide.



### X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.09491v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09491v1)
- **Published**: 2023-02-19 06:31:17+00:00
- **Updated**: 2023-02-19 06:31:17+00:00
- **Authors**: Aishan Liu, Jun Guo, Jiakai Wang, Siyuan Liang, Renshuai Tao, Wenbo Zhou, Cong Liu, Xianglong Liu, Dacheng Tao
- **Comment**: Accepted by USENIX Security 2023
- **Journal**: None
- **Summary**: Adversarial attacks are valuable for evaluating the robustness of deep learning models. Existing attacks are primarily conducted on the visible light spectrum (e.g., pixel-wise texture perturbation). However, attacks targeting texture-free X-ray images remain underexplored, despite the widespread application of X-ray imaging in safety-critical scenarios such as the X-ray detection of prohibited items. In this paper, we take the first step toward the study of adversarial attacks targeted at X-ray prohibited item detection, and reveal the serious threats posed by such attacks in this safety-critical scenario. Specifically, we posit that successful physical adversarial attacks in this scenario should be specially designed to circumvent the challenges posed by color/texture fading and complex overlapping. To this end, we propose X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage. To resolve the issues associated with color/texture fading, we develop a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes, using the gradients of a surrogate model rather than directly generating adversarial textures. To place the printed 3D adversarial objects in luggage with complex overlapped instances, we design a policy-based reinforcement learning strategy to find locations eliciting strong attack performance in worst-case scenarios whereby the prohibited items are heavily occluded by other items. To verify the effectiveness of the proposed X-Adv, we conduct extensive experiments in both the digital and the physical world (employing a commercial X-ray security inspection system for the latter case). Furthermore, we present the physical-world X-ray adversarial attack dataset XAD.



### Mutual Exclusive Modulator for Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.09498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09498v2)
- **Published**: 2023-02-19 07:31:49+00:00
- **Updated**: 2023-04-11 07:28:14+00:00
- **Authors**: Haixu Long, Xiaolin Zhang, Yanbin Liu, Zongtai Luo, Jianbo Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The long-tailed recognition (LTR) is the task of learning high-performance classifiers given extremely imbalanced training samples between categories. Most of the existing works address the problem by either enhancing the features of tail classes or re-balancing the classifiers to reduce the inductive bias. In this paper, we try to look into the root cause of the LTR task, i.e., training samples for each class are greatly imbalanced, and propose a straightforward solution. We split the categories into three groups, i.e., many, medium and few, according to the number of training images. The three groups of categories are separately predicted to reduce the difficulty for classification. This idea naturally arises a new problem of how to assign a given sample to the right class groups? We introduce a mutual exclusive modulator which can estimate the probability of an image belonging to each group. Particularly, the modulator consists of a light-weight module and learned with a mutual exclusive objective. Hence, the output probabilities of the modulator encode the data volume clues of the training dataset. They are further utilized as prior information to guide the prediction of the classifier. We conduct extensive experiments on multiple datasets, e.g., ImageNet-LT, Place-LT and iNaturalist 2018 to evaluate the proposed approach. Our method achieves competitive performance compared to the state-of-the-art benchmarks.



### Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking
- **Arxiv ID**: http://arxiv.org/abs/2302.09502v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09502v1)
- **Published**: 2023-02-19 07:48:12+00:00
- **Updated**: 2023-02-19 07:48:12+00:00
- **Authors**: Zixuan Huang, Xingyu Lin, David Held
- **Comment**: None
- **Journal**: International Conference on Robotics and Automation 2023
- **Summary**: State estimation is one of the greatest challenges for cloth manipulation due to cloth's high dimensionality and self-occlusion. Prior works propose to identify the full state of crumpled clothes by training a mesh reconstruction model in simulation. However, such models are prone to suffer from a sim-to-real gap due to differences between cloth simulation and the real world. In this work, we propose a self-supervised method to finetune a mesh reconstruction model in the real world. Since the full mesh of crumpled cloth is difficult to obtain in the real world, we design a special data collection scheme and an action-conditioned model-based cloth tracking method to generate pseudo-labels for self-supervised learning. By finetuning the pretrained mesh reconstruction model on this pseudo-labeled dataset, we show that we can improve the quality of the reconstructed mesh without requiring human annotations, and improve the performance of downstream manipulation task.



### Fashion-model pose recommendation and generation using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.08660v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.08660v1)
- **Published**: 2023-02-19 09:12:46+00:00
- **Updated**: 2023-02-19 09:12:46+00:00
- **Authors**: Vijitha Kannumuru, Santhosh Kannan S P, Krithiga Shankar, Joy Larnyoh, Rohith Mahadevan, Raja CSP Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion-model pose is an important attribute in the fashion industry. Creative directors, modeling production houses, and top photographers always look for professional models able to pose. without the skill to correctly pose, their chances of landing professional modeling employment are regrettably quite little. There are occasions when models and photographers are unsure of the best pose to strike while taking photographs. This research concentrates on suggesting the fashion personnel a series of similar images based on the input image. The image is segmented into different parts and similar images are suggested for the user. This was achieved by calculating the color histogram of the input image and applying the same for all the images in the dataset and comparing the histograms. Synthetic images have become popular to avoid privacy concerns and to overcome the high cost of photoshoots. Hence, this paper also extends the work of generating synthetic images from the recommendation engine using styleGAN to an extent.



### A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs
- **Arxiv ID**: http://arxiv.org/abs/2302.09516v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09516v1)
- **Published**: 2023-02-19 09:26:19+00:00
- **Updated**: 2023-02-19 09:26:19+00:00
- **Authors**: Atif Shah, Maged S. Al-Shaibani, Moataz Ahmad, Reem Bunyan
- **Comment**: None
- **Journal**: None
- **Summary**: Introduction: Multiple Sclerosis (MS) is a chronic disease that affects millions of people across the globe. MS can critically affect different organs of the central nervous system such as the eyes, the spinal cord, and the brain.   Background: To help physicians in diagnosing MS lesions, computer-aided methods are widely used. In this regard, a considerable research has been carried out in the area of automatic detection and segmentation of MS lesions in magnetic resonance images (MRIs).   Methodology: In this study, we review the different approaches that have been used in computer-aided detection and segmentation of MS lesions. Our review resulted in categorizing MS lesion segmentation approaches into six broad categories: data-driven, statistical, supervised machine learning, unsupervised machine learning, fuzzy, and deep learning-based techniques. We critically analyze the different techniques under these approaches and highlight their strengths and weaknesses.   Results: From the study, we observe that a considerable amount of work, around 25% of related literature, is focused on statistical-based MS lesion segmentation techniques, followed by 21.15% for data-driven based methods, 19.23% for deep learning and 15.38% for supervised methods.   Implication: The study points out the challenges/gaps to be addressed in future research. The study shows the work which has been done in last one decade in detection and segmentation of MS lesions. The results show that, in recent years, deep learning methods are outperforming all the others methods.



### Interactive Video Corpus Moment Retrieval using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.09522v1
- **DOI**: 10.1145/3503161.3548277
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2302.09522v1)
- **Published**: 2023-02-19 09:48:23+00:00
- **Updated**: 2023-02-19 09:48:23+00:00
- **Authors**: Zhixin Ma, Chong-Wah Ngo
- **Comment**: Accepted by ACM Multimedia 2022
- **Journal**: Proceedings of the 30th ACM International Conference on Multimedia
  (2022) 296-306
- **Summary**: Known-item video search is effective with human-in-the-loop to interactively investigate the search result and refine the initial query. Nevertheless, when the first few pages of results are swamped with visually similar items, or the search target is hidden deep in the ranked list, finding the know-item target usually requires a long duration of browsing and result inspection. This paper tackles the problem by reinforcement learning, aiming to reach a search target within a few rounds of interaction by long-term learning from user feedbacks. Specifically, the system interactively plans for navigation path based on feedback and recommends a potential target that maximizes the long-term reward for user comment. We conduct experiments for the challenging task of video corpus moment retrieval (VCMR) to localize moments from a large video corpus. The experimental results on TVR and DiDeMo datasets verify that our proposed work is effective in retrieving the moments that are hidden deep inside the ranked lists of CONQUER and HERO, which are the state-of-the-art auto-search engines for VCMR.



### A Comprehensive Evaluation Study on Risk Level Classification of Melanoma by Computer Vision on ISIC 2016-2020 Datasets
- **Arxiv ID**: http://arxiv.org/abs/2302.09528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09528v1)
- **Published**: 2023-02-19 09:58:58+00:00
- **Updated**: 2023-02-19 09:58:58+00:00
- **Authors**: Chengdong Yao
- **Comment**: 9 pages, 12 figures, 11 tables
- **Journal**: None
- **Summary**: Skin cancer is the most common type of cancer. Specifically, melanoma is the cause of 75% of skin cancer deaths, although it is the least common skin cancer. Better detection of melanoma could have a positive impact on millions of people. The ISIC archive contains the largest publicly available collection of dermatoscopic images of skin lesions. In this research, we investigate the efficacy of applying advanced deep learning techniques in computer vision to identify melanoma in images of skin lesions. Through reviewing previous methods, including pre-trained models, deep-learning classifiers, transfer learning, etc., we demonstrate the applicability of the popular deep learning methods on critical clinical problems such as identifying melanoma. Finally, we proposed a processing flow with a validation AUC greater than 94% and a sensitivity greater than 90% on ISIC 2016 - 2020 datasets.



### Mixed Hierarchy Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2302.09554v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09554v3)
- **Published**: 2023-02-19 12:18:45+00:00
- **Updated**: 2023-05-26 07:54:32+00:00
- **Authors**: Hu Gao, Depeng Dang
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration is a long-standing low-level vision problem, e.g., deblurring and deraining. In the process of image restoration, it is necessary to consider not only the spatial details and contextual information of restoration to ensure the quality, but also the system complexity. Although many methods have been able to guarantee the quality of image restoration, the system complexity of the state-of-the-art (SOTA) methods is increasing as well. Motivated by this, we present a mixed hierarchy network that can balance these competing goals. Our main proposal is a mixed hierarchy architecture, that progressively recovers contextual information and spatial details from degraded images while we design intra-blocks to reduce system complexity. Specifically, our model first learns the contextual information using encoder-decoder architectures, and then combines them with high-resolution branches that preserve spatial detail. In order to reduce the system complexity of this architecture for convenient analysis and comparison, we replace or remove the nonlinear activation function with multiplication and use a simple network structure. In addition, we replace spatial convolution with global self-attention for the middle block of encoder-decoder. The resulting tightly interlinked hierarchy architecture, named as MHNet, delivers strong performance gains on several image restoration tasks, including image deraining, and deblurring.



### Supervised Contrastive Learning and Feature Fusion for Improved Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2302.09556v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09556v1)
- **Published**: 2023-02-19 12:20:14+00:00
- **Updated**: 2023-02-19 12:20:14+00:00
- **Authors**: Nazim Bendib
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Kinship Verification is the task of determining the degree of familial relationship between two facial images. It has recently gained a lot of interest in various applications spanning forensic science, social media, and demographic studies. In the past decade, deep learning-based approaches have emerged as a promising solution to this problem, achieving state-of-the-art performance. In this paper, we propose a novel method for solving kinship verification by using supervised contrastive learning, which trains the model to maximize the similarity between related individuals and minimize it between unrelated individuals. Our experiments show state-of-the-art results and achieve 81.1% accuracy in the Families in the Wild (FIW) dataset.



### Deep Selector-JPEG: Adaptive JPEG Image Compression for Computer Vision in Image classification with Human Vision Criteria
- **Arxiv ID**: http://arxiv.org/abs/2302.09560v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.09560v1)
- **Published**: 2023-02-19 12:38:20+00:00
- **Updated**: 2023-02-19 12:38:20+00:00
- **Authors**: Hossam Amer, Sepideh Shaterian, En-hui Yang
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: With limited storage/bandwidth resources, input images to Computer Vision (CV) applications that use Deep Neural Networks (DNNs) are often encoded with JPEG that is tailored to Human Vision (HV). This paper presents Deep Selector-JPEG, an adaptive JPEG compression method that targets image classification while satisfying HV criteria. For each image, Deep Selector-JPEG selects adaptively a Quality Factor (QF) to compress the image so that a good trade-off between the Compression Ratio (CR) and DNN classifier Accuracy (Rate-Accuracy performance) can be achieved over a set of images for a variety of DNN classifiers while the MS-SSIM of such compressed image is greater than a threshold value predetermined by HV with a high probability. Deep Selector-JPEG is designed via light-weighted or heavy-weighted selector architectures. Experimental results show that in comparison with JPEG at the same CR, Deep Selector-JPEG achieves better Rate-Accuracy performance over the ImageNet validation set for all tested DNN classifiers with gains in classification accuracy between 0.2% and 1% at the same CRs while satisfying HV constraints. Deep Selector-JPEG can also roughly provide the original classification accuracy at higher CRs.



### TAX: Tendency-and-Assignment Explainer for Semantic Segmentation with Multi-Annotators
- **Arxiv ID**: http://arxiv.org/abs/2302.09561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09561v1)
- **Published**: 2023-02-19 12:40:22+00:00
- **Updated**: 2023-02-19 12:40:22+00:00
- **Authors**: Yuan-Chia Cheng, Zu-Yun Shiau, Fu-En Yang, Yu-Chiang Frank Wang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: To understand how deep neural networks perform classification predictions, recent research attention has been focusing on developing techniques to offer desirable explanations. However, most existing methods cannot be easily applied for semantic segmentation; moreover, they are not designed to offer interpretability under the multi-annotator setting. Instead of viewing ground-truth pixel-level labels annotated by a single annotator with consistent labeling tendency, we aim at providing interpretable semantic segmentation and answer two critical yet practical questions: "who" contributes to the resulting segmentation, and "why" such an assignment is determined. In this paper, we present a learning framework of Tendency-and-Assignment Explainer (TAX), designed to offer interpretability at the annotator and assignment levels. More specifically, we learn convolution kernel subsets for modeling labeling tendencies of each type of annotation, while a prototype bank is jointly observed to offer visual guidance for learning the above kernels. For evaluation, we consider both synthetic and real-world datasets with multi-annotators. We show that our TAX can be applied to state-of-the-art network architectures with comparable performances, while segmentation interpretability at both levels can be offered accordingly.



### Optimizing YOLOv7 for Semiconductor Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.09565v1
- **DOI**: 10.1117/12.2657564
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2302.09565v1)
- **Published**: 2023-02-19 12:51:07+00:00
- **Updated**: 2023-02-19 12:51:07+00:00
- **Authors**: Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt
- **Comment**: 8 pages, 4 figures, 5 tables. To be published by SPIE in the
  proceedings of Metrology, Inspection, and Process Control XXXVII
- **Journal**: Proc. SPIE 12496, Metrology, Inspection, and Process Control
  XXXVII, 124962D (27 April 2023)
- **Summary**: The field of object detection using Deep Learning (DL) is constantly evolving with many new techniques and models being proposed. YOLOv7 is a state-of-the-art object detector based on the YOLO family of models which have become popular for industrial applications. One such possible application domain can be semiconductor defect inspection. The performance of any machine learning model depends on its hyperparameters. Furthermore, combining predictions of one or more models in different ways can also affect performance. In this research, we experiment with YOLOv7, a recently proposed, state-of-the-art object detector, by training and evaluating models with different hyperparameters to investigate which ones improve performance in terms of detection precision for semiconductor line space pattern defects. The base YOLOv7 model with default hyperparameters and Non Maximum Suppression (NMS) prediction combining outperforms all RetinaNet models from previous work in terms of mean Average Precision (mAP). We find that vertically flipping images randomly during training yields a 3% improvement in the mean AP of all defect classes. Other hyperparameter values improved AP only for certain classes compared to the default model. Combining models that achieve the best AP for different defect classes was found to be an effective ensembling strategy. Combining predictions from ensembles using Weighted Box Fusion (WBF) prediction gave the best performance. The best ensemble with WBF improved on the mAP of the default model by 10%.



### SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering
- **Arxiv ID**: http://arxiv.org/abs/2302.09569v1
- **DOI**: 10.1117/12.2657555
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2302.09569v1)
- **Published**: 2023-02-19 13:12:28+00:00
- **Updated**: 2023-02-19 13:12:28+00:00
- **Authors**: MinJin Hwang, Bappaditya Dey, Enrique Dehaerne, Sandip Halder, Young-han Shin
- **Comment**: 7 pages, 6 figures, 5 tables. To be published by SPIE in the
  proceedings of Metrology, Inspection, and Process Control XXXVII
- **Journal**: Proc. SPIE 12496, Metrology, Inspection, and Process Control
  XXXVII, 1249608 (27 April 2023)
- **Summary**: In this study, we applied the PointRend (Point-based Rendering) method to semiconductor defect segmentation. PointRend is an iterative segmentation algorithm inspired by image rendering in computer graphics, a new image segmentation method that can generate high-resolution segmentation masks. It can also be flexibly integrated into common instance segmentation meta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN. We implemented a model, termed as SEMI-PointRend, to generate precise segmentation masks by applying the PointRend neural network module. In this paper, we focus on comparing the defect segmentation predictions of SEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single bridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend can outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average precision.



### Rethinking Data-Free Quantization as a Zero-Sum Game
- **Arxiv ID**: http://arxiv.org/abs/2302.09572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09572v1)
- **Published**: 2023-02-19 13:22:40+00:00
- **Updated**: 2023-02-19 13:22:40+00:00
- **Authors**: Biao Qian, Yang Wang, Richang Hong, Meng Wang
- **Comment**: 9 pages, 7 figures, accepted by AAAI 2023
- **Journal**: None
- **Summary**: Data-free quantization (DFQ) recovers the performance of quantized network (Q) without accessing the real data, but generates the fake sample via a generator (G) by learning from full-precision network (P) instead. However, such sample generation process is totally independent of Q, specialized as failing to consider the adaptability of the generated samples, i.e., beneficial or adversarial, over the learning process of Q, resulting into non-ignorable performance loss. Building on this, several crucial questions -- how to measure and exploit the sample adaptability to Q under varied bit-width scenarios? how to generate the samples with desirable adaptability to benefit the quantized network? -- impel us to revisit DFQ. In this paper, we answer the above questions from a game-theory perspective to specialize DFQ as a zero-sum game between two players -- a generator and a quantized network, and further propose an Adaptability-aware Sample Generation (AdaSG) method. Technically, AdaSG reformulates DFQ as a dynamic maximization-vs-minimization game process anchored on the sample adaptability. The maximization process aims to generate the sample with desirable adaptability, such sample adaptability is further reduced by the minimization process after calibrating Q for performance recovery. The Balance Gap is defined to guide the stationarity of the game process to maximally benefit Q. The theoretical analysis and empirical studies verify the superiority of AdaSG over the state-of-the-arts. Our code is available at https://github.com/hfutqian/AdaSG.



### Evaluating Representations with Readout Model Switching
- **Arxiv ID**: http://arxiv.org/abs/2302.09579v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09579v1)
- **Published**: 2023-02-19 14:08:01+00:00
- **Updated**: 2023-02-19 14:08:01+00:00
- **Authors**: Yazhe Li, Jorg Bornschein, Marcus Hutter
- **Comment**: None
- **Journal**: None
- **Summary**: Although much of the success of Deep Learning builds on learning good representations, a rigorous method to evaluate their quality is lacking. In this paper, we treat the evaluation of representations as a model selection problem and propose to use the Minimum Description Length (MDL) principle to devise an evaluation metric. Contrary to the established practice of limiting the capacity of the readout model, we design a hybrid discrete and continuous-valued model space for the readout models and employ a switching strategy to combine their predictions. The MDL score takes model complexity, as well as data efficiency into account. As a result, the most appropriate model for the specific task and representation will be chosen, making it a unified measure for comparison. The proposed metric can be efficiently computed with an online method and we present results for pre-trained vision encoders of various architectures (ResNet and ViT) and objective functions (supervised and self-supervised) on a range of downstream tasks. We compare our methods with accuracy-based approaches and show that the latter are inconsistent when multiple readout models are used. Finally, we discuss important properties revealed by our evaluations such as model scaling, preferred readout model, and data efficiency.



### DGP-Net: Dense Graph Prototype Network for Few-Shot SAR Target Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.09584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09584v1)
- **Published**: 2023-02-19 14:33:28+00:00
- **Updated**: 2023-02-19 14:33:28+00:00
- **Authors**: Xiangyu Zhou, Qianru Wei, Yuhui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The inevitable feature deviation of synthetic aperture radar (SAR) image due to the special imaging principle (depression angle variation) leads to poor recognition accuracy, especially in few-shot learning (FSL). To deal with this problem, we propose a dense graph prototype network (DGP-Net) to eliminate the feature deviation by learning potential features, and classify by learning feature distribution. The role of the prototype in this model is to solve the problem of large distance between congeneric samples taken due to the contingency of single sampling in FSL, and enhance the robustness of the model. Experimental results on the MSTAR dataset show that the DGP-Net has good classification results for SAR images with different depression angles and the recognition accuracy of it is higher than typical FSL methods.



### FusionMotion: Multi-Sensor Asynchronous Fusion for Continuous Occupancy Prediction via Neural-ODE
- **Arxiv ID**: http://arxiv.org/abs/2302.09585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09585v1)
- **Published**: 2023-02-19 14:38:01+00:00
- **Updated**: 2023-02-19 14:38:01+00:00
- **Authors**: Yining Shi, Kun Jiang, Ke Wang, Jiusi Li, Yunlong Wang, Diange Yang
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Occupancy maps are widely recognized as an efficient method for facilitating robot motion planning in static environments. However, for intelligent vehicles, occupancy of both the present and future moments is required to ensure safe driving. In the automotive industry, the accurate and continuous prediction of future occupancy maps in traffic scenarios remains a formidable challenge. This paper investigates multi-sensor spatio-temporal fusion strategies for continuous occupancy prediction in a systematic manner. This paper presents FusionMotion, a novel bird's eye view (BEV) occupancy predictor which is capable of achieving the fusion of asynchronous multi-sensor data and predicting the future occupancy map with variable time intervals and temporal horizons. Remarkably, FusionMotion features the adoption of neural ordinary differential equations on recurrent neural networks for occupancy prediction. FusionMotion learns derivatives of BEV features over temporal horizons, updates the implicit sensor's BEV feature measurements and propagates future states for each ODE step. Extensive experiments on large-scale nuScenes and Lyft L5 datasets demonstrate that FusionMotion significantly outperforms previous methods. In addition, it outperforms the BEVFusion-style fusion strategy on the Lyft L5 dataset while reducing synchronization requirements. Codes and models will be made available.



### Accelerated Video Annotation driven by Deep Detector and Tracker
- **Arxiv ID**: http://arxiv.org/abs/2302.09590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2302.09590v1)
- **Published**: 2023-02-19 15:16:05+00:00
- **Updated**: 2023-02-19 15:16:05+00:00
- **Authors**: Eric Price, Aamir Ahmad
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Annotating object ground truth in videos is vital for several downstream tasks in robot perception and machine learning, such as for evaluating the performance of an object tracker or training an image-based object detector. The accuracy of the annotated instances of the moving objects on every image frame in a video is crucially important. Achieving that through manual annotations is not only very time consuming and labor intensive, but is also prone to high error rate. State-of-the-art annotation methods depend on manually initializing the object bounding boxes only in the first frame and then use classical tracking methods, e.g., adaboost, or kernelized correlation filters, to keep track of those bounding boxes. These can quickly drift, thereby requiring tedious manual supervision. In this paper, we propose a new annotation method which leverages a combination of a learning-based detector (SSD) and a learning-based tracker (RE$^3$). Through this, we significantly reduce annotation drifts, and, consequently, the required manual supervision. We validate our approach through annotation experiments using our proposed annotation method and existing baselines on a set of drone video frames. Source code and detailed information on how to run the annotation program can be found at https://github.com/robot-perception-group/smarter-labelme



### Guided Depth Map Super-resolution: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.09598v2
- **DOI**: 10.1145/3584860
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09598v2)
- **Published**: 2023-02-19 15:43:54+00:00
- **Updated**: 2023-03-07 13:50:01+00:00
- **Authors**: Zhiwei Zhong, Xianming Liu, Junjun Jiang, Debin Zhao, Xiangyang Ji
- **Comment**: Accepted by ACM Computing Surveys
- **Journal**: None
- **Summary**: Guided depth map super-resolution (GDSR), which aims to reconstruct a high-resolution (HR) depth map from a low-resolution (LR) observation with the help of a paired HR color image, is a longstanding and fundamental problem, it has attracted considerable attention from computer vision and image processing communities. A myriad of novel and effective approaches have been proposed recently, especially with powerful deep learning techniques. This survey is an effort to present a comprehensive survey of recent progress in GDSR. We start by summarizing the problem of GDSR and explaining why it is challenging. Next, we introduce some commonly used datasets and image quality assessment methods. In addition, we roughly classify existing GDSR methods into three categories, i.e., filtering-based methods, prior-based methods, and learning-based methods. In each category, we introduce the general description of the published algorithms and design principles, summarize the representative methods, and discuss their highlights and limitations. Moreover, the depth related applications are introduced. Furthermore, we conduct experiments to evaluate the performance of some representative methods based on unified experimental configurations, so as to offer a systematic and fair performance evaluation to readers. Finally, we conclude this survey with possible directions and open problems for further research. All the related materials can be found at \url{https://github.com/zhwzhong/Guided-Depth-Map-Super-resolution-A-Survey}.



### Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution
- **Arxiv ID**: http://arxiv.org/abs/2302.09601v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09601v1)
- **Published**: 2023-02-19 15:47:24+00:00
- **Updated**: 2023-02-19 15:47:24+00:00
- **Authors**: Jie Wang, Rui Yang, Zijie Geng, Zhihao Shi, Mingxuan Ye, Qi Zhou, Shuiwang Ji, Bin Li, Yongdong Zhang, Feng Wu
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Generalization in partially observed markov decision processes (POMDPs) is critical for successful applications of visual reinforcement learning (VRL) in real scenarios. A widely used idea is to learn task-relevant representations that encode task-relevant information of common features in POMDPs, i.e., rewards and transition dynamics. As transition dynamics in the latent state space -- which are task-relevant and invariant to visual distractions -- are unknown to the agents, existing methods alternatively use transition dynamics in the observation space to extract task-relevant information in transition dynamics. However, such transition dynamics in the observation space involve task-irrelevant visual distractions, degrading the generalization performance of VRL methods. To tackle this problem, we propose the reward sequence distribution conditioned on the starting observation and the predefined subsequent action sequence (RSD-OA). The appealing features of RSD-OA include that: (1) RSD-OA is invariant to visual distractions, as it is conditioned on the predefined subsequent action sequence without task-irrelevant information from transition dynamics, and (2) the reward sequence captures long-term task-relevant information in both rewards and transition dynamics. Experiments demonstrate that our representation learning approach based on RSD-OA significantly improves the generalization performance on unseen environments, outperforming several state-of-the-arts on DeepMind Control tasks with visual distractions.



### BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image
- **Arxiv ID**: http://arxiv.org/abs/2302.09629v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09629v2)
- **Published**: 2023-02-19 17:15:56+00:00
- **Updated**: 2023-07-24 12:33:09+00:00
- **Authors**: Md Hafizur Rahman, Md Ali Azam, Md Abir Hossen, Shankarachary Ragi, Venkataramana Gadhamshetty
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Desulfovibrio alaskensis G20 (DA-G20) is utilized as a model for sulfate-reducing bacteria (SRB) that are associated with corrosion issues caused by microorganisms. SRB-based biofilms are thought to be responsible for the billion-dollar-per-year bio-corrosion of metal infrastructure. Understanding the extraction of the bacterial cells' shape and size properties in the SRB-biofilm at different growth stages will assist with the design of anti-corrosion techniques. However, numerous issues affect current approaches, including time-consuming geometric property extraction, low efficiency, and high error rates. This paper proposes BiofilScanner, a Yolact-based deep learning method integrated with invariant moments to address these problems. Our approach efficiently detects and segments bacterial cells in an SRB image while simultaneously invariant moments measure the geometric characteristics of the segmented cells with low errors. The numerical experiments of the proposed method demonstrate that the BiofilmScanner is 2.1x and 6.8x faster than our earlier Mask-RCNN and DLv3+ methods for detecting, segmenting, and measuring the geometric properties of the cell. Furthermore, the BiofilmScanner achieved an F1-score of 85.28% while Mask-RCNN and DLv3+ obtained F1-scores of 77.67% and 75.18%, respectively.



### Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.09636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09636v1)
- **Published**: 2023-02-19 17:46:16+00:00
- **Updated**: 2023-02-19 17:46:16+00:00
- **Authors**: Xinyue Hu, Lin Gu, Kazuma Kobayashi, Qiyuan An, Qingyu Chen, Zhiyong Lu, Chang Su, Tatsuya Harada, Yingying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Medical visual question answering (VQA) aims to answer clinically relevant questions regarding input medical images. This technique has the potential to improve the efficiency of medical professionals while relieving the burden on the public health system, particularly in resource-poor countries. Existing medical VQA methods tend to encode medical images and learn the correspondence between visual features and questions without exploiting the spatial, semantic, or medical knowledge behind them. This is partially because of the small size of the current medical VQA dataset, which often includes simple questions. Therefore, we first collected a comprehensive and large-scale medical VQA dataset, focusing on chest X-ray images. The questions involved detailed relationships, such as disease names, locations, levels, and types in our dataset. Based on this dataset, we also propose a novel baseline method by constructing three different relationship graphs: spatial relationship, semantic relationship, and implicit relationship graphs on the image regions, questions, and semantic labels. The answer and graph reasoning paths are learned for different questions.



### Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data
- **Arxiv ID**: http://arxiv.org/abs/2302.09657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09657v1)
- **Published**: 2023-02-19 19:13:24+00:00
- **Updated**: 2023-02-19 19:13:24+00:00
- **Authors**: Kaustubh Milind Kulkarni, Rohan S Jamadagni, Jeffrey Aaron Paul, Sucheth Shenoy
- **Comment**: 9 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: In this work, the novel task of detecting and classifying table tennis strokes solely using the ball trajectory has been explored. A single camera setup positioned in the umpire's view has been employed to procure a dataset consisting of six stroke classes executed by four professional table tennis players. Ball tracking using YOLOv4, a traditional object detection model, and TrackNetv2, a temporal heatmap based model, have been implemented on our dataset and their performances have been benchmarked. A mathematical approach developed to extract temporal boundaries of strokes using the ball trajectory data yielded a total of 2023 valid strokes in our dataset, while also detecting services and missed strokes successfully. The temporal convolutional network developed performed stroke recognition on completely unseen data with an accuracy of 87.155%. Several machine learning and deep learning based model architectures have been trained for stroke recognition using ball trajectory input and benchmarked based on their performances. While stroke recognition in the field of table tennis has been extensively explored based on human action recognition using video data focused on the player's actions, the use of ball trajectory data for the same is an unexplored characteristic of the sport. Hence, the motivation behind the work is to demonstrate that meaningful inferences such as stroke detection and recognition can be drawn using minimal input information.



### Mimicking a Pathologist: Dual Attention Model for Scoring of Gigapixel Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2302.09682v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09682v1)
- **Published**: 2023-02-19 22:26:25+00:00
- **Updated**: 2023-02-19 22:26:25+00:00
- **Authors**: Manahil Raza, Ruqayya Awan, Raja Muhammad Saad Bashir, Talha Qaiser, Nasir M. Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Some major challenges associated with the automated processing of whole slide images (WSIs) includes their sheer size, different magnification levels and high resolution. Utilizing these images directly in AI frameworks is computationally expensive due to memory constraints, while downsampling WSIs incurs information loss and splitting WSIs into tiles and patches results in loss of important contextual information. We propose a novel dual attention approach, consisting of two main components, to mimic visual examination by a pathologist. The first component is a soft attention model which takes as input a high-level view of the WSI to determine various regions of interest. We employ a custom sampling method to extract diverse and spatially distinct image tiles from selected high attention areas. The second component is a hard attention classification model, which further extracts a sequence of multi-resolution glimpses from each tile for classification. Since hard attention is non-differentiable, we train this component using reinforcement learning and predict the location of glimpses without processing all patches of a given tile, thereby aligning with pathologist's way of diagnosis. We train our components both separately and in an end-to-end fashion using a joint loss function to demonstrate the efficacy of our proposed model. We employ our proposed model on two different IHC use cases: HER2 prediction on breast cancer and prediction of Intact/Loss status of two MMR biomarkers, for colorectal cancer. We show that the proposed model achieves accuracy comparable to state-of-the-art methods while only processing a small fraction of the WSI at highest magnification.



### An Efficient and Robust Method for Chest X-Ray Rib Suppression that Improves Pulmonary Abnormality Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2302.09696v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09696v1)
- **Published**: 2023-02-19 23:47:02+00:00
- **Updated**: 2023-02-19 23:47:02+00:00
- **Authors**: Di Xu, Qifan Xu, Kevin Nhieu, Dan Ruan, Ke Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: Suppression of thoracic bone shadows on chest X-rays (CXRs) has been indicated to improve the diagnosis of pulmonary disease. Previous approaches can be categorized as unsupervised physical and supervised deep learning models. Nevertheless, with physical models able to preserve morphological details but at the cost of extremely long processing time, existing DL methods face challenges of gathering sufficient/qualitative ground truth (GT) for robust training, thus leading to failure in maintaining clinically acceptable false positive rates. We hereby propose a generalizable yet efficient workflow of two stages: (1) training pairs generation with GT bone shadows eliminated in by a physical model in spatially transformed gradient fields. (2) fully supervised image denoising network training on stage-one datasets for fast rib removal on incoming CXRs. For step two, we designed a densely connected network called SADXNet, combined with peak signal to noise ratio and multi-scale structure similarity index measure objective minimization to suppress bony structures. The SADXNet organizes spatial filters in U shape (e.g., X=7; filters = 16, 64, 256, 512, 256, 64, 16) and preserves the feature map dimension throughout the network flow. Visually, SADXNet can suppress the rib edge and that near the lung wall/vertebra without jeopardizing the vessel/abnormality conspicuity. Quantitively, it achieves RMSE of ~0 during testing with one prediction taking <1s. Downstream tasks including lung nodule detection as well as common lung disease classification and localization are used to evaluate our proposed rib suppression mechanism. We observed 3.23% and 6.62% area under the curve (AUC) increase as well as 203 and 385 absolute false positive decrease for lung nodule detection and common lung disease localization, separately.



