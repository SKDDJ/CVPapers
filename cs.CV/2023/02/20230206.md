# Arxiv Papers in cs.CV on 2023-02-06
### Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.02515v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02515v1)
- **Published**: 2023-02-06 01:01:00+00:00
- **Updated**: 2023-02-06 01:01:00+00:00
- **Authors**: Navid Mohammadi Foumani, Lynn Miller, Chang Wei Tan, Geoffrey I. Webb, Germain Forestier, Mahsa Salehi
- **Comment**: None
- **Journal**: None
- **Summary**: Time Series Classification and Extrinsic Regression are important and challenging machine learning tasks. Deep learning has revolutionized natural language processing and computer vision and holds great promise in other fields such as time series analysis where the relevant features must often be abstracted from the raw data but are not known a priori. This paper surveys the current state of the art in the fast-moving field of deep learning for time series classification and extrinsic regression. We review different network architectures and training methods used for these tasks and discuss the challenges and opportunities when applying deep learning to time series data. We also summarize two critical applications of time series classification and extrinsic regression, human activity recognition and satellite earth observation.



### RDFNet: Regional Dynamic FISTA-Net for Spectral Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.02519v1
- **DOI**: 10.1109/TCI.2023.3237175
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02519v1)
- **Published**: 2023-02-06 01:13:13+00:00
- **Updated**: 2023-02-06 01:13:13+00:00
- **Authors**: Shiyun Zhou, Tingfa Xu, Shaocong Dong, Jianan Li
- **Comment**: IEEE Transactions on Computational Imaging
- **Journal**: None
- **Summary**: Deep convolutional neural networks have recently shown promising results in compressive spectral reconstruction. Previous methods, however, usually adopt a single mapping function for sparse representation. Considering that different regions have distinct characteristics, it is desirable to apply various mapping functions to adjust different regions' transformations dynamically. With this in mind, we first introduce a regional dynamic way of using Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) to exploit regional characteristics and derive dynamic sparse representations. Then, we propose to unfold the process into a hierarchical dynamic deep network, dubbed RDFNet. The network comprises multiple regional dynamic blocks and corresponding pixel-wise adaptive soft-thresholding modules, respectively in charge of region-based dynamic mapping and pixel-wise soft-thresholding selection. The regional dynamic block guides the network to adjust the transformation domain for different regions. Equipped with the adaptive soft-thresholding, our proposed regional dynamic architecture can also learn appropriate shrinkage scale in a pixel-wise manner.   Extensive experiments on both simulated and real data demonstrate that our method outperforms prior state-of-the-arts.



### Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02521v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02521v2)
- **Published**: 2023-02-06 01:28:52+00:00
- **Updated**: 2023-07-14 23:49:10+00:00
- **Authors**: Yongsheng Mei, Guru Venkataramani, Tian Lan
- **Comment**: 2023 ICML Workshop on Machine Learning for Multimodal Healthcare Data
  (ML4MHD)
- **Journal**: None
- **Summary**: Learning with multiple modalities is crucial for automated brain tumor segmentation from magnetic resonance imaging data. Explicitly optimizing the common information shared among all modalities (e.g., by maximizing the total correlation) has been shown to achieve better feature representations and thus enhance the segmentation performance. However, existing approaches are oblivious to partial common information shared by subsets of the modalities. In this paper, we show that identifying such partial common information can significantly boost the discriminative power of image segmentation models. In particular, we introduce a novel concept of partial common information mask (PCI-mask) to provide a fine-grained characterization of what partial common information is shared by which subsets of the modalities. By solving a masked correlation maximization and simultaneously learning an optimal PCI-mask, we identify the latent microstructure of partial common information and leverage it in a self-attention module to selectively weight different feature representations in multi-modal data. We implement our proposed framework on the standard U-Net. Our experimental results on the Multi-modal Brain Tumor Segmentation Challenge (BraTS) datasets outperform those of state-of-the-art segmentation baselines, with validation Dice similarity coefficients of 0.920, 0.897, 0.837 for the whole tumor, tumor core, and enhancing tumor on BraTS-2020.



### Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity
- **Arxiv ID**: http://arxiv.org/abs/2302.02524v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2302.02524v3)
- **Published**: 2023-02-06 01:44:45+00:00
- **Updated**: 2023-02-16 03:02:52+00:00
- **Authors**: Sajid Rahim, Kourosh Sabri, Anna Ells, Alan Wassyng, Mark Lawford, Linyang Chu, Wenbo He
- **Comment**: 10 pages, 4 figures, 7 tables. arXiv admin note: text overlap with
  arXiv:1904.08796 by other authors
- **Journal**: None
- **Summary**: Retinopathy of Prematurity (ROP) is a potentially blinding eye disorder because of damage to the eye's retina which can affect babies born prematurely. Screening of ROP is essential for early detection and treatment. This is a laborious and manual process which requires trained physician performing dilated ophthalmological examination which can be subjective resulting in lower diagnosis success for clinically significant disease. Automated diagnostic methods can assist ophthalmologists increase diagnosis accuracy using deep learning. Several research groups have highlighted various approaches. This paper proposes the use of new novel fundus preprocessing methods using pretrained transfer learning frameworks to create hybrid models to give higher diagnosis accuracy. The evaluations show that these novel methods in comparison to traditional imaging processing contribute to higher accuracy in classifying Plus disease, Stages of ROP and Zones. We achieve accuracy of 97.65% for Plus disease, 89.44% for Stage, 90.24% for Zones with limited training dataset.



### PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration
- **Arxiv ID**: http://arxiv.org/abs/2302.02535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02535v1)
- **Published**: 2023-02-06 02:13:51+00:00
- **Updated**: 2023-02-06 02:13:51+00:00
- **Authors**: Dingxin Zhang, Jianhui Yu, Chaoyi Zhang, Weidong Cai
- **Comment**: Accepted by AAAI2023
- **Journal**: None
- **Summary**: Recent interest in point cloud analysis has led rapid progress in designing deep learning methods for 3D models. However, state-of-the-art models are not robust to rotations, which remains an unknown prior to real applications and harms the model performance. In this work, we introduce a novel Patch-wise Rotation-invariant network (PaRot), which achieves rotation invariance via feature disentanglement and produces consistent predictions for samples with arbitrary rotations. Specifically, we design a siamese training module which disentangles rotation invariance and equivariance from patches defined over different scales, e.g., the local geometry and global shape, via a pair of rotations. However, our disentangled invariant feature loses the intrinsic pose information of each patch. To solve this problem, we propose a rotation-invariant geometric relation to restore the relative pose with equivariant information for patches defined over different scales. Utilising the pose information, we propose a hierarchical module which implements intra-scale and inter-scale feature aggregation for 3D shape learning. Moreover, we introduce a pose-aware feature propagation process with the rotation-invariant relative pose information embedded. Experiments show that our disentanglement module extracts high-quality rotation-robust features and the proposed lightweight model achieves competitive results in rotated 3D object classification and part segmentation tasks. Our project page is released at: https://patchrot.github.io/.



### Domain Re-Modulation for Few-Shot Generative Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2302.02550v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02550v3)
- **Published**: 2023-02-06 03:55:35+00:00
- **Updated**: 2023-05-23 15:26:45+00:00
- **Authors**: Yi Wu, Ziqiang Li, Chaoyue Wang, Heliang Zheng, Shanshan Zhao, Bin Li, Dacheng Tao
- **Comment**: Under Review
- **Journal**: None
- **Summary**: In this study, we delve into the task of few-shot Generative Domain Adaptation (GDA), which involves transferring a pre-trained generator from one domain to a new domain using only a few reference images. Inspired by the way human brains acquire knowledge in new domains, we present an innovative generator structure called Domain Re-Modulation (DoRM). DoRM not only meets the criteria of high quality, large synthesis diversity, and cross-domain consistency, which were achieved by previous research in GDA, but also incorporates memory and domain association, akin to how human brains operate. Specifically, DoRM freezes the source generator and introduces new mapping and affine modules (M&A modules) to capture the attributes of the target domain during GDA. This process resembles the formation of new synapses in human brains. Consequently, a linearly combinable domain shift occurs in the style space. By incorporating multiple new M&A modules, the generator gains the capability to perform high-fidelity multi-domain and hybrid-domain generation. Moreover, to maintain cross-domain consistency more effectively, we introduce a similarity-based structure loss. This loss aligns the auto-correlation map of the target image with its corresponding auto-correlation map of the source image during training. Through extensive experiments, we demonstrate the superior performance of our DoRM and similarity-based structure loss in few-shot GDA, both quantitatively and qualitatively. The code will be available at https://github.com/wuyi2020/DoRM.



### CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets
- **Arxiv ID**: http://arxiv.org/abs/2302.02551v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02551v3)
- **Published**: 2023-02-06 03:59:15+00:00
- **Updated**: 2023-05-31 07:44:28+00:00
- **Authors**: Zachary Novack, Julian McAuley, Zachary C. Lipton, Saurabh Garg
- **Comment**: Accepted at ICML 2023
- **Journal**: None
- **Summary**: Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.



### A Correction-Based Dynamic Enhancement Framework towards Underwater Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02553v1)
- **Published**: 2023-02-06 04:11:52+00:00
- **Updated**: 2023-02-06 04:11:52+00:00
- **Authors**: Yanling Qiu, Qianxue Feng, Boqin Cai, Hongan Wei, Weiling Chen
- **Comment**: None
- **Journal**: None
- **Summary**: To assist underwater object detection for better performance, image enhancement technology is often used as a pre-processing step. However, most of the existing enhancement methods tend to pursue the visual quality of an image, instead of providing effective help for detection tasks. In fact, image enhancement algorithms should be optimized with the goal of utility improvement. In this paper, to adapt to the underwater detection tasks, we proposed a lightweight dynamic enhancement algorithm using a contribution dictionary to guide low-level corrections. Dynamic solutions are designed to capture differences in detection preferences. In addition, it can also balance the inconsistency between the contribution of correction operations and their time complexity. Experimental results in real underwater object detection tasks show the superiority of our proposed method in both generalization and real-time performance.



### Cluster-aware Contrastive Learning for Unsupervised Out-of-distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02598v1)
- **Published**: 2023-02-06 07:21:03+00:00
- **Updated**: 2023-02-06 07:21:03+00:00
- **Authors**: Menglong Chen, Xingtai Gui, Shicai Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised out-of-distribution (OOD) Detection aims to separate the samples falling outside the distribution of training data without label information. Among numerous branches, contrastive learning has shown its excellent capability of learning discriminative representation in OOD detection. However, for its limited vision, merely focusing on instance-level relationship between augmented samples, it lacks attention to the relationship between samples with same semantics. Based on the classic contrastive learning, we propose Cluster-aware Contrastive Learning (CCL) framework for unsupervised OOD detection, which considers both instance-level and semantic-level information. Specifically, we study a cooperation strategy of clustering and contrastive learning to effectively extract the latent semantics and design a cluster-aware contrastive loss function to enhance OOD discriminative ability. The loss function can simultaneously pay attention to the global and local relationships by treating both the cluster centers and the samples belonging to the same cluster as positive samples. We conducted sufficient experiments to verify the effectiveness of our framework and the model achieves significant improvement on various image benchmarks.



### Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need
- **Arxiv ID**: http://arxiv.org/abs/2302.02615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02615v2)
- **Published**: 2023-02-06 08:24:41+00:00
- **Updated**: 2023-04-11 12:46:52+00:00
- **Authors**: Jingyao Li, Pengguang Chen, Shaozuo Yu, Zexin He, Shu Liu, Jiaya Jia
- **Comment**: This paper is accepted by CVPR2023 and our codes are released here:
  https://github.com/JulietLJY/MOOD
- **Journal**: None
- **Summary**: The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection



### COVID-19 Infection Analysis Framework using Novel Boosted CNNs and Radiological Images
- **Arxiv ID**: http://arxiv.org/abs/2302.02619v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02619v1)
- **Published**: 2023-02-06 08:39:27+00:00
- **Updated**: 2023-02-06 08:39:27+00:00
- **Authors**: Saddam Hussain Khan
- **Comment**: 26 Pages, 11 Figures, 6 Tables. arXiv admin note: text overlap with
  arXiv:2209.10963
- **Journal**: None
- **Summary**: COVID-19 is a new pathogen that first appeared in the human population at the end of 2019, and it can lead to novel variants of pneumonia after infection. COVID-19 is a rapidly spreading infectious disease that infects humans faster. Therefore, efficient diagnostic systems may accurately identify infected patients and thus help control their spread. In this regard, a new two-stage analysis framework is developed to analyze minute irregularities of COVID-19 infection. A novel detection Convolutional Neural Network (CNN), STM-BRNet, is developed that incorporates the Split-Transform-Merge (STM) block and channel boosting (CB) to identify COVID-19 infected CT slices in the first stage. Each STM block extracts boundary and region-smoothing-specific features for COVID-19 infection detection. Moreover, the various boosted channels are obtained by introducing the new CB and Transfer Learning (TL) concept in STM blocks to capture small illumination and texture variations of COVID-19-specific images. The COVID-19 CTs are provided with new SA-CB-BRSeg segmentation CNN for delineating infection in images in the second stage. SA-CB-BRSeg methodically utilized smoothening and heterogeneous operations in the encoder and decoder to capture simultaneously COVID-19 specific patterns that are region homogeneity, texture variation, and boundaries. Additionally, the new CB concept is introduced in the decoder of SA-CB-BRSeg by combining additional channels using TL to learn the low contrast region. The proposed STM-BRNet and SA-CB-BRSeg yield considerable achievement in accuracy: 98.01 %, Recall: 98.12%, F-score: 98.11%, and Dice Similarity: 96.396%, IOU: 98.845 % for the COVID-19 infectious region, respectively. The proposed two-stage framework significantly increased performance compared to single-phase and other reported systems and reduced the burden on the radiologists.



### Uncertainty Calibration and its Application to Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.02622v1)
- **Published**: 2023-02-06 08:41:07+00:00
- **Updated**: 2023-02-06 08:41:07+00:00
- **Authors**: Fabian Küppers
- **Comment**: PhD thesis at University of Wuppertal, cite by: 'Fabian K\"uppers.
  "Uncertainty Calibration and its Application to Object Detection." PhD
  Thesis, University of Wuppertal, January 2023'
- **Journal**: None
- **Summary**: Image-based environment perception is an important component especially for driver assistance systems or autonomous driving. In this scope, modern neuronal networks are used to identify multiple objects as well as the according position and size information within a single frame. The performance of such an object detection model is important for the overall performance of the whole system. However, a detection model might also predict these objects under a certain degree of uncertainty. [...]   In this work, we examine the semantic uncertainty (which object type?) as well as the spatial uncertainty (where is the object and how large is it?). We evaluate if the predicted uncertainties of an object detection model match with the observed error that is achieved on real-world data. In the first part of this work, we introduce the definition for confidence calibration of the semantic uncertainty in the context of object detection, instance segmentation, and semantic segmentation. We integrate additional position information in our examinations to evaluate the effect of the object's position on the semantic calibration properties. Besides measuring calibration, it is also possible to perform a post-hoc recalibration of semantic uncertainty that might have turned out to be miscalibrated. [...]   The second part of this work deals with the spatial uncertainty obtained by a probabilistic detection model. [...] We review and extend common calibration methods so that it is possible to obtain parametric uncertainty distributions for the position information in a more flexible way.   In the last part, we demonstrate a possible use-case for our derived calibration methods in the context of object tracking. [...] We integrate our previously proposed calibration techniques and demonstrate the usefulness of semantic and spatial uncertainty calibration in a subsequent process. [...]



### Approximation of radiative transfer for surface spectral features
- **Arxiv ID**: http://arxiv.org/abs/2302.02641v2
- **DOI**: 10.1109/LGRS.2023.3263356
- **Categories**: **astro-ph.EP**, cs.CV, physics.ao-ph, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2302.02641v2)
- **Published**: 2023-02-06 09:22:58+00:00
- **Updated**: 2023-04-13 08:02:18+00:00
- **Authors**: Frédéric Schmidt
- **Comment**: 4 pages, 3 figures, submitted 21st october 2022 to IEEE Geoscience
  and Remote Sensing Letters
- **Journal**: IEEE Geoscience and Remote Sensing Letters, 2023, 20, 1-3
- **Summary**: Remote sensing hyperspectral and more generally spectral instruments are common tools to decipher surface features in Earth and Planetary science. While linear mixture is the most common approximation for compounds detection (mineral, water, ice, etc...), the transfer of light in surface and atmospheric medium are highly non-linear. The exact simulation of non-linearities can be estimated at very high numerical cost. Here I propose a very simple non-linear form (that includes the regular linear area mixture) of radiative transfer to approximate surface spectral feature. I demonstrate that this analytical form is able to approximate the grain size and intimate mixture dependence of surface features. In addition, the same analytical form can approximate the effect of Martian mineral aerosols. Unfortunately, Earth aerosols are more complex (water droplet, water ice, soot,...) and are not expected to follow the same trend.



### 1st Place Solution for PSG competition with ECCV'22 SenseHuman Workshop
- **Arxiv ID**: http://arxiv.org/abs/2302.02651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02651v1)
- **Published**: 2023-02-06 09:47:46+00:00
- **Updated**: 2023-02-06 09:47:46+00:00
- **Authors**: Qixun Wang, Xiaofeng Guo, Haofan Wang
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Panoptic Scene Graph (PSG) generation aims to generate scene graph representations based on panoptic segmentation instead of rigid bounding boxes. Existing PSG methods utilize one-stage paradigm which simultaneously generates scene graphs and predicts semantic segmentation masks or two-stage paradigm that first adopt an off-the-shelf panoptic segmentor, then pairwise relationship prediction between these predicted objects. One-stage approach despite having a simplified training paradigm, its segmentation results are usually under-satisfactory, while two-stage approach lacks global context and leads to low performance on relation prediction. To bridge this gap, in this paper, we propose GRNet, a Global Relation Network in two-stage paradigm, where the pre-extracted local object features and their corresponding masks are fed into a transformer with class embeddings. To handle relation ambiguity and predicate classification bias caused by long-tailed distribution, we formulate relation prediction in the second stage as a multi-class classification task with soft label. We conduct comprehensive experiments on OpenPSG dataset and achieve the state-of-art performance on the leadboard. We also show the effectiveness of our soft label strategy for long-tailed classes in ablation studies. Our code has been released in https://github.com/wangqixun/mfpsg.



### HyperSLICE: HyperBand optimized Spiral for Low-latency Interactive Cardiac Examination
- **Arxiv ID**: http://arxiv.org/abs/2302.02688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02688v2)
- **Published**: 2023-02-06 10:41:57+00:00
- **Updated**: 2023-06-16 15:55:15+00:00
- **Authors**: Dr. Olivier Jaubert, Dr. Javier Montalt-Tordera, Dr. Daniel Knight, Pr. Simon Arridge, Dr. Jennifer Steeden, Pr. Vivek Muthurangu
- **Comment**: None
- **Journal**: None
- **Summary**: PURPOSE: Interactive cardiac magnetic resonance imaging is used for fast scan planning and MR guided interventions. However, the requirement for real-time acquisition and near real-time visualization constrains the achievable spatio-temporal resolution. This study aims to improve interactive imaging resolution through optimization of undersampled spiral sampling and leveraging of deep learning for low-latency reconstruction (deep artifact suppression). METHODS: A variable density spiral trajectory was parametrized and optimized via HyperBand to provide the best candidate trajectory for rapid deep artifact suppression. Training data consisted of 692 breath-held CINEs. The developed interactive sequence was tested in simulations and prospectively in 13 subjects (10 for image evaluation, 2 during catheterization, 1 during exercise). In the prospective study, the optimized framework -HyperSLICE- was compared to conventional Cartesian real-time, and breath-hold CINE imaging in terms quantitative and qualitative image metrics. Statistical differences were tested using Friedman chi-squared tests with post-hoc Nemenyi test (p<0.05). RESULTS: In simulations the NRMSE, pSNR, SSIM and LAPE were all statistically significantly higher using optimized spiral compared to radial and uniform spiral sampling, particularly after scan plan changes (SSIM: 0.71 vs 0.45 and 0.43). Prospectively, HyperSLICE enabled a higher spatial and temporal resolution than conventional Cartesian real-time imaging. The pipeline was demonstrated in patients during catheter pull back showing sufficiently fast reconstruction for interactive imaging. CONCLUSION: HyperSLICE enables high spatial and temporal resolution interactive imaging. Optimizing the spiral sampling enabled better overall image quality and superior handling of image transitions compared to radial and uniform spiral trajectories.



### PatchDCT: Patch Refinement for High Quality Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02693v2)
- **Published**: 2023-02-06 10:51:21+00:00
- **Updated**: 2023-02-07 13:14:04+00:00
- **Authors**: Qinrou Wen, Jirui Yang, Xue Yang, Kewei Liang
- **Comment**: 15 pages, 7 figures, 13 tables, accepted by ICLR 2023, the source
  code is available at https://github.com/olivia-w12/PatchDCT
- **Journal**: None
- **Summary**: High-quality instance segmentation has shown emerging importance in computer vision. Without any refinement, DCT-Mask directly generates high-resolution masks by compressed vectors. To further refine masks obtained by compressed vectors, we propose for the first time a compressed vector based multi-stage refinement framework. However, the vanilla combination does not bring significant gains, because changes in some elements of the DCT vector will affect the prediction of the entire mask. Thus, we propose a simple and novel method named PatchDCT, which separates the mask decoded from a DCT vector into several patches and refines each patch by the designed classifier and regressor. Specifically, the classifier is used to distinguish mixed patches from all patches, and to correct previously mispredicted foreground and background patches. In contrast, the regressor is used for DCT vector prediction of mixed patches, further refining the segmentation quality at boundary locations. Experiments on COCO show that our method achieves 2.0%, 3.2%, 4.5% AP and 3.4%, 5.3%, 7.0% Boundary AP improvements over Mask-RCNN on COCO, LVIS, and Cityscapes, respectively. It also surpasses DCT-Mask by 0.7%, 1.1%, 1.3% AP and 0.9%, 1.7%, 4.2% Boundary AP on COCO, LVIS and Cityscapes. Besides, the performance of PatchDCT is also competitive with other state-of-the-art methods.



### AMD-HookNet for Glacier Front Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02744v1
- **DOI**: 10.1109/TGRS.2023.3245419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02744v1)
- **Published**: 2023-02-06 12:39:40+00:00
- **Updated**: 2023-02-06 12:39:40+00:00
- **Authors**: Fei Wu, Nora Gourmelon, Thorsten Seehaus, Jianlin Zhang, Matthias Braun, Andreas Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge on changes in glacier calving front positions is important for assessing the status of glaciers. Remote sensing imagery provides the ideal database for monitoring calving front positions, however, it is not feasible to perform this task manually for all calving glaciers globally due to time-constraints. Deep learning-based methods have shown great potential for glacier calving front delineation from optical and radar satellite imagery. The calving front is represented as a single thin line between the ocean and the glacier, which makes the task vulnerable to inaccurate predictions. The limited availability of annotated glacier imagery leads to a lack of data diversity (not all possible combinations of different weather conditions, terminus shapes, sensors, etc. are present in the data), which exacerbates the difficulty of accurate segmentation. In this paper, we propose Attention-Multi-hooking-Deep-supervision HookNet (AMD-HookNet), a novel glacier calving front segmentation framework for synthetic aperture radar (SAR) images. The proposed method aims to enhance the feature representation capability through multiple information interactions between low-resolution and high-resolution inputs based on a two-branch U-Net. The attention mechanism, integrated into the two branch U-Net, aims to interact between the corresponding coarse and fine-grained feature maps. This allows the network to automatically adjust feature relationships, resulting in accurate pixel-classification predictions. Extensive experiments and comparisons on the challenging glacier segmentation benchmark dataset CaFFe show that our AMD-HookNet achieves a mean distance error of 438 m to the ground truth outperforming the current state of the art by 42%, which validates its effectiveness.



### Baseline Method for the Sport Task of MediaEval 2022 with 3D CNNs using Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2302.02752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.02752v1)
- **Published**: 2023-02-06 12:58:01+00:00
- **Updated**: 2023-02-06 12:58:01+00:00
- **Authors**: Pierre-Etienne Martin
- **Comment**: Baseline paper for the sport Task of MediaEval 2022
- **Journal**: None
- **Summary**: This paper presents the baseline method proposed for the Sports Video task part of the MediaEval 2022 benchmark. This task proposes two subtasks: stroke classification from trimmed videos, and stroke detection from untrimmed videos. This baseline addresses both subtasks. We propose two types of 3D-CNN architectures to solve the two subtasks. Both 3D-CNNs use Spatio-temporal convolutions and attention mechanisms. The architectures and the training process are tailored to solve the addressed subtask. This baseline method is shared publicly online to help the participants in their investigation and alleviate eventually some aspects of the task such as video processing, training method, evaluation and submission routine. The baseline method reaches 86.4% of accuracy with our v2 model for the classification subtask. For the detection subtask, the baseline reaches a mAP of 0.131 and IoU of 0.515 with our v1 model.



### Fine-Grained Action Detection with RGB and Pose Information using Two Stream Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.02755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.02755v1)
- **Published**: 2023-02-06 13:05:55+00:00
- **Updated**: 2023-02-06 13:05:55+00:00
- **Authors**: Leonard Hacker, Finn Bartels, Pierre-Etienne Martin
- **Comment**: Working note paper of the sport task of MediaEval 2022 in Bergen,
  Norway, 12-13 Jan 2023
- **Journal**: None
- **Summary**: As participants of the MediaEval 2022 Sport Task, we propose a two-stream network approach for the classification and detection of table tennis strokes. Each stream is a succession of 3D Convolutional Neural Network (CNN) blocks using attention mechanisms. Each stream processes different 4D inputs. Our method utilizes raw RGB data and pose information computed from MMPose toolbox. The pose information is treated as an image by applying the pose either on a black background or on the original RGB frame it has been computed from. Best performance is obtained by feeding raw RGB data to one stream, Pose + RGB (PRGB) information to the other stream and applying late fusion on the features. The approaches were evaluated on the provided TTStroke-21 data sets. We can report an improvement in stroke classification, reaching 87.3% of accuracy, while the detection does not outperform the baseline but still reaches an IoU of 0.349 and mAP of 0.110.



### Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.02790v2
- **DOI**: 10.1109/IV55152.2023.10186609
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02790v2)
- **Published**: 2023-02-06 14:07:13+00:00
- **Updated**: 2023-03-31 11:43:42+00:00
- **Authors**: Daniel Bogdoll, Svenja Uhlemeyer, Kamil Kowol, J. Marius Zöllner
- **Comment**: Accepted for publication at IV 2023
- **Journal**: None
- **Summary**: Deep neural networks (DNN) which are employed in perception systems for autonomous driving require a huge amount of data to train on, as they must reliably achieve high performance in all kinds of situations. However, these DNN are usually restricted to a closed set of semantic classes available in their training data, and are therefore unreliable when confronted with previously unseen instances. Thus, multiple perception datasets have been created for the evaluation of anomaly detection methods, which can be categorized into three groups: real anomalies in real-world, synthetic anomalies augmented into real-world and completely synthetic scenes. This survey provides a structured and, to the best of our knowledge, complete overview and comparison of perception datasets for anomaly detection in autonomous driving. Each chapter provides information about tasks and ground truth, context information, and licenses. Additionally, we discuss current weaknesses and gaps in existing datasets to underline the importance of developing further data.



### Stop overkilling simple tasks with black-box models and use transparent models instead
- **Arxiv ID**: http://arxiv.org/abs/2302.02804v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02804v2)
- **Published**: 2023-02-06 14:28:49+00:00
- **Updated**: 2023-06-14 13:10:43+00:00
- **Authors**: Matteo Rizzo, Matteo Marcuzzo, Alessandro Zangari, Andrea Gasparetto, Andrea Albarelli
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the employment of deep learning methods has led to several significant breakthroughs in artificial intelligence. Different from traditional machine learning models, deep learning-based approaches are able to extract features autonomously from raw data. This allows for bypassing the feature engineering process, which is generally considered to be both error-prone and tedious. Moreover, deep learning strategies often outperform traditional models in terms of accuracy.



### MixFormer: End-to-End Tracking with Iterative Mixed Attention
- **Arxiv ID**: http://arxiv.org/abs/2302.02814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02814v2)
- **Published**: 2023-02-06 14:38:09+00:00
- **Updated**: 2023-02-09 18:15:41+00:00
- **Authors**: Yutao Cui, Cheng Jiang, Gangshan Wu, Limin Wang
- **Comment**: Extended version of the paper arXiv:2203.11082 presented at CVPR
  2022. In particular, the extented MixViT-L achieves AUC score of 73.3% on
  LaSOT. Besides, we design a new TrackMAE pre-training method for tracking
  Code has been released
- **Journal**: None
- **Summary**: Visual object tracking often employs a multi-stage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, in this paper, we present a compact tracking framework, termed as MixFormer, built upon transformers. Our core design is to utilize the flexibility of attention operations, and propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer trackers simply by stacking multiple MAMs and placing a localization head on top. Specifically, we instantiate two types of MixFormer trackers, a hierarchical tracker MixCvT, and a non-hierarchical tracker MixViT. For these two trackers, we investigate a series of pre-training methods and uncover the different behaviors between supervised pre-training and self-supervised pre-training in our MixFormer trackers. We also extend the masked pre-training to our MixFormer trackers and design the competitive TrackMAE pre-training technique. Finally, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. Our MixFormer trackers set a new state-of-the-art performance on seven tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10k, OTB100 and UAV123. In particular, our MixViT-L achieves AUC score of 73.3% on LaSOT, 86.1% on TrackingNet, EAO of 0.584 on VOT2020, and AO of 75.7% on GOT-10k. Code and trained models are publicly available at https://github.com/MCG-NJU/MixFormer.



### An Unsupervised Framework for Joint MRI Super Resolution and Gibbs Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/2302.02849v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02849v1)
- **Published**: 2023-02-06 15:14:58+00:00
- **Updated**: 2023-02-06 15:14:58+00:00
- **Authors**: Yikang Liu, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun
- **Comment**: Accepted by the 28th biennial international conference on Information
  Processing in Medical Imaging (IPMI 2023)
- **Journal**: None
- **Summary**: The k-space data generated from magnetic resonance imaging (MRI) is only a finite sampling of underlying signals. Therefore, MRI images often suffer from low spatial resolution and Gibbs ringing artifacts. Previous studies tackled these two problems separately, where super resolution methods tend to enhance Gibbs artifacts, whereas Gibbs ringing removal methods tend to blur the images. It is also a challenge that high resolution ground truth is hard to obtain in clinical MRI. In this paper, we propose an unsupervised learning framework for both MRI super resolution and Gibbs artifacts removal without using high resolution ground truth. Furthermore, we propose regularization methods to improve the model's generalizability across out-of-distribution MRI images. We evaluated our proposed methods with other state-of-the-art methods on eight MRI datasets with various contrasts and anatomical structures. Our method not only achieves the best SR performance but also significantly reduces the Gibbs artifacts. Our method also demonstrates good generalizability across different datasets, which is beneficial to clinical applications where training data are usually scarce and biased.



### TR3D: Towards Real-Time Indoor 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02858v2)
- **Published**: 2023-02-06 15:25:50+00:00
- **Updated**: 2023-02-08 13:06:41+00:00
- **Authors**: Danila Rukhovich, Anna Vorontsova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, sparse 3D convolutions have changed 3D object detection. Performing on par with the voting-based approaches, 3D CNNs are memory-efficient and scale to large scenes better. However, there is still room for improvement. With a conscious, practice-oriented approach to problem-solving, we analyze the performance of such methods and localize the weaknesses. Applying modifications that resolve the found issues one by one, we end up with TR3D: a fast fully-convolutional 3D object detection model trained end-to-end, that achieves state-of-the-art results on the standard benchmarks, ScanNet v2, SUN RGB-D, and S3DIS. Moreover, to take advantage of both point cloud and RGB inputs, we introduce an early fusion of 2D and 3D features. We employ our fusion module to make conventional 3D object detection methods multimodal and demonstrate an impressive boost in performance. Our model with early feature fusion, which we refer to as TR3D+FF, outperforms existing 3D object detection approaches on the SUN RGB-D dataset. Overall, besides being accurate, both TR3D and TR3D+FF models are lightweight, memory-efficient, and fast, thereby marking another milestone on the way toward real-time 3D object detection. Code is available at https://github.com/SamsungLabs/tr3d .



### Top-Down Beats Bottom-Up in 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02871v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02871v3)
- **Published**: 2023-02-06 15:38:21+00:00
- **Updated**: 2023-03-21 07:37:12+00:00
- **Authors**: Maksim Kolodiazhnyi, Danila Rukhovich, Anna Vorontsova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Most 3D instance segmentation methods exploit a bottom-up strategy, typically including resource-exhaustive post-processing. For point grouping, bottom-up methods rely on prior assumptions about the objects in the form of hyperparameters, which are domain-specific and need to be carefully tuned. On the contrary, we address 3D instance segmentation with a TD3D: top-down, fully data-driven, simple approach trained in an end-to-end manner. With its straightforward fully-convolutional pipeline, it performs surprisingly well on the standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS. Besides, our method is much faster on inference than the current state-of-the-art grouping-based approaches. Code is available at https://github.com/SamsungLabs/td3d .



### Intra-operative Brain Tumor Detection with Deep Learning-Optimized Hyperspectral Imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.02884v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02884v1)
- **Published**: 2023-02-06 15:52:03+00:00
- **Updated**: 2023-02-06 15:52:03+00:00
- **Authors**: Tommaso Giannantonio, Anna Alperovich, Piercosimo Semeraro, Manfredo Atzori, Xiaohan Zhang, Christoph Hauger, Alexander Freytag, Siri Luthman, Roeland Vandebriel, Murali Jayapala, Lien Solie, Steven de Vleeschouwer
- **Comment**: SPIE Photonics West 2023 conference Optical Biopsy XXI: Toward
  Real-Time Spectroscopic Imaging and Diagnosis. 18 pages, 11 figures
- **Journal**: None
- **Summary**: Surgery for gliomas (intrinsic brain tumors), especially when low-grade, is challenging due to the infiltrative nature of the lesion. Currently, no real-time, intra-operative, label-free and wide-field tool is available to assist and guide the surgeon to find the relevant demarcations for these tumors. While marker-based methods exist for the high-grade glioma case, there is no convenient solution available for the low-grade case; thus, marker-free optical techniques represent an attractive option. Although RGB imaging is a standard tool in surgical microscopes, it does not contain sufficient information for tissue differentiation. We leverage the richer information from hyperspectral imaging (HSI), acquired with a snapscan camera in the 468-787 nm range, coupled to a surgical microscope, to build a deep-learning-based diagnostic tool for cancer resection with potential for intra-operative guidance. However, the main limitation of the HSI snapscan camera is the image acquisition time, limiting its widespread deployment in the operation theater. Here, we investigate the effect of HSI channel reduction and pre-selection to scope the design space for the development of cheaper and faster sensors. Neural networks are used to identify the most important spectral channels for tumor tissue differentiation, optimizing the trade-off between the number of channels and precision to enable real-time intra-surgical application. We evaluate the performance of our method on a clinical dataset that was acquired during surgery on five patients. By demonstrating the possibility to efficiently detect low-grade glioma, these results can lead to better cancer resection demarcations, potentially improving treatment effectiveness and patient outcome.



### Neural Document Unwarping using Coupled Grids
- **Arxiv ID**: http://arxiv.org/abs/2302.02887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.02887v1)
- **Published**: 2023-02-06 15:53:34+00:00
- **Updated**: 2023-02-06 15:53:34+00:00
- **Authors**: Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung
- **Comment**: None
- **Journal**: None
- **Summary**: Restoring the original, flat appearance of a printed document from casual photographs of bent and wrinkled pages is a common everyday problem. In this paper we propose a novel method for grid-based single-image document unwarping. Our method performs geometric distortion correction via a deep fully convolutional neural network that learns to predict the 3D grid mesh of the document and the corresponding 2D unwarping grid in a multi-task fashion, implicitly encoding the coupling between the shape of a 3D object and its 2D image. We additionally create and publish our own dataset, called UVDoc, which combines pseudo-photorealistic document images with ground truth grid-based physical 3D and unwarping information, allowing unwarping models to train on data that is more realistic in appearance than the commonly used synthetic Doc3D dataset, whilst also being more physically accurate. Our dataset is labeled with all the information necessary to train our unwarping network, without having to engineer separate loss functions that can deal with the lack of ground-truth typically found in document in the wild datasets. We include a thorough evaluation that demonstrates that our dual-task unwarping network trained on a mix of synthetic and pseudo-photorealistic images achieves state-of-the-art performance on the DocUNet benchmark dataset. Our code, results and UVDoc dataset will be made publicly available upon publication.



### GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks
- **Arxiv ID**: http://arxiv.org/abs/2302.02907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02907v2)
- **Published**: 2023-02-06 16:23:24+00:00
- **Updated**: 2023-05-25 08:01:05+00:00
- **Authors**: Salah Ghamizi, Jingfeng Zhang, Maxime Cordy, Mike Papadakis, Masashi Sugiyama, Yves Le Traon
- **Comment**: None
- **Journal**: None
- **Summary**: While leveraging additional training data is well established to improve adversarial robustness, it incurs the unavoidable cost of data collection and the heavy computation to train models. To mitigate the costs, we propose Guided Adversarial Training (GAT), a novel adversarial training technique that exploits auxiliary tasks under a limited set of training data. Our approach extends single-task models into multi-task models during the min-max optimization of adversarial training, and drives the loss optimization with a regularization of the gradient curvature across multiple tasks. GAT leverages two types of auxiliary tasks: self-supervised tasks, where the labels are generated automatically, and domain-knowledge tasks, where human experts provide additional labels. Experimentally, GAT increases the robust AUC of CheXpert medical imaging dataset from 50% to 83% and On CIFAR-10, GAT outperforms eight state-of-the-art adversarial training and achieves 56.21% robust accuracy with Resnet-50. Overall, we demonstrate that guided multi-task learning is an actionable and promising avenue to push further the boundaries of model robustness.



### LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2302.02908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2302.02908v1)
- **Published**: 2023-02-06 16:24:41+00:00
- **Updated**: 2023-02-06 16:24:41+00:00
- **Authors**: Ziyang luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwen lin, Daxin Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-text retrieval (ITR) is a task to retrieve the relevant images/texts, given the query from another modality. The conventional dense retrieval paradigm relies on encoding images and texts into dense representations using dual-stream encoders, however, it faces challenges with low retrieval speed in large-scale retrieval scenarios. In this work, we propose the lexicon-weighting paradigm, where sparse representations in vocabulary space are learned for images and texts to take advantage of the bag-of-words models and efficient inverted indexes, resulting in significantly reduced retrieval latency. A crucial gap arises from the continuous nature of image data, and the requirement for a sparse vocabulary space representation. To bridge this gap, we introduce a novel pre-training framework, Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon representations. This framework features lexicon-bottlenecked modules between the dual-stream encoders and weakened text decoders, allowing for constructing continuous bag-of-words bottlenecks to learn lexicon-importance distributions. Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art performance on two benchmark ITR datasets, MSCOCO and Flickr30k. Furthermore, in large-scale retrieval scenarios, LexLIP outperforms CLIP with a 5.5 ~ 221.3X faster retrieval speed and 13.2 ~ 48.8X less index storage memory.



### Generating Evidential BEV Maps in Continuous Driving Space
- **Arxiv ID**: http://arxiv.org/abs/2302.02928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02928v1)
- **Published**: 2023-02-06 17:05:50+00:00
- **Updated**: 2023-02-06 17:05:50+00:00
- **Authors**: Yunshuang Yuan, Hao Cheng, Michael Ying Yang, Monika Sester
- **Comment**: None
- **Journal**: None
- **Summary**: Safety is critical for autonomous driving, and one aspect of improving safety is to accurately capture the uncertainties of the perception system, especially knowing the unknown. Different from only providing deterministic or probabilistic results, e.g., probabilistic object detection, that only provide partial information for the perception scenario, we propose a complete probabilistic model named GevBEV. It interprets the 2D driving space as a probabilistic Bird's Eye View (BEV) map with point-based spatial Gaussian distributions, from which one can draw evidence as the parameters for the categorical Dirichlet distribution of any new sample point in the continuous driving space. The experimental results show that GevBEV not only provides more reliable uncertainty quantification but also outperforms the previous works on the benchmark OPV2V of BEV map interpretation for cooperative perception. A critical factor in cooperative perception is the data transmission size through the communication channels. GevBEV helps reduce communication overhead by selecting only the most important information to share from the learned uncertainty, reducing the average information communicated by 80% with a slight performance drop.



### Private GANs, Revisited
- **Arxiv ID**: http://arxiv.org/abs/2302.02936v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02936v1)
- **Published**: 2023-02-06 17:11:09+00:00
- **Updated**: 2023-02-06 17:11:09+00:00
- **Authors**: Alex Bie, Gautam Kamath, Guojun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We show that the canonical approach for training differentially private GANs -- updating the discriminator with differentially private stochastic gradient descent (DPSGD) -- can yield significantly improved results after modifications to training. Existing instantiations of this approach neglect to consider how adding noise only to discriminator updates disrupts the careful balance between the generator and discriminator necessary for successful GAN training. We show that a simple fix -- taking more discriminator steps between generator steps -- restores parity and improves results. Additionally, with the goal of restoring parity between the generator and discriminator, we experiment with other modifications to improve discriminator training and see further improvements in generation quality. Our results demonstrate that on standard benchmarks, DPSGD outperforms all alternative GAN privatization schemes.



### Integrating Eye-Gaze Data into CXR DL Approaches: A Preliminary study
- **Arxiv ID**: http://arxiv.org/abs/2302.02940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.02940v1)
- **Published**: 2023-02-06 17:14:59+00:00
- **Updated**: 2023-02-06 17:14:59+00:00
- **Authors**: André Luís, Chihcheng Hsieh, Isabel Blanco Nobre, Sandra Costa Sousa, Anderson Maciel, Catarina Moreira, Joaquim Jorge
- **Comment**: A version of this paper has been accepted for presentation at the 2nd
  XR Health workshop - XR Technologies for Healthcare and Wellbeing
  https://ieeevr.org/2023/contribute/workshoppapers/#XRHealth
- **Journal**: None
- **Summary**: This paper proposes a novel multimodal DL architecture incorporating medical images and eye-tracking data for abnormality detection in chest x-rays. Our results show that applying eye gaze data directly into DL architectures does not show superior predictive performance in abnormality detection chest X-rays. These results support other works in the literature and suggest that human-generated data, such as eye gaze, needs a more thorough investigation before being applied to DL architectures.



### ConvoWaste: An Automatic Waste Segregation Machine Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.02976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02976v1)
- **Published**: 2023-02-06 18:08:33+00:00
- **Updated**: 2023-02-06 18:08:33+00:00
- **Authors**: Md. Shahariar Nafiz, Shuvra Smaran Das, Md. Kishor Morol, Abdullah Al Juabir, Dip Nandi
- **Comment**: Accepted at ICREST 2023
- **Journal**: None
- **Summary**: Nowadays, proper urban waste management is one of the biggest concerns for maintaining a green and clean environment. An automatic waste segregation system can be a viable solution to improve the sustainability of the country and boost the circular economy. This paper proposes a machine to segregate waste into different parts with the help of a smart object detection algorithm using ConvoWaste in the field of deep convolutional neural networks (DCNN) and image processing techniques. In this paper, deep learning and image processing techniques are applied to precisely classify the waste, and the detected waste is placed inside the corresponding bins with the help of a servo motor-based system. This machine has the provision to notify the responsible authority regarding the waste level of the bins and the time to trash out the bins filled with garbage by using the ultrasonic sensors placed in each bin and the dual-band GSM-based communication technology. The entire system is controlled remotely through an Android app in order to dump the separated waste in the desired place thanks to its automation properties. The use of this system can aid in the process of recycling resources that were initially destined to become waste, utilizing natural resources, and turning these resources back into usable products. Thus, the system helps fulfill the criteria of a circular economy through resource optimization and extraction. Finally, the system is designed to provide services at a low cost while maintaining a high level of accuracy in terms of technological advancement in the field of artificial intelligence (AI). We have gotten 98% accuracy for our ConvoWaste deep learning model.



### Learning disentangled representations for explainable chest X-ray classification using Dirichlet VAEs
- **Arxiv ID**: http://arxiv.org/abs/2302.02979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02979v1)
- **Published**: 2023-02-06 18:10:08+00:00
- **Updated**: 2023-02-06 18:10:08+00:00
- **Authors**: Rachael Harkness, Alejandro F Frangi, Kieran Zucker, Nishant Ravikumar
- **Comment**: 13 pages, 8 figures, to be published in SPIE Medical Imaging 2023
- **Journal**: None
- **Summary**: This study explores the use of the Dirichlet Variational Autoencoder (DirVAE) for learning disentangled latent representations of chest X-ray (CXR) images. Our working hypothesis is that distributional sparsity, as facilitated by the Dirichlet prior, will encourage disentangled feature learning for the complex task of multi-label classification of CXR images. The DirVAE is trained using CXR images from the CheXpert database, and the predictive capacity of multi-modal latent representations learned by DirVAE models is investigated through implementation of an auxiliary multi-label classification task, with a view to enforce separation of latent factors according to class-specific features. The predictive performance and explainability of the latent space learned using the DirVAE were quantitatively and qualitatively assessed, respectively, and compared with a standard Gaussian prior-VAE (GVAE). We introduce a new approach for explainable multi-label classification in which we conduct gradient-guided latent traversals for each class of interest. Study findings indicate that the DirVAE is able to disentangle latent factors into class-specific visual features, a property not afforded by the GVAE, and achieve a marginal increase in predictive performance relative to GVAE. We generate visual examples to show that our explainability method, when applied to the trained DirVAE, is able to highlight regions in CXR images that are clinically relevant to the class(es) of interest and additionally, can identify cases where classification relies on spurious feature correlations.



### Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/2302.02991v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.02991v1)
- **Published**: 2023-02-06 18:29:30+00:00
- **Updated**: 2023-02-06 18:29:30+00:00
- **Authors**: Wenhui Zhu, Peijie Qiu, Mohammad Farazi, Keshav Nandakumar, Oana M. Dumitrascu, Yalin Wang
- **Comment**: Accepted as a conference paper to 20th IEEE International Symposium
  on Biomedical Imaging(ISBI 2023)
- **Journal**: None
- **Summary**: Real-world non-mydriatic retinal fundus photography is prone to artifacts, imperfections and low-quality when certain ocular or systemic co-morbidities exist. Artifacts may result in inaccuracy or ambiguity in clinical diagnoses. In this paper, we proposed a simple but effective end-to-end framework for enhancing poor-quality retinal fundus images. Leveraging the optimal transport theory, we proposed an unpaired image-to-image translation scheme for transporting low-quality images to their high-quality counterparts. We theoretically proved that a Generative Adversarial Networks (GAN) model with a generator and discriminator is sufficient for this task. Furthermore, to mitigate the inconsistency of information between the low-quality images and their enhancements, an information consistency mechanism was proposed to maximally maintain structural consistency (optical discs, blood vessels, lesions) between the source and enhanced domains. Extensive experiments were conducted on the EyeQ dataset to demonstrate the superiority of our proposed method perceptually and quantitatively.



### OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing
- **Arxiv ID**: http://arxiv.org/abs/2302.03003v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.03003v4)
- **Published**: 2023-02-06 18:39:40+00:00
- **Updated**: 2023-04-09 00:56:15+00:00
- **Authors**: Wenhui Zhu, Peijie Qiu, Oana M. Dumitrascu, Jacob M. Sobczak, Mohammad Farazi, Zhangsihao Yang, Keshav Nandakumar, Yalin Wang
- **Comment**: Accepted as a conference paper to The 28th biennial international
  conference on Information Processing in Medical Imaging (IPMI 2023)
- **Journal**: None
- **Summary**: Non-mydriatic retinal color fundus photography (CFP) is widely available due to the advantage of not requiring pupillary dilation, however, is prone to poor quality due to operators, systemic imperfections, or patient-related causes. Optimal retinal image quality is mandated for accurate medical diagnoses and automated analyses. Herein, we leveraged the Optimal Transport (OT) theory to propose an unpaired image-to-image translation scheme for mapping low-quality retinal CFPs to high-quality counterparts. Furthermore, to improve the flexibility, robustness, and applicability of our image enhancement pipeline in the clinical practice, we generalized a state-of-the-art model-based image reconstruction method, regularization by denoising, by plugging in priors learned by our OT-guided image-to-image translation network. We named it as regularization by enhancing (RE). We validated the integrated framework, OTRE, on three publicly available retinal image datasets by assessing the quality after enhancement and their performance on various downstream tasks, including diabetic retinopathy grading, vessel segmentation, and diabetic lesion segmentation. The experimental results demonstrated the superiority of our proposed framework over some state-of-the-art unsupervised competitors and a state-of-the-art supervised method.



### Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.03004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.03004v1)
- **Published**: 2023-02-06 18:39:40+00:00
- **Updated**: 2023-02-06 18:39:40+00:00
- **Authors**: Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, Dacheng Tao
- **Comment**: ICLR 2023 (Notable-top-25%)
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL



### Structure and Content-Guided Video Synthesis with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.03011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03011v1)
- **Published**: 2023-02-06 18:50:23+00:00
- **Updated**: 2023-02-06 18:50:23+00:00
- **Authors**: Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis
- **Comment**: Project page at https://research.runwayml.com/gen1
- **Journal**: None
- **Summary**: Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.



### Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2302.03014v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.03014v3)
- **Published**: 2023-02-06 18:54:14+00:00
- **Updated**: 2023-02-24 10:50:28+00:00
- **Authors**: Neel Kanwal, Roger Amundsen, Helga Hardardottir, Luca Tomasetti, Erling Sandoy Undersrud, Emiel A. M. Janssen, Kjersti Engan
- **Comment**: Submitted to EUSIPCO 23
- **Journal**: None
- **Summary**: Melanoma diagnosed and treated in its early stages can increase the survival rate. A projected increase in skin cancer incidents and a dearth of dermatopathologists have emphasized the need for computational pathology (CPATH) systems. CPATH systems with deep learning (DL) models have the potential to identify the presence of melanoma by exploiting underlying morphological and cellular features. This paper proposes a DL method to detect melanoma and distinguish between normal skin and benign/malignant melanocytic lesions in Whole Slide Images (WSI). Our method detects lesions with high accuracy and localizes them on a WSI to identify potential regions of interest for pathologists. Interestingly, our DL method relies on using a single CNN network to create localization maps first and use them to perform slide-level predictions to determine patients who have melanoma. Our best model provides favorable patch-wise classification results with a 0.992 F1 score and 0.99 sensitivity on unseen data. The source code is https://github.com/RogerAmundsen/Melanoma-Diagnosis-and-Localization-from-Whole-Slide-Images-using-Convolutional-Neural-Networks.



### Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/2302.04634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.FL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04634v1)
- **Published**: 2023-02-06 18:56:20+00:00
- **Updated**: 2023-02-06 18:56:20+00:00
- **Authors**: Corina S. Pasareanu, Ravi Mangal, Divya Gopinath, Sinem Getir Yaman, Calum Imrie, Radu Calinescu, Huafeng Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are increasingly used in safety-critical autonomous systems as perception components processing high-dimensional image data. Formal analysis of these systems is particularly challenging due to the complexity of the perception DNNs, the sensors (cameras), and the environment conditions. We present a case study applying formal probabilistic analysis techniques to an experimental autonomous system that guides airplanes on taxiways using a perception DNN. We address the above challenges by replacing the camera and the network with a compact probabilistic abstraction built from the confusion matrices computed for the DNN on a representative image data set. We also show how to leverage local, DNN-specific analyses as run-time guards to increase the safety of the overall system. Our findings are applicable to other autonomous systems that use complex DNNs for perception.



### DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.03018v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.03018v1)
- **Published**: 2023-02-06 18:56:39+00:00
- **Updated**: 2023-02-06 18:56:39+00:00
- **Authors**: Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari
- **Comment**: To appear in ICLR 2023
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM$^2$), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM$^2$ demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics.



### RLSbench: Domain Adaptation Under Relaxed Label Shift
- **Arxiv ID**: http://arxiv.org/abs/2302.03020v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.03020v2)
- **Published**: 2023-02-06 18:57:14+00:00
- **Updated**: 2023-06-05 13:55:19+00:00
- **Authors**: Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, Zachary C. Lipton
- **Comment**: Accepted at ICML 2023. Paper website:
  https://sites.google.com/view/rlsbench/
- **Journal**: None
- **Summary**: Despite the emergence of principled methods for domain adaptation under label shift, their sensitivity to shifts in class conditional distributions is precariously under explored. Meanwhile, popular deep domain adaptation heuristics tend to falter when faced with label proportions shifts. While several papers modify these heuristics in attempts to handle label proportions shifts, inconsistencies in evaluation standards, datasets, and baselines make it difficult to gauge the current best practices. In this paper, we introduce RLSbench, a large-scale benchmark for relaxed label shift, consisting of $>$500 distribution shift pairs spanning vision, tabular, and language modalities, with varying label proportions. Unlike existing benchmarks, which primarily focus on shifts in class-conditional $p(x|y)$, our benchmark also focuses on label marginal shifts. First, we assess 13 popular domain adaptation methods, demonstrating more widespread failures under label proportion shifts than were previously known. Next, we develop an effective two-step meta-algorithm that is compatible with most domain adaptation heuristics: (i) pseudo-balance the data at each epoch; and (ii) adjust the final classifier with target label distribution estimate. The meta-algorithm improves existing domain adaptation heuristics under large label proportion shifts, often by 2--10\% accuracy points, while conferring minimal effect ($<$0.5\%) when label proportions do not shift. We hope that these findings and the availability of RLSbench will encourage researchers to rigorously evaluate proposed methods in relaxed label shift settings. Code is publicly available at https://github.com/acmi-lab/RLSbench.



### SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2302.03022v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.03022v3)
- **Published**: 2023-02-06 18:57:30+00:00
- **Updated**: 2023-08-30 20:36:09+00:00
- **Authors**: Joao Cartucho, Alistair Weld, Samyakh Tukra, Haozheng Xu, Hiroki Matsuzaki, Taiyo Ishikawa, Minjun Kwon, Yong Eun Jang, Kwang-Ju Kim, Gwang Lee, Bizhe Bai, Lueder Kahrs, Lars Boecking, Simeon Allmendinger, Leopold Muller, Yitong Zhang, Yueming Jin, Sophia Bano, Francisco Vasconcelos, Wolfgang Reiter, Jonas Hajek, Bruno Silva, Estevao Lima, Joao L. Vilaca, Sandro Queiros, Stamatia Giannarou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the ``SurgT: Surgical Tracking" challenge which was organised in conjunction with MICCAI 2022. There were two purposes for the creation of this challenge: (1) the establishment of the first standardised benchmark for the research community to assess soft-tissue trackers; and (2) to encourage the development of unsupervised deep learning methods, given the lack of annotated data in surgery. A dataset of 157 stereo endoscopic videos from 20 clinical cases, along with stereo camera calibration parameters, have been provided. Participants were assigned the task of developing algorithms to track the movement of soft tissues, represented by bounding boxes, in stereo endoscopic videos. At the end of the challenge, the developed methods were assessed on a previously hidden test subset. This assessment uses benchmarking metrics that were purposely developed for this challenge, to verify the efficacy of unsupervised deep learning algorithms in tracking soft-tissue. The metric used for ranking the methods was the Expected Average Overlap (EAO) score, which measures the average overlap between a tracker's and the ground truth bounding boxes. Coming first in the challenge was the deep learning submission by ICVS-2Ai with a superior EAO score of 0.617. This method employs ARFlow to estimate unsupervised dense optical flow from cropped images, using photometric and regularization losses. Second, Jmees with an EAO of 0.583, uses deep learning for surgical tool segmentation on top of a non-deep learning baseline method: CSRT. CSRT by itself scores a similar EAO of 0.563. The results from this challenge show that currently, non-deep learning methods are still competitive. The dataset and benchmarking tool created for this challenge have been made publicly available at https://surgt.grand-challenge.org/.



### V1T: large-scale mouse V1 response prediction using a Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.03023v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2302.03023v3)
- **Published**: 2023-02-06 18:58:38+00:00
- **Updated**: 2023-05-30 15:57:11+00:00
- **Authors**: Bryan M. Li, Isabel M. Cornacchia, Nathalie L. Rochefort, Arno Onken
- **Comment**: updated references and added link to code repository; add analysis on
  generalization and visualize aRFs
- **Journal**: None
- **Summary**: Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.



### AIM: Adapting Image Models for Efficient Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.03024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03024v1)
- **Published**: 2023-02-06 18:59:17+00:00
- **Updated**: 2023-02-06 18:59:17+00:00
- **Authors**: Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li
- **Comment**: Accepted to ICLR 2023. Project webpage is at
  https://adapt-image-models.github.io/
- **Journal**: None
- **Summary**: Recent vision transformer based video models mostly follow the ``image pre-training then finetuning" paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is \url{https://adapt-image-models.github.io/}.



### Zero-shot Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2302.03027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.03027v1)
- **Published**: 2023-02-06 18:59:51+00:00
- **Updated**: 2023-02-06 18:59:51+00:00
- **Authors**: Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu
- **Comment**: website: https://pix2pixzero.github.io/
- **Journal**: None
- **Summary**: Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is hard for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we propose pix2pix-zero, an image-to-image translation method that can preserve the content of the original image without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the general content structure after editing, we further propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing.



### Investigating Pulse-Echo Sound Speed Estimation in Breast Ultrasound with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.03064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.03064v1)
- **Published**: 2023-02-06 19:02:44+00:00
- **Updated**: 2023-02-06 19:02:44+00:00
- **Authors**: Walter A. Simson, Magdalini Paschali, Vasiliki Sideri-Lampretsa, Nassir Navab, Jeremy J. Dahl
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound is an adjunct tool to mammography that can quickly and safely aid physicians with diagnosing breast abnormalities. Clinical ultrasound often assumes a constant sound speed to form B-mode images for diagnosis. However, the various types of breast tissue, such as glandular, fat, and lesions, differ in sound speed. These differences can degrade the image reconstruction process. Alternatively, sound speed can be a powerful tool for identifying disease. To this end, we propose a deep-learning approach for sound speed estimation from in-phase and quadrature ultrasound signals. First, we develop a large-scale simulated ultrasound dataset that generates quasi-realistic breast tissue by modeling breast gland, skin, and lesions with varying echogenicity and sound speed. We developed a fully convolutional neural network architecture trained on a simulated dataset to produce an estimated sound speed map from inputting three complex-value in-phase and quadrature ultrasound images formed from plane-wave transmissions at separate angles. Furthermore, thermal noise augmentation is used during model optimization to enhance generalizability to real ultrasound data. We evaluate the model on simulated, phantom, and in-vivo breast ultrasound data, demonstrating its ability to accurately estimate sound speeds consistent with previously reported values in the literature. Our simulated dataset and model will be publicly available to provide a step towards accurate and generalizable sound speed estimation for pulse-echo ultrasound imaging.



### Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2302.03084v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03084v2)
- **Published**: 2023-02-06 19:40:04+00:00
- **Updated**: 2023-05-15 18:43:27+00:00
- **Authors**: Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, Tomas Pfister
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: In Composed Image Retrieval (CIR), a user combines a query image with text to describe their intended target. Existing methods rely on supervised learning of CIR models using labeled triplets consisting of the query image, text specification, and the target image. Labeling such triplets is expensive and hinders broad applicability of CIR. In this work, we propose to study an important task, Zero-Shot Composed Image Retrieval (ZS-CIR), whose goal is to build a CIR model without requiring labeled triplets for training. To this end, we propose a novel method, called Pic2Word, that requires only weakly labeled image-caption pairs and unlabeled image datasets to train. Unlike existing supervised CIR models, our model trained on weakly labeled or unlabeled datasets shows strong generalization across diverse ZS-CIR tasks, e.g., attribute editing, object composition, and domain conversion. Our approach outperforms several supervised CIR methods on the common CIR benchmark, CIRR and Fashion-IQ. Code will be made publicly available at https://github.com/google-research/composed_image_retrieval.



### From CAD models to soft point cloud labels: An automatic annotation pipeline for cheaply supervised 3D semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.03114v3
- **DOI**: 10.3390/rs15143578
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03114v3)
- **Published**: 2023-02-06 20:33:16+00:00
- **Updated**: 2023-07-25 14:11:18+00:00
- **Authors**: Galadrielle Humblot-Renaux, Simon Buus Jensen, Andreas Møgelmose
- **Comment**: updated version, published in the Remote Sensing journal
- **Journal**: Remote Sens. 15 (2023) 3578
- **Summary**: We propose a fully automatic annotation scheme that takes a raw 3D point cloud with a set of fitted CAD models as input and outputs convincing point-wise labels that can be used as cheap training data for point cloud segmentation. Compared with manual annotations, we show that our automatic labels are accurate while drastically reducing the annotation time and eliminating the need for manual intervention or dataset-specific parameters. Our labeling pipeline outputs semantic classes and soft point-wise object scores, which can either be binarized into standard one-hot-encoded labels, thresholded into weak labels with ambiguous points left unlabeled, or used directly as soft labels during training. We evaluate the label quality and segmentation performance of PointNet++ on a dataset of real industrial point clouds and Scan2CAD, a public dataset of indoor scenes. Our results indicate that reducing supervision in areas that are more difficult to label automatically is beneficial compared with the conventional approach of naively assigning a hard "best guess" label to every point.



### Studying Therapy Effects and Disease Outcomes in Silico using Artificial Counterfactual Tissue Samples
- **Arxiv ID**: http://arxiv.org/abs/2302.03120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03120v1)
- **Published**: 2023-02-06 20:59:02+00:00
- **Updated**: 2023-02-06 20:59:02+00:00
- **Authors**: Martin Paulikat, Christian M. Schürch, Christian F. Baumgartner
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the interactions of different cell types inside the immune tumor microenvironment (iTME) is crucial for the development of immunotherapy treatments as well as for predicting their outcomes. Highly multiplexed tissue imaging (HMTI) technologies offer a tool which can capture cell properties of tissue samples by measuring expression of various proteins and storing them in separate image channels. HMTI technologies can be used to gain insights into the iTME and in particular how the iTME differs for different patient outcome groups of interest (e.g., treatment responders vs. non-responders). Understanding the systematic differences in the iTME of different patient outcome groups is crucial for developing better treatments and personalising existing treatments. However, such analyses are inherently limited by the fact that any two tissue samples vary due to a large number of factors unrelated to the outcome. Here, we present CF-HistoGAN, a machine learning framework that employs generative adversarial networks (GANs) to create artificial counterfactual tissue samples that resemble the original tissue samples as closely as possible but capture the characteristics of a different patient outcome group. Specifically, we learn to "translate" HMTI samples from one patient group to create artificial paired samples. We show that this approach allows to directly study the effects of different patient outcomes on the iTMEs of individual tissue samples. We demonstrate that CF-HistoGAN can be employed as an explorative tool for understanding iTME effects on the pixel level. Moreover, we show that our method can be used to identify statistically significant differences in the expression of different proteins between patient groups with greater sensitivity compared to conventional approaches.



### Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation
- **Arxiv ID**: http://arxiv.org/abs/2302.03128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2302.03128v1)
- **Published**: 2023-02-06 21:30:08+00:00
- **Updated**: 2023-02-06 21:30:08+00:00
- **Authors**: Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Cooperative perception (CP) is attracting increasing attention and is regarded as the core foundation to support cooperative driving automation, a potential key solution to addressing the safety, mobility, and sustainability issues of contemporary transportation systems. However, current research on CP is still at the beginning stages where a systematic problem formulation of CP is still missing, acting as the essential guideline of the system design of a CP system under real-world situations. In this paper, we formulate a universal CP system into an optimization problem and a mobile-edge-cloud framework called Cooperverse. This system addresses CP in a mixed connectivity and automation environment. A Dynamic Feature Sharing (DFS) methodology is introduced to support this CP system under certain constraints and a Random Priority Filtering (RPF) method is proposed to conduct DFS with high performance. Experiments have been conducted based on a high-fidelity CP platform, and the results show that the Cooperverse framework is effective for dynamic node engagement and the proposed DFS methodology can improve system CP performance by 14.5% and the RPF method can reduce the communication cost for mobile nodes by 90% with only 1.7% drop for average precision.



### Spatial Functa: Scaling Functa to ImageNet Classification and Generation
- **Arxiv ID**: http://arxiv.org/abs/2302.03130v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.03130v2)
- **Published**: 2023-02-06 21:35:44+00:00
- **Updated**: 2023-02-09 12:43:24+00:00
- **Authors**: Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Richard Schwarz, Hyunjik Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Neural fields, also known as implicit neural representations, have emerged as a powerful means to represent complex signals of various modalities. Based on this Dupont et al. (2022) introduce a framework that views neural fields as data, termed *functa*, and proposes to do deep learning directly on this dataset of neural fields. In this work, we show that the proposed framework faces limitations when scaling up to even moderately complex datasets such as CIFAR-10. We then propose *spatial functa*, which overcome these limitations by using spatially arranged latent representations of neural fields, thereby allowing us to scale up the approach to ImageNet-1k at 256x256 resolution. We demonstrate competitive performance to Vision Transformers (Steiner et al., 2022) on classification and Latent Diffusion (Rombach et al., 2022) on image generation respectively.



### Novel Building Detection and Location Intelligence Collection in Aerial Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2302.03156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.03156v1)
- **Published**: 2023-02-06 23:30:51+00:00
- **Updated**: 2023-02-06 23:30:51+00:00
- **Authors**: Sandeep Singh, Christian Wiles, Ahmed Bilal
- **Comment**: 9 pages(5 main pages, 4 auxiliary pages)
- **Journal**: None
- **Summary**: Building structures detection and information about these buildings in aerial images is an important solution for city planning and management, land use analysis. It can be the center piece to answer important questions such as planning evacuation routes in case of an earthquake, flood management, etc. These applications rely on being able to accurately retrieve up-to-date information. Being able to accurately detect buildings in a bounding box centered on a specific latitude-longitude value can help greatly. The key challenge is to be able to detect buildings which can be commercial, industrial, hut settlements, or skyscrapers. Once we are able to detect such buildings, our goal will be to cluster and categorize similar types of buildings together.



