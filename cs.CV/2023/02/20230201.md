# Arxiv Papers in cs.CV on 2023-02-01
### Continual Segment: Towards a Single, Unified and Accessible Continual Segmentation Model of 143 Whole-body Organs in CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2302.00162v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00162v3)
- **Published**: 2023-02-01 00:49:21+00:00
- **Updated**: 2023-03-13 18:52:53+00:00
- **Authors**: Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Ke Yan, Le Lu, Minfeng Xu, Jingren Zhou, Qifeng Wang, Jia Ge, Mingchen Gao, Xianghua Ye, Dakai Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning empowers the mainstream medical image segmentation methods. Nevertheless current deep segmentation approaches are not capable of efficiently and effectively adapting and updating the trained models when new incremental segmentation classes (along with new training datasets or not) are required to be added. In real clinical environment, it can be preferred that segmentation models could be dynamically extended to segment new organs/tumors without the (re-)access to previous training datasets due to obstacles of patient privacy and data storage. This process can be viewed as a continual semantic segmentation (CSS) problem, being understudied for multi-organ segmentation. In this work, we propose a new architectural CSS learning framework to learn a single deep segmentation model for segmenting a total of 143 whole-body organs. Using the encoder/decoder network structure, we demonstrate that a continually-trained then frozen encoder coupled with incrementally-added decoders can extract and preserve sufficiently representative image features for new classes to be subsequently and validly segmented. To maintain a single network model complexity, we trim each decoder progressively using neural architecture search and teacher-student based knowledge distillation. To incorporate with both healthy and pathological organs appearing in different datasets, a novel anomaly-aware and confidence learning module is proposed to merge the overlapped organ predictions, originated from different decoders. Trained and validated on 3D CT scans of 2500+ patients from four datasets, our single network can segment total 143 whole-body organs with very high accuracy, closely reaching the upper bound performance level by training four separate segmentation models (i.e., one model per dataset/task).



### Detection of Tomato Ripening Stages using Yolov3-tiny
- **Arxiv ID**: http://arxiv.org/abs/2302.00164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00164v1)
- **Published**: 2023-02-01 00:57:58+00:00
- **Updated**: 2023-02-01 00:57:58+00:00
- **Authors**: Gerardo Antonio Alvarez Hernández, Juan Carlos Olguin, Juan Irving Vasquez, Abril Valeria Uriarte, Maria Claudia Villicaña Torres
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most important agricultural products in Mexico is the tomato (Solanum lycopersicum), which occupies the 4th place national most produced product . Therefore, it is necessary to improve its production, building automatic detection system that detect, classify an keep tacks of the fruits is one way to archieve it. So, in this paper, we address the design of a computer vision system to detect tomatoes at different ripening stages. To solve the problem, we use a neural network-based model for tomato classification and detection. Specifically, we use the YOLOv3-tiny model because it is one of the lightest current deep neural networks. To train it, we perform two grid searches testing several combinations of hyperparameters. Our experiments showed an f1-score of 90.0% in the localization and classification of ripening stages in a custom dataset.



### Program Generation from Diverse Video Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/2302.00178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00178v1)
- **Published**: 2023-02-01 01:51:45+00:00
- **Updated**: 2023-02-01 01:51:45+00:00
- **Authors**: Anthony Manchin, Jamie Sherrah, Qi Wu, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to use inductive reasoning to extract general rules from multiple observations is a vital indicator of intelligence. As humans, we use this ability to not only interpret the world around us, but also to predict the outcomes of the various interactions we experience. Generalising over multiple observations is a task that has historically presented difficulties for machines to grasp, especially when requiring computer vision. In this paper, we propose a model that can extract general rules from video demonstrations by simultaneously performing summarisation and translation. Our approach differs from prior works by framing the problem as a multi-sequence-to-sequence task, wherein summarisation is learnt by the model. This allows our model to utilise edge cases that would otherwise be suppressed or discarded by traditional summarisation techniques. Additionally, we show that our approach can handle noisy specifications without the need for additional filtering methods. We evaluate our model by synthesising programs from video demonstrations in the Vizdoom environment achieving state-of-the-art results with a relative increase of 11.75% program accuracy on prior works



### Stable Attribute Group Editing for Reliable Few-shot Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2302.00179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00179v1)
- **Published**: 2023-02-01 01:51:47+00:00
- **Updated**: 2023-02-01 01:51:47+00:00
- **Authors**: Guanqi Ding, Xinzhe Han, Shuhui Wang, Xin Jin, Dandan Tu, Qingming Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image generation aims to generate data of an unseen category based on only a few samples. Apart from basic content generation, a bunch of downstream applications hopefully benefit from this task, such as low-data detection and few-shot classification. To achieve this goal, the generated images should guarantee category retention for classification beyond the visual quality and diversity. In our preliminary work, we present an ``editing-based'' framework Attribute Group Editing (AGE) for reliable few-shot image generation, which largely improves the generation performance. Nevertheless, AGE's performance on downstream classification is not as satisfactory as expected. This paper investigates the class inconsistency problem and proposes Stable Attribute Group Editing (SAGE) for more stable class-relevant image generation. SAGE takes use of all given few-shot images and estimates a class center embedding based on the category-relevant attribute dictionary. Meanwhile, according to the projection weights on the category-relevant attribute dictionary, we can select category-irrelevant attributes from the similar seen categories. Consequently, SAGE injects the whole distribution of the novel class into StyleGAN's latent space, thus largely remains the category retention and stability of the generated images. Going one step further, we find that class inconsistency is a common problem in GAN-generated images for downstream classification. Even though the generated images look photo-realistic and requires no category-relevant editing, they are usually of limited help for downstream classification. We systematically discuss this issue from both the generative model and classification model perspectives, and propose to boost the downstream classification performance of SAGE by enhancing the pixel and frequency components.



### Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2302.00190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.00190v1)
- **Published**: 2023-02-01 02:47:53+00:00
- **Updated**: 2023-02-01 02:47:53+00:00
- **Authors**: Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li, Chi-Wing Fu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2209.08725
- **Journal**: None
- **Summary**: This paper presents a new approach for 3D shape generation, inversion, and manipulation, through a direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a diffusion-based generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details. Further, we may jointly train an encoder network to learn a latent space for inverting shapes, allowing us to enable a rich variety of whole-shape and region-aware shape manipulations. Both quantitative and qualitative experimental results manifest the compelling shape generation, inversion, and manipulation capabilities of our approach over the state-of-the-art methods.



### Density peak clustering using tensor network
- **Arxiv ID**: http://arxiv.org/abs/2302.00192v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2302.00192v1)
- **Published**: 2023-02-01 02:53:34+00:00
- **Updated**: 2023-02-01 02:53:34+00:00
- **Authors**: Xiao Shi, Yun Shang
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor networks, which have been traditionally used to simulate many-body physics, have recently gained significant attention in the field of machine learning due to their powerful representation capabilities. In this work, we propose a density-based clustering algorithm inspired by tensor networks. We encode classical data into tensor network states on an extended Hilbert space and train the tensor network states to capture the features of the clusters. Here, we define density and related concepts in terms of fidelity, rather than using a classical distance measure. We evaluate the performance of our algorithm on six synthetic data sets, four real world data sets, and three commonly used computer vision data sets. The results demonstrate that our method provides state-of-the-art performance on several synthetic data sets and real world data sets, even when the number of clusters is unknown. Additionally, our algorithm performs competitively with state-of-the-art algorithms on the MNIST, USPS, and Fashion-MNIST image data sets. These findings reveal the great potential of tensor networks for machine learning applications.



### QCRS: Improve Randomized Smoothing using Quasi-Concave Optimization
- **Arxiv ID**: http://arxiv.org/abs/2302.00209v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2302.00209v1)
- **Published**: 2023-02-01 03:25:43+00:00
- **Updated**: 2023-02-01 03:25:43+00:00
- **Authors**: Bo-Han Kung, Shang-Tse Chen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Randomized smoothing is currently the state-of-the-art method that provides certified robustness for deep neural networks. However, it often cannot achieve an adequate certified region on real-world datasets. One way to obtain a larger certified region is to use an input-specific algorithm instead of using a fixed Gaussian filter for all data points. Several methods based on this idea have been proposed, but they either suffer from high computational costs or gain marginal improvement in certified radius. In this work, we show that by exploiting the quasiconvex problem structure, we can find the optimal certified radii for most data points with slight computational overhead. This observation leads to an efficient and effective input-specific randomized smoothing algorithm. We conduct extensive experiments and empirical analysis on Cifar10 and ImageNet. The results show that the proposed method significantly enhances the certified radii with low computational overhead.



### Efficient Scopeformer: Towards Scalable and Rich Feature Extraction for Intracranial Hemorrhage Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.00220v1
- **DOI**: 10.1109/ACCESS.2023.3301160
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2302.00220v1)
- **Published**: 2023-02-01 03:51:27+00:00
- **Updated**: 2023-02-01 03:51:27+00:00
- **Authors**: Yassine Barhoumi, Nidhal C. Bouaynaya, Ghulam Rasool
- **Comment**: None
- **Journal**: None
- **Summary**: The quality and richness of feature maps extracted by convolution neural networks (CNNs) and vision Transformers (ViTs) directly relate to the robust model performance. In medical computer vision, these information-rich features are crucial for detecting rare cases within large datasets. This work presents the "Scopeformer," a novel multi-CNN-ViT model for intracranial hemorrhage classification in computed tomography (CT) images. The Scopeformer architecture is scalable and modular, which allows utilizing various CNN architectures as the backbone with diversified output features and pre-training strategies. We propose effective feature projection methods to reduce redundancies among CNN-generated features and to control the input size of ViTs. Extensive experiments with various Scopeformer models show that the model performance is proportional to the number of convolutional blocks employed in the feature extractor. Using multiple strategies, including diversifying the pre-training paradigms for CNNs, different pre-training datasets, and style transfer techniques, we demonstrate an overall improvement in the model performance at various computational budgets. Later, we propose smaller compute-efficient Scopeformer versions with three different types of input and output ViT configurations. Efficient Scopeformers use four different pre-trained CNN architectures as feature extractors to increase feature richness. Our best Efficient Scopeformer model achieved an accuracy of 96.94\% and a weighted logarithmic loss of 0.083 with an eight times reduction in the number of trainable parameters compared to the base Scopeformer. Another version of the Efficient Scopeformer model further reduced the parameter space by almost 17 times with negligible performance reduction. Hybrid CNNs and ViTs might provide the desired feature richness for developing accurate medical computer vision models



### Human Fall Detection- Multimodality Approach
- **Arxiv ID**: http://arxiv.org/abs/2302.00224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00224v1)
- **Published**: 2023-02-01 04:05:14+00:00
- **Updated**: 2023-02-01 04:05:14+00:00
- **Authors**: Xi Wang, Ramya Penta, Bhavya Sehgal, Dale Chen-Song
- **Comment**: None
- **Journal**: None
- **Summary**: Falls have become more frequent in recent years, which has been harmful for senior citizens.Therefore detecting falls have become important and several data sets and machine learning model have been introduced related to fall detection. In this project report, a human fall detection method is proposed using a multi modality approach. We used the UP-FALL detection data set which is collected by dozens of volunteers using different sensors and two cameras. We use wrist sensor with acclerometer data keeping labels to binary classification, namely fall and no fall from the data set.We used fusion of camera and sensor data to increase performance. The experimental results shows that using only wrist data as compared to multi sensor for binary classification did not impact the model prediction performance for fall detection.



### The Past, Current, and Future of Neonatal Intensive Care Units with Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2302.00225v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00225v1)
- **Published**: 2023-02-01 04:07:11+00:00
- **Updated**: 2023-02-01 04:07:11+00:00
- **Authors**: Elif Keles, Ulas Bagci
- **Comment**: 58 pages, review article
- **Journal**: None
- **Summary**: Artificial intelligence (AI), specifically a branch of AI called deep learning (DL), has proven revolutionary developments in almost all fields, from computer vision to health sciences, and its effects in medicine have changed clinical applications significantly. Although some sub-fields of medicine such as pediatrics have been relatively slow in receiving critical benefits of AI, related research in pediatrics started to be accumulated to a significant level too. Hence, in this paper, we review recently developed machine learning and deep learning based systems for neonatology applications. We systematically evaluate the role of AI in neonatology applications, define the methodologies, including algorithmic developments, and describe the remaining challenges in neonatal diseases. To date, survival analysis, neuroimaging, EEG, pattern analysis of vital parameters, and retinopathy of prematurity diagnosis with AI have been the main focus in neonatology. We have categorically summarized 96 research articles, from 1996 to 2022, and discussed their pros and cons, respectively. We also discuss possible directions for new AI models and the future of neonatology with the rising power of AI, suggesting roadmaps for integration of AI into neonatal intensive care units.



### SPIDE: A Purely Spike-based Method for Training Feedback Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.00232v1
- **DOI**: 10.1016/j.neunet.2023.01.026
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00232v1)
- **Published**: 2023-02-01 04:22:59+00:00
- **Updated**: 2023-02-01 04:22:59+00:00
- **Authors**: Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Yisen Wang, Zhouchen Lin
- **Comment**: Accepted by Neural Networks
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) with event-based computation are promising brain-inspired models for energy-efficient applications on neuromorphic hardware. However, most supervised SNN training methods, such as conversion from artificial neural networks or direct training with surrogate gradients, require complex computation rather than spike-based operations of spiking neurons during training. In this paper, we study spike-based implicit differentiation on the equilibrium state (SPIDE) that extends the recently proposed training method, implicit differentiation on the equilibrium state (IDE), for supervised learning with purely spike-based computation, which demonstrates the potential for energy-efficient training of SNNs. Specifically, we introduce ternary spiking neuron couples and prove that implicit differentiation can be solved by spikes based on this design, so the whole training procedure, including both forward and backward passes, is made as event-driven spike computation, and weights are updated locally with two-stage average firing rates. Then we propose to modify the reset membrane potential to reduce the approximation error of spikes. With these key components, we can train SNNs with flexible structures in a small number of time steps and with firing sparsity during training, and the theoretical estimation of energy costs demonstrates the potential for high efficiency. Meanwhile, experiments show that even with these constraints, our trained models can still achieve competitive results on MNIST, CIFAR-10, CIFAR-100, and CIFAR10-DVS. Our code is available at https://github.com/pkuxmq/SPIDE-FSNN.



### Object Dimension Extraction for Environment Mapping with Low Cost Cameras Fused with Laser Ranging
- **Arxiv ID**: http://arxiv.org/abs/2302.01387v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.01387v1)
- **Published**: 2023-02-01 04:35:16+00:00
- **Updated**: 2023-02-01 04:35:16+00:00
- **Authors**: E. M. S. P. Ekanayake, T. H. M. N. C. Thelasingha, U. V. B. L. Udugama, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, B. G. L. T. Samaranayake, J. V. Wijayakulasooriya
- **Comment**: Conference: 24th Annual Technical Conference of IET Sri Lanka Network
  2017 At: Colombo, Sri Lanka
- **Journal**: None
- **Summary**: It is essential to have a method to map an unknown terrain for various applications. For places where human access is not possible, a method should be proposed to identify the environment. Exploration, disaster relief, transportation and many other purposes would be convenient if a map of the environment is available. Replicating the human vision system using stereo cameras would be an optimum solution. In this work, we have used laser ranging based technique fused with stereo cameras to extract dimension of objects for mapping. The distortions were calibrated using mathematical model of the camera. By means of Semi Global Block Matching [1] disparity map was generated and reduces the noise using novel noise reduction method of disparity map by dilation. The Data from the Laser Range Finder (LRF) and noise reduced vision data has been used to identify the object parameters.



### Cloud-Based Deep Learning: End-To-End Full-Stack Handwritten Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2304.13506v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13506v1)
- **Published**: 2023-02-01 05:40:28+00:00
- **Updated**: 2023-02-01 05:40:28+00:00
- **Authors**: Ruida Zeng, Aadarsh Jha, Ashwin Kumar, Terry Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Herein, we present Stratus, an end-to-end full-stack deep learning application deployed on the cloud. The rise of productionized deep learning necessitates infrastructure in the cloud that can provide such service (IaaS). In this paper, we explore the use of modern cloud infrastructure and micro-services to deliver accurate and high-speed predictions to an end-user, using a Deep Neural Network (DNN) to predict handwritten digit input, interfaced via a full-stack application. We survey tooling from Spark ML, Apache Kafka, Chameleon Cloud, Ansible, Vagrant, Python Flask, Docker, and Kubernetes in order to realize this machine learning pipeline. Through our cloud-based approach, we are able to demonstrate benchmark performance on the MNIST dataset with a deep learning model.



### CertViT: Certified Robustness of Pre-Trained Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.10287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10287v1)
- **Published**: 2023-02-01 06:09:19+00:00
- **Updated**: 2023-02-01 06:09:19+00:00
- **Authors**: Kavya Gupta, Sagar Verma
- **Comment**: Preprint work. 13 pages, 3 figures,
  https://github.com/sagarverma/transformer-lipschitz
- **Journal**: None
- **Summary**: Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz



### Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.00268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00268v1)
- **Published**: 2023-02-01 06:20:54+00:00
- **Updated**: 2023-02-01 06:20:54+00:00
- **Authors**: Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao, Qianru Sun
- **Comment**: accepted by ICLR 2023
- **Journal**: None
- **Summary**: Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary predictions trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatio-temporal motion patterns of the subject-object compositions. Without bells and whistles, our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompts. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.



### Learning Generalized Zero-Shot Learners for Open-Domain Image Geolocalization
- **Arxiv ID**: http://arxiv.org/abs/2302.00275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00275v1)
- **Published**: 2023-02-01 06:44:07+00:00
- **Updated**: 2023-02-01 06:44:07+00:00
- **Authors**: Lukas Haas, Silas Alberti, Michal Skreta
- **Comment**: None
- **Journal**: None
- **Summary**: Image geolocalization is the challenging task of predicting the geographic coordinates of origin for a given photo. It is an unsolved problem relying on the ability to combine visual clues with general knowledge about the world to make accurate predictions across geographies. We present $\href{https://huggingface.co/geolocal/StreetCLIP}{\text{StreetCLIP}}$, a robust, publicly available foundation model not only achieving state-of-the-art performance on multiple open-domain image geolocalization benchmarks but also doing so in a zero-shot setting, outperforming supervised models trained on more than 4 million images. Our method introduces a meta-learning approach for generalized zero-shot learning by pretraining CLIP from synthetic captions, grounding CLIP in a domain of choice. We show that our method effectively transfers CLIP's generalized zero-shot capabilities to the domain of image geolocalization, improving in-domain generalized zero-shot performance without finetuning StreetCLIP on a fixed set of classes.



### MS-DETR: Multispectral Pedestrian Detection Transformer with Loosely Coupled Fusion and Modality-Balanced Optimization
- **Arxiv ID**: http://arxiv.org/abs/2302.00290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00290v2)
- **Published**: 2023-02-01 07:45:10+00:00
- **Updated**: 2023-08-08 11:59:25+00:00
- **Authors**: Yinghui Xing, Song Wang, Shizhou Zhang, Guoqiang Liang, Xiuwei Zhang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral pedestrian detection is an important task for many around-the-clock applications, since the visible and thermal modalities can provide complementary information especially under low light conditions. Most of the available multispectral pedestrian detectors are based on non-end-to-end detectors, while in this paper, we propose MultiSpectral pedestrian DEtection TRansformer (MS-DETR), an end-to-end multispectral pedestrian detector, which extends DETR into the field of multi-modal detection. MS-DETR consists of two modality-specific backbones and Transformer encoders, followed by a multi-modal Transformer decoder, and the visible and thermal features are fused in the multi-modal Transformer decoder. To well resist the misalignment between multi-modal images, we design a loosely coupled fusion strategy by sparsely sampling some keypoints from multi-modal features independently and fusing them with adaptively learned attention weights. Moreover, based on the insight that not only different modalities, but also different pedestrian instances tend to have different confidence scores to final detection, we further propose an instance-aware modality-balanced optimization strategy, which preserves visible and thermal decoder branches and aligns their predicted slots through an instance-wise dynamic loss. Our end-to-end MS-DETR shows superior performance on the challenging KAIST, CVC-14 and LLVIP benchmark datasets. The source code is available at https://github.com/YinghuiXing/MS-DETR .



### Development of Real-time Rendering Technology for High-Precision Models in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2302.00291v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00291v2)
- **Published**: 2023-02-01 07:45:54+00:00
- **Updated**: 2023-03-20 15:28:36+00:00
- **Authors**: Zhang Wencheng, Wang Chengyi
- **Comment**: 3 pages, 6 figures
- **Journal**: None
- **Summary**: Our autonomous driving simulation lab produces a high-precision 3D model simulating the parking lot. However, the current model still has poor rendering quality in some aspects. In this work, we develop a system to improve the rendering of the model and evaluate the quality of the rendered model.



### Data level and decision level fusion of satellite multi-sensor AOD retrievals for improving PM2.5 estimations, a study on Tehran
- **Arxiv ID**: http://arxiv.org/abs/2302.10278v1
- **DOI**: 10.1007/s12145-022-00912-6
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2302.10278v1)
- **Published**: 2023-02-01 08:07:00+00:00
- **Updated**: 2023-02-01 08:07:00+00:00
- **Authors**: Ali Mirzaei, Hossein Bagheri, Mehran Sattari
- **Comment**: Earth Sci Inform (2023)
- **Journal**: None
- **Summary**: One of the techniques for estimating the surface particle concentration with a diameter of fewer than 2.5 micrometers (PM2.5) is using aerosol optical depth (AOD) products. Different AOD products are retrieved from various satellite sensors, like MODIS and VIIRS, by various algorithms, such as Deep Blue and Dark Target. Therefore, they do not have the same accuracy and spatial resolution. Additionally, the weakness of algorithms in AOD retrieval reduces the spatial coverage of products, particularly in cloudy or snowy areas. Consequently, for the first time, the present study investigated the possibility of fusing AOD products from observations of MODIS and VIIRS sensors retrieved by Deep Blue and Dark Target algorithms to estimate PM2.5 more accurately. For this purpose, AOD products were fused by machine learning algorithms using different fusion strategies at two levels: the data level and the decision level. First, the performance of various machine learning algorithms for estimating PM2.5 using AOD data was evaluated. After that, the XGBoost algorithm was selected as the base model for the proposed fusion strategies. Then, AOD products were fused. The fusion results showed that the estimated PM2.5 accuracy at the data level in all three metrics, RMSE, MAE, and R2, was improved (R2=0.64). Despite the simplicity and lower computational cost of the data level fusion method, the spatial coverage did not improve considerably due to eliminating poor quality data through the fusion process. Afterward, the fusion of products at the decision level was followed in eleven scenarios. In this way, the best result was obtained by fusing Deep Blue products of MODIS and VIIRS sensors (R2=0.81)



### iPAL: A Machine Learning Based Smart Healthcare Framework For Automatic Diagnosis Of Attention Deficit/Hyperactivity Disorder (ADHD)
- **Arxiv ID**: http://arxiv.org/abs/2302.00332v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00332v1)
- **Published**: 2023-02-01 09:29:20+00:00
- **Updated**: 2023-02-01 09:29:20+00:00
- **Authors**: Abhishek Sharma, Arpit Jain, Shubhangi Sharma, Ashutosh Gupta, Prateek Jain, Saraju P. Mohanty
- **Comment**: None
- **Journal**: None
- **Summary**: ADHD is a prevalent disorder among the younger population. Standard evaluation techniques currently use evaluation forms, interviews with the patient, and more. However, its symptoms are similar to those of many other disorders like depression, conduct disorder, and oppositional defiant disorder, and these current diagnosis techniques are not very effective. Thus, a sophisticated computing model holds the potential to provide a promising diagnosis solution to this problem. This work attempts to explore methods to diagnose ADHD using combinations of multiple established machine learning techniques like neural networks and SVM models on the ADHD200 dataset and explore the field of neuroscience. In this work, multiclass classification is performed on phenotypic data using an SVM model. The better results have been analyzed on the phenotypic data compared to other supervised learning techniques like Logistic regression, KNN, AdaBoost, etc. In addition, neural networks have been implemented on functional connectivity from the MRI data of a sample of 40 subjects provided to achieve high accuracy without prior knowledge of neuroscience. It is combined with the phenotypic classifier using the ensemble technique to get a binary classifier. It is further trained and tested on 400 out of 824 subjects from the ADHD200 data set and achieved an accuracy of 92.5% for binary classification The training and testing accuracy has been achieved upto 99% using ensemble classifier.



### BackdoorBox: A Python Toolbox for Backdoor Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.01762v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.01762v1)
- **Published**: 2023-02-01 09:45:42+00:00
- **Updated**: 2023-02-01 09:45:42+00:00
- **Authors**: Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia
- **Comment**: BackdoorBox V0.1. The first two authors contributed equally to this
  toolbox. 13 pages
- **Journal**: None
- **Summary**: Third-party resources ($e.g.$, samples, backbones, and pre-trained models) are usually involved in the training of deep neural networks (DNNs), which brings backdoor attacks as a new training-phase threat. In general, backdoor attackers intend to implant hidden backdoor in DNNs, so that the attacked DNNs behave normally on benign samples whereas their predictions will be maliciously changed to a pre-defined target label if hidden backdoors are activated by attacker-specified trigger patterns. To facilitate the research and development of more secure training schemes and defenses, we design an open-sourced Python toolbox that implements representative and advanced backdoor attacks and defenses under a unified and flexible framework. Our toolbox has four important and promising characteristics, including consistency, simplicity, flexibility, and co-development. It allows researchers and developers to easily implement and compare different methods on benchmark or their local datasets. This Python toolbox, namely \texttt{BackdoorBox}, is available at \url{https://github.com/THUYimingLi/BackdoorBox}.



### Towards Label-Efficient Incremental Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.00353v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00353v3)
- **Published**: 2023-02-01 10:24:55+00:00
- **Updated**: 2023-02-11 12:09:01+00:00
- **Authors**: Mert Kilickaya, Joost van de Weijer, Yuki M. Asano
- **Comment**: None
- **Journal**: None
- **Summary**: The current dominant paradigm when building a machine learning model is to iterate over a dataset over and over until convergence. Such an approach is non-incremental, as it assumes access to all images of all categories at once. However, for many applications, non-incremental learning is unrealistic. To that end, researchers study incremental learning, where a learner is required to adapt to an incoming stream of data with a varying distribution while preventing forgetting of past knowledge. Significant progress has been made, however, the vast majority of works focus on the fully supervised setting, making these algorithms label-hungry thus limiting their real-life deployment. To that end, in this paper, we make the first attempt to survey recently growing interest in label-efficient incremental learning. We identify three subdivisions, namely semi-, few-shot- and self-supervised learning to reduce labeling efforts. Finally, we identify novel directions that can further enhance label-efficiency and improve incremental learning scalability. Project website: https://github.com/kilickaya/label-efficient-il.



### A Flexible Framework for Virtual Omnidirectional Vision to Improve Operator Situation Awareness
- **Arxiv ID**: http://arxiv.org/abs/2302.00362v1
- **DOI**: 10.1109/ECMR50962.2021.9568840
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.00362v1)
- **Published**: 2023-02-01 10:40:05+00:00
- **Updated**: 2023-02-01 10:40:05+00:00
- **Authors**: Martin Oehler, Oskar von Stryk
- **Comment**: Accepted to European Conference on Mobile Robots (ECMR) 2021. Video
  link: https://youtu.be/7pocpdsMxOM Project page:
  https://tu-darmstadt-ros-pkg.github.io/omnidirectional_vision
- **Journal**: European Conference on Mobile Robots (ECMR) 2021
- **Summary**: During teleoperation of a mobile robot, providing good operator situation awareness is a major concern as a single mistake can lead to mission failure. Camera streams are widely used for teleoperation but offer limited field-of-view. In this paper, we present a flexible framework for virtual projections to increase situation awareness based on a novel method to fuse multiple cameras mounted anywhere on the robot. Moreover, we propose a complementary approach to improve scene understanding by fusing camera images and geometric 3D Lidar data to obtain a colorized point cloud. The implementation on a compact omnidirectional camera reduces system complexity considerably and solves multiple use-cases on a much smaller footprint compared to traditional approaches such as actuated pan-tilt units. Finally, we demonstrate the generality of the approach by application to the multi-camera system of the Boston Dynamics Spot. The software implementation is available as open-source ROS packages on the project page https://tu-darmstadt-ros-pkg.github.io/omnidirectional_vision.



### Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.00368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00368v1)
- **Published**: 2023-02-01 10:55:27+00:00
- **Updated**: 2023-02-01 10:55:27+00:00
- **Authors**: Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi
- **Comment**: 8 pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of knowledge or domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE render it practical to be used with any off-the-shelf trained model to improve its predictions further.



### Netizens, Academicians, and Information Professionals' Opinions About AI With Special Reference To ChatGPT
- **Arxiv ID**: http://arxiv.org/abs/2302.07136v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.07136v1)
- **Published**: 2023-02-01 10:59:04+00:00
- **Updated**: 2023-02-01 10:59:04+00:00
- **Authors**: Subaveerapandiyan A, Vinoth A, Neelam Tiwary
- **Comment**: None
- **Journal**: None
- **Summary**: This study aims to understand the perceptions and opinions of academicians towards ChatGPT-3 by collecting and analyzing social media comments, and a survey was conducted with library and information science professionals. The research uses a content analysis method and finds that while ChatGPT-3 can be a valuable tool for research and writing, it is not 100% accurate and should be cross-checked. The study also finds that while some academicians may not accept ChatGPT-3, most are starting to accept it. The study is beneficial for academicians, content developers, and librarians.



### Fusion of Radio and Camera Sensor Data for Accurate Indoor Positioning
- **Arxiv ID**: http://arxiv.org/abs/2302.02952v1
- **DOI**: 10.1109/MASS.2014.52
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2302.02952v1)
- **Published**: 2023-02-01 11:37:41+00:00
- **Updated**: 2023-02-01 11:37:41+00:00
- **Authors**: Savvas Papaioannou, Hongkai Wen, Andrew Markham, Niki Trigoni
- **Comment**: None
- **Journal**: 2014 IEEE 11th International Conference on Mobile Ad Hoc and
  Sensor Systems (MASS)
- **Summary**: Indoor positioning systems have received a lot of attention recently due to their importance for many location-based services, e.g. indoor navigation and smart buildings. Lightweight solutions based on WiFi and inertial sensing have gained popularity, but are not fit for demanding applications, such as expert museum guides and industrial settings, which typically require sub-meter location information. In this paper, we propose a novel positioning system, RAVEL (Radio And Vision Enhanced Localization), which fuses anonymous visual detections captured by widely available camera infrastructure, with radio readings (e.g. WiFi radio data). Although visual trackers can provide excellent positioning accuracy, they are plagued by issues such as occlusions and people entering/exiting the scene, preventing their use as a robust tracking solution. By incorporating radio measurements, visually ambiguous or missing data can be resolved through multi-hypothesis tracking. We evaluate our system in a complex museum environment with dim lighting and multiple people moving around in a space cluttered with exhibit stands. Our experiments show that although the WiFi measurements are not by themselves sufficiently accurate, when they are fused with camera data, they become a catalyst for pulling together ambiguous, fragmented, and anonymous visual tracklets into accurate and continuous paths, yielding typical errors below 1 meter.



### Alphazzle: Jigsaw Puzzle Solver with Deep Monte-Carlo Tree Search
- **Arxiv ID**: http://arxiv.org/abs/2302.00384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00384v1)
- **Published**: 2023-02-01 11:41:21+00:00
- **Updated**: 2023-02-01 11:41:21+00:00
- **Authors**: Marie-Morgane Paumard, Hedi Tabia, David Picard
- **Comment**: None
- **Journal**: None
- **Summary**: Solving jigsaw puzzles requires to grasp the visual features of a sequence of patches and to explore efficiently a solution space that grows exponentially with the sequence length. Therefore, visual deep reinforcement learning (DRL) should answer this problem more efficiently than optimization solvers coupled with neural networks. Based on this assumption, we introduce Alphazzle, a reassembly algorithm based on single-player Monte Carlo Tree Search (MCTS). A major difference with DRL algorithms lies in the unavailability of game reward for MCTS, and we show how to estimate it from the visual input with neural networks. This constraint is induced by the puzzle-solving task and dramatically adds to the task complexity (and interest!). We perform an in-deep ablation study that shows the importance of MCTS and the neural networks working together. We achieve excellent results and get exciting insights into the combination of DRL and visual feature learning.



### EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design
- **Arxiv ID**: http://arxiv.org/abs/2302.00386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00386v1)
- **Published**: 2023-02-01 11:46:04+00:00
- **Updated**: 2023-02-01 11:46:04+00:00
- **Authors**: Kaiheng Weng, Xiangxiang Chu, Xiaoming Xu, Junshi Huang, Xiaoming Wei
- **Comment**: None
- **Journal**: None
- **Summary**: We present a hardware-efficient architecture of convolutional neural network, which has a repvgg-like architecture. Flops or parameters are traditional metrics to evaluate the efficiency of networks which are not sensitive to hardware including computing ability and memory bandwidth. Thus, how to design a neural network to efficiently use the computing ability and memory bandwidth of hardware is a critical problem. This paper proposes a method how to design hardware-aware neural network. Based on this method, we designed EfficientRep series convolutional networks, which are high-computation hardware(e.g. GPU) friendly and applied in YOLOv6 object detection framework. YOLOv6 has published YOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.



### PresSim: An End-to-end Framework for Dynamic Ground Pressure Profile Generation from Monocular Videos Using Physics-based 3D Simulation
- **Arxiv ID**: http://arxiv.org/abs/2302.00391v1
- **DOI**: 10.1109/PerComWorkshops56833.2023.10150221
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.00391v1)
- **Published**: 2023-02-01 12:02:04+00:00
- **Updated**: 2023-02-01 12:02:04+00:00
- **Authors**: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz
- **Comment**: Percom2023 workshop(UMUM2023)
- **Journal**: 2023 IEEE International Conference on Pervasive Computing and
  Communications Workshops and other Affiliated Events
- **Summary**: Ground pressure exerted by the human body is a valuable source of information for human activity recognition (HAR) in unobtrusive pervasive sensing. While data collection from pressure sensors to develop HAR solutions requires significant resources and effort, we present a novel end-to-end framework, PresSim, to synthesize sensor data from videos of human activities to reduce such effort significantly. PresSim adopts a 3-stage process: first, extract the 3D activity information from videos with computer vision architectures; then simulate the floor mesh deformation profiles based on the 3D activity information and gravity-included physics simulation; lastly, generate the simulated pressure sensor data with deep learning models. We explored two approaches for the 3D activity information: inverse kinematics with mesh re-targeting, and volumetric pose and shape estimation. We validated PresSim with an experimental setup with a monocular camera to provide input and a pressure-sensing fitness mat (80x28 spatial resolution) to provide the sensor ground truth, where nine participants performed a set of predefined yoga sequences.



### mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video
- **Arxiv ID**: http://arxiv.org/abs/2302.00402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.00402v1)
- **Published**: 2023-02-01 12:40:03+00:00
- **Updated**: 2023-02-01 12:40:03+00:00
- **Authors**: Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou
- **Comment**: None
- **Journal**: ICML2023
- **Summary**: Recent years have witnessed a big convergence of language, vision, and multi-modal pretraining. In this work, we present mPLUG-2, a new unified paradigm with modularized design for multi-modal pretraining, which can benefit from modality collaboration while addressing the problem of modality entanglement. In contrast to predominant paradigms of solely relying on sequence-to-sequence generation or encoder-based instance discrimination, mPLUG-2 introduces a multi-module composition network by sharing common universal modules for modality collaboration and disentangling different modality modules to deal with modality entanglement. It is flexible to select different modules for different understanding and generation tasks across all modalities including text, image, and video. Empirical study shows that mPLUG-2 achieves state-of-the-art or competitive results on a broad range of over 30 downstream tasks, spanning multi-modal tasks of image-text and video-text understanding and generation, and uni-modal tasks of text-only, image-only, and video-only understanding. Notably, mPLUG-2 shows new state-of-the-art results of 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and video caption tasks with a far smaller model size and data scale. It also demonstrates strong zero-shot transferability on vision-language and video-language tasks. Code and models will be released in https://github.com/alibaba/AliceMind.



### Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.07106v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.07106v3)
- **Published**: 2023-02-01 13:12:00+00:00
- **Updated**: 2023-05-28 16:51:14+00:00
- **Authors**: Nishant Kumar, Siniša Šegvić, Abouzar Eslami, Stefan Gumhold
- **Comment**: Accepted as CVPR 2023 Highlight (Top 10% of all acceptance)
- **Journal**: None
- **Summary**: Real-world deployment of reliable object detectors is crucial for applications such as autonomous driving. However, general-purpose object detectors like Faster R-CNN are prone to providing overconfident predictions for outlier objects. Recent outlier-aware object detection approaches estimate the density of instance-wide features with class-conditional Gaussians and train on synthesized outlier features from their low-likelihood regions. However, this strategy does not guarantee that the synthesized outlier features will have a low likelihood according to the other class-conditional Gaussians. We propose a novel outlier-aware object detection framework that distinguishes outliers from inlier objects by learning the joint data distribution of all inlier classes with an invertible normalizing flow. The appropriate sampling of the flow model ensures that the synthesized outliers have a lower likelihood than inliers of all object classes, thereby modeling a better decision boundary between inlier and outlier objects. Our approach significantly outperforms the state-of-the-art for outlier-aware object detection on both image and video datasets. Code available at https://github.com/nish03/FFS



### Do I Have Your Attention: A Large Scale Engagement Prediction Dataset and Baselines
- **Arxiv ID**: http://arxiv.org/abs/2302.00431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.00431v2)
- **Published**: 2023-02-01 13:25:54+00:00
- **Updated**: 2023-08-17 09:50:29+00:00
- **Authors**: Monisha Singh, Ximi Hoque, Donghuo Zeng, Yanan Wang, Kazushi Ikeda, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: The degree of concentration, enthusiasm, optimism, and passion displayed by individual(s) while interacting with a machine is referred to as `user engagement'. Engagement comprises of behavioral, cognitive, and affect related cues. To create engagement prediction systems that can work in real-world conditions, it is quintessential to learn from rich, diverse datasets. To this end, a large scale multi-faceted engagement in the wild dataset EngageNet is proposed. 31 hours duration data of 127 participants representing different illumination conditions are recorded. Thorough experiments are performed exploring the applicability of different features, action units, eye gaze, head pose, and MARLIN. Data from user interactions (question-answer) are analyzed to understand the relationship between effective learning and user engagement. To further validate the rich nature of the dataset, evaluation is also performed on the EngageWild dataset. The experiments show the usefulness of the proposed dataset. The code, models, and dataset link are publicly available at https://github.com/engagenet/engagenet_baselines.



### Learning Prototype Classifiers for Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.00491v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00491v3)
- **Published**: 2023-02-01 15:02:58+00:00
- **Updated**: 2023-06-26 05:42:36+00:00
- **Authors**: Saurabh Sharma, Yongqin Xian, Ning Yu, Ambuj Singh
- **Comment**: Accepted at IJCAI-23
- **Journal**: None
- **Summary**: The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that are biased in that they correlate classifier norm with the amount of training data for a given class. In this work, we show that learning prototype classifiers addresses the biased softmax problem in LTR. Prototype classifiers can deliver promising results simply using Nearest-Class- Mean (NCM), a special case where prototypes are empirical centroids. We go one step further and propose to jointly learn prototypes by using distances to prototypes in representation space as the logit scores for classification. Further, we theoretically analyze the properties of Euclidean distance based prototype classifiers that lead to stable gradient-based optimization which is robust to outliers. To enable independent distance scales along each channel, we enhance Prototype classifiers by learning channel-dependent temperature parameters. Our analysis shows that prototypes learned by Prototype classifiers are better separated than empirical centroids. Results on four LTR benchmarks show that Prototype classifier outperforms or is comparable to state-of-the-art methods. Our code is made available at https://github.com/saurabhsharma1993/prototype-classifier-ltr.



### Tracking People in Highly Dynamic Industrial Environments
- **Arxiv ID**: http://arxiv.org/abs/2302.00503v1
- **DOI**: 10.1109/TMC.2016.2613523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00503v1)
- **Published**: 2023-02-01 15:17:12+00:00
- **Updated**: 2023-02-01 15:17:12+00:00
- **Authors**: Savvas Papaioannou, Andrew Markham, Niki Trigoni
- **Comment**: None
- **Journal**: IEEE Transactions on Mobile Computing, vol. 16, no. 8, pp.
  2351-2365, 1 Aug. 2017
- **Summary**: To date, the majority of positioning systems have been designed to operate within environments that have long-term stable macro-structure with potential small-scale dynamics. These assumptions allow the existing positioning systems to produce and utilize stable maps. However, in highly dynamic industrial settings these assumptions are no longer valid and the task of tracking people is more challenging due to the rapid large-scale changes in structure. In this paper we propose a novel positioning system for tracking people in highly dynamic industrial environments, such as construction sites. The proposed system leverages the existing CCTV camera infrastructure found in many industrial settings along with radio and inertial sensors within each worker's mobile phone to accurately track multiple people. This multi-target multi-sensor tracking framework also allows our system to use cross-modality training in order to deal with the environment dynamics. In particular, we show how our system uses cross-modality training in order to automatically keep track environmental changes (i.e. new walls) by utilizing occlusion maps. In addition, we show how these maps can be used in conjunction with social forces to accurately predict human motion and increase the tracking accuracy. We have conducted extensive real-world experiments in a construction site showing significant accuracy improvement via cross-modality training and the use of social forces.



### Exploring Semantic Perturbations on Grover
- **Arxiv ID**: http://arxiv.org/abs/2302.00509v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00509v1)
- **Published**: 2023-02-01 15:28:55+00:00
- **Updated**: 2023-02-01 15:28:55+00:00
- **Authors**: Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan
- **Comment**: 15 pages, 12 figures, 1 table, capstone research in machine learning
- **Journal**: None
- **Summary**: With news and information being as easy to access as they currently are, it is more important than ever to ensure that people are not mislead by what they read. Recently, the rise of neural fake news (AI-generated fake news) and its demonstrated effectiveness at fooling humans has prompted the development of models to detect it. One such model is the Grover model, which can both detect neural fake news to prevent it, and generate it to demonstrate how a model could be misused to fool human readers. In this work we explore the Grover model's fake news detection capabilities by performing targeted attacks through perturbations on input news articles. Through this we test Grover's resilience to these adversarial attacks and expose some potential vulnerabilities which should be addressed in further iterations to ensure it can detect all types of fake news accurately.



### Towards Implementing Energy-aware Data-driven Intelligence for Smart Health Applications on Mobile Platforms
- **Arxiv ID**: http://arxiv.org/abs/2302.00514v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00514v1)
- **Published**: 2023-02-01 15:34:24+00:00
- **Updated**: 2023-02-01 15:34:24+00:00
- **Authors**: G. Dumindu Samaraweera, Hung Nguyen, Hadi Zanddizari, Behnam Zeinali, J. Morris Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthrough technological progressions of powerful mobile computing resources such as low-cost mobile GPUs along with cutting-edge, open-source software architectures have enabled high-performance deep learning on mobile platforms. These advancements have revolutionized the capabilities of today's mobile applications in different dimensions to perform data-driven intelligence locally, particularly for smart health applications. Unlike traditional machine learning (ML) architectures, modern on-device deep learning frameworks are proficient in utilizing computing resources in mobile platforms seamlessly, in terms of producing highly accurate results in less inference time. However, on the flip side, energy resources in a mobile device are typically limited. Hence, whenever a complex Deep Neural Network (DNN) architecture is fed into the on-device deep learning framework, while it achieves high prediction accuracy (and performance), it also urges huge energy demands during the runtime. Therefore, managing these resources efficiently within the spectrum of performance and energy efficiency is the newest challenge for any mobile application featuring data-driven intelligence beyond experimental evaluations. In this paper, first, we provide a timely review of recent advancements in on-device deep learning while empirically evaluating the performance metrics of current state-of-the-art ML architectures and conventional ML approaches with the emphasis given on energy characteristics by deploying them on a smart health application. With that, we are introducing a new framework through an energy-aware, adaptive model comprehension and realization (EAMCR) approach that can be utilized to make more robust and efficient inference decisions based on the available computing/energy resources in the mobile device during the runtime.



### Synthesis-based Imaging-Differentiation Representation Learning for Multi-Sequence 3D/4D MRI
- **Arxiv ID**: http://arxiv.org/abs/2302.00517v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00517v1)
- **Published**: 2023-02-01 15:37:35+00:00
- **Updated**: 2023-02-01 15:37:35+00:00
- **Authors**: Luyi Han, Tao Tan, Tianyu Zhang, Yunzhi Huang, Xin Wang, Yuan Gao, Jonas Teuwen, Ritse Mann
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-sequence MRIs can be necessary for reliable diagnosis in clinical practice due to the complimentary information within sequences. However, redundant information exists across sequences, which interferes with mining efficient representations by modern machine learning or deep learning models. To handle various clinical scenarios, we propose a sequence-to-sequence generation framework (Seq2Seq) for imaging-differentiation representation learning. In this study, not only do we propose arbitrary 3D/4D sequence generation within one model to generate any specified target sequence, but also we are able to rank the importance of each sequence based on a new metric estimating the difficulty of a sequence being generated. Furthermore, we also exploit the generation inability of the model to extract regions that contain unique information for each sequence. We conduct extensive experiments using three datasets including a toy dataset of 20,000 simulated subjects, a brain MRI dataset of 1,251 subjects, and a breast MRI dataset of 2,101 subjects, to demonstrate that (1) our proposed Seq2Seq is efficient and lightweight for complex clinical datasets and can achieve excellent image quality; (2) top-ranking sequences can be used to replace complete sequences with non-inferior performance; (3) combining MRI with our imaging-differentiation map leads to better performance in clinical tasks such as glioblastoma MGMT promoter methylation status prediction and breast cancer pathological complete response status prediction. Our code is available at https://github.com/fiy2W/mri_seq2seq.



### Uncertainty-Driven Dense Two-View Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2302.00523v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.00523v2)
- **Published**: 2023-02-01 15:52:24+00:00
- **Updated**: 2023-02-12 09:37:57+00:00
- **Authors**: Weirong Chen, Suryansh Kumar, Fisher Yu
- **Comment**: Accepted for publication at IEEE Robotics and Automation Letters
  (RA-L) 2023
- **Journal**: None
- **Summary**: This work introduces an effective and practical solution to the dense two-view structure from motion (SfM) problem. One vital question addressed is how to mindfully use per-pixel optical flow correspondence between two frames for accurate pose estimation -- as perfect per-pixel correspondence between two images is difficult, if not impossible, to establish. With the carefully estimated camera pose and predicted per-pixel optical flow correspondences, a dense depth of the scene is computed. Later, an iterative refinement procedure is introduced to further improve optical flow matching confidence, camera pose, and depth, exploiting their inherent dependency in rigid SfM. The fundamental idea presented is to benefit from per-pixel uncertainty in the optical flow estimation and provide robustness to the dense SfM system via an online refinement. Concretely, we introduce our uncertainty-driven Dense Two-View SfM pipeline (DTV-SfM), consisting of an uncertainty-aware dense optical flow estimation approach that provides per-pixel correspondence with their confidence score of matching; a weighted dense bundle adjustment formulation that depends on optical flow uncertainty and bidirectional optical flow consistency to refine both pose and depth; a depth estimation network that considers its consistency with the estimated poses and optical flow respecting epipolar constraint. Extensive experiments show that the proposed approach achieves remarkable depth accuracy and state-of-the-art camera pose results superseding SuperPoint and SuperGlue accuracy when tested on benchmark datasets such as DeMoN, YFCC100M, and ScanNet. Code and more materials are available at http://vis.xyz/pub/dtv-sfm.



### A latent space for unsupervised MR image quality control via artifact assessment
- **Arxiv ID**: http://arxiv.org/abs/2302.00528v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00528v1)
- **Published**: 2023-02-01 15:55:34+00:00
- **Updated**: 2023-02-01 15:55:34+00:00
- **Authors**: Lianrui Zuo, Yuan Xue, Blake E. Dewey, Yihao Liu, Jerry L. Prince, Aaron Carass
- **Comment**: Accepted at the International Society for Optics and Photonics -
  Medical Imaging (SPIE-MI) 2023
- **Journal**: None
- **Summary**: Image quality control (IQC) can be used in automated magnetic resonance (MR) image analysis to exclude erroneous results caused by poorly acquired or artifact-laden images. Existing IQC methods for MR imaging generally require human effort to craft meaningful features or label large datasets for supervised training. The involvement of human labor can be burdensome and biased, as labeling MR images based on their quality is a subjective task. In this paper, we propose an automatic IQC method that evaluates the extent of artifacts in MR images without supervision. In particular, we design an artifact encoding network that learns representations of artifacts based on contrastive learning. We then use a normalizing flow to estimate the density of learned representations for unsupervised classification. Our experiments on large-scale multi-cohort MR datasets show that the proposed method accurately detects images with high levels of artifacts, which can inform downstream analysis tasks about potentially flawed data.



### An Out-of-Domain Synapse Detection Challenge for Microwasp Brain Connectomes
- **Arxiv ID**: http://arxiv.org/abs/2302.00545v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2302.00545v1)
- **Published**: 2023-02-01 16:08:58+00:00
- **Updated**: 2023-02-01 16:08:58+00:00
- **Authors**: Jingpeng Wu, Yicong Li, Nishika Gupta, Kazunori Shinomiya, Pat Gunn, Alexey Polilov, Hanspeter Pfister, Dmitri Chklovskii, Donglai Wei
- **Comment**: None
- **Journal**: None
- **Summary**: The size of image stacks in connectomics studies now reaches the terabyte and often petabyte scales with a great diversity of appearance across brain regions and samples. However, manual annotation of neural structures, e.g., synapses, is time-consuming, which leads to limited training data often smaller than 0.001\% of the test data in size. Domain adaptation and generalization approaches were proposed to address similar issues for natural images, which were less evaluated on connectomics data due to a lack of out-of-domain benchmarks.



### Correspondence-free online human motion retargeting
- **Arxiv ID**: http://arxiv.org/abs/2302.00556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00556v1)
- **Published**: 2023-02-01 16:23:21+00:00
- **Updated**: 2023-02-01 16:23:21+00:00
- **Authors**: Mathieu Marsot, Rim Rekik, Stefanie Wuhrer, Jean-Sébastien Franco, Anne-Hélène Olivier
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel data-driven framework for unsupervised human motion retargeting which animates a target body shape with a source motion. This allows to retarget motions between different characters by animating a target subject with a motion of a source subject. Our method is correspondence-free,~\ie neither spatial correspondences between the source and target shapes nor temporal correspondences between different frames of the source motion are required. Our proposed method directly animates a target shape with arbitrary sequences of humans in motion, possibly captured using 4D acquisition platforms or consumer devices. Our framework takes into account long-term temporal context of $1$ second during retargeting while accounting for surface details. To achieve this, we take inspiration from two lines of existing work: skeletal motion retargeting, which leverages long-term temporal context at the cost of surface detail, and surface-based retargeting, which preserves surface details without considering long-term temporal context. We unify the advantages of these works by combining a learnt skinning field with a skeletal retargeting approach. During inference, our method runs online,~\ie the input can be processed in a serial way, and retargeting is performed in a single forward pass per frame. Experiments show that including long-term temporal context during training improves the method's accuracy both in terms of the retargeted skeletal motion and the detail preservation. Furthermore, our method generalizes well on unobserved motions and body shapes. We demonstrate that the proposed framework achieves state-of-the-art results on two test datasets.



### An automated, geometry-based method for hippocampal shape and thickness analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.00573v2
- **DOI**: 10.1016/j.neuroimage.2023.120182
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2302.00573v2)
- **Published**: 2023-02-01 16:46:46+00:00
- **Updated**: 2023-06-12 12:21:34+00:00
- **Authors**: Kersten Diers, Hannah Baumeister, Frank Jessen, Emrah Düzel, David Berron, Martin Reuter
- **Comment**: Updated to journal publication
- **Journal**: Neuroimage 276 (2023) 120182
- **Summary**: The hippocampus is one of the most studied neuroanatomical structures due to its involvement in attention, learning, and memory as well as its atrophy in ageing, neurological, and psychiatric diseases. Hippocampal shape changes, however, are complex and cannot be fully characterized by a single summary metric such as hippocampal volume as determined from MR images. In this work, we propose an automated, geometry-based approach for the unfolding, point-wise correspondence, and local analysis of hippocampal shape features such as thickness and curvature. Starting from an automated segmentation of hippocampal subfields, we create a 3D tetrahedral mesh model as well as a 3D intrinsic coordinate system of the hippocampal body. From this coordinate system, we derive local curvature and thickness estimates as well as a 2D sheet for hippocampal unfolding. We evaluate the performance of our algorithm with a series of experiments to quantify neurodegenerative changes in Mild Cognitive Impairment and Alzheimer's disease dementia. We find that hippocampal thickness estimates detect known differences between clinical groups and can determine the location of these effects on the hippocampal sheet. Further, thickness estimates improve classification of clinical groups and cognitively unimpaired controls when added as an additional predictor. Comparable results are obtained with different datasets and segmentation algorithms. Taken together, we replicate canonical findings on hippocampal volume/shape changes in dementia, extend them by gaining insight into their spatial localization on the hippocampal sheet, and provide additional, complementary information beyond traditional measures. We provide a new set of sensitive processing and analysis tools for the analysis of hippocampal geometry that allows comparisons across studies without relying on image registration or requiring manual intervention.



### Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization
- **Arxiv ID**: http://arxiv.org/abs/2302.00624v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.00624v3)
- **Published**: 2023-02-01 17:44:17+00:00
- **Updated**: 2023-05-31 02:54:28+00:00
- **Authors**: Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: 12 pages, 4 figures, ICML 2023
- **Journal**: None
- **Summary**: Contrastive Language-Image Pretraining (CLIP) has demonstrated impressive zero-shot learning abilities for image understanding, yet limited effort has been made to investigate CLIP for zero-shot video recognition. We introduce Open-VCLIP, a simple yet effective approach that transforms CLIP into a strong zero-shot video classifier that can recognize unseen actions and events at test time. Our framework extends CLIP with minimal modifications to model spatial-temporal relationships in videos, making it a specialized video classifier, while striving for generalization. We formally show that training an Open-VCLIP is equivalent to continual learning with zero historical data. To address this problem, we propose Interpolated Weight Optimization, which utilizes the benefit of weight interpolation in both training and test time. We evaluate our method on three popular and challenging action recognition datasets following various zero-shot evaluation protocols and we demonstrate our approach outperforms state-of-the-art methods by clear margins. In particular, we achieve 87.9%, 58.3%, 81.1% zero-shot accuracy on UCF, HMDB and Kinetics-600 respectively, outperforming state-of-the-art methods by 8.3%, 7.8% and 12.2%. Code is released at https://github.com/wengzejia1/Open-VCLIP.



### Continuous U-Net: Faster, Greater and Noiseless
- **Arxiv ID**: http://arxiv.org/abs/2302.00626v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00626v1)
- **Published**: 2023-02-01 17:46:00+00:00
- **Updated**: 2023-02-01 17:46:00+00:00
- **Authors**: Chun-Wun Cheng, Christina Runkel, Lihao Liu, Raymond H Chan, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation is a fundamental task in image analysis and clinical practice. The current state-of-the-art techniques are based on U-shape type encoder-decoder networks with skip connections, called U-Net. Despite the powerful performance reported by existing U-Net type networks, they suffer from several major limitations. Issues include the hard coding of the receptive field size, compromising the performance and computational cost, as well as the fact that they do not account for inherent noise in the data. They have problems associated with discrete layers, and do not offer any theoretical underpinning. In this work we introduce continuous U-Net, a novel family of networks for image segmentation. Firstly, continuous U-Net is a continuous deep neural network that introduces new dynamic blocks modelled by second order ordinary differential equations. Secondly, we provide theoretical guarantees for our network demonstrating faster convergence, higher robustness and less sensitivity to noise. Thirdly, we derive qualitative measures to tailor-made segmentation tasks. We demonstrate, through extensive numerical and visual results, that our model outperforms existing U-Net blocks for several medical image segmentation benchmarking datasets.



### Deep Dependency Networks for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.00633v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00633v2)
- **Published**: 2023-02-01 17:52:40+00:00
- **Updated**: 2023-02-06 17:56:38+00:00
- **Authors**: Shivvrat Arya, Yu Xiang, Vibhav Gogate
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple approach which combines the strengths of probabilistic graphical models and deep learning architectures for solving the multi-label classification task, focusing specifically on image and video data. First, we show that the performance of previous approaches that combine Markov Random Fields with neural networks can be modestly improved by leveraging more powerful methods such as iterative join graph propagation, integer linear programming, and $\ell_1$ regularization-based structure learning. Then we propose a new modeling framework called deep dependency networks, which augments a dependency network, a model that is easy to train and learns more accurate dependencies but is limited to Gibbs sampling for inference, to the output layer of a neural network. We show that despite its simplicity, jointly learning this new architecture yields significant improvements in performance over the baseline neural network. In particular, our experimental evaluation on three video activity classification datasets: Charades, Textually Annotated Cooking Scenes (TACoS), and Wetlab, and three multi-label image classification datasets: MS-COCO, PASCAL VOC, and NUS-WIDE show that deep dependency networks are almost always superior to pure neural architectures that do not use dependency networks.



### Image-Based Vehicle Classification by Synergizing Features from Supervised and Self-Supervised Learning Paradigms
- **Arxiv ID**: http://arxiv.org/abs/2302.00648v1
- **DOI**: 10.3390/eng4010027
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00648v1)
- **Published**: 2023-02-01 18:22:23+00:00
- **Updated**: 2023-02-01 18:22:23+00:00
- **Authors**: Shihan Ma, Jidong J. Yang
- **Comment**: 15 pages, 7 figures, 7 tables
- **Journal**: Eng. 2023; 4(1):444-456
- **Summary**: This paper introduces a novel approach to leverage features learned from both supervised and self-supervised paradigms, to improve image classification tasks, specifically for vehicle classification. Two state-of-the-art self-supervised learning methods, DINO and data2vec, were evaluated and compared for their representation learning of vehicle images. The former contrasts local and global views while the latter uses masked prediction on multi-layered representations. In the latter case, supervised learning is employed to finetune a pretrained YOLOR object detector for detecting vehicle wheels, from which definitive wheel positional features are retrieved. The representations learned from these self-supervised learning methods were combined with the wheel positional features for the vehicle classification task. Particularly, a random wheel masking strategy was utilized to finetune the previously learned representations in harmony with the wheel positional features during the training of the classifier. Our experiments show that the data2vec-distilled representations, which are consistent with our wheel masking strategy, outperformed the DINO counterpart, resulting in a celebrated Top-1 classification accuracy of 97.2% for classifying the 13 vehicle classes defined by the Federal Highway Administration.



### Detecting Histologic & Clinical Glioblastoma Patterns of Prognostic Relevance
- **Arxiv ID**: http://arxiv.org/abs/2302.00669v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00669v2)
- **Published**: 2023-02-01 18:56:09+00:00
- **Updated**: 2023-05-15 17:07:56+00:00
- **Authors**: Bhakti Baheti, Sunny Rai, Shubham Innani, Garv Mehdiratta, Sharath Chandra Guntuku, MacLean P. Nasrallah, Spyridon Bakas
- **Comment**: None
- **Journal**: None
- **Summary**: Glioblastoma is the most common and aggressive malignant adult tumor of the central nervous system, with a grim prognosis and heterogeneous morphologic and molecular profiles. Since adopting the current standard-of-care treatment 18 years ago, no substantial prognostic improvement has been noticed. Accurate prediction of patient overall survival (OS) from histopathology whole slide images (WSI) integrated with clinical data using advanced computational methods could optimize clinical decision-making and patient management. Here, we focus on identifying prognostically relevant glioblastoma characteristics from H&E stained WSI & clinical data relating to OS. The exact approach for WSI capitalizes on the comprehensive curation of apparent artifactual content and an interpretability mechanism via a weakly supervised attention-based multiple-instance learning algorithm that further utilizes clustering to constrain the search space. The automatically placed patterns of high diagnostic value classify each WSI as representative of short or long-survivors. Further assessment of the prognostic relevance of the associated clinical patient data is performed both in isolation and in an integrated manner, using XGBoost and SHapley Additive exPlanations (SHAP). Identifying tumor morphological & clinical patterns associated with short and long OS will enable the clinical neuropathologist to provide additional relevant prognostic information to the treating team and suggest avenues of biological investigation for understanding and potentially treating glioblastoma.



### Stable Target Field for Reduced Variance Score Estimation in Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.00670v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00670v2)
- **Published**: 2023-02-01 18:57:01+00:00
- **Updated**: 2023-02-17 16:19:02+00:00
- **Authors**: Yilun Xu, Shangyuan Tong, Tommi Jaakkola
- **Comment**: Accepted by ICLR 2023. Code available at:
  https://github.com/Newbeeer/stf
- **Journal**: None
- **Summary**: Diffusion models generate samples by reversing a fixed forward diffusion process. Despite already providing impressive empirical results, these diffusion models algorithms can be further improved by reducing the variance of the training targets in their denoising score-matching objective. We argue that the source of such variance lies in the handling of intermediate noise-variance scales, where multiple modes in the data affect the direction of reverse paths. We propose to remedy the problem by incorporating a reference batch which we use to calculate weighted conditional scores as more stable training targets. We show that the procedure indeed helps in the challenging intermediate regime by reducing (the trace of) the covariance of training targets. The new stable targets can be seen as trading bias for reduced variance, where the bias vanishes with increasing reference batch size. Empirically, we show that the new objective improves the image quality, stability, and training speed of various popular diffusion models across datasets with both general ODE and SDE solvers. When used in combination with EDM, our method yields a current SOTA FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10 generation task. The code is available at https://github.com/Newbeeer/stf



### ADAPT: Action-aware Driving Caption Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.00673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.00673v1)
- **Published**: 2023-02-01 18:59:19+00:00
- **Updated**: 2023-02-01 18:59:19+00:00
- **Authors**: Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu
- **Comment**: Accepted to ICRA2023. Code: https://github.com/jxbbb/ADAPT
- **Journal**: None
- **Summary**: End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.



### Towards CGAN-based Satellite Image Synthesis with Partial Pixel-Wise Annotation
- **Arxiv ID**: http://arxiv.org/abs/2303.11175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.11175v1)
- **Published**: 2023-02-01 21:53:33+00:00
- **Updated**: 2023-02-01 21:53:33+00:00
- **Authors**: Hadi Mansourifar, Steven J. Simske
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Nets (CGANs) need a significantly huge dataset with a detailed pixel-wise annotation to generate high-quality images. Unfortunately, any amount of missing pixel annotations may significantly impact the result not only locally, but also in annotated areas. To the best of our knowledge, such a challenge has never been investigated in the broader field of GANs. In this paper, we take the first step in this direction to study the problem of CGAN-based satellite image synthesis given partially annotated images. We first define the problem of image synthesis using partially annotated data, and we discuss a scenario in which we face such a challenge. We then propose an effective solution called detail augmentation to address this problem. To do so, we tested two different approaches to augment details to compensate for missing pixel-wise annotations. In the first approach, we augmented the original images with their Canny edges to using the CGAN to compensate for the missing annotations. The second approach, however, attempted to assign a color to all pixels with missing annotation. Eventually, a different CGAN was trained to translate the new feature images into a final output.



### SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.00785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00785v1)
- **Published**: 2023-02-01 22:39:51+00:00
- **Updated**: 2023-02-01 22:39:51+00:00
- **Authors**: Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, James Zou
- **Comment**: NeurIPS 2022 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: For the deployment of artificial intelligence (AI) in high-risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels that are semantically meaningful to humans. However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as melanoma. In dermatology, skin disease is described using an established clinical lexicon that allows clinicians to describe physical exam findings to one another. To provide a medical dataset densely annotated by domain experts with annotations useful across multiple disease processes, we developed SkinCon: a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include "plaque", "scale", and "erosion". The same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SkinCon dataset, such as probing models, concept-based explanations, and concept bottlenecks. Furthermore, we use SkinCon to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.



