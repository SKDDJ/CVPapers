# Arxiv Papers in cs.CV on 2023-02-05
### Divide and Compose with Score Based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2302.02272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02272v1)
- **Published**: 2023-02-05 00:53:33+00:00
- **Updated**: 2023-02-05 00:53:33+00:00
- **Authors**: Sandesh Ghimire, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy
- **Comment**: None
- **Journal**: None
- **Summary**: While score based generative models, or diffusion models, have found success in image synthesis, they are often coupled with text data or image label to be able to manipulate and conditionally generate images. Even though manipulation of images by changing the text prompt is possible, our understanding of the text embedding and our ability to modify it to edit images is quite limited. Towards the direction of having more control over image manipulation and conditional generation, we propose to learn image components in an unsupervised manner so that we can compose those components to generate and manipulate images in informed manner. Taking inspiration from energy based models, we interpret different score components as the gradient of different energy functions. We show how score based learning allows us to learn interesting components and we can visualize them through generation. We also show how this novel decomposition allows us to compose, generate and modify images in interesting ways akin to dreaming. We make our code available at https://github.com/sandeshgh/Score-based-disentanglement



### JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph Attention Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.02276v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02276v1)
- **Published**: 2023-02-05 01:42:19+00:00
- **Updated**: 2023-02-05 01:42:19+00:00
- **Authors**: Qiyun Liu, Zhiguang Yang, Hanzhou Wu
- **Comment**: https://scholar.google.com/citations?user=IdiF7M0AAAAJ&hl=en
- **Journal**: Journal of Electronic Imaging 2023
- **Summary**: The purpose of image steganalysis is to determine whether the carrier image contains hidden information or not. Since JEPG is the most commonly used image format over social networks, steganalysis in JPEG images is also the most urgently needed to be explored. However, in order to detect whether secret information is hidden within JEPG images, the majority of existing algorithms are designed in conjunction with the popular computer vision related networks, without considering the key characteristics appeared in image steganalysis. It is crucial that the steganographic signal, as an extremely weak signal, can be enhanced during its representation learning process. Motivated by this insight, in this paper, we introduce a novel representation learning algorithm for JPEG steganalysis that is mainly consisting of a graph attention learning module and a feature enhancement module. The graph attention learning module is designed to avoid global feature loss caused by the local feature learning of convolutional neural network and reliance on depth stacking to extend the perceptual domain. The feature enhancement module is applied to prevent the stacking of convolutional layers from weakening the steganographic information. In addition, pretraining as a way to initialize the network weights with a large-scale dataset is utilized to enhance the ability of the network to extract discriminative features. We advocate pretraining with ALASKA2 for the model trained with BOSSBase+BOWS2. The experimental results indicate that the proposed algorithm outperforms previous arts in terms of detection accuracy, which has verified the superiority and applicability of the proposed work.



### Recurrence With Correlation Network for Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2302.02283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02283v1)
- **Published**: 2023-02-05 02:41:46+00:00
- **Updated**: 2023-02-05 02:41:46+00:00
- **Authors**: Vignesh Sivan, Teodora Vujovic, Raj Ranabhat, Alexander Wong, Stewart Mclachlin, Michael Hardisty
- **Comment**: None
- **Journal**: None
- **Summary**: We present Recurrence with Correlation Network (RWCNet), a medical image registration network with multi-scale features and a cost volume layer. We demonstrate that these architectural features improve medical image registration accuracy in two image registration datasets prepared for the MICCAI 2022 Learn2Reg Workshop Challenge. On the large-displacement National Lung Screening Test (NLST) dataset, RWCNet is able to achieve a total registration error (TRE) of 2.11mm between corresponding keypoints without instance fine-tuning. On the OASIS brain MRI dataset, RWCNet is able to achieve an average dice overlap of 81.7% for 35 different anatomical labels. It outperforms another multi-scale network, the Laplacian Image Registration Network (LapIRN), on both datasets. Ablation experiments are performed to highlight the contribution of the various architectural features. While multi-scale features improved validation accuracy for both datasets, the cost volume layer and number of recurrent steps only improved performance on the large-displacement NLST dataset. This result suggests that cost volume layer and iterative refinement using RNN provide good support for optimization and generalization in large-displacement medical image registration. The code for RWCNet is available at https://github.com/vigsivan/optimization-based-registration.



### Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation
- **Arxiv ID**: http://arxiv.org/abs/2302.02284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.02284v1)
- **Published**: 2023-02-05 02:47:13+00:00
- **Updated**: 2023-02-05 02:47:13+00:00
- **Authors**: Shiqi Sun, Shancheng Fang, Qian He, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models are able to generate photorealistic images in arbitrary scenes. However, when applying diffusion models to image translation, there exists a trade-off between maintaining spatial structure and high-quality content. Besides, existing methods are mainly based on test-time optimization or fine-tuning model for each input image, which are extremely time-consuming for practical applications. To address these issues, we propose a new approach for flexible image translation by learning a layout-aware image condition together with a text condition. Specifically, our method co-encodes images and text into a new domain during the training phase. In the inference stage, we can choose images/text or both as the conditions for each time step, which gives users more flexible control over layout and content. Experimental comparisons of our method with state-of-the-art methods demonstrate our model performs best in both style image translation and semantic image translation and took the shortest time.



### ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2302.02285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02285v1)
- **Published**: 2023-02-05 03:01:28+00:00
- **Updated**: 2023-02-05 03:01:28+00:00
- **Authors**: Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.



### Selecting the Best Optimizers for Deep Learning based Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.02289v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02289v1)
- **Published**: 2023-02-05 03:49:27+00:00
- **Updated**: 2023-02-05 03:49:27+00:00
- **Authors**: Aliasghar Mortazi, Vedat Cicek, Elif Keles, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to identify the best optimizers for deep learning in the context of cardiac image segmentation and to provide guidance on how to design segmentation networks with effective optimization strategies. Adaptive learning helps with fast convergence by starting with a larger learning rate (LR) and gradually decreasing it. Momentum optimizers are particularly effective at quickly optimizing neural networks within the accelerated schemes category. By revealing the potential interplay between these two types of algorithms (LR and momentum optimizers or momentum rate (MR) in short), in this article, we explore the two variants of SGD algorithms in a single setting. We suggest using cyclic learning as the base optimizer and integrating optimal values of learning rate and momentum rate. We investigated the relationship of LR and MR under an important problem of medical image segmentation of cardiac structures from MRI and CT scans. We conducted experiments using the cardiac imaging dataset from the ACDC challenge of MICCAI 2017, and four different architectures shown to be successful for cardiac image segmentation problems. Our comprehensive evaluations demonstrated that the proposed optimizer achieved better results (over a 2\% improvement in the dice metric) than other optimizers in deep learning literature with similar or lower computational cost in both single and multi-object segmentation settings. We hypothesized that combination of accelerated and adaptive optimization methods can have a drastic effect in medical image segmentation performances. To this end, we proposed a new cyclic optimization method (\textit{CLMR}) to address the efficiency and accuracy problems in deep learning based medical image segmentation. The proposed strategy yielded better generalization in comparison to adaptive optimizers.



### A Disparity Refinement Framework for Learning-based Stereo Matching Methods in Cross-domain Setting for Laparoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2302.02294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02294v1)
- **Published**: 2023-02-05 04:14:19+00:00
- **Updated**: 2023-02-05 04:14:19+00:00
- **Authors**: Zixin Yang, Richard Simon, Cristian A. Linte
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Stereo matching methods that enable depth estimation are crucial for visualization enhancement applications in computer-assisted surgery (CAS). Learning-based stereo matching methods are promising to predict accurate results on laparoscopic images. However, they require a large amount of training data, and their performance may be degraded due to domain shifts.   Methods: Maintaining robustness and improving the accuracy of learning-based methods are still open problems. To overcome the limitations of learning-based methods, we propose a disparity refinement framework consisting of a local disparity refinement method and a global disparity refinement method to improve the results of learning-based stereo matching methods in a cross-domain setting. Those learning-based stereo matching methods are pre-trained on a large public dataset of natural images and are tested on two datasets of laparoscopic images.   Results: Qualitative and quantitative results suggest that our proposed disparity framework can effectively refine disparity maps when they are noise-corrupted on an unseen dataset, without compromising prediction accuracy when the network can generalize well on an unseen dataset.   Conclusion: Our proposed disparity refinement framework could work with learning-based methods to achieve robust and accurate disparity prediction. Yet, as a large laparoscopic dataset for training learning-based methods does not exist and the generalization ability of networks remains to be improved, the incorporation of the proposed disparity refinement framework into existing networks will contribute to improving their overall accuracy and robustness associated with depth estimation.



### Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization
- **Arxiv ID**: http://arxiv.org/abs/2302.10185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10185v1)
- **Published**: 2023-02-05 04:45:08+00:00
- **Updated**: 2023-02-05 04:45:08+00:00
- **Authors**: Daniel D Kim, Rajat S Chandra, Jian Peng, Jing Wu, Xue Feng, Michael Atalay, Chetan Bettegowda, Craig Jones, Haris Sair, Wei-hua Liao, Chengzhang Zhu, Beiji Zou, Li Yang, Anahita Fathi Kazerooni, Ali Nabavizadeh, Harrison X Bai, Zhicheng Jiao
- **Comment**: 22 pages, 3 figures, 3 tables, 1 supplementary data document.
  Submitted to Medical Physics in Jan 2023
- **Journal**: None
- **Summary**: Deep learning models have demonstrated great potential in medical 3D imaging, but their development is limited by the expensive, large volume of annotated data required. Active learning (AL) addresses this by training a model on a subset of the most informative data samples without compromising performance. We compared different AL strategies and propose a framework that minimizes the amount of data needed for state-of-the-art performance. 638 multi-institutional brain tumor MRI images were used to train a 3D U-net model and compare AL strategies. We investigated uncertainty sampling, annotation redundancy restriction, and initial dataset selection techniques. Uncertainty estimation techniques including Bayesian estimation with dropout, bootstrapping, and margins sampling were compared to random query. Strategies to avoid annotation redundancy by removing similar images within the to-be-annotated subset were considered as well. We determined the minimum amount of data necessary to achieve similar performance to the model trained on the full dataset ({\alpha} = 0.1). A variance-based selection strategy using radiomics to identify the initial training dataset is also proposed. Bayesian approximation with dropout at training and testing showed similar results to that of the full data model with less than 20% of the training data (p=0.293) compared to random query achieving similar performance at 56.5% of the training data (p=0.814). Annotation redundancy restriction techniques achieved state-of-the-art performance at approximately 40%-50% of the training data. Radiomics dataset initialization had higher Dice with initial dataset sizes of 20 and 80 images, but improvements were not significant. In conclusion, we investigated various AL strategies with dropout uncertainty estimation achieving state-of-the-art performance with the least annotated data.



### CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.02314v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02314v3)
- **Published**: 2023-02-05 06:27:45+00:00
- **Updated**: 2023-07-31 15:56:24+00:00
- **Authors**: Zhaoshan Liu, Lei Shen
- **Comment**: 24 pages, 5 figures
- **Journal**: None
- **Summary**: The convolutional neural network (CNN) and transformer are two of the most widely implemented models in the computer vision field. However, the former (latter) one mainly captures local (global) features only. To address the limitation in model performance caused by the lack of features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from existing methods, our CECT can capture features at both multi-local and global scales without any bells and whistles. Moreover, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a diagnosis assistant. Code is available at https://github.com/NUS-Tim/CECT.



### Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.02316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02316v1)
- **Published**: 2023-02-05 06:52:25+00:00
- **Updated**: 2023-02-05 06:52:25+00:00
- **Authors**: Binqian Xu, Xiangbo Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has been successfully leveraged to learn action representations for addressing the problem of semi-supervised skeleton-based action recognition. However, most contrastive learning-based methods only contrast global features mixing spatiotemporal information, which confuses the spatial- and temporal-specific information reflecting different semantic at the frame level and joint level. Thus, we propose a novel Spatiotemporal Decouple-and-Squeeze Contrastive Learning (SDS-CL) framework to comprehensively learn more abundant representations of skeleton-based actions by jointly contrasting spatial-squeezing features, temporal-squeezing features, and global features. In SDS-CL, we design a new Spatiotemporal-decoupling Intra-Inter Attention (SIIA) mechanism to obtain the spatiotemporal-decoupling attentive features for capturing spatiotemporal specific information by calculating spatial- and temporal-decoupling intra-attention maps among joint/motion features, as well as spatial- and temporal-decoupling inter-attention maps between joint and motion features. Moreover, we present a new Spatial-squeezing Temporal-contrasting Loss (STL), a new Temporal-squeezing Spatial-contrasting Loss (TSL), and the Global-contrasting Loss (GL) to contrast the spatial-squeezing joint and motion features at the frame level, temporal-squeezing joint and motion features at the joint level, as well as global joint and motion features at the skeleton level. Extensive experimental results on four public datasets show that the proposed SDS-CL achieves performance gains compared with other competitive methods.



### Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2302.02318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02318v2)
- **Published**: 2023-02-05 06:58:35+00:00
- **Updated**: 2023-05-22 12:40:49+00:00
- **Authors**: Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, Li Yi
- **Comment**: Accepted at ICML 2023
- **Journal**: None
- **Summary**: Mainstream 3D representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics: (i) contrastive models are data-hungry that suffer from a representation over-fitting issue; (ii) generative models have a data filling issue that shows inferior data scaling capacity compared to contrastive models. This motivates us to learn 3D representations by sharing the merits of both paradigms, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose Contrast with Reconstruct (ReCon) that unifies these two paradigms. ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student. An encoder-decoder style ReCon-block is proposed that transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues. ReCon achieves a new state-of-the-art in 3D representation learning, e.g., 91.26% accuracy on ScanObjectNN. Codes have been released at https://github.com/qizekun/ReCon.



### Pyramid Self-attention Polymerization Learning for Semi-supervised Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.02327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02327v1)
- **Published**: 2023-02-05 07:43:02+00:00
- **Updated**: 2023-02-05 07:43:02+00:00
- **Authors**: Binqian Xu, Xiangbo Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Most semi-supervised skeleton-based action recognition approaches aim to learn the skeleton action representations only at the joint level, but neglect the crucial motion characteristics at the coarser-grained body (e.g., limb, trunk) level that provide rich additional semantic information, though the number of labeled data is limited. In this work, we propose a novel Pyramid Self-attention Polymerization Learning (dubbed as PSP Learning) framework to jointly learn body-level, part-level, and joint-level action representations of joint and motion data containing abundant and complementary semantic information via contrastive learning covering coarse-to-fine granularity. Specifically, to complement semantic information from coarse to fine granularity in skeleton actions, we design a new Pyramid Polymerizing Attention (PPA) mechanism that firstly calculates the body-level attention map, part-level attention map, and joint-level attention map, as well as polymerizes these attention maps in a level-by-level way (i.e., from body level to part level, and further to joint level). Moreover, we present a new Coarse-to-fine Contrastive Loss (CCL) including body-level contrast loss, part-level contrast loss, and joint-level contrast loss to jointly measure the similarity between the body/part/joint-level contrasting features of joint and motion data. Finally, extensive experiments are conducted on the NTU RGB+D and North-Western UCLA datasets to demonstrate the competitive performance of the proposed PSP Learning in the semi-supervised skeleton-based action recognition task. The source codes of PSP Learning are publicly available at https://github.com/1xbq1/PSP-Learning.



### CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.02330v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.02330v2)
- **Published**: 2023-02-05 07:50:46+00:00
- **Updated**: 2023-07-18 20:54:24+00:00
- **Authors**: Xia Xu, Jochen Triesch
- **Comment**: 12 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Self-supervised representation learning (SSRL) methods have shown great success in computer vision. In recent studies, augmentation-based contrastive learning methods have been proposed for learning representations that are invariant or equivariant to pre-defined data augmentation operations. However, invariant or equivariant features favor only specific downstream tasks depending on the augmentations chosen. They may result in poor performance when the learned representation does not match task requirements. Here, we consider an active observer that can manipulate views of an object and has knowledge of the action(s) that generated each view. We introduce Contrastive Invariant and Predictive Equivariant Representation learning (CIPER). CIPER comprises both invariant and equivariant learning objectives using one shared encoder and two different output heads on top of the encoder. One output head is a projection head with a state-of-the-art contrastive objective to encourage invariance to augmentations. The other is a prediction head estimating the augmentation parameters, capturing equivariant features. Both heads are discarded after training and only the encoder is used for downstream tasks. We evaluate our method on static image tasks and time-augmented image datasets. Our results show that CIPER outperforms a baseline contrastive method on various tasks. Interestingly, CIPER encourages the formation of hierarchically structured representations where different views of an object become systematically organized in the latent representation space.



### Self-supervised Geometric Features Discovery via Interpretable Attentio for Vehicle Re-Identification and Beyond (Complete Version)
- **Arxiv ID**: http://arxiv.org/abs/2303.11169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.11169v1)
- **Published**: 2023-02-05 08:21:00+00:00
- **Updated**: 2023-02-05 08:21:00+00:00
- **Authors**: Ming Li, Xinming Huang, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: To learn distinguishable patterns, most of recent works in vehicle re-identification (ReID) struggled to redevelop official benchmarks to provide various supervisions, which requires prohibitive human labors. In this paper, we seek to achieve the similar goal but do not involve more human efforts. To this end, we introduce a novel framework, which successfully encodes both geometric local features and global representations to distinguish vehicle instances, optimized only by the supervision from official ID labels. Specifically, given our insight that objects in ReID share similar geometric characteristics, we propose to borrow self-supervised representation learning to facilitate geometric features discovery. To condense these features, we introduce an interpretable attention module, with the core of local maxima aggregation instead of fully automatic learning, whose mechanism is completely understandable and whose response map is physically reasonable. To the best of our knowledge, we are the first that perform self-supervised learning to discover geometric features. We conduct comprehensive experiments on three most popular datasets for vehicle ReID, i.e., VeRi-776, CityFlow-ReID, and VehicleID. We report our state-of-the-art (SOTA) performances and promising visualization results. We also show the excellent scalability of our approach on other ReID related tasks, i.e., person ReID and multi-target multi-camera (MTMC) vehicle tracking. The code is available at https://github.com/ ming1993li/Self-supervised-Geometric.



### Semi-Supervised Domain Adaptation with Source Label Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2302.02335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02335v2)
- **Published**: 2023-02-05 08:34:04+00:00
- **Updated**: 2023-03-24 20:54:44+00:00
- **Authors**: Yu-Chu Yu, Hsuan-Tien Lin
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Semi-Supervised Domain Adaptation (SSDA) involves learning to classify unseen target data with a few labeled and lots of unlabeled target data, along with many labeled source data from a related domain. Current SSDA approaches usually aim at aligning the target data to the labeled source data with feature space mapping and pseudo-label assignments. Nevertheless, such a source-oriented model can sometimes align the target data to source data of the wrong classes, degrading the classification performance. This paper presents a novel source-adaptive paradigm that adapts the source data to match the target data. Our key idea is to view the source data as a noisily-labeled version of the ideal target data. Then, we propose an SSDA model that cleans up the label noise dynamically with the help of a robust cleaner component designed from the target perspective. Since the paradigm is very different from the core ideas behind existing SSDA approaches, our proposed model can be easily coupled with them to improve their performance. Empirical results on two state-of-the-art SSDA approaches demonstrate that the proposed model effectively cleans up the noise within the source labels and exhibits superior performance over those approaches across benchmark datasets. Our code is available at https://github.com/chu0802/SLA .



### Using Intermediate Forward Iterates for Intermediate Generator Optimization
- **Arxiv ID**: http://arxiv.org/abs/2302.02336v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02336v1)
- **Published**: 2023-02-05 08:46:15+00:00
- **Updated**: 2023-02-05 08:46:15+00:00
- **Authors**: Harsh Mishra, Jurijs Nazarovs, Manmohan Dogra, Sathya N. Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: Score-based models have recently been introduced as a richer framework to model distributions in high dimensions and are generally more suitable for generative tasks. In score-based models, a generative task is formulated using a parametric model (such as a neural network) to directly learn the gradient of such high dimensional distributions, instead of the density functions themselves, as is done traditionally. From the mathematical point of view, such gradient information can be utilized in reverse by stochastic sampling to generate diverse samples. However, from a computational perspective, existing score-based models can be efficiently trained only if the forward or the corruption process can be computed in closed form. By using the relationship between the process and layers in a feed-forward network, we derive a backpropagation-based procedure which we call Intermediate Generator Optimization to utilize intermediate iterates of the process with negligible computational overhead. The main advantage of IGO is that it can be incorporated into any standard autoencoder pipeline for the generative task. We analyze the sample complexity properties of IGO to solve downstream tasks like Generative PCA. We show applications of the IGO on two dense predictive tasks viz., image extrapolation, and point cloud denoising. Our experiments indicate that obtaining an ensemble of generators for various time points is possible using first-order methods.



### Explainable Machine Learning: The Importance of a System-Centric Perspective
- **Arxiv ID**: http://arxiv.org/abs/2302.02347v1
- **DOI**: 10.1109/MSP.2022.3211368
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02347v1)
- **Published**: 2023-02-05 09:22:00+00:00
- **Updated**: 2023-02-05 09:22:00+00:00
- **Authors**: Manish Narwaria
- **Comment**: None
- **Journal**: None
- **Summary**: The landscape in the context of several signal processing applications and even education appears to be significantly affected by the emergence of machine learning (ML) and in particular deep learning (DL).The main reason for this is the ability of DL to model complex and unknown relationships between signals and the tasks of interest. Particularly, supervised DL algorithms have been fairly successful at recognizing perceptually or semantically useful signal information in different applications. In all of these, the training process uses labeled data to learn a mapping function (typically implicitly) from signals to the desired information (class label or target label). The trained DL model is then expected to correctly recognize/classify relevant information in a given test signal. A DL based framework is therefore, in general, very appealing since the features and characteristics of the required mapping are learned almost exclusively from the data without resorting to explicit model/system development. The focus on implicit modeling however also raises the issue of lack of explainability/interpretability of the resultant DL based mapping or the black box problem. As a result, explainable ML/DL is an active research area where the primary goal is to elaborate how the ML/DL model arrived at a prediction. We however note that despite the efforts, the commentary on black box problem appears to lack a technical discussion from the view point of: a) its origin and underlying reasons, and b) its practical implications on the design and deployment of ML/DL systems. Accordingly, a reasonable question that can be raised is as follows. Can the traditional system-centric approach (which places emphasis on explicit system modeling) provide useful insights into the nature of black box problem, and help develop more transparent ML/DL systems?



### Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2302.02350v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02350v4)
- **Published**: 2023-02-05 09:48:57+00:00
- **Updated**: 2023-04-03 03:17:42+00:00
- **Authors**: Daoan Zhang, Mingkai Chen, Chenming Li, Lingyun Huang, Jianguo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Generalization (DG) is a fundamental challenge for machine learning models, which aims to improve model generalization on various domains. Previous methods focus on generating domain invariant features from various source domains. However, we argue that the domain variantions also contain useful information, ie, classification-aware information, for downstream tasks, which has been largely ignored. Different from learning domain invariant features from source domains, we decouple the input images into Domain Expert Features and noise. The proposed domain expert features lie in a learned latent space where the images in each domain can be classified independently, enabling the implicit use of classification-aware domain variations. Based on the analysis, we proposed a novel paradigm called Domain Disentanglement Network (DDN) to disentangle the domain expert features from the source domain images and aggregate the source domain expert features for representing the target test domain. We also propound a new contrastive learning method to guide the domain expert features to form a more balanced and separable feature space. Experiments on the widely-used benchmarks of PACS, VLCS, OfficeHome, DomainNet, and TerraIncognita demonstrate the competitive performance of our method compared to the recently proposed alternatives.



### Towards Precision in Appearance-based Gaze Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2302.02353v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.02353v2)
- **Published**: 2023-02-05 10:09:35+00:00
- **Updated**: 2023-02-14 04:57:35+00:00
- **Authors**: Murthy L. R. D., Abhishek Mukhopadhyay, Shambhavi Aggarwal, Ketan Anand, Pradipta Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: Appearance-based gaze estimation systems have shown great progress recently, yet the performance of these techniques depend on the datasets used for training. Most of the existing gaze estimation datasets setup in interactive settings were recorded in laboratory conditions and those recorded in the wild conditions display limited head pose and illumination variations. Further, we observed little attention so far towards precision evaluations of existing gaze estimation approaches. In this work, we present a large gaze estimation dataset, PARKS-Gaze, with wider head pose and illumination variation and with multiple samples for a single Point of Gaze (PoG). The dataset contains 974 minutes of data from 28 participants with a head pose range of 60 degrees in both yaw and pitch directions. Our within-dataset and cross-dataset evaluations and precision evaluations indicate that the proposed dataset is more challenging and enable models to generalize on unseen participants better than the existing in-the-wild datasets. The project page can be accessed here: https://github.com/lrdmurthy/PARKS-Gaze



### FastPillars: A Deployment-friendly Pillar-based 3D Detector
- **Arxiv ID**: http://arxiv.org/abs/2302.02367v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.02367v3)
- **Published**: 2023-02-05 12:13:27+00:00
- **Updated**: 2023-02-08 03:25:51+00:00
- **Authors**: Sifan Zhou, Zhi Tian, Xiangxiang Chu, Xinyu Zhang, Bo Zhang, Xiaobo Lu, Chengjian Feng, Zequn Jie, Patrick Yin Chiang, Lin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The deployment of 3D detectors strikes one of the major challenges in real-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View) detectors favor sparse convolution (known as SPConv) to speed up training and inference, which puts a hard barrier for deployment especially for on-device applications. In this paper, we tackle the problem of efficient 3D object detection from LiDAR point clouds with deployment in mind. To reduce computational burden, we propose a pillar-based 3D detector with high performance from an industry perspective, termed FastPillars. Compared with previous methods, we introduce a more effective Max-and-Attention pillar encoding (MAPE) module, and redesigning a powerful and lightweight backbone CRVNet imbued with Cross Stage Partial network (CSP) in a reparameterization style, forming a compact feature representation framework. Extensive experiments demonstrate that our FastPillars surpasses the state-of-the-art 3D detectors regarding both on-device speed and performance. Specifically, FastPillars can be effectively deployed through TensorRT, obtaining real-time performance (24FPS) on a single RTX3070Ti GPU with 64.6 mAP on the nuScenes test set. Our code is publicly available at: https://github.com/StiphyJay/FastPillars.



### ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2302.02373v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02373v3)
- **Published**: 2023-02-05 12:48:21+00:00
- **Updated**: 2023-03-25 08:00:23+00:00
- **Authors**: Zijian Zhang, Zhou Zhao, Jun Yu, Qi Tian
- **Comment**: Accepted by AAAI 2023 Conference
- **Journal**: None
- **Summary**: Diffusion models have recently exhibited remarkable abilities to synthesize striking image samples since the introduction of denoising diffusion probabilistic models (DDPMs). Their key idea is to disrupt images into noise through a fixed forward process and learn its reverse process to generate samples from noise in a denoising way. For conditional DDPMs, most existing practices relate conditions only to the reverse process and fit it to the reversal of unconditional forward process. We find this will limit the condition modeling and generation in a small time window. In this paper, we propose a novel and flexible conditional diffusion model by introducing conditions into the forward process. We utilize extra latent space to allocate an exclusive diffusion trajectory for each condition based on some shifting rules, which will disperse condition modeling to all timesteps and improve the learning capacity of model. We formulate our method, which we call \textbf{ShiftDDPMs}, and provide a unified point of view on existing related methods. Extensive qualitative and quantitative experiments on image synthesis demonstrate the feasibility and effectiveness of ShiftDDPMs.



### Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2302.02394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02394v2)
- **Published**: 2023-02-05 14:30:22+00:00
- **Updated**: 2023-02-07 02:57:45+00:00
- **Authors**: Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success of text-to-image generation diffusion models has also revolutionized semantic image editing, enabling the manipulation of images based on query/target texts. Despite these advancements, a significant challenge lies in the potential introduction of prior bias in pre-trained models during image editing, e.g., making unexpected modifications to inappropriate regions. To this point, we present a novel Dual-Cycle Diffusion model that addresses the issue of prior bias by generating an unbiased mask as the guidance of image editing. The proposed model incorporates a Bias Elimination Cycle that consists of both a forward path and an inverted path, each featuring a Structural Consistency Cycle to ensure the preservation of image content during the editing process. The forward path utilizes the pre-trained model to produce the edited image, while the inverted path converts the result back to the source image. The unbiased mask is generated by comparing differences between the processed source image and the edited image to ensure that both conform to the same distribution. Our experiments demonstrate the effectiveness of the proposed method, as it significantly improves the D-CLIP score from 0.272 to 0.283. The code will be available at https://github.com/JohnDreamer/DualCycleDiffsion.



### Diffusion Model for Generative Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2302.02398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02398v1)
- **Published**: 2023-02-05 14:53:07+00:00
- **Updated**: 2023-02-05 14:53:07+00:00
- **Authors**: Yutong Xie, Minne Yuan, Bin Dong, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: In supervised learning for image denoising, usually the paired clean images and noisy images are collected or synthesised to train a denoising model. L2 norm loss or other distance functions are used as the objective function for training. It often leads to an over-smooth result with less image details. In this paper, we regard the denoising task as a problem of estimating the posterior distribution of clean images conditioned on noisy images. We apply the idea of diffusion model to realize generative image denoising. According to the noise model in denoising tasks, we redefine the diffusion process such that it is different from the original one. Hence, the sampling of the posterior distribution is a reverse process of dozens of steps from the noisy image. We consider three types of noise model, Gaussian, Gamma and Poisson noise. With the guarantee of theory, we derive a unified strategy for model training. Our method is verified through experiments on three types of noise models and achieves excellent performance.



### Multi-View Masked World Models for Visual Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2302.02408v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02408v2)
- **Published**: 2023-02-05 15:37:02+00:00
- **Updated**: 2023-05-31 08:13:44+00:00
- **Authors**: Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, Pieter Abbeel
- **Comment**: Accepted to ICML 2023. First two authors contributed equally. Project
  webpage: https://sites.google.com/view/mv-mwm
- **Journal**: None
- **Summary**: Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https://sites.google.com/view/mv-mwm.



### Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2302.02410v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.02410v2)
- **Published**: 2023-02-05 15:46:57+00:00
- **Updated**: 2023-08-21 03:46:50+00:00
- **Authors**: Pengfei Ren, Chao Wen, Xiaozheng Zheng, Zhou Xue, Haifeng Sun, Qi Qi, Jingyu Wang, Jianxin Liao
- **Comment**: Accepted to ICCV 2023 (Oral)
- **Journal**: None
- **Summary**: Reconstructing interacting hands from a single RGB image is a very challenging task. On the one hand, severe mutual occlusion and similar local appearance between two hands confuse the extraction of visual features, resulting in the misalignment of estimated hand meshes and the image. On the other hand, there are complex spatial relationship between interacting hands, which significantly increases the solution space of hand poses and increases the difficulty of network learning. In this paper, we propose a decoupled iterative refinement framework to achieve pixel-alignment hand reconstruction while efficiently modeling the spatial relationship between hands. Specifically, we define two feature spaces with different characteristics, namely 2D visual feature space and 3D joint feature space. First, we obtain joint-wise features from the visual feature map and utilize a graph convolution network and a transformer to perform intra- and inter-hand information interaction in the 3D joint feature space, respectively. Then, we project the joint features with global information back into the 2D visual feature space in an obfuscation-free manner and utilize the 2D convolution for pixel-wise enhancement. By performing multiple alternate enhancements in the two feature spaces, our method can achieve an accurate and robust reconstruction of interacting hands. Our method outperforms all existing two-hand reconstruction methods by a large margin on the InterHand2.6M dataset.



### Mixture of Diffusers for scene composition and high resolution image generation
- **Arxiv ID**: http://arxiv.org/abs/2302.02412v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2302.02412v1)
- **Published**: 2023-02-05 15:49:26+00:00
- **Updated**: 2023-02-05 15:49:26+00:00
- **Authors**: Álvaro Barbero Jiménez
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion methods have been proven to be very effective to generate images while conditioning on a text prompt. However, and although the quality of the generated images is unprecedented, these methods seem to struggle when trying to generate specific image compositions. In this paper we present Mixture of Diffusers, an algorithm that builds over existing diffusion models to provide a more detailed control over composition. By harmonizing several diffusion processes acting on different regions of a canvas, it allows generating larger images, where the location of each object and style is controlled by a separate diffusion process.



### Spatio-Temporal Point Process for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2302.02444v1
- **DOI**: 10.1109/TNNLS.2020.2997006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.02444v1)
- **Published**: 2023-02-05 18:14:08+00:00
- **Updated**: 2023-02-05 18:14:08+00:00
- **Authors**: Tao Wang, Kean Chen, Weiyao Lin, John See, Zenghui Zhang, Qian Xu, Xia Jia
- **Comment**: This manuscript is the accepted version for TNNLS(IEEE Transactions
  on Neural Networks and Learning Systems)
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) focuses on modeling the relationship of detected objects among consecutive frames and merge them into different trajectories. MOT remains a challenging task as noisy and confusing detection results often hinder the final performance. Furthermore, most existing research are focusing on improving detection algorithms and association strategies. As such, we propose a novel framework that can effectively predict and mask-out the noisy and confusing detection results before associating the objects into trajectories. In particular, we formulate such "bad" detection results as a sequence of events and adopt the spatio-temporal point process}to model such events. Traditionally, the occurrence rate in a point process is characterized by an explicitly defined intensity function, which depends on the prior knowledge of some specific tasks. Thus, designing a proper model is expensive and time-consuming, with also limited ability to generalize well. To tackle this problem, we adopt the convolutional recurrent neural network (conv-RNN) to instantiate the point process, where its intensity function is automatically modeled by the training data. Furthermore, we show that our method captures both temporal and spatial evolution, which is essential in modeling events for MOT. Experimental results demonstrate notable improvements in addressing noisy and confusing detection results in MOT datasets. An improved state-of-the-art performance is achieved by incorporating our baseline MOT algorithm with the spatio-temporal point process model.



### KDEformer: Accelerating Transformers via Kernel Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.02451v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/2302.02451v2)
- **Published**: 2023-02-05 18:23:49+00:00
- **Updated**: 2023-06-29 17:51:10+00:00
- **Authors**: Amir Zandieh, Insu Han, Majid Daliri, Amin Karbasi
- **Comment**: 26 pages, 7 figures
- **Journal**: None
- **Summary**: Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na\"ive exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, and runtime on various pre-trained models. On BigGAN image generation, we achieve better generative scores than the exact computation with over $4\times$ speedup. For ImageNet classification with T2T-ViT, KDEformer shows over $18\times$ speedup while the accuracy drop is less than $0.5\%$.



### Deep Learning Approach for Early Stage Lung Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.02456v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2302.02456v2)
- **Published**: 2023-02-05 18:50:12+00:00
- **Updated**: 2023-02-15 23:36:32+00:00
- **Authors**: Saleh Abunajm, Nelly Elsayed, Zag ElSayed, Murat Ozer
- **Comment**: Under review in FLAIRS 2023
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of death among different types of cancers. Every year, the lives lost due to lung cancer exceed those lost to pancreatic, breast, and prostate cancer combined. The survival rate for lung cancer patients is very low compared to other cancer patients due to late diagnostics. Thus, early lung cancer diagnostics is crucial for patients to receive early treatments, increasing the survival rate or even becoming cancer-free. This paper proposed a deep-learning model for early lung cancer prediction and diagnosis from Computed Tomography (CT) scans. The proposed mode achieves high accuracy. In addition, it can be a beneficial tool to support radiologists' decisions in predicting and detecting lung cancer and its stage.



### Multi-Task Self-Supervised Learning for Image Segmentation Task
- **Arxiv ID**: http://arxiv.org/abs/2302.02483v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02483v1)
- **Published**: 2023-02-05 21:25:59+00:00
- **Updated**: 2023-02-05 21:25:59+00:00
- **Authors**: Lichun Gao, Chinmaya Khamesra, Uday Kumbhar, Ashay Aglawe
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to breakthroughs in AI and Deep learning methodology, Computer vision techniques are rapidly improving. Most computer vision applications require sophisticated image segmentation to comprehend what is image and to make an analysis of each section easier. Training deep learning networks for semantic segmentation required a large amount of annotated data, which presents a major challenge in practice as it is expensive and labor-intensive to produce such data. The paper presents 1. Self-supervised techniques to boost semantic segmentation performance using multi-task learning with Depth prediction and Surface Normalization . 2. Performance evaluation of the different types of weighing techniques (UW, Nash-MTL) used for Multi-task learning. NY2D dataset was used for performance evaluation. According to our evaluation, the Nash-MTL method outperforms single task learning(Semantic Segmentation).



### Handwriting and Drawing for Depression Detection: A Preliminary Study
- **Arxiv ID**: http://arxiv.org/abs/2302.02499v1
- **DOI**: 10.1007/978-3-031-24801-6_23
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.02499v1)
- **Published**: 2023-02-05 22:33:49+00:00
- **Updated**: 2023-02-05 22:33:49+00:00
- **Authors**: Gennaro Raimo, Michele Buonanno, Massimiliano Conson, Gennaro Cordasco, Marcos Faundez-Zanuy, Stefano Marrone, Fiammetta Marulli, Alessandro Vinciarelli, Anna Esposito
- **Comment**: In: Mahmud, M., Ieracitano, C., Kaiser, M.S., Mammone, N., Morabito,
  F.C. (eds) Applied Intelligence and Informatics. AII 2022. Communications in
  Computer and Information Science, vol 1724. Springer, Cham
- **Journal**: None
- **Summary**: The events of the past 2 years related to the pandemic have shown that it is increasingly important to find new tools to help mental health experts in diagnosing mood disorders. Leaving aside the longcovid cognitive (e.g., difficulty in concentration) and bodily (e.g., loss of smell) effects, the short-term covid effects on mental health were a significant increase in anxiety and depressive symptoms. The aim of this study is to use a new tool, the online handwriting and drawing analysis, to discriminate between healthy individuals and depressed patients. To this aim, patients with clinical depression (n = 14), individuals with high sub-clinical (diagnosed by a test rather than a doctor) depressive traits (n = 15) and healthy individuals (n = 20) were recruited and asked to perform four online drawing /handwriting tasks using a digitizing tablet and a special writing device. From the raw collected online data, seventeen drawing/writing features (categorized into five categories) were extracted, and compared among the three groups of the involved participants, through ANOVA repeated measures analyses. Results shows that Time features are more effective in discriminating between healthy and participants with sub-clinical depressive characteristics. On the other hand, Ductus and Pressure features are more effective in discriminating between clinical depressed and healthy participants.



### Rethinking Robust Contrastive Learning from the Adversarial Perspective
- **Arxiv ID**: http://arxiv.org/abs/2302.02502v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.02502v2)
- **Published**: 2023-02-05 22:43:50+00:00
- **Updated**: 2023-06-06 20:08:59+00:00
- **Authors**: Fatemeh Ghofrani, Mehdi Yaghouti, Pooyan Jamshidi
- **Comment**: None
- **Journal**: None
- **Summary**: To advance the understanding of robust deep learning, we delve into the effects of adversarial training on self-supervised and supervised contrastive learning alongside supervised learning. Our analysis uncovers significant disparities between adversarial and clean representations in standard-trained networks across various learning algorithms. Remarkably, adversarial training mitigates these disparities and fosters the convergence of representations toward a universal set, regardless of the learning scheme used. Additionally, increasing the similarity between adversarial and clean representations, particularly near the end of the network, enhances network robustness. These findings offer valuable insights for designing and training effective and robust deep learning networks. Our code is released at \textcolor{magenta}{\url{https://github.com/softsys4ai/CL-Robustness}}.



### Leaving Reality to Imagination: Robust Classification via Generated Datasets
- **Arxiv ID**: http://arxiv.org/abs/2302.02503v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.02503v2)
- **Published**: 2023-02-05 22:49:33+00:00
- **Updated**: 2023-05-24 01:39:41+00:00
- **Authors**: Hritik Bansal, Aditya Grover
- **Comment**: 22 pages, 12 Figures, 9 Tables. Results for ImageNet-C, and finetuned
  generative model are included now
- **Journal**: None
- **Summary**: Recent research on robustness has revealed significant performance gaps between neural image classifiers trained on datasets that are similar to the test set, and those that are from a naturally shifted distribution, such as sketches, paintings, and animations of the object categories observed during training. Prior work focuses on reducing this gap by designing engineered augmentations of training data or through unsupervised pretraining of a single large model on massive in-the-wild training datasets scraped from the Internet. However, the notion of a dataset is also undergoing a paradigm shift in recent years. With drastic improvements in the quality, ease-of-use, and access to modern generative models, generated data is pervading the web. In this light, we study the question: How do these generated datasets influence the natural robustness of image classifiers? We find that Imagenet classifiers trained on real data augmented with generated data achieve higher accuracy and effective robustness than standard training and popular augmentation strategies in the presence of natural distribution shifts. We analyze various factors influencing these results, including the choice of conditioning strategies and the amount of generated data. Additionally, we find that the standard ImageNet classifiers suffer a performance degradation of upto 20\% on the generated data, indicating their fragility at accurately classifying the objects under novel variations. Lastly, we demonstrate that the image classifiers, which have been trained on real data augmented with generated data from the base generative model, exhibit greater resilience to natural distribution shifts compared to the classifiers trained on real data augmented with generated data from the finetuned generative model on the real data. The code, models, and datasets are available at https://github.com/Hritikbansal/generative-robustness.



### Reconstruction-driven motion estimation for motion-compensated MR CINE imaging
- **Arxiv ID**: http://arxiv.org/abs/2302.02504v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.02504v1)
- **Published**: 2023-02-05 22:51:27+00:00
- **Updated**: 2023-02-05 22:51:27+00:00
- **Authors**: Jiazhen Pan, Wenqi Huang, Daniel Rueckert, Thomas Küstner, Kerstin Hammernik
- **Comment**: None
- **Journal**: None
- **Summary**: In cardiac CINE, motion-compensated MR reconstruction (MCMR) is an effective approach to address highly undersampled acquisitions by incorporating motion information between frames. In this work, we propose a deep learning-based framework to address the MCMR problem efficiently. Contrary to state-of-the-art (SOTA) MCMR methods which break the original problem into two sub-optimization problems, i.e. motion estimation and reconstruction, we formulate this problem as a single entity with one single optimization. We discard the canonical motion-warping loss (similarity measurement between motion-warped images and target images) to estimate the motion, but drive the motion estimation process directly by the final reconstruction performance. The higher reconstruction quality is achieved without using any smoothness loss terms and without iterative processing between motion estimation and reconstruction. Therefore, we avoid non-trivial loss weighting factors tuning and time-consuming iterative processing. Experiments on 43 in-house acquired 2D CINE datasets indicate that the proposed MCMR framework can deliver artifact-free motion estimation and high-quality MR images even for imaging accelerations up to 20x. The proposed framework is compared to SOTA non-MCMR and MCMR methods and outperforms these methods qualitatively and quantitatively in all applied metrics across all experiments with different acceleration rates.



