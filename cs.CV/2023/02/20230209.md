# Arxiv Papers in cs.CV on 2023-02-09
### Optimized Hybrid Focal Margin Loss for Crack Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.04395v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04395v1)
- **Published**: 2023-02-09 01:26:38+00:00
- **Updated**: 2023-02-09 01:26:38+00:00
- **Authors**: Jiajie Chen
- **Comment**: 7 pages, 4 figures, camera-ready version at DICTA 2022
- **Journal**: None
- **Summary**: Many loss functions have been derived from cross-entropy loss functions such as large-margin softmax loss and focal loss. The large-margin softmax loss makes the classification more rigorous and prevents overfitting. The focal loss alleviates class imbalance in object detection by down-weighting the loss of well-classified examples. Recent research has shown that these two loss functions derived from cross entropy have valuable applications in the field of image segmentation. However, to the best of our knowledge, there is no unified formulation that combines these two loss functions so that they can not only be transformed mutually, but can also be used to simultaneously address class imbalance and overfitting. To this end, we subdivide the entropy-based loss into the regularizer-based entropy loss and the focal-based entropy loss, and propose a novel optimized hybrid focal loss to handle extreme class imbalance and prevent overfitting for crack segmentation. We have evaluated our proposal in comparison with three crack segmentation datasets (DeepCrack-DB, CRACK500 and our private PanelCrack dataset). Our experiments demonstrate that the focal margin component can significantly increase the IoU of cracks by 0.43 on DeepCrack-DB and 0.44 on our PanelCrack dataset, respectively.



### Help the Blind See: Assistance for the Visually Impaired through Augmented Acoustic Simulation
- **Arxiv ID**: http://arxiv.org/abs/2303.13536v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2303.13536v1)
- **Published**: 2023-02-09 02:32:33+00:00
- **Updated**: 2023-02-09 02:32:33+00:00
- **Authors**: Alexander Mehta, Ritik Jalisatgi
- **Comment**: None
- **Journal**: None
- **Summary**: An estimated 253 million people have visual impairments. These visual impairments affect everyday lives, and limit their understanding of the outside world. This can pose a risk to health from falling or collisions. We propose a solution to this through quick and detailed communication of environmental spatial geometry through sound, providing the blind and visually impaired the ability to understand their spatial environment through sound technology. The model consists of fast object detection and 3D environmental mapping, which is communicated through a series of quick sound notes. These sound notes are at different frequencies, pitches, and arrangements in order to precisely communicate the depth and location of points within the environment. Sounds are communicated in the form of musical notes in order to be easily recognizable and distinguishable. A unique algorithm is used to segment objects, providing minimal accuracy loss and improvement from the normal O(n2 ) to O(n) (which is significant, as N in point clouds can often be in the range of 105 ). In testing, we achieved an R-value of 0.866 on detailed objects and an accuracy of 87.5% on an outdoor scene at night with large amounts of noise. We also provide a supplementary video demo of our system.



### An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.04419v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04419v3)
- **Published**: 2023-02-09 03:11:21+00:00
- **Updated**: 2023-06-12 06:29:22+00:00
- **Authors**: Jaesik Yoon, Yi-Fu Wu, Heechul Bae, Sungjin Ahn
- **Comment**: We study unsupervised object-centric representations in reinforcement
  learning through systematic investigation
- **Journal**: ICML 2023
- **Summary**: Unsupervised object-centric representation (OCR) learning has recently drawn attention as a new paradigm of visual representation. This is because of its potential of being an effective pre-training technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning (RL) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in RL has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of OCR pre-training for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual RL benchmark and conduct experiments to answer questions such as ``Does OCR pre-training improve performance on object-centric tasks?'' and ``Can OCR pre-training help with out-of-distribution generalization?''. Our results provide empirical evidence for valuable insights into the effectiveness of OCR pre-training for RL and the potential limitations of its use in certain scenarios. Additionally, this study also examines the critical aspects of incorporating OCR pre-training in RL, including performance in a visually complex environment and the appropriate pooling layer to aggregate the object representations.



### Zero-Knowledge Zero-Shot Learning for Novel Visual Category Discovery
- **Arxiv ID**: http://arxiv.org/abs/2302.04427v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04427v1)
- **Published**: 2023-02-09 03:40:50+00:00
- **Updated**: 2023-02-09 03:40:50+00:00
- **Authors**: Zhaonan Li, Hongfu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) and Open-Set Recognition (OSR) are two mainstream settings that greatly extend conventional visual object recognition. However, the limitations of their problem settings are not negligible. The novel categories in GZSL require pre-defined semantic labels, making the problem setting less realistic; the oversimplified unknown class in OSR fails to explore the innate fine-grained and mixed structures of novel categories. In light of this, we are motivated to consider a new problem setting named Zero-Knowledge Zero-Shot Learning (ZK-ZSL) that assumes no prior knowledge of novel classes and aims to classify seen and unseen samples and recover semantic attributes of the fine-grained novel categories for further interpretation. To achieve this, we propose a novel framework that recovers the clustering structures of both seen and unseen categories where the seen class structures are guided by source labels. In addition, a structural alignment loss is designed to aid the semantic learning of unseen categories with their recovered structures. Experimental results demonstrate our method's superior performance in classification and semantic recovery on four benchmark datasets.



### Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples
- **Arxiv ID**: http://arxiv.org/abs/2302.04440v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04440v2)
- **Published**: 2023-02-09 04:57:27+00:00
- **Updated**: 2023-05-29 18:59:56+00:00
- **Authors**: Marco Jiralerspong, Avishek Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, Gauthier Gidel
- **Comment**: None
- **Journal**: None
- **Summary**: The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models.



### Contour Completion using Deep Structural Priors
- **Arxiv ID**: http://arxiv.org/abs/2302.04447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04447v1)
- **Published**: 2023-02-09 05:45:33+00:00
- **Updated**: 2023-02-09 05:45:33+00:00
- **Authors**: Ali Shiraee, Morteza Rezanejad, Mohammad Khodadad, Dirk B. Walther, Hamidreza Mahyar
- **Comment**: None
- **Journal**: None
- **Summary**: Humans can easily perceive illusory contours and complete missing forms in fragmented shapes. This work investigates whether such capability can arise in convolutional neural networks (CNNs) using deep structural priors computed directly from images. In this work, we present a framework that completes disconnected contours and connects fragmented lines and curves. In our framework, we propose a model that does not even need to know which regions of the contour are eliminated. We introduce an iterative process that completes an incomplete image and we propose novel measures that guide this to find regions it needs to complete. Our model trains on a single image and fills in the contours with no additional training data. Our work builds a robust framework to achieve contour completion using deep structural priors and extensively investigate how such a model could be implemented.



### GFM: Building Geospatial Foundation Models via Continual Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2302.04476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04476v2)
- **Published**: 2023-02-09 07:39:02+00:00
- **Updated**: 2023-03-30 21:26:37+00:00
- **Authors**: Matias Mendieta, Boran Han, Xingjian Shi, Yi Zhu, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Geospatial technologies are becoming increasingly essential in our world for a wide range of applications, including agriculture, urban planning, and disaster response. To help improve the applicability and performance of deep learning models on these geospatial tasks, various works have begun investigating foundation models for this domain. Researchers have explored two prominent approaches for introducing such models in geospatial applications, but both have drawbacks in terms of limited performance benefit or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We first construct a compact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we investigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously providing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classification, multi-label classification, semantic segmentation, and super-resolution.



### IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2302.10896v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10896v2)
- **Published**: 2023-02-09 07:50:46+00:00
- **Updated**: 2023-05-31 13:24:13+00:00
- **Authors**: Xiaoyun Xu, Guilherme Perin, Stjepan Picek
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method, IB-RAR, which uses Information Bottleneck (IB) to strengthen adversarial robustness for both adversarial training and non-adversarial-trained methods. We first use the IB theory to build regularizers as learning objectives in the loss function. Then, we filter out unnecessary features of intermediate representation according to their mutual information (MI) with labels, as the network trained with IB provides easily distinguishable MI for its features. Experimental results show that our method can be naturally combined with adversarial training and provides consistently better accuracy on new adversarial examples. Our method improves the accuracy by an average of 3.07% against five adversarial attacks for the VGG16 network, trained with three adversarial training benchmarks and the CIFAR-10 dataset. In addition, our method also provides good robustness for undefended methods, such as training with cross-entropy loss only. Finally, in the absence of adversarial training, the VGG16 network trained using our method and the CIFAR-10 dataset reaches an accuracy of 35.86% against PGD examples, while using all layers reaches 25.61% accuracy.



### A General Mobile Manipulator Automation Framework for Flexible Manufacturing in Hostile Industrial Environments
- **Arxiv ID**: http://arxiv.org/abs/2302.04486v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04486v1)
- **Published**: 2023-02-09 08:17:38+00:00
- **Updated**: 2023-02-09 08:17:38+00:00
- **Authors**: Can Pu, Chuanyu Yang, Jinnian Pu, Robert B. Fisher
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: To enable a mobile manipulator to perform human tasks from a single teaching demonstration is vital to flexible manufacturing. We call our proposed method MMPA (Mobile Manipulator Process Automation with One-shot Teaching). Currently, there is no effective and robust MMPA framework which is not influenced by harsh industrial environments and the mobile base's parking precision. The proposed MMPA framework consists of two stages: collecting data (mobile base's location, environment information, end-effector's path) in the teaching stage for robot learning; letting the end-effector repeat the nearly same path as the reference path in the world frame to reproduce the work in the automation stage. More specifically, in the automation stage, the robot navigates to the specified location without the need of a precise parking. Then, based on colored point cloud registration, the proposed IPE (Iterative Pose Estimation by Eye & Hand) algorithm could estimate the accurate 6D relative parking pose of the robot arm base without the need of any marker. Finally, the robot could learn the error compensation from the parking pose's bias to modify the end-effector's path to make it repeat a nearly same path in the world coordinate system as recorded in the teaching stage. Hundreds of trials have been conducted with a real mobile manipulator to show the superior robustness of the system and the accuracy of the process automation regardless of the harsh industrial conditions and parking precision. For the released code, please contact marketing@amigaga.com



### 3D reconstruction from spherical images: A review of techniques, applications, and prospects
- **Arxiv ID**: http://arxiv.org/abs/2302.04495v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04495v4)
- **Published**: 2023-02-09 08:45:27+00:00
- **Updated**: 2023-05-18 01:23:40+00:00
- **Authors**: San Jiang, Yaxin Li, Duojie Weng, Kan You, Wu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively used due to the capability of recording surrounding environments from one camera exposure. In contrast to perspective images with limited FOV (Field of View), spherical images can cover the whole scene with full horizontal and vertical FOV and facilitate camera tracking and data acquisition in these complex scenes. With the rapid evolution and extensive use of professional and consumer-grade spherical cameras, spherical images show great potential for the 3D modeling of urban and indoor scenes. Classical 3D reconstruction pipelines, however, cannot be directly used for spherical images. Besides, there exist few software packages that are designed for the 3D reconstruction of spherical images. As a result, this research provides a thorough survey of the state-of-the-art for 3D reconstruction of spherical images in terms of data acquisition, feature detection and matching, image orientation, and dense matching as well as presenting promising applications and discussing potential prospects. We anticipate that this study offers insightful clues to direct future research.



### IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.04521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04521v1)
- **Published**: 2023-02-09 09:27:40+00:00
- **Updated**: 2023-02-09 09:27:40+00:00
- **Authors**: Xiaoibin Wang, Shuang Gao, Yuntao Zou, Jianlan Guo, Chu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: For the problems of low recognition rate and slow recognition speed of traditional detection methods in IC appearance defect detection, we propose an IC appearance defect detection algo-rithm IH-ViT. Our proposed model takes advantage of the respective strengths of CNN and ViT to acquire image features from both local and global aspects, and finally fuses the two features for decision making to determine the class of defects, thus obtaining better accuracy of IC defect recognition. To address the problem that IC appearance defects are mainly reflected in the dif-ferences in details, which are difficult to identify by traditional algorithms, we improved the tra-ditional ViT by performing an additional convolution operation inside the batch. For the problem of information imbalance of samples due to diverse sources of data sets, we adopt a dual-channel image segmentation technique to further improve the accuracy of IC appearance defects. Finally, after testing, our proposed hybrid IH-ViT model achieved 72.51% accuracy, which is 2.8% and 6.06% higher than ResNet50 and ViT models alone. The proposed algorithm can quickly and accurately detect the defect status of IC appearance and effectively improve the productivity of IC packaging and testing companies.



### Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2302.04527v1
- **DOI**: 10.1109/TITS.2022.3217342
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04527v1)
- **Published**: 2023-02-09 09:39:59+00:00
- **Updated**: 2023-02-09 09:39:59+00:00
- **Authors**: Dichao Liu, Toshihiko Yamasaki, Yu Wang, Kenji Mase, Jien Kato
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 24,
  NO. 1, JANUARY 2023
- **Summary**: The number of traffic accidents has been continuously increasing in recent years worldwide. Many accidents are caused by distracted drivers, who take their attention away from driving. Motivated by the success of Convolutional Neural Networks (CNNs) in computer vision, many researchers developed CNN-based algorithms to recognize distracted driving from a dashcam and warn the driver against unsafe behaviors. However, current models have too many parameters, which is unfeasible for vehicle-mounted computing. This work proposes a novel knowledge-distillation-based framework to solve this problem. The proposed framework first constructs a high-performance teacher network by progressively strengthening the robustness to illumination changes from shallow to deep layers of a CNN. Then, the teacher network is used to guide the architecture searching process of a student network through knowledge distillation. After that, we use the teacher network again to transfer knowledge to the student network by knowledge distillation. Experimental results on the Statefarm Distracted Driver Detection Dataset and AUC Distracted Driver Dataset show that the proposed approach is highly effective for recognizing distracted driving behaviors from photos: (1) the teacher network's accuracy surpasses the previous best accuracy; (2) the student network achieves very high accuracy with only 0.42M parameters (around 55% of the previous most lightweight model). Furthermore, the student network architecture can be extended to a spatial-temporal 3D CNN for recognizing distracted driving from video clips. The 3D student network largely surpasses the previous best accuracy with only 2.03M parameters on the Drive&Act Dataset. The source code is available at https://github.com/Dichao-Liu/Lightweight_Distracted_Driver_Recognition_with_Distillation-Based_NAS_and_Knowledge_Transfer.



### Efficient Attention via Control Variates
- **Arxiv ID**: http://arxiv.org/abs/2302.04542v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04542v1)
- **Published**: 2023-02-09 10:16:20+00:00
- **Updated**: 2023-02-09 10:16:20+00:00
- **Authors**: Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong
- **Comment**: 36 pages, code publicly available at
  https://github.com/HKUNLP/efficient-attention
- **Journal**: None
- **Summary**: Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.



### GMConv: Modulating Effective Receptive Fields for Convolutional Kernels
- **Arxiv ID**: http://arxiv.org/abs/2302.04544v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04544v3)
- **Published**: 2023-02-09 10:17:17+00:00
- **Updated**: 2023-04-20 03:35:13+00:00
- **Authors**: Qi Chen, Chao Li, Jia Ning, Stephen Lin, Kun He
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: In convolutional neural networks, the convolutions are conventionally performed using a square kernel with a fixed N $\times$ N receptive field (RF). However, what matters most to the network is the effective receptive field (ERF) that indicates the extent with which input pixels contribute to an output pixel. Inspired by the property that ERFs typically exhibit a Gaussian distribution, we propose a Gaussian Mask convolutional kernel (GMConv) in this work. Specifically, GMConv utilizes the Gaussian function to generate a concentric symmetry mask that is placed over the kernel to refine the RF. Our GMConv can directly replace the standard convolutions in existing CNNs and can be easily trained end-to-end by standard back-propagation. We evaluate our approach through extensive experiments on image classification and object detection tasks. Over several tasks and standard base models, our approach compares favorably against the standard convolution. For instance, using GMConv for AlexNet and ResNet-50, the top-1 accuracy on ImageNet classification is boosted by 0.98% and 0.85%, respectively.



### Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2302.04578v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04578v2)
- **Published**: 2023-02-09 11:36:39+00:00
- **Updated**: 2023-06-06 06:34:46+00:00
- **Authors**: Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan
- **Comment**: Accepted by ICML2023 (Oral)
- **Journal**: None
- **Summary**: Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.



### Complex Network for Complex Problems: A comparative study of CNN and Complex-valued CNN
- **Arxiv ID**: http://arxiv.org/abs/2302.04584v1
- **DOI**: 10.1109/IPAS55744.2022.10053060
- **Categories**: **cs.CV**, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2302.04584v1)
- **Published**: 2023-02-09 11:51:46+00:00
- **Updated**: 2023-02-09 11:51:46+00:00
- **Authors**: Soumick Chatterjee, Pavan Tummala, Oliver Speck, Andreas Nürnberger
- **Comment**: None
- **Journal**: 2022 IEEE 5th International Conference on Image Processing
  Applications and Systems (IPAS)
- **Summary**: Neural networks, especially convolutional neural networks (CNN), are one of the most common tools these days used in computer vision. Most of these networks work with real-valued data using real-valued features. Complex-valued convolutional neural networks (CV-CNN) can preserve the algebraic structure of complex-valued input data and have the potential to learn more complex relationships between the input and the ground-truth. Although some comparisons of CNNs and CV-CNNs for different tasks have been performed in the past, a large-scale investigation comparing different models operating on different tasks has not been conducted. Furthermore, because complex features contain both real and imaginary components, CV-CNNs have double the number of trainable parameters as real-valued CNNs in terms of the actual number of trainable parameters. Whether or not the improvements in performance with CV-CNN observed in the past have been because of the complex features or just because of having double the number of trainable parameters has not yet been explored. This paper presents a comparative study of CNN, CNNx2 (CNN with double the number of trainable parameters as the CNN), and CV-CNN. The experiments were performed using seven models for two different tasks - brain tumour classification and segmentation in brain MRIs. The results have revealed that the CV-CNN models outperformed the CNN and CNNx2 models.



### Liver Segmentation in Time-resolved C-arm CT Volumes Reconstructed from Dynamic Perfusion Scans using Time Separation Technique
- **Arxiv ID**: http://arxiv.org/abs/2302.04585v1
- **DOI**: 10.1109/IPAS55744.2022.10052849
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04585v1)
- **Published**: 2023-02-09 11:57:09+00:00
- **Updated**: 2023-02-09 11:57:09+00:00
- **Authors**: Soumick Chatterjee, Hana Haseljić, Robert Frysch, Vojtěch Kulvait, Vladimir Semshchikov, Bennet Hensen, Frank Wacker, Inga Brüschx, Thomas Werncke, Oliver Speck, Andreas Nürnberger, Georg Rose
- **Comment**: None
- **Journal**: 2022 IEEE 5th International Conference on Image Processing
  Applications and Systems (IPAS)
- **Summary**: Perfusion imaging is a valuable tool for diagnosing and treatment planning for liver tumours. The time separation technique (TST) has been successfully used for modelling C-arm cone-beam computed tomography (CBCT) perfusion data. The reconstruction can be accompanied by the segmentation of the liver - for better visualisation and for generating comprehensive perfusion maps. Recently introduced Turbolift learning has been seen to perform well while working with TST reconstructions, but has not been explored for the time-resolved volumes (TRV) estimated out of TST reconstructions. The segmentation of the TRVs can be useful for tracking the movement of the liver over time. This research explores this possibility by training the multi-scale attention UNet of Turbolift learning at its third stage on the TRVs and shows the robustness of Turbolift learning since it can even work efficiently with the TRVs, resulting in a Dice score of 0.864$\pm$0.004.



### MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.04589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04589v1)
- **Published**: 2023-02-09 12:06:08+00:00
- **Updated**: 2023-02-09 12:06:08+00:00
- **Authors**: Yuhe Ding, Jian Liang, Bo Jiang, Aihua Zheng, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Existing cross-domain keypoint detection methods always require accessing the source data during adaptation, which may violate the data privacy law and pose serious security concerns. Instead, this paper considers a realistic problem setting called source-free domain adaptive keypoint detection, where only the well-trained source model is provided to the target domain. For the challenging problem, we first construct a teacher-student learning baseline by stabilizing the predictions under data augmentation and network ensembles. Built on this, we further propose a unified approach, Mixup Augmentation and Progressive Selection (MAPS), to fully exploit the noisy pseudo labels of unlabeled target data during training. On the one hand, MAPS regularizes the model to favor simple linear behavior in-between the target samples via self-mixup augmentation, preventing the model from over-fitting to noisy predictions. On the other hand, MAPS employs the self-paced learning paradigm and progressively selects pseudo-labeled samples from `easy' to `hard' into the training process to reduce noise accumulation. Results on four keypoint detection datasets show that MAPS outperforms the baseline and achieves comparable or even better results in comparison to previous non-source-free counterparts.



### Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search
- **Arxiv ID**: http://arxiv.org/abs/2302.04607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04607v1)
- **Published**: 2023-02-09 12:45:20+00:00
- **Updated**: 2023-02-09 12:45:20+00:00
- **Authors**: Jiabei Wang, Yanwei Pang, Jiale Cao, Hanqing Sun, Zhuang Shao, Xuelong Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only person bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusion-level variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scale-invariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets CUHK-SYSU and PRW, respectively. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches. We hope that our simple intra-image contrastive learning can provide more paradigms on weakly supervised person search. The source code is available at \url{https://github.com/jiabeiwangTJU/DICL}.



### Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2302.04625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04625v1)
- **Published**: 2023-02-09 13:20:49+00:00
- **Updated**: 2023-02-09 13:20:49+00:00
- **Authors**: Kooshan Hashemifard, Pau Climent-Perez, Francisco Florez-Revuelta
- **Comment**: 14 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Human skin segmentation is a crucial task in computer vision and biometric systems, yet it poses several challenges such as variability in skin color, pose, and illumination. This paper presents a robust data-driven skin segmentation method for a single image that addresses these challenges through the integration of contextual information and efficient network design. In addition to robustness and accuracy, the integration into real-time systems requires a careful balance between computational power, speed, and performance. The proposed method incorporates two attention modules, Body Attention and Skin Attention, that utilize contextual information to improve segmentation results. These modules draw attention to the desired areas, focusing on the body boundaries and skin pixels, respectively. Additionally, an efficient network architecture is employed in the encoder part to minimize computational power while retaining high performance. To handle the issue of noisy labels in skin datasets, the proposed method uses a weakly supervised training strategy, relying on the Skin Attention module. The results of this study demonstrate that the proposed method is comparable to, or outperforms, state-of-the-art methods on benchmark datasets.



### Better Diffusion Models Further Improve Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2302.04638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04638v2)
- **Published**: 2023-02-09 13:46:42+00:00
- **Updated**: 2023-06-01 10:23:16+00:00
- **Authors**: Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, Shuicheng Yan
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\ell_\infty$-norm threat model with $\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10 ($+4.44\%$). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets. Our code is available at https://github.com/wzekai99/DM-Improves-AT.



### Mixed-order self-paced curriculum learning for universal lesion detection
- **Arxiv ID**: http://arxiv.org/abs/2302.04677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.04677v1)
- **Published**: 2023-02-09 14:52:44+00:00
- **Updated**: 2023-02-09 14:52:44+00:00
- **Authors**: Han Li, Hu Han, S. Kevin Zhou
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Self-paced curriculum learning (SCL) has demonstrated its great potential in computer vision, natural language processing, etc. During training, it implements easy-to-hard sampling based on online estimation of data difficulty. Most SCL methods commonly adopt a loss-based strategy of estimating data difficulty and deweighting the `hard' samples in the early training stage. While achieving success in a variety of applications, SCL stills confront two challenges in a medical image analysis task, such as universal lesion detection, featuring insufficient and highly class-imbalanced data: (i) the loss-based difficulty measurer is inaccurate; ii) the hard samples are under-utilized from a deweighting mechanism. To overcome these challenges, in this paper we propose a novel mixed-order self-paced curriculum learning (Mo-SCL) method. We integrate both uncertainty and loss to better estimate difficulty online and mix both hard and easy samples in the same mini-batch to appropriately alleviate the problem of under-utilization of hard samples. We provide a theoretical investigation of our method in the context of stochastic gradient descent optimization and extensive experiments based on the DeepLesion benchmark dataset for universal lesion detection (ULD). When applied to two state-of-the-art ULD methods, the proposed mixed-order SCL method can provide a free boost to lesion detection accuracy without extra special network designs.



### Partial Optimality in Cubic Correlation Clustering
- **Arxiv ID**: http://arxiv.org/abs/2302.04694v2
- **DOI**: None
- **Categories**: **cs.DM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04694v2)
- **Published**: 2023-02-09 15:25:52+00:00
- **Updated**: 2023-03-31 14:50:55+00:00
- **Authors**: David Stein, Silvia Di Gregorio, Bjoern Andres
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: The higher-order correlation clustering problem is an expressive model, and recently, local search heuristics have been proposed for several applications. Certifying optimality, however, is NP-hard and practically hampered already by the complexity of the problem statement. Here, we focus on establishing partial optimality conditions for the special case of complete graphs and cubic objective functions. In addition, we define and implement algorithms for testing these conditions and examine their effect numerically, on two datasets.



### Constrained Empirical Risk Minimization: Theory and Practice
- **Arxiv ID**: http://arxiv.org/abs/2302.04729v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04729v1)
- **Published**: 2023-02-09 16:11:58+00:00
- **Updated**: 2023-02-09 16:11:58+00:00
- **Authors**: Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen
- **Comment**: 50 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are widely used for their ability to effectively approximate large classes of functions. This flexibility, however, makes the strict enforcement of constraints on DNNs an open problem. Here we present a framework that, under mild assumptions, allows the exact enforcement of constraints on parameterized sets of functions such as DNNs. Instead of imposing "soft'' constraints via additional terms in the loss, we restrict (a subset of) the DNN parameters to a submanifold on which the constraints are satisfied exactly throughout the entire training procedure. We focus on constraints that are outside the scope of equivariant networks used in Geometric Deep Learning. As a major example of the framework, we restrict filters of a Convolutional Neural Network (CNN) to be wavelets, and apply these wavelet networks to the task of contour prediction in the medical domain.



### 3D Human Pose and Shape Estimation via HybrIK-Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.04774v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04774v4)
- **Published**: 2023-02-09 17:08:43+00:00
- **Updated**: 2023-04-22 18:11:30+00:00
- **Authors**: Boris N. Oreshkin
- **Comment**: None
- **Journal**: None
- **Summary**: HybrIK relies on a combination of analytical inverse kinematics and deep learning to produce more accurate 3D pose estimation from 2D monocular images. HybrIK has three major components: (1) pretrained convolution backbone, (2) deconvolution to lift 3D pose from 2D convolution features, (3) analytical inverse kinematics pass correcting deep learning prediction using learned distribution of plausible twist and swing angles. In this paper we propose an enhancement of the 2D to 3D lifting module, replacing deconvolution with Transformer, resulting in accuracy and computational efficiency improvement relative to the original HybrIK method. We demonstrate our results on commonly used H36M, PW3D, COCO and HP3D datasets. Our code is publicly available https://github.com/boreshkinai/hybrik-transformer.



### Drawing Attention to Detail: Pose Alignment through Self-Attention for Fine-Grained Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.04800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04800v1)
- **Published**: 2023-02-09 17:47:47+00:00
- **Updated**: 2023-02-09 17:47:47+00:00
- **Authors**: Salwa Al Khatib, Mohamed El Amine Boudjoghra, Jameel Hassan
- **Comment**: Course Assignment
- **Journal**: None
- **Summary**: Intra-class variations in the open world lead to various challenges in classification tasks. To overcome these challenges, fine-grained classification was introduced, and many approaches were proposed. Some rely on locating and using distinguishable local parts within images to achieve invariance to viewpoint changes, intra-class differences, and local part deformations. Our approach, which is inspired by P2P-Net, offers an end-to-end trainable attention-based parts alignment module, where we replace the graph-matching component used in it with a self-attention mechanism. The attention module is able to learn the optimal arrangement of parts while attending to each other, before contributing to the global loss.



### To Perceive or Not to Perceive: Lightweight Stacked Hourglass Network
- **Arxiv ID**: http://arxiv.org/abs/2302.04815v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04815v1)
- **Published**: 2023-02-09 18:04:43+00:00
- **Updated**: 2023-02-09 18:04:43+00:00
- **Authors**: Jameel Hassan Abdul Samadh, Salwa K. Al Khatib
- **Comment**: Course project
- **Journal**: None
- **Summary**: Human pose estimation (HPE) is a classical task in computer vision that focuses on representing the orientation of a person by identifying the positions of their joints. We design a lighterversion of the stacked hourglass network with minimal loss in performance of the model. The lightweight 2-stacked hourglass has a reduced number of channels with depthwise separable convolutions, residual connections with concatenation, and residual connections between the necks of the hourglasses. The final model has a marginal drop in performance with 79% reduction in the number of parameters and a similar drop in MAdds



### High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model
- **Arxiv ID**: http://arxiv.org/abs/2302.04820v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.04820v2)
- **Published**: 2023-02-09 18:15:08+00:00
- **Updated**: 2023-03-27 09:21:29+00:00
- **Authors**: Stevo Racković, Cláudia Soares, Dušan Jakovetić, Zoranka Desnica
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to fit arbitrarily accurate blendshape rig models by solving the inverse rig problem in realistic human face animation. The method considers blendshape models with different levels of added corrections and solves the regularized least-squares problem using coordinate descent, i.e., iteratively estimating blendshape weights. Besides making the optimization easier to solve, this approach ensures that mutually exclusive controllers will not be activated simultaneously and improves the goodness of fit after each iteration. We show experimentally that the proposed method yields solutions with mesh error comparable to or lower than the state-of-the-art approaches while significantly reducing the cardinality of the weight vector (over 20 percent), hence giving a high-fidelity reconstruction of the reference expression that is easier to manipulate in the post-production manually. Python scripts for the algorithm will be publicly available upon acceptance of the paper.



### Lithium Metal Battery Quality Control via Transformer-CNN Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.04824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04824v1)
- **Published**: 2023-02-09 18:25:24+00:00
- **Updated**: 2023-02-09 18:25:24+00:00
- **Authors**: Jerome Quenum, David Perlmutter, Ying Huang, Iryna Zenyuk, Daniela Ushizima
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Lithium metal battery (LMB) has the potential to be the next-generation battery system because of their high theoretical energy density. However, defects known as dendrites are formed by heterogeneous lithium (Li) plating, which hinder the development and utilization of LMBs. Non-destructive techniques to observe the dendrite morphology often use computerized X-ray tomography (XCT) imaging to provide cross-sectional views. To retrieve three-dimensional structures inside a battery, image segmentation becomes essential to quantitatively analyze XCT images. This work proposes a new binary semantic segmentation approach using a transformer-based neural network (T-Net) model capable of segmenting out dendrites from XCT data. In addition, we compare the performance of the proposed T-Net with three other algorithms, such as U-Net, Y-Net, and E-Net, consisting of an Ensemble Network model for XCT analysis. Our results show the advantages of using T-Net in terms of object metrics, such as mean Intersection over Union (mIoU) and mean Dice Similarity Coefficient (mDSC) as well as qualitatively through several comparative visualizations.



### Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting
- **Arxiv ID**: http://arxiv.org/abs/2302.04832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04832v1)
- **Published**: 2023-02-09 18:39:28+00:00
- **Updated**: 2023-02-09 18:39:28+00:00
- **Authors**: Viraj Prabhu, David Acuna, Andrew Liao, Rafid Mahmood, Marc T. Law, Judy Hoffman, Sanja Fidler, James Lucas
- **Comment**: None
- **Journal**: None
- **Summary**: Sim2Real domain adaptation (DA) research focuses on the constrained setting of adapting from a labeled synthetic source domain to an unlabeled or sparsely labeled real target domain. However, for high-stakes applications (e.g. autonomous driving), it is common to have a modest amount of human-labeled real data in addition to plentiful auto-labeled source data (e.g. from a driving simulator). We study this setting of supervised sim2real DA applied to 2D object detection. We propose Domain Translation via Conditional Alignment and Reweighting (CARE) a novel algorithm that systematically exploits target labels to explicitly close the sim2real appearance and content gaps. We present an analytical justification of our algorithm and demonstrate strong gains over competing methods on standard benchmarks.



### Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2302.04841v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04841v2)
- **Published**: 2023-02-09 18:49:13+00:00
- **Updated**: 2023-06-27 09:30:19+00:00
- **Authors**: Anton Voronov, Mikhail Khoroshikh, Artem Babenko, Max Ryabinin
- **Comment**: Code: https://github.com/yandex-research/DVAR. 19 pages, 14 figures
- **Journal**: None
- **Summary**: Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on Stable Diffusion for a range of concepts and for three personalization methods demonstrate the competitive performance of our approach, making adaptation up to 8 times faster with no significant drops in quality.



### Robot Synesthesia: A Sound and Emotion Guided AI Painter
- **Arxiv ID**: http://arxiv.org/abs/2302.04850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04850v1)
- **Published**: 2023-02-09 18:53:44+00:00
- **Updated**: 2023-02-09 18:53:44+00:00
- **Authors**: Vihaan Misra, Peter Schaldenbrand, Jean Oh
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: If a picture paints a thousand words, sound may voice a million. While recent robotic painting and image synthesis methods have achieved progress in generating visuals from text inputs, the translation of sound into images is vastly unexplored. Generally, sound-based interfaces and sonic interactions have the potential to expand accessibility and control for the user and provide a means to convey complex emotions and the dynamic aspects of the real world. In this paper, we propose an approach for using sound and speech to guide a robotic painting process, known here as robot synesthesia. For general sound, we encode the simulated paintings and input sounds into the same latent space. For speech, we decouple speech into its transcribed text and the tone of the speech. Whereas we use the text to control the content, we estimate the emotions from the tone to guide the mood of the painting. Our approach has been fully integrated with FRIDA, a robotic painting framework, adding sound and speech to FRIDA's existing input modalities, such as text and style. In two surveys, participants were able to correctly guess the emotion or natural sound used to generate a given painting more than twice as likely as random chance. On our sound-guided image manipulation and music-guided paintings, we discuss the results qualitatively.



### Trading Information between Latents in Hierarchical Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2302.04855v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2302.04855v1)
- **Published**: 2023-02-09 18:56:11+00:00
- **Updated**: 2023-02-09 18:56:11+00:00
- **Authors**: Tim Z. Xiao, Robert Bamler
- **Comment**: Accepted for ICLR 2023; 9 pages + appendix
- **Journal**: International Conference on Learning Representations (ICLR), 2023
- **Summary**: Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling, 2014) as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content ("bit rate") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.



### One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.04856v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04856v1)
- **Published**: 2023-02-09 18:56:37+00:00
- **Updated**: 2023-02-09 18:56:37+00:00
- **Authors**: Matthew Chang, Saurabh Gupta
- **Comment**: Project Page: https://matthewchang.github.io/awda_site
- **Journal**: None
- **Summary**: In this paper, we analyze the behavior of existing techniques and design new solutions for the problem of one-shot visual imitation. In this setting, an agent must solve a novel instance of a novel task given just a single visual demonstration. Our analysis reveals that current methods fall short because of three errors: the DAgger problem arising from purely offline training, last centimeter errors in interacting with objects, and mis-fitting to the task context rather than to the actual task. This motivates the design of our modular approach where we a) separate out task inference (what to do) from task execution (how to do it), and b) develop data augmentation and generation techniques to mitigate mis-fitting. The former allows us to leverage hand-crafted motor primitives for task execution which side-steps the DAgger problem and last centimeter errors, while the latter gets the model to focus on the task rather than the task context. Our model gets 100% and 48% success rates on two recent benchmarks, improving upon the current state-of-the-art by absolute 90% and 20% respectively.



### Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2302.04858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04858v1)
- **Published**: 2023-02-09 18:57:56+00:00
- **Updated**: 2023-02-09 18:57:56+00:00
- **Authors**: Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Ming-Yu Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities. We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4 times less parameters compared with baseline methods.



### Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors
- **Arxiv ID**: http://arxiv.org/abs/2302.04860v1
- **DOI**: 10.1007/978-3-031-20047-2_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04860v1)
- **Published**: 2023-02-09 18:58:07+00:00
- **Updated**: 2023-02-09 18:58:07+00:00
- **Authors**: Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui
- **Comment**: ECCV 2022 (Oral); Project Page: https://sirui-xu.github.io/STARS/
- **Journal**: None
- **Summary**: Predicting diverse human motions given a sequence of historical poses has received increasing attention. Despite rapid progress, existing work captures the multi-modal nature of human motions primarily through likelihood-based sampling, where the mode collapse has been widely observed. In this paper, we propose a simple yet effective approach that disentangles randomly sampled codes with a deterministic learnable component named anchors to promote sample precision and diversity. Anchors are further factorized into spatial anchors and temporal anchors, which provide attractively interpretable control over spatial-temporal disparity. In principle, our spatial-temporal anchor-based sampling (STARS) can be applied to different motion predictors. Here we propose an interaction-enhanced spatial-temporal graph convolutional network (IE-STGCN) that encodes prior knowledge of human motions (e.g., spatial locality), and incorporate the anchors into it. Extensive experiments demonstrate that our approach outperforms state of the art in both stochastic and deterministic prediction, suggesting it as a unified framework for modeling human motions. Our code and pretrained models are available at https://github.com/Sirui-Xu/STARS.



### Polynomial Neural Fields for Subband Decomposition and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2302.04862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04862v1)
- **Published**: 2023-02-09 18:59:04+00:00
- **Updated**: 2023-02-09 18:59:04+00:00
- **Authors**: Guandao Yang, Sagie Benaim, Varun Jampani, Kyle Genova, Jonathan T. Barron, Thomas Funkhouser, Bharath Hariharan, Serge Belongie
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Neural fields have emerged as a new paradigm for representing signals, thanks to their ability to do it compactly while being easy to optimize. In most applications, however, neural fields are treated like black boxes, which precludes many signal manipulation tasks. In this paper, we propose a new class of neural fields called polynomial neural fields (PNFs). The key advantage of a PNF is that it can represent a signal as a composition of a number of manipulable and interpretable components without losing the merits of neural fields representation. We develop a general theoretical framework to analyze and design PNFs. We use this framework to design Fourier PNFs, which match state-of-the-art performance in signal representation tasks that use neural fields. In addition, we empirically demonstrate that Fourier PNFs enable signal manipulation applications such as texture transfer and scale-space interpolation. Code is available at https://github.com/stevenygd/PNF.



### Learning by Asking for Embodied Visual Navigation and Task Completion
- **Arxiv ID**: http://arxiv.org/abs/2302.04865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04865v1)
- **Published**: 2023-02-09 18:59:41+00:00
- **Updated**: 2023-02-09 18:59:41+00:00
- **Authors**: Ying Shen, Ismini Lourentzou
- **Comment**: None
- **Journal**: None
- **Summary**: The research community has shown increasing interest in designing intelligent embodied agents that can assist humans in accomplishing tasks. Despite recent progress on related vision-language benchmarks, most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments. To empower embodied agents with the ability to interact with humans, in this work, we propose an Embodied Learning-By-Asking (ELBA) model that learns when and what questions to ask to dynamically acquire additional information for completing the task. We evaluate our model on the TEACH vision-dialog navigation and task completion dataset. Experimental results show that ELBA achieves improved task performance compared to baseline models without question-answering capabilities.



### RelightableHands: Efficient Neural Relighting of Articulated Hand Models
- **Arxiv ID**: http://arxiv.org/abs/2302.04866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.04866v1)
- **Published**: 2023-02-09 18:59:48+00:00
- **Updated**: 2023-02-09 18:59:48+00:00
- **Authors**: Shun Iwase, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Timur Bagautdinov, Rohan Joshi, Fabian Prada, Takaaki Shiratori, Yaser Sheikh, Jason Saragih
- **Comment**: 8 pages, 16 figures, Website: https://sh8.io/#/relightable_hands
- **Journal**: None
- **Summary**: We present the first neural relighting approach for rendering high-fidelity personalized hands that can be animated in real-time under novel illumination. Our approach adopts a teacher-student framework, where the teacher learns appearance under a single point light from images captured in a light-stage, allowing us to synthesize hands in arbitrary illuminations but with heavy compute. Using images rendered by the teacher model as training data, an efficient student model directly predicts appearance under natural illuminations in real-time. To achieve generalization, we condition the student model with physics-inspired illumination features such as visibility, diffuse shading, and specular reflections computed on a coarse proxy geometry, maintaining a small computational overhead. Our key insight is that these features have strong correlation with subsequent global light transport effects, which proves sufficient as conditioning data for the neural relighting network. Moreover, in contrast to bottleneck illumination conditioning, these features are spatially aligned based on underlying geometry, leading to better generalization to unseen illuminations and poses. In our experiments, we demonstrate the efficacy of our illumination feature representations, outperforming baseline approaches. We also show that our approach can photorealistically relight two interacting hands at real-time speeds. https://sh8.io/#/relightable_hands



### UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.04867v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.04867v2)
- **Published**: 2023-02-09 18:59:48+00:00
- **Updated**: 2023-02-12 16:51:30+00:00
- **Authors**: Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu
- **Comment**: Project page: https://unipc.ivg-research.xyz
- **Journal**: None
- **Summary**: Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM usually requires hundreds of model evaluations, which is computationally expensive. Despite recent progress in designing high-order solvers for DPMs, there still exists room for further speedup, especially in extremely few steps (e.g., 5~10 steps). Inspired by the predictor-corrector for ODE solvers, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods. We evaluate our methods through extensive experiments including both unconditional and conditional sampling using pixel-space and latent-space DPMs. Our UniPC can achieve 3.87 FID on CIFAR10 (unconditional) and 7.51 FID on ImageNet 256$\times$256 (conditional) with only 10 function evaluations. Code is available at https://github.com/wl-zhao/UniPC



### MEGANE: Morphable Eyeglass and Avatar Network
- **Arxiv ID**: http://arxiv.org/abs/2302.04868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.04868v1)
- **Published**: 2023-02-09 18:59:49+00:00
- **Updated**: 2023-02-09 18:59:49+00:00
- **Authors**: Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Hongdong Li, Jason Saragih
- **Comment**: Project page: https://junxuan-li.github.io/megane/
- **Journal**: None
- **Summary**: Eyeglasses play an important role in the perception of identity. Authentic virtual representations of faces can benefit greatly from their inclusion. However, modeling the geometric and appearance interactions of glasses and the face of virtual representations of humans is challenging. Glasses and faces affect each other's geometry at their contact points, and also induce appearance changes due to light transport. Most existing approaches do not capture these physical interactions since they model eyeglasses and faces independently. Others attempt to resolve interactions as a 2D image synthesis problem and suffer from view and temporal inconsistencies. In this work, we propose a 3D compositional morphable model of eyeglasses that accurately incorporates high-fidelity geometric and photometric interaction effects. To support the large variation in eyeglass topology efficiently, we employ a hybrid representation that combines surface geometry and a volumetric representation. Unlike volumetric approaches, our model naturally retains correspondences across glasses, and hence explicit modification of geometry, such as lens insertion and frame deformation, is greatly simplified. In addition, our model is relightable under point lights and natural illumination, supporting high-fidelity rendering of various frame materials, including translucent plastic and metal within a single morphable model. Importantly, our approach models global light transport effects, such as casting shadows between faces and glasses. Our morphable model for eyeglasses can also be fit to novel glasses via inverse rendering. We compare our approach to state-of-the-art methods and demonstrate significant quality improvements.



### Reversible Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.04869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.04869v1)
- **Published**: 2023-02-09 18:59:54+00:00
- **Updated**: 2023-02-09 18:59:54+00:00
- **Authors**: Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, Jitendra Malik
- **Comment**: Oral at CVPR 2022, updated version
- **Journal**: None
- **Summary**: We present Reversible Vision Transformers, a memory efficient architecture design for visual recognition. By decoupling the GPU memory requirement from the depth of the model, Reversible Vision Transformers enable scaling up architectures with efficient memory usage. We adapt two popular models, namely Vision Transformer and Multiscale Vision Transformers, to reversible variants and benchmark extensively across both model sizes and tasks of image classification, object detection and video classification. Reversible Vision Transformers achieve a reduced memory footprint of up to 15.5x at roughly identical model complexity, parameters and accuracy, demonstrating the promise of reversible vision transformers as an efficient backbone for hardware resource limited training regimes. Finally, we find that the additional computational burden of recomputing activations is more than overcome for deeper models, where throughput can increase up to 2.3x over their non-reversible counterparts. Full code and trained models are available at https://github.com/facebookresearch/slowfast. A simpler, easy to understand and modify version is also available at https://github.com/karttikeya/minREV



### Offsite-Tuning: Transfer Learning without Full Model
- **Arxiv ID**: http://arxiv.org/abs/2302.04870v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04870v1)
- **Published**: 2023-02-09 18:59:55+00:00
- **Updated**: 2023-02-09 18:59:55+00:00
- **Authors**: Guangxuan Xiao, Ji Lin, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.



### In-N-Out: Face Video Inversion and Editing with Volumetric Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2302.04871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04871v2)
- **Published**: 2023-02-09 18:59:56+00:00
- **Updated**: 2023-04-04 20:04:44+00:00
- **Authors**: Yiran Xu, Zhixin Shu, Cameron Smith, Jia-Bin Huang, Seoung Wug Oh
- **Comment**: Project page: https://in-n-out-3d.github.io/
- **Journal**: None
- **Summary**: 3D-aware GANs offer new capabilities for creative content editing, such as view synthesis, while preserving the editing capability of their 2D counterparts. These methods use GAN inversion to reconstruct images or videos by optimizing a latent code, allowing for semantic editing by manipulating the code. However, a model pre-trained on a face dataset (e.g., FFHQ) often has difficulty handling faces with out-of-distribution (OOD) objects, e.g., heavy make-up or occlusions. We address this issue by explicitly modeling OOD objects in face videos. Our core idea is to represent the face in a video using two neural radiance fields, one for the in-distribution and the other for the out-of-distribution object, and compose them together for reconstruction. Such explicit decomposition alleviates the inherent trade-off between reconstruction fidelity and editability. We evaluate our method's reconstruction accuracy and editability on challenging real videos and showcase favorable results against other baselines.



### Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data
- **Arxiv ID**: http://arxiv.org/abs/2302.04936v1
- **DOI**: 10.1016/j.gsf.2023.101562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.04936v1)
- **Published**: 2023-02-09 21:03:03+00:00
- **Updated**: 2023-02-09 21:03:03+00:00
- **Authors**: Lloyd Windrim, Arman Melkumyan, Richard J. Murphy, Anna Chlingaryan, Raymond Leung
- **Comment**: Manuscript has been accepted for publication in Geoscience Frontiers.
  Keywords: Hyperspectral imaging, remote sensing, mineral mapping, machine
  learning, convolutional neural networks, transfer learning, data
  augmentation, illumination invariance
- **Journal**: Geoscience Frontiers 14 (2023) 101562
- **Summary**: The remote mapping of minerals and discrimination of ore and waste on surfaces are important tasks for geological applications such as those in mining. Such tasks have become possible using ground-based, close-range hyperspectral sensors which can remotely measure the reflectance properties of the environment with high spatial and spectral resolution. However, autonomous mapping of mineral spectra measured on an open-cut mine face remains a challenging problem due to the subtleness of differences in spectral absorption features between mineral and rock classes as well as variability in the illumination of the scene. An additional layer of difficulty arises when there is no annotated data available to train a supervised learning algorithm. A pipeline for unsupervised mapping of spectra on a mine face is proposed which draws from several recent advances in the hyperspectral machine learning literature. The proposed pipeline brings together unsupervised and self-supervised algorithms in a unified system to map minerals on a mine face without the need for human-annotated training data. The pipeline is evaluated with a hyperspectral image dataset of an open-cut mine face comprising mineral ore martite and non-mineralised shale. The combined system is shown to produce a superior map to its constituent algorithms, and the consistency of its mapping capability is demonstrated using data acquired at two different times of day.



### Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames
- **Arxiv ID**: http://arxiv.org/abs/2302.04973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04973v2)
- **Published**: 2023-02-09 23:25:28+00:00
- **Updated**: 2023-07-21 01:40:31+00:00
- **Authors**: Ondrej Biza, Sjoerd van Steenkiste, Mehdi S. M. Sajjadi, Gamaleldin F. Elsayed, Aravindh Mahendran, Thomas Kipf
- **Comment**: Accepted at ICML 2023. Project page: https://invariantsa.github.io/
- **Journal**: None
- **Summary**: Automatically discovering composable abstractions from raw perceptual data is a long-standing challenge in machine learning. Recent slot-based neural networks that learn about objects in a self-supervised manner have made exciting progress in this direction. However, they typically fall short at adequately capturing spatial symmetries present in the visual world, which leads to sample inefficiency, such as when entangling object appearance and pose. In this paper, we present a simple yet highly effective method for incorporating spatial symmetries via slot-centric reference frames. We incorporate equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings. These changes result in little computational overhead, are easy to implement, and can result in large gains in terms of data efficiency and overall improvements to object discovery. We evaluate our method on a wide range of synthetic object discovery benchmarks namely CLEVR, Tetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising improvements on the challenging real-world Waymo Open dataset.



### Mithridates: Boosting Natural Resistance to Backdoor Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.04977v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.04977v2)
- **Published**: 2023-02-09 23:34:17+00:00
- **Updated**: 2023-05-23 20:44:58+00:00
- **Authors**: Eugene Bagdasaryan, Vitaly Shmatikov
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a "backdoor" task (e.g., misclassify inputs with a certain feature) in addition to its main task. While backdoor attacks remain largely a hypothetical threat, state-of-the-art defenses require massive changes to the existing ML pipelines and are too complex for practical deployment.   In this paper, we take a pragmatic view and investigate natural resistance of ML pipelines to backdoor attacks, i.e., resistance that can be achieved without changes to how models are trained. We design, implement, and evaluate Mithridates, a new method that helps practitioners answer two actionable questions: (1) how well does my model resist backdoor poisoning attacks?, and (2) how can I increase its resistance without changing the training pipeline? Mithridates leverages hyperparameter search $\unicode{x2013}$ a tool that ML developers already extensively use $\unicode{x2013}$ to balance the model's accuracy and resistance to backdoor learning, without disruptive changes to the pipeline.   We show that hyperparameters found by Mithridates increase resistance to multiple types of backdoor attacks by 3-5x with only a slight impact on model accuracy. We also discuss extensions to AutoML and federated learning.



