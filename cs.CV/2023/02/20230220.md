# Arxiv Papers in cs.CV on 2023-02-20
### Seeing the Fruit for the Leaves: Towards Automated Apple Fruitlet Thinning
- **Arxiv ID**: http://arxiv.org/abs/2302.09716v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09716v1)
- **Published**: 2023-02-20 01:55:00+00:00
- **Updated**: 2023-02-20 01:55:00+00:00
- **Authors**: Ans Qureshi, Neville Loh, Young Min Kwon, David Smith, Trevor Gee, Oliver Bachelor, Josh McCulloch, Mahla Nejati, JongYoon Lim, Richard Green, Ho Seok Ahn, Bruce MacDonald, Henry Williams
- **Comment**: Accepted and Presented at the Australasian Conference on Robotics and
  Automation (ACRA 2022)
- **Journal**: None
- **Summary**: Following a global trend, the lack of reliable access to skilled labour is causing critical issues for the effective management of apple orchards. One of the primary challenges is maintaining skilled human operators capable of making precise fruitlet thinning decisions. Thinning requires accurately measuring the true crop load for individual apple trees to provide optimal thinning decisions on an individual basis. A challenging task due to the dense foliage obscuring the fruitlets within the tree structure. This paper presents the initial design, implementation, and evaluation details of the vision system for an automatic apple fruitlet thinning robot to meet this need. The platform consists of a UR5 robotic arm and stereo cameras which enable it to look around the leaves to map the precise number and size of the fruitlets on the apple branches. We show that this platform can measure the fruitlet load on the apple tree to with 84% accuracy in a real-world commercial apple orchard while being 87% precise.



### STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2302.09736v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2302.09736v2)
- **Published**: 2023-02-20 03:13:45+00:00
- **Updated**: 2023-05-24 01:03:09+00:00
- **Authors**: Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, Bing Qin
- **Comment**: AAAI 2023, 7 pages, 3 figures
- **Journal**: None
- **Summary**: Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predicting actions found in the text. Extensive experiments on three downstream tasks (video captioning, text-video retrieval, and video question answering) demonstrate the effectiveness of our proposed STOA-VLP (e.g. 3.7 Rouge-L improvements on MSR-VTT video captioning benchmark, 2.9% accuracy improvements on MSVD video question answering benchmark, compared to previous approaches).



### Metropolis Theorem and Its Applications in Single Image Detail Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2302.09762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09762v1)
- **Published**: 2023-02-20 05:00:20+00:00
- **Updated**: 2023-02-20 05:00:20+00:00
- **Authors**: He Jiang, Mujtaba Asad, Jingjing Liu, Haoxiang Zhang, Deqiang Cheng
- **Comment**: 23 pages, 17 figures
- **Journal**: None
- **Summary**: Traditional image detail enhancement is local filter-based or global filter-based. In both approaches, the original image is first divided into the base layer and the detail layer, and then the enhanced image is obtained by amplifying the detail layer. Our method is different, and its innovation lies in the special way to get the image detail layer. The detail layer in our method is obtained by updating the residual features, and the updating mechanism is usually based on searching and matching similar patches. However, due to the diversity of image texture features, perfect matching is often not possible. In this paper, the process of searching and matching is treated as a thermodynamic process, where the Metropolis theorem can minimize the internal energy and get the global optimal solution of this task, that is, to find a more suitable feature for a better detail enhancement performance. Extensive experiments have proven that our algorithm can achieve better results in quantitative metrics testing and visual effects evaluation. The source code can be obtained from the link.



### ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.09765v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09765v3)
- **Published**: 2023-02-20 05:15:23+00:00
- **Updated**: 2023-07-31 03:05:22+00:00
- **Authors**: Moon Ye-Bin, Dongmin Choi, Yongjin Kwon, Junsik Kim, Tae-Hyun Oh
- **Comment**: Accepted at Pattern Recognition (PR)
- **Journal**: None
- **Summary**: We address a weakly-supervised low-shot instance segmentation, an annotation-efficient training method to deal with novel classes effectively. Since it is an under-explored problem, we first investigate the difficulty of the problem and identify the performance bottleneck by conducting systematic analyses of model components and individual sub-tasks with a simple baseline model. Based on the analyses, we propose ENInst with sub-task enhancement methods: instance-wise mask refinement for enhancing pixel localization quality and novel classifier composition for improving classification accuracy. Our proposed method lifts the overall performance by enhancing the performance of each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in achieving comparable performance to the existing fully-supervised few-shot models and even outperforms them at times.



### Composer: Creative and Controllable Image Synthesis with Composable Conditions
- **Arxiv ID**: http://arxiv.org/abs/2302.09778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2302.09778v2)
- **Published**: 2023-02-20 05:48:41+00:00
- **Updated**: 2023-02-22 02:14:55+00:00
- **Authors**: Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, Jingren Zhou
- **Comment**: Project page: https://damo-vilab.github.io/composer-page/
- **Journal**: None
- **Summary**: Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.



### Incremental Few-Shot Object Detection via Simple Fine-Tuning Approach
- **Arxiv ID**: http://arxiv.org/abs/2302.09779v1
- **DOI**: 10.1109/ICRA48891.2023.10160283
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2302.09779v1)
- **Published**: 2023-02-20 05:48:46+00:00
- **Updated**: 2023-02-20 05:48:46+00:00
- **Authors**: Tae-Min Choi, Jong-Hwan Kim
- **Comment**: Accepted to ICRA 2023
- **Journal**: 2023 IEEE International Conference on Robotics and Automation
  (ICRA), 9289-9295
- **Summary**: In this paper, we explore incremental few-shot object detection (iFSD), which incrementally learns novel classes using only a few examples without revisiting base classes. Previous iFSD works achieved the desired results by applying meta-learning. However, meta-learning approaches show insufficient performance that is difficult to apply to practical problems. In this light, we propose a simple fine-tuning-based approach, the Incremental Two-stage Fine-tuning Approach (iTFA) for iFSD, which contains three steps: 1) base training using abundant base classes with the class-agnostic box regressor, 2) separation of the RoI feature extractor and classifier into the base and novel class branches for preserving base knowledge, and 3) fine-tuning the novel branch using only a few novel class examples. We evaluate our iTFA on the real-world datasets PASCAL VOC, COCO, and LVIS. iTFA achieves competitive performance in COCO and shows a 30% higher AP accuracy than meta-learning methods in the LVIS dataset. Experimental results show the effectiveness and applicability of our proposed method.



### Towards Simultaneous Segmentation of Liver Tumors and Intrahepatic Vessels via Cross-attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2302.09785v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09785v1)
- **Published**: 2023-02-20 06:17:03+00:00
- **Updated**: 2023-02-20 06:17:03+00:00
- **Authors**: Haopeng Kuang, Dingkang Yang, Shunli Wang, Xiaoying Wang, Lihua Zhang
- **Comment**: accepted to ICASSP 2023
- **Journal**: None
- **Summary**: Accurate visualization of liver tumors and their surrounding blood vessels is essential for noninvasive diagnosis and prognosis prediction of tumors. In medical image segmentation, there is still a lack of in-depth research on the simultaneous segmentation of liver tumors and peritumoral blood vessels. To this end, we collect the first liver tumor, and vessel segmentation benchmark datasets containing 52 portal vein phase computed tomography images with liver, liver tumor, and vessel annotations. In this case, we propose a 3D U-shaped Cross-Attention Network (UCA-Net) that utilizes a tailored cross-attention mechanism instead of the traditional skip connection to effectively model the encoder and decoder feature. Specifically, the UCA-Net uses a channel-wise cross-attention module to reduce the semantic gap between encoder and decoder and a slice-wise cross-attention module to enhance the contextual semantic learning ability among distinct slices. Experimental results show that the proposed UCA-Net can accurately segment 3D medical images and achieve state-of-the-art performance on the liver tumor and intrahepatic vessel segmentation task.



### Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement
- **Arxiv ID**: http://arxiv.org/abs/2302.09789v2
- **DOI**: 10.1109/TCSVT.2023.3275584
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09789v2)
- **Published**: 2023-02-20 06:28:52+00:00
- **Updated**: 2023-06-15 08:35:44+00:00
- **Authors**: Zhong Liu, Ran Li, Shuwei Shao, Xingming Wu, Weihai Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation plays a fundamental role in computer vision. Due to the costly acquisition of depth ground truth, self-supervised methods that leverage adjacent frames to establish a supervisory signal have emerged as the most promising paradigms. In this work, we propose two novel ideas to improve self-supervised monocular depth estimation: 1) self-reference distillation and 2) disparity offset refinement. Specifically, we use a parameter-optimized model as the teacher updated as the training epochs to provide additional supervision during the training process. The teacher model has the same structure as the student model, with weights inherited from the historical student model. In addition, a multiview check is introduced to filter out the outliers produced by the teacher model. Furthermore, we leverage the contextual consistency between high-scale and low-scale features to obtain multiscale disparity offsets, which are used to refine the disparity output incrementally by aligning disparity information at different scales. The experimental results on the KITTI and Make3D datasets show that our method outperforms previous state-of-the-art competitors.



### HTNet: Human Topology Aware Network for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.09790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09790v1)
- **Published**: 2023-02-20 06:31:29+00:00
- **Updated**: 2023-02-20 06:31:29+00:00
- **Authors**: Jialun Cai, Hong Liu, Runwei Ding, Wenhao Li, Jianbing Wu, Miaoju Ban
- **Comment**: ICASSP23 Accepted Paper
- **Journal**: None
- **Summary**: 3D human pose estimation errors would propagate along the human body topology and accumulate at the end joints of limbs. Inspired by the backtracking mechanism in automatic control systems, we design an Intra-Part Constraint module that utilizes the parent nodes as the reference to build topological constraints for end joints at the part level. Further considering the hierarchy of the human topology, joint-level and body-level dependencies are captured via graph convolutional networks and self-attentions, respectively. Based on these designs, we propose a novel Human Topology aware Network (HTNet), which adopts a channel-split progressive strategy to sequentially learn the structural priors of the human topology from multiple semantic levels: joint, part, and body. Extensive experiments show that the proposed method improves the estimation accuracy by 18.7% on the end joints of limbs and achieves state-of-the-art results on Human3.6M and MPI-INF-3DHP datasets. Code is available at https://github.com/vefalun/HTNet.



### Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.09794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09794v1)
- **Published**: 2023-02-20 06:46:09+00:00
- **Updated**: 2023-02-20 06:46:09+00:00
- **Authors**: Chaewon Park, Minhyeok Lee, Suhwan Cho, Donghyeong Kim, Sangyoun Lee
- **Comment**: Accepted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2023
- **Journal**: None
- **Summary**: Image reconstruction-based anomaly detection has recently been in the spotlight because of the difficulty of constructing anomaly datasets. These approaches work by learning to model normal features without seeing abnormal samples during training and then discriminating anomalies at test time based on the reconstructive errors. However, these models have limitations in reconstructing the abnormal samples due to their indiscriminate conveyance of features. Moreover, these approaches are not explicitly optimized for distinguishable anomalies. To address these problems, we propose a two-stream decoder network (TSDN), designed to learn both normal and abnormal features. Additionally, we propose a feature normality estimator (FNE) to eliminate abnormal features and prevent high-quality reconstruction of abnormal regions. Evaluation on a standard benchmark demonstrated performance better than state-of-the-art models.



### Simple Disentanglement of Style and Content in Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2302.09795v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.09795v2)
- **Published**: 2023-02-20 06:48:19+00:00
- **Updated**: 2023-05-31 17:25:09+00:00
- **Authors**: Lilian Ngweta, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin
- **Comment**: International Conference on Machine Learning (ICML) 2023
- **Journal**: None
- **Summary**: Learning visual representations with interpretable features, i.e., disentangled representations, remains a challenging problem. Existing methods demonstrate some success but are hard to apply to large-scale vision datasets like ImageNet. In this work, we propose a simple post-processing framework to disentangle content and style in learned representations from pre-trained vision models. We model the pre-trained features probabilistically as linearly entangled combinations of the latent content and style factors and develop a simple disentanglement algorithm based on the probabilistic model. We show that the method provably disentangles content and style features and verify its efficacy empirically. Our post-processed features yield significant domain generalization performance improvements when the distribution shift occurs due to style changes or style-related spurious correlations.



### A Novel Collaborative Self-Supervised Learning Method for Radiomic Data
- **Arxiv ID**: http://arxiv.org/abs/2302.09807v1
- **DOI**: 10.1016/j.neuroimage.2023.120229
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2302.09807v1)
- **Published**: 2023-02-20 07:15:28+00:00
- **Updated**: 2023-02-20 07:15:28+00:00
- **Authors**: Zhiyuan Li, Hailong Li, Anca L. Ralescu, Jonathan R. Dillman, Nehal A. Parikh, Lili He
- **Comment**: 14 pages, 7 figures
- **Journal**: Neuroimage. 2023;120229
- **Summary**: The computer-aided disease diagnosis from radiomic data is important in many medical applications. However, developing such a technique relies on annotating radiological images, which is a time-consuming, labor-intensive, and expensive process. In this work, we present the first novel collaborative self-supervised learning method to solve the challenge of insufficient labeled radiomic data, whose characteristics are different from text and image data. To achieve this, we present two collaborative pretext tasks that explore the latent pathological or biological relationships between regions of interest and the similarity and dissimilarity information between subjects. Our method collaboratively learns the robust latent feature representations from radiomic data in a self-supervised manner to reduce human annotation efforts, which benefits the disease diagnosis. We compared our proposed method with other state-of-the-art self-supervised learning methods on a simulation study and two independent datasets. Extensive experimental results demonstrated that our method outperforms other self-supervised learning methods on both classification and regression tasks. With further refinement, our method shows the potential advantage in automatic disease diagnosis with large-scale unlabeled data available.



### Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2302.09814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09814v1)
- **Published**: 2023-02-20 07:29:34+00:00
- **Updated**: 2023-02-20 07:29:34+00:00
- **Authors**: Xiaojian Yuan, Kejiang Chen, Jie Zhang, Weiming Zhang, Nenghai Yu, Yang Zhang
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Model inversion (MI) attacks have raised increasing concerns about privacy, which can reconstruct training data from public models. Indeed, MI attacks can be formalized as an optimization problem that seeks private data in a certain space. Recent MI attacks leverage a generative adversarial network (GAN) as an image prior to narrow the search space, and can successfully reconstruct even the high-dimensional data (e.g., face images). However, these generative MI attacks do not fully exploit the potential capabilities of the target model, still leading to a vague and coupled search space, i.e., different classes of images are coupled in the search space. Besides, the widely used cross-entropy loss in these attacks suffers from gradient vanishing. To address these problems, we propose Pseudo Label-Guided MI (PLG-MI) attack via conditional GAN (cGAN). At first, a top-n selection strategy is proposed to provide pseudo-labels for public data, and use pseudo-labels to guide the training of the cGAN. In this way, the search space is decoupled for different classes of images. Then a max-margin loss is introduced to improve the search process on the subspace of a target class. Extensive experiments demonstrate that our PLG-MI attack significantly improves the attack success rate and visual quality for various datasets and models, notably, 2~3 $\times$ better than state-of-the-art attacks under large distributional shifts. Our code is available at: https://github.com/LetheSec/PLG-MI-Attack.



### Personalized and privacy-preserving federated heterogeneous medical image analysis with PPPML-HMI
- **Arxiv ID**: http://arxiv.org/abs/2302.11571v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.11571v1)
- **Published**: 2023-02-20 07:37:03+00:00
- **Updated**: 2023-02-20 07:37:03+00:00
- **Authors**: Juexiao Zhou, Longxi Zhou, Di Wang, Xiaopeng Xu, Haoyang Li, Yuetan Chu, Wenkai Han, Xin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Heterogeneous data is endemic due to the use of diverse models and settings of devices by hospitals in the field of medical imaging. However, there are few open-source frameworks for federated heterogeneous medical image analysis with personalization and privacy protection simultaneously without the demand to modify the existing model structures or to share any private data. In this paper, we proposed PPPML-HMI, an open-source learning paradigm for personalized and privacy-preserving federated heterogeneous medical image analysis. To our best knowledge, personalization and privacy protection were achieved simultaneously for the first time under the federated scenario by integrating the PerFedAvg algorithm and designing our novel cyclic secure aggregation with the homomorphic encryption algorithm. To show the utility of PPPML-HMI, we applied it to a simulated classification task namely the classification of healthy people and patients from the RAD-ChestCT Dataset, and one real-world segmentation task namely the segmentation of lung infections from COVID-19 CT scans. For the real-world task, PPPML-HMI achieved $\sim$5\% higher Dice score on average compared to conventional FL under the heterogeneous scenario. Meanwhile, we applied the improved deep leakage from gradients to simulate adversarial attacks and showed the solid privacy-preserving capability of PPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks, a varied number of users, and sample sizes, we further demonstrated the strong robustness of PPPML-HMI.



### Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2302.09817v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09817v2)
- **Published**: 2023-02-20 07:45:25+00:00
- **Updated**: 2023-02-23 15:46:35+00:00
- **Authors**: Surbhi Madan, Monika Gahalawat, Tanaya Guha, Roland Goecke, Ramanathan Subramanian
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the efficacy of multimodal behavioral cues for explainable prediction of personality and interview-specific traits. We utilize elementary head-motion units named kinemes, atomic facial movements termed action units and speech features to estimate these human-centered traits. Empirical results confirm that kinemes and action units enable discovery of multiple trait-specific behaviors while also enabling explainability in support of the predictions. For fusing cues, we explore decision and feature-level fusion, and an additive attention-based fusion strategy which quantifies the relative importance of the three modalities for trait prediction. Examining various long-short term memory (LSTM) architectures for classification and regression on the MIT Interview and First Impressions Candidate Screening (FICS) datasets, we note that: (1) Multimodal approaches outperform unimodal counterparts; (2) Efficient trait predictions and plausible explanations are achieved with both unimodal and multimodal approaches, and (3) Following the thin-slice approach, effective trait prediction is achieved even from two-second behavioral snippets.



### TBPos: Dataset for Large-Scale Precision Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2302.09825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09825v1)
- **Published**: 2023-02-20 08:14:13+00:00
- **Updated**: 2023-02-20 08:14:13+00:00
- **Authors**: Masud Fahim, Ilona Söchting, Luca Ferranti, Juho Kannala, Jani Boutellier
- **Comment**: Scandinavian Conference on Image Analysis 2023
- **Journal**: None
- **Summary**: Image based localization is a classical computer vision challenge, with several well-known datasets. Generally, datasets consist of a visual 3D database that captures the modeled scenery, as well as query images whose 3D pose is to be discovered. Usually the query images have been acquired with a camera that differs from the imaging hardware used to collect the 3D database; consequently, it is hard to acquire accurate ground truth poses between query images and the 3D database. As the accuracy of visual localization algorithms constantly improves, precise ground truth becomes increasingly important. This paper proposes TBPos, a novel large-scale visual dataset for image based positioning, which provides query images with fully accurate ground truth poses: both the database images and the query images have been derived from the same laser scanner data. In the experimental part of the paper, the proposed dataset is evaluated by means of an image-based localization pipeline.



### Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.09833v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09833v2)
- **Published**: 2023-02-20 08:42:06+00:00
- **Updated**: 2023-05-03 20:03:53+00:00
- **Authors**: Soham Rohit Chitnis, Sidong Liu, Tirtharaj Dash, Tanmay Tulsidas Verlekar, Antonio Di Ieva, Shlomo Berkovsky, Lovekesh Vig, Ashwin Srinivasan
- **Comment**: Accepted in EMBC 2023
- **Journal**: None
- **Summary**: Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the models' confidence and predictive performance in detecting primary brain tumors - gliomas. Domain-specific pre-training improves the confidence of the models and also achieves a new state-of-the-art performance of WSI-based glioma subtype classification, showing a high clinical applicability in assisting glioma diagnosis. We will publicly share our code and experimental results at https://github.com/soham-chitnis10/WSI-domain-specific.



### Simple U-net Based Synthetic Polyp Image Generation: Polyp to Negative and Negative to Polyp
- **Arxiv ID**: http://arxiv.org/abs/2302.09835v1
- **DOI**: 10.1016/j.bspc.2022.103491
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09835v1)
- **Published**: 2023-02-20 08:47:44+00:00
- **Updated**: 2023-02-20 08:47:44+00:00
- **Authors**: Hemin Ali Qadir, Ilangko Balasingham, Younghak Shin
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic polyp generation is a good alternative to overcome the privacy problem of medical data and the lack of various polyp samples. In this study, we propose a deep learning-based polyp image generation framework that generates synthetic polyp images that are similar to real ones. We suggest a framework that converts a given polyp image into a negative image (image without a polyp) using a simple conditional GAN architecture and then converts the negative image into a new-looking polyp image using the same network. In addition, by using the controllable polyp masks, polyps with various characteristics can be generated from one input condition. The generated polyp images can be used directly as training images for polyp detection and segmentation without additional labeling. To quantitatively assess the quality of generated synthetic polyps, we use public polyp image and video datasets combined with the generated synthetic images to examine the performance improvement of several detection and segmentation models. Experimental results show that we obtain performance gains when the generated polyp images are added to the training set.



### JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2302.09838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09838v1)
- **Published**: 2023-02-20 08:55:00+00:00
- **Updated**: 2023-02-20 08:55:00+00:00
- **Authors**: Jiamu Sheng, Jiayuan Fan, Peng Ye, Jianjian Cao
- **Comment**: Accepted by 48th IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)
- **Journal**: None
- **Summary**: Despite substantial progress in no-reference image quality assessment (NR-IQA), previous training models often suffer from over-fitting due to the limited scale of used datasets, resulting in model performance bottlenecks. To tackle this challenge, we explore the potential of leveraging data augmentation to improve data efficiency and enhance model robustness. However, most existing data augmentation methods incur a serious issue, namely that it alters the image quality and leads to training images mismatching with their original labels. Additionally, although only a few data augmentation methods are available for NR-IQA task, their ability to enrich dataset diversity is still insufficient. To address these issues, we propose a effective and general data augmentation based on just noticeable difference (JND) noise mixing for NR-IQA task, named JNDMix. In detail, we randomly inject the JND noise, imperceptible to the human visual system (HVS), into the training image without any adjustment to its label. Extensive experiments demonstrate that JNDMix significantly improves the performance and data efficiency of various state-of-the-art NR-IQA models and the commonly used baseline models, as well as the generalization ability. More importantly, JNDMix facilitates MANIQA to achieve the state-of-the-art performance on LIVEC and KonIQ-10k.



### Constraint and Union for Partially-Supervised Temporal Sentence Grounding
- **Arxiv ID**: http://arxiv.org/abs/2302.09850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09850v1)
- **Published**: 2023-02-20 09:14:41+00:00
- **Updated**: 2023-02-20 09:14:41+00:00
- **Authors**: Chen Ju, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya Zhang, Peisen Zhao, Jianlong Chang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal sentence grounding aims to detect the event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great performance but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation cost, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip or even single-frame labels are available during training. To take full advantage of partial labels, we propose a novel quadruple constraint pipeline to comprehensively shape event-query aligned representations, covering intra- and inter-samples, uni- and multi-modalities. The former raises intra-cluster compactness and inter-cluster separability; while the latter enables event-background separation and event-query gather. To achieve more powerful performance with explicit grounding optimization, we further introduce a partial-full union framework, i.e., bridging with an additional fully-supervised branch, to enjoy its impressive grounding bonus, and be robust to partial annotations. Extensive experiments and ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision and our superior performance.



### GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.09884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09884v1)
- **Published**: 2023-02-20 10:20:07+00:00
- **Updated**: 2023-02-20 10:20:07+00:00
- **Authors**: Zezheng Zhang, Ryan K. Y. Chan, Kenneth K. Y. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, self-supervised monocular depth estimation has drawn much attention since it frees of depth annotations and achieved remarkable results on standard benchmarks. However, most of existing methods only focus on either daytime or nighttime images, thus their performance degrades on the other domain because of the large domain shift between daytime and nighttime images. To address this problem, in this paper we propose a two-branch network named GlocalFuse-Depth for self-supervised depth estimation of all-day images. The daytime and nighttime image in input image pair are fed into the two branches: CNN branch and Transformer branch, respectively, where both fine-grained details and global dependency can be efficiently captured. Besides, a novel fusion module is proposed to fuse multi-dimensional features from the two branches. Extensive experiments demonstrate that GlocalFuse-Depth achieves state-of-the-art results for all-day images on the Oxford RobotCar dataset, which proves the superiority of our method.



### InOR-Net: Incremental 3D Object Recognition Network for Point Cloud Representation
- **Arxiv ID**: http://arxiv.org/abs/2302.09886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09886v1)
- **Published**: 2023-02-20 10:30:16+00:00
- **Updated**: 2023-02-20 10:30:16+00:00
- **Authors**: Jiahua Dong, Yang Cong, Gan Sun, Lixu Wang, Lingjuan Lyu, Jun Li, Ender Konukoglu
- **Comment**: Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
- **Journal**: None
- **Summary**: 3D object recognition has successfully become an appealing research topic in the real-world. However, most existing recognition models unreasonably assume that the categories of 3D objects cannot change over time in the real-world. This unrealistic assumption may result in significant performance degradation for them to learn new classes of 3D objects consecutively, due to the catastrophic forgetting on old learned classes. Moreover, they cannot explore which 3D geometric characteristics are essential to alleviate the catastrophic forgetting on old classes of 3D objects. To tackle the above challenges, we develop a novel Incremental 3D Object Recognition Network (i.e., InOR-Net), which could recognize new classes of 3D objects continuously via overcoming the catastrophic forgetting on old classes. Specifically, a category-guided geometric reasoning is proposed to reason local geometric structures with distinctive 3D characteristics of each class by leveraging intrinsic category information. We then propose a novel critic-induced geometric attention mechanism to distinguish which 3D geometric characteristics within each class are beneficial to overcome the catastrophic forgetting on old classes of 3D objects, while preventing the negative influence of useless 3D characteristics. In addition, a dual adaptive fairness compensations strategy is designed to overcome the forgetting brought by class imbalance, by compensating biased weights and predictions of the classifier. Comparison experiments verify the state-of-the-art performance of the proposed InOR-Net model on several public point cloud datasets.



### A Survey on Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.09899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09899v1)
- **Published**: 2023-02-20 10:54:03+00:00
- **Updated**: 2023-02-20 10:54:03+00:00
- **Authors**: Adrian Peláez-Vegas, Pablo Mesejo, Julián Luengo
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is one of the most challenging tasks in computer vision. However, in many applications, a frequent obstacle is the lack of labeled images, due to the high cost of pixel-level labeling. In this scenario, it makes sense to approach the problem from a semi-supervised point of view, where both labeled and unlabeled images are exploited. In recent years this line of research has gained much interest and many approaches have been published in this direction. Therefore, the main objective of this study is to provide an overview of the current state of the art in semi-supervised semantic segmentation, offering an updated taxonomy of all existing methods to date. This is complemented by an experimentation with a variety of models representing all the categories of the taxonomy on the most widely used becnhmark datasets in the literature, and a final discussion on the results obtained, the challenges and the most promising lines of future research.



### General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2302.09907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09907v1)
- **Published**: 2023-02-20 11:08:07+00:00
- **Updated**: 2023-02-20 11:08:07+00:00
- **Authors**: Liang Xie, Yibo Yang, Wenxiao Wang, Binbin Lin, Deng Cai, Xiaofei He
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: Compared to 2D images, 3D point clouds are much more sensitive to rotations. We expect the point features describing certain patterns to keep invariant to the rotation transformation. There are many recent SOTA works dedicated to rotation-invariant learning for 3D point clouds. However, current rotation-invariant methods lack generalizability on the point clouds in the open scenes due to the reliance on the global distribution, \ie the global scene and backgrounds. Considering that the output activation is a function of the pattern and its orientation, we need to eliminate the effect of the orientation.In this paper, inspired by the idea that the network weights can be considered a set of points distributed in the same 3D space as the input points, we propose Weight-Feature Alignment (WFA) to construct a local Invariant Reference Frame (IRF) via aligning the features with the principal axes of the network weights. Our WFA algorithm provides a general solution for the point clouds of all scenes. WFA ensures the model achieves the target that the response activity is a necessary and sufficient condition of the pattern matching degree. Practically, we perform experiments on the point clouds of both single objects and open large-range scenes. The results suggest that our method almost bridges the gap between rotation invariance learning and normal methods.



### Interactive Face Video Coding: A Generative Compression Framework
- **Arxiv ID**: http://arxiv.org/abs/2302.09919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09919v1)
- **Published**: 2023-02-20 11:24:23+00:00
- **Updated**: 2023-02-20 11:24:23+00:00
- **Authors**: Bolin Chen, Zhao Wang, Binzhe Li, Shurun Wang, Shiqi Wang, Yan Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework for Interactive Face Video Coding (IFVC), which allows humans to interact with the intrinsic visual representations instead of the signals. The proposed solution enjoys several distinct advantages, including ultra-compact representation, low delay interaction, and vivid expression and headpose animation. In particular, we propose the Internal Dimension Increase (IDI) based representation, greatly enhancing the fidelity and flexibility in rendering the appearance while maintaining reasonable representation cost. By leveraging strong statistical regularities, the visual signals can be effectively projected into controllable semantics in the three dimensional space (e.g., mouth motion, eye blinking, head rotation and head translation), which are compressed and transmitted. The editable bitstream, which naturally supports the interactivity at the semantic level, can synthesize the face frames via the strong inference ability of the deep generative model. Experimental results have demonstrated the performance superiority and application prospects of our proposed IFVC scheme. In particular, the proposed scheme not only outperforms the state-of-the-art video coding standard Versatile Video Coding (VVC) and the latest generative compression schemes in terms of rate-distortion performance for face videos, but also enables the interactive coding without introducing additional manipulation processes. Furthermore, the proposed framework is expected to shed lights on the future design of the digital human communication in the metaverse.



### Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision
- **Arxiv ID**: http://arxiv.org/abs/2302.09922v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09922v2)
- **Published**: 2023-02-20 11:35:55+00:00
- **Updated**: 2023-02-22 08:51:08+00:00
- **Authors**: Zisong Chen, Chunyu Lin, Lang Nie, Kang Liao, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Omnidirectional multi-view stereo (MVS) vision is attractive for its ultra-wide field-of-view (FoV), enabling machines to perceive 360{\deg} 3D surroundings. However, the existing solutions require expensive dense depth labels for supervision, making them impractical in real-world applications. In this paper, we propose the first unsupervised omnidirectional MVS framework based on multiple fisheye images. To this end, we project all images to a virtual view center and composite two panoramic images with spherical geometry from two pairs of back-to-back fisheye images. The two 360{\deg} images formulate a stereo pair with a special pose, and the photometric consistency is leveraged to establish the unsupervised constraint, which we term "Pseudo-Stereo Supervision". In addition, we propose Un-OmniMVS, an efficient unsupervised omnidirectional MVS network, to facilitate the inference speed with two efficient components. First, a novel feature extractor with frequency attention is proposed to simultaneously capture the non-local Fourier features and local spatial features, explicitly facilitating the feature representation. Then, a variance-based light cost volume is put forward to reduce the computational complexity. Experiments exhibit that the performance of our unsupervised solution is competitive to that of the state-of-the-art (SoTA) supervised methods with better generalization in real-world data.



### Generalization capabilities of conditional GAN for turbulent flow under changes of geometry
- **Arxiv ID**: http://arxiv.org/abs/2302.09945v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09945v1)
- **Published**: 2023-02-20 12:21:34+00:00
- **Updated**: 2023-02-20 12:21:34+00:00
- **Authors**: Claudia Drygala, Francesca di Mare, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: Turbulent flow consists of structures with a wide range of spatial and temporal scales which are hard to resolve numerically. Classical numerical methods as the Large Eddy Simulation (LES) are able to capture fine details of turbulent structures but come at high computational cost. Applying generative adversarial networks (GAN) for the synthetic modeling of turbulence is a mathematically well-founded approach to overcome this issue. In this work, we investigate the generalization capabilites of GAN-based synthetic turbulence generators when geometrical changes occur in the flow configuration (e.g. aerodynamic geometric optimization of structures such as airfoils). As training data, we use the flow around a low-pressure turbine (LPT) stator with periodic wake impact obtained from highly resolved LES. To simulate the flow around a LPT stator, we use the conditional deep convolutional GAN framework pix2pixHD conditioned on the position of a rotating wake in front of the stator. For the generalization experiments we exclude images of wake positions located at certain regions from the training data and use the unseen data for testing. We show the abilities and limits of generalization for the conditional GAN by extending the regions of the extracted wake positions successively. Finally, we evaluate the statistical properties of the synthesized flow field by comparison with the corresponding LES results.



### SpecXAI -- Spectral interpretability of Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2302.09949v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09949v1)
- **Published**: 2023-02-20 12:36:54+00:00
- **Updated**: 2023-02-20 12:36:54+00:00
- **Authors**: Stefan Druc, Peter Wooldridge, Adarsh Krishnamurthy, Soumik Sarkar, Aditya Balu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is becoming increasingly adopted in business and industry due to its ability to transform large quantities of data into high-performing models. These models, however, are generally regarded as black boxes, which, in spite of their performance, could prevent their use. In this context, the field of eXplainable AI attempts to develop techniques that temper the impenetrable nature of the models and promote a level of understanding of their behavior. Here we present our contribution to XAI methods in the form of a framework that we term SpecXAI, which is based on the spectral characterization of the entire network. We show how this framework can be used to not only understand the network but also manipulate it into a linear interpretable symbolic representation.



### Because Every Sensor Is Unique, so Is Every Pair: Handling Dynamicity in Traffic Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2302.09956v2
- **DOI**: 10.1145/3576842.3582362
- **Categories**: **cs.LG**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2302.09956v2)
- **Published**: 2023-02-20 12:57:31+00:00
- **Updated**: 2023-02-28 18:30:37+00:00
- **Authors**: Arian Prabowo, Wei Shao, Hao Xue, Piotr Koniusz, Flora D. Salim
- **Comment**: 20 pages, IoTDI 2023; Correction on Fig. 4
- **Journal**: IoTDI 2023
- **Summary**: Traffic forecasting is a critical task to extract values from cyber-physical infrastructures, which is the backbone of smart transportation. However owing to external contexts, the dynamics at each sensor are unique. For example, the afternoon peaks at sensors near schools are more likely to occur earlier than those near residential areas. In this paper, we first analyze real-world traffic data to show that each sensor has a unique dynamic. Further analysis also shows that each pair of sensors also has a unique dynamic. Then, we explore how node embedding learns the unique dynamics at every sensor location. Next, we propose a novel module called Spatial Graph Transformers (SGT) where we use node embedding to leverage the self-attention mechanism to ensure that the information flow between two sensors is adaptive with respect to the unique dynamic of each pair. Finally, we present Graph Self-attention WaveNet (G-SWaN) to address the complex, non-linear spatiotemporal traffic dynamics. Through empirical experiments on four real-world, open datasets, we show that the proposed method achieves superior performance on both traffic speed and flow forecasting. Code is available at: https://github.com/aprbw/G-SWaN



### Advanced Image Quality Assessment for Hand- and Fingervein Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2302.09973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09973v2)
- **Published**: 2023-02-20 13:35:28+00:00
- **Updated**: 2023-02-21 10:55:57+00:00
- **Authors**: Simon Kirchgasser, Christof Kauba, Georg Wimmer, Andreas Uhl
- **Comment**: None
- **Journal**: None
- **Summary**: Natural Scene Statistics commonly used in non-reference image quality measures and a deep learning based quality assessment approach are proposed as biometric quality indicators for vasculature images. While NIQE and BRISQUE if trained on common images with usual distortions do not work well for assessing vasculature pattern samples' quality, their variants being trained on high and low quality vasculature sample data behave as expected from a biometric quality estimator in most cases (deviations from the overall trend occur for certain datasets or feature extraction methods). The proposed deep learning based quality metric is capable of assigning the correct quality class to the vaculature pattern samples in most cases, independent of finger or hand vein patterns being assessed. The experiments were conducted on a total of 13 publicly available finger and hand vein datasets and involve three distinct template representations (two of them especially designed for vascular biometrics). The proposed (trained) quality measures are compared to a several classical quality metrics, with their achieved results underlining their promising behaviour.



### Analyzing the Posterior Collapse in Hierarchical Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2302.09976v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09976v1)
- **Published**: 2023-02-20 13:44:47+00:00
- **Updated**: 2023-02-20 13:44:47+00:00
- **Authors**: Anna Kuzina, Jakub M. Tomczak
- **Comment**: Code: https://github.com/AKuzina/dct_vae
- **Journal**: None
- **Summary**: Hierarchical Variational Autoencoders (VAEs) are among the most popular likelihood-based generative models. There is rather a consensus that the top-down hierarchical VAEs allow to effectively learn deep latent structures and avoid problems like the posterior collapse. Here, we show that it is not necessarily the case and the problem of collapsing posteriors remains. To discourage the posterior collapse, we propose a new deep hierarchical VAE with a partly fixed encoder, specifically, we use Discrete Cosine Transform to obtain top latent variables. In a series of experiments, we observe that the proposed modification allows us to achieve better utilization of the latent space. Further, we demonstrate that the proposed approach can be useful for compression and robustness to adversarial attacks.



### Deep Vision in Analysis and Recognition of Radar Data: Achievements, Advancements and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2302.09990v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2, I.4, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2302.09990v1)
- **Published**: 2023-02-20 14:06:45+00:00
- **Updated**: 2023-02-20 14:06:45+00:00
- **Authors**: Qi Liu, Zhiyun Yang, Ru Ji, Yonghong Zhang, Muhammad Bilal, Xiaodong Liu, S Vimal, Xiaolong Xu
- **Comment**: This article has been accepted for publication in IEEE Systems, Man
  and Cybernetics Magazine
- **Journal**: None
- **Summary**: Radars are widely used to obtain echo information for effective prediction, such as precipitation nowcasting. In this paper, recent relevant scientific investigation and practical efforts using Deep Learning (DL) models for weather radar data analysis and pattern recognition have been reviewed; particularly, in the fields of beam blockage correction, radar echo extrapolation, and precipitation nowcast. Compared to traditional approaches, present DL methods depict better performance and convenience but suffer from stability and generalization. In addition to recent achievements, the latest advancements and existing challenges are also presented and discussed in this paper, trying to lead to reasonable potentials and trends in this highly-concerned field.



### A Large Scale Homography Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2302.09997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09997v1)
- **Published**: 2023-02-20 14:18:09+00:00
- **Updated**: 2023-02-20 14:18:09+00:00
- **Authors**: Daniel Barath, Dmytro Mishkin, Michal Polic, Wolfgang Förstner, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a large-scale dataset of Planes in 3D, Pi3D, of roughly 1000 planes observed in 10 000 images from the 1DSfM dataset, and HEB, a large-scale homography estimation benchmark leveraging Pi3D. The applications of the Pi3D dataset are diverse, e.g. training or evaluating monocular depth, surface normal estimation and image matching algorithms. The HEB dataset consists of 226 260 homographies and includes roughly 4M correspondences. The homographies link images that often undergo significant viewpoint and illumination changes. As applications of HEB, we perform a rigorous evaluation of a wide range of robust estimators and deep learning-based correspondence filtering methods, establishing the current state-of-the-art in robust homography estimation. We also evaluate the uncertainty of the SIFT orientations and scales w.r.t. the ground truth coming from the underlying homographies and provide codes for comparing uncertainty of custom detectors. The dataset is available at \url{https://github.com/danini/homography-benchmark}.



### Gesture Recognition with Keypoint and Radar Stream Fusion for Automated Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2302.09998v1
- **DOI**: 10.1007/978-3-031-25056-9_36
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09998v1)
- **Published**: 2023-02-20 14:18:11+00:00
- **Updated**: 2023-02-20 14:18:11+00:00
- **Authors**: Adrian Holzbock, Nicolai Kern, Christian Waldschmidt, Klaus Dietmayer, Vasileios Belagiannis
- **Comment**: Accepted for presentation at the 3rd AVVision Workshop at ECCV 2022,
  October 23, 2022, Tel Aviv, Israel
- **Journal**: In Computer Vision-ECCV 2022 Workshops: Tel Aviv, Israel, October
  23-27, 2022, Proceedings, Part I (pp. 570-584). Cham: Springer Nature
  Switzerland
- **Summary**: We present a joint camera and radar approach to enable autonomous vehicles to understand and react to human gestures in everyday traffic. Initially, we process the radar data with a PointNet followed by a spatio-temporal multilayer perceptron (stMLP). Independently, the human body pose is extracted from the camera frame and processed with a separate stMLP network. We propose a fusion neural network for both modalities, including an auxiliary loss for each modality. In our experiments with a collected dataset, we show the advantages of gesture recognition with two modalities. Motivated by adverse weather conditions, we also demonstrate promising performance when one of the sensors lacks functionality.



### STB-VMM: Swin Transformer Based Video Motion Magnification
- **Arxiv ID**: http://arxiv.org/abs/2302.10001v2
- **DOI**: 10.1016/j.knosys.2023.110493
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.10001v2)
- **Published**: 2023-02-20 14:21:56+00:00
- **Updated**: 2023-03-27 20:18:45+00:00
- **Authors**: Ricard Lado-Roigé, Marco A. Pérez
- **Comment**: Code available at: https://github.com/RLado/STB-VMM
- **Journal**: Knowl.-Based Syst. 269 (2023) 110493
- **Summary**: The goal of video motion magnification techniques is to magnify small motions in a video to reveal previously invisible or unseen movement. Its uses extend from bio-medical applications and deepfake detection to structural modal analysis and predictive maintenance. However, discerning small motion from noise is a complex task, especially when attempting to magnify very subtle, often sub-pixel movement. As a result, motion magnification techniques generally suffer from noisy and blurry outputs. This work presents a new state-of-the-art model based on the Swin Transformer, which offers better tolerance to noisy inputs as well as higher-quality outputs that exhibit less noise, blurriness, and artifacts than prior-art. Improvements in output image quality will enable more precise measurements for any application reliant on magnified video sequences, and may enable further development of video motion magnification techniques in new technical fields.



### Simulating analogue film damage to analyse and improve artefact restoration on high-resolution scans
- **Arxiv ID**: http://arxiv.org/abs/2302.10004v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10004v1)
- **Published**: 2023-02-20 14:24:18+00:00
- **Updated**: 2023-02-20 14:24:18+00:00
- **Authors**: Daniela Ivanova, John Williamson, Paul Henderson
- **Comment**: Accepted as full paper at Eurographics 2023
- **Journal**: None
- **Summary**: Digital scans of analogue photographic film typically contain artefacts such as dust and scratches. Automated removal of these is an important part of preservation and dissemination of photographs of historical and cultural importance.   While state-of-the-art deep learning models have shown impressive results in general image inpainting and denoising, film artefact removal is an understudied problem. It has particularly challenging requirements, due to the complex nature of analogue damage, the high resolution of film scans, and potential ambiguities in the restoration. There are no publicly available high-quality datasets of real-world analogue film damage for training and evaluation, making quantitative studies impossible.   We address the lack of ground-truth data for evaluation by collecting a dataset of 4K damaged analogue film scans paired with manually-restored versions produced by a human expert, allowing quantitative evaluation of restoration performance. We construct a larger synthetic dataset of damaged images with paired clean versions using a statistical model of artefact shape and occurrence learnt from real, heavily-damaged images. We carefully validate the realism of the simulated damage via a human perceptual study, showing that even expert users find our synthetic damage indistinguishable from real. In addition, we demonstrate that training with our synthetically damaged dataset leads to improved artefact segmentation performance when compared to previously proposed synthetic analogue damage.   Finally, we use these datasets to train and analyse the performance of eight state-of-the-art image restoration methods on high-resolution scans. We compare both methods which directly perform the restoration task on scans with artefacts, and methods which require a damage mask to be provided for the inpainting of artefacts.



### On the Metrics for Evaluating Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.10007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10007v1)
- **Published**: 2023-02-20 14:33:09+00:00
- **Updated**: 2023-02-20 14:33:09+00:00
- **Authors**: Akhil Gurram, Antonio M. Lopez
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Monocular Depth Estimation (MDE) is performed to produce 3D information that can be used in downstream tasks such as those related to on-board perception for Autonomous Vehicles (AVs) or driver assistance. Therefore, a relevant arising question is whether the standard metrics for MDE assessment are a good indicator of the accuracy of future MDE-based driving-related perception tasks. We address this question in this paper. In particular, we take the task of 3D object detection on point clouds as a proxy of on-board perception. We train and test state-of-the-art 3D object detectors using 3D point clouds coming from MDE models. We confront the ranking of object detection results with the ranking given by the depth estimation metrics of the MDE models. We conclude that, indeed, MDE evaluation metrics give rise to a ranking of methods that reflects relatively well the 3D object detection results we may expect. Among the different metrics, the absolute relative (abs-rel) error seems to be the best for that purpose.



### Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective
- **Arxiv ID**: http://arxiv.org/abs/2302.10021v2
- **DOI**: 10.1145/3594806.3594829
- **Categories**: **cs.CV**, cs.LG, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2302.10021v2)
- **Published**: 2023-02-20 15:07:24+00:00
- **Updated**: 2023-05-25 19:53:08+00:00
- **Authors**: Nikolaos Kegkeroglou, Panagiotis P. Filntisis, Petros Maragos
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The COVID-19 pandemic has undoubtedly changed the standards and affected all aspects of our lives, especially social communication. It has forced people to extensively wear medical face masks, in order to prevent transmission. This face occlusion can strongly irritate emotional reading from the face and urges us to incorporate the whole body as an emotional cue. In this paper, we conduct insightful studies about the effect of face occlusion on emotion recognition performance, and showcase the superiority of full body input over the plain masked face. We utilize a deep learning model based on the Temporal Segment Network framework, and aspire to fully overcome the face mask consequences. Although facial and bodily features can be learned from a single input, this may lead to irrelevant information confusion. By processing those features separately and fusing their prediction scores, we are more effectively taking advantage of both modalities. This framework also naturally supports temporal modeling, by mingling information among neighboring frames. In combination, these techniques form an effective system capable of tackling emotion recognition difficulties, caused by safety protocols applied in crucial areas.



### Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.10035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.10035v1)
- **Published**: 2023-02-20 15:34:03+00:00
- **Updated**: 2023-02-20 15:34:03+00:00
- **Authors**: Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao
- **Comment**: Accepted by Machine Intelligence Research
- **Journal**: None
- **Summary**: With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the validation of large-scale MM-PTMs, including generative, classification, and regression tasks. We also give visualization and analysis of the model parameters and results on representative downstream tasks. Finally, we point out possible research directions for this topic that may benefit future works. In addition, we maintain a continuously updated paper list for large-scale pre-trained multi-modal big models: https://github.com/wangxiao5791509/MultiModal_BigModels_Survey



### Ontology-aware Network for Zero-shot Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2302.10040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10040v1)
- **Published**: 2023-02-20 15:44:41+00:00
- **Updated**: 2023-02-20 15:44:41+00:00
- **Authors**: Haoxiang Zhang, He Jiang, Ziqiang Wang, Deqiang Cheng
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Zero-Shot Sketch-Based Image Retrieval (ZSSBIR) is an emerging task. The pioneering work focused on the modal gap but ignored inter-class information. Although recent work has begun to consider the triplet-based or contrast-based loss to mine inter-class information, positive and negative samples need to be carefully selected, or the model is prone to lose modality-specific information. To respond to these issues, an Ontology-Aware Network (OAN) is proposed. Specifically, the smooth inter-class independence learning mechanism is put forward to maintain inter-class peculiarity. Meanwhile, distillation-based consistency preservation is utilized to keep modality-specific information. Extensive experiments have demonstrated the superior performance of our algorithm on two challenging Sketchy and Tu-Berlin datasets.



### UAVStereo: A Multiple Resolution Dataset for Stereo Matching in UAV Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2302.10082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10082v1)
- **Published**: 2023-02-20 16:45:27+00:00
- **Updated**: 2023-02-20 16:45:27+00:00
- **Authors**: Zhang Xiaoyi, Cao Xuefeng, Yu Anzhu, Yu Wenshuai, Li Zhenqi, Quan Yujun
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo matching is a fundamental task for 3D scene reconstruction. Recently, deep learning based methods have proven effective on some benchmark datasets, such as KITTI and Scene Flow. UAVs (Unmanned Aerial Vehicles) are commonly utilized for surface observation, and their captured images are frequently used for detailed 3D reconstruction due to high resolution and low-altitude acquisition. At present, the mainstream supervised learning network requires a significant amount of training data with ground-truth labels to learn model parameters. However, due to the scarcity of UAV stereo matching datasets, the learning-based network cannot be applied to UAV images. To facilitate further research, this paper proposes a novel pipeline to generate accurate and dense disparity maps using detailed meshes reconstructed by UAV images and LiDAR point clouds. Through the proposed pipeline, this paper constructs a multi-resolution UAV scenario dataset, called UAVStereo, with over 34k stereo image pairs covering 3 typical scenes. As far as we know, UAVStereo is the first stereo matching dataset of UAV low-altitude scenarios. The dataset includes synthetic and real stereo pairs to enable generalization from the synthetic domain to the real domain. Furthermore, our UAVStereo dataset provides multi-resolution and multi-scene images pairs to accommodate a variety of sensors and environments. In this paper, we evaluate traditional and state-of-the-art deep learning methods, highlighting their limitations in addressing challenges in UAV scenarios and offering suggestions for future research. The dataset is available at https://github.com/rebecca0011/UAVStereo.git



### NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2302.10109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10109v1)
- **Published**: 2023-02-20 17:12:00+00:00
- **Updated**: 2023-02-20 17:12:00+00:00
- **Authors**: Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi
- **Comment**: Project page: https://jiataogu.me/nerfdiff/
- **Journal**: None
- **Summary**: Novel view synthesis from a single image requires inferring occluded regions of objects and scenes whilst simultaneously maintaining semantic and physical consistency with the input. Existing approaches condition neural radiance fields (NeRF) on local image features, projecting points to the input image plane, and aggregating 2D features to perform volume rendering. However, under severe occlusion, this projection fails to resolve uncertainty, resulting in blurry renderings that lack details. In this work, we propose NerfDiff, which addresses this issue by distilling the knowledge of a 3D-aware conditional diffusion model (CDM) into NeRF through synthesizing and refining a set of virtual views at test time. We further propose a novel NeRF-guided distillation algorithm that simultaneously generates 3D consistent virtual views from the CDM samples, and finetunes the NeRF based on the improved virtual views. Our approach significantly outperforms existing NeRF-based and geometry-free approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.



### iQPP: A Benchmark for Image Query Performance Prediction
- **Arxiv ID**: http://arxiv.org/abs/2302.10126v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2302.10126v3)
- **Published**: 2023-02-20 17:56:57+00:00
- **Updated**: 2023-04-10 06:41:46+00:00
- **Authors**: Eduard Poesina, Radu Tudor Ionescu, Josiane Mothe
- **Comment**: Accepted at SIGIR 2023
- **Journal**: None
- **Summary**: To date, query performance prediction (QPP) in the context of content-based image retrieval remains a largely unexplored task, especially in the query-by-example scenario, where the query is an image. To boost the exploration of the QPP task in image retrieval, we propose the first benchmark for image query performance prediction (iQPP). First, we establish a set of four data sets (PASCAL VOC 2012, Caltech-101, ROxford5k and RParis6k) and estimate the ground-truth difficulty of each query as the average precision or the precision@k, using two state-of-the-art image retrieval models. Next, we propose and evaluate novel pre-retrieval and post-retrieval query performance predictors, comparing them with existing or adapted (from text to image) predictors. The empirical results show that most predictors do not generalize across evaluation scenarios. Our comprehensive experiments indicate that iQPP is a challenging benchmark, revealing an important research gap that needs to be addressed in future work. We release our code and data as open source at https://github.com/Eduard6421/iQPP, to foster future research.



### Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2302.10164v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10164v1)
- **Published**: 2023-02-20 18:50:18+00:00
- **Updated**: 2023-02-20 18:50:18+00:00
- **Authors**: Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, Sven Gowal
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training is widely used to make classifiers robust to a specific threat or adversary, such as $\ell_p$-norm bounded perturbations of a given $p$-norm. However, existing methods for training classifiers robust to multiple threats require knowledge of all attacks during training and remain vulnerable to unseen distribution shifts. In this work, we describe how to obtain adversarially-robust model soups (i.e., linear combinations of parameters) that smoothly trade-off robustness to different $\ell_p$-norm bounded adversaries. We demonstrate that such soups allow us to control the type and level of robustness, and can achieve robustness to all threats without jointly training on all of them. In some cases, the resulting model soups are more robust to a given $\ell_p$-norm adversary than the constituent model specialized against that same adversary. Finally, we show that adversarially-robust model soups can be a viable tool to adapt to distribution shifts from a few examples.



### Cross-domain Compositing with Pretrained Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2302.10167v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10167v2)
- **Published**: 2023-02-20 18:54:04+00:00
- **Updated**: 2023-05-25 06:30:04+00:00
- **Authors**: Roy Hachnochi, Mingrui Zhao, Nadav Orzech, Rinon Gal, Ali Mahdavi-Amiri, Daniel Cohen-Or, Amit Haim Bermano
- **Comment**: Code:
  https://github.com/cross-domain-compositing/cross-domain-compositing
- **Journal**: None
- **Summary**: Diffusion models have enabled high-quality, conditional image editing capabilities. We propose to expand their arsenal, and demonstrate that off-the-shelf diffusion models can be used for a wide range of cross-domain compositing tasks. Among numerous others, these include image blending, object immersion, texture-replacement and even CG2Real translation or stylization. We employ a localized, iterative refinement scheme which infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo. We conduct a range of qualitative and quantitative comparisons to prior work, and exhibit that our method produces higher quality and realistic results without requiring any annotations or training. Finally, we demonstrate how our method may be used for data augmentation of downstream tasks.



### Towards Universal Fake Image Detectors that Generalize Across Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2302.10174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10174v1)
- **Published**: 2023-02-20 18:59:04+00:00
- **Updated**: 2023-02-20 18:59:04+00:00
- **Authors**: Utkarsh Ojha, Yuheng Li, Yong Jae Lee
- **Comment**: None
- **Journal**: None
- **Summary**: With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a sink class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the SoTA by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models.



### Unsupervised Learning on a DIET: Datum IndEx as Target Free of Self-Supervision, Reconstruction, Projector Head
- **Arxiv ID**: http://arxiv.org/abs/2302.10260v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10260v1)
- **Published**: 2023-02-20 19:46:54+00:00
- **Updated**: 2023-02-20 19:46:54+00:00
- **Authors**: Randall Balestriero
- **Comment**: None
- **Journal**: None
- **Summary**: Costly, noisy, and over-specialized, labels are to be set aside in favor of unsupervised learning if we hope to learn cheap, reliable, and transferable models. To that end, spectral embedding, self-supervised learning, or generative modeling have offered competitive solutions. Those methods however come with numerous challenges \textit{e.g.} estimating geodesic distances, specifying projector architectures and anti-collapse losses, or specifying decoder architectures and reconstruction losses. In contrast, we introduce a simple explainable alternative -- coined \textbf{DIET} -- to learn representations from unlabeled data, free of those challenges. \textbf{DIET} is blatantly simple: take one's favorite classification setup and use the \textbf{D}atum \textbf{I}nd\textbf{E}x as its \textbf{T}arget class, \textit{i.e. each sample is its own class}, no further changes needed. \textbf{DIET} works without a decoder/projector network, is not based on positive pairs nor reconstruction, introduces no hyper-parameters, and works out-of-the-box across datasets and architectures. Despite \textbf{DIET}'s simplicity, the learned representations are of high-quality and often on-par with the state-of-the-art \textit{e.g.} using a linear classifier on top of DIET's learned representation reaches $71.4\%$ on CIFAR100 with a Resnet101, $52.5\%$ on TinyImagenet with a Resnext50.



### Kernel function impact on convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2302.10266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.10266v1)
- **Published**: 2023-02-20 19:57:01+00:00
- **Updated**: 2023-02-20 19:57:01+00:00
- **Authors**: M. Amine Mahmoudi, Aladine Chetouani, Fatma Boufera, Hedi Tabia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the usage of kernel functions at the different layers in a convolutional neural network. We carry out extensive studies of their impact on convolutional, pooling and fully-connected layers. We notice that the linear kernel may not be sufficiently effective to fit the input data distributions, whereas high order kernels prone to over-fitting. This leads to conclude that a trade-off between complexity and performance should be reached. We show how one can effectively leverage kernel functions, by introducing a more distortion aware pooling layers which reduces over-fitting while keeping track of the majority of the information fed into subsequent layers. We further propose Kernelized Dense Layers (KDL), which replace fully-connected layers, and capture higher order feature interactions. The experiments on conventional classification datasets i.e. MNIST, FASHION-MNIST and CIFAR-10, show that the proposed techniques improve the performance of the network compared to classical convolution, pooling and fully connected layers. Moreover, experiments on fine-grained classification i.e. facial expression databases, namely RAF-DB, FER2013 and ExpW demonstrate that the discriminative power of the network is boosted, since the proposed techniques improve the awareness to slight visual details and allows the network reaching state-of-the-art results.



### Image Reconstruction via Deep Image Prior Subspaces
- **Arxiv ID**: http://arxiv.org/abs/2302.10279v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10279v2)
- **Published**: 2023-02-20 20:19:36+00:00
- **Updated**: 2023-06-05 09:50:48+00:00
- **Authors**: Riccardo Barbano, Javier Antorán, Johannes Leuschner, José Miguel Hernández-Lobato, Bangti Jin, Željko Kereta
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been widely used for solving image reconstruction tasks but its deployability has been held back due to the shortage of high-quality training data. Unsupervised learning methods, such as the deep image prior (DIP), naturally fill this gap, but bring a host of new issues: the susceptibility to overfitting due to a lack of robust early stopping strategies and unstable convergence. We present a novel approach to tackle these issues by restricting DIP optimisation to a sparse linear subspace of its parameters, employing a synergy of dimensionality reduction techniques and second order optimisation methods. The low-dimensionality of the subspace reduces DIP's tendency to fit noise and allows the use of stable second order optimisation methods, e.g., natural gradient descent or L-BFGS. Experiments across both image restoration and tomographic tasks of different geometry and ill-posedness show that second order optimisation within a low-dimensional subspace is favourable in terms of optimisation stability to reconstruction fidelity trade-off.



### Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models
- **Arxiv ID**: http://arxiv.org/abs/2302.10289v9
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10289v9)
- **Published**: 2023-02-20 20:25:41+00:00
- **Updated**: 2023-07-07 05:50:00+00:00
- **Authors**: Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich
- **Comment**: 2nd Workshop on Spurious Correlations, Invariance, and Stability,
  ICML 2023
- **Journal**: None
- **Summary**: We use concept-based interpretable models to mitigate shortcut learning. Existing methods lack interpretability. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each expert explains a subset of data using First Order Logic (FOL). While explaining a sample, the FOL from biased BB-derived MoIE detects the shortcut effectively. Finetuning the BB with Metadata Normalization (MDN) eliminates the shortcut. The FOLs from the finetuned-BB-derived MoIE verify the elimination of the shortcut. Our experiments show that MoIE does not hurt the accuracy of the original BB and eliminates shortcuts effectively.



### Hadamard Layer to Improve Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.10318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2302.10318v1)
- **Published**: 2023-02-20 21:05:17+00:00
- **Updated**: 2023-02-20 21:05:17+00:00
- **Authors**: Angello Hoyos, Mariano Rivera
- **Comment**: Accepted in ICASSP 2023
- **Journal**: None
- **Summary**: The Hadamard Layer, a simple and computationally efficient way to improve results in semantic segmentation tasks, is presented. This layer has no free parameters that require to be trained. Therefore it does not increase the number of model parameters, and the extra computational cost is marginal. Experimental results show that the new Hadamard layer substantially improves the performance of the investigated models (variants of the Pix2Pix model). The performance's improvement can be explained by the Hadamard layer forcing the network to produce an internal encoding of the classes so that all bins are active. Therefore, the network computation is more distributed. In a sort that the Hadamard layer requires that to change the predicted class, it is necessary to modify $2^{k-1}$ bins, assuming $k$ bins in the encoding. A specific loss function allows a stable and fast training convergence.



### Unsupervised Out-of-Distribution Detection with Diffusion Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2302.10326v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10326v2)
- **Published**: 2023-02-20 21:34:59+00:00
- **Updated**: 2023-08-16 20:46:43+00:00
- **Authors**: Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, Kilian Q. Weinberger
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: Unsupervised out-of-distribution detection (OOD) seeks to identify out-of-domain data by learning only from unlabeled in-domain data. We present a novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent advancement in diffusion models. Diffusion models are one type of generative models. At their core, they learn an iterative denoising process that gradually maps a noisy image closer to their training manifolds. LMD leverages this intuition for OOD detection. Specifically, LMD lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For an out-of-domain image, the mapped image would have a large distance away from its original manifold, and LMD would identify it as OOD accordingly. We show through extensive experiments that LMD achieves competitive performance across a broad variety of datasets. Code can be found at https://github.com/zhenzhel/lift_map_detect.



### Take Me Home: Reversing Distribution Shifts using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.10341v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10341v2)
- **Published**: 2023-02-20 22:06:26+00:00
- **Updated**: 2023-02-24 20:31:06+00:00
- **Authors**: Vivian Lin, Kuk Jin Jang, Souradeep Dutta, Michele Caprio, Oleg Sokolsky, Insup Lee
- **Comment**: preprint (under submission)
- **Journal**: None
- **Summary**: Deep neural networks have repeatedly been shown to be non-robust to the uncertainties of the real world. Even subtle adversarial attacks and naturally occurring distribution shifts wreak havoc on systems relying on deep neural networks. In response to this, current state-of-the-art techniques use data-augmentation to enrich the training distribution of the model and consequently improve robustness to natural distribution shifts. We propose an alternative approach that allows the system to recover from distribution shifts online. Specifically, our method applies a sequence of semantic-preserving transformations to bring the shifted data closer in distribution to the training set, as measured by the Wasserstein distance. We formulate the problem of sequence selection as an MDP, which we solve using reinforcement learning. To aid in our estimates of Wasserstein distance, we employ dimensionality reduction through orthonormal projection. We provide both theoretical and empirical evidence that orthonormal projection preserves characteristics of the data at the distributional level. Finally, we apply our distribution shift recovery approach to the ImageNet-C benchmark for distribution shifts, targeting shifts due to additive noise and image histogram modifications. We demonstrate an improvement in average accuracy up to 14.21% across a variety of state-of-the-art ImageNet classifiers.



### Non-rigid Medical Image Registration using Physics-informed Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.10343v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2302.10343v1)
- **Published**: 2023-02-20 22:17:29+00:00
- **Updated**: 2023-02-20 22:17:29+00:00
- **Authors**: Zhe Min, Zachary M. C. Baum, Shaheer U. Saeed, Mark Emberton, Dean C. Barratt, Zeike A. Taylor, Yipeng Hu
- **Comment**: IPMI 2023
- **Journal**: None
- **Summary**: Biomechanical modelling of soft tissue provides a non-data-driven method for constraining medical image registration, such that the estimated spatial transformation is considered biophysically plausible. This has not only been adopted in real-world clinical applications, such as the MR-to-ultrasound registration for prostate intervention of interest in this work, but also provides an explainable means of understanding the organ motion and spatial correspondence establishment. This work instantiates the recently-proposed physics-informed neural networks (PINNs) to a 3D linear elastic model for modelling prostate motion commonly encountered during transrectal ultrasound guided procedures. To overcome a widely-recognised challenge in generalising PINNs to different subjects, we propose to use PointNet as the nodal-permutation-invariant feature extractor, together with a registration algorithm that aligns point sets and simultaneously takes into account the PINN-imposed biomechanics. The proposed method has been both developed and validated in both patient-specific and multi-patient manner.



