# Arxiv Papers in cs.CV on 2023-02-17
### Low Latency Video Denoising for Online Conferencing Using CNN Architectures
- **Arxiv ID**: http://arxiv.org/abs/2302.08638v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08638v1)
- **Published**: 2023-02-17 00:55:54+00:00
- **Updated**: 2023-02-17 00:55:54+00:00
- **Authors**: Altanai Bisht, Ana Carolina de Souza Mendes, Justin David Thoreson II, Shadrokh Samavi
- **Comment**: 8 pages 14 figures
- **Journal**: None
- **Summary**: In this paper, we propose a pipeline for real-time video denoising with low runtime cost and high perceptual quality. The vast majority of denoising studies focus on image denoising. However, a minority of research works focusing on video denoising do so with higher performance costs to obtain higher quality while maintaining temporal coherence. The approach we introduce in this paper leverages the advantages of both image and video-denoising architectures. Our pipeline first denoises the keyframes or one-fifth of the frames using HI-GAN blind image denoising architecture. Then, the remaining four-fifths of the noisy frames and the denoised keyframe data are fed into the FastDVDnet video denoising model. The final output is rendered in the user's display in real-time. The combination of these low-latency neural network architectures produces real-time denoising with high perceptual quality with applications in video conferencing and other real-time media streaming systems. A custom noise detector analyzer provides real-time feedback to adapt the weights and improve the models' output.



### Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2302.08641v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08641v1)
- **Published**: 2023-02-17 01:13:58+00:00
- **Updated**: 2023-02-17 01:13:58+00:00
- **Authors**: Shiv Ram Dubey, Satish Kumar Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been very successful for synthesizing the images in a given dataset. The artificially generated images by GANs are very realistic. The GANs have shown potential usability in several computer vision applications, including image generation, image-to-image translation, video synthesis, and others. Conventionally, the generator network is the backbone of GANs, which generates the samples and the discriminator network is used to facilitate the training of the generator network. The discriminator network is usually a Convolutional Neural Network (CNN). Whereas, the generator network is usually either an Up-CNN for image generation or an Encoder-Decoder network for image-to-image translation. The convolution-based networks exploit the local relationship in a layer, which requires the deep networks to extract the abstract features. Hence, CNNs suffer to exploit the global relationship in the feature space. However, recently developed Transformer networks are able to exploit the global relationship at every layer. The Transformer networks have shown tremendous performance improvement for several problems in computer vision. Motivated from the success of Transformer networks and GANs, recent works have tried to exploit the Transformers in GAN framework for the image/video synthesis. This paper presents a comprehensive survey on the developments and advancements in GANs utilizing the Transformer networks for computer vision applications. The performance comparison for several applications on benchmark datasets is also performed and analyzed. The conducted survey will be very useful to deep learning and computer vision community to understand the research trends \& gaps related with Transformer-based GANs and to develop the advanced GAN architectures by exploiting the global and local relationships for different applications.



### AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2302.08646v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08646v3)
- **Published**: 2023-02-17 01:31:53+00:00
- **Updated**: 2023-03-30 16:10:25+00:00
- **Authors**: Tianyue Zheng, Ang Li, Zhe Chen, Hongbo Wang, Jun Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, \textit{federated learning} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-based data imputation method to fill missing data modality (of certain AVs) with the available ones. To further reconcile the heterogeneity, we finally present a client selection mechanism exploiting the similarities among client models to improve both training stability and convergence rate. Our experiments on benchmark dataset confirm that AutoFed substantially improves over status quo approaches in both precision and recall, while demonstrating strong robustness to adverse weather conditions.



### Conformers are All You Need for Visual Speech Recogntion
- **Arxiv ID**: http://arxiv.org/abs/2302.10915v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2302.10915v1)
- **Published**: 2023-02-17 01:31:55+00:00
- **Updated**: 2023-02-17 01:31:55+00:00
- **Authors**: Oscar Chang, Hank Liao, Dmitriy Serdyuk, Ankit Shah, Olivier Siohan
- **Comment**: None
- **Journal**: None
- **Summary**: Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surprisingly, our work shows that complex visual front-ends are not necessary. Instead of allocating resources to a sophisticated visual front-end, we find that a linear visual front-end paired with a larger Conformer encoder results in lower latency, more efficient memory usage, and improved WER performance. We achieve a new state-of-the-art of $12.8\%$ WER for visual speech recognition on the TED LRS3 dataset, which rivals the performance of audio-only models from just four years ago.



### Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.08650v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2302.08650v1)
- **Published**: 2023-02-17 01:50:46+00:00
- **Updated**: 2023-02-17 01:50:46+00:00
- **Authors**: Xuefeng Liang, Hexin Jiang, Wenxin Xu, Ying Zhou
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In speech emotion recognition tasks, models learn emotional representations from datasets. We find the data distribution in the IEMOCAP dataset is very imbalanced, which may harm models to learn a better representation. To address this issue, we propose a novel Pairwise-emotion Data Distribution Smoothing (PDDS) method. PDDS considers that the distribution of emotional data should be smooth in reality, then applies Gaussian smoothing to emotion-pairs for constructing a new training set with a smoother distribution. The required new data are complemented using the mixup augmentation. As PDDS is model and modality agnostic, it is evaluated with three SOTA models on the IEMOCAP dataset. The experimental results show that these models are improved by 0.2\% - 4.8\% and 1.5\% - 5.9\% in terms of WA and UA. In addition, an ablation study demonstrates that the key advantage of PDDS is the reasonable data distribution rather than a simple data augmentation.



### Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression
- **Arxiv ID**: http://arxiv.org/abs/2302.08662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08662v1)
- **Published**: 2023-02-17 02:48:33+00:00
- **Updated**: 2023-02-17 02:48:33+00:00
- **Authors**: Zhiyu Pan, Yinpeng Chen, Jiale Zhang, Hao Lu, Zhiguo Cao, Weicai Zhong
- **Comment**: Accepted to AAAI 2023 (Oral); 9 pages, 6 figures
- **Journal**: None
- **Summary**: Automatic image cropping algorithms aim to recompose images like human-being photographers by generating the cropping boxes with improved composition quality. Cropping box regression approaches learn the beauty of composition from annotated cropping boxes. However, the bias of annotations leads to quasi-trivial recomposing results, which has an obvious tendency to the average location of training samples. The crux of this predicament is that the task is naively treated as a box regression problem, where rare samples might be dominated by normal samples, and the composition patterns of rare samples are not well exploited. Observing that similar composition patterns tend to be shared by the cropping boundaries annotated nearly, we argue to find the beauty of composition from the rare samples by clustering the samples with similar cropping boundary annotations, ie, similar composition patterns. We propose a novel Contrastive Composition Clustering (C2C) to regularize the composition features by contrasting dynamically established similar and dissimilar pairs. In this way, common composition patterns of multiple images can be better summarized, which especially benefits the rare samples and endows our model with better generalizability to render nontrivial results. Extensive experimental results show the superiority of our model compared with prior arts. We also illustrate the philosophy of our design with an interesting analytical visualization.



### Cascaded information enhancement and cross-modal attention feature fusion for multispectral pedestrian detection
- **Arxiv ID**: http://arxiv.org/abs/2302.08670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2302.08670v1)
- **Published**: 2023-02-17 03:30:00+00:00
- **Updated**: 2023-02-17 03:30:00+00:00
- **Authors**: Yang Yang, Kaixiong Xu, Kaizheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral pedestrian detection is a technology designed to detect and locate pedestrians in Color and Thermal images, which has been widely used in automatic driving, video surveillance, etc. So far most available multispectral pedestrian detection algorithms only achieved limited success in pedestrian detection because of the lacking take into account the confusion of pedestrian information and background noise in Color and Thermal images. Here we propose a multispectral pedestrian detection algorithm, which mainly consists of a cascaded information enhancement module and a cross-modal attention feature fusion module. On the one hand, the cascaded information enhancement module adopts the channel and spatial attention mechanism to perform attention weighting on the features fused by the cascaded feature fusion block. Moreover, it multiplies the single-modal features with the attention weight element by element to enhance the pedestrian features in the single-modal and thus suppress the interference from the background. On the other hand, the cross-modal attention feature fusion module mines the features of both Color and Thermal modalities to complement each other, then the global features are constructed by adding the cross-modal complemented features element by element, which are attentionally weighted to achieve the effective fusion of the two modal features. Finally, the fused features are input into the detection head to detect and locate pedestrians. Extensive experiments have been performed on two improved versions of annotations (sanitized annotations and paired annotations) of the public dataset KAIST. The experimental results show that our method demonstrates a lower pedestrian miss rate and more accurate pedestrian detection boxes compared to the comparison method. Additionally, the ablation experiment also proved the effectiveness of each module designed in this paper.



### Multimodal Subtask Graph Generation from Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2302.08672v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08672v1)
- **Published**: 2023-02-17 03:41:38+00:00
- **Updated**: 2023-02-17 03:41:38+00:00
- **Authors**: Yunseok Jang, Sungryull Sohn, Lajanugen Logeswaran, Tiange Luo, Moontae Lee, Honglak Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty pan needs to be washed before it can be used for cooking). In this work, we aim to model the causal dependencies between such subtasks from instructional videos describing the task. This is a challenging problem since complete information about the world is often inaccessible from videos, which demands robust learning mechanisms to understand the causal structure of events. We present Multimodal Subtask Graph Generation (MSG2), an approach that constructs a Subtask Graph defining the dependency between a task's subtasks relevant to a task from noisy web videos. Graphs generated by our multimodal approach are closer to human-annotated graphs compared to prior approaches. MSG2 further performs the downstream task of next subtask prediction 85% and 30% more accurately than recent video transformer models in the ProceL and CrossTask datasets, respectively.



### EnfoMax: Domain Entropy and Mutual Information Maximization for Domain Generalized Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2302.08674v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08674v2)
- **Published**: 2023-02-17 03:54:18+00:00
- **Updated**: 2023-06-04 11:28:48+00:00
- **Authors**: Tianyi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The face anti-spoofing (FAS) method performs well under intra-domain setups. However, its cross-domain performance is unsatisfactory. As a result, the domain generalization (DG) method has gained more attention in FAS. Existing methods treat FAS as a simple binary classification task and propose a heuristic training objective to learn domain-invariant features. However, there is no theoretical explanation of what a domain-invariant feature is. Additionally, the lack of theoretical support makes domain generalization techniques such as adversarial training lack training stability. To address these issues, this paper proposes the EnfoMax framework, which uses information theory to analyze cross-domain FAS tasks. This framework provides theoretical guarantees and optimization objectives for domain-generalized FAS tasks. EnfoMax maximizes the domain entropy and mutual information of live samples in source domains without using adversarial learning. Experimental results demonstrate that our approach performs well on extensive public datasets and outperforms state-of-the-art methods.



### Random Padding Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2302.08682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08682v1)
- **Published**: 2023-02-17 04:15:33+00:00
- **Updated**: 2023-02-17 04:15:33+00:00
- **Authors**: Nan Yang, Laicheng Zhong, Fan Huang, Dong Yuan, Wei Bao
- **Comment**: None
- **Journal**: None
- **Summary**: The convolutional neural network (CNN) learns the same object in different positions in images, which can improve the recognition accuracy of the model. An implication of this is that CNN may know where the object is. The usefulness of the features' spatial information in CNNs has not been well investigated. In this paper, we found that the model's learning of features' position information hindered the learning of the features' relationship. Therefore, we introduced Random Padding, a new type of padding method for training CNNs that impairs the architecture's capacity to learn position information by adding zero-padding randomly to half of the border of feature maps. Random Padding is parameter-free, simple to construct, and compatible with the majority of CNN-based recognition models. This technique is also complementary to data augmentations such as random cropping, rotation, flipping and erasing, and consistently improves the performance of image classification over strong baselines.



### Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2302.08689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08689v1)
- **Published**: 2023-02-17 04:42:19+00:00
- **Updated**: 2023-02-17 04:42:19+00:00
- **Authors**: Shengqin Wang, Yongji Zhang, Hong Qi, Minghao Zhao, Yu Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition relies on the extraction of spatial-temporal topological information. Hypergraphs can establish prior unnatural dependencies for the skeleton. However, the existing methods only focus on the construction of spatial topology and ignore the time-point dependence. This paper proposes a dynamic spatial-temporal hypergraph convolutional network (DST-HCN) to capture spatial-temporal information for skeleton-based action recognition. DST-HCN introduces a time-point hypergraph (TPH) to learn relationships at time points. With multiple spatial static hypergraphs and dynamic TPH, our network can learn more complete spatial-temporal features. In addition, we use the high-order information fusion module (HIF) to fuse spatial-temporal information synchronously. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets show that our model achieves state-of-the-art, especially compared with hypergraph methods.



### Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2302.08706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08706v2)
- **Published**: 2023-02-17 05:44:05+00:00
- **Updated**: 2023-02-20 09:38:50+00:00
- **Authors**: Haoran Sun, Yang Wang, Haipeng Liu, Biao Qian
- **Comment**: 13 pages, 8 figures, accepted by Chinese Journal of Electronics
- **Journal**: None
- **Summary**: Text-to-image synthesis refers to generating visual-realistic and semantically consistent images from given textual descriptions. Previous approaches generate an initial low-resolution image and then refine it to be high-resolution. Despite the remarkable progress, these methods are limited in fully utilizing the given texts and could generate text-mismatched images, especially when the text description is complex. We propose a novel Fine-grained text-image Fusion based Generative Adversarial Networks, dubbed FF-GAN, which consists of two modules: Fine-grained text-image Fusion Block (FF-Block) and Global Semantic Refinement (GSR). The proposed FF-Block integrates an attention block and several convolution layers to effectively fuse the fine-grained word-context features into the corresponding visual features, in which the text information is fully used to refine the initial image with more details. And the GSR is proposed to improve the global semantic consistency between linguistic and visual features during the refinement process. Extensive experiments on CUB-200 and COCO datasets demonstrate the superiority of FF-GAN over other state-of-the-art approaches in generating images with semantic consistency to the given texts.Code is available at https://github.com/haoranhfut/FF-GAN.



### Multimodal Propaganda Processing
- **Arxiv ID**: http://arxiv.org/abs/2302.08709v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08709v1)
- **Published**: 2023-02-17 05:49:55+00:00
- **Updated**: 2023-02-17 05:49:55+00:00
- **Authors**: Vincent Ng, Shengjie Li
- **Comment**: Accepted in AAAI 2023
- **Journal**: None
- **Summary**: Propaganda campaigns have long been used to influence public opinion via disseminating biased and/or misleading information. Despite the increasing prevalence of propaganda content on the Internet, few attempts have been made by AI researchers to analyze such content. We introduce the task of multimodal propaganda processing, where the goal is to automatically analyze propaganda content. We believe that this task presents a long-term challenge to AI researchers and that successful processing of propaganda could bring machine understanding one important step closer to human understanding. We discuss the technical challenges associated with this task and outline the steps that need to be taken to address it.



### Binary Embedding-based Retrieval at Tencent
- **Arxiv ID**: http://arxiv.org/abs/2302.08714v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08714v1)
- **Published**: 2023-02-17 06:10:02+00:00
- **Updated**: 2023-02-17 06:10:02+00:00
- **Authors**: Yukang Gan, Yixiao Ge, Chang Zhou, Shupeng Su, Zhouchuan Xu, Xuyuan Xu, Quanchao Hui, Xiang Chen, Yexin Wang, Ying Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale embedding-based retrieval (EBR) is the cornerstone of search-related industrial applications. Given a user query, the system of EBR aims to identify relevant information from a large corpus of documents that may be tens or hundreds of billions in size. The storage and computation turn out to be expensive and inefficient with massive documents and high concurrent queries, making it difficult to further scale up. To tackle the challenge, we propose a binary embedding-based retrieval (BEBR) engine equipped with a recurrent binarization algorithm that enables customized bits per dimension. Specifically, we compress the full-precision query and document embeddings, formulated as float vectors in general, into a composition of multiple binary vectors using a lightweight transformation model with residual multilayer perception (MLP) blocks. We can therefore tailor the number of bits for different applications to trade off accuracy loss and cost savings. Importantly, we enable task-agnostic efficient training of the binarization model using a new embedding-to-embedding strategy. We also exploit the compatible training of binary embeddings so that the BEBR engine can support indexing among multiple embedding versions within a unified system. To further realize efficient search, we propose Symmetric Distance Calculation (SDC) to achieve lower response time than Hamming codes. We successfully employed the introduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World, etc. The binarization algorithm can be seamlessly generalized to various tasks with multiple modalities. Extensive experiments on offline benchmarks and online A/B tests demonstrate the efficiency and effectiveness of our method, significantly saving 30%~50% index costs with almost no loss of accuracy at the system level.



### EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2302.08715v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08715v2)
- **Published**: 2023-02-17 06:14:37+00:00
- **Updated**: 2023-08-27 10:08:54+00:00
- **Authors**: Zicheng Zhang, Wei Sun, Yingjie Zhou, Wei Lu, Yucheng Zhu, Xiongkuo Min, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, great numbers of efforts have been put into improving the effectiveness of 3D model quality assessment (3DQA) methods. However, little attention has been paid to the computational costs and inference time, which is also important for practical applications. Unlike 2D media, 3D models are represented by more complicated and irregular digital formats, such as point cloud and mesh. Thus it is normally difficult to perform an efficient module to extract quality-aware features of 3D models. In this paper, we address this problem from the aspect of projection-based 3DQA and develop a no-reference (NR) \underline{E}fficient and \underline{E}ffective \underline{P}rojection-based \underline{3D} Model \underline{Q}uality \underline{A}ssessment (\textbf{EEP-3DQA}) method. The input projection images of EEP-3DQA are randomly sampled from the six perpendicular viewpoints of the 3D model and are further spatially downsampled by the grid-mini patch sampling strategy. Further, the lightweight Swin-Transformer tiny is utilized as the backbone to extract the quality-aware features. Finally, the proposed EEP-3DQA and EEP-3DQA-t (tiny version) achieve the best performance than the existing state-of-the-art NR-3DQA methods and even outperforms most full-reference (FR) 3DQA methods on the point cloud and mesh quality assessment databases while consuming less inference time than the compared 3DQA methods.



### Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales
- **Arxiv ID**: http://arxiv.org/abs/2302.08720v2
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08720v2)
- **Published**: 2023-02-17 06:29:12+00:00
- **Updated**: 2023-07-27 00:09:30+00:00
- **Authors**: Nicolaas J. Annau, Alex J. Cannon, Adam H. Monahan
- **Comment**: 43 pages, including 11 main figures, and 16 supplemental figures
- **Journal**: None
- **Summary**: This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations influence spatial structures in the generated fields, particularly biases in spatial variability spectra. Using power spectra to evaluate the FS experiments reveals that successful applications of FS in computer vision do not translate to climate fields. However, the FS experiments demonstrate the sensitivity of power spectra to a commonly used GAN-based SR objective function, which helps interpret and understand its role in determining spatial structures. This result motivates the development of a novel partial frequency-separation scheme as a promising configuration option. We also quantify the influence on GAN performance of non-idealized LR fields resulting from internal variability. Furthermore, we conduct a spectra-based feature-importance experiment allowing us to explore the dependence of the spatial structure of generated fields on different physically relevant LR covariates.



### GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2302.08722v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.08722v3)
- **Published**: 2023-02-17 06:33:06+00:00
- **Updated**: 2023-03-21 12:59:20+00:00
- **Authors**: Yizhe Zhang, Danny Z. Chen
- **Comment**: Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large language models for broader MIA applications.



### New Insights for the Stability-Plasticity Dilemma in Online Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.08741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08741v1)
- **Published**: 2023-02-17 07:43:59+00:00
- **Updated**: 2023-02-17 07:43:59+00:00
- **Authors**: Dahuin Jung, Dongjin Lee, Sunwon Hong, Hyemi Jang, Ho Bae, Sungroh Yoon
- **Comment**: Accepted to ICLR2023
- **Journal**: None
- **Summary**: The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.



### MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model
- **Arxiv ID**: http://arxiv.org/abs/2302.08751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08751v2)
- **Published**: 2023-02-17 08:29:33+00:00
- **Updated**: 2023-05-08 12:22:30+00:00
- **Authors**: Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak
- **Comment**: UAI 2023
- **Journal**: None
- **Summary**: One of the major challenges in multi-person pose estimation is instance-aware keypoint estimation. Previous methods address this problem by leveraging an off-the-shelf detector, heuristic post-grouping process or explicit instance identification process, hindering further improvements in the inference speed which is an important factor for practical applications. From the statistical point of view, those additional processes for identifying instances are necessary to bypass learning the high-dimensional joint distribution of human keypoints, which is a critical factor for another major challenge, the occlusion scenario. In this work, we propose a novel framework of single-stage instance-aware pose estimation by modeling the joint distribution of human keypoints with a mixture density model, termed as MDPose. Our MDPose estimates the distribution of human keypoints' coordinates using a mixture density model with an instance-aware keypoint head consisting simply of 8 convolutional layers. It is trained by minimizing the negative log-likelihood of the ground truth keypoints. Also, we propose a simple yet effective training strategy, Random Keypoint Grouping (RKG), which significantly alleviates the underflow problem leading to successful learning of relations between keypoints. On OCHuman dataset, which consists of images with highly occluded people, our MDPose achieves state-of-the-art performance by successfully learning the high-dimensional joint distribution of human keypoints. Furthermore, our MDPose shows significant improvement in inference speed with a competitive accuracy on MS COCO, a widely-used human keypoint dataset, thanks to the proposed much simpler single-stage pipeline.



### A Pervasive Framework for Human Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2303.11170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.11170v1)
- **Published**: 2023-02-17 08:35:39+00:00
- **Updated**: 2023-02-17 08:35:39+00:00
- **Authors**: Fesatidis Georgios, Bratsos Dimitrios, Kostas Kolomvatsos
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of the Edge Computing (EC) leads to a huge ecosystem where numerous nodes can interact with data collection devices located close to end users. Human detection and tracking can be realized at edge nodes that perform the surveillance of an area under consideration through the assistance of a set of sensors (e.g., cameras). Our target is to incorporate the discussed functionalities to embedded devices present at the edge keeping their size limited while increasing their processing capabilities. In this paper, we propose two models for human detection accompanied by algorithms for tracing the corresponding trajectories. We provide the description of the proposed models and extend them to meet the challenges of the problem. Our evaluation aims at identifying models' accuracy while presenting their requirements to have them executed in embedded devices.



### 3D Human Pose Lifting with Grid Convolution
- **Arxiv ID**: http://arxiv.org/abs/2302.08760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08760v1)
- **Published**: 2023-02-17 08:52:16+00:00
- **Updated**: 2023-02-17 08:52:16+00:00
- **Authors**: Yangyuxuan Kang, Yuyang Liu, Anbang Yao, Shandong Wang, Enhua Wu
- **Comment**: Oral paper at AAAI 2023. Project website:
  https://github.com/OSVAI/GridConv
- **Journal**: None
- **Summary**: Existing lifting networks for regressing 3D human poses from 2D single-view poses are typically constructed with linear layers based on graph-structured representation learning. In sharp contrast to them, this paper presents Grid Convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is based on a novel Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map the irregular graph-structured human pose onto a regular weave-like grid pose representation joint by joint, enabling layer-wise feature learning with GridConv operations. We provide two ways to implement SGT, including handcrafted and learnable designs. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. We show that our fully convolutional grid lifting network outperforms state-of-the-art methods with noticeable margins under (1) conventional evaluation on Human3.6M and (2) cross-evaluation on MPI-INF-3DHP. Code is available at https://github.com/OSVAI/GridConv



### Adversarial Contrastive Distillation with Adaptive Denoising
- **Arxiv ID**: http://arxiv.org/abs/2302.08764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08764v2)
- **Published**: 2023-02-17 09:00:18+00:00
- **Updated**: 2023-02-23 04:18:05+00:00
- **Authors**: Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yang Liu, Siao Liu, Wenqiang Zhang, Lizhe Qi
- **Comment**: accepted for ICASSP 2023
- **Journal**: None
- **Summary**: Adversarial Robustness Distillation (ARD) is a novel method to boost the robustness of small models. Unlike general adversarial training, its robust knowledge transfer can be less easily restricted by the model capacity. However, the teacher model that provides the robustness of knowledge does not always make correct predictions, interfering with the student's robust performances. Besides, in the previous ARD methods, the robustness comes entirely from one-to-one imitation, ignoring the relationship between examples. To this end, we propose a novel structured ARD method called Contrastive Relationship DeNoise Distillation (CRDND). We design an adaptive compensation module to model the instability of the teacher. Moreover, we utilize the contrastive relationship to explore implicit robustness knowledge among multiple examples. Experimental results on multiple attack benchmarks show CRDND can transfer robust knowledge efficiently and achieves state-of-the-art performances.



### On the Regularising Levenberg-Marquardt Method for Blinn-Phong Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/2302.08765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08765v1)
- **Published**: 2023-02-17 09:01:24+00:00
- **Updated**: 2023-02-17 09:01:24+00:00
- **Authors**: Georg Radow, Michael Breuß
- **Comment**: 11 pages, 3 figure, to be published in proceedings of OAGM 2022
- **Journal**: None
- **Summary**: Photometric stereo refers to the process to compute the 3D shape of an object using information on illumination and reflectance from several input images from the same point of view. The most often used reflectance model is the Lambertian reflectance, however this does not include specular highlights in input images. In this paper we consider the arising non-linear optimisation problem when employing Blinn-Phong reflectance for modeling specular effects. To this end we focus on the regularising Levenberg-Marquardt scheme. We show how to derive an explicit bound that gives information on the convergence reliability of the method depending on given data, and we show how to gain experimental evidence of numerical correctness of the iteration by making use of the Scherzer condition. The theoretical investigations that are at the heart of this paper are supplemented by some tests with real-world imagery.



### Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization
- **Arxiv ID**: http://arxiv.org/abs/2302.08769v1
- **DOI**: 10.1109/TII.2023.3241579
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.08769v1)
- **Published**: 2023-02-17 09:05:22+00:00
- **Updated**: 2023-02-17 09:05:22+00:00
- **Authors**: Yunkang Cao, Xiaohao Xu, Zhaoge Liu, Weiming Shen
- **Comment**: Accepted by IEEE Transactions on Industrial Informatics
- **Journal**: IEEE Transactions on Industrial Informatics
- **Summary**: Most unsupervised image anomaly localization methods suffer from overgeneralization because of the high generalization abilities of convolutional neural networks, leading to unreliable predictions. To mitigate the overgeneralization, this study proposes to collaboratively optimize normal and abnormal feature distributions with the assistance of synthetic anomalies, namely collaborative discrepancy optimization (CDO). CDO introduces a margin optimization module and an overlap optimization module to optimize the two key factors determining the localization performance, i.e., the margin and the overlap between the discrepancy distributions (DDs) of normal and abnormal samples. With CDO, a large margin and a small overlap between normal and abnormal DDs are obtained, and the prediction reliability is boosted. Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the overgeneralization and achieves great anomaly localization performance with real-time computation efficiency. A real-world automotive plastic parts inspection application further demonstrates the capability of the proposed CDO. Code is available on https://github.com/caoyunkang/CDO.



### Explicit and Implicit Knowledge Distillation via Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2302.08771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08771v2)
- **Published**: 2023-02-17 09:10:41+00:00
- **Updated**: 2023-02-23 04:14:09+00:00
- **Authors**: Yuzheng Wang, Zuhao Ge, Zhaoyu Chen, Xian Liu, Chuangjia Ma, Yunquan Sun, Lizhe Qi
- **Comment**: accepted for ICASSP 2023
- **Journal**: None
- **Summary**: Data-free knowledge distillation is a challenging model lightweight task for scenarios in which the original dataset is not available. Previous methods require a lot of extra computational costs to update one or more generators and their naive imitate-learning lead to lower distillation efficiency. Based on these observations, we first propose an efficient unlabeled sample selection method to replace high computational generators and focus on improving the training efficiency of the selected samples. Then, a class-dropping mechanism is designed to suppress the label noise caused by the data domain shifts. Finally, we propose a distillation method that incorporates explicit features and implicit structured relations to improve the effect of distillation. Experimental results show that our method can quickly converge and obtain higher accuracy than other state-of-the-art methods.



### Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach
- **Arxiv ID**: http://arxiv.org/abs/2302.10798v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10798v2)
- **Published**: 2023-02-17 09:37:17+00:00
- **Updated**: 2023-05-26 12:29:59+00:00
- **Authors**: Xiaoying Zhi, Varun Babbar, Pheobe Sun, Fran Silavong, Ruibo Shi, Sean Moran
- **Comment**: None
- **Journal**: None
- **Summary**: The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and training simultaneously, which saves energy in both the training and inference phases and avoids extra computational overhead from gating modules at inference time. Our results on CIFAR-10 and CIFAR-100 suggest that our scheme can remove 50% of connections in deep networks with 1% reduction in classification accuracy. Compared to other related pruning methods, our method demonstrates a lower drop in accuracy for equivalent reductions in computational cost.



### Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2302.08785v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08785v2)
- **Published**: 2023-02-17 09:52:36+00:00
- **Updated**: 2023-03-03 10:07:50+00:00
- **Authors**: Jilin Mei, Junbao Zhou, Yu Hu
- **Comment**: Accepted by ICRA 2023
- **Journal**: None
- **Summary**: In autonomous driving, the novel objects and lack of annotations challenge the traditional 3D LiDAR semantic segmentation based on deep learning. Few-shot learning is a feasible way to solve these issues. However, currently few-shot semantic segmentation methods focus on camera data, and most of them only predict the novel classes without considering the base classes. This setting cannot be directly applied to autonomous driving due to safety concerns. Thus, we propose a few-shot 3D LiDAR semantic segmentation method that predicts both novel classes and base classes simultaneously. Our method tries to solve the background ambiguity problem in generalized few-shot semantic segmentation. We first review the original cross-entropy and knowledge distillation losses, then propose a new loss function that incorporates the background information to achieve 3D LiDAR few-shot semantic segmentation. Extensive experiments on SemanticKITTI demonstrate the effectiveness of our method.



### MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs
- **Arxiv ID**: http://arxiv.org/abs/2302.08788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08788v2)
- **Published**: 2023-02-17 10:07:35+00:00
- **Updated**: 2023-04-12 05:15:11+00:00
- **Authors**: Seunghyeon Seo, Donghoon Han, Yeonjin Chang, Nojun Kwak
- **Comment**: CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.



### Risk Classification of Brain Metastases via Radiomics, Delta-Radiomics and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.08802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08802v1)
- **Published**: 2023-02-17 10:55:18+00:00
- **Updated**: 2023-02-17 10:55:18+00:00
- **Authors**: Philipp Sommer, Yixing Huang, Christoph Bert, Andreas Maier, Manuel Schmidt, Arnd Dörfler, Rainer Fietkau, Florian Putz
- **Comment**: None
- **Journal**: None
- **Summary**: Stereotactic radiotherapy (SRT) is one of the most important treatment for patients with brain metastases (BM). Conventionally, following SRT patients are monitored by serial imaging and receive salvage treatments in case of significant tumor growth. We hypothesized that using radiomics and machine learning (ML), metastases at high risk for subsequent progression could be identified during follow-up prior to the onset of significant tumor growth, enabling personalized follow-up intervals and early selection for salvage treatment. All experiments are performed on a dataset from clinical routine of the Radiation Oncology department of the University Hospital Erlangen (UKER). The classification is realized via the maximum-relevance minimal-redundancy (MRMR) technique and support vector machines (SVM). The pipeline leads to a classification with a mean area under the curve (AUC) score of 0.83 in internal cross-validation and allows a division of the cohort into two subcohorts that differ significantly in their median time to progression (low-risk metastasis (LRM): 17.3 months, high-risk metastasis (HRM): 9.6 months, p < 0.01). The classification performance is especially enhanced by the analysis of medical images from different points in time (AUC 0.53 -> AUC 0.74). The results indicate that risk stratification of BM based on radiomics and machine learning during post-SRT follow-up is possible with good accuracy and should be further pursued to personalize and improve post-SRT follow-up.



### Paint it Black: Generating paintings from text descriptions
- **Arxiv ID**: http://arxiv.org/abs/2302.08808v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08808v1)
- **Published**: 2023-02-17 11:07:53+00:00
- **Updated**: 2023-02-17 11:07:53+00:00
- **Authors**: Mahnoor Shahid, Mark Koch, Niklas Schneider
- **Comment**: None
- **Journal**: None
- **Summary**: Two distinct tasks - generating photorealistic pictures from given text prompts and transferring the style of a painting to a real image to make it appear as though it were done by an artist, have been addressed many times, and several approaches have been proposed to accomplish them. However, the intersection of these two, i.e., generating paintings from a given caption, is a relatively unexplored area with little data available. In this paper, we have explored two distinct strategies and have integrated them together. First strategy is to generate photorealistic images and then apply style transfer and the second strategy is to train an image generation model on real images with captions and then fine-tune it on captioned paintings later. These two models are evaluated using different metrics as well as a user study is conducted to get human feedback on the produced results.



### Apple scab detection in orchards using deep learning on colour and multispectral images
- **Arxiv ID**: http://arxiv.org/abs/2302.08818v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2302.08818v1)
- **Published**: 2023-02-17 11:33:17+00:00
- **Updated**: 2023-02-17 11:33:17+00:00
- **Authors**: Robert Rouš, Joseph Peller, Gerrit Polder, Selwin Hageraats, Thijs Ruigrok, Pieter M. Blok
- **Comment**: 6 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: Apple scab is a fungal disease caused by Venturia inaequalis. Disease is of particular concern for growers, as it causes significant damage to fruit and leaves, leading to loss of fruit and yield. This article examines the ability of deep learning and hyperspectral imaging to accurately identify an apple symptom infection in apple trees. In total, 168 image scenes were collected using conventional RGB and Visible to Near-infrared (VIS-NIR) spectral imaging (8 channels) in infected orchards. Spectral data were preprocessed with an Artificial Neural Network (ANN) trained in segmentation to detect scab pixels based on spectral information. Linear Discriminant Analysis (LDA) was used to find the most discriminating channels in spectral data based on the healthy leaf and scab infested leaf spectra. Five combinations of false-colour images were created from the spectral data and the segmentation net results. The images were trained and evaluated with a modified version of the YOLOv5 network. Despite the promising results of deep learning using RGB images (P=0.8, mAP@50=0.73), the detection of apple scab in apple trees using multispectral imaging proved to be a difficult task. The high-light environment of the open field made it difficult to collect a balanced spectrum from the multispectral camera, since the infrared channel and the visible channels needed to be constantly balanced so that they did not overexpose in the images.



### Lip-to-Speech Synthesis in the Wild with Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.08841v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2302.08841v1)
- **Published**: 2023-02-17 12:31:26+00:00
- **Updated**: 2023-02-17 12:31:26+00:00
- **Authors**: Minsu Kim, Joanna Hong, Yong Man Ro
- **Comment**: Accepted at ICASSP 2023. Demo available:
  https://github.com/joannahong/Lip-to-Speech-Synthesis-in-the-Wild
- **Journal**: None
- **Summary**: Recent studies have shown impressive performance in Lip-to-speech synthesis that aims to reconstruct speech from visual information alone. However, they have been suffering from synthesizing accurate speech in the wild, due to insufficient supervision for guiding the model to infer the correct content. Distinct from the previous methods, in this paper, we develop a powerful Lip2Speech method that can reconstruct speech with correct contents from the input lip movements, even in a wild environment. To this end, we design multi-task learning that guides the model using multimodal supervision, i.e., text and audio, to complement the insufficient word representations of acoustic feature reconstruction loss. Thus, the proposed framework brings the advantage of synthesizing speech containing the right content of multiple speakers with unconstrained sentences. We verify the effectiveness of the proposed method using LRS2, LRS3, and LRW datasets.



### PhaseNet: Phase-Encode Denoising Network for Compressed Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/2302.08861v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08861v1)
- **Published**: 2023-02-17 13:16:17+00:00
- **Updated**: 2023-02-17 13:16:17+00:00
- **Authors**: Marlon E. Bran Lorenzana, Shekhar S. Chandra, Feng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse reconstruction is an important aspect of modern medical imaging, reducing the acquisition time of relatively slow modalities such as magnetic resonance imaging (MRI). Popular methods are based mostly on compressed sensing (CS), which relies on the random sampling of Fourier coefficients ($k$-space) to produce incoherent (noise-like) artefacts that can be removed via convex optimisation. Hardware constraints currently limit Cartesian CS to one dimensional (1D) phase-encode undersampling schemes, leading to coherent and structured artefacts. Reconstruction algorithms typically deploy an idealised and limited 2D regularisation for artefact removal, which increases the difficulty of image recovery. Recognising that phase-encode artefacts can be separated into contiguous 1D signals, we develop two decoupling techniques that enable explicit 1D regularisation. We thereby leverage the excellent incoherence characteristics in the phase-encode direction. We also derive a combined 1D + 2D reconstruction technique that further takes advantage of spatial relationships within the image, leading to an improvement of existing 2D deep-learned (DL) recovery techniques. Performance is evaluated on a brain and knee dataset. We find the proposed 1D CNN modules significantly improve PSNR and SSIM scores compared to the base 2D models, demonstrating a superior scaling of performance compared to increasing the size of 2D network layers.



### Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning
- **Arxiv ID**: http://arxiv.org/abs/2302.08867v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08867v2)
- **Published**: 2023-02-17 13:28:06+00:00
- **Updated**: 2023-02-22 02:09:51+00:00
- **Authors**: Jack Breen, Katie Allen, Kieran Zucker, Geoff Hall, Nicolas M. Orsi, Nishant Ravikumar
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly-supervised classification of histopathology slides is a computationally intensive task, with a typical whole slide image (WSI) containing billions of pixels to process. We propose Discriminative Region Active Sampling for Multiple Instance Learning (DRAS-MIL), a computationally efficient slide classification method using attention scores to focus sampling on highly discriminative regions. We apply this to the diagnosis of ovarian cancer histological subtypes, which is an essential part of the patient care pathway as different subtypes have different genetic and molecular profiles, treatment options, and patient outcomes. We use a dataset of 714 WSIs acquired from 147 epithelial ovarian cancer patients at Leeds Teaching Hospitals NHS Trust to distinguish the most common subtype, high-grade serous carcinoma, from the other four subtypes (low-grade serous, endometrioid, clear cell, and mucinous carcinomas) combined. We demonstrate that DRAS-MIL can achieve similar classification performance to exhaustive slide analysis, with a 3-fold cross-validated AUC of 0.8679 compared to 0.8781 with standard attention-based MIL classification. Our approach uses at most 18% as much memory as the standard approach, while taking 33% of the time when evaluating on a GPU and only 14% on a CPU alone. Reducing prediction time and memory requirements may benefit clinical deployment and the democratisation of AI, reducing the extent to which computational hardware limits end-user adoption.



### Less is More: The Influence of Pruning on the Explainability of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2302.08878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.08878v1)
- **Published**: 2023-02-17 13:50:53+00:00
- **Updated**: 2023-02-17 13:50:53+00:00
- **Authors**: David Weber, Florian Merkle, Pascal Schöttle, Stephan Schlögl
- **Comment**: None
- **Journal**: None
- **Summary**: Modern, state-of-the-art Convolutional Neural Networks (CNNs) in computer vision have millions of parameters. Thus, explaining the complex decisions of such networks to humans is challenging. A technical approach to reduce CNN complexity is network pruning, where less important parameters are deleted. The work presented in this paper investigates whether this technical complexity reduction also helps with perceived explainability. To do so, we conducted a pre-study and two human-grounded experiments, assessing the effects of different pruning ratios on CNN explainability. Overall, we evaluated four different compression rates (i.e., CPR 2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that lower compression rates have a positive influence on explainability, while higher compression rates show negative effects. Furthermore, we were able to identify sweet spots that increase both the perceived explainability and the model's performance.



### Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2302.08890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08890v2)
- **Published**: 2023-02-17 14:19:28+00:00
- **Updated**: 2023-07-14 07:51:42+00:00
- **Authors**: Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, Lin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are bio-inspired sensors that capture the per-pixel intensity changes asynchronously and produce event streams encoding the time, pixel position, and polarity (sign) of the intensity changes. Event cameras possess a myriad of advantages over canonical frame-based cameras, such as high temporal resolution, high dynamic range, low latency, etc. Being capable of capturing information in challenging visual conditions, event cameras have the potential to overcome the limitations of frame-based cameras in the computer vision and robotics community. In very recent years, deep learning (DL) has been brought to this emerging field and inspired active research endeavors in mining its potential. However, there is still a lack of taxonomies in DL techniques for event-based vision. We first scrutinize the typical event representations with quality enhancement methods as they play a pivotal role as inputs to the DL models. We then provide a comprehensive taxonomy for existing DL-based methods by structurally grouping them into two major categories: 1) image reconstruction and restoration; 2) event-based scene understanding and 3D vision. Importantly, we conduct benchmark experiments for the existing methods in some representative research directions (eg, object recognition) to identify some critical insights and problems. Finally, we make important discussions regarding the challenges and provide new perspectives for inspiring more research studies.



### LDFA: Latent Diffusion Face Anonymization for Self-driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2302.08931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08931v1)
- **Published**: 2023-02-17 15:14:00+00:00
- **Updated**: 2023-02-17 15:14:00+00:00
- **Authors**: Marvin Klemp, Kevin Rösch, Royden Wagner, Jannik Quehl, Martin Lauer
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: In order to protect vulnerable road users (VRUs), such as pedestrians or cyclists, it is essential that intelligent transportation systems (ITS) accurately identify them. Therefore, datasets used to train perception models of ITS must contain a significant number of vulnerable road users. However, data protection regulations require that individuals are anonymized in such datasets. In this work, we introduce a novel deep learning-based pipeline for face anonymization in the context of ITS. In contrast to related methods, we do not use generative adversarial networks (GANs) but build upon recent advances in diffusion models. We propose a two-stage method, which contains a face detection model followed by a latent diffusion model to generate realistic face in-paintings. To demonstrate the versatility of anonymized images, we train segmentation methods on anonymized data and evaluate them on non-anonymized data. Our experiment reveal that our pipeline is better suited to anonymize data for segmentation than naive methods and performes comparably with recent GAN-based methods. Moreover, face detectors achieve higher mAP scores for faces anonymized by our method compared to naive or recent GAN-based methods.



### Long Range Object-Level Monocular Depth Estimation for UAVs
- **Arxiv ID**: http://arxiv.org/abs/2302.08943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08943v1)
- **Published**: 2023-02-17 15:26:04+00:00
- **Updated**: 2023-02-17 15:26:04+00:00
- **Authors**: David Silva, Nicolas Jourdan, Nils Gählert
- **Comment**: 16 pages, SCIA 2023
- **Journal**: None
- **Summary**: Computer vision-based object detection is a key modality for advanced Detect-And-Avoid systems that allow for autonomous flight missions of UAVs. While standard object detection frameworks do not predict the actual depth of an object, this information is crucial to avoid collisions. In this paper, we propose several novel extensions to state-of-the-art methods for monocular object detection from images at long range. Firstly, we propose Sigmoid and ReLU-like encodings when modeling depth estimation as a regression task. Secondly, we frame the depth estimation as a classification problem and introduce a Soft-Argmax function in the calculation of the training loss. The extensions are exemplarily applied to the YOLOX object detection framework. We evaluate the performance using the Amazon Airborne Object Tracking dataset. In addition, we introduce the Fitness score as a new metric that jointly assesses both object detection and depth estimation performance. Our results show that the proposed methods outperform state-of-the-art approaches w.r.t. existing, as well as the proposed metrics.



### Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization
- **Arxiv ID**: http://arxiv.org/abs/2302.08947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08947v1)
- **Published**: 2023-02-17 15:30:13+00:00
- **Updated**: 2023-02-17 15:30:13+00:00
- **Authors**: Shinnosuke Matsuo, Ryoma Bise, Seiichi Uchida, Daiki Suehiro
- **Comment**: Accepted at ICASSP2023
- **Journal**: None
- **Summary**: This paper proposes a novel and efficient method for Learning from Label Proportions (LLP), whose goal is to train a classifier only by using the class label proportions of instance sets, called bags. We propose a novel LLP method based on an online pseudo-labeling method with regret minimization. As opposed to the previous LLP methods, the proposed method effectively works even if the bag sizes are large. We demonstrate the effectiveness of the proposed method using some benchmark datasets.



### Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories
- **Arxiv ID**: http://arxiv.org/abs/2302.08948v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08948v1)
- **Published**: 2023-02-17 15:30:44+00:00
- **Updated**: 2023-02-17 15:30:44+00:00
- **Authors**: Bertrand Duménieu, Edwin Carlinet, Nathalie Abadie, Joseph Chazalon
- **Comment**: None
- **Journal**: None
- **Summary**: When extracting structured data from repetitively organized documents, such as dictionaries, directories, or even newspapers, a key challenge is to correctly segment what constitutes the basic text regions for the target database. Traditionally, such a problem was tackled as part of the layout analysis and was mostly based on visual clues for dividing (top-down) approaches. Some agglomerating (bottom-up) approaches started to consider textual information to link similar contents, but they required a proper over-segmentation of fine-grained units. In this work, we propose a new pragmatic approach whose efficiency is demonstrated on 19th century French Trade Directories. We propose to consider two sub-problems: coarse layout detection (text columns and reading order), which is assumed to be effective and not detailed here, and a fine-grained entry separation stage for which we propose to adapt a state-of-the-art Named Entity Recognition (NER) approach. By injecting special visual tokens, coding, for instance, indentation or breaks, into the token stream of the language model used for NER purpose, we can leverage both textual and visual knowledge simultaneously. Code, data, results and models are available at https://github.com/soduco/paper-entryseg-icdar23-code, https://huggingface.co/HueyNemud/ (icdar23-entrydetector* variants)



### Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts
- **Arxiv ID**: http://arxiv.org/abs/2302.08958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08958v1)
- **Published**: 2023-02-17 15:43:42+00:00
- **Updated**: 2023-02-17 15:43:42+00:00
- **Authors**: Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical types, \textit{i.e.}, the fusion-encoder type and the dual-encoder type, depending on whether a heavy fusion module is used. The former is superior at multi-modal tasks owing to the sufficient interaction between modalities; the latter is good at uni-modal and cross-modal tasks due to the single-modality encoding ability. To take advantage of these two types, we propose an effective yet straightforward scheme named PTUnifier to unify the two types. We first unify the input format by introducing visual and textual prompts, which serve as a feature bank that stores the most representative images/texts. By doing so, a single model could serve as a \textit{foundation model} that processes various tasks adopting different input formats (\textit{i.e.}, image-only, text-only, and image-text-pair). Furthermore, we construct a prompt pool (instead of static ones) to improve diversity and scalability. Experimental results show that our approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (\textit{i.e.}, image/text classification and text summarization), cross-modal tasks (\textit{i.e.}, image-to-text generation and image-text/text-image retrieval), and multi-modal tasks (\textit{i.e.}, visual question answering), demonstrating the effectiveness of our approach. Note that the adoption of prompts is orthogonal to most existing Med-VLP approaches and could be a beneficial and complementary extension to these approaches.



### sMRI-PatchNet: A novel explainable patch-based deep learning network for Alzheimer's disease diagnosis and discriminative atrophy localisation with Structural MRI
- **Arxiv ID**: http://arxiv.org/abs/2302.08967v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08967v2)
- **Published**: 2023-02-17 16:01:15+00:00
- **Updated**: 2023-02-20 02:03:55+00:00
- **Authors**: Xin Zhang, Liangxiu Han, Lianghao Han, Haoming Chen, Darren Dancey, Daoqiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Structural magnetic resonance imaging (sMRI) can identify subtle brain changes due to its high contrast for soft tissues and high spatial resolution. It has been widely used in diagnosing neurological brain diseases, such as Alzheimer disease (AD). However, the size of 3D high-resolution data poses a significant challenge for data analysis and processing. Since only a few areas of the brain show structural changes highly associated with AD, the patch-based methods dividing the whole image data into several small regular patches have shown promising for more efficient sMRI-based image analysis. The major challenges of the patch-based methods on sMRI include identifying the discriminative patches, combining features from the discrete discriminative patches, and designing appropriate classifiers. This work proposes a novel patch-based deep learning network (sMRI-PatchNet) with explainable patch localisation and selection for AD diagnosis using sMRI. Specifically, it consists of two primary components: 1) A fast and efficient explainable patch selection mechanism for determining the most discriminative patches based on computing the SHapley Additive exPlanations (SHAP) contribution to a transfer learning model for AD diagnosis on massive medical data; and 2) A novel patch-based network for extracting deep features and AD classfication from the selected patches with position embeddings to retain position information, capable of capturing the global and local information of inter- and intra-patches. This method has been applied for the AD classification and the prediction of the transitional state moderate cognitive impairment (MCI) conversion with real datasets.



### Model Doctor for Diagnosing and Treating Segmentation Error
- **Arxiv ID**: http://arxiv.org/abs/2302.08980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.08980v2)
- **Published**: 2023-02-17 16:35:24+00:00
- **Updated**: 2023-02-23 02:02:32+00:00
- **Authors**: Zhijie Jia, Lin Chen, Kaiwen Hu, Lechao Cheng, Zunlei Feng, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the remarkable progress in semantic segmentation tasks with the advancement of deep neural networks, existing U-shaped hierarchical typical segmentation networks still suffer from local misclassification of categories and inaccurate target boundaries. In an effort to alleviate this issue, we propose a Model Doctor for semantic segmentation problems. The Model Doctor is designed to diagnose the aforementioned problems in existing pre-trained models and treat them without introducing additional data, with the goal of refining the parameters to achieve better performance. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our method. Code is available at \url{https://github.com/zhijiejia/SegDoctor}.



### CovidExpert: A Triplet Siamese Neural Network framework for the detection of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2302.09004v1
- **DOI**: 10.1016/j.imu.2022.101156
- **Categories**: **cs.CV**, cs.LG, I.2.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2302.09004v1)
- **Published**: 2023-02-17 17:18:02+00:00
- **Updated**: 2023-02-17 17:18:02+00:00
- **Authors**: Tareque Rahman Ornob, Gourab Roy, Enamul Hassan
- **Comment**: None
- **Journal**: Informatics in Medicine Unlocked, 37, 2023, 101156
- **Summary**: Patients with the COVID-19 infection may have pneumonia-like symptoms as well as respiratory problems which may harm the lungs. From medical images, coronavirus illness may be accurately identified and predicted using a variety of machine learning methods. Most of the published machine learning methods may need extensive hyperparameter adjustment and are unsuitable for small datasets. By leveraging the data in a comparatively small dataset, few-shot learning algorithms aim to reduce the requirement of large datasets. This inspired us to develop a few-shot learning model for early detection of COVID-19 to reduce the post-effect of this dangerous disease. The proposed architecture combines few-shot learning with an ensemble of pre-trained convolutional neural networks to extract feature vectors from CT scan images for similarity learning. The proposed Triplet Siamese Network as the few-shot learning model classified CT scan images into Normal, COVID-19, and Community-Acquired Pneumonia. The suggested model achieved an overall accuracy of 98.719%, a specificity of 99.36%, a sensitivity of 98.72%, and a ROC score of 99.9% with only 200 CT scans per category for training data.



### Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences
- **Arxiv ID**: http://arxiv.org/abs/2302.09018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09018v2)
- **Published**: 2023-02-17 17:35:05+00:00
- **Updated**: 2023-02-22 07:47:39+00:00
- **Authors**: Yujie Zhou, Haodong Duan, Anyi Rao, Bing Su, Jiaqi Wang
- **Comment**: Accepted by AAAI 2023(Oral)
- **Journal**: None
- **Summary**: Self-supervised learning has demonstrated remarkable capability in representation learning for skeleton-based action recognition. Existing methods mainly focus on applying global data augmentation to generate different views of the skeleton sequence for contrastive learning. However, due to the rich action clues in the skeleton sequences, existing methods may only take a global perspective to learn to discriminate different skeletons without thoroughly leveraging the local relationship between different skeleton joints and video frames, which is essential for real-world applications. In this work, we propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the local relationship from a partial skeleton sequences built by a unique spatio-temporal masking strategy. Specifically, we construct a negative-sample-free triplet steam structure that is composed of an anchor stream without any masking, a spatial masking stream with Central Spatial Masking (CSM), and a temporal masking stream with Motion Attention Temporal Masking (MATM). The feature cross-correlation matrix is measured between the anchor stream and the other two masking streams, respectively. (1) Central Spatial Masking discards selected joints from the feature calculation process, where the joints with a higher degree of centrality have a higher possibility of being selected. (2) Motion Attention Temporal Masking leverages the motion of action and remove frames that move faster with a higher possibility. Our method achieves SOTA performance on NTU-60, NTU-120 and PKU-MMD under various downstream tasks. A practical evaluation is performed where some skeleton joints are lost in downstream tasks. In contrast to previous methods that suffer from large performance drops, our PSTL can still achieve remarkable results, validating the robustness of our method. Code: https://github.com/YujieOuO/PSTL.git.



### Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?
- **Arxiv ID**: http://arxiv.org/abs/2303.01255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01255v1)
- **Published**: 2023-02-17 17:39:41+00:00
- **Updated**: 2023-02-17 17:39:41+00:00
- **Authors**: Gonzalo Martínez, Lauren Watson, Pedro Reviriego, José Alberto Hernández, Marc Juarez, Rik Sarkar
- **Comment**: First version
- **Journal**: None
- **Summary**: In the span of a few months, generative Artificial Intelligence (AI) tools that can generate realistic images or text have taken the Internet by storm, making them one of the technologies with fastest adoption ever. Some of these generative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide public notoriety. Interestingly, these tools are possible because of the massive amount of data (text and images) available on the Internet. The tools are trained on massive data sets that are scraped from Internet sites. And now, these generative AI tools are creating massive amounts of new data that are being fed into the Internet. Therefore, future versions of generative AI tools will be trained with Internet data that is a mix of original and AI-generated data. As time goes on, a mixture of original data and data generated by different versions of AI tools will populate the Internet. This raises a few intriguing questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve with the new data sets or degenerate? Will evolution introduce biases in subsequent generations of generative AI tools? In this document, we explore these questions and report some very initial simulation results using a simple image-generation AI tool. These results suggest that the quality of the generated images degrades as more AI-generated data is used for training thus suggesting that generative AI may degenerate. Although these results are preliminary and cannot be generalised without further study, they serve to illustrate the potential issues of the interaction between generative AI and the Internet.



### CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2302.09027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2302.09027v1)
- **Published**: 2023-02-17 17:49:26+00:00
- **Updated**: 2023-02-17 17:49:26+00:00
- **Authors**: Zhi Zhang, Helen Yannakoudakis, Xiantong Zhen, Ekaterina Shutova
- **Comment**: None
- **Journal**: None
- **Summary**: The task of multimodal referring expression comprehension (REC), aiming at localizing an image region described by a natural language expression, has recently received increasing attention within the research comminity. In this paper, we specifically focus on referring expression comprehension with commonsense knowledge (KB-Ref), a task which typically requires reasoning beyond spatial, visual or semantic information. We propose a novel framework for Commonsense Knowledge Enhanced Transformers (CK-Transformer) which effectively integrates commonsense knowledge into the representations of objects in an image, facilitating identification of the target objects referred to by the expressions. We conduct extensive experiments on several benchmarks for the task of KB-Ref. Our results show that the proposed CK-Transformer achieves a new state of the art, with an absolute improvement of 3.14% accuracy over the existing state of the art.



### Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences
- **Arxiv ID**: http://arxiv.org/abs/2302.09043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09043v1)
- **Published**: 2023-02-17 18:18:27+00:00
- **Updated**: 2023-02-17 18:18:27+00:00
- **Authors**: Christopher Lang, Alexander Braun, Lars Schillingmann, Karsten Haug, Abhinav Valada
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Self-supervised feature learning enables perception systems to benefit from the vast amount of raw data being recorded by vehicle fleets all over the world. However, their potential to learn dense representations from sequential data has been relatively unexplored. In this work, we propose TempO, a temporal ordering pretext task for pre-training region-level feature representations for perception tasks. We embed each frame by an unordered set of proposal feature vectors, a representation that is natural for instance-level perception architectures, and formulate the sequential ordering prediction by comparing similarities between sets of feature vectors in a transformer-based multi-frame architecture. Extensive evaluation in automated driving domains on the BDD100K and MOT17 datasets shows that our TempO approach outperforms existing self-supervised single-frame pre-training methods as well as supervised transfer learning initialization strategies on standard object detection and multi-object tracking benchmarks.



### OTB-morph: One-Time Biometrics via Morphing
- **Arxiv ID**: http://arxiv.org/abs/2302.09053v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.09053v1)
- **Published**: 2023-02-17 18:39:40+00:00
- **Updated**: 2023-02-17 18:39:40+00:00
- **Authors**: Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Aythami Morales, Ignacio Serna
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2111.13213
- **Journal**: None
- **Summary**: Cancelable biometrics are a group of techniques to transform the input biometric to an irreversible feature intentionally using a transformation function and usually a key in order to provide security and privacy in biometric recognition systems. This transformation is repeatable enabling subsequent biometric comparisons. This paper is introducing a new idea to exploit as a transformation function for cancelable biometrics aimed at protecting the templates against iterative optimization attacks. Our proposed scheme is based on time-varying keys (random biometrics in our case) and morphing transformations. An experimental implementation of the proposed scheme is given for face biometrics. The results confirm that the proposed approach is able to withstand against leakage attacks while improving the recognition performance.



### Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent
- **Arxiv ID**: http://arxiv.org/abs/2302.09057v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2302.09057v1)
- **Published**: 2023-02-17 18:45:04+00:00
- **Updated**: 2023-02-17 18:45:04+00:00
- **Authors**: Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, Constantinos Daskalakis
- **Comment**: 29 pages, 8 figures
- **Journal**: None
- **Summary**: Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a \emph{consistency} property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm



### ViTA: A Vision Transformer Inference Accelerator for Edge Applications
- **Arxiv ID**: http://arxiv.org/abs/2302.09108v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.09108v1)
- **Published**: 2023-02-17 19:35:36+00:00
- **Updated**: 2023-02-17 19:35:36+00:00
- **Authors**: Shashank Nag, Gourav Datta, Souvik Kundu, Nitin Chandrachoodan, Peter A. Beerel
- **Comment**: Accepted at ISCAS 2023
- **Journal**: None
- **Summary**: Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.



### A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems
- **Arxiv ID**: http://arxiv.org/abs/2302.09119v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.09119v3)
- **Published**: 2023-02-17 20:07:28+00:00
- **Updated**: 2023-06-09 20:27:16+00:00
- **Authors**: Victor Uc-Cetina, Laura Alvarez-Gonzalez, Anabel Martin-Gonzalez
- **Comment**: None
- **Journal**: None
- **Summary**: Interest in automatic people re-identification systems has significantly grown in recent years, mainly for developing surveillance and smart shops software. Due to the variability in person posture, different lighting conditions, and occluded scenarios, together with the poor quality of the images obtained by different cameras, it is currently an unsolved problem. In machine learning-based computer vision applications with reduced data sets, one possibility to improve the performance of re-identification system is through the augmentation of the set of images or videos available for training the neural models. Currently, one of the most robust ways to generate synthetic information for data augmentation, whether it is video, images or text, are the generative adversarial networks. This article reviews the most relevant recent approaches to improve the performance of person re-identification models through data augmentation, using generative adversarial networks. We focus on three categories of data augmentation approaches: style transfer, pose transfer, and random generation.



### Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer
- **Arxiv ID**: http://arxiv.org/abs/2302.09187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.09187v1)
- **Published**: 2023-02-17 23:39:34+00:00
- **Updated**: 2023-02-17 23:39:34+00:00
- **Authors**: Nguyen Huu Phong, Bernardete Ribeiro
- **Comment**: 15
- **Journal**: None
- **Summary**: Human Action Recognition (HAR) involves the task of categorizing actions present in video sequences. Although it presents interesting problems, it remains one of the most challenging domains in pattern recognition. Convolutional Neural Networks (ConvNets) have demonstrated exceptional success in image recognition and related areas. However, these advanced techniques are not always directly applicable to HAR, as the consideration of temporal features is crucial. In this paper, we present a dynamic PSO-ConvNet model for learning actions in video, drawing on our recent research in image recognition. Our methods are based on a framework where the weight vector of each neural network serves as the position of a particle in phase space, and particles exchange their current weight vectors and gradient estimates of the Loss function. We extend the approach to video by integrating a ConvNet with state-of-the-art temporal methods such as Transformer and Recurrent Neural Networks. The results reveal substantial advancements, with improvements of up to 9% on UCF-101 dataset. The code is available at https://github.com/leonlha/Video-Action-Recognition-via-PSO-ConvNet-Transformer-Collaborative-Learning-with-Dynamics.



