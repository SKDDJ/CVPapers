# Arxiv Papers in cs.CV on 2023-01-07
### Mimicking non-ideal instrument behavior for hologram processing using neural style translation
- **Arxiv ID**: http://arxiv.org/abs/2301.02757v1
- **DOI**: 10.1364/OE.486741
- **Categories**: **physics.ins-det**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02757v1)
- **Published**: 2023-01-07 01:01:27+00:00
- **Updated**: 2023-01-07 01:01:27+00:00
- **Authors**: John S. Schreck, Matthew Hayman, Gabrielle Gantos, Aaron Bansemer, David John Gagne
- **Comment**: 23 pages, 9 figures
- **Journal**: None
- **Summary**: Holographic cloud probes provide unprecedented information on cloud particle density, size and position. Each laser shot captures particles within a large volume, where images can be computationally refocused to determine particle size and shape. However, processing these holograms, either with standard methods or with machine learning (ML) models, requires considerable computational resources, time and occasional human intervention. ML models are trained on simulated holograms obtained from the physical model of the probe since real holograms have no absolute truth labels. Using another processing method to produce labels would be subject to errors that the ML model would subsequently inherit. Models perform well on real holograms only when image corruption is performed on the simulated images during training, thereby mimicking non-ideal conditions in the actual probe (Schreck et. al, 2022). Optimizing image corruption requires a cumbersome manual labeling effort.   Here we demonstrate the application of the neural style translation approach (Gatys et. al, 2016) to the simulated holograms. With a pre-trained convolutional neural network (VGG-19), the simulated holograms are ``stylized'' to resemble the real ones obtained from the probe, while at the same time preserving the simulated image ``content'' (e.g. the particle locations and sizes). Two image similarity metrics concur that the stylized images are more like real holograms than the synthetic ones. With an ML model trained to predict particle locations and shapes on the stylized data sets, we observed comparable performance on both simulated and real holograms, obviating the need to perform manual labeling. The described approach is not specific to hologram images and could be applied in other domains for capturing noise and imperfections in observational instruments to make simulated data more like real world observations.



### Active Deep Learning Guided by Efficient Gaussian Process Surrogates
- **Arxiv ID**: http://arxiv.org/abs/2301.02761v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02761v1)
- **Published**: 2023-01-07 01:35:25+00:00
- **Updated**: 2023-01-07 01:35:25+00:00
- **Authors**: Yunpyo An, Suyeong Park, Kwang In Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The success of active learning relies on the exploration of the underlying data-generating distributions, populating sparsely labeled data areas, and exploitation of the information about the task gained by the baseline (neural network) learners. In this paper, we present a new algorithm that combines these two active learning modes. Our algorithm adopts a Bayesian surrogate for the baseline learner, and it optimizes the exploration process by maximizing the gain of information caused by new labels. Further, by instantly updating the surrogate learner for each new data instance, our model can faithfully simulate and exploit the continuous learning behavior of the learner without having to actually retrain it per label. In experiments with four benchmark classification datasets, our method demonstrated significant performance gain over state-of-the-arts.



### Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment
- **Arxiv ID**: http://arxiv.org/abs/2301.02778v2
- **DOI**: 10.1109/TGRS.2023.3235717
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02778v2)
- **Published**: 2023-01-07 04:33:51+00:00
- **Updated**: 2023-04-03 05:02:47+00:00
- **Authors**: Gongyang Li, Zhi Liu, Xinpeng Zhang, Weisi Lin
- **Comment**: 11 pages, 4 figures, Accepted by IEEE Transactions on Geoscience and
  Remote Sensing 2023
- **Journal**: None
- **Summary**: Recently, relying on convolutional neural networks (CNNs), many methods for salient object detection in optical remote sensing images (ORSI-SOD) are proposed. However, most methods ignore the huge parameters and computational cost brought by CNNs, and only a few pay attention to the portability and mobility. To facilitate practical applications, in this paper, we propose a novel lightweight network for ORSI-SOD based on semantic matching and edge alignment, termed SeaNet. Specifically, SeaNet includes a lightweight MobileNet-V2 for feature extraction, a dynamic semantic matching module (DSMM) for high-level features, an edge self-alignment module (ESAM) for low-level features, and a portable decoder for inference. First, the high-level features are compressed into semantic kernels. Then, semantic kernels are used to activate salient object locations in two groups of high-level features through dynamic convolution operations in DSMM. Meanwhile, in ESAM, cross-scale edge information extracted from two groups of low-level features is self-aligned through L2 loss and used for detail enhancement. Finally, starting from the highest-level features, the decoder infers salient objects based on the accurate locations and fine details contained in the outputs of the two modules. Extensive experiments on two public datasets demonstrate that our lightweight SeaNet not only outperforms most state-of-the-art lightweight methods but also yields comparable accuracy with state-of-the-art conventional methods, while having only 2.76M parameters and running with 1.7G FLOPs for 288x288 inputs. Our code and results are available at https://github.com/MathLee/SeaNet.



### CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction
- **Arxiv ID**: http://arxiv.org/abs/2301.02789v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02789v2)
- **Published**: 2023-01-07 06:28:04+00:00
- **Updated**: 2023-03-08 16:08:37+00:00
- **Authors**: Gangwei Xu, Huan Zhou, Xin Yang
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we propose CGI-Stereo, a novel neural network architecture that can concurrently achieve real-time performance, competitive accuracy, and strong generalization ability. The core of our CGI-Stereo is a Context and Geometry Fusion (CGF) block which adaptively fuses context and geometry information for more effective cost aggregation and meanwhile provides feedback to feature learning to guide more effective contextual feature extraction. The proposed CGF can be easily embedded into many existing stereo matching networks, such as PSMNet, GwcNet and ACVNet. The resulting networks show a significant improvement in accuracy. Specially, the model which incorporates our CGF with ACVNet ranks $1^{st}$ on the KITTI 2012 and 2015 leaderboards among all the published methods. We further propose an informative and concise cost volume, named Attention Feature Volume (AFV), which exploits a correlation volume as attention weights to filter a feature volume. Based on CGF and AFV, the proposed CGI-Stereo outperforms all other published real-time methods on KITTI benchmarks and shows better generalization ability than other real-time methods. Code is available at https://github.com/gangweiX/CGI-Stereo.



### A Novel Improved Mask RCNN for Multiple Targets Detection in the Indoor Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/2302.05293v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.05293v1)
- **Published**: 2023-01-07 07:28:13+00:00
- **Updated**: 2023-01-07 07:28:13+00:00
- **Authors**: Zongmin Liu, Jirui Wang, Jie Li, Pengda Liu, Kai Ren
- **Comment**: None
- **Journal**: None
- **Summary**: With the expansive aging of global population, service robot with living assistance applied in indoor scenes will serve as a crucial role in the field of elderly care and health in the future. Service robots need to detect multiple targets when completing auxiliary tasks. However, indoor scenes are usually complex and there are many types of interference factors, leading to great challenges in the multiple targets detection. To overcome this technical difficulty, a novel improved Mask RCNN method for multiple targets detection in the indoor complex scenes is proposed in this paper. The improved model utilizes Mask RCNN as the network framework. On this basis, Convolutional Block Attention Module (CBAM) with channel mechanism and space mechanism is integrated, and the influence of different background, distance, angle and interference factors are comprehensively considered. Meanwhile, in order to evaluate the detection and identification effects of the established model, a comprehensive evaluation system based on loss function and Mean Average Precision (mAP) is established. For verification, experiments on the detection and identification effects under different distances, backgrounds, angles and interference factors were conducted. The results show that designed model improves the accuracy to a higher level and has a better anti-interference ability than other methods when the detection speed was nearly the same.



### Image Data Augmentation Approaches: A Comprehensive Survey and Future directions
- **Arxiv ID**: http://arxiv.org/abs/2301.02830v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.02830v4)
- **Published**: 2023-01-07 11:37:32+00:00
- **Updated**: 2023-03-12 01:00:17+00:00
- **Authors**: Teerath Kumar, Alessandra Mileo, Rob Brennan, Malika Bendechache
- **Comment**: We need to make a lot changes to make its quality better
- **Journal**: None
- **Summary**: Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and semantic segmentation. For results reproducibility, we compiled available codes of all data augmentation techniques. Finally, we discuss the challenges and difficulties, and possible future direction for the research community. We believe, this survey provides several benefits i) readers will understand the data augmentation working mechanism to fix overfitting problems ii) results will save the searching time of the researcher for comparison purposes. iii) Codes of the mentioned data augmentation techniques are available at https://github.com/kmr2017/Advanced-Data-augmentation-codes iv) Future work will spark interest in research community.



### Dynamic Local Feature Aggregation for Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2301.02836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02836v1)
- **Published**: 2023-01-07 12:18:08+00:00
- **Updated**: 2023-01-07 12:18:08+00:00
- **Authors**: Zihao Li, Pan Gao, Hui Yuan, Ran Wei
- **Comment**: 14 pages , 4 figures , submitted to Signal Processing:image
  communications
- **Journal**: None
- **Summary**: Existing point cloud learning methods aggregate features from neighbouring points relying on constructing graph in the spatial domain, which results in feature update for each point based on spatially-fixed neighbours throughout layers. In this paper, we propose a dynamic feature aggregation (DFA) method that can transfer information by constructing local graphs in the feature domain without spatial constraints. By finding k-nearest neighbors in the feature domain, we perform relative position encoding and semantic feature encoding to explore latent position and feature similarity information, respectively, so that rich local features can be learned. At the same time, we also learn low-dimensional global features from the original point cloud for enhancing feature representation. Between DFA layers, we dynamically update the constructed local graph structure, so that we can learn richer information, which greatly improves adaptability and efficiency. We demonstrate the superiority of our method by conducting extensive experiments on point cloud classification and segmentation tasks. Implementation code is available: https://github.com/jiamang/DFA.



### Deep Learning-Based UAV Aerial Triangulation without Image Control Points
- **Arxiv ID**: http://arxiv.org/abs/2301.02869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02869v1)
- **Published**: 2023-01-07 15:01:38+00:00
- **Updated**: 2023-01-07 15:01:38+00:00
- **Authors**: Jiageng Zhong, Ming Li, Jiangying Qin, Hanqi Zhang
- **Comment**: Accepted to the 42nd Asian Conference on Remote Sensing 2021
  (ACRS2021)
- **Journal**: None
- **Summary**: The emerging drone aerial survey has the advantages of low cost, high efficiency, and flexible use. However, UAVs are often equipped with cheap POS systems and non-measurement cameras, and their flight attitudes are easily affected. How to realize the large-scale mapping of UAV image-free control supported by POS faces many technical problems. The most basic and important core technology is how to accurately realize the absolute orientation of images through advanced aerial triangulation technology. In traditional aerial triangulation, image matching algorithms are constrained to varying degrees by preset prior knowledge. In recent years, deep learning has developed rapidly in the field of photogrammetric computer vision. It has surpassed the performance of traditional handcrafted features in many aspects. It has shown stronger stability in image-based navigation and positioning tasks, especially it has better resistance to unfavorable factors such as blur, illumination changes, and geometric distortion. Based on the introduction of the key technologies of aerial triangulation without image control points, this paper proposes a new drone image registration method based on deep learning image features to solve the problem of high mismatch rate in traditional methods. It adopts SuperPoint as the feature detector, uses the superior generalization performance of CNN to extract precise feature points from the UAV image, thereby achieving high-precision aerial triangulation. Experimental results show that under the same pre-processing and post-processing conditions, compared with the traditional method based on the SIFT algorithm, this method achieves suitable precision more efficiently, which can meet the requirements of UAV aerial triangulation without image control points in large-scale surveys.



### Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching
- **Arxiv ID**: http://arxiv.org/abs/2301.02903v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02903v1)
- **Published**: 2023-01-07 17:24:11+00:00
- **Updated**: 2023-01-07 17:24:11+00:00
- **Authors**: Byoungjip Kim, Sungik Choi, Dasol Hwang, Moontae Lee, Honglak Lee
- **Comment**: 20 pages, 10 figures, NeurIPS 2022
- **Journal**: None
- **Summary**: Despite surprising performance on zero-shot transfer, pre-training a large-scale multimodal model is often prohibitive as it requires a huge amount of data and computing resources. In this paper, we propose a method (BeamCLIP) that can effectively transfer the representations of a large pre-trained multimodal model (CLIP-ViT) into a small target model (e.g., ResNet-18). For unsupervised transfer, we introduce cross-modal similarity matching (CSM) that enables a student model to learn the representations of a teacher model by matching the relative similarity distribution across text prompt embeddings. To better encode the text prompts, we design context-based prompt augmentation (CPA) that can alleviate the lexical ambiguity of input text prompts. Our experiments show that unsupervised representation transfer of a pre-trained vision-language model enables a small ResNet-18 to achieve a better ImageNet-1K top-1 linear probe accuracy (66.2%) than vision-only self-supervised learning (SSL) methods (e.g., SimCLR: 51.8%, SwAV: 63.7%), while closing the gap with supervised learning (69.8%).



### Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants
- **Arxiv ID**: http://arxiv.org/abs/2301.02911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02911v2)
- **Published**: 2023-01-07 18:08:43+00:00
- **Updated**: 2023-01-15 21:00:08+00:00
- **Authors**: Bruno Tafur, Marwa Mahmoud, Staci Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: Infants' neurological development is heavily influenced by their motor skills. Evaluating a baby's movements is key to understanding possible risks of developmental disorders in their growth. Previous research in psychology has shown that measuring specific movements or gestures such as face touches in babies is essential to analyse how babies understand themselves and their context. This research proposes the first automatic approach that detects face touches from video recordings by tracking infants' movements and gestures. The study uses a multimodal feature fusion approach mixing spatial and temporal features and exploits skeleton tracking information to generate more than 170 aggregated features of hand, face and body. This research proposes data-driven machine learning models for the detection and classification of face touch in infants. We used cross dataset testing to evaluate our proposed models. The models achieved 87.0% accuracy in detecting face touches and 71.4% macro-average accuracy in detecting specific face touch locations with significant improvements over Zero Rule and uniform random chance baselines. Moreover, we show that when we run our model to extract face touch frequencies of a larger dataset, we can predict the development of fine motor skills during the first 5 months after birth.



### Multiclass Semantic Segmentation to Identify Anatomical Sub-Regions of Brain and Measure Neuronal Health in Parkinson's Disease
- **Arxiv ID**: http://arxiv.org/abs/2301.02925v1
- **DOI**: 10.1016/j.neuri.2023.100131
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02925v1)
- **Published**: 2023-01-07 19:35:28+00:00
- **Updated**: 2023-01-07 19:35:28+00:00
- **Authors**: Hosein Barzekar, Hai Ngu, Han Hui Lin, Mohsen Hejrati, Steven Ray Valdespino, Sarah Chu, Baris Bingol, Somaye Hashemifar, Soumitra Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: Automated segmentation of anatomical sub-regions with high precision has become a necessity to enable the quantification and characterization of cells/ tissues in histology images. Currently, a machine learning model to analyze sub-anatomical regions of the brain to analyze 2D histological images is not available. The scientists rely on manually segmenting anatomical sub-regions of the brain which is extremely time-consuming and prone to labeler-dependent bias. One of the major challenges in accomplishing such a task is the lack of high-quality annotated images that can be used to train a generic artificial intelligence model. In this study, we employed a UNet-based architecture, compared model performance with various combinations of encoders, image sizes, and sample selection techniques. Additionally, to increase the sample set we resorted to data augmentation which provided data diversity and robust learning. In this study, we trained our best fit model on approximately one thousand annotated 2D brain images stained with Nissl/ Haematoxylin and Tyrosine Hydroxylase enzyme (TH, indicator of dopaminergic neuron viability). The dataset comprises of different animal studies enabling the model to be trained on different datasets. The model effectively is able to detect two sub-regions compacta (SNCD) and reticulata (SNr) in all the images. In spite of limited training data, our best model achieves a mean intersection over union (IOU) of 79% and a mean dice coefficient of 87%. In conclusion, the UNet-based model with EffiecientNet as an encoder outperforms all other encoders, resulting in a first of its kind robust model for multiclass segmentation of sub-brain regions in 2D images.



### Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer
- **Arxiv ID**: http://arxiv.org/abs/2301.02933v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02933v1)
- **Published**: 2023-01-07 20:38:36+00:00
- **Updated**: 2023-01-07 20:38:36+00:00
- **Authors**: Pushpak Pati, Guillaume Jaume, Zeineb Ayadi, Kevin Thandiackal, Behzad Bozorgtabar, Maria Gabrani, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation and automatic identification of histological regions of diagnostic interest offer a valuable aid to pathologists. However, segmentation methods are hampered by the difficulty of obtaining pixel-level annotations, which are tedious and expensive to obtain for Whole-Slide images (WSI). To remedy this, weakly supervised methods have been developed to exploit the annotations directly available at the image level. However, to our knowledge, none of these techniques is adapted to deal with WSIs. In this paper, we propose WholeSIGHT, a weakly-supervised method, to simultaneously segment and classify WSIs of arbitrary shapes and sizes. Formally, WholeSIGHT first constructs a tissue-graph representation of the WSI, where the nodes and edges depict tissue regions and their interactions, respectively. During training, a graph classification head classifies the WSI and produces node-level pseudo labels via post-hoc feature attribution. These pseudo labels are then used to train a node classification head for WSI segmentation. During testing, both heads simultaneously render class prediction and segmentation for an input WSI. We evaluated WholeSIGHT on three public prostate cancer WSI datasets. Our method achieved state-of-the-art weakly-supervised segmentation performance on all datasets while resulting in better or comparable classification with respect to state-of-the-art weakly-supervised WSI classification methods. Additionally, we quantify the generalization capability of our method in terms of segmentation and classification performance, uncertainty estimation, and model calibration.



### Advancing 3D finger knuckle recognition via deep feature learning
- **Arxiv ID**: http://arxiv.org/abs/2301.02934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02934v1)
- **Published**: 2023-01-07 20:55:16+00:00
- **Updated**: 2023-01-07 20:55:16+00:00
- **Authors**: Kevin H. M. Cheng, Xu Cheng, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Contactless 3D finger knuckle patterns have emerged as an effective biometric identifier due to its discriminativeness, visibility from a distance, and convenience. Recent research has developed a deep feature collaboration network which simultaneously incorporates intermediate features from deep neural networks with multiple scales. However, this approach results in a large feature dimension, and the trained classification layer is required for comparing probe samples, which limits the introduction of new classes. This paper advances this approach by investigating the possibility of learning a discriminative feature vector with the least possible dimension for representing 3D finger knuckle images. Experimental results are presented using a publicly available 3D finger knuckle images database with comparisons to popular deep learning architectures and the state-of-the-art 3D finger knuckle recognition methods. The proposed approach offers outperforming results in classification and identification tasks under the more practical feature comparison scenario, i.e., using the extracted deep feature instead of the trained classification layer for comparing probe samples. More importantly, this approach can offer 99% reduction in the size of feature templates, which is highly attractive for deploying biometric systems in the real world. Experiments are also performed using other two public biometric databases with similar patterns to ascertain the effectiveness and generalizability of our proposed approach.



