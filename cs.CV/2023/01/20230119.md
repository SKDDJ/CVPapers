# Arxiv Papers in cs.CV on 2023-01-19
### Masked Autoencoding Does Not Help Natural Language Supervision at Scale
- **Arxiv ID**: http://arxiv.org/abs/2301.07836v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.07836v4)
- **Published**: 2023-01-19 01:05:18+00:00
- **Updated**: 2023-05-15 17:05:32+00:00
- **Authors**: Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training.



### Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment
- **Arxiv ID**: http://arxiv.org/abs/2301.07845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.07845v2)
- **Published**: 2023-01-19 01:51:37+00:00
- **Updated**: 2023-03-08 18:07:33+00:00
- **Authors**: Qiuhao Zeng, Wei Wang, Fan Zhou, Charles Ling, Boyu Wang
- **Comment**: 12 pages, 6 figures, accepted by AAAI 2023
- **Journal**: None
- **Summary**: Existing domain generalization aims to learn a generalizable model to perform well even on unseen domains. For many real-world machine learning applications, the data distribution often shifts gradually along domain indices. For example, a self-driving car with a vision system drives from dawn to dusk, with the sky darkening gradually. Therefore, the system must be able to adapt to changes in ambient illumination and continue to drive safely on the road. In this paper, we formulate such problems as Evolving Domain Generalization, where a model aims to generalize well on a target domain by discovering and leveraging the evolving pattern of the environment. We then propose Directional Domain Augmentation (DDA), which simulates the unseen target features by mapping source data as augmentations through a domain transformer. Specifically, we formulate DDA as a bi-level optimization problem and solve it through a novel meta-learning approach in the representation space. We evaluate the proposed method on both synthetic datasets and realworld datasets, and empirical results show that our approach can outperform other existing methods.



### Improving Food Detection For Images From a Wearable Egocentric Camera
- **Arxiv ID**: http://arxiv.org/abs/2301.07861v1
- **DOI**: 10.2352/ISSN.2470-1173.2021.8.IMAWM-286
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07861v1)
- **Published**: 2023-01-19 03:12:05+00:00
- **Updated**: 2023-01-19 03:12:05+00:00
- **Authors**: Yue Han, Sri Kalyan Yarlagadda, Tonmoy Ghosh, Fengqing Zhu, Edward Sazonov, Edward J. Delp
- **Comment**: 6 pages, 6 figures, Conference Paper for Imaging and Multimedia
  Analytics in a Web and Mobile World Conference, IS&T Electronic Imaging
  Symposium, Burlingame, CA (Virtual), January, 2021
- **Journal**: None
- **Summary**: Diet is an important aspect of our health. Good dietary habits can contribute to the prevention of many diseases and improve the overall quality of life. To better understand the relationship between diet and health, image-based dietary assessment systems have been developed to collect dietary information. We introduce the Automatic Ingestion Monitor (AIM), a device that can be attached to one's eye glasses. It provides an automated hands-free approach to capture eating scene images. While AIM has several advantages, images captured by the AIM are sometimes blurry. Blurry images can significantly degrade the performance of food image analysis such as food detection. In this paper, we propose an approach to pre-process images collected by the AIM imaging sensor by rejecting extremely blurry images to improve the performance of food detection.



### MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.11798v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11798v1)
- **Published**: 2023-01-19 03:42:36+00:00
- **Updated**: 2023-01-19 03:42:36+00:00
- **Authors**: Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu
- **Comment**: Code will be released at https://github.com/WuJunde/MedSegDiff
- **Journal**: None
- **Summary**: The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent studies have also found DPM to be useful in the field of medical image analysis, as evidenced by the strong performance of the medical image segmentation model MedSegDiff in various tasks. While these models were originally designed with a UNet backbone, they may also potentially benefit from the incorporation of vision transformer techniques. However, we discovered that simply combining these two approaches resulted in subpar performance. In this paper, we propose a novel transformer-based conditional UNet framework, as well as a new Spectrum-Space Transformer (SS-Former) to model the interaction between noise and semantic features. This architectural improvement leads to a new diffusion-based medical image segmentation method called MedSegDiff-V2, which significantly improves the performance of MedSegDiff. We have verified the effectiveness of MedSegDiff-V2 on eighteen organs of five segmentation datasets with different image modalities. Our experimental results demonstrate that MedSegDiff-V2 outperforms state-of-the-art (SOTA) methods by a considerable margin, further proving the generalizability and effectiveness of the proposed model.



### Multimodal Video Adapter for Parameter Efficient Video Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2301.07868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07868v1)
- **Published**: 2023-01-19 03:42:56+00:00
- **Updated**: 2023-01-19 03:42:56+00:00
- **Authors**: Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Zhao Zhang, Peng Wang, Xiaohui Shen, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art video-text retrieval (VTR) methods usually fully fine-tune the pre-trained model (e.g. CLIP) on specific datasets, which may suffer from substantial storage costs in practical applications since a separate model per task needs to be stored. To overcome this issue, we present the premier work on performing parameter-efficient VTR from the pre-trained model, i.e., only a small number of parameters are tunable while freezing the backbone. Towards this goal, we propose a new method dubbed Multimodal Video Adapter (MV-Adapter) for efficiently transferring the knowledge in the pre-trained CLIP from image-text to video-text. Specifically, MV-Adapter adopts bottleneck structures in both video and text branches and introduces two novel components. The first is a Temporal Adaptation Module employed in the video branch to inject global and local temporal contexts. We also learn weights calibrations to adapt to the dynamic variations across frames. The second is a Cross-Modal Interaction Module that generates weights for video/text branches through a shared parameter space, for better aligning between modalities. Thanks to above innovations, MV-Adapter can achieve on-par or better performance than standard fine-tuning with negligible parameters overhead. Notably, on five widely used VTR benchmarks (MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet), MV-Adapter consistently outperforms various competing methods in V2T/T2V tasks with large margins. Codes will be released.



### Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception
- **Arxiv ID**: http://arxiv.org/abs/2301.07870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07870v1)
- **Published**: 2023-01-19 03:58:48+00:00
- **Updated**: 2023-01-19 03:58:48+00:00
- **Authors**: Bin Huang, Yangguang Li, Enze Xie, Feng Liang, Luya Wang, Mingzhu Shen, Fenggang Liu, Tianqi Wang, Ping Luo, Jing Shao
- **Comment**: Accepted by NeurIPS2022_ML4AD on October 22, 2022
- **Journal**: NeurIPS2022_ML4AD
- **Summary**: Recently, the pure camera-based Bird's-Eye-View (BEV) perception removes expensive Lidar sensors, making it a feasible solution for economical autonomous driving. However, most existing BEV solutions either suffer from modest performance or require considerable resources to execute on-vehicle inference. This paper proposes a simple yet effective framework, termed Fast-BEV, which is capable of performing real-time BEV perception on the on-vehicle chips. Towards this goal, we first empirically find that the BEV representation can be sufficiently powerful without expensive view transformation or depth representation. Starting from M2BEV baseline, we further introduce (1) a strong data augmentation strategy for both image and BEV space to avoid over-fitting (2) a multi-frame feature fusion mechanism to leverage the temporal information (3) an optimized deployment-friendly view transformation to speed up the inference. Through experiments, we show Fast-BEV model family achieves considerable accuracy and efficiency on edge. In particular, our M1 model (R18@256x704) can run over 50FPS on the Tesla T4 platform, with 47.0% NDS on the nuScenes validation set. Our largest model (R101@900x1600) establishes a new state-of-the-art 53.5% NDS on the nuScenes validation set. The code is released at: https://github.com/Sense-GVT/Fast-BEV.



### Unposed: Unsupervised Pose Estimation based Product Image Recommendations
- **Arxiv ID**: http://arxiv.org/abs/2301.07879v1
- **DOI**: 10.1145/3564121.3564126
- **Categories**: **cs.CV**, I.m
- **Links**: [PDF](http://arxiv.org/pdf/2301.07879v1)
- **Published**: 2023-01-19 05:02:55+00:00
- **Updated**: 2023-01-19 05:02:55+00:00
- **Authors**: Saurabh Sharma, Faizan Ahemad
- **Comment**: None
- **Journal**: None
- **Summary**: Product images are the most impressing medium of customer interaction on the product detail pages of e-commerce websites. Millions of products are onboarded on to webstore catalogues daily and maintaining a high quality bar for a product's set of images is a problem at scale. Grouping products by categories, clothing is a very high volume and high velocity category and thus deserves its own attention. Given the scale it is challenging to monitor the completeness of image set, which adequately details the product for the consumers, which in turn often leads to a poor customer experience and thus customer drop off.   To supervise the quality and completeness of the images in the product pages for these product types and suggest improvements, we propose a Human Pose Detection based unsupervised method to scan the image set of a product for the missing ones. The unsupervised approach suggests a fair approach to sellers based on product and category irrespective of any biases. We first create a reference image set of popular products with wholesome imageset. Then we create clusters of images to label most desirable poses to form the classes for the reference set from these ideal products set. Further, for all test products we scan the images for all desired pose classes w.r.t. reference set poses, determine the missing ones and sort them in the order of potential impact. These missing poses can further be used by the sellers to add enriched product listing image. We gathered data from popular online webstore and surveyed ~200 products manually, a large fraction of which had at least 1 repeated image or missing variant, and sampled 3K products(~20K images) of which a significant proportion had scope for adding many image variants as compared to high rated products which had more than double image variants, indicating that our model can potentially be used on a large scale.



### Spatially Covariant Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.07895v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2301.07895v1)
- **Published**: 2023-01-19 05:50:28+00:00
- **Updated**: 2023-01-19 05:50:28+00:00
- **Authors**: Hang Zhang, Rongguang Wang, Jinwei Zhang, Dongdong Liu, Chao Li, Jiahao Li
- **Comment**: 9 pages, 7 figures, and 2 tables
- **Journal**: None
- **Summary**: Compared to natural images, medical images usually show stronger visual patterns and therefore this adds flexibility and elasticity to resource-limited clinical applications by injecting proper priors into neural networks. In this paper, we propose spatially covariant pixel-aligned classifier (SCP) to improve the computational efficiency and meantime maintain or increase accuracy for lesion segmentation. SCP relaxes the spatial invariance constraint imposed by convolutional operations and optimizes an underlying implicit function that maps image coordinates to network weights, the parameters of which are obtained along with the backbone network training and later used for generating network weights to capture spatially covariant contextual information. We demonstrate the effectiveness and efficiency of the proposed SCP using two lesion segmentation tasks from different imaging modalities: white matter hyperintensity segmentation in magnetic resonance imaging and liver tumor segmentation in contrast-enhanced abdominal computerized tomography. The network using SCP has achieved 23.8%, 64.9% and 74.7% reduction in GPU memory usage, FLOPs, and network size with similar or better accuracy for lesion segmentation.



### Surface Recognition for e-Scooter Using Smartphone IMU Sensor
- **Arxiv ID**: http://arxiv.org/abs/2302.12720v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.12720v1)
- **Published**: 2023-01-19 06:01:50+00:00
- **Updated**: 2023-01-19 06:01:50+00:00
- **Authors**: Areej Eweida, Nimord Segol, Maxim Freydin, Niv Sfaradi, Barak Or
- **Comment**: 4 pages, Preprint
- **Journal**: None
- **Summary**: In recent years, as the use of micromobility gained popularity, technological challenges connected to e-scooters became increasingly important. This paper focuses on road surface recognition, an important task in this area. A reliable and accurate method for road surface recognition can help improve the safety and stability of the vehicle. Here a data-driven method is proposed to recognize if an e-scooter is on a road or a sidewalk. The proposed method uses only the widely available inertial measurement unit (IMU) sensors on a smartphone device. deep neural networks (DNNs) are used to infer whether an e-scooteris driving on a road or on a sidewalk by solving a binary classification problem. A data set is collected and several different deep models as well as classical machine learning approaches for the binary classification problem are applied and compared. Experiment results on a route containing the two surfaces are presented demonstrating the DNNs ability to distinguish between them.



### Spatio-Temporal Context Modeling for Road Obstacle Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.07921v1
- **DOI**: 10.1007/978-3-031-20096-0_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07921v1)
- **Published**: 2023-01-19 07:06:35+00:00
- **Updated**: 2023-01-19 07:06:35+00:00
- **Authors**: Xiuen Wu, Tao Wang, Lingyu Liang, Zuoyong Li, Fum Yew Ching
- **Comment**: Paper accepted by the 4th International Conference on Machine
  Learning for Cyber Security (ML4CS 2022), Guangzhou, China
- **Journal**: None
- **Summary**: Road obstacle detection is an important problem for vehicle driving safety. In this paper, we aim to obtain robust road obstacle detection based on spatio-temporal context modeling. Firstly, a data-driven spatial context model of the driving scene is constructed with the layouts of the training data. Then, obstacles in the input image are detected via the state-of-the-art object detection algorithms, and the results are combined with the generated scene layout. In addition, to further improve the performance and robustness, temporal information in the image sequence is taken into consideration, and the optical flow is obtained in the vicinity of the detected objects to track the obstacles across neighboring frames. Qualitative and quantitative experiments were conducted on the Small Obstacle Detection (SOD) dataset and the Lost and Found dataset. The results indicate that our method with spatio-temporal context modeling is superior to existing methods for road obstacle detection.



### Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.07923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07923v1)
- **Published**: 2023-01-19 07:26:33+00:00
- **Updated**: 2023-01-19 07:26:33+00:00
- **Authors**: Snehashis Majhi, Rui Dai, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection in surveillance systems with only video-level labels (i.e. weakly-supervised) is challenging. This is due to, (i) the complex integration of human and scene based anomalies comprising of subtle and sharp spatio-temporal cues in real-world scenarios, (ii) non-optimal optimization between normal and anomaly instances under weak supervision. In this paper, we propose a Human-Scene Network to learn discriminative representations by capturing both subtle and strong cues in a dissociative manner. In addition, a self-rectifying loss is also proposed that dynamically computes the pseudo temporal annotations from video-level labels for optimizing the Human-Scene Network effectively. The proposed Human-Scene Network optimized with self-rectifying loss is validated on three publicly available datasets i.e. UCF-Crime, ShanghaiTech and IITB-Corridor, outperforming recently reported state-of-the-art approaches on five out of the six scenarios considered.



### Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.07927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07927v2)
- **Published**: 2023-01-19 07:32:23+00:00
- **Updated**: 2023-04-26 01:44:28+00:00
- **Authors**: Shuzhen Rao, Jun Huang, Zengming Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In cross-domain few-shot learning, the core issue is that the model trained on source domains struggles to generalize to the target domain, especially when the domain shift is large. Motivated by the observation that the domain shift between training tasks and target tasks usually can reflect in their style variation, we propose Task Augmented Meta-Learning (TAML) to conduct style transfer-based task augmentation to improve the domain generalization ability. Firstly, Multi-task Interpolation (MTI) is introduced to fuse features from multiple tasks with different styles, which makes more diverse styles available. Furthermore, a novel task-augmentation strategy called Multi-Task Style Transfer (MTST) is proposed to perform style transfer on existing tasks to learn discriminative style-independent features. We also introduce a Feature Modulation module (FM) to add random styles and improve generalization of the model. The proposed TAML increases the diversity of styles of training tasks, and contributes to training a model with better domain generalization ability. The effectiveness is demonstrated via theoretical analysis and thorough experiments on two popular cross-domain few-shot benchmarks.



### Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.07944v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.07944v2)
- **Published**: 2023-01-19 08:34:04+00:00
- **Updated**: 2023-04-08 03:29:05+00:00
- **Authors**: Jiazheng Xing, Mengmeng Wang, Yong Liu, Boyu Mu
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial and temporal modeling is one of the most core aspects of few-shot action recognition. Most previous works mainly focus on long-term temporal relation modeling based on high-level spatial representations, without considering the crucial low-level spatial features and short-term temporal relations. Actually, the former feature could bring rich local semantic information, and the latter feature could represent motion characteristics of adjacent frames, respectively. In this paper, we propose SloshNet, a new framework that revisits the spatial and temporal modeling for few-shot action recognition in a finer manner. First, to exploit the low-level spatial features, we design a feature fusion architecture search module to automatically search for the best combination of the low-level and high-level spatial features. Next, inspired by the recent transformer, we introduce a long-term temporal modeling module to model the global temporal relations based on the extracted spatial appearance features. Meanwhile, we design another short-term temporal modeling module to encode the motion characteristics between adjacent frame representations. After that, the final predictions can be obtained by feeding the embedded rich spatial-temporal features to a common frame-level class prototype matcher. We extensively validate the proposed SloshNet on four few-shot action recognition datasets, including Something-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorable results against state-of-the-art methods in all datasets.



### Point Cloud Data Simulation and Modelling with Aize Workspace
- **Arxiv ID**: http://arxiv.org/abs/2301.07947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.07947v1)
- **Published**: 2023-01-19 08:47:31+00:00
- **Updated**: 2023-01-19 08:47:31+00:00
- **Authors**: Boris Mocialov, Eirik Eythorsson, Reza Parseh, Hoang Tran, Vegard Flovik
- **Comment**: Extended abstract, Northern Lights Deep Learning Conference, 2023
- **Journal**: None
- **Summary**: This work takes a look at data models often used in digital twins and presents preliminary results specifically from surface reconstruction and semantic segmentation models trained using simulated data. This work is expected to serve as a ground work for future endeavours in data contextualisation inside a digital twin.



### RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2301.07958v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2301.07958v2)
- **Published**: 2023-01-19 09:18:06+00:00
- **Updated**: 2023-02-05 15:53:56+00:00
- **Authors**: Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Radiance fields have gradually become a main representation of media. Although its appearance editing has been studied, how to achieve view-consistent recoloring in an efficient manner is still under explored. We present RecolorNeRF, a novel user-friendly color editing approach for the neural radiance fields. Our key idea is to decompose the scene into a set of pure-colored layers, forming a palette. By this means, color manipulation can be conducted by altering the color components of the palette directly. To support efficient palette-based editing, the color of each layer needs to be as representative as possible. In the end, the problem is formulated as an optimization problem, where the layers and their blending weights are jointly optimized with the NeRF itself. Extensive experiments show that our jointly-optimized layer decomposition can be used against multiple backbones and produce photo-realistic recolored novel-view renderings. We demonstrate that RecolorNeRF outperforms baseline methods both quantitatively and qualitatively for color editing even in complex real-world scenes.



### Fast Inference in Denoising Diffusion Models via MMD Finetuning
- **Arxiv ID**: http://arxiv.org/abs/2301.07969v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.07969v1)
- **Published**: 2023-01-19 09:48:07+00:00
- **Updated**: 2023-01-19 09:48:07+00:00
- **Authors**: Emanuele Aiello, Diego Valsesia, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Denoising Diffusion Models (DDMs) have become a popular tool for generating high-quality samples from complex data distributions. These models are able to capture sophisticated patterns and structures in the data, and can generate samples that are highly diverse and representative of the underlying distribution. However, one of the main limitations of diffusion models is the complexity of sample generation, since a large number of inference timesteps is required to faithfully capture the data distribution. In this paper, we present MMD-DDM, a novel method for fast sampling of diffusion models. Our approach is based on the idea of using the Maximum Mean Discrepancy (MMD) to finetune the learned distribution with a given budget of timesteps. This allows the finetuned model to significantly improve the speed-quality trade-off, by substantially increasing fidelity in inference regimes with few steps or, equivalently, by reducing the required number of steps to reach a target fidelity, thus paving the way for a more practical adoption of diffusion models in a wide range of applications. We evaluate our approach on unconditional image generation with extensive experiments across the CIFAR-10, CelebA, ImageNet and LSUN-Church datasets. Our findings show that the proposed method is able to produce high-quality samples in a fraction of the time required by widely-used diffusion models, and outperforms state-of-the-art techniques for accelerated sampling. Code is available at: https://github.com/diegovalsesia/MMD-DDM.



### Hybrid Open-set Segmentation with Synthetic Negative Data
- **Arxiv ID**: http://arxiv.org/abs/2301.08555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08555v2)
- **Published**: 2023-01-19 11:02:44+00:00
- **Updated**: 2023-07-28 10:18:40+00:00
- **Authors**: Matej Grcić, Siniša Šegvić
- **Comment**: None
- **Journal**: None
- **Summary**: Open-set segmentation is often conceived by complementing closed-set classification with anomaly detection. Existing dense anomaly detectors operate either through generative modelling of regular training data or by discriminating with respect to negative training data. These two approaches optimize different objectives and therefore exhibit different failure modes. Consequently, we propose the first dense hybrid anomaly score that fuses generative and discriminative cues. The proposed score can be efficiently implemented by upgrading any semantic segmentation model with dense estimates of data likelihood and dataset posterior. Our design is a remarkably good fit for efficient inference on large images due to negligible computational overhead over the closed-set baseline. The resulting dense hybrid open-set models require negative training images that can be sampled from an auxiliary negative dataset, from a jointly trained generative model, or from a mixture of both sources. We evaluate our contributions on benchmarks for dense anomaly detection and open-set segmentation. The experiments reveal strong open-set performance in spite of negligible computational overhead.



### On the Importance of Sign Labeling: The Hamburg Sign Language Notation System Case Study
- **Arxiv ID**: http://arxiv.org/abs/2302.10768v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10768v2)
- **Published**: 2023-01-19 11:11:57+00:00
- **Updated**: 2023-03-07 15:11:10+00:00
- **Authors**: Maria Ferlin, Sylwia Majchrowska, Marta Plantykow, Alicja Kwaśniwska, Agnieszka Mikołajczyk-Bareła, Milena Olech, Jakub Nalepa
- **Comment**: 20 pages, 13 figures
- **Journal**: None
- **Summary**: Labeling is the cornerstone of supervised machine learning, which has been exploited in a plethora of various applications, with sign language recognition being one of them. However, such algorithms must be fed with a huge amount of consistently labeled data during the training process to elaborate a well-generalizing model. In addition, there is a great need for an automated solution that works with any nationally diversified sign language. Although there are language-agnostic transcription systems, such as the Hamburg Sign Language Notation System (HamNoSys) that describe the signer's initial position and body movement instead of the glosses' meanings, there are still issues with providing accurate and reliable labels for every real-world use case. In this context, the industry relies heavily on manual attribution and labeling of the available video data. In this work, we tackle this issue and thoroughly analyze the HamNoSys labels provided by various maintainers of open sign language corpora in five sign languages, in order to examine the challenges encountered in labeling video data. We also investigate the consistency and objectivity of HamNoSys-based labels for the purpose of training machine learning models. Our findings provide valuable insights into the limitations of the current labeling methods and pave the way for future research on developing more accurate and efficient solutions for sign language recognition.



### Evaluation of the potential of Near Infrared Hyperspectral Imaging for monitoring the invasive brown marmorated stink bug
- **Arxiv ID**: http://arxiv.org/abs/2301.08252v1
- **DOI**: 10.1016/j.chemolab.2023.104751
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08252v1)
- **Published**: 2023-01-19 11:37:20+00:00
- **Updated**: 2023-01-19 11:37:20+00:00
- **Authors**: Veronica Ferrari, Rosalba Calvini, Bas Boom, Camilla Menozzi, Aravind Krishnaswamy Rangarajan, Lara Maistrello, Peter Offermans, Alessandro Ulrici
- **Comment**: Accepted manuscript
- **Journal**: Chemometrics and Intelligent Laboratory Systems, 2023, 234, 104751
- **Summary**: The brown marmorated stink bug (BMSB), Halyomorpha halys, is an invasive insect pest of global importance that damages several crops, compromising agri-food production. Field monitoring procedures are fundamental to perform risk assessment operations, in order to promptly face crop infestations and avoid economical losses. To improve pest management, spectral cameras mounted on Unmanned Aerial Vehicles (UAVs) and other Internet of Things (IoT) devices, such as smart traps or unmanned ground vehicles, could be used as an innovative technology allowing fast, efficient and real-time monitoring of insect infestations. The present study consists in a preliminary evaluation at the laboratory level of Near Infrared Hyperspectral Imaging (NIR-HSI) as a possible technology to detect BMSB specimens on different vegetal backgrounds, overcoming the problem of BMSB mimicry. Hyperspectral images of BMSB were acquired in the 980-1660 nm range, considering different vegetal backgrounds selected to mimic a real field application scene. Classification models were obtained following two different chemometric approaches. The first approach was focused on modelling spectral information and selecting relevant spectral regions for discrimination by means of sparse-based variable selection coupled with Soft Partial Least Squares Discriminant Analysis (s-Soft PLS-DA) classification algorithm. The second approach was based on modelling spatial and spectral features contained in the hyperspectral images using Convolutional Neural Networks (CNN). Finally, to further improve BMSB detection ability, the two strategies were merged, considering only the spectral regions selected by s-Soft PLS-DA for CNN modelling.



### Reference Guided Image Inpainting using Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/2301.08044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08044v1)
- **Published**: 2023-01-19 12:39:08+00:00
- **Updated**: 2023-01-19 12:39:08+00:00
- **Authors**: Dongsik Yoon, Jeonggi Kwak, Yuanming Li, David Han, Youngsaeng Jin, Hanseok Ko
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Image inpainting is a technique of completing missing pixels such as occluded region restoration, distracting objects removal, and facial completion. Among these inpainting tasks, facial completion algorithm performs face inpainting according to the user direction. Existing approaches require delicate and well controlled input by the user, thus it is difficult for an average user to provide the guidance sufficiently accurate for the algorithm to generate desired results. To overcome this limitation, we propose an alternative user-guided inpainting architecture that manipulates facial attributes using a single reference image as the guide. Our end-to-end model consists of attribute extractors for accurate reference image attribute transfer and an inpainting model to map the attributes realistically and accurately to generated images. We customize MS-SSIM loss and learnable bidirectional attention maps in which importance structures remain intact even with irregular shaped masks. Based on our evaluation using the publicly available dataset CelebA-HQ, we demonstrate that the proposed method delivers superior performance compared to some state-of-the-art methods specialized in inpainting tasks.



### Position Regression for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.08064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08064v1)
- **Published**: 2023-01-19 13:22:11+00:00
- **Updated**: 2023-01-19 13:22:11+00:00
- **Authors**: Florentin Bieder, Julia Wolleb, Robin Sandkühler, Philippe C. Cattin
- **Comment**: None
- **Journal**: Proceedings of The 5th International Conference on Medical Imaging
  with Deep Learning, PMLR 172:160-172, 2022
- **Summary**: In recent years, anomaly detection has become an essential field in medical image analysis. Most current anomaly detection methods for medical images are based on image reconstruction. In this work, we propose a novel anomaly detection approach based on coordinate regression. Our method estimates the position of patches within a volume, and is trained only on data of healthy subjects. During inference, we can detect and localize anomalies by considering the error of the position estimate of a given patch. We apply our method to 3D CT volumes and evaluate it on patients with intracranial haemorrhages and cranial fractures. The results show that our method performs well in detecting these anomalies. Furthermore, we show that our method requires less memory than comparable approaches that involve image reconstruction. This is highly relevant for processing large 3D volumes, for instance, CT or MRI scans.



### Interpreting CNN Predictions using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.08067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.08067v1)
- **Published**: 2023-01-19 13:26:12+00:00
- **Updated**: 2023-01-19 13:26:12+00:00
- **Authors**: Akash Guna R T, Raul Benitez, Sikha O K
- **Comment**: Article submitted to JMLR. 19 Pages
- **Journal**: None
- **Summary**: We propose a novel method that trains a conditional Generative Adversarial Network (GAN) to generate visual interpretations of a Convolutional Neural Network (CNN). To comprehend a CNN, the GAN is trained with information on how the CNN processes an image when making predictions. Supplying that information has two main challenges: how to represent this information in a form that is feedable to the GANs and how to effectively feed the representation to the GAN. To address these issues, we developed a suitable representation of CNN architectures by cumulatively averaging intermediate interpretation maps. We also propose two alternative approaches to feed the representations to the GAN and to choose an effective training strategy. Our approach learned the general aspects of CNNs and was agnostic to datasets and CNN architectures. The study includes both qualitative and quantitative evaluations and compares the proposed GANs with state-of-the-art approaches. We found that the initial layers of CNNs and final layers are equally crucial for interpreting CNNs upon interpreting the proposed GAN. We believe training a GAN to interpret CNNs would open doors for improved interpretations by leveraging fast-paced deep learning advancements. The code used for experimentation is publicly available at https://github.com/Akash-guna/Explain-CNN-With-GANS



### Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.08072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08072v1)
- **Published**: 2023-01-19 13:37:19+00:00
- **Updated**: 2023-01-19 13:37:19+00:00
- **Authors**: Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Color plays an important role in human visual perception, reflecting the spectrum of objects. However, the existing infrared and visible image fusion methods rarely explore how to handle multi-spectral/channel data directly and achieve high color fidelity. This paper addresses the above issue by proposing a novel method with diffusion models, termed as Dif-Fusion, to generate the distribution of the multi-channel input data, which increases the ability of multi-source information aggregation and the fidelity of colors. In specific, instead of converting multi-channel images into single-channel data in existing fusion methods, we create the multi-channel data distribution with a denoising network in a latent space with forward and reverse diffusion process. Then, we use the the denoising network to extract the multi-channel diffusion features with both visible and infrared information. Finally, we feed the multi-channel diffusion features to the multi-channel fusion module to directly generate the three-channel fused image. To retain the texture and intensity information, we propose multi-channel gradient loss and intensity loss. Along with the current evaluation metrics for measuring texture and intensity fidelity, we introduce a new evaluation metric to quantify color fidelity. Extensive experiments indicate that our method is more effective than other state-of-the-art image fusion methods, especially in color fidelity.



### RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2301.08092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08092v1)
- **Published**: 2023-01-19 14:22:44+00:00
- **Updated**: 2023-01-19 14:22:44+00:00
- **Authors**: Utkarsh Nath, Yancheng Wang, Yingzhen Yang
- **Comment**: 17 pages, 12 figures
- **Journal**: None
- **Summary**: Deep Neural Networks are vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the driving tools of deep neural networks, demonstrates superior performance in prediction accuracy in various machine learning applications. However, it is unclear how it performs against adversarial attacks. Given the presence of a robust teacher, it would be interesting to investigate if NAS would produce robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student/teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental result evidences the effectiveness of RNAS-CL and shows that RNAS-CL produces small and robust neural architecture.



### Soft Thresholding for Visual Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2301.08113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08113v1)
- **Published**: 2023-01-19 15:05:13+00:00
- **Updated**: 2023-01-19 15:05:13+00:00
- **Authors**: Christoph Dalitz
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Thresholding converts a greyscale image into a binary image, and is thus often a necessary segmentation step in image processing. For a human viewer however, thresholding usually has a negative impact on the legibility of document images. This report describes a simple method for "smearing out" the threshold and transforming the greyscale image into a different greyscale image. The method is similar to fuzzy thresholding, but is discussed here in the simpler context of greyscale transformations and, unlike fuzzy thresholding, it is independent from the method for finding the threshold. A simple formula is presented for automatically determining the width of the threshold spread. The method can be used, e.g., for enhancing images for the presentation in online facsimile repositories.



### Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.08125v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.08125v2)
- **Published**: 2023-01-19 15:38:43+00:00
- **Updated**: 2023-07-17 03:00:47+00:00
- **Authors**: Conghao Xiong, Hao Chen, Joseph J. Y. Sung, Irwin King
- **Comment**: Accepted to IJCAI2023
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) and transformers are increasingly popular in histopathology Whole Slide Image (WSI) classification. However, unlike human pathologists who selectively observe specific regions of histopathology tissues under different magnifications, most methods do not incorporate multiple resolutions of the WSIs, hierarchically and attentively, thereby leading to a loss of focus on the WSIs and information from other resolutions. To resolve this issue, we propose a Hierarchical Attention-Guided Multiple Instance Learning framework to fully exploit the WSIs. This framework can dynamically and attentively discover the discriminative regions across multiple resolutions of the WSIs. Within this framework, an Integrated Attention Transformer is proposed to further enhance the performance of the transformer and obtain a more holistic WSI (bag) representation. This transformer consists of multiple Integrated Attention Modules, which is the combination of a transformer layer and an aggregation module that produces a bag representation based on every instance representation in that bag. The experimental results show that our method achieved state-of-the-art performances on multiple datasets, including Camelyon16, TCGA-RCC, TCGA-NSCLC, and an in-house IMGC dataset. The code is available at https://github.com/BearCleverProud/HAG-MIL.



### Regularising disparity estimation via multi task learning with structured light reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2301.08140v1
- **DOI**: 10.1080/21681163.2022.2156391
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08140v1)
- **Published**: 2023-01-19 15:54:52+00:00
- **Updated**: 2023-01-19 15:54:52+00:00
- **Authors**: Alistair Weld, Joao Cartucho, Chi Xu, Joseph Davids, Stamatia Giannarou
- **Comment**: None
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization 2022
- **Summary**: 3D reconstruction is a useful tool for surgical planning and guidance. However, the lack of available medical data stunts research and development in this field, as supervised deep learning methods for accurate disparity estimation rely heavily on large datasets containing ground truth information. Alternative approaches to supervision have been explored, such as self-supervision, which can reduce or remove entirely the need for ground truth. However, no proposed alternatives have demonstrated performance capabilities close to what would be expected from a supervised setup. This work aims to alleviate this issue. In this paper, we investigate the learning of structured light projections to enhance the development of direct disparity estimation networks. We show for the first time that it is possible to accurately learn the projection of structured light on a scene, implicitly learning disparity. Secondly, we \textcolor{black}{explore the use of a multi task learning (MTL) framework for the joint training of structured light and disparity. We present results which show that MTL with structured light improves disparity training; without increasing the number of model parameters. Our MTL setup outperformed the single task learning (STL) network in every validation test. Notably, in the medical generalisation test, the STL error was 1.4 times worse than that of the best MTL performance. The benefit of using MTL is emphasised when the training data is limited.} A dataset containing stereoscopic images, disparity maps and structured light projections on medical phantoms and ex vivo tissue was created for evaluation together with virtual scenes. This dataset will be made publicly available in the future.



### RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2301.08147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.08147v1)
- **Published**: 2023-01-19 15:59:10+00:00
- **Updated**: 2023-01-19 15:59:10+00:00
- **Authors**: Leonard Bruns, Patric Jensfelt
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2202.10346
- **Journal**: None
- **Summary**: Recently, various methods for 6D pose and shape estimation of objects at a per-category level have been proposed. This work provides an overview of the field in terms of methods, datasets, and evaluation protocols. First, an overview of existing works and their commonalities and differences is provided. Second, we take a critical look at the predominant evaluation protocol, including metrics and datasets. Based on the findings, we propose a new set of metrics, contribute new annotations for the Redwood dataset, and evaluate state-of-the-art methods in a fair comparison. The results indicate that existing methods do not generalize well to unconstrained orientations and are actually heavily biased towards objects being upright. We provide an easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset interfaces, which allows evaluation and comparison with various state-of-the-art approaches (https://github.com/roym899/pose_and_shape_evaluation).



### SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines
- **Arxiv ID**: http://arxiv.org/abs/2301.08153v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.08153v2)
- **Published**: 2023-01-19 16:14:28+00:00
- **Updated**: 2023-02-17 09:38:12+00:00
- **Authors**: Shizun Wang, Weihong Zeng, Xu Wang, Hao Yang, Li Chen, Yi Yuan, Yunzhao Zeng, Min Zheng, Chuang Zhang, Ming Wu
- **Comment**: AAAI 2023 Oral
- **Journal**: None
- **Summary**: The creation of a parameterized stylized character involves careful selection of numerous parameters, also known as the "avatar vectors" that can be interpreted by the avatar engine. Existing unsupervised avatar vector estimation methods that auto-create avatars for users, however, often fail to work because of the domain gap between realistic faces and stylized avatar images. To this end, we propose SwiftAvatar, a novel avatar auto-creation framework that is evidently superior to previous works. SwiftAvatar introduces dual-domain generators to create pairs of realistic faces and avatar images using shared latent codes. The latent codes can then be bridged with the avatar vectors as pairs, by performing GAN inversion on the avatar images rendered from the engine using avatar vectors. Through this way, we are able to synthesize paired data in high-quality as many as possible, consisting of avatar vectors and their corresponding realistic faces. We also propose semantic augmentation to improve the diversity of synthesis. Finally, a light-weight avatar vector estimator is trained on the synthetic pairs to implement efficient auto-creation. Our experiments demonstrate the effectiveness and efficiency of SwiftAvatar on two different avatar engines. The superiority and advantageous flexibility of SwiftAvatar are also verified in both subjective and objective evaluations.



### SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation for Colonoscopy Endorobots
- **Arxiv ID**: http://arxiv.org/abs/2301.08157v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08157v1)
- **Published**: 2023-01-19 16:22:17+00:00
- **Updated**: 2023-01-19 16:22:17+00:00
- **Authors**: Alwyn Mathew, Ludovic Magerand, Emanuele Trucco, Luigi Manfredi
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer is the third most common cause of cancer death worldwide. Optical colonoscopy is the gold standard for detecting colorectal cancer; however, about 25 percent of polyps are missed during the procedure. A vision-based autonomous endorobot can improve colonoscopy procedures significantly through systematic, complete screening of the colonic mucosa. The reliable robot navigation needed requires a three-dimensional understanding of the environment and lumen tracking to support autonomous tasks. We propose a novel multi-task model that simultaneously predicts dense depth and lumen segmentation with an ensemble of deep networks. The depth estimation sub-network is trained in a self-supervised fashion guided by view synthesis; the lumen segmentation sub-network is supervised. The two sub-networks are interconnected with pathways that enable information exchange and thereby mutual learning. As the lumen is in the image's deepest visual space, lumen segmentation helps with the depth estimation at the farthest location. In turn, the estimated depth guides the lumen segmentation network as the lumen location defines the farthest scene location. Unlike other environments, view synthesis often fails in the colon because of the deformable wall, textureless surface, specularities, and wide field of view image distortions, all challenges that our pipeline addresses. We conducted qualitative analysis on a synthetic dataset and quantitative analysis on a colon training model and real colonoscopy videos. The experiments show that our model predicts accurate scale-invariant depth maps and lumen segmentation from colonoscopy images in near real-time.



### FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network
- **Arxiv ID**: http://arxiv.org/abs/2301.08160v1
- **DOI**: 10.1109/TMM.2023.3238521
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08160v1)
- **Published**: 2023-01-19 16:31:13+00:00
- **Updated**: 2023-01-19 16:31:13+00:00
- **Authors**: Huafeng Liu, Pai Peng, Tao Chen, Qiong Wang, Yazhou Yao, Xian-Sheng Hua
- **Comment**: accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Few-shot semantic segmentation is the task of learning to locate each pixel of the novel class in the query image with only a few annotated support images. The current correlation-based methods construct pair-wise feature correlations to establish the many-to-many matching because the typical prototype-based approaches cannot learn fine-grained correspondence relations. However, the existing methods still suffer from the noise contained in naive correlations and the lack of context semantic information in correlations. To alleviate these problems mentioned above, we propose a Feature-Enhanced Context-Aware Network (FECANet). Specifically, a feature enhancement module is proposed to suppress the matching noise caused by inter-class local similarity and enhance the intra-class relevance in the naive correlation. In addition, we propose a novel correlation reconstruction module that encodes extra correspondence relations between foreground and background and multi-scale context semantic features, significantly boosting the encoder to capture a reliable matching pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that our proposed FECANet leads to remarkable improvement compared to previous state-of-the-arts, demonstrating its effectiveness.



### Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection Guidance in Neurosurgery
- **Arxiv ID**: http://arxiv.org/abs/2301.08174v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08174v1)
- **Published**: 2023-01-19 17:05:07+00:00
- **Updated**: 2023-01-19 17:05:07+00:00
- **Authors**: Alistair Weld, Michael Dyck, Julian Klodmann, Giulio Anichini, Luke Dixon, Sophie Camp, Alin Albu-Schäffer, Stamatia Giannarou
- **Comment**: None
- **Journal**: Proceedings of the Hamlyn Symposium 2022
- **Summary**: The aim of this paper is to introduce a robotic platform for autonomous iUS tissue scanning to optimise intraoperative diagnosis and improve surgical resection during robot-assisted operations. To guide anatomy specific robotic scanning and generate a representation of the robot task space, fast and accurate techniques for the recovery of 3D morphological structures of the surgical cavity are developed. The prototypic DLR MIRO surgical robotic arm is used to control the applied force and the in-plane motion of the US transducer. A key application of the proposed platform is the scanning of brain tissue to guide tumour resection.



### A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs
- **Arxiv ID**: http://arxiv.org/abs/2301.08187v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2301.08187v1)
- **Published**: 2023-01-19 17:33:48+00:00
- **Updated**: 2023-01-19 17:33:48+00:00
- **Authors**: Fabian Falck, Christopher Williams, Dominic Danks, George Deligiannidis, Christopher Yau, Chris Holmes, Arnaud Doucet, Matthew Willetts
- **Comment**: NeurIPS 2022 (selected as oral)
- **Journal**: None
- **Summary**: U-Net architectures are ubiquitous in state-of-the-art deep learning, however their regularisation properties and relationship to wavelets are understudied. In this paper, we formulate a multi-resolution framework which identifies U-Nets as finite-dimensional truncations of models on an infinite-dimensional function space. We provide theoretical results which prove that average pooling corresponds to projection within the space of square-integrable functions and show that U-Nets with average pooling implicitly learn a Haar wavelet basis representation of the data. We then leverage our framework to identify state-of-the-art hierarchical VAEs (HVAEs), which have a U-Net architecture, as a type of two-step forward Euler discretisation of multi-resolution diffusion processes which flow from a point mass, introducing sampling instabilities. We also demonstrate that HVAEs learn a representation of time which allows for improved parameter efficiency through weight-sharing. We use this observation to achieve state-of-the-art HVAE performance with half the number of parameters of existing models, exploiting the properties of our continuous-time formulation.



### Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications
- **Arxiv ID**: http://arxiv.org/abs/2301.08189v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2301.08189v1)
- **Published**: 2023-01-19 17:37:40+00:00
- **Updated**: 2023-01-19 17:37:40+00:00
- **Authors**: Mihir Durve, Sibilla Orsini, Adriano Tiribocchi, Andrea Montessori, Jean-Michel Tucny, Marco Lauricella, Andrea Camposeo, Dario Pisignano, Sauro Succi
- **Comment**: 13 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Tracking droplets in microfluidics is a challenging task. The difficulty arises in choosing a tool to analyze general microfluidic videos to infer physical quantities. The state-of-the-art object detector algorithm You Only Look Once (YOLO) and the object tracking algorithm Simple Online and Realtime Tracking with a Deep Association Metric (DeepSORT) are customizable for droplet identification and tracking. The customization includes training YOLO and DeepSORT networks to identify and track the objects of interest. We trained several YOLOv5 and YOLOv7 models and the DeepSORT network for droplet identification and tracking from microfluidic experimental videos. We compare the performance of the droplet tracking applications with YOLOv5 and YOLOv7 in terms of training time and time to analyze a given video across various hardware configurations. Despite the latest YOLOv7 being 10% faster, the real-time tracking is only achieved by lighter YOLO models on RTX 3070 Ti GPU machine due to additional significant droplet tracking costs arising from the DeepSORT algorithm. This work is a benchmark study for the YOLOv5 and YOLOv7 networks with DeepSORT in terms of the training time and inference time for a custom dataset of microfluidic droplets.



### Estimating Remaining Lifespan from the Face
- **Arxiv ID**: http://arxiv.org/abs/2301.08229v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2301.08229v1)
- **Published**: 2023-01-19 18:38:04+00:00
- **Updated**: 2023-01-19 18:38:04+00:00
- **Authors**: Amir Fekrazad
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: The face is a rich source of information that can be utilized to infer a person's biological age, sex, phenotype, genetic defects, and health status. All of these factors are relevant for predicting an individual's remaining lifespan. In this study, we collected a dataset of over 24,000 images (from Wikidata/Wikipedia) of individuals who died of natural causes, along with the number of years between when the image was taken and when the person passed away. We made this dataset publicly available. We fine-tuned multiple Convolutional Neural Network (CNN) models on this data, at best achieving a mean absolute error of 8.3 years in the validation data using VGGFace. However, the model's performance diminishes when the person was younger at the time of the image. To demonstrate the potential applications of our remaining lifespan model, we present examples of using it to estimate the average loss of life (in years) due to the COVID-19 pandemic and to predict the increase in life expectancy that might result from a health intervention such as weight loss. Additionally, we discuss the ethical considerations associated with such models.



### LoCoNet: Long-Short Context Network for Active Speaker Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.08237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08237v1)
- **Published**: 2023-01-19 18:54:43+00:00
- **Updated**: 2023-01-19 18:54:43+00:00
- **Authors**: Xizi Wang, Feng Cheng, Gedas Bertasius, David Crandall
- **Comment**: tech report
- **Journal**: None
- **Summary**: Active Speaker Detection (ASD) aims to identify who is speaking in each frame of a video. ASD reasons from audio and visual information from two contexts: long-term intra-speaker context and short-term inter-speaker context. Long-term intra-speaker context models the temporal dependencies of the same speaker, while short-term inter-speaker context models the interactions of speakers in the same scene. These two contexts are complementary to each other and can help infer the active speaker. Motivated by these observations, we propose LoCoNet, a simple yet effective Long-Short Context Network that models the long-term intra-speaker context and short-term inter-speaker context. We use self-attention to model long-term intra-speaker context due to its effectiveness in modeling long-range dependencies, and convolutional blocks that capture local patterns to model short-term inter-speaker context. Extensive experiments show that LoCoNet achieves state-of-the-art performance on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker, 68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and 59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple speakers are present, or face of active speaker is much smaller than other faces in the same scene, LoCoNet outperforms previous state-of-the-art methods by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at https://github.com/SJTUwxz/LoCoNet_ASD.



### Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
- **Arxiv ID**: http://arxiv.org/abs/2301.08243v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08243v3)
- **Published**: 2023-01-19 18:59:01+00:00
- **Updated**: 2023-04-13 17:59:37+00:00
- **Authors**: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas
- **Comment**: 2023 IEEE/CVF International Conference on Computer Vision
- **Journal**: None
- **Summary**: This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.



### Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2301.08245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08245v1)
- **Published**: 2023-01-19 18:59:28+00:00
- **Updated**: 2023-01-19 18:59:28+00:00
- **Authors**: Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano
- **Comment**: Extension of the paper "Open Challenges in Deep Stereo: the Booster
  Dataset" that was presented at CVPR 2022
- **Journal**: None
- **Summary**: Estimating depth from images nowadays yields outstanding results, both in terms of in-domain accuracy and generalization. However, we identify two main challenges that remain open in this field: dealing with non-Lambertian materials and effectively processing high-resolution images. Purposely, we propose a novel dataset that includes accurate and dense ground-truth labels at high resolution, featuring scenes containing several specular and transparent surfaces. Our acquisition pipeline leverages a novel deep space-time stereo framework, enabling easy and accurate labeling with sub-pixel precision. The dataset is composed of 606 samples collected in 85 different scenes, each sample includes both a high-resolution pair (12 Mpx) as well as an unbalanced stereo pair (Left: 12 Mpx, Right: 1.1 Mpx). Additionally, we provide manually annotated material segmentation masks and 15K unlabeled samples. We divide the dataset into a training set, and two testing sets, the latter devoted to the evaluation of stereo and monocular depth estimation networks respectively to highlight the open challenges and future research directions in this field.



### Multiview Compressive Coding for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2301.08247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08247v1)
- **Published**: 2023-01-19 18:59:52+00:00
- **Updated**: 2023-01-19 18:59:52+00:00
- **Authors**: Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, Georgia Gkioxari
- **Comment**: Project page: https://mcc3d.github.io/
- **Journal**: None
- **Summary**: A central goal of visual recognition is to understand objects and scenes from a single image. 2D recognition has witnessed tremendous progress thanks to large-scale learning and general-purpose representations. Comparatively, 3D poses new challenges stemming from occlusions not depicted in the image. Prior works try to overcome these by inferring from multiple views or rely on scarce CAD models and category-specific priors which hinder scaling to novel settings. In this work, we explore single-view 3D reconstruction by learning generalizable representations inspired by advances in self-supervised learning. We introduce a simple framework that operates on 3D points of single objects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos. Our model, Multiview Compressive Coding (MCC), learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder. MCC's generality and efficiency allow it to learn from large-scale and diverse data sources with strong generalization to novel objects imagined by DALL$\cdot$E 2 or captured in-the-wild with an iPhone.



### Learning ultrasound plane pose regression: assessing generalized pose coordinates in the fetal brain
- **Arxiv ID**: http://arxiv.org/abs/2301.08317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07, I.2.0; I.4.0; J.2; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2301.08317v1)
- **Published**: 2023-01-19 21:16:36+00:00
- **Updated**: 2023-01-19 21:16:36+00:00
- **Authors**: Chiara Di Vece, Maela Le Lous, Brian Dromey, Francisco Vasconcelos, Anna L David, Donald Peebles, Danail Stoyanov
- **Comment**: 12 pages, 9 figures, 2 tables. This work has been submitted to the
  IEEE for possible publication (IEEE TMRB). Copyright may be transferred
  without notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: In obstetric ultrasound (US) scanning, the learner's ability to mentally build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US image represents a significant challenge in skill acquisition. We aim to build a US plane localization system for 3D visualization, training, and guidance without integrating additional sensors. This work builds on top of our previous work, which predicts the six-dimensional (6D) pose of arbitrarily-oriented US planes slicing the fetal brain with respect to a normalized reference frame using a convolutional neural network (CNN) regression network. Here, we analyze in detail the assumptions of the normalized fetal brain reference frame and quantify its accuracy with respect to the acquisition of transventricular (TV) standard plane (SP) for fetal biometry. We investigate the impact of registration quality in the training and testing data and its subsequent effect on trained models. Finally, we introduce data augmentations and larger training sets that improve the results of our previous work, achieving median errors of 3.53 mm and 6.42 degrees for translation and rotation, respectively.



### The role of noise in denoising models for anomaly detection in medical images
- **Arxiv ID**: http://arxiv.org/abs/2301.08330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T99, 92C55, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2301.08330v1)
- **Published**: 2023-01-19 21:39:38+00:00
- **Updated**: 2023-01-19 21:39:38+00:00
- **Authors**: Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O'Neil
- **Comment**: Submitted to Medical Image Analysis special issue for MIDL 2022
- **Journal**: None
- **Summary**: Pathological brain lesions exhibit diverse appearance in brain images, in terms of intensity, texture, shape, size, and location. Comprehensive sets of data and annotations are difficult to acquire. Therefore, unsupervised anomaly detection approaches have been proposed using only normal data for training, with the aim of detecting outlier anomalous voxels at test time. Denoising methods, for instance classical denoising autoencoders (DAEs) and more recently emerging diffusion models, are a promising approach, however naive application of pixelwise noise leads to poor anomaly detection performance. We show that optimization of the spatial resolution and magnitude of the noise improves the performance of different model training regimes, with similar noise parameter adjustments giving good performance for both DAEs and diffusion models. Visual inspection of the reconstructions suggests that the training noise influences the trade-off between the extent of the detail that is reconstructed and the extent of erasure of anomalies, both of which contribute to better anomaly detection performance. We validate our findings on two real-world datasets (tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain CT), showing good detection on diverse anomaly appearances. Overall, we find that a DAE trained with coarse noise is a fast and simple method that gives state-of-the-art accuracy. Diffusion models applied to anomaly detection are as yet in their infancy and provide a promising avenue for further research.



