# Arxiv Papers in cs.CV on 2023-01-23
### Exploring Active 3D Object Detection from a Generalization Perspective
- **Arxiv ID**: http://arxiv.org/abs/2301.09249v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.09249v2)
- **Published**: 2023-01-23 02:43:03+00:00
- **Updated**: 2023-02-08 12:14:34+00:00
- **Authors**: Yadan Luo, Zhuoxiao Chen, Zijian Wang, Xin Yu, Zi Huang, Mahsa Baktashmotlagh
- **Comment**: To appear in ICLR 2023
- **Journal**: None
- **Summary**: To alleviate the high annotation cost in LiDAR-based 3D object detection, active learning is a promising solution that learns to select only a small portion of unlabeled data to annotate, without compromising model performance. Our empirical study, however, suggests that mainstream uncertainty-based and diversity-based active learning policies are not effective when applied in the 3D detection task, as they fail to balance the trade-off between point cloud informativeness and box-level annotation costs. To overcome this limitation, we jointly investigate three novel criteria in our framework Crb for point cloud acquisition - label conciseness}, feature representativeness and geometric balance, which hierarchically filters out the point clouds of redundant 3D bounding box labels, latent features and geometric characteristics (e.g., point cloud density) from the unlabeled sample pool and greedily selects informative ones with fewer objects to annotate. Our theoretical analysis demonstrates that the proposed criteria align the marginal distributions of the selected subset and the prior distributions of the unseen test set, and minimizes the upper bound of the generalization error. To validate the effectiveness and applicability of Crb, we conduct extensive experiments on the two benchmark 3D object detection datasets of KITTI and Waymo and examine both one-stage (i.e., Second) and two-stage 3D detectors (i.e., Pv-rcnn). Experiments evidence that the proposed approach outperforms existing active learning strategies and achieves fully supervised performance requiring $1\%$ and $8\%$ annotations of bounding boxes and point clouds, respectively. Source code: https://github.com/Luoyadan/CRB-active-3Ddet.



### CircNet: Meshing 3D Point Clouds with Circumcenter Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.09253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09253v2)
- **Published**: 2023-01-23 03:32:57+00:00
- **Updated**: 2023-08-20 23:34:39+00:00
- **Authors**: Huan Lei, Ruitao Leng, Liang Zheng, Hongdong Li
- **Comment**: accepted to ICLR2023
- **Journal**: None
- **Summary**: Reconstructing 3D point clouds into triangle meshes is a key problem in computational geometry and surface reconstruction. Point cloud triangulation solves this problem by providing edge information to the input points. Since no vertex interpolation is involved, it is beneficial to preserve sharp details on the surface. Taking advantage of learning-based techniques in triangulation, existing methods enumerate the complete combinations of candidate triangles, which is both complex and inefficient. In this paper, we leverage the duality between a triangle and its circumcenter, and introduce a deep neural network that detects the circumcenters to achieve point cloud triangulation. Specifically, we introduce multiple anchor priors to divide the neighborhood space of each point. The neural network then learns to predict the presences and locations of circumcenters under the guidance of those anchors. We extract the triangles dual to the detected circumcenters to form a primitive mesh, from which an edge-manifold mesh is produced via simple post-processing. Unlike existing learning-based triangulation methods, the proposed method bypasses an exhaustive enumeration of triangle combinations and local surface parameterization. We validate the efficiency, generalization, and robustness of our method on prominent datasets of both watertight and open surfaces. The code and trained models are provided at https://github.com/EnyaHermite/CircNet.



### Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference
- **Arxiv ID**: http://arxiv.org/abs/2301.09254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09254v1)
- **Published**: 2023-01-23 03:33:38+00:00
- **Updated**: 2023-01-23 03:33:38+00:00
- **Authors**: Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, Peter A. Beerel
- **Comment**: 15 pages, 10 figures, 11 tables. Accepted as a conference paper at
  ICLR 2023
- **Journal**: None
- **Summary**: The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers' ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer's activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet's superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to ~2x fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved classification accuracy, evaluated on CIFAR-100.



### Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.09255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09255v2)
- **Published**: 2023-01-23 03:41:02+00:00
- **Updated**: 2023-03-03 14:09:15+00:00
- **Authors**: Teru Nagamori, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, privacy-preserving methods for deep learning have become an urgent problem. Accordingly, we propose the combined use of federated learning (FL) and encrypted images for privacy-preserving image classification under the use of the vision transformer (ViT). The proposed method allows us not only to train models over multiple participants without directly sharing their raw data but to also protect the privacy of test (query) images for the first time. In addition, it can also maintain the same accuracy as normally trained models. In an experiment, the proposed method was demonstrated to well work without any performance degradation on the CIFAR-10 and CIFAR-100 datasets.



### Real-Time Simultaneous Localization and Mapping with LiDAR intensity
- **Arxiv ID**: http://arxiv.org/abs/2301.09257v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.09257v2)
- **Published**: 2023-01-23 03:59:48+00:00
- **Updated**: 2023-06-19 19:32:29+00:00
- **Authors**: Wenqiang Du, Giovanni Beltrame
- **Comment**: Accepted by ICRA 2023, code released:
  https://github.com/SnowCarter/Intensity_based_LiDAR_SLAM.git
- **Journal**: None
- **Summary**: We propose a novel real-time LiDAR intensity image-based simultaneous localization and mapping method , which addresses the geometry degeneracy problem in unstructured environments. Traditional LiDAR-based front-end odometry mostly relies on geometric features such as points, lines and planes. A lack of these features in the environment can lead to the failure of the entire odometry system. To avoid this problem, we extract feature points from the LiDAR-generated point cloud that match features identified in LiDAR intensity images. We then use the extracted feature points to perform scan registration and estimate the robot ego-movement. For the back-end, we jointly optimize the distance between the corresponding feature points, and the point to plane distance for planes identified in the map. In addition, we use the features extracted from intensity images to detect loop closure candidates from previous scans and perform pose graph optimization. Our experiments show that our method can run in real time with high accuracy and works well with illumination changes, low-texture, and unstructured environments.



### Efficient Training Under Limited Resources
- **Arxiv ID**: http://arxiv.org/abs/2301.09264v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2301.09264v1)
- **Published**: 2023-01-23 04:26:20+00:00
- **Updated**: 2023-01-23 04:26:20+00:00
- **Authors**: Mahdi Zolnouri, Dounia Lakhmiri, Christophe Tribes, Eyyüb Sari, Sébastien Le Digabel
- **Comment**: None
- **Journal**: None
- **Summary**: Training time budget and size of the dataset are among the factors affecting the performance of a Deep Neural Network (DNN). This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments. We present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition. The competition results can be found at haet2021.github.io/challenge and our source code can be found at github.com/DouniaLakhmiri/ICLR\_HAET2021.



### FInC Flow: Fast and Invertible $k \times k$ Convolutions for Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2301.09266v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09266v1)
- **Published**: 2023-01-23 04:31:03+00:00
- **Updated**: 2023-01-23 04:31:03+00:00
- **Authors**: Aditya Kallappa, Sandeep Nagar, Girish Varma
- **Comment**: accepted: VISAPP'23
- **Journal**: VISIGRAPP, 2023
- **Summary**: Invertible convolutions have been an essential element for building expressive normalizing flow-based generative models since their introduction in Glow. Several attempts have been made to design invertible $k \times k$ convolutions that are efficient in training and sampling passes. Though these attempts have improved the expressivity and sampling efficiency, they severely lagged behind Glow which used only $1 \times 1$ convolutions in terms of sampling time. Also, many of the approaches mask a large number of parameters of the underlying convolution, resulting in lower expressivity on a fixed run-time budget. We propose a $k \times k$ convolutional layer and Deep Normalizing Flow architecture which i.) has a fast parallel inversion algorithm with running time O$(n k^2)$ ($n$ is height and width of the input image and k is kernel size), ii.) masks the minimal amount of learnable parameters in a layer. iii.) gives better forward pass and sampling times comparable to other $k \times k$ convolution-based models on real-world benchmarks. We provide an implementation of the proposed parallel algorithm for sampling using our invertible convolutions on GPUs. Benchmarks on CIFAR-10, ImageNet, and CelebA datasets show comparable performance to previous works regarding bits per dimension while significantly improving the sampling time.



### PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2301.09268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09268v1)
- **Published**: 2023-01-23 04:34:25+00:00
- **Updated**: 2023-01-23 04:34:25+00:00
- **Authors**: Brian Li, Steven Palayew, Francis Li, Saad Abbasi, Saeejith Nair, Alexander Wong
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: There can be numerous electronic components on a given PCB, making the task of visual inspection to detect defects very time-consuming and prone to error, especially at scale. There has thus been significant interest in automatic PCB component detection, particularly leveraging deep learning. However, deep neural networks typically require high computational resources, possibly limiting their feasibility in real-world use cases in manufacturing, which often involve high-volume and high-throughput detection with constrained edge computing resource availability. As a result of an exploration of efficient deep neural network architectures for this use case, we introduce PCBDet, an attention condenser network design that provides state-of-the-art inference throughput while achieving superior PCB component detection performance compared to other state-of-the-art efficient architecture designs. Experimental results show that PCBDet can achieve up to 2$\times$ inference speed-up on an ARM Cortex A72 processor when compared to an EfficientNet-based design while achieving $\sim$2-4\% higher mAP on the FICS-PCB benchmark dataset.



### Classification of Luminal Subtypes in Full Mammogram Images Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.09282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09282v1)
- **Published**: 2023-01-23 05:58:26+00:00
- **Updated**: 2023-01-23 05:58:26+00:00
- **Authors**: Adarsh Bhandary Panambur, Prathmesh Madhu, Andreas Maier
- **Comment**: Submitted to IEEE ISBI 2023
- **Journal**: None
- **Summary**: Automatic identification of patients with luminal and non-luminal subtypes during a routine mammography screening can support clinicians in streamlining breast cancer therapy planning. Recent machine learning techniques have shown promising results in molecular subtype classification in mammography; however, they are highly dependent on pixel-level annotations, handcrafted, and radiomic features. In this work, we provide initial insights into the luminal subtype classification in full mammogram images trained using only image-level labels. Transfer learning is applied from a breast abnormality classification task, to finetune a ResNet-18-based luminal versus non-luminal subtype classification task. We present and compare our results on the publicly available CMMD dataset and show that our approach significantly outperforms the baseline classifier by achieving a mean AUC score of 0.6688 and a mean F1 score of 0.6693 on the test dataset. The improvement over baseline is statistically significant, with a p-value of p<0.0001.



### Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay
- **Arxiv ID**: http://arxiv.org/abs/2301.09299v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09299v1)
- **Published**: 2023-01-23 07:00:04+00:00
- **Updated**: 2023-01-23 07:00:04+00:00
- **Authors**: Yinheng Li, Han Ding, Shaofei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has become a popular approach in recent years for its ability to learn meaningful representations without the need for data annotation. This paper proposes a novel image augmentation technique, overlaying images, which has not been widely applied in self-supervised learning. This method is designed to provide better guidance for the model to understand underlying information, resulting in more useful representations. The proposed method is evaluated using contrastive learning, a widely used self-supervised learning method that has shown solid performance in downstream tasks. The results demonstrate the effectiveness of the proposed augmentation technique in improving the performance of self-supervised models.



### AI-Based Framework for Understanding Car Following Behaviors of Drivers in A Naturalistic Driving Environment
- **Arxiv ID**: http://arxiv.org/abs/2301.09315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09315v1)
- **Published**: 2023-01-23 08:24:33+00:00
- **Updated**: 2023-01-23 08:24:33+00:00
- **Authors**: Armstrong Aboah, Abdul Rashid Mussah, Yaw Adu-Gyamfi
- **Comment**: None
- **Journal**: None
- **Summary**: The most common type of accident on the road is a rear-end crash. These crashes have a significant negative impact on traffic flow and are frequently fatal. To gain a more practical understanding of these scenarios, it is necessary to accurately model car following behaviors that result in rear-end crashes. Numerous studies have been carried out to model drivers' car-following behaviors; however, the majority of these studies have relied on simulated data, which may not accurately represent real-world incidents. Furthermore, most studies are restricted to modeling the ego vehicle's acceleration, which is insufficient to explain the behavior of the ego vehicle. As a result, the current study attempts to address these issues by developing an artificial intelligence framework for extracting features relevant to understanding driver behavior in a naturalistic environment. Furthermore, the study modeled the acceleration of both the ego vehicle and the leading vehicle using extracted information from NDS videos. According to the study's findings, young people are more likely to be aggressive drivers than elderly people. In addition, when modeling the ego vehicle's acceleration, it was discovered that the relative velocity between the ego vehicle and the leading vehicle was more important than the distance between the two vehicles.



### Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.09318v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09318v3)
- **Published**: 2023-01-23 08:35:00+00:00
- **Updated**: 2023-06-01 12:37:41+00:00
- **Authors**: Johannes Jakubik, Michal Muszynski, Michael Vössing, Niklas Kühl, Thomas Brunschwiler
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2023)
- **Journal**: None
- **Summary**: Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for the segmentation of unseen natural hazards. Importantly, our method is invariant to geographic differences and differences in the type of frequency bands of satellite data. By leveraging characteristics of unlabeled images from the target domain that are publicly available, our approach is able to further improve the generalization behavior without fine-tuning. Thereby, our approach supports the development of foundation models for earth monitoring with the objective of directly segmenting unseen natural hazards across novel geographic regions given different sources of satellite imagery.



### Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2301.09322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09322v1)
- **Published**: 2023-01-23 08:46:17+00:00
- **Updated**: 2023-01-23 08:46:17+00:00
- **Authors**: Neus Rodeja Ferrer, Malini Vendela Sagar, Kiril Vadimovic Klein, Christina Kruuse, Mads Nielsen, Mostafa Mehdipour Ghazi
- **Comment**: International Symposium on Biomedical Imaging (ISBI) 2023
- **Journal**: None
- **Summary**: Cerebral Microbleeds (CMBs), typically captured as hypointensities from susceptibility-weighted imaging (SWI), are particularly important for the study of dementia, cerebrovascular disease, and normal aging. Recent studies on COVID-19 have shown an increase in CMBs of coronavirus cases. Automatic detection of CMBs is challenging due to the small size and amount of CMBs making the classes highly imbalanced, lack of publicly available annotated data, and similarity with CMB mimics such as calcifications, irons, and veins. Hence, the existing deep learning methods are mostly trained on very limited research data and fail to generalize to unseen data with high variability and cannot be used in clinical setups. To this end, we propose an efficient 3D deep learning framework that is actively trained on multi-domain data. Two public datasets assigned for normal aging, stroke, and Alzheimer's disease analysis as well as an in-house dataset for COVID-19 assessment are used to train and evaluate the models. The obtained results show that the proposed method is robust to low-resolution images and achieves 78% recall and 80% precision on the entire test set with an average false positive of 1.6 per scan.



### Employing similarity to highlight differences: On the impact of anatomical assumptions in chest X-ray registration methods
- **Arxiv ID**: http://arxiv.org/abs/2301.09338v2
- **DOI**: 10.1016/j.compbiomed.2023.106543
- **Categories**: **cs.CV**, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2301.09338v2)
- **Published**: 2023-01-23 09:42:49+00:00
- **Updated**: 2023-01-24 10:18:24+00:00
- **Authors**: Astrid Berg, Eva Vandersmissen, Maria Wimmer, David Major, Theresa Neubauer, Dimitrios Lenis, Jeroen Cant, Annemiek Snoeckx, Katja Bühler
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, Volume 154, 2023, 106543, ISSN
  0010-4825
- **Summary**: To facilitate both the detection and the interpretation of findings in chest X-rays, comparison with a previous image of the same patient is very valuable to radiologists. Today, the most common approach for deep learning methods to automatically inspect chest X-rays disregards the patient history and classifies only single images as normal or abnormal. Nevertheless, several methods for assisting in the task of comparison through image registration have been proposed in the past. However, as we illustrate, they tend to miss specific types of pathological changes like cardiomegaly and effusion. Due to assumptions on fixed anatomical structures or their measurements of registration quality, they produce unnaturally deformed warp fields impacting visualization of differences between moving and fixed images. We aim to overcome these limitations, through a new paradigm based on individual rib pair segmentation for anatomy penalized registration. Our method proves to be a natural way to limit the folding percentage of the warp field to 1/6 of the state of the art while increasing the overlap of ribs by more than 25%, implying difference images showing pathological changes overlooked by other methods. We develop an anatomically penalized convolutional multi-stage solution on the National Institutes of Health (NIH) data set, starting from less than 25 fully and 50 partly labeled training images, employing sequential instance memory segmentation with hole dropout, weak labeling, coarse-to-fine refinement and Gaussian mixture model histogram matching. We statistically evaluate the benefits of our method and highlight the limits of currently used metrics for registration of chest X-rays.



### Computer Vision for a Camel-Vehicle Collision Mitigation System
- **Arxiv ID**: http://arxiv.org/abs/2301.09339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.09339v1)
- **Published**: 2023-01-23 09:45:31+00:00
- **Updated**: 2023-01-23 09:45:31+00:00
- **Authors**: Khalid Alnujaidi, Ghadah Alhabib
- **Comment**: None
- **Journal**: None
- **Summary**: As the population grows and more land is being used for urbanization, ecosystems are disrupted by our roads and cars. This expansion of infrastructure cuts through wildlife territories, leading to many instances of Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue that is having a global socio-economic impact, resulting in billions of dollars in property damage and, at times, fatalities for vehicle occupants. In Saudi Arabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC) being particularly deadly due to the large size of camels, which results in a 25% fatality rate [4]. The focus of this work is to test different object detection models on the task of detecting camels on the road. The Deep Learning (DL) object detection models used in the experiments are: CenterNet, EfficientDet, Faster R-CNN, and SSD. Results of the experiments show that CenterNet performed the best in terms of accuracy and was the most efficient in training. In the future, the plan is to expand on this work by developing a system to make countryside roads safer.



### Crowd3D: Towards Hundreds of People Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2301.09376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09376v2)
- **Published**: 2023-01-23 11:45:27+00:00
- **Updated**: 2023-04-01 14:17:26+00:00
- **Authors**: Hao Wen, Jing Huang, Huili Cui, Haozhe Lin, YuKun Lai, Lu Fang, Kun Li
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Image-based multi-person reconstruction in wide-field large scenes is critical for crowd analysis and security alert. However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the challenges of large number of people, large variations in human scale, and complex spatial distribution. In this paper, we propose Crowd3D, the first framework to reconstruct the 3D poses, shapes and locations of hundreds of people with global consistency from a single large-scene image. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interaction Point (HVIP). To reconstruct the crowd with global consistency, we propose a progressive reconstruction network based on HVIP by pre-estimating a scene-level camera and a ground plane. To deal with a large number of persons and various human sizes, we also design an adaptive human-centric cropping scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd reconstruction in a large scene. Experimental results demonstrate the effectiveness of the proposed method. The code and datasets will be made public.



### Is Autoencoder Truly Applicable for 3D CT Super-Resolution?
- **Arxiv ID**: http://arxiv.org/abs/2302.10272v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10272v2)
- **Published**: 2023-01-23 12:48:08+00:00
- **Updated**: 2023-03-31 16:33:22+00:00
- **Authors**: Weixun Luo, Xiaodan Xing, Guang Yang
- **Comment**: ISBI 2023
- **Journal**: None
- **Summary**: Featured by a bottleneck structure, autoencoder (AE) and its variants have been largely applied in various medical image analysis tasks, such as segmentation, reconstruction and de-noising. Despite of their promising performances in aforementioned tasks, in this paper, we claim that AE models are not applicable to single image super-resolution (SISR) for 3D CT data. Our hypothesis is that the bottleneck architecture that resizes feature maps in AE models degrades the details of input images, thus can sabotage the performance of super-resolution. Although U-Net proposed skip connections that merge information from different levels, we claim that the degrading impact of feature resizing operations could hardly be removed by skip connections. By conducting large-scale ablation experiments and comparing the performance between models with and without the bottleneck design on a public CT lung dataset , we have discovered that AE models, including U-Net, have failed to achieve a compatible SISR result ($p<0.05$ by Student's t-test) compared to the baseline model. Our work is the first comparative study investigating the suitability of AE architecture for 3D CT SISR tasks and brings a rationale for researchers to re-think the choice of model architectures especially for 3D CT SISR tasks. The full implementation and trained models can be found at: https://github.com/Roldbach/Autoencoder-3D-CT-SISR



### ViGU: Vision GNN U-Net for Fast MRI
- **Arxiv ID**: http://arxiv.org/abs/2302.10273v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10273v1)
- **Published**: 2023-01-23 12:51:57+00:00
- **Updated**: 2023-01-23 12:51:57+00:00
- **Authors**: Jiahao Huang, Angelica Aviles-Rivero, Carola-Bibiane Schonlieb, Guang Yang
- **Comment**: ISBI2023
- **Journal**: None
- **Summary**: Deep learning models have been widely applied for fast MRI. The majority of existing deep learning models, e.g., convolutional neural networks, work on data with Euclidean or regular grids structures. However, high-dimensional features extracted from MR data could be encapsulated in non-Euclidean manifolds. This disparity between the go-to assumption of existing models and data requirements limits the flexibility to capture irregular anatomical features in MR data. In this work, we introduce a novel Vision GNN type network for fast MRI called Vision GNN U-Net (ViGU). More precisely, the pixel array is first embedded into patches and then converted into a graph. Secondly, a U-shape network is developed using several graph blocks in symmetrical encoder and decoder paths. Moreover, we show that the proposed ViGU can also benefit from Generative Adversarial Networks yielding to its variant ViGU-GAN. We demonstrate, through numerical and visual experiments, that the proposed ViGU and GAN variant outperform existing CNN and GAN-based methods. Moreover, we show that the proposed network readily competes with approaches based on Transformers while requiring a fraction of the computational cost. More importantly, the graph structure of the network reveals how the network extracts features from MR images, providing intuitive explainability.



### RainDiffusion: When Unsupervised Learning Meets Diffusion Models for Real-world Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2301.09430v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09430v3)
- **Published**: 2023-01-23 13:34:01+00:00
- **Updated**: 2023-03-09 03:19:30+00:00
- **Authors**: Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Jing Qin, Fu Lee Wang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recent diffusion models show great potential in generative modeling tasks. This motivates us to raise an intriguing question - What will happen when unsupervised learning meets diffusion models for real-world image deraining? Before answering it, we observe two major obstacles of diffusion models in real-world image deraining: the need for paired training data and the limited utilization of multi-scale rain patterns. To overcome the obstacles, we propose RainDiffusion, the first real-world image deraining paradigm based on diffusion models. RainDiffusion is a non-adversarial training paradigm that introduces stable training of unpaired real-world data, rather than weakly adversarial training, serving as a new standard bar for real-world image deraining. It consists of two cooperative branches: Non-diffusive Translation Branch (NTB) and Diffusive Translation Branch (DTB). NTB exploits a cycle-consistent architecture to bypass the difficulty in unpaired training of regular diffusion models by generating initial clean/rainy image pairs. Given initial image pairs, DTB leverages multi-scale diffusion models to progressively refine the desired output via diffusive generative and multi-scale priors, to obtain a better generalization capacity of real-world image deraining. Extensive experiments confirm the superiority of our RainDiffusion over eight un/semi-supervised methods and show its competitive advantages over seven fully-supervised ones.



### Multi-domain stain normalization for digital pathology: A cycle-consistent adversarial network for whole slide images
- **Arxiv ID**: http://arxiv.org/abs/2301.09431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09431v1)
- **Published**: 2023-01-23 13:34:49+00:00
- **Updated**: 2023-01-23 13:34:49+00:00
- **Authors**: Martin J. Hetz, Tabea-Clara Bucher, Titus J. Brinker
- **Comment**: 19 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: The variation in histologic staining between different medical centers is one of the most profound challenges in the field of computer-aided diagnosis. The appearance disparity of pathological whole slide images causes algorithms to become less reliable, which in turn impedes the wide-spread applicability of downstream tasks like cancer diagnosis. Furthermore, different stainings lead to biases in the training which in case of domain shifts negatively affect the test performance. Therefore, in this paper we propose MultiStain-CycleGAN, a multi-domain approach to stain normalization based on CycleGAN. Our modifications to CycleGAN allow us to normalize images of different origins without retraining or using different models. We perform an extensive evaluation of our method using various metrics and compare it to commonly used methods that are multi-domain capable. First, we evaluate how well our method fools a domain classifier that tries to assign a medical center to an image. Then, we test our normalization on the tumor classification performance of a downstream classifier. Furthermore, we evaluate the image quality of the normalized images using the Structural similarity index and the ability to reduce the domain shift using the Fr\'echet inception distance. We show that our method proves to be multi-domain capable, provides the highest image quality among the compared methods, and can most reliably fool the domain classifier while keeping the tumor classifier performance high. By reducing the domain influence, biases in the data can be removed on the one hand and the origin of the whole slide image can be disguised on the other, thus enhancing patient data privacy.



### GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.10018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10018v2)
- **Published**: 2023-01-23 13:44:15+00:00
- **Updated**: 2023-05-29 11:46:42+00:00
- **Authors**: Haipeng Li, Kunming Luo, Bing Zeng, Shuaicheng Liu
- **Comment**: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:2103.13725
- **Journal**: None
- **Summary**: Existing homography and optical flow methods are erroneous in challenging scenes, such as fog, rain, night, and snow because the basic assumptions such as brightness and gradient constancy are broken. To address this issue, we present an unsupervised learning approach that fuses gyroscope into homography and optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module (SGF) to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. Meanwhile, we propose a homography decoder module (HD) to combine gyro field and intermediate results of SGF to produce the homography. To the best of our knowledge, this is the first deep learning framework that fuses gyroscope data and image content for both deep homography and optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-the-art methods in both regular and challenging scenes.



### A Simple Recipe for Competitive Low-compute Self supervised Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2301.09451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09451v1)
- **Published**: 2023-01-23 14:20:01+00:00
- **Updated**: 2023-01-23 14:20:01+00:00
- **Authors**: Quentin Duval, Ishan Misra, Nicolas Ballas
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised methods in vision have been mostly focused on large architectures as they seem to suffer from a significant performance drop for smaller architectures. In this paper, we propose a simple self-supervised distillation technique that can train high performance low-compute neural networks. Our main insight is that existing joint-embedding based SSL methods can be repurposed for knowledge distillation from a large self-supervised teacher to a small student model. Thus, we call our method Replace one Branch (RoB) as it simply replaces one branch of the joint-embedding training with a large teacher model. RoB is widely applicable to a number of architectures such as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV or iBOT. When pretraining on the ImageNet dataset, RoB yields models that compete with supervised knowledge distillation. When applied to MSN, RoB produces students with strong semi-supervised capabilities. Finally, our best ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\%$ and are on par or better than a supervised distilled DeiT on five downstream transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We hope RoB enables practical self-supervision at smaller scale.



### Fast and robust single particle reconstruction in 3D fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/2301.09452v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09452v1)
- **Published**: 2023-01-23 14:20:01+00:00
- **Updated**: 2023-01-23 14:20:01+00:00
- **Authors**: Thibaut Eloy, Etienne Baudrier, Marine Laporte, Virginie Hamel, Paul Guichard, Denis Fortun
- **Comment**: None
- **Journal**: None
- **Summary**: Single particle reconstruction has recently emerged in 3D fluorescence microscopy as a powerful technique to improve the axial resolution and the degree of fluorescent labeling. It is based on the reconstruction of an average volume of a biological particle from the acquisition multiple views with unknown poses. Current methods are limited either by template bias, restriction to 2D data, high computational cost or a lack of robustness to low fluorescent labeling. In this work, we propose a single particle reconstruction method dedicated to convolutional models in 3D fluorescence microscopy that overcome these issues. We address the joint reconstruction and estimation of the poses of the particles, which translates into a challenging non-convex optimization problem. Our approach is based on a multilevel reformulation of this problem, and the development of efficient optimization techniques at each level. We demonstrate on synthetic data that our method outperforms the standard approaches in terms of resolution and reconstruction error, while achieving a low computational cost. We also perform successful reconstruction on real datasets of centrioles to show the potential of our method in concrete applications.



### HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2301.09460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09460v1)
- **Published**: 2023-01-23 14:36:38+00:00
- **Updated**: 2023-01-23 14:36:38+00:00
- **Authors**: Kun Li, George Vosselman, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) is an important and challenging multimodal task in computer vision. Recently, a few efforts have been made to bring VQA task to aerial images, due to its potential real-world applications in disaster monitoring, urban planning, and digital earth product generation. However, not only the huge variation in the appearance, scale and orientation of the concepts in aerial images, but also the scarcity of the well-annotated datasets restricts the development of VQA in this domain. In this paper, we introduce a new dataset, HRVQA, which provides collected 53512 aerial images of 1024*1024 pixels and semi-automatically generated 1070240 QA pairs. To benchmark the understanding capability of VQA models for aerial images, we evaluate the relevant methods on HRVQA. Moreover, we propose a novel model, GFTransformer, with gated attention modules and a mutual fusion module. The experiments show that the proposed dataset is quite challenging, especially the specific attribute related questions. Our method achieves superior performance in comparison to the previous state-of-the-art approaches. The dataset and the source code will be released at https://hrvqa.nl/.



### Study on the identification limits of craniofacial superimposition
- **Arxiv ID**: http://arxiv.org/abs/2301.09461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68U10, I.4.9; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2301.09461v2)
- **Published**: 2023-01-23 14:46:43+00:00
- **Updated**: 2023-01-24 09:58:36+00:00
- **Authors**: Óscar Ibáñez, Enrique Bermejo, Andrea Valsecchi
- **Comment**: 7 pages, 4 figures. To be submitted to Scientific Reports
- **Journal**: None
- **Summary**: Craniofacial Superimposition involves the superimposition of an image of a skull with a number of ante-mortem face images of an individual and the analysis of their morphological correspondence. Despite being used for one century, it is not yet a mature and fully accepted technique due to the absence of solid scientific approaches, significant reliability studies, and international standards. In this paper we present a comprehensive experimentation on the limitations of Craniofacial Superimposition as a forensic identification technique. The study involves different experiments over more than 1 Million comparisons performed by a landmark-based automatic 3D/2D superimposition method. The total sample analyzed consists of 320 subjects and 29 craniofacial landmarks.



### Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.09489v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09489v4)
- **Published**: 2023-01-23 15:32:27+00:00
- **Updated**: 2023-05-23 13:50:31+00:00
- **Authors**: Alessandro Flaborea, Guido D'Amely, Stefano D'Arrigo, Marco Aurelio Sterpa, Alessio Sampieri, Fabio Galasso
- **Comment**: Submitted to Pattern Recognition Journal
- **Journal**: None
- **Summary**: Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model that encodes skeletal human motion by a graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Video Anomaly Detection. We propose three latent spaces: the commonly-adopted Euclidean and the novel spherical and hyperbolic. All variants outperform the state-of-the-art on the most recent UBnormal dataset, for which we contribute a human-related version with annotated skeletons. COSKAD sets a new state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK Avenue, with performance comparable to video-based methods. Source code and dataset will be released upon acceptance.



### Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2301.09498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09498v2)
- **Published**: 2023-01-23 15:52:12+00:00
- **Updated**: 2023-03-16 01:51:00+00:00
- **Authors**: Fei Shen, Xiaoyu Du, Liyan Zhang, Xiangbo Shu, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Part feature learning is critical for fine-grained semantic understanding in vehicle re-identification. However, existing approaches directly model part features and global features, which can easily lead to serious gradient vanishing issues due to their unequal feature information and unreliable pseudo-labels for unsupervised vehicle re-identification. To address this problem, in this paper, we propose a simple Triplet Contrastive Representation Learning (TCRL) framework which leverages cluster features to bridge the part features and global features for unsupervised vehicle re-identification. Specifically, TCRL devises three memory banks to store the instance/cluster features and proposes a Proxy Contrastive Loss (PCL) to make contrastive learning between adjacent memory banks, thus presenting the associations between the part and global features as a transition of the part-cluster and cluster-global associations. Since the cluster memory bank copes with all the vehicle features, it can summarize them into a discriminative feature representation. To deeply exploit the instance/cluster information, TCRL proposes two additional loss functions. For the instance-level feature, a Hybrid Contrastive Loss (HCL) re-defines the sample correlations by approaching the positive instance features and pushing the all negative instance features away. For the cluster-level feature, a Weighted Regularization Cluster Contrastive Loss (WRCCL) refines the pseudo labels by penalizing the mislabeled images according to the instance similarity. Extensive experiments show that TCRL outperforms many state-of-the-art unsupervised vehicle re-identification approaches.



### OvarNet: Towards Open-vocabulary Object Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.09506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09506v1)
- **Published**: 2023-01-23 15:59:29+00:00
- **Updated**: 2023-01-23 15:59:29+00:00
- **Authors**: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories.



### StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.09515v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09515v1)
- **Published**: 2023-01-23 16:05:45+00:00
- **Updated**: 2023-01-23 16:05:45+00:00
- **Authors**: Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Timo Aila
- **Comment**: Project page: https://sites.google.com/view/stylegan-t/
- **Journal**: None
- **Summary**: Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.



### Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff
- **Arxiv ID**: http://arxiv.org/abs/2301.09522v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09522v2)
- **Published**: 2023-01-23 16:14:09+00:00
- **Updated**: 2023-08-01 17:15:30+00:00
- **Authors**: Dengyu Wu, Gaojie Jin, Han Yu, Xinping Yi, Xiaowei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs), a variant of artificial neural networks (ANNs) with the benefit of energy efficiency, have achieved the accuracy close to its ANN counterparts, on benchmark datasets such as CIFAR10/100 and ImageNet. However, comparing with frame-based input (e.g., images), event-based inputs from e.g., Dynamic Vision Sensor (DVS) can make a better use of SNNs thanks to the SNNs' asynchronous working mechanism. In this paper, we strengthen the marriage between SNNs and event-based inputs with a proposal to consider anytime optimal inference SNNs, or AOI-SNNs, which can terminate anytime during the inference to achieve optimal inference result. Two novel optimisation techniques are presented to achieve AOI-SNNs: a regularisation and a cutoff. The regularisation enables the training and construction of SNNs with optimised performance, and the cutoff technique optimises the inference of SNNs on event-driven inputs. We conduct an extensive set of experiments on multiple benchmark event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128 Gesture. The experimental results demonstrate that our techniques are superior to the state-of-the-art with respect to the accuracy and latency.



### DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.09525v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09525v1)
- **Published**: 2023-01-23 16:16:24+00:00
- **Updated**: 2023-01-23 16:16:24+00:00
- **Authors**: Nima Hatami
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2104.00669
- **Journal**: None
- **Summary**: Computational pathology tasks have some unique characterises such as multi-gigapixel images, tedious and frequently uncertain annotations, and unavailability of large number of cases [13]. To address some of these issues, we present Deep Fastfood Ensembles - a simple, fast and yet effective method for combining deep features pooled from popular CNN models pre-trained on totally different source domains (e.g., natural image objects) and projected onto diverse dimensions using random projections, the so-called Fastfood [11]. The final ensemble output is obtained by a consensus of simple individual classifiers, each of which is trained on a different collection of random basis vectors. This offers extremely fast and yet effective solution, especially when training times and domain labels are of the essence. We demonstrate the effectiveness of the proposed deep fastfood ensemble learning as compared to the state-of-the-art methods for three different tasks in histopathology image analysis.



### Improving Presentation Attack Detection for ID Cards on Remote Verification Systems
- **Arxiv ID**: http://arxiv.org/abs/2301.09542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09542v1)
- **Published**: 2023-01-23 16:59:26+00:00
- **Updated**: 2023-01-23 16:59:26+00:00
- **Authors**: Sebastian Gonzalez, Juan Tapia
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an updated two-stage, end-to-end Presentation Attack Detection method for remote biometric verification systems of ID cards, based on MobileNetV2, is presented. Several presentation attack species such as printed, display, composite (based on cropped and spliced areas), plastic (PVC), and synthetic ID card images using different capture sources are used. This proposal was developed using a database consisting of 190.000 real case Chilean ID card images with the support of a third-party company. Also, a new framework called PyPAD, used to estimate multi-class metrics compliant with the ISO/IEC 30107-3 standard was developed, and will be made available for research purposes. Our method is trained on two convolutional neural networks separately, reaching BPCER\textsubscript{100} scores on ID cards attacks of 1.69\% and 2.36\% respectively. The two-stage method using both models together can reach a BPCER\textsubscript{100} score of 0.92\%.



### Learning to View: Decision Transformers for Active Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.09544v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09544v1)
- **Published**: 2023-01-23 17:00:48+00:00
- **Updated**: 2023-01-23 17:00:48+00:00
- **Authors**: Wenhao Ding, Nathalie Majcherczyk, Mohit Deshpande, Xuewei Qi, Ding Zhao, Rajasimman Madhivanan, Arnie Sen
- **Comment**: Accepted to ICRA 2023
- **Journal**: None
- **Summary**: Active perception describes a broad class of techniques that couple planning and perception systems to move the robot in a way to give the robot more information about the environment. In most robotic systems, perception is typically independent of motion planning. For example, traditional object detection is passive: it operates only on the images it receives. However, we have a chance to improve the results if we allow planning to consume detection signals and move the robot to collect views that maximize the quality of the results. In this paper, we use reinforcement learning (RL) methods to control the robot in order to obtain images that maximize the detection quality. Specifically, we propose using a Decision Transformer with online fine-tuning, which first optimizes the policy with a pre-collected expert dataset and then improves the learned policy by exploring better solutions in the environment. We evaluate the performance of proposed method on an interactive dataset collected from an indoor scenario simulator. Experimental results demonstrate that our method outperforms all baselines, including expert policy and pure offline RL methods. We also provide exhaustive analyses of the reward distribution and observation space.



### Zorro: the masked multimodal transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.09595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09595v2)
- **Published**: 2023-01-23 17:51:39+00:00
- **Updated**: 2023-02-22 18:58:10+00:00
- **Authors**: Adrià Recasens, Jason Lin, Joāo Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac, Pauline Luc, Antoine Miech, Lucas Smaira, Ross Hemsley, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: Attention-based models are appealing for multimodal processing because inputs from multiple modalities can be concatenated and fed to a single backbone network - thus requiring very little fusion engineering. The resulting representations are however fully entangled throughout the network, which may not always be desirable: in learning, contrastive audio-visual self-supervised learning requires independent audio and visual features to operate, otherwise learning collapses; in inference, evaluation of audio-visual models should be possible on benchmarks having just audio or just video. In this paper, we introduce Zorro, a technique that uses masks to control how inputs from each modality are routed inside Transformers, keeping some parts of the representation modality-pure. We apply this technique to three popular transformer-based architectures (ViT, Swin and HiP) and show that with contrastive pre-training Zorro achieves state-of-the-art results on most relevant benchmarks for multimodal tasks (AudioSet and VGGSound). Furthermore, the resulting models are able to perform unimodal inference on both video and audio benchmarks such as Kinetics-400 or ESC-50.



### Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.09602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09602v1)
- **Published**: 2023-01-23 18:06:35+00:00
- **Updated**: 2023-01-23 18:06:35+00:00
- **Authors**: Joao P. C. Bertoldo, Santiago Velasco-Forero, Jesus Angulo, Etienne Decencière
- **Comment**: Submitted to the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)
- **Journal**: None
- **Summary**: We propose an incremental improvement to Fully Convolutional Data Description (FCDD), an adaptation of the one-class classification approach from anomaly detection to image anomaly segmentation (a.k.a. anomaly localization). We analyze its original loss function and propose a substitute that better resembles its predecessor, the Hypersphere Classifier (HSC). Both are compared on the MVTec Anomaly Detection Dataset (MVTec-AD) -- training images are flawless objects/textures and the goal is to segment unseen defects -- showing that consistent improvement is achieved by better designing the pixel-wise supervision.



### Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study
- **Arxiv ID**: http://arxiv.org/abs/2301.09617v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09617v2)
- **Published**: 2023-01-23 18:33:38+00:00
- **Updated**: 2023-03-01 13:25:59+00:00
- **Authors**: Sophia J. Wagner, Daniel Reisenbüchler, Nicholas P. West, Jan Moritz Niehues, Gregory Patrick Veldhuizen, Philip Quirke, Heike I. Grabsch, Piet A. van den Brandt, Gordon G. A. Hutchins, Susan D. Richman, Tanwei Yuan, Rupert Langer, Josien Christina Anna Jenniskens, Kelly Offermans, Wolfram Mueller, Richard Gray, Stephen B. Gruber, Joel K. Greenson, Gad Rennert, Joseph D. Bonner, Daniel Schmolze, Jacqueline A. James, Maurice B. Loughrey, Manuel Salto-Tellez, Hermann Brenner, Michael Hoffmeister, Daniel Truhn, Julia A. Schnabel, Melanie Boxberg, Tingying Peng, Jakob Nikolas Kather
- **Comment**: Updated Figure 2 and Table A.5
- **Journal**: None
- **Summary**: Background: Deep learning (DL) can extract predictive and prognostic biomarkers from routine pathology slides in colorectal cancer. For example, a DL test for the diagnosis of microsatellite instability (MSI) in CRC has been approved in 2022. Current approaches rely on convolutional neural networks (CNNs). Transformer networks are outperforming CNNs and are replacing them in many applications, but have not been used for biomarker prediction in cancer at a large scale. In addition, most DL approaches have been trained on small patient cohorts, which limits their clinical utility. Methods: In this study, we developed a new fully transformer-based pipeline for end-to-end biomarker prediction from pathology slides. We combine a pre-trained transformer encoder and a transformer network for patch aggregation, capable of yielding single and multi-target prediction at patient level. We train our pipeline on over 9,000 patients from 10 colorectal cancer cohorts. Results: A fully transformer-based approach massively improves the performance, generalizability, data efficiency, and interpretability as compared with current state-of-the-art algorithms. After training on a large multicenter cohort, we achieve a sensitivity of 0.97 with a negative predictive value of 0.99 for MSI prediction on surgical resection specimens. We demonstrate for the first time that resection specimen-only training reaches clinical-grade performance on endoscopic biopsy tissue, solving a long-standing diagnostic problem. Interpretation: A fully transformer-based end-to-end pipeline trained on thousands of pathology slides yields clinical-grade performance for biomarker prediction on surgical resections and biopsies. Our new methods are freely available under an open source license.



### Tracking the industrial growth of modern China with high-resolution panchromatic imagery: A sequential convolutional approach
- **Arxiv ID**: http://arxiv.org/abs/2301.09620v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09620v2)
- **Published**: 2023-01-23 18:40:21+00:00
- **Updated**: 2023-02-14 18:59:56+00:00
- **Authors**: Ethan Brewer, Zhonghui Lv, Dan Runfola
- **Comment**: Fixed typos
- **Journal**: None
- **Summary**: Due to insufficient or difficult to obtain data on development in inaccessible regions, remote sensing data is an important tool for interested stakeholders to collect information on economic growth. To date, no studies have utilized deep learning to estimate industrial growth at the level of individual sites. In this study, we harness high-resolution panchromatic imagery to estimate development over time at 419 industrial sites in the People's Republic of China using a multi-tier computer vision framework. We present two methods for approximating development: (1) structural area coverage estimated through a Mask R-CNN segmentation algorithm, and (2) imputing development directly with visible & infrared radiance from the Visible Infrared Imaging Radiometer Suite (VIIRS). Labels generated from these methods are comparatively evaluated and tested. On a dataset of 2,078 50 cm resolution images spanning 19 years, the results indicate that two dimensions of industrial development can be estimated using high-resolution daytime imagery, including (a) the total square meters of industrial development (average error of 0.021 $\textrm{km}^2$), and (b) the radiance of lights (average error of 9.8 $\mathrm{\frac{nW}{cm^{2}sr}}$). Trend analysis of the techniques reveal estimates from a Mask R-CNN-labeled CNN-LSTM track ground truth measurements most closely. The Mask R-CNN estimates positive growth at every site from the oldest image to the most recent, with an average change of 4,084 $\textrm{m}^2$.



### Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2301.09624v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09624v1)
- **Published**: 2023-01-23 18:47:41+00:00
- **Updated**: 2023-01-23 18:47:41+00:00
- **Authors**: Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas
- **Comment**: * Joint first authorship Accepted: IEEE - ISBI 2023 International
  Symposium on Biomedical Imaging
- **Journal**: None
- **Summary**: How similar are two images? In computational pathology, where Whole Slide Images (WSIs) of digitally scanned tissue samples from patients can be multi-gigapixels in size, determination of degree of similarity between two WSIs is a challenging task with a number of practical applications. In this work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy (MMD) analysis for determination of pairwise similarity between WSIs. The proposed approach works by calculating MMD between two WSIs using kernels over deep features of image patches. This allows representation of an entire dataset of WSIs as a kernel matrix for WSI level clustering, weakly-supervised prediction of TP-53 mutation status in breast cancer patients from their routine WSIs as well as survival analysis with state of the art prediction performance. We believe that this work will open up further avenues for application of WSI-level kernels for predictive and prognostic tasks in computational pathology.



### LEGO-Net: Learning Regular Rearrangements of Objects in Rooms
- **Arxiv ID**: http://arxiv.org/abs/2301.09629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09629v2)
- **Published**: 2023-01-23 18:58:02+00:00
- **Updated**: 2023-03-24 22:49:15+00:00
- **Authors**: Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, Leonidas Guibas
- **Comment**: Project page: https://ivl.cs.brown.edu/projects/lego-net
- **Journal**: None
- **Summary**: Humans universally dislike the task of cleaning up a messy room. If machines were to help us with this task, they must understand human criteria for regular arrangements, such as several types of symmetry, co-linearity or co-circularity, spacing uniformity in linear or circular patterns, and further inter-object relationships that relate to style and functionality. Previous approaches for this task relied on human input to explicitly specify goal state, or synthesized scenes from scratch -- but such methods do not address the rearrangement of existing messy scenes without providing a goal state. In this paper, we present LEGO-Net, a data-driven transformer-based iterative method for LEarning reGular rearrangement of Objects in messy rooms. LEGO-Net is partly inspired by diffusion models -- it starts with an initial messy state and iteratively ''de-noises'' the position and orientation of objects to a regular state while reducing distance traveled. Given randomly perturbed object positions and orientations in an existing dataset of professionally-arranged scenes, our method is trained to recover a regular re-arrangement. Results demonstrate that our method is able to reliably rearrange room scenes and outperform other methods. We additionally propose a metric for evaluating regularity in room arrangements using number-theoretic machinery.



### HexPlane: A Fast Representation for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2301.09632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09632v2)
- **Published**: 2023-01-23 18:59:25+00:00
- **Updated**: 2023-03-27 16:39:58+00:00
- **Authors**: Ang Cao, Justin Johnson
- **Comment**: CVPR 2023, Camera Ready Project page:
  https://caoang327.github.io/HexPlane
- **Journal**: None
- **Summary**: Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than $100\times$. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlane is a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.



### InfiniCity: Infinite-Scale City Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.09637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09637v2)
- **Published**: 2023-01-23 18:59:59+00:00
- **Updated**: 2023-08-15 01:05:21+00:00
- **Authors**: Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, Sergey Tulyakov
- **Comment**: None
- **Journal**: None
- **Summary**: Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/



### Improving Performance of Object Detection using the Mechanisms of Visual Recognition in Humans
- **Arxiv ID**: http://arxiv.org/abs/2301.09667v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09667v2)
- **Published**: 2023-01-23 19:09:36+00:00
- **Updated**: 2023-03-14 16:26:17+00:00
- **Authors**: Amir Ghasemi, Nasrin Bayat, Fatemeh Mottaghian, Akram Bayat
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition systems are usually trained and evaluated on high resolution images. However, in real world applications, it is common that the images have low resolutions or have small sizes. In this study, we first track the performance of the state-of-the-art deep object recognition network, Faster- RCNN, as a function of image resolution. The results reveals negative effects of low resolution images on recognition performance. They also show that different spatial frequencies convey different information about the objects in recognition process. It means multi-resolution recognition system can provides better insight into optimal selection of features that results in better recognition of objects. This is similar to the mechanisms of the human visual systems that are able to implement multi-scale representation of a visual scene simultaneously. Then, we propose a multi-resolution object recognition framework rather than a single-resolution network. The proposed framework is evaluated on the PASCAL VOC2007 database. The experimental results show the performance of our adapted multi-resolution Faster-RCNN framework outperforms the single-resolution Faster-RCNN on input images with various resolutions with an increase in the mean Average Precision (mAP) of 9.14% across all resolutions and 1.2% on the full-spectrum images. Furthermore, the proposed model yields robustness of the performance over a wide range of spatial frequencies.



### Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2301.09702v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09702v1)
- **Published**: 2023-01-23 20:11:24+00:00
- **Updated**: 2023-01-23 20:11:24+00:00
- **Authors**: Jiaqi Guo, Amy R. Reibman, Edward J. Delp
- **Comment**: 10 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the proposed SMB outperforms other synthesis methods on several re-ID benchmarks.



### Long-tail Detection with Effective Class-Margins
- **Arxiv ID**: http://arxiv.org/abs/2301.09724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09724v1)
- **Published**: 2023-01-23 21:25:24+00:00
- **Updated**: 2023-01-23 21:25:24+00:00
- **Authors**: Jang Hyun Cho, Philipp Krähenbühl
- **Comment**: ECCV 2022 Oral. Code is available at
  https://github.com/janghyuncho/ECM-Loss
- **Journal**: None
- **Summary**: Large-scale object detection and instance segmentation face a severe data imbalance. The finer-grained object classes become, the less frequent they appear in our datasets. However, at test-time, we expect a detector that performs well for all classes and not just the most frequent ones. In this paper, we provide a theoretical understanding of the long-trail detection problem. We show how the commonly used mean average precision evaluation metric on an unknown test set is bound by a margin-based binary classification error on a long-tailed object detection training set. We optimize margin-based binary classification error with a novel surrogate objective called \textbf{Effective Class-Margin Loss} (ECM). The ECM loss is simple, theoretically well-motivated, and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide range of architecture and detectors. Code is available at \url{https://github.com/janghyuncho/ECM-Loss}.



### Minimally Invasive Live Tissue High-fidelity Thermophysical Modeling using Real-time Thermography
- **Arxiv ID**: http://arxiv.org/abs/2301.09733v1
- **DOI**: 10.1109/TBME.2022.3230728
- **Categories**: **physics.med-ph**, cs.CV, eess.IV, 93B53, 93C20, I.4.8; I.6.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2301.09733v1)
- **Published**: 2023-01-23 21:54:01+00:00
- **Updated**: 2023-01-23 21:54:01+00:00
- **Authors**: Hamza El-Kebir, Junren Ran, Yongseok Lee, Leonardo P. Chamorro, Martin Ostoja-Starzewski, Richard Berlin, Gabriela M. Aguiluz Cornejo, Enrico Benedetti, Pier C. Giulianotti, Joseph Bentsman
- **Comment**: Accepted for publication in the IEEE Transactions on Biomedical
  Engineering. Research reported in this publication was supported by the
  National Institute of Biomedical Imaging and Bioengineering of the National
  Institutes of Health under award number R01EB029766
- **Journal**: None
- **Summary**: We present a novel thermodynamic parameter estimation framework for energy-based surgery on live tissue, with direct applications to tissue characterization during electrosurgery. This framework addresses the problem of estimating tissue-specific thermodynamics in real-time, which would enable accurate prediction of thermal damage impact to the tissue and damage-conscious planning of electrosurgical procedures. Our approach provides basic thermodynamic information such as thermal diffusivity, and also allows for obtaining the thermal relaxation time and a model of the heat source, yielding in real-time a controlled hyperbolic thermodynamics model. The latter accounts for the finite thermal propagation time necessary for modeling of the electrosurgical action, in which the probe motion speed often surpasses the speed of thermal propagation in the tissue operated on. Our approach relies solely on thermographer feedback and a knowledge of the power level and position of the electrosurgical pencil, imposing only very minor adjustments to normal electrosurgery to obtain a high-fidelity model of the tissue-probe interaction. Our method is minimally invasive and can be performed in situ. We apply our method first to simulated data based on porcine muscle tissue to verify its accuracy and then to in vivo liver tissue, and compare the results with those from the literature. This comparison shows that parameterizing the Maxwell--Cattaneo model through the framework proposed yields a noticeably higher fidelity real-time adaptable representation of the thermodynamic tissue response to the electrosurgical impact than currently available. A discussion on the differences between the live and the dead tissue thermodynamics is also provided.



