# Arxiv Papers in cs.CV on 2023-01-01
### Causal Deep Learning: Causal Capsules and Tensor Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.00314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.4.10; I.2.4; I.2.10; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2301.00314v1)
- **Published**: 2023-01-01 00:47:03+00:00
- **Updated**: 2023-01-01 00:47:03+00:00
- **Authors**: M. Alex O. Vasilescu
- **Comment**: The document contains both the article and the supplemental material
- **Journal**: Proceedings of the 26th International Conference on Pattern
  Recognition (ICPR 2022) Montreal, Canada, Aug. 21-25, 2022
- **Summary**: We derive a set of causal deep neural networks whose architectures are a consequence of tensor (multilinear) factor analysis. Forward causal questions are addressed with a neural network architecture composed of causal capsules and a tensor transformer. The former estimate a set of latent variables that represent the causal factors, and the latter governs their interaction. Causal capsules and tensor transformers may be implemented using shallow autoencoders, but for a scalable architecture we employ block algebra and derive a deep neural network composed of a hierarchy of autoencoders. An interleaved kernel hierarchy preprocesses the data resulting in a hierarchy of kernel tensor factor models. Inverse causal questions are addressed with a neural network that implements multilinear projection and estimates the causes of effects. As an alternative to aggressive bottleneck dimension reduction or regularized regression that may camouflage an inherently underdetermined inverse problem, we prescribe modeling different aspects of the mechanism of data formation with piecewise tensor models whose multilinear projections are well-defined and produce multiple candidate solutions. Our forward and inverse neural network architectures are suitable for asynchronous parallel computation.



### Yuille-Poggio's Flow and Global Minimizer of polynomials through convexification by Heat Evolution
- **Arxiv ID**: http://arxiv.org/abs/2301.00326v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.00326v1)
- **Published**: 2023-01-01 02:08:32+00:00
- **Updated**: 2023-01-01 02:08:32+00:00
- **Authors**: Qiao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the possibility of the backward-differential-flow-like algorithm which starts from the minimum of convexification version of the polynomial. We apply the heat evolution convexification approach through Gaussian filtering, which is actually an accumulation version of Steklov's regularization. We generalize the fingerprint theory which was proposed in the theory of computer vision by A.L. Yuille and T. Poggio in 1980s, in particular their fingerprint trajectory equation, to characterize the evolution of minimizers across the scale. On the other hand, we propose the "seesaw" polynomials $p(x|s)$ and we find a seesaw differential equation $\frac{\partial p(x|s)}{\,ds}=-\frac{1}{p''(x)}$ to characterize the evolution of global minimizer $x^*(s)$ of $p(x|s)$ while varying $s$. Essentially, both the fingerprints $\mathcal{FP}_2$ and $\mathcal{FP}_3$ of $p(x)$, consisting of the zeros of $\frac{\partial^2 p(x,t)}{\partial x^2}$ and $\frac{\partial^3 p(x,t)}{\partial x^3}$, respectively, are independent of seesaw coefficient $s$, upon which we define the Confinement Zone and Escape Zone. Meanwhile, varying $s$ will monotonically condition the location of global minimizer of $p(x|s)$, and all these location form the Attainable Zone. Based on these concepts, we prove that the global minimizer $x^*$ of $p(x)$ can be inversely evolved from the global minimizer of its convexification polynomial $p(x,t_0)$ if and only if $x^*$ is included in the Escape Zone. In particular, we give detailed analysis for quartic and six degree polynomials.



### Efficient On-device Training via Gradient Filtering
- **Arxiv ID**: http://arxiv.org/abs/2301.00330v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00330v2)
- **Published**: 2023-01-01 02:33:03+00:00
- **Updated**: 2023-03-25 02:12:09+00:00
- **Authors**: Yuedong Yang, Guihong Li, Radu Marculescu
- **Comment**: CVPR2023, 19 pages, 13 figures
- **Journal**: None
- **Summary**: Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\times$ speedup and 77.1% memory savings on ImageNet classification with only 0.1% accuracy loss. Finally, our method is easy to implement and deploy; over 20$\times$ speedup and 90% energy savings have been observed compared to highly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano. Consequently, our approach opens up a new direction of research with a huge potential for on-device training.



### Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.01143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01143v1)
- **Published**: 2023-01-01 04:10:03+00:00
- **Updated**: 2023-01-01 04:10:03+00:00
- **Authors**: Fengbei Liu, Yuanhong Chen, Chong Wang, Yu Tain, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: Learning with noisy-labels has become an important research topic in computer vision where state-of-the-art (SOTA) methods explore: 1) prediction disagreement with co-teaching strategy that updates two models when they disagree on the prediction of training samples; and 2) sample selection to divide the training set into clean and noisy sets based on small training loss. However, the quick convergence of co-teaching models to select the same clean subsets combined with relatively fast overfitting of noisy labels may induce the wrong selection of noisy label samples as clean, leading to an inevitable confirmation bias that damages accuracy. In this paper, we introduce our noisy-label learning approach, called Asymmetric Co-teaching (AsyCo), which introduces novel prediction disagreement that produces more consistent divergent results of the co-teaching models, and a new sample selection approach that does not require small-loss assumption to enable a better robustness to confirmation bias than previous methods. More specifically, the new prediction disagreement is achieved with the use of different training strategies, where one model is trained with multi-class learning and the other with multi-label learning. Also, the new sample selection is based on multi-view consensus, which uses the label views from training labels and model predictions to divide the training set into clean and noisy for training the multi-class model and to re-label the training samples with multiple top-ranked labels for training the multi-label model. Extensive experiments on synthetic and real-world noisy-label datasets show that AsyCo improves over current SOTA methods.



### MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction
- **Arxiv ID**: http://arxiv.org/abs/2301.00345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00345v1)
- **Published**: 2023-01-01 04:54:03+00:00
- **Updated**: 2023-01-01 04:54:03+00:00
- **Authors**: Jorge Quesada, Lakshmi Sathidevi, Ran Liu, Nauman Ahad, Joy M. Jackson, Mehdi Azabou, Jingyun Xiao, Christopher Liding, Matthew Jin, Carolina Urzay, William Gray-Roncal, Erik C. Johnson, Eva L. Dyer
- **Comment**: 10 pages, 4 figures, Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: There are multiple scales of abstraction from which we can describe the same image, depending on whether we are focusing on fine-grained details or a more global attribute of the image. In brain mapping, learning to automatically parse images to build representations of both small-scale features (e.g., the presence of cells or blood vessels) and global properties of an image (e.g., which brain region the image comes from) is a crucial and open challenge. However, most existing datasets and benchmarks for neuroanatomy consider only a single downstream task at a time. To bridge this gap, we introduce a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography images spanning a large thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions. We generated a number of different prediction challenges and evaluated several supervised and self-supervised models for brain-region prediction and pixel-level semantic segmentation of microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations that capture multiple attributes of a single image and perform well on a variety of downstream tasks. Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/ .



### EvidenceCap: Towards trustworthy medical image segmentation via evidential identity cap
- **Arxiv ID**: http://arxiv.org/abs/2301.00349v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.00349v1)
- **Published**: 2023-01-01 05:02:46+00:00
- **Updated**: 2023-01-01 05:02:46+00:00
- **Authors**: Ke Zou, Xuedong Yuan, Xiaojing Shen, Yidi Chen, Meng Wang, Rick Siow Mong Goh, Yong Liu, Huazhu Fu
- **Comment**: 38 pages, 6 figures
- **Journal**: None
- **Summary**: Medical image segmentation (MIS) is essential for supporting disease diagnosis and treatment effect assessment. Despite considerable advances in artificial intelligence (AI) for MIS, clinicians remain skeptical of its utility, maintaining low confidence in such black box systems, with this problem being exacerbated by low generalization for out-of-distribution (OOD) data. To move towards effective clinical utilization, we propose a foundation model named EvidenceCap, which makes the box transparent in a quantifiable way by uncertainty estimation. EvidenceCap not only makes AI visible in regions of uncertainty and OOD data, but also enhances the reliability, robustness, and computational efficiency of MIS. Uncertainty is modeled explicitly through subjective logic theory to gather strong evidence from features. We show the effectiveness of EvidenceCap in three segmentation datasets and apply it to the clinic. Our work sheds light on clinical safe applications and explainable AI, and can contribute towards trustworthiness in the medical domain.



### Mapping smallholder cashew plantations to inform sustainable tree crop expansion in Benin
- **Arxiv ID**: http://arxiv.org/abs/2301.00363v2
- **DOI**: 10.1016/j.rse.2023.113695
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2301.00363v2)
- **Published**: 2023-01-01 07:18:47+00:00
- **Updated**: 2023-01-15 18:04:42+00:00
- **Authors**: Leikun Yin, Rahul Ghosh, Chenxi Lin, David Hale, Christoph Weigl, James Obarowski, Junxiong Zhou, Jessica Till, Xiaowei Jia, Troy Mao, Vipin Kumar, Zhenong Jin
- **Comment**: None
- **Journal**: Remote Sensing of Environment, 295, p.113695 (2023)
- **Summary**: Cashews are grown by over 3 million smallholders in more than 40 countries worldwide as a principal source of income. As the third largest cashew producer in Africa, Benin has nearly 200,000 smallholder cashew growers contributing 15% of the country's national export earnings. However, a lack of information on where and how cashew trees grow across the country hinders decision-making that could support increased cashew production and poverty alleviation. By leveraging 2.4-m Planet Basemaps and 0.5-m aerial imagery, newly developed deep learning algorithms, and large-scale ground truth datasets, we successfully produced the first national map of cashew in Benin and characterized the expansion of cashew plantations between 2015 and 2021. In particular, we developed a SpatioTemporal Classification with Attention (STCA) model to map the distribution of cashew plantations, which can fully capture texture information from discriminative time steps during a growing season. We further developed a Clustering Augmented Self-supervised Temporal Classification (CASTC) model to distinguish high-density versus low-density cashew plantations by automatic feature extraction and optimized clustering. Results show that the STCA model has an overall accuracy over 85% and the CASTC model achieved an overall accuracy of 76%. We found that the cashew area in Benin almost doubled from 2015 to 2021 with 60% of new plantation development coming from cropland or fallow land, while encroachment of cashew plantations into protected areas has increased by 55%. Only half of cashew plantations were high-density in 2021, suggesting high potential for intensification. Our study illustrates the power of combining high-resolution remote sensing imagery and state-of-the-art deep learning algorithms to better understand tree crops in the heterogeneous smallholder landscape.



### Generalizable Black-Box Adversarial Attack with Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.00364v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.00364v1)
- **Published**: 2023-01-01 07:24:12+00:00
- **Updated**: 2023-01-01 07:24:12+00:00
- **Authors**: Fei Yin, Yong Zhang, Baoyuan Wu, Yan Feng, Jingyi Zhang, Yanbo Fan, Yujiu Yang
- **Comment**: T-PAMI 2022. Project Page is at https://github.com/SCLBD/MCG-Blackbox
- **Journal**: None
- **Summary**: In the scenario of black-box adversarial attack, the target model's parameters are unknown, and the attacker aims to find a successful adversarial perturbation based on query feedback under a query budget. Due to the limited feedback information, existing query-based black-box attack methods often require many queries for attacking each benign example. To reduce query cost, we propose to utilize the feedback information across historical attacks, dubbed example-level adversarial transferability. Specifically, by treating the attack on each benign example as one task, we develop a meta-learning framework by training a meta-generator to produce perturbations conditioned on benign examples. When attacking a new benign example, the meta generator can be quickly fine-tuned based on the feedback information of the new task as well as a few historical attacks to produce effective perturbations. Moreover, since the meta-train procedure consumes many queries to learn a generalizable generator, we utilize model-level adversarial transferability to train the meta-generator on a white-box surrogate model, then transfer it to help the attack against the target model. The proposed framework with the two types of adversarial transferability can be naturally combined with any off-the-shelf query-based attack methods to boost their performance, which is verified by extensive experiments.



### SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.00366v3
- **DOI**: 10.3390/s23073649
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00366v3)
- **Published**: 2023-01-01 07:42:50+00:00
- **Updated**: 2023-04-04 18:44:03+00:00
- **Authors**: Kunal Chaturvedi, Ali Braytee, Jun Li, Mukesh Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel self-supervised based Cut-and-Paste GAN to perform foreground object segmentation and generate realistic composite images without manual annotations. We accomplish this goal by a simple yet effective self-supervised approach coupled with the U-Net based discriminator. The proposed method extends the ability of the standard discriminators to learn not only the global data representations via classification (real/fake) but also learn semantic and structural information through pseudo labels created using the self-supervised task. The proposed method empowers the generator to create meaningful masks by forcing it to learn informative per-pixel as well as global image feedback from the discriminator. Our experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on the standard benchmark datasets.



### Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment
- **Arxiv ID**: http://arxiv.org/abs/2301.00371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.00371v1)
- **Published**: 2023-01-01 08:38:07+00:00
- **Updated**: 2023-01-01 08:38:07+00:00
- **Authors**: Libo Zhang, Wenzhang Zhou, Heng Fan, Tiejian Luo, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptive detection aims to improve the generalization of detectors on target domain. To reduce discrepancy in feature distributions between two domains, recent approaches achieve domain adaption through feature alignment in different granularities via adversarial learning. However, they neglect the relationship between multiple granularities and different features in alignment, degrading detection. Addressing this, we introduce a unified multi-granularity alignment (MGA)-based detection framework for domain-invariant feature learning. The key is to encode the dependencies across different granularities including pixel-, instance-, and category-levels simultaneously to align two domains. Specifically, based on pixel-level features, we first develop an omni-scale gated fusion (OSGF) module to aggregate discriminative representations of instances with scale-aware convolutions, leading to robust multi-scale detection. Besides, we introduce multi-granularity discriminators to identify where, either source or target domains, different granularities of samples come from. Note that, MGA not only leverages instance discriminability in different categories but also exploits category consistency between two domains for detection. Furthermore, we present an adaptive exponential moving average (AEMA) strategy that explores model assessments for model update to improve pseudo labels and alleviate local misalignment problem, boosting detection robustness. Extensive experiments on multiple domain adaption scenarios validate the superiority of MGA over other approaches on FCOS and Faster R-CNN detectors. Code will be released at https://github.com/tiankongzhang/MGA.



### Discriminative Radial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2301.00383v2
- **DOI**: 10.1109/TIP.2023.3235583
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.00383v2)
- **Published**: 2023-01-01 10:56:31+00:00
- **Updated**: 2023-02-13 10:52:25+00:00
- **Authors**: Zenan Huang, Jun Wen, Siheng Chen, Linchao Zhu, Nenggan Zheng
- **Comment**: 13 pages, 14 figures
- **Journal**: None
- **Summary**: Domain adaptation methods reduce domain shift typically by learning domain-invariant features. Most existing methods are built on distribution matching, e.g., adversarial domain adaptation, which tends to corrupt feature discriminability. In this paper, we propose Discriminative Radial Domain Adaptation (DRDA) which bridges source and target domains via a shared radial structure. It's motivated by the observation that as the model is trained to be progressively discriminative, features of different categories expand outwards in different directions, forming a radial structure. We show that transferring such an inherently discriminative structure would enable to enhance feature transferability and discriminability simultaneously. Specifically, we represent each domain with a global anchor and each category a local anchor to form a radial structure and reduce domain shift via structure matching. It consists of two parts, namely isometric transformation to align the structure globally and local refinement to match each category. To enhance the discriminability of the structure, we further encourage samples to cluster close to the corresponding local anchors based on optimal-transport assignment. Extensively experimenting on multiple benchmarks, our method is shown to consistently outperforms state-of-the-art approaches on varied tasks, including the typical unsupervised domain adaptation, multi-source domain adaptation, domain-agnostic learning, and domain generalization.



### Deep Learning Technique for Human Parsing: A Survey and Outlook
- **Arxiv ID**: http://arxiv.org/abs/2301.00394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.00394v1)
- **Published**: 2023-01-01 12:39:57+00:00
- **Updated**: 2023-01-01 12:39:57+00:00
- **Authors**: Lu Yang, Wenhe Jia, Shan Li, Qing Song
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (under review)
- **Journal**: None
- **Summary**: Human parsing aims to partition humans in image or video into multiple pixel-level semantic parts. In the last decade, it has gained significantly increased interest in the computer vision community and has been utilized in a broad range of practical applications, from security monitoring, to social media, to visual special effects, just to name a few. Although deep learning-based human parsing solutions have made remarkable achievements, many important concepts, existing challenges, and potential research directions are still confusing. In this survey, we comprehensively review three core sub-tasks: single human parsing, multiple human parsing, and video human parsing, by introducing their respective task settings, background concepts, relevant problems and applications, representative literature, and datasets. We also present quantitative performance comparisons of the reviewed methods on benchmark datasets. Additionally, to promote sustainable development of the community, we put forward a transformer-based human parsing framework, providing a high-performance baseline for follow-up research through universal, concise, and extensible solutions. Finally, we point out a set of under-investigated open issues in this field and suggest new directions for future study. We also provide a regularly updated project page, to continuously track recent developments in this fast-advancing field: https://github.com/soeaver/awesome-human-parsing.



### Curvature regularization for Non-line-of-sight Imaging from Under-sampled Data
- **Arxiv ID**: http://arxiv.org/abs/2301.00406v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.00406v3)
- **Published**: 2023-01-01 14:10:43+00:00
- **Updated**: 2023-04-26 02:33:44+00:00
- **Authors**: Rui Ding, Juntian Ye, Qifeng Gao, Feihu Xu, Yuping Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Non-line-of-sight (NLOS) imaging aims to reconstruct the three-dimensional hidden scenes from the data measured in the line-of-sight, which uses photon time-of-flight information encoded in light after multiple diffuse reflections. The under-sampled scanning data can facilitate fast imaging. However, the resulting reconstruction problem becomes a serious ill-posed inverse problem, the solution of which is of high possibility to be degraded due to noises and distortions. In this paper, we propose two novel NLOS reconstruction models based on curvature regularization, i.e., the object-domain curvature regularization model and the dual (i.e., signal and object)-domain curvature regularization model. Fast numerical optimization algorithms are developed relying on the alternating direction method of multipliers (ADMM) with the backtracking stepsize rule, which are further accelerated by GPU implementation. We evaluate the proposed algorithms on both synthetic and real datasets, which achieve state-of-the-art performance, especially in the compressed sensing setting. All our codes and data are available at https://github.com/Duanlab123/CurvNLOS.



### Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification
- **Arxiv ID**: http://arxiv.org/abs/2301.00409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.00409v1)
- **Published**: 2023-01-01 14:19:52+00:00
- **Updated**: 2023-01-01 14:19:52+00:00
- **Authors**: Shizhan Gong, Cheng Chen, Yuqi Gong, Nga Yan Chan, Wenao Ma, Calvin Hoi-Kwan Mak, Jill Abrigo, Qi Dou
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Brain midline shift (MLS) is one of the most critical factors to be considered for clinical diagnosis and treatment decision-making for intracranial hemorrhage. Existing computational methods on MLS quantification not only require intensive labeling in millimeter-level measurement but also suffer from poor performance due to their dependence on specific landmarks or simplified anatomical assumptions. In this paper, we propose a novel semi-supervised framework to accurately measure the scale of MLS from head CT scans. We formulate the MLS measurement task as a deformation estimation problem and solve it using a few MLS slices with sparse labels. Meanwhile, with the help of diffusion models, we are able to use a great number of unlabeled MLS data and 2793 non-MLS cases for representation learning and regularization. The extracted representation reflects how the image is different from a non-MLS image and regularization serves an important role in the sparse-to-dense refinement of the deformation field. Our experiment on a real clinical brain hemorrhage dataset has achieved state-of-the-art performance and can generate interpretable deformation fields.



### Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2301.00411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.00411v2)
- **Published**: 2023-01-01 14:39:09+00:00
- **Updated**: 2023-01-11 08:11:07+00:00
- **Authors**: Boyu Zhang, Wenbo Xu, Zheng Zhu, Guan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Representing and synthesizing novel views in real-world dynamic scenes from casual monocular videos is a long-standing problem. Existing solutions typically approach dynamic scenes by applying geometry techniques or utilizing temporal information between several adjacent frames without considering the underlying background distribution in the entire scene or the transmittance over the ray dimension, limiting their performance on static and occlusion areas. Our approach $\textbf{D}$istribution-$\textbf{D}$riven neural radiance fields offers high-quality view synthesis and a 3D solution to $\textbf{D}$etach the background from the entire $\textbf{D}$ynamic scene, which is called $\text{D}^4$NeRF. Specifically, it employs a neural representation to capture the scene distribution in the static background and a 6D-input NeRF to represent dynamic objects, respectively. Each ray sample is given an additional occlusion weight to indicate the transmittance lying in the static and dynamic components. We evaluate $\text{D}^4$NeRF on public dynamic scenes and our urban driving scenes acquired from an autonomous-driving dataset. Extensive experiments demonstrate that our approach outperforms previous methods in rendering texture details and motion areas while also producing a clean static background. Our code will be released at https://github.com/Luciferbobo/D4NeRF.



### GoogLe2Net: Going Transverse with Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2301.00424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.00424v1)
- **Published**: 2023-01-01 15:16:10+00:00
- **Updated**: 2023-01-01 15:16:10+00:00
- **Authors**: Yuanpeng He
- **Comment**: 33 pages, 7 figures
- **Journal**: None
- **Summary**: Capturing feature information effectively is of great importance in vision tasks. With the development of convolutional neural networks (CNNs), concepts like residual connection and multiple scales promote continual performance gains on diverse deep learning vision tasks. However, the existing methods do not organically combined advantages of these valid ideas. In this paper, we propose a novel CNN architecture called GoogLe2Net, it consists of residual feature-reutilization inceptions (ResFRI) or split residual feature-reutilization inceptions (Split-ResFRI) which create transverse passages between adjacent groups of convolutional layers to enable features flow to latter processing branches and possess residual connections to better process information. Our GoogLe2Net is able to reutilize information captured by foregoing groups of convolutional layers and express multi-scale features at a fine-grained level, which improves performances in image classification. And the inception we proposed could be embedded into inception-like networks directly without any migration costs. Moreover, in experiments based on popular vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%) and Tiny Imagenet (70.54%), we obtain better results on image classification task compared with other modern models.



### Optimization of Image Transmission in a Cooperative Semantic Communication Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.00433v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2301.00433v1)
- **Published**: 2023-01-01 15:59:13+00:00
- **Updated**: 2023-01-01 15:59:13+00:00
- **Authors**: Wenjing Zhang, Yining Wang, Mingzhe Chen, Tao Luo, Dusit Niyato
- **Comment**: 29 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, a semantic communication framework for image transmission is developed. In the investigated framework, a set of servers cooperatively transmit images to a set of users utilizing semantic communication techniques. To evaluate the performance of studied semantic communication system, a multimodal metric is proposed to measure the correlation between the extracted semantic information and the original image. To meet the ISS requirement of each user, each server must jointly determine the semantic information to be transmitted and the resource blocks (RBs) used for semantic information transmission. We formulate this problem as an optimization problem aiming to minimize each server's transmission latency while reaching the ISS requirement. To solve this problem, a value decomposition based entropy-maximized multi-agent reinforcement learning (RL) is proposed, which enables servers to coordinate for training and execute RB allocation in a distributed manner to approach to a globally optimal performance with less training iterations. Compared to traditional multi-agent RL, the proposed RL improves the valuable action exploration of servers and the probability of finding a globally optimal RB allocation policy based on local observation. Simulation results show that the proposed algorithm can reduce the transmission delay by up to 16.1% compared to traditional multi-agent RL.



### Hierarchical Explanations for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.00436v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00436v3)
- **Published**: 2023-01-01 16:24:12+00:00
- **Updated**: 2023-04-03 20:04:08+00:00
- **Authors**: Sadaf Gulshad, Teng Long, Nanne van Noord
- **Comment**: None
- **Journal**: None
- **Summary**: To interpret deep neural networks, one main approach is to dissect the visual input and find the prototypical parts responsible for the classification. However, existing methods often ignore the hierarchical relationship between these prototypes, and thus can not explain semantic concepts at both higher level (e.g., water sports) and lower level (e.g., swimming). In this paper inspired by human cognition system, we leverage hierarchal information to deal with uncertainty: When we observe water and human activity, but no definitive action it can be recognized as the water sports parent class. Only after observing a person swimming can we definitively refine it to the swimming action. To this end, we propose HIerarchical Prototype Explainer (HIPE) to build hierarchical relations between prototypes and classes. HIPE enables a reasoning process for video action classification by dissecting the input video frames on multiple levels of the class hierarchy, our method is also applicable to other video tasks. The faithfulness of our method is verified by reducing accuracy-explainability trade off on ActivityNet and UCF-101 while providing multi-level explanations.



### Image To Tree with Recursive Prompting
- **Arxiv ID**: http://arxiv.org/abs/2301.00447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00447v1)
- **Published**: 2023-01-01 17:35:24+00:00
- **Updated**: 2023-01-01 17:35:24+00:00
- **Authors**: James Batten, Matthew Sinclair, Ben Glocker, Michiel Schaap
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Extracting complex structures from grid-based data is a common key step in automated medical image analysis. The conventional solution to recovering tree-structured geometries typically involves computing the minimal cost path through intermediate representations derived from segmentation masks. However, this methodology has significant limitations in the context of projective imaging of tree-structured 3D anatomical data such as coronary arteries, since there are often overlapping branches in the 2D projection. In this work, we propose a novel approach to predicting tree connectivity structure which reformulates the task as an optimization problem over individual steps of a recursive process. We design and train a two-stage model which leverages the UNet and Transformer architectures and introduces an image-based prompting technique. Our proposed method achieves compelling results on a pair of synthetic datasets, and outperforms a shortest-path baseline.



### Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.00452v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.00452v2)
- **Published**: 2023-01-01 18:05:25+00:00
- **Updated**: 2023-06-06 05:48:09+00:00
- **Authors**: Yonghao Long, Wang Wei, Tao Huang, Yuehao Wang, Qi Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical robot automation has attracted increasing research interest over the past decade, expecting its potential to benefit surgeons, nurses and patients. Recently, the learning paradigm of embodied intelligence has demonstrated promising ability to learn good control policies for various complex tasks, where embodied AI simulators play an essential role to facilitate relevant research. However, existing open-sourced simulators for surgical robot are still not sufficiently supporting human interactions through physical input devices, which further limits effective investigations on how the human demonstrations would affect policy learning. In this work, we study human-in-the-loop embodied intelligence with a new interactive simulation platform for surgical robot learning. Specifically, we establish our platform based on our previously released SurRoL simulator with several new features co-developed to allow high-quality human interaction via an input device. We showcase the improvement of our simulation environment with the designed new features, and validate effectiveness of incorporating human factors in embodied intelligence through the use of human demonstrations and reinforcement learning as a representative example. Promising results are obtained in terms of learning efficiency. Lastly, five new surgical robot training tasks are developed and released, with which we hope to pave the way for future research on surgical embodied intelligence. Our learning platform is publicly released and will be continuously updated in the website: https://med-air.github.io/SurRoL.



