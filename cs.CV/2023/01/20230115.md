# Arxiv Papers in cs.CV on 2023-01-15
### Empirical study of the modulus as activation function in computer vision applications
- **Arxiv ID**: http://arxiv.org/abs/2301.05993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.05993v1)
- **Published**: 2023-01-15 00:32:03+00:00
- **Updated**: 2023-01-15 00:32:03+00:00
- **Authors**: Iván Vallés-Pérez, Emilio Soria-Olivas, Marcelino Martínez-Sober, Antonio J. Serrano-López, Joan Vila-Francés, Juan Gómez-Sanchís
- **Comment**: Accepted at Engineering Applications of AI
- **Journal**: None
- **Summary**: In this work we propose a new non-monotonic activation function: the modulus. The majority of the reported research on nonlinearities is focused on monotonic functions. We empirically demonstrate how by using the modulus activation function on computer vision tasks the models generalize better than with other nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10, relative to the best of the benchmark activations tested. With the proposed activation function the vanishing gradient and dying neurons problems disappear, because the derivative of the activation function is always 1 or -1. The simplicity of the proposed function and its derivative make this solution specially suitable for TinyML and hardware applications.



### Min-Max-Jump distance and its applications
- **Arxiv ID**: http://arxiv.org/abs/2301.05994v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05994v4)
- **Published**: 2023-01-15 00:55:40+00:00
- **Updated**: 2023-04-10 10:22:32+00:00
- **Authors**: Gangli Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We explore three applications of Min-Max-Jump distance (MMJ distance). MMJ-based K-means revises K-means with MMJ distance. MMJ-based Silhouette coefficient revises Silhouette coefficient with MMJ distance. We also tested the Clustering with Neural Network and Index (CNNI) model with MMJ-based Silhouette coefficient. In the last application, we tested using Min-Max-Jump distance for predicting labels of new points, after a clustering analysis of data. Result shows Min-Max-Jump distance achieves good performances in all the three proposed applications.



### Exploiting Prompt Caption for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2301.05997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05997v2)
- **Published**: 2023-01-15 02:04:02+00:00
- **Updated**: 2023-03-28 10:49:59+00:00
- **Authors**: Hongxiang Li, Meng Cao, Xuxin Cheng, Zhihong Zhu, Yaowei Li, Yuexian Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Video grounding aims to locate a moment of interest matching the given query sentence from an untrimmed video. Previous works ignore the \emph{sparsity dilemma} in video annotations, which fails to provide the context information between potential events and query sentences in the dataset. In this paper, we contend that exploiting easily available captions which describe general actions \ie, prompt captions (PC) defined in our paper, will significantly boost the performance. To this end, we propose a Prompt Caption Network (PCNet) for video grounding. Specifically, we first introduce dense video captioning to generate dense captions and then obtain prompt captions by Non-Prompt Caption Suppression (NPCS). To capture the potential information in prompt captions, we propose Caption Guided Attention (CGA) project the semantic relations between prompt captions and query sentences into temporal space and fuse them into visual representations. Considering the gap between prompt captions and ground truth, we propose Asymmetric Cross-modal Contrastive Learning (ACCL) for constructing more negative pairs to maximize cross-modal mutual information. Without bells and whistles, extensive experiments on three public datasets (\ie, ActivityNet Captions, TACoS and ActivityNet-CG) demonstrate that our method significantly outperforms state-of-the-art methods.



### ACTIVE: A Deep Model for Sperm and Impurity Detection in Microscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/2301.06002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06002v1)
- **Published**: 2023-01-15 02:24:17+00:00
- **Updated**: 2023-01-15 02:24:17+00:00
- **Authors**: Ao Chen, Jinghua Zhang, Md Mamunur Rahaman, Hongzan Sun, M. D., Tieyong Zeng, Marcin Grzegorzek, Feng-Lei Fan, Chen Li
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate detection of sperms and impurities is a very challenging task, facing problems such as the small size of targets, indefinite target morphologies, low contrast and resolution of the video, and similarity of sperms and impurities. So far, the detection of sperms and impurities still largely relies on the traditional image processing and detection techniques which only yield limited performance and often require manual intervention in the detection process, therefore unfavorably escalating the time cost and injecting the subjective bias into the analysis. Encouraged by the successes of deep learning methods in numerous object detection tasks, here we report a deep learning model based on Double Branch Feature Extraction Network (DBFEN) and Cross-conjugate Feature Pyramid Networks (CCFPN).DBFEN is designed to extract visual features from tiny objects with a double branch structure, and CCFPN is further introduced to fuse the features extracted by DBFEN to enhance the description of position and high-level semantic information. Our work is the pioneer of introducing deep learning approaches to the detection of sperms and impurities. Experiments show that the highest AP50 of the sperm and impurity detection is 91.13% and 59.64%, which lead its competitors by a substantial margin and establish new state-of-the-art results in this problem.



### Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.06013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06013v1)
- **Published**: 2023-01-15 03:36:33+00:00
- **Updated**: 2023-01-15 03:36:33+00:00
- **Authors**: Jiayi Han, Longbin Zeng, Liang Du, Weiyang Ding, Jianfeng Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel complementary learning approach to enhance test-time adaptation (TTA), which has been proven to exhibit good performance on testing data with distribution shifts such as corruptions. In test-time adaptation tasks, information from the source domain is typically unavailable and the model has to be optimized without supervision for test-time samples. Hence, usual methods assign labels for unannotated data with the prediction by a well-trained source model in an unsupervised learning framework. Previous studies have employed unsupervised objectives, such as the entropy of model predictions, as optimization targets to effectively learn features for test-time samples. However, the performance of the model is easily compromised by the quality of pseudo-labels, since inaccuracies in pseudo-labels introduce noise to the model. Therefore, we propose to leverage the "less probable categories" to decrease the risk of incorrect pseudo-labeling. The complementary label is introduced to designate these categories. We highlight that the risk function of complementary labels agrees with their Vanilla loss formula under the conventional true label distribution. Experiments show that the proposed learning algorithm achieves state-of-the-art performance on different datasets and experiment settings.



### Diffusion-based Generation, Optimization, and Planning in 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2301.06015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06015v1)
- **Published**: 2023-01-15 03:43:45+00:00
- **Updated**: 2023-01-15 03:43:45+00:00
- **Authors**: Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: We introduce SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior works, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate SceneDiffuser with various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of SceneDiffuser for the broad community of 3D scene understanding.



### CMAE-V: Contrastive Masked Autoencoders for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.06018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06018v1)
- **Published**: 2023-01-15 05:07:41+00:00
- **Updated**: 2023-01-15 05:07:41+00:00
- **Authors**: Cheng-Ze Lu, Xiaojie Jin, Zhicheng Huang, Qibin Hou, Ming-Ming Cheng, Jiashi Feng
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Contrastive Masked Autoencoder (CMAE), as a new self-supervised framework, has shown its potential of learning expressive feature representations in visual image recognition. This work shows that CMAE also trivially generalizes well on video action recognition without modifying the architecture and the loss criterion. By directly replacing the original pixel shift with the temporal shift, our CMAE for visual action recognition, CMAE-V for short, can generate stronger feature representations than its counterpart based on pure masked autoencoders. Notably, CMAE-V, with a hybrid architecture, can achieve 82.2% and 71.6% top-1 accuracy on the Kinetics-400 and Something-something V2 datasets, respectively. We hope this report could provide some informative inspiration for future works.



### Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/2301.06020v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06020v1)
- **Published**: 2023-01-15 05:31:52+00:00
- **Updated**: 2023-01-15 05:31:52+00:00
- **Authors**: Kai Jia, Hongwen Zhang, Liang An, Yebin Liu
- **Comment**: Project Page: https://kairobo.github.io/PaFF/
- **Journal**: None
- **Summary**: Regression-based methods have shown high efficiency and effectiveness for multi-view human mesh recovery. The key components of a typical regressor lie in the feature extraction of input views and the fusion of multi-view features. In this paper, we present Pixel-aligned Feedback Fusion (PaFF) for accurate yet efficient human mesh recovery from multi-view images. PaFF is an iterative regression framework that performs feature extraction and fusion alternately. At each iteration, PaFF extracts pixel-aligned feedback features from each input view according to the reprojection of the current estimation and fuses them together with respect to each vertex of the downsampled mesh. In this way, our regressor can not only perceive the misalignment status of each view from the feedback features but also correct the mesh parameters more effectively based on the feature fusion on mesh vertices. Additionally, our regressor disentangles the global orientation and translation of the body mesh from the estimation of mesh parameters such that the camera parameters of input views can be better utilized in the regression process. The efficacy of our method is validated in the Human3.6M dataset via comprehensive ablation experiments, where PaFF achieves 33.02 MPJPE and brings significant improvements over the previous best solutions by more than 29%. The project page with code and video results can be found at https://kairobo.github.io/PaFF/.



### Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels
- **Arxiv ID**: http://arxiv.org/abs/2301.06043v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06043v1)
- **Published**: 2023-01-15 08:28:33+00:00
- **Updated**: 2023-01-15 08:28:33+00:00
- **Authors**: Sihan Wang, Fuping Wu, Lei Li, Zheyao Gao, Byung-Woo Hong, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac segmentation is in great demand for clinical practice. Due to the enormous labor of manual delineation, unsupervised segmentation is desired. The ill-posed optimization problem of this task is inherently challenging, requiring well-designed constraints. In this work, we propose an unsupervised framework for multi-class segmentation with both intensity and shape constraints. Firstly, we extend a conventional non-convex energy function as an intensity constraint and implement it with U-Net. For shape constraint, synthetic images are generated from anatomical labels via image-to-image translation, as shape supervision for the segmentation network. Moreover, augmentation invariance is applied to facilitate the segmentation network to learn the latent features in terms of shape. We evaluated the proposed framework using the public datasets from MICCAI2019 MSCMR Challenge and achieved promising results on cardiac MRIs with Dice scores of 0.5737, 0.7796, and 0.6287 in Myo, LV, and RV, respectively.



### DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets
- **Arxiv ID**: http://arxiv.org/abs/2301.06051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06051v2)
- **Published**: 2023-01-15 09:31:58+00:00
- **Updated**: 2023-03-20 16:36:27+00:00
- **Authors**: Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen Wang, Di He, Bernt Schiele, Liwei Wang
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Designing an efficient yet deployment-friendly 3D backbone to handle sparse point clouds is a fundamental problem in 3D perception. Compared with the customized sparse convolution, the attention mechanism in Transformers is more appropriate for flexibly modeling long-range relationships and is easier to be deployed in real-world applications. However, due to the sparse characteristics of point clouds, it is non-trivial to apply a standard transformer on sparse points. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a single-stride window-based voxel Transformer backbone for outdoor 3D perception. In order to efficiently process sparse points in parallel, we propose Dynamic Sparse Window Attention, which partitions a series of local regions in each window according to its sparsity and then computes the features of all regions in a fully parallel manner. To allow the cross-set connection, we design a rotated set partitioning strategy that alternates between two partitioning configurations in consecutive self-attention layers. To support effective downsampling and better encode geometric information, we also propose an attention-style 3D pooling module on sparse points, which is powerful and deployment-friendly without utilizing any customized CUDA operations. Our model achieves state-of-the-art performance with a broad range of 3D perception tasks. More importantly, DSVT can be easily deployed by TensorRT with real-time inference speed (27Hz). Code will be available at \url{https://github.com/Haiyang-W/DSVT}.



### T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations
- **Arxiv ID**: http://arxiv.org/abs/2301.06052v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06052v3)
- **Published**: 2023-01-15 09:34:42+00:00
- **Updated**: 2023-02-28 05:23:51+00:00
- **Authors**: Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen
- **Comment**: Accepted to CVPR 2023. Project page:
  https://mael-zys.github.io/T2M-GPT/
- **Journal**: None
- **Summary**: In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.



### Learning Audio-Driven Viseme Dynamics for 3D Face Animation
- **Arxiv ID**: http://arxiv.org/abs/2301.06059v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06059v1)
- **Published**: 2023-01-15 09:55:46+00:00
- **Updated**: 2023-01-15 09:55:46+00:00
- **Authors**: Linchao Bao, Haoxian Zhang, Yue Qian, Tangli Xue, Changhai Chen, Xuefei Zhe, Di Kang
- **Comment**: Project page: https://linchaobao.github.io/viseme2023/
- **Journal**: None
- **Summary**: We present a novel audio-driven facial animation approach that can generate realistic lip-synchronized 3D facial animations from the input audio. Our approach learns viseme dynamics from speech videos, produces animator-friendly viseme curves, and supports multilingual speech inputs. The core of our approach is a novel parametric viseme fitting algorithm that utilizes phoneme priors to extract viseme parameters from speech videos. With the guidance of phonemes, the extracted viseme curves can better correlate with phonemes, thus more controllable and friendly to animators. To support multilingual speech inputs and generalizability to unseen voices, we take advantage of deep audio feature models pretrained on multiple languages to learn the mapping from audio to viseme curves. Our audio-to-curves mapping achieves state-of-the-art performance even when the input audio suffers from distortions of volume, pitch, speed, or noise. Lastly, a viseme scanning approach for acquiring high-fidelity viseme assets is presented for efficient speech animation production. We show that the predicted viseme curves can be applied to different viseme-rigged characters to yield various personalized animations with realistic and natural facial motions. Our approach is artist-friendly and can be easily integrated into typical animation production workflows including blendshape or bone based animation.



### MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation
- **Arxiv ID**: http://arxiv.org/abs/2301.06077v3
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; I.5.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2301.06077v3)
- **Published**: 2023-01-15 11:56:32+00:00
- **Updated**: 2023-03-26 06:04:43+00:00
- **Authors**: Takato Yasuno, Masahiro Okano, Junichiro Fujii
- **Comment**: 8 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: For infrastructure inspections, damage representation does not constantly match the predefined classes of damage grade, resulting in detailed clusters of unseen damages or more complex clusters from overlapped space between two grades. The damage representation has fundamentally complex features; consequently, not all the damage classes can be perfectly predefined. The proposed MN-pair contrastive learning method helps to explore an embedding damage representation beyond the predefined classes by including more detailed clusters. It maximizes both the similarity of M-1 positive images close to an anchor and dissimilarity of N-1 negative images using both weighting loss functions. It learns faster than the N-pair algorithm using one positive image. We proposed a pipeline to obtain the damage representation and used a density-based clustering on a 2-D reduction space to automate finer cluster discrimination. We also visualized the explanation of the damage feature using Grad-CAM for MN-pair damage metric learning. We demonstrated our method in three experimental studies: steel product defect, concrete crack, and the effectiveness of our method and discuss future works.



### Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics
- **Arxiv ID**: http://arxiv.org/abs/2301.06103v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.06103v1)
- **Published**: 2023-01-15 14:13:03+00:00
- **Updated**: 2023-01-15 14:13:03+00:00
- **Authors**: Sania Zahan, Ghulam Mubashar Hassan, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Athlete performance measurement in sports videos requires modeling long sequences since the entire spatio-temporal progression contributes dominantly to the performance. It is crucial to comprehend local discriminative spatial dependencies and global semantics for accurate evaluation. However, existing benchmark datasets mainly incorporate sports where the performance lasts only a few seconds. Consequently, state-ofthe-art sports quality assessment methods specifically focus on spatial structure. Although they achieve high performance in short-term sports, they are unable to model prolonged video sequences and fail to achieve similar performance in long-term sports. To facilitate such analysis, we introduce a new dataset, coined AGF-Olympics, that incorporates artistic gymnastic floor routines. AFG-Olympics provides highly challenging scenarios with extensive background, viewpoint, and scale variations over an extended sample duration of up to 2 minutes. In addition, we propose a discriminative attention module to map the dense feature space into a sparse representation by disentangling complex associations. Extensive experiments indicate that our proposed module provides an effective way to embed long-range spatial and temporal correlation semantics.



### Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.06115v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06115v1)
- **Published**: 2023-01-15 15:18:02+00:00
- **Updated**: 2023-01-15 15:18:02+00:00
- **Authors**: Chuanmin Jia, Feng Ye, Huifang Sun, Siwei Ma, Wen Gao
- **Comment**: MPAI End-to-end Video group progress report, DCC 2023
- **Journal**: None
- **Summary**: During the past decade, the Unmanned-Aerial-Vehicles (UAVs) have attracted increasing attention due to their flexible, extensive, and dynamic space-sensing capabilities. The volume of video captured by UAVs is exponentially growing along with the increased bitrate generated by the advancement of the sensors mounted on UAVs, bringing new challenges for on-device UAV storage and air-ground data transmission. Most existing video compression schemes were designed for natural scenes without consideration of specific texture and view characteristics of UAV videos. In this work, we first contribute a detailed analysis of the current state of the field of UAV video coding. Then we propose to establish a novel task for learned UAV video coding and construct a comprehensive and systematic benchmark for such a task, present a thorough review of high quality UAV video datasets and benchmarks, and contribute extensive rate-distortion efficiency comparison of learned and conventional codecs after. Finally, we discuss the challenges of encoding UAV videos. It is expected that the benchmark will accelerate the research and development in video coding on drone platforms.



### Maximally Compact and Separated Features with Regular Polytope Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.06116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06116v1)
- **Published**: 2023-01-15 15:20:57+00:00
- **Updated**: 2023-01-15 15:20:57+00:00
- **Authors**: Federico Pernici, Matteo Bruni, Claudio Baecchi, Alberto Del Bimbo
- **Comment**: DEEPVISION 2019 CVPR 2019, LONG BEACH Sunday, 16th June @ Room
  "Terrace Theater" https://sites.google.com/view/deepvision2019/program
  https://openaccess.thecvf.com/content_CVPRW_2019/html/Deep_Vision_Workshop/Pernici_Maximally_Compact_and_Separated_Features_with_Regular_Polytope_Networks_CVPRW_2019_paper.html.
  arXiv admin note: text overlap with arXiv:1902.10441
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) trained with the Softmax loss are widely used classification models for several vision tasks. Typically, a learnable transformation (i.e. the classifier) is placed at the end of such models returning class scores that are further normalized into probabilities by Softmax. This learnable transformation has a fundamental role in determining the network internal feature representation.   In this work we show how to extract from CNNs features with the properties of \emph{maximum} inter-class separability and \emph{maximum} intra-class compactness by setting the parameters of the classifier transformation as not trainable (i.e. fixed). We obtain features similar to what can be obtained with the well-known ``Center Loss'' \cite{wen2016discriminative} and other similar approaches but with several practical advantages including maximal exploitation of the available feature space representation, reduction in the number of network parameters, no need to use other auxiliary losses besides the Softmax.   Our approach unifies and generalizes into a common approach two apparently different classes of methods regarding: discriminative features, pioneered by the Center Loss \cite{wen2016discriminative} and fixed classifiers, firstly evaluated in \cite{hoffer2018fix}.   Preliminary qualitative experimental results provide some insight on the potentialities of our combined strategy.



### CORE: Learning Consistent Ordinal REpresentations for Image Ordinal Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.06122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06122v1)
- **Published**: 2023-01-15 15:42:26+00:00
- **Updated**: 2023-01-15 15:42:26+00:00
- **Authors**: Yiming Lei, Zilong Li, Yangyang Li, Junping Zhang, Hongming Shan
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The goal of image ordinal estimation is to estimate the ordinal label of a given image with a convolutional neural network. Existing methods are mainly based on ordinal regression and particularly focus on modeling the ordinal mapping from the feature representation of the input to the ordinal label space. However, the manifold of the resultant feature representations does not maintain the intrinsic ordinal relations of interest, which hinders the effectiveness of the image ordinal estimation. Therefore, this paper proposes learning intrinsic Consistent Ordinal REpresentations (CORE) from ordinal relations residing in groundtruth labels while encouraging the feature representations to embody the ordinal low-dimensional manifold. First, we develop an ordinal totally ordered set (toset) distribution (OTD), which can (i) model the label embeddings to inherit ordinal information and measure distances between ordered labels of samples in a neighborhood, and (ii) model the feature embeddings to infer numerical magnitude with unknown ordinal information among the features of different samples. Second, through OTD, we convert the feature representations and labels into the same embedding space for better alignment, and then compute the Kullback Leibler (KL) divergence between the ordinal labels and feature representations to endow the latent space with consistent ordinal relations. Third, we optimize the KL divergence through ordinal prototype-constrained convex programming with dual decomposition; our theoretical analysis shows that we can obtain the optimal solutions via gradient backpropagation. Extensive experimental results demonstrate that the proposed CORE can accurately construct an ordinal latent space and significantly enhance existing deep ordinal regression methods to achieve better results.



### Deep Diversity-Enhanced Feature Representation of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/2301.06132v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06132v1)
- **Published**: 2023-01-15 16:19:18+00:00
- **Updated**: 2023-01-15 16:19:18+00:00
- **Authors**: Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, Deyu Meng
- **Comment**: 15 pages, 12 figures. arXiv admin note: substantial text overlap with
  arXiv:2207.04266
- **Journal**: None
- **Summary**: In this paper, we study the problem of embedding the high-dimensional spatio-spectral information of hyperspectral (HS) images efficiently and effectively, oriented by feature diversity. To be specific, based on the theoretical formulation that feature diversity is correlated with the rank of the unfolded kernel matrix, we rectify 3D convolution by modifying its topology to boost the rank upper-bound, yielding a rank-enhanced spatial-spectral symmetrical convolution set (ReS$^3$-ConvSet), which is able to not only learn diverse and powerful feature representations but also save network parameters. In addition, we also propose a novel diversity-aware regularization (DA-Reg) term, which acts directly on the feature maps to maximize the independence among elements. To demonstrate the superiority of the proposed ReS$^3$-ConvSet and DA-Reg, we apply them to various HS image processing and analysis tasks, including denoising, spatial super-resolution, and classification. Extensive experiments demonstrate that the proposed approaches outperform state-of-the-art methods to a significant extent both quantitatively and qualitatively. The code is publicly available at \url{https://github.com/jinnh/ReSSS-ConvSet}.



### Improving Reliability of Fine-tuning with Block-wise Optimisation
- **Arxiv ID**: http://arxiv.org/abs/2301.06133v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06133v1)
- **Published**: 2023-01-15 16:20:18+00:00
- **Updated**: 2023-01-15 16:20:18+00:00
- **Authors**: Basel Barakat, Qiang Huang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Finetuning can be used to tackle domain-specific tasks by transferring knowledge. Previous studies on finetuning focused on adapting only the weights of a task-specific classifier or re-optimizing all layers of the pre-trained model using the new task data. The first type of methods cannot mitigate the mismatch between a pre-trained model and the new task data, and the second type of methods easily cause over-fitting when processing tasks with limited data. To explore the effectiveness of fine-tuning, we propose a novel block-wise optimization mechanism, which adapts the weights of a group of layers of a pre-trained model. In our work, the layer selection can be done in four different ways. The first is layer-wise adaptation, which aims to search for the most salient single layer according to the classification performance. The second way is based on the first one, jointly adapting a small number of top-ranked layers instead of using an individual layer. The third is block based segmentation, where the layers of a deep network is segmented into blocks by non-weighting layers, such as the MaxPooling layer and Activation layer. The last one is to use a fixed-length sliding window to group layers block by block. To identify which group of layers is the most suitable for finetuning, the search starts from the target end and is conducted by freezing other layers excluding the selected layers and the classification layers. The most salient group of layers is determined in terms of classification performance. In our experiments, the proposed approaches are tested on an often-used dataset, Tf_flower, by finetuning five typical pre-trained models, VGG16, MobileNet-v1, MobileNet-v2, MobileNet-v3, and ResNet50v2, respectively. The obtained results show that the use of our proposed block-wise approaches can achieve better performances than the two baseline methods and the layer-wise method.



### Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2301.06143v1
- **DOI**: 10.1145/3572864.3580337
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06143v1)
- **Published**: 2023-01-15 16:52:59+00:00
- **Updated**: 2023-01-15 16:52:59+00:00
- **Authors**: Yiqin Zhao, Sean Fanello, Tian Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Lighting understanding plays an important role in virtual object composition, including mobile augmented reality (AR) applications. Prior work often targets recovering lighting from the physical environment to support photorealistic AR rendering. Because the common workflow is to use a back-facing camera to capture the physical world for overlaying virtual objects, we refer to this usage pattern as back-facing AR. However, existing methods often fall short in supporting emerging front-facing mobile AR applications, e.g., virtual try-on where a user leverages a front-facing camera to explore the effect of various products (e.g., glasses or hats) of different styles. This lack of support can be attributed to the unique challenges of obtaining 360$^\circ$ HDR environment maps, an ideal format of lighting representation, from the front-facing camera and existing techniques. In this paper, we propose to leverage dual-camera streaming to generate a high-quality environment map by combining multi-view lighting reconstruction and parametric directional lighting estimation. Our preliminary results show improved rendering quality using a dual-camera setup for front-facing AR compared to a commercial solution.



### Inpainting borehole images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.06152v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06152v1)
- **Published**: 2023-01-15 18:15:52+00:00
- **Updated**: 2023-01-15 18:15:52+00:00
- **Authors**: Rachid Belmeskine, Abed Benaichouche
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we propose a GAN-based approach for gap filling in borehole images created by wireline microresistivity imaging tools. The proposed method utilizes a generator, global discriminator, and local discriminator to inpaint the missing regions of the image. The generator is based on an auto-encoder architecture with skip-connections, and the loss function used is the Wasserstein GAN loss. Our experiments on a dataset of borehole images demonstrate that the proposed model can effectively deal with large-scale missing pixels and generate realistic completion results. This approach can improve the quantitative evaluation of reservoirs and provide an essential basis for interpreting geological phenomena and reservoir parameters.



### TextileNet: A Material Taxonomy-based Fashion Textile Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.06160v1
- **DOI**: None
- **Categories**: **cs.DL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06160v1)
- **Published**: 2023-01-15 19:02:18+00:00
- **Updated**: 2023-01-15 19:02:18+00:00
- **Authors**: Shu Zhong, Miriam Ribul, Youngjun Cho, Marianna Obrist
- **Comment**: 10 papes, 4 figures, 2 tables
- **Journal**: None
- **Summary**: The rise of Machine Learning (ML) is gradually digitalizing and reshaping the fashion industry. Recent years have witnessed a number of fashion AI applications, for example, virtual try-ons. Textile material identification and categorization play a crucial role in the fashion textile sector, including fashion design, retails, and recycling. At the same time, Net Zero is a global goal and the fashion industry is undergoing a significant change so that textile materials can be reused, repaired and recycled in a sustainable manner. There is still a challenge in identifying textile materials automatically for garments, as we lack a low-cost and effective technique for identifying them. In light of this, we build the first fashion textile dataset, TextileNet, based on textile material taxonomies - a fibre taxonomy and a fabric taxonomy generated in collaboration with material scientists. TextileNet can be used to train and evaluate the state-of-the-art Deep Learning models for textile materials. We hope to standardize textile related datasets through the use of taxonomies. TextileNet contains 33 fibres labels and 27 fabrics labels, and has in total 760,949 images. We use standard Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to establish baselines for this dataset. Future applications for this dataset range from textile classification to optimization of the textile supply chain and interactive design for consumers. We envision that this can contribute to the development of a new AI-based fashion platform.



### Secure Video Streaming Using Dedicated Hardware
- **Arxiv ID**: http://arxiv.org/abs/2301.06180v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06180v2)
- **Published**: 2023-01-15 20:24:46+00:00
- **Updated**: 2023-03-25 20:56:14+00:00
- **Authors**: Nicholas Murray-Hill, Laura Fontes, Pedro Machado, Isibor Kennedy Ihianle
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: The purpose of this article is to present a system that enhances the security, efficiency, and reconfigurability of an Internet-of-Things (IoT) system used for surveillance and monitoring. Methods: A Multi-Processor System-On-Chip (MPSoC) composed of Central Processor Unit (CPU) and Field-Programmable Gate Array (FPGA) is proposed for increasing the security and the frame rate of a smart IoT edge device. The private encryption key is safely embedded in the FPGA unit to avoid being exposed in the Random Access Memory (RAM). This allows the edge device to securely store and authenticate the key, protecting the data transmitted from the same Integrated Circuit (IC). Additionally, the edge device can simultaneously publish and route a camera stream using a lightweight communication protocol, achieving a frame rate of 14 frames per Second (fps). The performance of the MPSoC is compared to a NVIDIA Jetson Nano (NJN) and a Raspberry Pi 4 (RPI4) and it is found that the RPI4 is the most cost-effective solution but with lower frame rate, the NJN is the fastest because it can achieve higher frame-rate but it is not secure, and the MPSoC is the optimal solution because it offers a balanced frame rate and it is secure because it never exposes the secure key into the memory. Results: The proposed system successfully addresses the challenges of security, scalability, and efficiency in an IoT system used for surveillance and monitoring. The encryption key is securely stored and authenticated, and the edge device is able to simultaneously publish and route a camera stream feed high-definition images at 14 fps.



### LitAR: Visually Coherent Lighting for Mobile Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2301.06184v1
- **DOI**: 10.1145/3550291
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06184v1)
- **Published**: 2023-01-15 20:47:38+00:00
- **Updated**: 2023-01-15 20:47:38+00:00
- **Authors**: Yiqin Zhao, Chongyang Ma, Haibin Huang, Tian Guo
- **Comment**: None
- **Journal**: None
- **Summary**: An accurate understanding of omnidirectional environment lighting is crucial for high-quality virtual object rendering in mobile augmented reality (AR). In particular, to support reflective rendering, existing methods have leveraged deep learning models to estimate or have used physical light probes to capture physical lighting, typically represented in the form of an environment map. However, these methods often fail to provide visually coherent details or require additional setups. For example, the commercial framework ARKit uses a convolutional neural network that can generate realistic environment maps; however the corresponding reflective rendering might not match the physical environments. In this work, we present the design and implementation of a lighting reconstruction framework called LitAR that enables realistic and visually-coherent rendering. LitAR addresses several challenges of supporting lighting information for mobile AR. First, to address the spatial variance problem, LitAR uses two-field lighting reconstruction to divide the lighting reconstruction task into the spatial variance-aware near-field reconstruction and the directional-aware far-field reconstruction. The corresponding environment map allows reflective rendering with correct color tones. Second, LitAR uses two noise-tolerant data capturing policies to ensure data quality, namely guided bootstrapped movement and motion-based automatic capturing. Third, to handle the mismatch between the mobile computation capability and the high computation requirement of lighting reconstruction, LitAR employs two novel real-time environment map rendering techniques called multi-resolution projection and anchor extrapolation. These two techniques effectively remove the need of time-consuming mesh reconstruction while maintaining visual quality.



### CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior from Videos: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2301.06187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2301.06187v1)
- **Published**: 2023-01-15 20:54:44+00:00
- **Updated**: 2023-01-15 20:54:44+00:00
- **Authors**: Michael Perez, Corey Toler-Franklin
- **Comment**: 29 pages, 20 figures
- **Journal**: None
- **Summary**: Classifying the behavior of humans or animals from videos is important in biomedical fields for understanding brain function and response to stimuli. Action recognition, classifying activities performed by one or more subjects in a trimmed video, forms the basis of many of these techniques. Deep learning models for human action recognition have progressed significantly over the last decade. Recently, there is an increased interest in research that incorporates deep learning-based action recognition for animal behavior classification. However, human action recognition methods are more developed. This survey presents an overview of human action recognition and pose estimation methods that are based on convolutional neural network (CNN) architectures and have been adapted for animal behavior classification in neuroscience. Pose estimation, estimating joint positions from an image frame, is included because it is often applied before classifying animal behavior. First, we provide foundational information on algorithms that learn spatiotemporal features through 2D, two-stream, and 3D CNNs. We explore motivating factors that determine optimizers, loss functions and training procedures, and compare their performance on benchmark datasets. Next, we review animal behavior frameworks that use or build upon these methods, organized by the level of supervision they require. Our discussion is uniquely focused on the technical evolution of the underlying CNN models and their architectural adaptations (which we illustrate), rather than their usability in a neuroscience lab. We conclude by discussing open research problems, and possible research directions. Our survey is designed to be a resource for researchers developing fully unsupervised animal behavior classification systems of which there are only a few examples in the literature.



### BuildSeg: A General Framework for the Segmentation of Buildings
- **Arxiv ID**: http://arxiv.org/abs/2301.06190v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06190v1)
- **Published**: 2023-01-15 21:09:00+00:00
- **Updated**: 2023-01-15 21:09:00+00:00
- **Authors**: Lei Li, Tianfang Zhang, Stefan Oehmcke, Fabian Gieseke, Christian Igel
- **Comment**: None
- **Journal**: None
- **Summary**: Building segmentation from aerial images and 3D laser scanning (LiDAR) is a challenging task due to the diversity of backgrounds, building textures, and image quality. While current research using different types of convolutional and transformer networks has considerably improved the performance on this task, even more accurate segmentation methods for buildings are desirable for applications such as automatic mapping. In this study, we propose a general framework termed \emph{BuildSeg} employing a generic approach that can be quickly applied to segment buildings. Different data sources were combined to increase generalization performance. The approach yields good results for different data sources as shown by experiments on high-resolution multi-spectral and LiDAR imagery of cities in Norway, Denmark and France. We applied ConvNeXt and SegFormer based models on the high resolution aerial image dataset from the MapAI-competition. The methods achieved an IOU of 0.7902 and a boundary IOU of 0.6185. We used post-processing to account for the rectangular shape of the objects. This increased the boundary IOU from 0.6185 to 0.6189.



### RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs
- **Arxiv ID**: http://arxiv.org/abs/2301.06193v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06193v1)
- **Published**: 2023-01-15 21:27:35+00:00
- **Updated**: 2023-01-15 21:27:35+00:00
- **Authors**: André Santos, João Dinis Ferreira, Onur Mutlu, Gabriel Falcao
- **Comment**: 17 pages, 4 figures, 14 tables
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Networks (CNNs) have become the standard class of deep neural network for image processing, classification and segmentation tasks. However, the large strides in accuracy obtained by CNNs have been derived from increasing the complexity of network topologies, which incurs sizeable performance and energy penalties in the training and inference of CNNs. Many recent works have validated the effectiveness of parameter quantization, which consists in reducing the bit width of the network's parameters, to enable the attainment of considerable performance and energy efficiency gains without significantly compromising accuracy. However, it is difficult to compare the relative effectiveness of different quantization methods. To address this problem, we introduce RedBit, an open-source framework that provides a transparent, extensible and easy-to-use interface to evaluate the effectiveness of different algorithms and parameter configurations on network accuracy. We use RedBit to perform a comprehensive survey of five state-of-the-art quantization methods applied to the MNIST, CIFAR-10 and ImageNet datasets. We evaluate a total of 2300 individual bit width combinations, independently tuning the width of the network's weight and input activation parameters, from 32 bits down to 1 bit (e.g., 8/8, 2/2, 1/32, 1/1, for weights/activations). Upwards of 20000 hours of computing time in a pool of state-of-the-art GPUs were used to generate all the results in this paper. For 1-bit quantization, the accuracy losses for the MNIST, CIFAR-10 and ImageNet datasets range between [0.26%, 0.79%], [9.74%, 32.96%] and [10.86%, 47.36%] top-1, respectively. We actively encourage the reader to download the source code and experiment with RedBit, and to submit their own observed results to our public repository, available at https://github.com/IT-Coimbra/RedBit.



