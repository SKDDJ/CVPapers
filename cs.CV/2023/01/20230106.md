# Arxiv Papers in cs.CV on 2023-01-06
### A survey on Organoid Image Analysis Platforms
- **Arxiv ID**: http://arxiv.org/abs/2301.02341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02341v1)
- **Published**: 2023-01-06 00:15:05+00:00
- **Updated**: 2023-01-06 00:15:05+00:00
- **Authors**: Alireza Ranjbaran, Azadeh Nazemi
- **Comment**: 19 pages, 10 figures, 5 tables, research review
- **Journal**: None
- **Summary**: An in-vitro cell culture system is used for biological discoveries and hypothesis-driven research on a particular cell type to understand mechanistic or test pharmaceutical drugs. Conventional in-vitro cultures have been applied to primary cells and immortalised cell lines plated on 2D surfaces. However, they are unreliable in complex physiological environments and can not always predict in-vivo behaviour correctly. Organoids are multicellular spheroids of a primary donor or stem cells that are replaced in vitro cell culture systems and are widely used in biological, biomedical and translational studies. Native heterogeneity, microanatomy, and functionality of an organ or diseased tissue can be represented by three-dimensional in-vitro tissue models such as organoids. Organoids are essential in in-vitro models for drug discovery and personalised drug screening. Many imaging artefacts such as organoid occlusion, overlap, out-of-focus spheroids and considerable heterogeneity in size cause difficulty in conventional image processing. Despite the power of organoid models for biology, their size and shape have mostly not been considered. Drug responses depend on dynamic changes in individual organoid morphology, number and size, which means differences in organoid shape and size, movement through focal planes, and live-cell staining with limited options cause challenges for drug response and growth analysis. This study primarily introduces the importance of the role of the organoid culture system in different disciplines of medical science and various scopes of utilising organoids. Then studies the challenges of operating organoids, followed by reviewing image analysis systems or platforms applied to organoids to address organoid utilising challenges.



### Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise
- **Arxiv ID**: http://arxiv.org/abs/2301.03393v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03393v4)
- **Published**: 2023-01-06 01:14:56+00:00
- **Updated**: 2023-06-16 09:25:24+00:00
- **Authors**: Kevin Bui, Yifei Lou, Fredrick Park, Jack Xin
- **Comment**: Accepted to Frontiers in Computer Science:
  https://www.frontiersin.org/articles/10.3389/fcomp.2023.1131317/abstract;
  Arxiv version has clearer images best for zooming in
- **Journal**: None
- **Summary**: In this paper, we aim to segment an image degraded by blur and Poisson noise. We adopt a smoothing-and-thresholding (SaT) segmentation framework that finds a piecewise-smooth solution, followed by $k$-means clustering to segment the image. Specifically for the image smoothing step, we replace the least-squares fidelity for Gaussian noise in the Mumford-Shah model with a maximum posterior (MAP) term to deal with Poisson noise and we incorporate the weighted difference of anisotropic and isotropic total variation (AITV) as a regularization to promote the sparsity of image gradients. For such a nonconvex model, we develop a specific splitting scheme and utilize a proximal operator to apply the alternating direction method of multipliers (ADMM). Convergence analysis is provided to validate the efficacy of the ADMM scheme. Numerical experiments on various segmentation scenarios (grayscale/color and multiphase) showcase that our proposed method outperforms a number of segmentation methods, including the original SaT.



### Text2Poster: Laying out Stylized Texts on Retrieved Images
- **Arxiv ID**: http://arxiv.org/abs/2301.02363v1
- **DOI**: 10.1109/ICASSP43922.2022.9747465
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02363v1)
- **Published**: 2023-01-06 04:06:23+00:00
- **Updated**: 2023-01-06 04:06:23+00:00
- **Authors**: Chuhao Jin, Hongteng Xu, Ruihua Song, Zhiwu Lu
- **Comment**: 5 pages, Accepted to ICASSP 2022
- **Journal**: None
- **Summary**: Poster generation is a significant task for a wide range of applications, which is often time-consuming and requires lots of manual editing and artistic experience. In this paper, we propose a novel data-driven framework, called \textit{Text2Poster}, to automatically generate visually-effective posters from textual information. Imitating the process of manual poster editing, our framework leverages a large-scale pretrained visual-textual model to retrieve background images from given texts, lays out the texts on the images iteratively by cascaded auto-encoders, and finally, stylizes the texts by a matching-based method. We learn the modules of the framework by weakly- and self-supervised learning strategies, mitigating the demand for labeled data. Both objective and subjective experiments demonstrate that our Text2Poster outperforms state-of-the-art methods, including academic research and commercial software, on the quality of generated posters.



### Object as Query: Lifting any 2D Object Detector to 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.02364v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02364v2)
- **Published**: 2023-01-06 04:08:20+00:00
- **Updated**: 2023-06-05 05:40:56+00:00
- **Authors**: Zitian Wang, Zehao Huang, Jiahui Fu, Naiyan Wang, Si Liu
- **Comment**: technical report
- **Journal**: None
- **Summary**: 3D object detection from multi-view images has drawn much attention over the past few years. Existing methods mainly establish 3D representations from multi-view images and adopt a dense detection head for object detection, or employ object queries distributed in 3D space to localize objects. In this paper, we design Multi-View 2D Objects guided 3D Object Detector (MV2D), which can lift any 2D object detector to multi-view 3D object detection. Since 2D detections can provide valuable priors for object existence, MV2D exploits 2D detectors to generate object queries conditioned on the rich image semantics. These dynamically generated queries help MV2D to recall objects in the field of view and show a strong capability of localizing 3D objects. For the generated queries, we design a sparse cross attention module to force them to focus on the features of specific objects, which suppresses interference from noises. The evaluation results on the nuScenes dataset demonstrate the dynamic object queries and sparse feature aggregation can promote 3D detection capability. MV2D also exhibits a state-of-the-art performance among existing methods. We hope MV2D can serve as a new baseline for future research.



### Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.02371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02371v2)
- **Published**: 2023-01-06 04:35:31+00:00
- **Updated**: 2023-03-28 04:28:30+00:00
- **Authors**: Shaofei Huang, Zhenwei Shen, Zehao Huang, Zi-han Ding, Jiao Dai, Jizhong Han, Naiyan Wang, Si Liu
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Monocular 3D lane detection is a challenging task due to its lack of depth information. A popular solution is to first transform the front-viewed (FV) images or features into the bird-eye-view (BEV) space with inverse perspective mapping (IPM) and detect lanes from BEV features. However, the reliance of IPM on flat ground assumption and loss of context information make it inaccurate to restore 3D information from BEV representations. An attempt has been made to get rid of BEV and predict 3D lanes from FV representations directly, while it still underperforms other BEV-based methods given its lack of structured representation for 3D lanes. In this paper, we define 3D lane anchors in the 3D space and propose a BEV-free method named Anchor3DLane to predict 3D lanes directly from FV representations. 3D lane anchors are projected to the FV features to extract their features which contain both good structural and context information to make accurate predictions. In addition, we also develop a global optimization method that makes use of the equal-width property between lanes to reduce the lateral error of predictions. Extensive experiments on three popular 3D lane detection benchmarks show that our Anchor3DLane outperforms previous BEV-based methods and achieves state-of-the-art performances. The code is available at: https://github.com/tusen-ai/Anchor3DLane.



### CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior
- **Arxiv ID**: http://arxiv.org/abs/2301.02379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02379v2)
- **Published**: 2023-01-06 05:04:32+00:00
- **Updated**: 2023-04-03 15:58:43+00:00
- **Authors**: Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, Tien-Tsin Wong
- **Comment**: CVPR2023 Camera-Ready. Project Page:
  https://doubiiu.github.io/projects/codetalker/, Code:
  https://github.com/Doubiiu/CodeTalker
- **Journal**: None
- **Summary**: Speech-driven 3D facial animation has been widely studied, yet there is still a gap to achieving realism and vividness due to the highly ill-posed nature and scarcity of audio-visual data. Existing works typically formulate the cross-modal mapping into a regression task, which suffers from the regression-to-mean problem leading to over-smoothed facial motions. In this paper, we propose to cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively promotes the vividness of the generated motions by reducing the cross-modal mapping uncertainty. The codebook is learned by self-reconstruction over real facial motions and thus embedded with realistic facial motion priors. Over the discrete motion space, a temporal autoregressive model is employed to sequentially synthesize facial motions from the input speech signal, which guarantees lip-sync as well as plausible facial expressions. We demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. Also, a user study further justifies our superiority in perceptual quality.



### Generating corneal panoramic images from contact specular microscope images
- **Arxiv ID**: http://arxiv.org/abs/2301.02388v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02388v1)
- **Published**: 2023-01-06 05:46:42+00:00
- **Updated**: 2023-01-06 05:46:42+00:00
- **Authors**: Yusuke Nagira, Yuzuha Hara, Satoru Hiwa, Naoki Okumura, Noriko Koizumi, Tomoyuki Hiroyasu
- **Comment**: None
- **Journal**: None
- **Summary**: The contact specular microscope has a wider angle of view than that of the non-contact specular microscope but still cannot capture an image of the entire cornea. To obtain such an image, it is necessary to prepare film on the parts of the image captured sequentially and combine them to create a complete image. This study proposes a framework to automatically generate an entire corneal image from videos captured using a contact specular microscope. Relatively focused images were extracted from the videos and panoramic compositing was performed. If an entire image can be generated, it is possible to detect guttae from the image and examine the extent of their presence. The system was implemented and the effectiveness of the proposed framework was examined. The system was implemented using custom-made composite software, Image Composite Software (ICS, K.I. Technology Co., Ltd., Japan, internal algorithms not disclosed), and a supervised learning model using U-Net was used for guttae detection. Several images were correctly synthesized when the constructed system was applied to 94 different corneal videos obtained from Fuchs endothelial corneal dystrophy (FECD) mouse model. The implementation and application of the method to the data in this study confirmed its effectiveness. Owing to the minimal quantitative evaluation performed, such as accuracy with implementation, it may pose some limitations for future investigations.



### Deep-learning models in medical image analysis: Detection of esophagitis from the Kvasir Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.02390v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02390v1)
- **Published**: 2023-01-06 05:53:01+00:00
- **Updated**: 2023-01-06 05:53:01+00:00
- **Authors**: Kyoka Yoshiok, Kensuke Tanioka, Satoru Hiwa, Tomoyuki Hiroyasu
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of esophagitis is important because this condition can progress to cancer if left untreated. However, the accuracies of different deep learning models in detecting esophagitis have yet to be compared. Thus, this study aimed to compare the accuracies of convolutional neural network models (GoogLeNet, ResNet-50, MobileNet V2, and MobileNet V3) in detecting esophagitis from the open Kvasir dataset of endoscopic images. Results showed that among the models, GoogLeNet achieved the highest F1-scores. Based on the average of true positive rate, MobileNet V3 predicted esophagitis more confidently than the other models. The results obtained using the models were also compared with those obtained using SHapley Additive exPlanations and Gradient-weighted Class Activation Mapping.



### Graph Convolution Based Cross-Network Multi-Scale Feature Fusion for Deep Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.02393v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02393v1)
- **Published**: 2023-01-06 05:56:50+00:00
- **Updated**: 2023-01-06 05:56:50+00:00
- **Authors**: Gangming Zhao, Kongming Liang, Chengwei Pan, Fandong Zhang, Xianpeng Wu, Xinyang Hu, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Vessel segmentation is widely used to help with vascular disease diagnosis. Vessels reconstructed using existing methods are often not sufficiently accurate to meet clinical use standards. This is because 3D vessel structures are highly complicated and exhibit unique characteristics, including sparsity and anisotropy. In this paper, we propose a novel hybrid deep neural network for vessel segmentation. Our network consists of two cascaded subnetworks performing initial and refined segmentation respectively. The second subnetwork further has two tightly coupled components, a traditional CNN-based U-Net and a graph U-Net. Cross-network multi-scale feature fusion is performed between these two U-shaped networks to effectively support high-quality vessel segmentation. The entire cascaded network can be trained from end to end. The graph in the second subnetwork is constructed according to a vessel probability map as well as appearance and semantic similarities in the original CT volume. To tackle the challenges caused by the sparsity and anisotropy of vessels, a higher percentage of graph nodes are distributed in areas that potentially contain vessels while a higher percentage of edges follow the orientation of potential nearbyvessels. Extensive experiments demonstrate our deep network achieves state-of-the-art 3D vessel segmentation performance on multiple public and in-house datasets.



### CyberLoc: Towards Accurate Long-term Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2301.02403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02403v1)
- **Published**: 2023-01-06 06:49:36+00:00
- **Updated**: 2023-01-06 06:49:36+00:00
- **Authors**: Liu Liu, Yukai Lin, Xiao Liang, Qichao Xu, Miao Jia, Yangdong Liu, Yuxiang Wen, Wei Luo, Jiangwei Li
- **Comment**: MLAD-ECCV 2022
- **Journal**: None
- **Summary**: This technical report introduces CyberLoc, an image-based visual localization pipeline for robust and accurate long-term pose estimation under challenging conditions. The proposed method comprises four modules connected in a sequence. First, a mapping module is applied to build accurate 3D maps of the scene, one map for each reference sequence if there exist multiple reference sequences under different conditions. Second, a single-image-based localization pipeline (retrieval--matching--PnP) is performed to estimate 6-DoF camera poses for each query image, one for each 3D map. Third, a consensus set maximization module is proposed to filter out outlier 6-DoF camera poses, and outputs one 6-DoF camera pose for a query. Finally, a robust pose refinement module is proposed to optimize 6-DoF query poses, taking candidate global 6-DoF camera poses and their corresponding global 2D-3D matches, sparse 2D-2D feature matches between consecutive query images and SLAM poses of the query sequence as input. Experiments on the 4seasons dataset show that our method achieves high accuracy and robustness. In particular, our approach wins the localization challenge of ECCV 2022 workshop on Map-based Localization for Autonomous Driving (MLAD-ECCV2022).



### Exploring Efficient Few-shot Adaptation for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.02419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02419v1)
- **Published**: 2023-01-06 08:42:05+00:00
- **Updated**: 2023-01-06 08:42:05+00:00
- **Authors**: Chengming Xu, Siqian Yang, Yabiao Wang, Zhanxiong Wang, Yanwei Fu, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: The task of Few-shot Learning (FSL) aims to do the inference on novel categories containing only few labeled examples, with the help of knowledge learned from base categories containing abundant labeled training samples. While there are numerous works into FSL task, Vision Transformers (ViTs) have rarely been taken as the backbone to FSL with few trials focusing on naive finetuning of whole backbone or classification layer.} Essentially, despite ViTs have been shown to enjoy comparable or even better performance on other vision tasks, it is still very nontrivial to efficiently finetune the ViTs in real-world FSL scenarios. To this end, we propose a novel efficient Transformer Tuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The key novelties come from the newly presented Attentive Prefix Tuning (APT) and Domain Residual Adapter (DRA) for the task and backbone tuning, individually. Specifically, in APT, the prefix is projected to new key and value pairs that are attached to each self-attention layer to provide the model with task-specific information. Moreover, we design the DRA in the form of learnable offset vectors to handle the potential domain gaps between base and novel data. To ensure the APT would not deviate from the initial task-specific information much, we further propose a novel prototypical regularization, which maximizes the similarity between the projected distribution of prefix and initial prototypes, regularizing the update procedure. Our method receives outstanding performance on the challenging Meta-Dataset. We conduct extensive experiments to show the efficacy of our model.



### Valid P-Value for Deep Learning-Driven Salient Region
- **Arxiv ID**: http://arxiv.org/abs/2301.02437v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.02437v1)
- **Published**: 2023-01-06 09:51:09+00:00
- **Updated**: 2023-01-06 09:51:09+00:00
- **Authors**: Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi
- **Comment**: None
- **Journal**: None
- **Summary**: Various saliency map methods have been proposed to interpret and explain predictions of deep learning models. Saliency maps allow us to interpret which parts of the input signals have a strong influence on the prediction results. However, since a saliency map is obtained by complex computations in deep learning models, it is often difficult to know how reliable the saliency map itself is. In this study, we propose a method to quantify the reliability of a salient region in the form of p-values. Our idea is to consider a salient region as a selected hypothesis by the trained deep learning model and employ the selective inference framework. The proposed method can provably control the probability of false positive detections of salient regions. We demonstrate the validity of the proposed method through numerical examples in synthetic and real datasets. Furthermore, we develop a Keras-based framework for conducting the proposed selective inference for a wide class of CNNs without additional implementation cost.



### An Image captioning algorithm based on the Hybrid Deep Learning Technique (CNN+GRU)
- **Arxiv ID**: http://arxiv.org/abs/2301.02440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.02440v1)
- **Published**: 2023-01-06 10:00:06+00:00
- **Updated**: 2023-01-06 10:00:06+00:00
- **Authors**: Rana Adnan Ahmad, Muhammad Azhar, Hina Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning by the encoder-decoder framework has shown tremendous advancement in the last decade where CNN is mainly used as encoder and LSTM is used as a decoder. Despite such an impressive achievement in terms of accuracy in simple images, it lacks in terms of time complexity and space complexity efficiency. In addition to this, in case of complex images with a lot of information and objects, the performance of this CNN-LSTM pair downgraded exponentially due to the lack of semantic understanding of the scenes presented in the images. Thus, to take these issues into consideration, we present CNN-GRU encoder decode framework for caption-to-image reconstructor to handle the semantic context into consideration as well as the time complexity. By taking the hidden states of the decoder into consideration, the input image and its similar semantic representations is reconstructed and reconstruction scores from a semantic reconstructor are used in conjunction with likelihood during model training to assess the quality of the generated caption. As a result, the decoder receives improved semantic information, enhancing the caption production process. During model testing, combining the reconstruction score and the log-likelihood is also feasible to choose the most appropriate caption. The suggested model outperforms the state-of-the-art LSTM-A5 model for picture captioning in terms of time complexity and accuracy.



### Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.02464v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2301.02464v1)
- **Published**: 2023-01-06 11:22:59+00:00
- **Updated**: 2023-01-06 11:22:59+00:00
- **Authors**: Vincenzo Lomonaco, Lorenzo Pellegrini, Gabriele Graffieti, Davide Maltoni
- **Comment**: Book Chapter Preprint: 15 pages, 7 figures, 2 tables. arXiv admin
  note: text overlap with arXiv:1912.01100
- **Journal**: None
- **Summary**: In recent years we have witnessed a renewed interest in machine learning methodologies, especially for deep representation learning, that could overcome basic i.i.d. assumptions and tackle non-stationary environments subject to various distributional shifts or sample selection biases. Within this context, several computational approaches based on architectural priors, regularizers and replay policies have been proposed with different degrees of success depending on the specific scenario in which they were developed and assessed. However, designing comprehensive hybrid solutions that can flexibly and generally be applied with tunable efficiency-effectiveness trade-offs still seems a distant goal. In this paper, we propose "Architect, Regularize and Replay" (ARR), an hybrid generalization of the renowned AR1 algorithm and its variants, that can achieve state-of-the-art results in classic scenarios (e.g. class-incremental learning) but also generalize to arbitrary data streams generated from real-world datasets such as CIFAR-100, CORe50 and ImageNet-1000.



### Deep Learning For Classification Of Chest X-Ray Images (Covid 19)
- **Arxiv ID**: http://arxiv.org/abs/2301.02468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/2301.02468v1)
- **Published**: 2023-01-06 11:44:57+00:00
- **Updated**: 2023-01-06 11:44:57+00:00
- **Authors**: Benbakreti Samir, Said Mwanahija, Benbakreti Soumia, Umut Özkaya
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: In medical practice, the contribution of information technology can be considerable. Most of these practices include the images that medical assistance uses to identify different pathologies of the human body. One of them is X-ray images which cover much of our work in this paper. Chest x-rays have played an important role in Covid 19 identification and diagnosis. The Covid 19 virus has been declared a global pandemic since 2020 after the first case found in Wuhan China in December 2019. Our goal in this project is to be able to classify different chest X-ray images containing Covid 19, viral pneumonia, lung opacity and normal images. We used CNN architecture and different pre-trained models. The best result is obtained by the use of the ResNet 18 architecture with 94.1% accuracy. We also note that The GPU execution time is optimal in the case of AlexNet but what requires our attention is that the pretrained models converge much faster than the CNN. The time saving is very considerable. With these results not only will solve the diagnosis time for patients, but will provide an interesting tool for practitioners, thus helping them in times of strong pandemic in particular.



### Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering
- **Arxiv ID**: http://arxiv.org/abs/2301.02484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02484v1)
- **Published**: 2023-01-06 12:43:13+00:00
- **Updated**: 2023-01-06 12:43:13+00:00
- **Authors**: Huibing Wang, Mingze Yao, Guangqi Jiang, Zetian Mi, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised hashing methods have attracted widespread attention with the explosive growth of large-scale data, which can greatly reduce storage and computation by learning compact binary codes. Existing unsupervised hashing methods attempt to exploit the valuable information from samples, which fails to take the local geometric structure of unlabeled samples into consideration. Moreover, hashing based on auto-encoders aims to minimize the reconstruction loss between the input data and binary codes, which ignores the potential consistency and complementarity of multiple sources data. To address the above issues, we propose a hashing algorithm based on auto-encoders for multi-view binary clustering, which dynamically learns affinity graphs with low-rank constraints and adopts collaboratively learning between auto-encoders and affinity graphs to learn a unified binary code, called Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically, we propose a multi-view affinity graphs learning model with low-rank constraint, which can mine the underlying geometric information from multi-view data. Then, we design an encoder-decoder paradigm to collaborate the multiple affinity graphs, which can learn a unified binary code effectively. Notably, we impose the decorrelation and code balance constraints on binary codes to reduce the quantization errors. Finally, we utilize an alternating iterative optimization scheme to obtain the multi-view clustering results. Extensive experimental results on $5$ public datasets are provided to reveal the effectiveness of the algorithm and its superior performance over other state-of-the-art alternatives.



### TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.02488v1
- **DOI**: 10.1109/TGRS.2022.3213748
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02488v1)
- **Published**: 2023-01-06 12:56:53+00:00
- **Updated**: 2023-01-06 12:56:53+00:00
- **Authors**: Weicheng Gao, Xiaopeng Yang, Xiaodong Qu, Tian Lan
- **Comment**: Publisher: IEEE Transactions on Geoscience and Remote Sensing
  (Volume: 60). Total Pages: 17. Total Figures: 17
- **Journal**: in IEEE Transactions on Geoscience and Remote Sensing, vol. 60,
  pp. 1-17, 2022, Art no. 5118617
- **Summary**: To solve the problems of reduced accuracy and prolonging convergence time of through-the-wall radar (TWR) human motion due to wall attenuation, multipath effect, and system interference, we propose a multilink auto-encoding neural network (TWR-MCAE) data augmentation method. Specifically, the TWR-MCAE algorithm is jointly constructed by a singular value decomposition (SVD)-based data preprocessing module, an improved coordinate attention module, a compressed sensing learnable iterative shrinkage threshold reconstruction algorithm (LISTA) module, and an adaptive weight module. The data preprocessing module achieves wall clutter, human motion features, and noise subspaces separation. The improved coordinate attention module achieves clutter and noise suppression. The LISTA module achieves human motion feature enhancement. The adaptive weight module learns the weights and fuses the three subspaces. The TWR-MCAE can suppress the low-rank characteristics of wall clutter and enhance the sparsity characteristics in human motion at the same time. It can be linked before the classification step to improve the feature extraction capability without adding other prior knowledge or recollecting more data. Experiments show that the proposed algorithm gets a better peak signal-to-noise ratio (PSNR), which increases the recognition accuracy and speeds up the training process of the back-end classifiers.



### End-to-End 3D Dense Captioning with Vote2Cap-DETR
- **Arxiv ID**: http://arxiv.org/abs/2301.02508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02508v1)
- **Published**: 2023-01-06 13:46:45+00:00
- **Updated**: 2023-01-06 13:46:45+00:00
- **Authors**: Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Tao Chen, Gang YU
- **Comment**: None
- **Journal**: None
- **Summary**: 3D dense captioning aims to generate multiple captions localized with their associated object regions. Existing methods follow a sophisticated ``detect-then-describe'' pipeline equipped with numerous hand-crafted components. However, these hand-crafted components would yield suboptimal performance given cluttered object spatial and class distributions among different scenes. In this paper, we propose a simple-yet-effective transformer framework Vote2Cap-DETR based on recent popular \textbf{DE}tection \textbf{TR}ansformer (DETR). Compared with prior arts, our framework has several appealing advantages: 1) Without resorting to numerous hand-crafted components, our method is based on a full transformer encoder-decoder architecture with a learnable vote query driven object decoder, and a caption decoder that produces the dense captions in a set-prediction manner. 2) In contrast to the two-stage scheme, our method can perform detection and captioning in one-stage. 3) Without bells and whistles, extensive experiments on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our Vote2Cap-DETR surpasses current state-of-the-arts by 11.13\% and 7.11\% in CIDEr@0.5IoU, respectively. Codes will be released soon.



### Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.03396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03396v2)
- **Published**: 2023-01-06 14:16:54+00:00
- **Updated**: 2023-07-29 19:45:54+00:00
- **Authors**: Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zięba, Stavros Petridis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking human head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results on both of them.



### Tackling Data Bias in Painting Classification with Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2301.02524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02524v1)
- **Published**: 2023-01-06 14:33:53+00:00
- **Updated**: 2023-01-06 14:33:53+00:00
- **Authors**: Mridula Vijendran, Frederick W. B. Li, Hubert P. H. Shum
- **Comment**: International Conference on Computer Vision Theory and Applications
  (VISAPP), 2023 ,12 pages, 9 figures
- **Journal**: None
- **Summary**: It is difficult to train classifiers on paintings collections due to model bias from domain gaps and data bias from the uneven distribution of artistic styles. Previous techniques like data distillation, traditional data augmentation and style transfer improve classifier training using task specific training datasets or domain adaptation. We propose a system to handle data bias in small paintings datasets like the Kaokore dataset while simultaneously accounting for domain adaptation in fine-tuning a model trained on real world images. Our system consists of two stages which are style transfer and classification. In the style transfer stage, we generate the stylized training samples per class with uniformly sampled content and style images and train the style transformation network per domain. In the classification stage, we can interpret the effectiveness of the style and content layers at the attention layers when training on the original training dataset and the stylized images. We can tradeoff the model performance and convergence by dynamically varying the proportion of augmented samples in the majority and minority classes. We achieve comparable results to the SOTA with fewer training epochs and a classifier with fewer training parameters.



### In Defense of Structural Symbolic Representation for Video Event-Relation Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.03410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03410v2)
- **Published**: 2023-01-06 14:43:19+00:00
- **Updated**: 2023-04-12 15:19:16+00:00
- **Authors**: Andrew Lu, Xudong Lin, Yulei Niu, Shih-Fu Chang
- **Comment**: CVPRW 23, Learning with Limited Labelled Data
- **Journal**: None
- **Summary**: Understanding event relationships in videos requires a model to understand the underlying structures of events (i.e. the event type, the associated argument roles, and corresponding entities) and factual knowledge for reasoning. Structural symbolic representation (SSR) based methods directly take event types and associated argument roles/entities as inputs to perform reasoning. However, the state-of-the-art video event-relation prediction system shows the necessity of using continuous feature vectors from input videos; existing methods based solely on SSR inputs fail completely, even when given oracle event types and argument roles. In this paper, we conduct an extensive empirical analysis to answer the following questions: 1) why SSR-based method failed; 2) how to understand the evaluation setting of video event relation prediction properly; 3) how to uncover the potential of SSR-based methods. We first identify suboptimal training settings as causing the failure of previous SSR-based video event prediction models. Then through qualitative and quantitative analysis, we show how evaluation that takes only video as inputs is currently unfeasible, as well as the reliance on oracle event information to obtain an accurate evaluation. Based on these findings, we propose to further contextualize the SSR-based model to an Event-Sequence Model and equip it with more factual knowledge through a simple yet effective way of reformulating external visual commonsense knowledge bases into an event-relation prediction pretraining dataset. The resultant new state-of-the-art model eventually establishes a 25% Macro-accuracy performance boost.



### A CAD System for Colorectal Cancer from WSI: A Clinically Validated Interpretable ML-based Prototype
- **Arxiv ID**: http://arxiv.org/abs/2301.02608v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02608v1)
- **Published**: 2023-01-06 17:10:32+00:00
- **Updated**: 2023-01-06 17:10:32+00:00
- **Authors**: Pedro C. Neto, Diana Montezuma, Sara P. Oliveira, Domingos Oliveira, João Fraga, Ana Monteiro, João Monteiro, Liliana Ribeiro, Sofia Gonçalves, Stefan Reinhard, Inti Zlobec, Isabel M. Pinto, Jaime S. Cardoso
- **Comment**: Under Review
- **Journal**: None
- **Summary**: The integration of Artificial Intelligence (AI) and Digital Pathology has been increasing over the past years. Nowadays, applications of deep learning (DL) methods to diagnose cancer from whole-slide images (WSI) are, more than ever, a reality within different research groups. Nonetheless, the development of these systems was limited by a myriad of constraints regarding the lack of training samples, the scaling difficulties, the opaqueness of DL methods, and, more importantly, the lack of clinical validation. As such, we propose a system designed specifically for the diagnosis of colorectal samples. The construction of such a system consisted of four stages: (1) a careful data collection and annotation process, which resulted in one of the largest WSI colorectal samples datasets; (2) the design of an interpretable mixed-supervision scheme to leverage the domain knowledge introduced by pathologists through spatial annotations; (3) the development of an effective sampling approach based on the expected severeness of each tile, which decreased the computation cost by a factor of almost 6x; (4) the creation of a prototype that integrates the full set of features of the model to be evaluated in clinical practice. During these stages, the proposed method was evaluated in four separate test sets, two of them are external and completely independent. On the largest of those sets, the proposed approach achieved an accuracy of 93.44%. DL for colorectal samples is a few steps closer to stop being research exclusive and to become fully integrated in clinical practice.



### Triple-stream Deep Metric Learning of Great Ape Behavioural Actions
- **Arxiv ID**: http://arxiv.org/abs/2301.02642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.02642v1)
- **Published**: 2023-01-06 18:36:04+00:00
- **Updated**: 2023-01-06 18:36:04+00:00
- **Authors**: Otto Brookes, Majid Mirmehdi, Hjalmar Kühl, Tilo Burghardt
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first metric learning system for the recognition of great ape behavioural actions. Our proposed triple stream embedding architecture works on camera trap videos taken directly in the wild and demonstrates that the utilisation of an explicit DensePose-C chimpanzee body part segmentation stream effectively complements traditional RGB appearance and optical flow streams. We evaluate system variants with different feature fusion techniques and long-tail recognition approaches. Results and ablations show performance improvements of ~12% in top-1 accuracy over previous results achieved on the PanAf-500 dataset containing 180,000 manually annotated frames across nine behavioural actions. Furthermore, we provide a qualitative analysis of our findings and augment the metric learning system with long-tail recognition techniques showing that average per class accuracy -- critical in the domain -- can be improved by ~23% compared to the literature on that dataset. Finally, since our embedding spaces are constructed as metric, we provide first data-driven visualisations of the great ape behavioural action spaces revealing emerging geometry and topology. We hope that the work sparks further interest in this vital application area of computer vision for the benefit of endangered great apes.



### Model-Agnostic Hierarchical Attention for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.02650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02650v1)
- **Published**: 2023-01-06 18:52:12+00:00
- **Updated**: 2023-01-06 18:52:12+00:00
- **Authors**: Manli Shu, Le Xue, Ning Yu, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong, Ran Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers as versatile network architectures have recently seen great success in 3D point cloud object detection. However, the lack of hierarchy in a plain transformer makes it difficult to learn features at different scales and restrains its ability to extract localized features. Such limitation makes them have imbalanced performance on objects of different sizes, with inferior performance on smaller ones. In this work, we propose two novel attention mechanisms as modularized hierarchical designs for transformer-based 3D detectors. To enable feature learning at different scales, we propose Simple Multi-Scale Attention that builds multi-scale tokens from a single-scale input feature. For localized feature aggregation, we propose Size-Adaptive Local Attention with adaptive attention ranges for every bounding box proposal. Both of our attention modules are model-agnostic network layers that can be plugged into existing point cloud transformers for end-to-end training. We evaluate our method on two widely used indoor 3D point cloud object detection benchmarks. By plugging our proposed modules into the state-of-the-art transformer-based 3D detector, we improve the previous best results on both benchmarks, with the largest improvement margin on small objects.



### TarViS: A Unified Approach for Target-based Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.02657v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.6; I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2301.02657v2)
- **Published**: 2023-01-06 18:59:52+00:00
- **Updated**: 2023-05-10 16:40:04+00:00
- **Authors**: Ali Athar, Alexander Hermans, Jonathon Luiten, Deva Ramanan, Bastian Leibe
- **Comment**: Accepted to CVPR'23 (Highlight). Code is available at:
  https://github.com/Ali2500/TarViS
- **Journal**: None
- **Summary**: The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), Video Object Segmentation (VOS) and Point Exemplar-guided Tracking (PET). Our unified, jointly trained model achieves state-of-the-art performance on 5/7 benchmarks spanning these four tasks, and competitive performance on the remaining two. Code and model weights are available at: https://github.com/Ali2500/TarViS



### Design of Arabic Sign Language Recognition Model
- **Arxiv ID**: http://arxiv.org/abs/2301.02693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02693v1)
- **Published**: 2023-01-06 19:19:25+00:00
- **Updated**: 2023-01-06 19:19:25+00:00
- **Authors**: Muhammad Al-Barham, Ahmad Jamal, Musa Al-Yaman
- **Comment**: None
- **Journal**: None
- **Summary**: Deaf people are using sign language for communication, and it is a combination of gestures, movements, postures, and facial expressions that correspond to alphabets and words in spoken languages. The proposed Arabic sign language recognition model helps deaf and hard hearing people communicate effectively with ordinary people. The recognition has four stages of converting the alphabet into letters as follows: Image Loading stage, which loads the images of Arabic sign language alphabets that were used later to train and test the model, a pre-processing stage which applies image processing techniques such as normalization, Image augmentation, resizing, and filtering to extract the features which are necessary to accomplish the recognition perfectly, a training stage which is achieved by deep learning techniques like CNN, a testing stage which demonstrates how effectively the model performs for images did not see it before, and the model was built and tested mainly using PyTorch library. The model is tested on ArASL2018, consisting of 54,000 images for 32 alphabet signs gathered from 40 signers, and the dataset has two sets: training dataset and testing dataset. We had to ensure that the system is reliable in terms of accuracy, time, and flexibility of use explained in detail in this report. Finally, the future work will be a model that converts Arabic sign language into Arabic text.



### 3DAvatarGAN: Bridging Domains for Personalized Editable Avatars
- **Arxiv ID**: http://arxiv.org/abs/2301.02700v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2301.02700v2)
- **Published**: 2023-01-06 19:58:47+00:00
- **Updated**: 2023-03-26 11:26:54+00:00
- **Authors**: Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, Sergey Tulyakov
- **Comment**: Project Page: https://rameenabdal.github.io/3DAvatarGAN/
- **Journal**: None
- **Summary**: Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We then distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling -- as a byproduct -- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions -- for the first time -- allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.



### RUPNet: Residual upsampling network for real-time polyp segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.02703v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02703v2)
- **Published**: 2023-01-06 20:21:37+00:00
- **Updated**: 2023-04-18 22:15:16+00:00
- **Authors**: Nikhil Kumar Tomar, Ulas Bagci, Debesh Jha
- **Comment**: Accepted SPIE Medical Imaging 2023
- **Journal**: None
- **Summary**: Colorectal cancer is among the most prevalent cause of cancer-related mortality worldwide. Detection and removal of polyps at an early stage can help reduce mortality and even help in spreading over adjacent organs. Early polyp detection could save the lives of millions of patients over the world as well as reduce the clinical burden. However, the detection polyp rate varies significantly among endoscopists. There is numerous deep learning-based method proposed, however, most of the studies improve accuracy. Here, we propose a novel architecture, Residual Upsampling Network (RUPNet) for colon polyp segmentation that can process in real-time and show high recall and precision. The proposed architecture, RUPNet, is an encoder-decoder network that consists of three encoders, three decoder blocks, and some additional upsampling blocks at the end of the network. With an image size of $512 \times 512$, the proposed method achieves an excellent real-time operation speed of 152.60 frames per second with an average dice coefficient of 0.7658, mean intersection of union of 0.6553, sensitivity of 0.8049, precision of 0.7995, and F2-score of 0.9361. The results suggest that RUPNet can give real-time feedback while retaining high accuracy indicating a good benchmark for early polyp detection.



### Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation
- **Arxiv ID**: http://arxiv.org/abs/2301.02726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.02726v1)
- **Published**: 2023-01-06 22:04:47+00:00
- **Updated**: 2023-01-06 22:04:47+00:00
- **Authors**: Hilmil Pradana, Minh-Son Dao, Koji Zettsu
- **Comment**: 8 pages, conference
- **Journal**: International Conference on Digital Image Computing: Techniques
  and Applications (DICTA) 2022
- **Summary**: To develop the advanced self-driving systems, many researchers are focusing to alert all possible traffic risk cases from closed-circuit television (CCTV) and dashboard-mounted cameras. Most of these methods focused on identifying frame-by-frame in which an anomaly has occurred, but they are unrealized, which road traffic participant can cause ego-vehicle leading into collision because of available annotation dataset only to detect anomaly on traffic video. Near-miss is one type of accident and can be defined as a narrowly avoided accident. However, there is no difference between accident and near-miss at the time before the accident happened, so our contribution is to redefine the accident definition and re-annotate the accident inconsistency on DADA-2000 dataset together with near-miss. By extending the start and end time of accident duration, our annotation can precisely cover all ego-motions during an incident and consistently classify all possible traffic risk accidents including near-miss to give more critical information for real-world driving assistance systems. The proposed method integrates two different components: conditional style translation (CST) and separable 3-dimensional convolutional neural network (S3D). CST architecture is derived by unsupervised image-to-image translation networks (UNIT) used for augmenting the re-annotation DADA-2000 dataset to increase the number of traffic risk accident videos and to generalize the performance of video classification model on different types of conditions while S3D is useful for video classification to prove dataset re-annotation consistency. In evaluation, the proposed method achieved a significant improvement result by 10.25% positive margin from the baseline model for accuracy on cross-validation analysis.



### Designing an Improved Deep Learning-based Model for COVID-19 Recognition in Chest X-ray Images: A Knowledge Distillation Approach
- **Arxiv ID**: http://arxiv.org/abs/2301.02735v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.02735v1)
- **Published**: 2023-01-06 22:31:05+00:00
- **Updated**: 2023-01-06 22:31:05+00:00
- **Authors**: AmirReza BabaAhmadi, Sahar Khalafi, Masoud ShariatPanahi, Moosa Ayati
- **Comment**: 15 pages, 3 figures , 4 tables
- **Journal**: None
- **Summary**: COVID-19 has adversely affected humans and societies in different aspects. Numerous people have perished due to inaccurate COVID-19 identification and, consequently, a lack of appropriate medical treatment. Numerous solutions based on manual and automatic feature extraction techniques have been investigated to address this issue by researchers worldwide. Typically, automatic feature extraction methods, particularly deep learning models, necessitate a powerful hardware system to perform the necessary computations. Unfortunately, many institutions and societies cannot benefit from these advancements due to the prohibitively high cost of high-quality hardware equipment. As a result, this study focused on two primary goals: first, lowering the computational costs associated with running the proposed model on embedded devices, mobile devices, and conventional computers; and second, improving the model's performance in comparison to previously published methods (at least performs on par with state-of-the-art models) in order to ensure its performance and accuracy for the medical recognition task. This study used two neural networks to improve feature extraction from our dataset: VGG19 and ResNet50V2. Both of these networks are capable of providing semantic features from the nominated dataset. To this end, An alternative network was considered, namely MobileNetV2, which excels at extracting semantic features while requiring minimal computation on mobile and embedded devices. Knowledge distillation (KD) was used to transfer knowledge from the teacher network (concatenated ResNet50V2 and VGG19) to the student network (MobileNetV2) to improve MobileNetV2 performance and to achieve a robust and accurate model for the COVID-19 identification task from chest X-ray images.



