# Arxiv Papers in cs.CV on 2023-01-26
### Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing
- **Arxiv ID**: http://arxiv.org/abs/2301.10876v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10876v2)
- **Published**: 2023-01-26 00:03:09+00:00
- **Updated**: 2023-06-27 19:00:35+00:00
- **Authors**: Saharsh Barve, Jody M. Webster, Rohitash Chandra
- **Comment**: None
- **Journal**: Information, 2023
- **Summary**: Environmental damage has been of much concern, particularly in coastal areas and the oceans, given climate change and the drastic effects of pollution and extreme climate events. Our present-day analytical capabilities, along with advancements in information acquisition techniques such as remote sensing, can be utilised for the management and study of coral reef ecosystems. In this paper, we present Reef-Insight, an unsupervised machine learning framework that features advanced clustering methods and remote sensing for reef habitat mapping. Our framework compares different clustering methods for reef habitat mapping using remote sensing data. We evaluate four major clustering approaches based on qualitative and visual assessments which include k-means, hierarchical clustering, Gaussian mixture model, and density-based clustering. We utilise remote sensing data featuring the One Tree Island reef in Australia's Southern Great Barrier Reef. Our results indicate that clustering methods using remote sensing data can well identify benthic and geomorphic clusters in reefs when compared with other studies. Our results indicate that Reef-Insight can generate detailed reef habitat maps outlining distinct reef habitats and has the potential to enable further insights for reef restoration projects.



### The Projection-Enhancement Network (PEN)
- **Arxiv ID**: http://arxiv.org/abs/2301.10877v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2301.10877v1)
- **Published**: 2023-01-26 00:07:22+00:00
- **Updated**: 2023-01-26 00:07:22+00:00
- **Authors**: Christopher Z. Eddy, Austin Naylor, Bo Sun
- **Comment**: Main text: 14 pages, 5 figures; Supplementary text: 4 pages, 2
  figures
- **Journal**: None
- **Summary**: Contemporary approaches to instance segmentation in cell science use 2D or 3D convolutional networks depending on the experiment and data structures. However, limitations in microscopy systems or efforts to prevent phototoxicity commonly require recording sub-optimally sampled data regimes that greatly reduces the utility of such 3D data, especially in crowded environments with significant axial overlap between objects. In such regimes, 2D segmentations are both more reliable for cell morphology and easier to annotate. In this work, we propose the Projection Enhancement Network (PEN), a novel convolutional module which processes the sub-sampled 3D data and produces a 2D RGB semantic compression, and is trained in conjunction with an instance segmentation network of choice to produce 2D segmentations. Our approach combines augmentation to increase cell density using a low-density cell image dataset to train PEN, and curated datasets to evaluate PEN. We show that with PEN, the learned semantic representation in CellPose encodes depth and greatly improves segmentation performance in comparison to maximum intensity projection images as input, but does not similarly aid segmentation in region-based networks like Mask-RCNN. Finally, we dissect the segmentation strength against cell density of PEN with CellPose on disseminated cells from side-by-side spheroids. We present PEN as a data-driven solution to form compressed representations of 3D data that improve 2D segmentations from instance segmentation networks.



### Graph Contrastive Learning for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.10900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10900v2)
- **Published**: 2023-01-26 02:09:16+00:00
- **Updated**: 2023-06-10 10:32:06+00:00
- **Authors**: Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng
- **Comment**: Accepted by ICLR2023
- **Journal**: None
- **Summary**: In the field of skeleton-based action recognition, current top-performing graph convolutional networks (GCNs) exploit intra-sequence context to construct adaptive graphs for feature aggregation. However, we argue that such context is still \textit{local} since the rich cross-sequence relations have not been explicitly investigated. In this paper, we propose a graph contrastive learning framework for skeleton-based action recognition (\textit{SkeletonGCL}) to explore the \textit{global} context across all sequences. In specific, SkeletonGCL associates graph learning across sequences by enforcing graphs to be class-discriminative, \emph{i.e.,} intra-class compact and inter-class dispersed, which improves the GCN capacity to distinguish various action patterns. Besides, two memory banks are designed to enrich cross-sequence context from two complementary levels, \emph{i.e.,} instance and semantic levels, enabling graph contrastive learning in multiple context scales. Consequently, SkeletonGCL establishes a new training paradigm, and it can be seamlessly incorporated into current GCNs. Without loss of generality, we combine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), and achieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. The source code will be available at \url{https://github.com/OliverHxh/SkeletonGCL}.



### Facial Expression Recognition using Squeeze and Excitation-powered Swin Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.10906v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10906v7)
- **Published**: 2023-01-26 02:29:17+00:00
- **Updated**: 2023-04-29 01:02:43+00:00
- **Authors**: Arpita Vats, Aman Chadha
- **Comment**: arXiv admin note: text overlap with arXiv:2103.14030 by other authors
- **Journal**: None
- **Summary**: The ability to recognize and interpret facial emotions is a critical component of human communication, as it allows individuals to understand and respond to emotions conveyed through facial expressions and vocal tones. The recognition of facial emotions is a complex cognitive process that involves the integration of visual and auditory information, as well as prior knowledge and social cues. It plays a crucial role in social interaction, affective processing, and empathy, and is an important aspect of many real-world applications, including human-computer interaction, virtual assistants, and mental health diagnosis and treatment. The development of accurate and efficient models for facial emotion recognition is therefore of great importance and has the potential to have a significant impact on various fields of study.The field of Facial Emotion Recognition (FER) is of great significance in the areas of computer vision and artificial intelligence, with vast commercial and academic potential in fields such as security, advertising, and entertainment. We propose a FER framework that employs Swin Vision Transformers (SwinT) and squeeze and excitation block (SE) to address vision tasks. The approach uses a transformer model with an attention mechanism, SE, and SAM to improve the efficiency of the model, as transformers often require a large amount of data. Our focus was to create an efficient FER model based on SwinT architecture that can recognize facial emotions using minimal data. We trained our model on a hybrid dataset and evaluated its performance on the AffectNet dataset, achieving an F1-score of 0.5420, which surpassed the winner of the Affective Behavior Analysis in the Wild (ABAW) Competition held at the European Conference on Computer Vision (ECCV) 2022~\cite{Kollias}.



### Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.10908v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10908v3)
- **Published**: 2023-01-26 02:38:37+00:00
- **Updated**: 2023-07-03 01:18:19+00:00
- **Authors**: Hanxun Huang, Xingjun Ma, Sarah Erfani, James Bailey
- **Comment**: ICLR2023
- **Journal**: None
- **Summary**: This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the "minimal essence" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks. We also show that CD can potentially be applied to help detect potential biases from face datasets. Code is available at \url{https://github.com/HanxunH/CognitiveDistillation}.



### ITstyler: Image-optimized Text-based Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2301.10916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10916v1)
- **Published**: 2023-01-26 03:08:43+00:00
- **Updated**: 2023-01-26 03:08:43+00:00
- **Authors**: Yunpeng Bai, Jiayue Liu, Chao Dong, Chun Yuan
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: Text-based style transfer is a newly-emerging research topic that uses text information instead of style image to guide the transfer process, significantly extending the application scenario of style transfer. However, previous methods require extra time for optimization or text-image paired data, leading to limited effectiveness. In this work, we achieve a data-efficient text-based style transfer method that does not require optimization at the inference stage. Specifically, we convert text input to the style space of the pre-trained VGG network to realize a more effective style swap. We also leverage CLIP's multi-modal embedding space to learn the text-to-style mapping with the image dataset only. Our method can transfer arbitrary new styles of text input in real-time and synthesize high-quality artistic images.



### SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.10921v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10921v2)
- **Published**: 2023-01-26 03:53:25+00:00
- **Updated**: 2023-03-15 15:49:07+00:00
- **Authors**: Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, Marios Savvides
- **Comment**: Accepted by ICLR 2023
- **Journal**: None
- **Summary**: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.



### Detecting Building Changes with Off-Nadir Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2301.10922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10922v1)
- **Published**: 2023-01-26 04:04:14+00:00
- **Updated**: 2023-01-26 04:04:14+00:00
- **Authors**: Chao Pang, Jiang Wu, Jian Ding, Can Song, Gui-Song Xia
- **Comment**: None
- **Journal**: SCIENCE CHINA Information Sciences (SCIS) 2023
- **Summary**: The tilted viewing nature of the off-nadir aerial images brings severe challenges to the building change detection (BCD) problem: the mismatch of the nearby buildings and the semantic ambiguity of the building facades. To tackle these challenges, we present a multi-task guided change detection network model, named as MTGCD-Net. The proposed model approaches the specific BCD problem by designing three auxiliary tasks, including: (1) a pixel-wise classification task to predict the roofs and facades of buildings; (2) an auxiliary task for learning the roof-to-footprint offsets of each building to account for the misalignment between building roof instances; and (3) an auxiliary task for learning the identical roof matching flow between bi-temporal aerial images to tackle the building roof mismatch problem. These auxiliary tasks provide indispensable and complementary building parsing and matching information. The predictions of the auxiliary tasks are finally fused to the main building change detection branch with a multi-modal distillation module. To train and test models for the BCD problem with off-nadir aerial images, we create a new benchmark dataset, named BANDON. Extensive experiments demonstrate that our model achieves superior performance over the previous state-of-the-art competitors.



### Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.10931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10931v1)
- **Published**: 2023-01-26 04:32:00+00:00
- **Updated**: 2023-01-26 04:32:00+00:00
- **Authors**: Linfeng Xu, Qingbo Wu, Lili Pan, Fanman Meng, Hongliang Li, Chiyuan He, Hanxin Wang, Shaoxu Cheng, Yu Dai
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.



### Compact Transformer Tracker with Correlative Masked Modeling
- **Arxiv ID**: http://arxiv.org/abs/2301.10938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10938v1)
- **Published**: 2023-01-26 04:58:08+00:00
- **Updated**: 2023-01-26 04:58:08+00:00
- **Authors**: Zikai Song, Run Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang
- **Comment**: AAAI2023 oral
- **Journal**: None
- **Summary**: Transformer framework has been showing superior performances in visual object tracking for its great strength in information aggregation across the template and search image with the well-known attention mechanism. Most recent advances focus on exploring attention mechanism variants for better information aggregation. We find these schemes are equivalent to or even just a subset of the basic self-attention mechanism. In this paper, we prove that the vanilla self-attention structure is sufficient for information aggregation, and structural adaption is unnecessary. The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image. Based on this finding, we adopt the basic vision transformer (ViT) architecture as our main tracker and concatenate the template and search image for feature embedding. To guide the encoder to capture the invariant feature for tracking, we attach a lightweight correlative masked decoder which reconstructs the original template and search image from the corresponding masked tokens. The correlative masked decoder serves as a plugin for the compact transform tracker and is skipped in inference. Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps. Extensive experiments show the proposed compact transform tracker outperforms existing approaches, including advanced attention variants, and demonstrates the sufficiency of self-attention in tracking tasks. Our method achieves state-of-the-art performance on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/HUSTDML/CTTrack.



### Affective Faces for Goal-Driven Dyadic Communication
- **Arxiv ID**: http://arxiv.org/abs/2301.10939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10939v1)
- **Published**: 2023-01-26 05:00:09+00:00
- **Updated**: 2023-01-26 05:00:09+00:00
- **Authors**: Scott Geng, Revant Teotia, Purva Tendulkar, Sachit Menon, Carl Vondrick
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a video framework for modeling the association between verbal and non-verbal communication during dyadic conversation. Given the input speech of a speaker, our approach retrieves a video of a listener, who has facial expressions that would be socially appropriate given the context. Our approach further allows the listener to be conditioned on their own goals, personalities, or backgrounds. Our approach models conversations through a composition of large language models and vision-language models, creating internal representations that are interpretable and controllable. To study multimodal communication, we propose a new video dataset of unscripted conversations covering diverse topics and demographics. Experiments and visualizations show our approach is able to output listeners that are significantly more socially appropriate than baselines. However, many challenges remain, and we release our dataset publicly to spur further progress. See our website for video results, data, and code: https://realtalk.cs.columbia.edu.



### GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency
- **Arxiv ID**: http://arxiv.org/abs/2301.10941v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10941v3)
- **Published**: 2023-01-26 05:14:12+00:00
- **Updated**: 2023-04-27 05:34:01+00:00
- **Authors**: Min-seop Kwak, Jiuhn Song, Seungryong Kim
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: We present a novel framework to regularize Neural Radiance Field (NeRF) in a few-shot setting with a geometry-aware consistency regularization. The proposed approach leverages a rendered depth map at unobserved viewpoint to warp sparse input images to the unobserved viewpoint and impose them as pseudo ground truths to facilitate learning of NeRF. By encouraging such geometry-aware consistency at a feature-level instead of using pixel-level reconstruction loss, we regularize the NeRF at semantic and structural levels while allowing for modeling view dependent radiance to account for color variations across viewpoints. We also propose an effective method to filter out erroneous warped solutions, along with training strategies to stabilize training during optimization. We show that our model achieves competitive results compared to state-of-the-art few-shot NeRF models. Project page is available at https://ku-cvlab.github.io/GeCoNeRF/.



### Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images
- **Arxiv ID**: http://arxiv.org/abs/2301.10951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2301.10951v1)
- **Published**: 2023-01-26 06:02:28+00:00
- **Updated**: 2023-01-26 06:02:28+00:00
- **Authors**: Nathan Hadjiyski, Ali Vosoughi, Axel Wismueller
- **Comment**: Accepted to Computer-Aided Diagnosis, SPIE Medical Imaging 2023
- **Journal**: None
- **Summary**: Deep learning models can be applied successfully in real-work problems; however, training most of these models requires massive data. Recent methods use language and vision, but unfortunately, they rely on datasets that are not usually publicly available. Here we pave the way for further research in the multimodal language-vision domain for radiology. In this paper, we train a representation learning method that uses local and global representations of the language and vision through an attention mechanism and based on the publicly available Indiana University Radiology Report (IU-RR) dataset. Furthermore, we use the learned representations to diagnose five lung pathologies: atelectasis, cardiomegaly, edema, pleural effusion, and consolidation. Finally, we use both supervised and zero-shot classifications to extensively analyze the performance of the representation learning on the IU-RR dataset. Average Area Under the Curve (AUC) is used to evaluate the accuracy of the classifiers for classifying the five lung pathologies. The average AUC for classifying the five lung pathologies on the IU-RR test set ranged from 0.85 to 0.87 using the different training datasets, namely CheXpert and CheXphoto. These results compare favorably to other studies using UI-RR. Extensive experiments confirm consistent results for classifying lung pathologies using the multimodal global local representations of language and vision information.



### Neurorehab: An Interface for Rehabilitation
- **Arxiv ID**: http://arxiv.org/abs/2301.10957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10957v1)
- **Published**: 2023-01-26 06:28:45+00:00
- **Updated**: 2023-01-26 06:28:45+00:00
- **Authors**: Atul Dhingra, Adeboye A. Adejare Jr, Adam Fendler, Roopeswar Kommalapati
- **Comment**: None
- **Journal**: None
- **Summary**: About 15% of the world population is affected by a disability in some form, amongst whom only 31% perform the recommended exercises without intervention. We are working on developing a motivating and effective way to encourage people. In our work, we leverage the fact that repetitive exercises can help people with motor disabilities due to the robust plasticity of the pre-frontal cognitive control system in the brain. We investigate the role of repetitive activities for neurorehabilitation with the help of a brain computer interface, formulated using immersive game design with Kinect v2.0 and Unity 3D. We also introduce a game design paradigm for adaptive learning for the patients.



### Learning Large Scale Sparse Models
- **Arxiv ID**: http://arxiv.org/abs/2301.10958v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10958v1)
- **Published**: 2023-01-26 06:29:49+00:00
- **Updated**: 2023-01-26 06:29:49+00:00
- **Authors**: Atul Dhingra, Jie Shen, Nicholas Kleene
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we consider learning sparse models in large scale settings, where the number of samples and the feature dimension can grow as large as millions or billions. Two immediate issues occur under such challenging scenario: (i) computational cost; (ii) memory overhead. In particular, the memory issue precludes a large volume of prior algorithms that are based on batch optimization technique. To remedy the problem, we propose to learn sparse models such as Lasso in an online manner where in each iteration, only one randomly chosen sample is revealed to update a sparse iterate. Thereby, the memory cost is independent of the sample size and gradient evaluation for one sample is efficient. Perhaps amazingly, we find that with the same parameter, sparsity promoted by batch methods is not preserved in online fashion. We analyze such interesting phenomenon and illustrate some effective variants including mini-batch methods and a hard thresholding based stochastic gradient algorithm. Extensive experiments are carried out on a public dataset which supports our findings and algorithms.



### On the Importance of Noise Scheduling for Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.10972v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.10972v4)
- **Published**: 2023-01-26 07:37:22+00:00
- **Updated**: 2023-05-21 07:07:55+00:00
- **Authors**: Ting Chen
- **Comment**: tech report
- **Journal**: None
- **Summary**: We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\times$1024 resolution (without upsampling/cascades).



### Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization
- **Arxiv ID**: http://arxiv.org/abs/2301.11785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11785v1)
- **Published**: 2023-01-26 09:37:28+00:00
- **Updated**: 2023-01-26 09:37:28+00:00
- **Authors**: Shangrong Yang, Chunyu Lin, Kang Liao, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Fisheye image rectification has a long-term unresolved issue with synthetic-to-real generalization. In most previous works, the model trained on the synthetic images obtains unsatisfactory performance on the real-world fisheye image. To this end, we propose a Dual Diffusion Architecture (DDA) for the fisheye rectification with a better generalization ability. The proposed DDA is simultaneously trained with paired synthetic fisheye images and unlabeled real fisheye images. By gradually introducing noises, the synthetic and real fisheye images can eventually develop into a consistent noise distribution, improving the generalization and achieving unlabeled real fisheye correction. The original image serves as the prior guidance in existing DDPMs (Denoising Diffusion Probabilistic Models). However, the non-negligible indeterminate relationship between the prior condition and the target affects the generation performance. Especially in the rectification task, the radial distortion can cause significant artifacts. Therefore, we provide an unsupervised one-pass network that produces a plausible new condition to strengthen guidance. This network can be regarded as an alternate scheme for fast producing reliable results without iterative inference. Compared with the state-of-the-art methods, our approach can reach superior performance in both synthetic and real fisheye image corrections.



### Explore the Power of Dropout on Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.11015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11015v1)
- **Published**: 2023-01-26 10:10:27+00:00
- **Updated**: 2023-01-26 10:10:27+00:00
- **Authors**: Shaobo Lin, Xingyu Zeng, Rui Zhao
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2210.06409
- **Journal**: None
- **Summary**: The generalization power of the pre-trained model is the key for few-shot deep learning. Dropout is a regularization technique used in traditional deep learning methods. In this paper, we explore the power of dropout on few-shot learning and provide some insights about how to use it. Extensive experiments on the few-shot object detection and few-shot image classification datasets, i.e., Pascal VOC, MS COCO, CUB, and mini-ImageNet, validate the effectiveness of our method.



### Semantic Segmentation Enhanced Transformer Model for Human Attention Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.11022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11022v1)
- **Published**: 2023-01-26 10:27:51+00:00
- **Updated**: 2023-01-26 10:27:51+00:00
- **Authors**: Shuo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency Prediction aims to predict the attention distribution of human eyes given an RGB image. Most of the recent state-of-the-art methods are based on deep image feature representations from traditional CNNs. However, the traditional convolution could not capture the global features of the image well due to its small kernel size. Besides, the high-level factors which closely correlate to human visual perception, e.g., objects, color, light, etc., are not considered. Inspired by these, we propose a Transformer-based method with semantic segmentation as another learning objective. More global cues of the image could be captured by Transformer. In addition, simultaneously learning the object segmentation simulates the human visual perception, which we would verify in our investigation of human gaze control in cognitive science. We build an extra decoder for the subtask and the multiple tasks share the same Transformer encoder, forcing it to learn from multiple feature spaces. We find in practice simply adding the subtask might confuse the main task learning, hence Multi-task Attention Module is proposed to deal with the feature interaction between the multiple learning targets. Our method achieves competitive performance compared to other state-of-the-art methods.



### Understanding and contextualising diffusion models
- **Arxiv ID**: http://arxiv.org/abs/2302.01394v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2302.01394v2)
- **Published**: 2023-01-26 11:24:27+00:00
- **Updated**: 2023-02-10 11:36:38+00:00
- **Authors**: Stefano Scotta, Alberto Messina
- **Comment**: None
- **Journal**: None
- **Summary**: The latest developments in Artificial Intelligence include diffusion generative models, quite popular tools which can produce original images both unconditionally and, in some cases, conditioned by some inputs provided by the user. Apart from implementation details, which are outside the scope of this work, all of the main models used to generate images are substantially based on a common theory which restores a new image from a completely degraded one. In this work we explain how this is possible by focusing on the mathematical theory behind them, i.e. without analyzing in detail the specific implementations and related methods. The aim of this work is to clarify to the interested reader what all this means mathematically and intuitively.



### Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/2301.11063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11063v1)
- **Published**: 2023-01-26 12:32:01+00:00
- **Updated**: 2023-01-26 12:32:01+00:00
- **Authors**: Athul Shibu, Abhishek Kumar, Heechul Jung, Dong-Gyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have a large number of parameters and take significantly large hardware resources to compute, so edge devices struggle to run high-level networks. This paper proposes a novel method to reduce the parameters and FLOPs for computational efficiency in deep learning models. We introduce accuracy and efficiency coefficients to control the trade-off between the accuracy of the network and its computing efficiency. The proposed Rewarded meta-pruning algorithm trains a network to generate weights for a pruned model chosen based on the approximate parameters of the final model by controlling the interactions using a reward function. The reward function allows more control over the metrics of the final pruned model. Extensive experiments demonstrate superior performances of the proposed method over the state-of-the-art methods in pruning ResNet-50, MobileNetV1, and MobileNetV2 networks.



### Inspecting class hierarchies in classification-based metric learning models
- **Arxiv ID**: http://arxiv.org/abs/2301.11065v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.11065v1)
- **Published**: 2023-01-26 12:40:12+00:00
- **Updated**: 2023-01-26 12:40:12+00:00
- **Authors**: Hyeongji Kim, Pekka Parviainen, Terje Berge, Ketil Malde
- **Comment**: The main manuscript is 22 pages. The whole paper is 49 pages. The
  codes for our experiments will be available in
  https://github.com/hjk92g/Inspecting_Hierarchies_ML . The plankton datasets
  are available from the Norwegian Marine Data Center (MicroS:
  https://doi.org/10.21335/NMDC-2102309336 , MicroL:
  https://doi.org/10.21335/NMDC-573815973 , MesoZ:
  https://doi.org/10.21335/NMDC-1805578916 )
- **Journal**: None
- **Summary**: Most classification models treat all misclassifications equally. However, different classes may be related, and these hierarchical relationships must be considered in some classification problems. These problems can be addressed by using hierarchical information during training. Unfortunately, this information is not available for all datasets. Many classification-based metric learning methods use class representatives in embedding space to represent different classes. The relationships among the learned class representatives can then be used to estimate class hierarchical structures. If we have a predefined class hierarchy, the learned class representatives can be assessed to determine whether the metric learning model learned semantic distances that match our prior knowledge. In this work, we train a softmax classifier and three metric learning models with several training options on benchmark and real-world datasets. In addition to the standard classification accuracy, we evaluate the hierarchical inference performance by inspecting learned class representatives and the hierarchy-informed performance, i.e., the classification performance, and the metric learning performance by considering predefined hierarchical structures. Furthermore, we investigate how the considered measures are affected by various models and training options. When our proposed ProxyDR model is trained without using predefined hierarchical structures, the hierarchical inference performance is significantly better than that of the popular NormFace model. Additionally, our model enhances some hierarchy-informed performance measures under the same training options. We also found that convolutional neural networks (CNNs) with random weights correspond to the predefined hierarchies better than random chance.



### Invariant Meta Learning for Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2301.11779v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11779v2)
- **Published**: 2023-01-26 12:53:21+00:00
- **Updated**: 2023-02-21 16:46:29+00:00
- **Authors**: Penghao Jiang, Ke Xin, Zifeng Wang, Chunxi Li
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2022 The
  Ninth Workshop on Fine-Grained Visual Categorization
- **Journal**: None
- **Summary**: Modern deep learning techniques have illustrated their excellent capabilities in many areas, but relies on large training data. Optimization-based meta-learning train a model on a variety tasks, such that it can solve new learning tasks using only a small number of training samples.However, these methods assumes that training and test dataare identically and independently distributed. To overcome such limitation, in this paper, we propose invariant meta learning for out-of-distribution tasks. Specifically, invariant meta learning find invariant optimal meta-initialization,and fast adapt to out-of-distribution tasks with regularization penalty. Extensive experiments demonstrate the effectiveness of our proposed invariant meta learning on out-of-distribution few-shot tasks.



### simple diffusion: End-to-end diffusion for high resolution images
- **Arxiv ID**: http://arxiv.org/abs/2301.11093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.11093v1)
- **Published**: 2023-01-26 13:35:02+00:00
- **Updated**: 2023-01-26 13:35:02+00:00
- **Authors**: Emiel Hoogeboom, Jonathan Heek, Tim Salimans
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework.   This paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The paper is centered around the research question: How can one train a standard denoising diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches?   The four main findings are: 1) the noise schedule should be adjusted for high resolution images, 2) It is sufficient to scale only a particular part of the architecture, 3) dropout should be added at specific locations in the architecture, and 4) downsampling is an effective strategy to avoid high resolution feature maps. Combining these simple yet effective techniques, we achieve state-of-the-art on image generation among diffusion models without sampling modifiers on ImageNet.



### Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities
- **Arxiv ID**: http://arxiv.org/abs/2301.11100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2301.11100v1)
- **Published**: 2023-01-26 13:44:31+00:00
- **Updated**: 2023-01-26 13:44:31+00:00
- **Authors**: Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan Misra, Candace Ross
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image and that aggregating analyses over all concepts can mask these concerns; (ii) model calibration (i.e. the relationship between accuracy and confidence) also differs distinctly by perceived gender, even when evaluating on similar representations of concepts; and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can also contribute to social biases in zero-shot vision settings. Furthermore, biases can further propagate when foundational models like CLIP are used by other models to enable zero-shot capabilities.



### Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2301.11104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11104v3)
- **Published**: 2023-01-26 13:58:46+00:00
- **Updated**: 2023-05-24 11:06:31+00:00
- **Authors**: Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. To address this, we introduce the bias-to-text (B2T) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. Our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. To achieve this, we analyze common keywords in the captions of mispredicted or generated images. Here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. Additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the B2T framework. We demonstrate the effectiveness of our framework on various image classification and generation tasks. For classifiers, we discover a new spurious correlation between the keywords "(sports) player" and "female" in Kaggle Face and improve the worst-group accuracy on Waterbirds by 11% through debiasing, compared to the baseline. For generative models, we detect and effectively prevent unfair (e.g., gender-biased) and unsafe (e.g., "naked") image generation.



### Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring
- **Arxiv ID**: http://arxiv.org/abs/2301.11116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.11116v1)
- **Published**: 2023-01-26 14:12:02+00:00
- **Updated**: 2023-01-26 14:12:02+00:00
- **Authors**: Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, Thomas H. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image-text pretrained models, e.g., CLIP, have shown impressive general multi-modal knowledge learned from large-scale image-text data pairs, thus attracting increasing attention for their potential to improve visual representation learning in the video domain. In this paper, based on the CLIP model, we revisit temporal modeling in the context of image-to-video knowledge transferring, which is the key point for extending image-text pretrained models to the video domain. We find that current temporal modeling mechanisms are tailored to either high-level semantic-dominant tasks (e.g., retrieval) or low-level visual pattern-dominant tasks (e.g., recognition), and fail to work on the two cases simultaneously. The key difficulty lies in modeling temporal dependency while taking advantage of both high-level and low-level knowledge in CLIP model. To tackle this problem, we present Spatial-Temporal Auxiliary Network (STAN) -- a simple and effective temporal modeling mechanism extending CLIP model to diverse video tasks. Specifically, to realize both low-level and high-level knowledge transferring, STAN adopts a branch structure with decomposed spatial-temporal modules that enable multi-level CLIP features to be spatial-temporally contextualized. We evaluate our method on two representative video tasks: Video-Text Retrieval and Video Recognition. Extensive experiments demonstrate the superiority of our model over the state-of-the-art methods on various datasets, including MSR-VTT, DiDeMo, LSMDC, MSVD, Kinetics-400, and Something-Something-V2. Codes will be available at https://github.com/farewellthree/STAN



### Learning from Mistakes: Self-Regularizing Hierarchical Semantic Representations in Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.11145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.11145v1)
- **Published**: 2023-01-26 14:52:30+00:00
- **Updated**: 2023-01-26 14:52:30+00:00
- **Authors**: Elena Camuffo, Umberto Michieli, Simone Milani
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in autonomous robotic technologies have highlighted the growing need for precise environmental analysis. LiDAR semantic segmentation has gained attention to accomplish fine-grained scene understanding by acting directly on raw content provided by sensors. Recent solutions showed how different learning techniques can be used to improve the performance of the model, without any architectural or dataset change. Following this trend, we present a coarse-to-fine setup that LEArns from classification mistaKes (LEAK) derived from a standard model. First, classes are clustered into macro groups according to mutual prediction errors; then, the learning process is regularized by: (1) aligning class-conditional prototypical feature representation for both fine and coarse classes, (2) weighting instances with a per-class fairness index. Our LEAK approach is very general and can be seamlessly applied on top of any segmentation architecture; indeed, experimental results showed that it enables state-of-the-art performances on different architectures, datasets and tasks, while ensuring more balanced class-wise results and faster convergence.



### Multitemporal and multispectral data fusion for super-resolution of Sentinel-2 images
- **Arxiv ID**: http://arxiv.org/abs/2301.11154v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11154v1)
- **Published**: 2023-01-26 15:01:25+00:00
- **Updated**: 2023-01-26 15:01:25+00:00
- **Authors**: Tomasz Tarasiewicz, Jakub Nalepa, Reuben A. Farrugia, Gianluca Valentino, Mang Chen, Johann A. Briffa, Michal Kawulok
- **Comment**: Submitted to IEEE Transactions On Geoscience And Remote Sensing
- **Journal**: None
- **Summary**: Multispectral Sentinel-2 images are a valuable source of Earth observation data, however spatial resolution of their spectral bands limited to 10 m, 20 m, and 60 m ground sampling distance remains insufficient in many cases. This problem can be addressed with super-resolution, aimed at reconstructing a high-resolution image from a low-resolution observation. For Sentinel-2, spectral information fusion allows for enhancing the 20 m and 60 m bands to the 10 m resolution. Also, there were attempts to combine multitemporal stacks of individual Sentinel-2 bands, however these two approaches have not been combined so far. In this paper, we introduce DeepSent -- a new deep network for super-resolving multitemporal series of multispectral Sentinel-2 images. It is underpinned with information fusion performed simultaneously in the spectral and temporal dimensions to generate an enlarged multispectral image. In our extensive experimental study, we demonstrate that our solution outperforms other state-of-the-art techniques that realize either multitemporal or multispectral data fusion. Furthermore, we show that the advantage of DeepSent results from how these two fusion types are combined in a single architecture, which is superior to performing such fusion in a sequential manner. Importantly, we have applied our method to super-resolve real-world Sentinel-2 images, enhancing the spatial resolution of all the spectral bands to 3.3 m nominal ground sampling distance, and we compare the outcome with very high-resolution WorldView-2 images. We will publish our implementation upon paper acceptance, and we expect it will increase the possibilities of exploiting super-resolved Sentinel-2 images in real-life applications.



### Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2301.11174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11174v1)
- **Published**: 2023-01-26 15:25:43+00:00
- **Updated**: 2023-01-26 15:25:43+00:00
- **Authors**: Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon
- **Comment**: Journal extension of our EMNLP 2019 paper (arXiv:1909.02201)
- **Journal**: None
- **Summary**: We present a novel data-efficient semi-supervised framework to improve the generalization of image captioning models. Constructing a large-scale labeled image captioning dataset is an expensive task in terms of labor, time, and cost. In contrast to manually annotating all the training samples, separately collecting uni-modal datasets is immensely easier, e.g., a large-scale image dataset and a sentence dataset. We leverage such massive unpaired image and caption data upon standard paired data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples in an adversarial learning fashion, where the joint distribution of image and caption is learned. Our method trains a captioner to learn from a paired data and to progressively associate unpaired data. This approach shows noticeable performance improvement even in challenging scenarios including out-of-task data (i.e., relational captioning, where the target task is different from the unpaired data) and web-crawled data. We also show that our proposed method is theoretically well-motivated and has a favorable global optimal property. Our extensive and comprehensive empirical results both on (1) image-based and (2) dense region-based captioning datasets followed by comprehensive analysis on the scarcely-paired COCO dataset demonstrate the consistent effectiveness of our semisupervised learning method with unpaired data compared to competing methods.



### Low-Rank Winograd Transformation for 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.11180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11180v1)
- **Published**: 2023-01-26 15:44:22+00:00
- **Updated**: 2023-01-26 15:44:22+00:00
- **Authors**: Ziran Qin, Mingbao Lin, Weiyao Lin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on Winograd transformation in 3D convolutional neural networks (CNNs) that are more over-parameterized compared with the 2D version. The over-increasing Winograd parameters not only exacerbate training complexity but also barricade the practical speedups due simply to the volume of element-wise products in the Winograd domain. We attempt to reduce trainable parameters by introducing a low-rank Winograd transformation, a novel training paradigm that decouples the original large tensor into two less storage-required trainable tensors, leading to a significant complexity reduction. Built upon our low-rank Winograd transformation, we take one step ahead by proposing a low-rank oriented sparse granularity that measures column-wise parameter importance. By simply involving the non-zero columns in the element-wise product, our sparse granularity is empowered with the ability to produce a very regular sparse pattern to acquire effectual Winograd speedups. To better understand the efficacy of our method, we perform extensive experiments on 3D CNNs. Results manifest that our low-rank Winograd transformation well outperforms the vanilla Winograd transformation. We also show that our proposed low-rank oriented sparse granularity permits practical Winograd acceleration compared with the vanilla counterpart.



### Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models
- **Arxiv ID**: http://arxiv.org/abs/2301.11189v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2301.11189v3)
- **Published**: 2023-01-26 15:55:43+00:00
- **Updated**: 2023-08-11 02:21:27+00:00
- **Authors**: Matthew J. Muckley, Alaaeldin El-Nouby, Karen Ullrich, Hervé Jégou, Jakob Verbeek
- **Comment**: Upload camera-ready to arXiv. Official version available at
  https://proceedings.mlr.press/v202/muckley23a.html
- **Journal**: Proceedings of the 40th International Conference on Machine
  Learning (2023) 25426-25443
- **Summary**: Lossy image compression aims to represent images in as few bits as possible while maintaining fidelity to the original. Theoretical results indicate that optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a discrepancy in the statistics of original images from those of reconstructions, in particular at low bitrates, often manifested by the blurring of the compressed images. Previous work has leveraged adversarial discriminators to improve statistical fidelity. Yet these binary discriminators adopted from generative modeling tasks may not be ideal for image compression. In this paper, we introduce a non-binary discriminator that is conditioned on quantized local image representations obtained via VQ-VAE autoencoders. Our evaluations on the CLIC2020, DIV2K and Kodak datasets show that our discriminator is more effective for jointly optimizing distortion (e.g., PSNR) and statistical fidelity (e.g., FID) than the PatchGAN of the state-of-the-art HiFiC model. On CLIC2020, we obtain the same FID as HiFiC with 30-40\% fewer bits.



### I-24 MOTION: An instrument for freeway traffic science
- **Arxiv ID**: http://arxiv.org/abs/2301.11198v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11198v2)
- **Published**: 2023-01-26 16:16:20+00:00
- **Updated**: 2023-01-30 17:25:30+00:00
- **Authors**: Derek Gloudemans, Yanbing Wang, Junyi Ji, Gergely Zachar, Will Barbour, Daniel B. Work
- **Comment**: None
- **Journal**: None
- **Summary**: The Interstate-24 MObility Technology Interstate Observation Network (I-24 MOTION) is a new instrument for traffic science located near Nashville, Tennessee. I-24 MOTION consists of 276 pole-mounted high-resolution traffic cameras that provide seamless coverage of approximately 4.2 miles I-24, a 4-5 lane (each direction) freeway with frequently observed congestion. The cameras are connected via fiber optic network to a compute facility where vehicle trajectories are extracted from the video imagery using computer vision techniques. Approximately 230 million vehicle miles of travel occur within I-24 MOTION annually. The main output of the instrument are vehicle trajectory datasets that contain the position of each vehicle on the freeway, as well as other supplementary information vehicle dimensions and class. This article describes the design and creation of the instrument, and provides the first publicly available datasets generated from the instrument. The datasets published with this article contains at least 4 hours of vehicle trajectory data for each of 10 days. As the system continues to mature, all trajectory data will be made publicly available at i24motion.org/data.



### Relative-Interior Solution for (Incomplete) Linear Assignment Problem with Applications to Quadratic Assignment Problem
- **Arxiv ID**: http://arxiv.org/abs/2301.11201v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11201v1)
- **Published**: 2023-01-26 16:22:01+00:00
- **Updated**: 2023-01-26 16:22:01+00:00
- **Authors**: Tomáš Dlask, Bogdan Savchynskyy
- **Comment**: None
- **Journal**: None
- **Summary**: We study the set of optimal solutions of the dual linear programming formulation of the linear assignment problem (LAP) to propose a method for computing a solution from the relative interior of this set. Assuming that an arbitrary dual-optimal solution and an optimal assignment are available (for which many efficient algorithms already exist), our method computes a relative-interior solution in linear time. Since LAP occurs as a subproblem in the linear programming relaxation of quadratic assignment problem (QAP), we employ our method as a new component in the family of dual-ascent algorithms that provide bounds on the optimal value of QAP. To make our results applicable to incomplete QAP, which is of interest in practical use-cases, we also provide a linear-time reduction from incomplete LAP to complete LAP along with a mapping that preserves optimality and membership in the relative interior. Our experiments on publicly available benchmarks indicate that our approach with relative-interior solution is frequently capable of providing superior bounds and otherwise is at least comparable.



### BiBench: Benchmarking and Analyzing Network Binarization
- **Arxiv ID**: http://arxiv.org/abs/2301.11233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11233v2)
- **Published**: 2023-01-26 17:17:16+00:00
- **Updated**: 2023-05-20 11:04:19+00:00
- **Authors**: Haotong Qin, Mingyuan Zhang, Yifu Ding, Aoyu Li, Zhongang Cai, Ziwei Liu, Fisher Yu, Xianglong Liu
- **Comment**: None
- **Journal**: 2023 International Conference on Machine Learning
- **Summary**: Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released https://github.com/htqin/BiBench .



### Self-Supervised RGB-T Tracking with Cross-Input Consistency
- **Arxiv ID**: http://arxiv.org/abs/2301.11274v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.11274v1)
- **Published**: 2023-01-26 18:11:16+00:00
- **Updated**: 2023-01-26 18:11:16+00:00
- **Authors**: Xingchen Zhang, Yiannis Demiris
- **Comment**: 12 pages,9 figures
- **Journal**: None
- **Summary**: In this paper, we propose a self-supervised RGB-T tracking method. Different from existing deep RGB-T trackers that use a large number of annotated RGB-T image pairs for training, our RGB-T tracker is trained using unlabeled RGB-T video pairs in a self-supervised manner. We propose a novel cross-input consistency-based self-supervised training strategy based on the idea that tracking can be performed using different inputs. Specifically, we construct two distinct inputs using unlabeled RGB-T video pairs. We then track objects using these two inputs to generate results, based on which we construct our cross-input consistency loss. Meanwhile, we propose a reweighting strategy to make our loss function robust to low-quality training samples. We build our tracker on a Siamese correlation filter network. To the best of our knowledge, our tracker is the first self-supervised RGB-T tracker. Extensive experiments on two public RGB-T tracking benchmarks demonstrate that the proposed training strategy is effective. Remarkably, despite training only with a corpus of unlabeled RGB-T video pairs, our tracker outperforms seven supervised RGB-T trackers on the GTOT dataset.



### Text-To-4D Dynamic Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.11280v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11280v1)
- **Published**: 2023-01-26 18:14:32+00:00
- **Updated**: 2023-01-26 18:14:32+00:00
- **Authors**: Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman
- **Comment**: None
- **Journal**: None
- **Summary**: We present MAV3D (Make-A-Video3D), a method for generating three-dimensional dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural Radiance Field (NeRF), which is optimized for scene appearance, density, and motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The dynamic video output generated from the provided text can be viewed from any camera location and angle, and can be composited into any 3D environment. MAV3D does not require any 3D or 4D data and the T2V model is trained only on Text-Image pairs and unlabeled videos. We demonstrate the effectiveness of our approach using comprehensive quantitative and qualitative experiments and show an improvement over previously established internal baselines. To the best of our knowledge, our method is the first to generate 3D dynamic scenes given a text description.



### Learning Good Features to Transfer Across Tasks and Domains
- **Arxiv ID**: http://arxiv.org/abs/2301.11310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11310v1)
- **Published**: 2023-01-26 18:49:39+00:00
- **Updated**: 2023-01-26 18:49:39+00:00
- **Authors**: Pierluigi Zama Ramirez, Adriano Cardace, Luca De Luigi, Alessio Tonioni, Samuele Salti, Luigi Di Stefano
- **Comment**: Extended version of the paper "Learning Across Tasks and Domains"
  presented at ICCV 2019. Accepted at TPAMI
- **Journal**: None
- **Summary**: Availability of labelled data is the major obstacle to the deployment of deep learning algorithms for computer vision tasks in new domains. The fact that many frameworks adopted to solve different tasks share the same architecture suggests that there should be a way of reusing the knowledge learned in a specific setting to solve novel tasks with limited or no additional supervision. In this work, we first show that such knowledge can be shared across tasks by learning a mapping between task-specific deep features in a given domain. Then, we show that this mapping function, implemented by a neural network, is able to generalize to novel unseen domains. Besides, we propose a set of strategies to constrain the learned feature spaces, to ease learning and increase the generalization capability of the mapping network, thereby considerably improving the final performance of our framework. Our proposal obtains compelling results in challenging synthetic-to-real adaptation scenarios by transferring knowledge between monocular depth estimation and semantic segmentation tasks.



### Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served patient populations
- **Arxiv ID**: http://arxiv.org/abs/2301.11315v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.11315v2)
- **Published**: 2023-01-26 18:53:09+00:00
- **Updated**: 2023-01-29 14:38:09+00:00
- **Authors**: Mingquan Lin, Yuyun Xiao, Bojian Hou, Tingyi Wanyan, Mohit Manoj Sharma, Zhangyang Wang, Fei Wang, Sarah Van Tassel, Yifan Peng
- **Comment**: 9 pages, 2 figures, Accepted by AMIA 2023 Informatics Summit
- **Journal**: AMIA 2023 Informatics Summit
- **Summary**: In the United States, primary open-angle glaucoma (POAG) is the leading cause of blindness, especially among African American and Hispanic individuals. Deep learning has been widely used to detect POAG using fundus images as its performance is comparable to or even surpasses diagnosis by clinicians. However, human bias in clinical diagnosis may be reflected and amplified in the widely-used deep learning models, thus impacting their performance. Biases may cause (1) underdiagnosis, increasing the risks of delayed or inadequate treatment, and (2) overdiagnosis, which may increase individuals' stress, fear, well-being, and unnecessary/costly treatment. In this study, we examined the underdiagnosis and overdiagnosis when applying deep learning in POAG detection based on the Ocular Hypertension Treatment Study (OHTS) from 22 centers across 16 states in the United States. Our results show that the widely-used deep learning model can underdiagnose or overdiagnose underserved populations. The most underdiagnosed group is female younger (< 60 yrs) group, and the most overdiagnosed group is Black older (>=60 yrs) group. Biased diagnosis through traditional deep learning methods may delay disease detection, treatment and create burdens among under-served populations, thereby, raising ethical concerns about using deep learning models in ophthalmology clinics.



### Open Problems in Applied Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.11316v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.HC, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2301.11316v1)
- **Published**: 2023-01-26 18:55:43+00:00
- **Updated**: 2023-01-26 18:55:43+00:00
- **Authors**: Maziar Raissi
- **Comment**: None
- **Journal**: None
- **Summary**: This work formulates the machine learning mechanism as a bi-level optimization problem. The inner level optimization loop entails minimizing a properly chosen loss function evaluated on the training data. This is nothing but the well-studied training process in pursuit of optimal model parameters. The outer level optimization loop is less well-studied and involves maximizing a properly chosen performance metric evaluated on the validation data. This is what we call the "iteration process", pursuing optimal model hyper-parameters. Among many other degrees of freedom, this process entails model engineering (e.g., neural network architecture design) and management, experiment tracking, dataset versioning and augmentation. The iteration process could be automated via Automatic Machine Learning (AutoML) or left to the intuitions of machine learning students, engineers, and researchers. Regardless of the route we take, there is a need to reduce the computational cost of the iteration step and as a direct consequence reduce the carbon footprint of developing artificial intelligence algorithms. Despite the clean and unified mathematical formulation of the iteration step as a bi-level optimization problem, its solutions are case specific and complex. This work will consider such cases while increasing the level of complexity from supervised learning to semi-supervised, self-supervised, unsupervised, few-shot, federated, reinforcement, and physics-informed learning. As a consequence of this exercise, this proposal surfaces a plethora of open problems in the field, many of which can be addressed in parallel.



### Cut and Learn for Unsupervised Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.11320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11320v1)
- **Published**: 2023-01-26 18:57:13+00:00
- **Updated**: 2023-01-26 18:57:13+00:00
- **Authors**: Xudong Wang, Rohit Girdhar, Stella X. Yu, Ishan Misra
- **Comment**: Tech report. Project page:
  http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/. Code is available
  at https://github.com/facebookresearch/CutLER
- **Journal**: None
- **Summary**: We propose Cut-and-LEaRn (CutLER), a simple approach for training unsupervised object detection and segmentation models. We leverage the property of self-supervised models to 'discover' objects without supervision and amplify it to train a state-of-the-art localization model without any human labels. CutLER first uses our proposed MaskCut approach to generate coarse masks for multiple objects in an image and then learns a detector on these masks using our robust loss function. We further improve the performance by self-training the model on its predictions. Compared to prior work, CutLER is simpler, compatible with different detection architectures, and detects multiple objects. CutLER is also a zero-shot unsupervised detector and improves detection performance AP50 by over 2.7 times on 11 benchmarks across domains like video frames, paintings, sketches, etc. With finetuning, CutLER serves as a low-shot detector surpassing MoCo-v2 by 7.3% APbox and 6.6% APmask on COCO when training with 5% labels.



### Unsupervised Volumetric Animation
- **Arxiv ID**: http://arxiv.org/abs/2301.11326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11326v1)
- **Published**: 2023-01-26 18:58:54+00:00
- **Updated**: 2023-01-26 18:58:54+00:00
- **Authors**: Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Kyle Olszewski, Jian Ren, Hsin-Ying Lee, Menglei Chai, Sergey Tulyakov
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for unsupervised 3D animation of non-rigid deformable objects. Our method learns the 3D structure and dynamics of objects solely from single-view RGB videos, and can decompose them into semantically meaningful parts that can be tracked and animated. Using a 3D autodecoder framework, paired with a keypoint estimator via a differentiable PnP algorithm, our model learns the underlying object geometry and parts decomposition in an entirely unsupervised manner. This allows it to perform 3D segmentation, 3D keypoint estimation, novel view synthesis, and animation. We primarily evaluate the framework on two video datasets: VoxCeleb $256^2$ and TEDXPeople $256^2$. In addition, on the Cats $256^2$ image dataset, we show it even learns compelling 3D geometry from still images. Finally, we show our model can obtain animatable 3D objects from a single or few images. Code and visual results available on our project website, see https://snap-research.github.io/unsupervised-volumetric-animation .



### Anatomy-aware and acquisition-agnostic joint registration with SynthMorph
- **Arxiv ID**: http://arxiv.org/abs/2301.11329v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11329v1)
- **Published**: 2023-01-26 18:59:33+00:00
- **Updated**: 2023-01-26 18:59:33+00:00
- **Authors**: Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca
- **Comment**: 24 pages, 14 figures, 4 tables, affine registration, deformable
  registration, deep learning, domain shift, magnetic resonance imaging
- **Journal**: None
- **Summary**: Affine image registration is a cornerstone of medical-image processing and analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every new image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the functions is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as the contrast or resolution. A majority of affine methods are also agnostic to the anatomy the user wishes to align; the registration will be inaccurate if algorithms consider all structures in the image. We address these shortcomings with a fast, robust, and easy-to-use DL tool for affine and deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we rigorously analyze how competing architectures learn affine transforms across a diverse set of neuroimaging data, aiming to truly capture the behavior of methods in the real world. Second, we leverage a recent strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics. Third, we optimize the spatial overlap of select anatomical labels, which enables networks to distinguish between anatomy of interest and irrelevant structures, removing the need for preprocessing that excludes content that would otherwise reduce the accuracy of anatomy-specific registration. We combine the affine model with prior work on deformable registration and test brain-specific registration across a landscape of MRI protocols unseen at training, demonstrating consistent and improved accuracy compared to existing tools. We distribute our code and tool at https://w3id.org/synthmorph, providing a single complete end-to-end solution for registration of brain MRI.



### Multimodal Event Transformer for Image-guided Story Ending Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.11357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2301.11357v1)
- **Published**: 2023-01-26 19:10:07+00:00
- **Updated**: 2023-01-26 19:10:07+00:00
- **Authors**: Yucheng Zhou, Guodong Long
- **Comment**: EACL 2023
- **Journal**: None
- **Summary**: Image-guided story ending generation (IgSEG) is to generate a story ending based on given story plots and ending image. Existing methods focus on cross-modal feature fusion but overlook reasoning and mining implicit information from story plots and ending image. To tackle this drawback, we propose a multimodal event transformer, an event-based reasoning framework for IgSEG. Specifically, we construct visual and semantic event graphs from story plots and ending image, and leverage event-based reasoning to reason and mine implicit information in a single modality. Next, we connect visual and semantic event graphs and utilize cross-modal fusion to integrate different-modality features. In addition, we propose a multimodal injector to adaptive pass essential information to decoder. Besides, we present an incoherence detection to enhance the understanding context of a story plot and the robustness of graph modeling for our model. Experimental results show that our method achieves state-of-the-art performance for the image-guided story ending generation.



### The Power of Linear Combinations: Learning with Random Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2301.11360v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.11360v2)
- **Published**: 2023-01-26 19:17:10+00:00
- **Updated**: 2023-06-21 19:56:14+00:00
- **Authors**: Paul Gavrikov, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall performance and robustness. Conversely, retaining the ability to learn filter updates can impair network performance. Lastly, although we only observe relatively small gains from learning $3\times 3$ convolutions, the learning gains increase proportionally with kernel size, owing to the non-idealities of the independent and identically distributed (\textit{i.i.d.}) nature of default initialization techniques.



### Improving Cross-modal Alignment for Text-Guided Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2301.11362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2301.11362v1)
- **Published**: 2023-01-26 19:18:27+00:00
- **Updated**: 2023-01-26 19:18:27+00:00
- **Authors**: Yucheng Zhou, Guodong Long
- **Comment**: EACL 2023
- **Journal**: None
- **Summary**: Text-guided image inpainting (TGII) aims to restore missing regions based on a given text in a damaged image. Existing methods are based on a strong vision encoder and a cross-modal fusion model to integrate cross-modal features. However, these methods allocate most of the computation to visual encoding, while light computation on modeling modality interactions. Moreover, they take cross-modal fusion for depth features, which ignores a fine-grained alignment between text and image. Recently, vision-language pre-trained models (VLPM), encapsulating rich cross-modal alignment knowledge, have advanced in most multimodal tasks. In this work, we propose a novel model for TGII by improving cross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language encoder, an image generator and global-local discriminators. To explore cross-modal alignment knowledge for image restoration, we introduce cross-modal alignment distillation and in-sample distribution distillation. In addition, we employ adversarial training to enhance the model to fill the missing region in complicated structures effectively. Experiments are conducted on two popular vision-language datasets. Results show that our model achieves state-of-the-art performance compared with other strong competitors.



### Style-Aware Contrastive Learning for Multi-Style Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2301.11367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2301.11367v1)
- **Published**: 2023-01-26 19:21:39+00:00
- **Updated**: 2023-01-26 19:21:39+00:00
- **Authors**: Yucheng Zhou, Guodong Long
- **Comment**: Findings of EACL 2023
- **Journal**: None
- **Summary**: Existing multi-style image captioning methods show promising results in generating a caption with accurate visual content and desired linguistic style. However, existing methods overlook the relationship between linguistic style and visual content. To overcome this drawback, we propose style-aware contrastive learning for multi-style image captioning. First, we present a style-aware visual encoder with contrastive learning to mine potential visual content relevant to style. Moreover, we propose a style-aware triplet contrast objective to distinguish whether the image, style and caption matched. To provide positive and negative samples for contrastive learning, we present three retrieval schemes: object-based retrieval, RoI-based retrieval and triplet-based retrieval, and design a dynamic trade-off function to calculate retrieval scores. Experimental results demonstrate that our approach achieves state-of-the-art performance. In addition, we conduct an extensive analysis to verify the effectiveness of our method.



### Universal Domain Adaptation for Remote Sensing Image Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.11387v1
- **DOI**: 10.1109/TGRS.2023.3235988
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11387v1)
- **Published**: 2023-01-26 20:04:24+00:00
- **Updated**: 2023-01-26 20:04:24+00:00
- **Authors**: Qingsong Xu, Yilei Shi, Xin Yuan, Xiao Xiang Zhu
- **Comment**: 15 pages, 6 figures, IEEE Transactions on Geoscience and Remote
  Sensing
- **Journal**: None
- **Summary**: The domain adaptation (DA) approaches available to date are usually not well suited for practical DA scenarios of remote sensing image classification, since these methods (such as unsupervised DA) rely on rich prior knowledge about the relationship between label sets of source and target domains, and source data are often not accessible due to privacy or confidentiality issues. To this end, we propose a practical universal domain adaptation setting for remote sensing image scene classification that requires no prior knowledge on the label sets. Furthermore, a novel universal domain adaptation method without source data is proposed for cases when the source data is unavailable. The architecture of the model is divided into two parts: the source data generation stage and the model adaptation stage. The first stage estimates the conditional distribution of source data from the pre-trained model using the knowledge of class-separability in the source domain and then synthesizes the source data. With this synthetic source data in hand, it becomes a universal DA task to classify a target sample correctly if it belongs to any category in the source label set, or mark it as ``unknown" otherwise. In the second stage, a novel transferable weight that distinguishes the shared and private label sets in each domain promotes the adaptation in the automatically discovered shared label set and recognizes the ``unknown'' samples successfully. Empirical results show that the proposed model is effective and practical for remote sensing image scene classification, regardless of whether the source data is available or not. The code is available at https://github.com/zhu-xlab/UniDA.



### Discriminative Entropy Clustering and its Relation to K-means and SVM
- **Arxiv ID**: http://arxiv.org/abs/2301.11405v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11405v2)
- **Published**: 2023-01-26 20:35:30+00:00
- **Updated**: 2023-05-23 21:25:10+00:00
- **Authors**: Zhongwen Zhang, Yuri Boykov
- **Comment**: None
- **Journal**: None
- **Summary**: Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.



### Are Labels Needed for Incremental Instance Learning?
- **Arxiv ID**: http://arxiv.org/abs/2301.11417v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11417v4)
- **Published**: 2023-01-26 21:07:12+00:00
- **Updated**: 2023-04-06 10:14:14+00:00
- **Authors**: Mert Kilickaya, Joaquin Vanschoren
- **Comment**: Accepted at CVPRW on CLVISION (Oral)
- **Journal**: None
- **Summary**: In this paper, we learn to classify visual object instances, incrementally and via self-supervision (self-incremental). Our learner observes a single instance at a time, which is then discarded from the dataset. Incremental instance learning is challenging, since longer learning sessions exacerbate forgetfulness, and labeling instances is cumbersome. We overcome these challenges via three contributions: i. We propose VINIL, a self-incremental learner that can learn object instances sequentially, ii. We equip VINIL with self-supervision to by-pass the need for instance labelling, iii. We compare VINIL to label-supervised variants on two large-scale benchmarks, and show that VINIL significantly improves accuracy while reducing forgetfulness.



### Parkinson gait modelling from an anomaly deep representation
- **Arxiv ID**: http://arxiv.org/abs/2301.11418v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2301.11418v2)
- **Published**: 2023-01-26 21:09:45+00:00
- **Updated**: 2023-08-29 22:36:22+00:00
- **Authors**: Edgar Rangel, Fabio Martinez
- **Comment**: Journal not submitted to any editorial
- **Journal**: None
- **Summary**: Parkinson's Disease (PD) is associated with gait movement disorders, such as bradykinesia, stiffness, tremors and postural instability, caused by progressive dopamine deficiency. Today, some approaches have implemented learning representations to quantify kinematic patterns during locomotion, supporting clinical procedures such as diagnosis and treatment planning. These approaches assumes a large amount of stratified and labeled data to optimize discriminative representations. Nonetheless these considerations may restrict the approaches to be operable in real scenarios during clinical practice. This work introduces a self-supervised generative representation to learn gait-motion-related patterns, under the pretext of video reconstruction and an anomaly detection framework. This architecture is trained following a one-class weakly supervised learning to avoid inter-class variance and approach the multiple relationships that represent locomotion. The proposed approach was validated with 14 PD patients and 23 control subjects, and trained with the control population only, achieving an AUC of 95%, homocedasticity level of 70% and shapeness level of 70% in the classification task considering its generalization.



### RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans
- **Arxiv ID**: http://arxiv.org/abs/2301.11422v1
- **DOI**: 10.1088/1361-6560/acb484
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11422v1)
- **Published**: 2023-01-26 21:20:14+00:00
- **Updated**: 2023-01-26 21:20:14+00:00
- **Authors**: Donghoon Lee, Ellen Yorke, Masoud Zarepisheh, Saad Nadeem, Yu-Chi Hu
- **Comment**: Physics in Medicine & Biology 2023. Last two authors contributed
  equally
- **Journal**: None
- **Summary**: This work aims to generate realistic anatomical deformations from static patient scans. Specifically, we present a method to generate these deformations/augmentations via deep learning driven respiratory motion simulation that provides the ground truth for validating deformable image registration (DIR) algorithms and driving more accurate deep learning based DIR. We present a novel 3D Seq2Seq deep learning respiratory motion simulator (RMSim) that learns from 4D-CT images and predicts future breathing phases given a static CT image. The predicted respiratory patterns, represented by time-varying displacement vector fields (DVFs) at different breathing phases, are modulated through auxiliary inputs of 1D breathing traces so that a larger amplitude in the trace results in more significant predicted deformation. Stacked 3D-ConvLSTMs are used to capture the spatial-temporal respiration patterns. Training loss includes a smoothness loss in the DVF and mean-squared error between the predicted and ground truth phase images. A spatial transformer deforms the static CT with the predicted DVF to generate the predicted phase image. 10-phase 4D-CTs of 140 internal patients were used to train and test RMSim. The trained RMSim was then used to augment a public DIR challenge dataset for training VoxelMorph to show the effectiveness of RMSim-generated deformation augmentation. We validated our RMSim output with both private and public benchmark datasets (healthy and cancer patients). The proposed approach can be used for validating DIR algorithms as well as for patient-specific augmentations to improve deep learning DIR algorithms. The code, pretrained models, and augmented DIR validation datasets will be released at https://github.com/nadeemlab/SeqX2Y.



### Semidefinite Relaxations for Robust Multiview Triangulation
- **Arxiv ID**: http://arxiv.org/abs/2301.11431v4
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2301.11431v4)
- **Published**: 2023-01-26 21:31:32+00:00
- **Updated**: 2023-04-05 07:11:11+00:00
- **Authors**: Linus Härenstam-Nielsen, Niclas Zeller, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach based on convex relaxations for certifiably optimal robust multiview triangulation. To this end, we extend existing relaxation approaches to non-robust multiview triangulation by incorporating a truncated least squares cost function. We propose two formulations, one based on epipolar constraints and one based on fractional reprojection constraints. The first is lower dimensional and remains tight under moderate noise and outlier levels, while the second is higher dimensional and therefore slower but remains tight even under extreme noise and outlier levels. We demonstrate through extensive experiments that the proposed approaches allow us to compute provably optimal reconstructions even under significant noise and a large percentage of outliers.



### 3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.11445v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2301.11445v3)
- **Published**: 2023-01-26 22:23:03+00:00
- **Updated**: 2023-05-01 22:19:24+00:00
- **Authors**: Biao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka
- **Comment**: Accepted by SIGGRAPH 2023 (Journal Track), Project website:
  https://1zb.github.io/3DShape2VecSet/, Project demo:
  https://youtu.be/KKQsQccpBFk
- **Journal**: None
- **Summary**: We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation.



### Boundary Aware U-Net for Glacier Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.11454v1
- **DOI**: 10.7557/18.6789
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.11454v1)
- **Published**: 2023-01-26 22:58:23+00:00
- **Updated**: 2023-01-26 22:58:23+00:00
- **Authors**: Bibek Aryal, Katie E. Miles, Sergio A. Vargas Zesati, Olac Fuentes
- **Comment**: Vol. 4 (2023): Proceedings of the Northern Lights Deep Learning
  Workshop 2023
- **Journal**: None
- **Summary**: Large-scale study of glaciers improves our understanding of global glacier change and is imperative for monitoring the ecological environment, preventing disasters, and studying the effects of global climate change. Glaciers in the Hindu Kush Himalaya (HKH) are particularly interesting as the HKH is one of the world's most sensitive regions for climate change. In this work, we: (1) propose a modified version of the U-Net for large-scale, spatially non-overlapping, clean glacial ice, and debris-covered glacial ice segmentation; (2) introduce a novel self-learning boundary-aware loss to improve debris-covered glacial ice segmentation performance; and (3) propose a feature-wise saliency score to understand the contribution of each feature in the multispectral Landsat 7 imagery for glacier mapping. Our results show that the debris-covered glacial ice segmentation model trained using self-learning boundary-aware loss outperformed the model trained using dice loss. Furthermore, we conclude that red, shortwave infrared, and near-infrared bands have the highest contribution toward debris-covered glacial ice segmentation from Landsat 7 images.



### Attacking Important Pixels for Anchor-free Detectors
- **Arxiv ID**: http://arxiv.org/abs/2301.11457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.11457v1)
- **Published**: 2023-01-26 23:03:03+00:00
- **Updated**: 2023-01-26 23:03:03+00:00
- **Authors**: Yunxu Xie, Shu Hu, Xin Wang, Quanyu Liao, Bin Zhu, Xi Wu, Siwei Lyu
- **Comment**: Yunxu Xie and Shu Hu contributed equally
- **Journal**: None
- **Summary**: Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbation can completely change the prediction result. Existing adversarial attacks on object detection focus on attacking anchor-based detectors, which may not work well for anchor-free detectors. In this paper, we propose the first adversarial attack dedicated to anchor-free detectors. It is a category-wise attack that attacks important pixels of all instances of a category simultaneously. Our attack manifests in two forms, sparse category-wise attack (SCA) and dense category-wise attack (DCA), that minimize the $L_0$ and $L_\infty$ norm-based perturbations, respectively. For DCA, we present three variants, DCA-G, DCA-L, and DCA-S, that select a global region, a local region, and a semantic region, respectively, to attack. Our experiments on large-scale benchmark datasets including PascalVOC, MS-COCO, and MS-COCO Keypoints indicate that our proposed methods achieve state-of-the-art attack performance and transferability on both object detection and human pose estimation tasks.



