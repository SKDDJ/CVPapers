# Arxiv Papers in cs.CV on 2023-01-16
### Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.06226v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06226v1)
- **Published**: 2023-01-16 01:08:32+00:00
- **Updated**: 2023-01-16 01:08:32+00:00
- **Authors**: Shubham Innani, Prasad Dutande, Bhakti Baheti, Ujjwal Baid, Sanjay Talbar
- **Comment**: Accepted to be published in 7th International Conference, CVIP 2022,
  Nagpur, India November 04-06, 2022
- **Journal**: None
- **Summary**: Automatic lesion analysis is critical in skin cancer diagnosis and ensures effective treatment. The computer aided diagnosis of such skin cancer in dermoscopic images can significantly reduce the clinicians workload and help improve diagnostic accuracy. Although researchers are working extensively to address this problem, early detection and accurate identification of skin lesions remain challenging. This research focuses on a two step framework for skin lesion segmentation followed by classification for lesion analysis. We explored the effectiveness of deep convolutional neural network based architectures by designing an encoder-decoder architecture for skin lesion segmentation and CNN based classification network. The proposed approaches are evaluated quantitatively in terms of the Accuracy, mean Intersection over Union and Dice Similarity Coefficient. Our cascaded end to end deep learning based approach is the first of its kind, where the classification accuracy of the lesion is significantly improved because of prior segmentation.



### Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems
- **Arxiv ID**: http://arxiv.org/abs/2301.06230v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06230v2)
- **Published**: 2023-01-16 01:41:35+00:00
- **Updated**: 2023-01-31 13:50:47+00:00
- **Authors**: Pierre-Yves Lajoie, Giovanni Beltrame
- **Comment**: Code: https://github.com/MISTLab/Swarm-SLAM
- **Journal**: None
- **Summary**: Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital component for successful multi-robot operations in environments without an external positioning system, such as indoors, underground or underwater. In this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is designed to be scalable, flexible, decentralized, and sparse, which are all key properties in swarm robotics. Our system supports inertial, lidar, stereo, and RGB-D sensing, and it includes a novel inter-robot loop closure prioritization technique that reduces communication and accelerates convergence. We evaluated our ROS-2 implementation on five different datasets, and in a real-world experiment with three robots communicating through an ad-hoc network. Our code is publicly available: https://github.com/MISTLab/Swarm-SLAM



### Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2301.06262v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06262v4)
- **Published**: 2023-01-16 05:08:50+00:00
- **Updated**: 2023-08-30 08:21:13+00:00
- **Authors**: Yushan Han, Hui Zhang, Huifang Li, Yi Jin, Congyan Lang, Yidong Li
- **Comment**: 18 pages, 6 figures. Accepted by IEEE Intelligent Transportation
  Systems Magazine. URL:
  https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving
- **Journal**: None
- **Summary**: Collaborative perception is essential to address occlusion and sensor failure issues in autonomous driving. In recent years, theoretical and experimental investigations of novel works for collaborative perception have increased tremendously. So far, however, few reviews have focused on systematical collaboration modules and large-scale collaborative perception datasets. This work reviews recent achievements in this field to bridge this gap and motivate future research. We start with a brief overview of collaboration schemes. After that, we systematically summarize the collaborative perception methods for ideal scenarios and real-world issues. The former focuses on collaboration modules and efficiency, and the latter is devoted to addressing the problems in actual application. Furthermore, we present large-scale public datasets and summarize quantitative results on these benchmarks. Finally, we highlight gaps and overlook challenges between current academic research and real-world applications. The project page is https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving



### Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models
- **Arxiv ID**: http://arxiv.org/abs/2301.06267v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.06267v4)
- **Published**: 2023-01-16 05:40:42+00:00
- **Updated**: 2023-08-03 01:56:35+00:00
- **Authors**: Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan
- **Comment**: CVPR 2023. Project website:
  https://linzhiqiu.github.io/papers/cross_modal/
- **Journal**: None
- **Summary**: The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification.



### DarkVision: A Benchmark for Low-light Image/Video Perception
- **Arxiv ID**: http://arxiv.org/abs/2301.06269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06269v1)
- **Published**: 2023-01-16 05:55:59+00:00
- **Updated**: 2023-01-16 05:55:59+00:00
- **Authors**: Bo Zhang, Yuchen Guo, Runzhao Yang, Zhihong Zhang, Jiayi Xie, Jinli Suo, Qionghai Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging and perception in photon-limited scenarios is necessary for various applications, e.g., night surveillance or photography, high-speed photography, and autonomous driving. In these cases, cameras suffer from low signal-to-noise ratio, which degrades the image quality severely and poses challenges for downstream high-level vision tasks like object detection and recognition. Data-driven methods have achieved enormous success in both image restoration and high-level vision tasks. However, the lack of high-quality benchmark dataset with task-specific accurate annotations for photon-limited images/videos delays the research progress heavily. In this paper, we contribute the first multi-illuminance, multi-camera, and low-light dataset, named DarkVision, serving for both image enhancement and object detection. We provide bright and dark pairs with pixel-wise registration, in which the bright counterpart provides reliable reference for restoration and annotation. The dataset consists of bright-dark pairs of 900 static scenes with objects from 15 categories, and 32 dynamic scenes with 4-category objects. For each scene, images/videos were captured at 5 illuminance levels using three cameras of different grades, and average photons can be reliably estimated from the calibration data for quantitative studies. The static-scene images and dynamic videos respectively contain around 7,344 and 320,667 instances in total. With DarkVision, we established baselines for image/video enhancement and object detection by representative algorithms. To demonstrate an exemplary application of DarkVision, we propose two simple yet effective approaches for improving performance in video enhancement and object detection respectively. We believe DarkVision would advance the state-of-the-arts in both imaging and related computer vision tasks in low-light environment.



### DPE: Disentanglement of Pose and Expression for General Video Portrait Editing
- **Arxiv ID**: http://arxiv.org/abs/2301.06281v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06281v2)
- **Published**: 2023-01-16 06:39:51+00:00
- **Updated**: 2023-03-01 08:21:23+00:00
- **Authors**: Youxin Pang, Yong Zhang, Weize Quan, Yanbo Fan, Xiaodong Cun, Ying Shan, Dong-ming Yan
- **Comment**: https://carlyx.github.io/DPE/
- **Journal**: None
- **Summary**: One-shot video-driven talking face generation aims at producing a synthetic talking video by transferring the facial motion from a video to an arbitrary portrait image. Head pose and facial expression are always entangled in facial motion and transferred simultaneously. However, the entanglement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged. One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blenshapes, which has side effects on motion transfer. In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints. Evaluations demonstrate our method can control pose or expression independently and be used for general video editing.



### Meta Generative Attack on Person Reidentification
- **Arxiv ID**: http://arxiv.org/abs/2301.06286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06286v1)
- **Published**: 2023-01-16 07:08:51+00:00
- **Updated**: 2023-01-16 07:08:51+00:00
- **Authors**: A V Subramanyam
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks have been recently investigated in person re-identification. These attacks perform well under cross dataset or cross model setting. However, the challenges present in cross-dataset cross-model scenario does not allow these models to achieve similar accuracy. To this end, we propose our method with the goal of achieving better transferability against different models and across datasets. We generate a mask to obtain better performance across models and use meta learning to boost the generalizability in the challenging cross-dataset cross-model setting. Experiments on Market-1501, DukeMTMC-reID and MSMT-17 demonstrate favorable results compared to other attacks.



### Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.06293v1
- **DOI**: None
- **Categories**: **cs.CV**, 49Q22, 62M10, I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2301.06293v1)
- **Published**: 2023-01-16 07:48:37+00:00
- **Updated**: 2023-01-16 07:48:37+00:00
- **Authors**: Felix Ott, David Rügamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler
- **Comment**: Accepted at IAPR Intl. Workshop on Multimodal Pattern Recognition of
  Social Signals in Human Computer Interaction (MPRSS), Montreal, Canada,
  August 2022
- **Journal**: None
- **Summary**: The performance of a machine learning model degrades when it is applied to data from a similar but different domain than the data it has initially been trained on. The goal of domain adaptation (DA) is to mitigate this domain shift problem by searching for an optimal feature transformation to learn a domain-invariant representation. Such a domain shift can appear in handwriting recognition (HWR) applications where the motion pattern of the hand and with that the motion pattern of the pen is different for writing on paper and on tablet. This becomes visible in the sensor data for online handwriting (OnHW) from pens with integrated inertial measurement units. This paper proposes a supervised DA approach to enhance learning for OnHW recognition between tablet and paper data. Our method exploits loss functions such as maximum mean discrepancy and correlation alignment to learn a domain-invariant feature representation (i.e., similar covariances between tablet and paper features). We use a triplet loss that takes negative samples of the auxiliary domain (i.e., paper samples) to increase the amount of samples of the tablet dataset. We conduct an evaluation on novel sequence-based OnHW datasets (i.e., words) and show an improvement on the paper domain with an early fusion strategy by using pairwise learning.



### LYSTO: The Lymphocyte Assessment Hackathon and Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.06304v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07, I.4.9; I.5.4; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2301.06304v2)
- **Published**: 2023-01-16 08:18:57+00:00
- **Updated**: 2023-04-13 15:05:19+00:00
- **Authors**: Yiping Jiao, Jeroen van der Laak, Shadi Albarqouni, Zhang Li, Tao Tan, Abhir Bhalerao, Jiabo Ma, Jiamei Sun, Johnathan Pocock, Josien P. W. Pluim, Navid Alemi Koohbanani, Raja Muhammad Saad Bashir, Shan E Ahmed Raza, Sibo Liu, Simon Graham, Suzanne Wetstein, Syed Ali Khurram, Thomas Watson, Nasir Rajpoot, Mitko Veta, Francesco Ciompi
- **Comment**: will be sumitted to IEEE-JBHI
- **Journal**: None
- **Summary**: We introduce LYSTO, the Lymphocyte Assessment Hackathon, which was held in conjunction with the MICCAI 2019 Conference in Shenzen (China). The competition required participants to automatically assess the number of lymphocytes, in particular T-cells, in histopathological images of colon, breast, and prostate cancer stained with CD3 and CD8 immunohistochemistry. Differently from other challenges setup in medical image analysis, LYSTO participants were solely given a few hours to address this problem. In this paper, we describe the goal and the multi-phase organization of the hackathon; we describe the proposed methods and the on-site results. Additionally, we present post-competition results where we show how the presented methods perform on an independent set of lung cancer slides, which was not part of the initial competition, as well as a comparison on lymphocyte assessment between presented methods and a panel of pathologists. We show that some of the participants were capable to achieve pathologist-level performance at lymphocyte assessment. After the hackathon, LYSTO was left as a lightweight plug-and-play benchmark dataset on grand-challenge website, together with an automatic evaluation platform. LYSTO has supported a number of research in lymphocyte assessment in oncology. LYSTO will be a long-lasting educational challenge for deep learning and digital pathology, it is available at https://lysto.grand-challenge.org/.



### UATVR: Uncertainty-Adaptive Text-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2301.06309v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06309v2)
- **Published**: 2023-01-16 08:43:17+00:00
- **Updated**: 2023-08-19 02:28:10+00:00
- **Authors**: Bo Fang, Wenhao Wu, Chang Liu, Yu Zhou, Yuxin Song, Weiping Wang, Xiangbo Shu, Xiangyang Ji, Jingdong Wang
- **Comment**: To appear at ICCV2023
- **Journal**: None
- **Summary**: With the explosive growth of web videos and emerging large-scale vision-language pre-training models, e.g., CLIP, retrieving videos of interest with text instructions has attracted increasing attention. A common practice is to transfer text-video pairs to the same embedding space and craft cross-modal interactions with certain entities in specific granularities for semantic correspondence. Unfortunately, the intrinsic uncertainties of optimal entity combinations in appropriate granularities for cross-modal queries are understudied, which is especially critical for modalities with hierarchical semantics, e.g., video, text, etc. In this paper, we propose an Uncertainty-Adaptive Text-Video Retrieval approach, termed UATVR, which models each look-up as a distribution matching procedure. Concretely, we add additional learnable tokens in the encoders to adaptively aggregate multi-grained semantics for flexible high-level reasoning. In the refined embedding space, we represent text-video pairs as probabilistic distributions where prototypes are sampled for matching evaluation. Comprehensive experiments on four benchmarks justify the superiority of our UATVR, which achieves new state-of-the-art results on MSR-VTT (50.8%), VATEX (64.5%), MSVD (49.7%), and DiDeMo (45.8%). The code is available at https://github.com/bofang98/UATVR.



### Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.06324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06324v2)
- **Published**: 2023-01-16 09:29:29+00:00
- **Updated**: 2023-04-18 06:45:31+00:00
- **Authors**: Youngjae Song, Sung Kuk Shyn, Kwang-su Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional tabular classifiers provide explainable decision-making with interpretable features(concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, we expect Img2Tab classifiers to discover class-relevant concepts automatically from the StyleGAN features. Thus, we propose a novel method using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. Using this method, we investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy that is on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can easily debug Img2Tab classifiers at the concept level to ensure unbiased and fair decision-making without sacrificing accuracy.



### Post-Train Adaptive U-Net for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.06358v1
- **DOI**: 10.32782/IT/2022-2-8
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06358v1)
- **Published**: 2023-01-16 11:06:05+00:00
- **Updated**: 2023-01-16 11:06:05+00:00
- **Authors**: Kostiantyn Khabarlak
- **Comment**: None
- **Journal**: None
- **Summary**: Typical neural network architectures used for image segmentation cannot be changed without further training. This is quite limiting as the network might not only be executed on a powerful server, but also on a mobile or edge device. Adaptive neural networks offer a solution to the problem by allowing certain adaptivity after the training process is complete. In this work for the first time, we apply Post-Train Adaptive (PTA) approach to the task of image segmentation. We introduce U-Net+PTA neural network, which can be trained once, and then adapted to different device performance categories. The two key components of the approach are PTA blocks and PTA-sampling training strategy. The post-train configuration can be done at runtime on any inference device including mobile. Also, the PTA approach has allowed to improve image segmentation Dice score on the CamVid dataset. The final trained model can be switched at runtime between 6 PTA configurations, which differ by inference time and quality. Importantly, all of the configurations have better quality than the original U-Net (No PTA) model.



### Multimodal Side-Tuning for Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.07502v2
- **DOI**: 10.1109/ICPR48806.2021.9413208
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.07502v2)
- **Published**: 2023-01-16 11:08:03+00:00
- **Updated**: 2023-01-23 14:28:15+00:00
- **Authors**: Stefano Pio Zingaro, Giuseppe Lisanti, Maurizio Gabbrielli
- **Comment**: 2020 25th International Conference on Pattern Recognition (ICPR)
- **Journal**: None
- **Summary**: In this paper, we propose to exploit the side-tuning framework for multimodal document classification. Side-tuning is a methodology for network adaptation recently introduced to solve some of the problems related to previous approaches. Thanks to this technique it is actually possible to overcome model rigidity and catastrophic forgetting of transfer learning by fine-tuning. The proposed solution uses off-the-shelf deep learning architectures leveraging the side-tuning framework to combine a base model with a tandem of two side networks. We show that side-tuning can be successfully employed also when different data sources are considered, e.g. text and images in document classification. The experimental results show that this approach pushes further the limit for document classification accuracy with respect to the state of the art.



### A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems
- **Arxiv ID**: http://arxiv.org/abs/2301.06363v2
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06363v2)
- **Published**: 2023-01-16 11:17:32+00:00
- **Updated**: 2023-07-24 23:39:15+00:00
- **Authors**: Andrea Coletta, Flavio Giorgi, Gaia Maselli, Matteo Prata, Domenicomichele Silvestri, Jonathan Ashdown, Francesco Restuccia
- **Comment**: Accepted to INFOCOM 2023
- **Journal**: None
- **Summary**: To perform advanced surveillance, Unmanned Aerial Vehicles (UAVs) require the execution of edge-assisted computer vision (CV) tasks. In multi-hop UAV networks, the successful transmission of these tasks to the edge is severely challenged due to severe bandwidth constraints. For this reason, we propose a novel A$^2$-UAV framework to optimize the number of correctly executed tasks at the edge. In stark contrast with existing art, we take an application-aware approach and formulate a novel pplication-Aware Task Planning Problem (A$^2$-TPP) that takes into account (i) the relationship between deep neural network (DNN) accuracy and image compression for the classes of interest based on the available dataset, (ii) the target positions, (iii) the current energy/position of the UAVs to optimize routing, data pre-processing and target assignment for each UAV. We demonstrate A$^2$-TPP is NP-Hard and propose a polynomial-time algorithm to solve it efficiently. We extensively evaluate A$^2$-UAV through real-world experiments with a testbed composed by four DJI Mavic Air 2 UAVs. We consider state-of-the-art image classification tasks with four different DNN models (i.e., DenseNet, ResNet152, ResNet50 and MobileNet-V2) and object detection tasks using YoloV4 trained on the ImageNet dataset. Results show that A$^2$-UAV attains on average around 38% more accomplished tasks than the state-of-the-art, with 400% more accomplished tasks when the number of targets increases significantly. To allow full reproducibility, we pledge to share datasets and code with the research community.



### Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images
- **Arxiv ID**: http://arxiv.org/abs/2301.06366v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06366v1)
- **Published**: 2023-01-16 11:25:17+00:00
- **Updated**: 2023-01-16 11:25:17+00:00
- **Authors**: Anuja Vats, Marius Pedersen, Ahmed Mohammed, Øistein Hovde
- **Comment**: 13 pages,10 figures
- **Journal**: None
- **Summary**: Wireless Capsule Endoscopy (WCE) is being increasingly used as an alternative imaging modality for complete and non-invasive screening of the gastrointestinal tract. Although this is advantageous in reducing unnecessary hospital admissions, it also demands that a WCE diagnostic protocol be in place so larger populations can be effectively screened. This calls for training and education protocols attuned specifically to this modality. Like training in other modalities such as traditional endoscopy, CT, MRI, etc., a WCE training protocol would require an atlas comprising of a large corpora of images that show vivid descriptions of pathologies and abnormalities, ideally observed over a period of time. Since such comprehensive atlases are presently lacking in WCE, in this work, we propose a deep learning method for utilizing already available studies across different institutions for the creation of a realistic WCE atlas using StyleGAN. We identify clinically relevant attributes in WCE such that synthetic images can be generated with selected attributes on cue. Beyond this, we also simulate several disease progression scenarios. The generated images are evaluated for realism and plausibility through three subjective online experiments with the participation of eight gastroenterology experts from three geographical locations and a variety of years of experience. The results from the experiments indicate that the images are highly realistic and the disease scenarios plausible. The images comprising the atlas are available publicly for use in training applications as well as supplementing real datasets for deep learning.



### Disambiguation of One-Shot Visual Classification Tasks: A Simplex-Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2301.06372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06372v1)
- **Published**: 2023-01-16 11:37:05+00:00
- **Updated**: 2023-01-16 11:37:05+00:00
- **Authors**: Yassir Bendou, Lucas Drumetz, Vincent Gripon, Giulia Lioi, Bastien Pasdeloup
- **Comment**: None
- **Journal**: None
- **Summary**: The field of visual few-shot classification aims at transferring the state-of-the-art performance of deep learning visual systems onto tasks where only a very limited number of training samples are available. The main solution consists in training a feature extractor using a large and diverse dataset to be applied to the considered few-shot task. Thanks to the encoded priors in the feature extractors, classification tasks with as little as one example (or "shot'') for each class can be solved with high accuracy, even when the shots display individual features not representative of their classes. Yet, the problem becomes more complicated when some of the given shots display multiple objects. In this paper, we present a strategy which aims at detecting the presence of multiple and previously unseen objects in a given shot. This methodology is based on identifying the corners of a simplex in a high dimensional space. We introduce an optimization routine and showcase its ability to successfully detect multiple (previously unseen) objects in raw images. Then, we introduce a downstream classifier meant to exploit the presence of multiple objects to improve the performance of few-shot classification, in the case of extreme settings where only one shot is given for its class. Using standard benchmarks of the field, we show the ability of the proposed method to slightly, yet statistically significantly, improve accuracy in these settings.



### OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.06375v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.AI, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2301.06375v1)
- **Published**: 2023-01-16 11:40:50+00:00
- **Updated**: 2023-01-16 11:40:50+00:00
- **Authors**: Jeongkyun Park, Jung-Wook Hwang, Kwanghee Choi, Seung-Hyun Lee, Jun Hwan Ahn, Rae-Hong Park, Hyung-Min Park
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.



### I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/2301.06392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06392v1)
- **Published**: 2023-01-16 12:25:42+00:00
- **Updated**: 2023-01-16 12:25:42+00:00
- **Authors**: Jiwan Hur, Jae Young Lee, Jaehyun Choi, Junmo Kim
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Light field (LF) camera captures rich information from a scene. Using the information, the LF de-occlusion (LF-DeOcc) task aims to reconstruct the occlusion-free center view image. Existing LF-DeOcc studies mainly focus on the sparsely sampled (sparse) LF images where most of the occluded regions are visible in other views due to the large disparity. In this paper, we expand LF-DeOcc in more challenging datasets, densely sampled (dense) LF images, which are taken by a micro-lens-based portable LF camera. Due to the small disparity ranges of dense LF images, most of the background regions are invisible in any view. To apply LF-DeOcc in both LF datasets, we propose a framework, ISTY, which is defined and divided into three roles: (1) extract LF features, (2) define the occlusion, and (3) inpaint occluded regions. By dividing the framework into three specialized components according to the roles, the development and analysis can be easier. Furthermore, an explainable intermediate representation, an occlusion mask, can be obtained in the proposed framework. The occlusion mask is useful for comprehensive analysis of the model and other applications by manipulating the mask. In experiments, qualitative and quantitative results show that the proposed framework outperforms state-of-the-art LF-DeOcc methods in both sparse and dense LF datasets.



### Linguistic Query-Guided Mask Generation for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.06429v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06429v3)
- **Published**: 2023-01-16 13:38:22+00:00
- **Updated**: 2023-03-22 12:01:42+00:00
- **Authors**: Zhichao Wei, Xiaohao Chen, Mingqiang Chen, Siyu Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Referring image segmentation aims to segment the image region of interest according to the given language expression, which is a typical multi-modal task. Existing methods either adopt the pixel classification-based or the learnable query-based framework for mask generation, both of which are insufficient to deal with various text-image pairs with a fix number of parametric prototypes. In this work, we propose an end-to-end framework built on transformer to perform Linguistic query-Guided mask generation, dubbed LGFormer. It views the linguistic features as query to generate a specialized prototype for arbitrary input image-text pair, thus generating more consistent segmentation results. Moreover, we design several cross-modal interaction modules (\eg, vision-language bidirectional attention module, VLBA) in both encoder and decoder to achieve better cross-modal alignment.



### A semi-trailer truck right-hook turn blind spot alert system for detecting vulnerable road users using transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2303.11223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.4.0; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2303.11223v1)
- **Published**: 2023-01-16 13:54:13+00:00
- **Updated**: 2023-01-16 13:54:13+00:00
- **Authors**: Charles Tang
- **Comment**: 9 pages, 13 figures
- **Journal**: None
- **Summary**: Cycling is an increasingly popular method of transportation for sustainability and health benefits. However, cyclists face growing risks, especially when encountering semi-trailer trucks. This study aims to reduce the number of truck-cyclist collisions, which are often caused by semi-trailer trucks making right-hook turns and poor driver attention to blind spots. To achieve this, we designed a visual-based blind spot warning system that can detect cyclists for semi-trailer truck drivers using deep learning. First, several greater than 90% mAP cyclist detection models, such as the EfficientDet Lite 1 and SSD MobileNetV2, were created using state-of-the-art lightweight deep learning architectures fine-tuned on a newly proposed cyclist image dataset composed of a diverse set of over 20,000 images. Next, the object detection model was deployed onto a Google Coral Dev Board mini-computer with a camera module and analyzed for speed, reaching inference times as low as 15 milliseconds. Lastly, the end-to-end blind spot cyclist detection device was tested in real-time to model traffic scenarios and analyzed further for performance and feasibility. We concluded that this portable blind spot alert device can accurately and quickly detect cyclists and have the potential to significantly improve cyclist safety. Future studies could determine the feasibility of the proposed device in the trucking industry and improvements to cyclist safety over time.



### Modeling Uncertain Feature Representation for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2301.06442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06442v1)
- **Published**: 2023-01-16 14:25:02+00:00
- **Updated**: 2023-01-16 14:25:02+00:00
- **Authors**: Xiaotong Li, Zixuan Hu, Jun Liu, Yixiao Ge, Yongxing Dai, Ling-Yu Duan
- **Comment**: This work is an extension of our ICLR 2022 paper [arXiv:2202.03958]
  https://openreview.net/forum?id=6HN7LHyzGgC
- **Journal**: None
- **Summary**: Though deep neural networks have achieved impressive success on various vision tasks, obvious performance degradation still exists when models are tested in out-of-distribution scenarios. In addressing this limitation, we ponder that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Existing methods commonly consider feature statistics as deterministic values measured from the learned features and do not explicitly model the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling domain shifts with uncertainty (DSU), i.e., characterizing the feature statistics as uncertain distributions during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. During inference, we propose an instance-wise adaptation strategy that can adaptively deal with the unforeseeable shift and further enhance the generalization ability of the trained model with negligible additional cost. We also conduct theoretical analysis on the aspects of generalization error bound and the implicit regularization effect, showing the efficacy of our method. Extensive experiments demonstrate that our method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, instance retrieval, and pose estimation. Our methods are simple yet effective and can be readily integrated into networks without additional trainable parameters or loss constraints. Code will be released in https://github.com/lixiaotong97/DSU.



### Sparse resultant based minimal solvers in computer vision and their connection with the action matrix
- **Arxiv ID**: http://arxiv.org/abs/2301.06443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06443v1)
- **Published**: 2023-01-16 14:25:19+00:00
- **Updated**: 2023-01-16 14:25:19+00:00
- **Authors**: Snehal Bhayani, Janne Heikkilä, Zuzana Kukelova
- **Comment**: arXiv admin note: text overlap with arXiv:1912.10268
- **Journal**: None
- **Summary**: Many computer vision applications require robust and efficient estimation of camera geometry from a minimal number of input data measurements, ie, solving minimal problems in a RANSAC framework. Minimal problems are usually formulated as complex systems of polynomial equations. Many state-of-the-art efficient polynomial solvers are based on the action matrix method that has been automated and highly optimised in recent years. In this paper we explore the theory of sparse resultants for generating minimal solvers and propose a novel approach based on a using an extra polynomial with a special form. We show that for some camera geometry problems our extra polynomial-based method leads to smaller and more stable solvers than the state-of-the-art Gr\"obner basis-based solvers. The proposed method can be fully automated and incorporated into existing tools for automatic generation of efficient polynomial solvers. It provides a competitive alternative to popular Gr\"obner basis-based methods for minimal problems in computer vision. Additionally, we study the conditions under which the minimal solvers generated by the state-of-the-art action matrix-based methods and the proposed extra polynomial resultant-based method, are equivalent. Specifically we consider a step-by-step comparison between the approaches based on the action matrix and the sparse resultant, followed by a set of substitutions, which would lead to equivalent minimal solvers.



### Simplex Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2301.06489v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06489v1)
- **Published**: 2023-01-16 15:57:03+00:00
- **Updated**: 2023-01-16 15:57:03+00:00
- **Authors**: Aymene Mohammed Bouayed, David Naccache
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic data generation is increasingly important due to privacy concerns. While Autoencoder-based approaches have been widely used for this purpose, sampling from their latent spaces can be challenging. Mixture models are currently the most efficient way to sample from these spaces. In this work, we propose a new approach that models the latent space of an Autoencoder as a simplex, allowing for a novel heuristic for determining the number of components in the mixture model. This heuristic is independent of the number of classes and produces comparable results. We also introduce a sampling method based on probability mass functions, taking advantage of the compactness of the latent space. We evaluate our approaches on a synthetic dataset and demonstrate their performance on three benchmark datasets: MNIST, CIFAR-10, and Celeba. Our approach achieves an image generation FID of 4.29, 13.55, and 11.90 on the MNIST, CIFAR-10, and Celeba datasets, respectively. The best AE FID results to date on those datasets are respectively 6.3, 85.3 and 35.6 we hence substantially improve those figures (the lower is the FID the better). However, AEs are not the best performing algorithms on the concerned datasets and all FID records are currently held by GANs. While we do not perform better than GANs on CIFAR and Celeba we do manage to squeeze-out a non-negligible improvement (of 0.21) over the current GAN-held record for the MNIST dataset.



### Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning
- **Arxiv ID**: http://arxiv.org/abs/2301.06496v3
- **DOI**: None
- **Categories**: **physics.optics**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06496v3)
- **Published**: 2023-01-16 16:10:52+00:00
- **Updated**: 2023-08-24 16:39:42+00:00
- **Authors**: Joowon Lim, Jannes Gladrow, Douglas Kelly, Greg O'Shea, Govert Verkes, Ioan Stefanovici, Sebastian Nowozin, Benn Thomsen
- **Comment**: 21 pages, 5 figures
- **Journal**: None
- **Summary**: Retrieving images transmitted through multi-mode fibers is of growing interest, thanks to their ability to confine and transport light efficiently in a compact system. Here, we demonstrate machine-learning-based decoding of large-scale digital images (pages), maximizing page capacity for optical storage applications. Using a millimeter-sized square cross-section waveguide, we image an 8-bit spatial light modulator, presenting data as a matrix of symbols. Normally, decoders will incur a prohibitive O(n^2) computational scaling to decode n symbols in spatially scrambled data. However, by combining a digital twin of the setup with a U-Net, we can retrieve up to 66 kB using efficient convolutional operations only. We compare trainable ray-tracing-based with eigenmode-based twins and show the former to be superior thanks to its ability to overcome the simulation-to-experiment gap by adjusting to optical imperfections. We train the pipeline end-to-end using a differentiable mutual-information estimator based on the von-Mises distribution, generally applicable to phase-coding channels.



### Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2301.06567v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06567v2)
- **Published**: 2023-01-16 19:04:23+00:00
- **Updated**: 2023-08-16 03:45:46+00:00
- **Authors**: Hunsoo Song, Jinha Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Despite substantial technological advancements, the comprehensive mapping of surface water, particularly smaller bodies (<1ha), continues to be a challenge due to a lack of robust, scalable methods. Standard methods require either training labels or site-specific parameter tuning, which complicates automated mapping and introduces biases related to training data and parameters. The reliance on water's reflectance properties, including LiDAR intensity, further complicates the matter, as higher-resolution images inherently produce more noise. To mitigate these difficulties, we propose a unique method that focuses on the geometric characteristics of water instead of its variable reflectance properties. Unlike preceding approaches, our approach relies entirely on 3D coordinate observations from airborne LiDAR data, taking advantage of the principle that connected surface water remains flat due to gravity. By harnessing this natural law in conjunction with connectivity, our method can accurately and scalably identify small water bodies, eliminating the need for training labels or repetitive parameter tuning. Consequently, our approach enables the creation of comprehensive 3D topographic maps that include both water and terrain, all performed in an unsupervised manner using only airborne laser scanning data, potentially enhancing the process of generating reliable 3D topographic maps. We validated our method across extensive and diverse landscapes, while comparing it to highly competitive Normalized Difference Water Index (NDWI)-based methods and assessing it using a reference surface water map. In conclusion, our method offers a new approach to address persistent difficulties in robust, scalable surface water mapping and 3D topographic mapping, using solely airborne LiDAR data.



### TAAL: Test-time Augmentation for Active Learning in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.06624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.06624v1)
- **Published**: 2023-01-16 22:19:41+00:00
- **Updated**: 2023-01-16 22:19:41+00:00
- **Authors**: Mélanie Gaillochet, Christian Desrosiers, Hervé Lombaert
- **Comment**: Accepted to MICCAI-DALI 2022 (LNCS Proceedings, vol.13567), 11 pages
- **Journal**: None
- **Summary**: Deep learning methods typically depend on the availability of labeled data, which is expensive and time-consuming to obtain. Active learning addresses such effort by prioritizing which samples are best to annotate in order to maximize the performance of the task model. While frameworks for active learning have been widely explored in the context of classification of natural images, they have been only sparsely used in medical image segmentation. The challenge resides in obtaining an uncertainty measure that reveals the best candidate data for annotation. This paper proposes Test-time Augmentation for Active Learning (TAAL), a novel semi-supervised active learning approach for segmentation that exploits the uncertainty information offered by data transformations. Our method applies cross-augmentation consistency during training and inference to both improve model learning in a semi-supervised fashion and identify the most relevant unlabeled samples to annotate next. In addition, our consistency loss uses a modified version of the JSD to further improve model performance. By relying on data transformations rather than on external modules or simple heuristics typically used in uncertainty-based strategies, TAAL emerges as a simple, yet powerful task-agnostic semi-supervised active learning approach applicable to the medical domain. Our results on a publicly-available dataset of cardiac images show that TAAL outperforms existing baseline methods in both fully-supervised and semi-supervised settings. Our implementation is publicly available on https://github.com/melinphd/TAAL.



### Masked Vector Quantization
- **Arxiv ID**: http://arxiv.org/abs/2301.06626v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06626v1)
- **Published**: 2023-01-16 22:30:53+00:00
- **Updated**: 2023-01-16 22:30:53+00:00
- **Authors**: David D. Nguyen, David Leibowitz, Surya Nepal, Salil S. Kanhere
- **Comment**: Main paper (10 pages), References (2 pages), Appendix (7 pages)
- **Journal**: None
- **Summary**: Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\%$ at 2 tokens per instance and $57\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\textit{--}45\times$ speed-up in token sampling during inference. As an additional benefit, we find that smaller latent spaces lead to MVQ identifying transferable visual representations where multiple can be smoothly combined.



### Diverse Multimedia Layout Generation with Multi Choice Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.06629v1
- **DOI**: 10.1145/3474085.3475525
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.06629v1)
- **Published**: 2023-01-16 22:53:55+00:00
- **Updated**: 2023-01-16 22:53:55+00:00
- **Authors**: David D. Nguyen, Surya Nepal, Salil S. Kanhere
- **Comment**: 9 pages
- **Journal**: Proceedings of the 29th ACM International Conference on Multimedia
  2021
- **Summary**: Designing visually appealing layouts for multimedia documents containing text, graphs and images requires a form of creative intelligence. Modelling the generation of layouts has recently gained attention due to its importance in aesthetics and communication style. In contrast to standard prediction tasks, there are a range of acceptable layouts which depend on user preferences. For example, a poster designer may prefer logos on the top-left while another prefers logos on the bottom-right. Both are correct choices yet existing machine learning models treat layouts as a single choice prediction problem. In such situations, these models would simply average over all possible choices given the same input forming a degenerate sample. In the above example, this would form an unacceptable layout with a logo in the centre. In this paper, we present an auto-regressive neural network architecture, called LayoutMCL, that uses multi-choice prediction and winner-takes-all loss to effectively stabilise layout generation. LayoutMCL avoids the averaging problem by using multiple predictors to learn a range of possible options for each layout object. This enables LayoutMCL to generate multiple and diverse layouts from a single input which is in contrast with existing approaches which yield similar layouts with minor variations. Through quantitative benchmarks on real data (magazine, document and mobile app layouts), we demonstrate that LayoutMCL reduces Fr\'echet Inception Distance (FID) by 83-98% and generates significantly more diversity in comparison to existing approaches.



