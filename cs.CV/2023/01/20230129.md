# Arxiv Papers in cs.CV on 2023-01-29
### Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2301.12332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12332v1)
- **Published**: 2023-01-29 02:59:14+00:00
- **Updated**: 2023-01-29 02:59:14+00:00
- **Authors**: Peng Qiao, Sidun Liu, Tao Sun, Ke Yang, Yong Dou
- **Comment**: None
- **Journal**: None
- **Summary**: The great success of Deep Neural Networks (DNNs) has inspired the algorithmic development of DNN-based Fixed-Point (DNN-FP) for computer vision tasks. DNN-FP methods, trained by Back-Propagation Through Time or computing the inaccurate inversion of the Jacobian, suffer from inferior representation ability. Motivated by the representation power of the Transformer, we propose a framework to unroll the FP and approximate each unrolled process via Transformer blocks, called FPformer. To reduce the high consumption of memory and computation, we come up with FPRformer by sharing parameters between the successive blocks. We further design a module to adapt Anderson acceleration to FPRformer to enlarge the unrolled iterations and improve the performance, called FPAformer. In order to fully exploit the capability of the Transformer, we apply the proposed model to image restoration, using self-supervised pre-training and supervised fine-tuning. 161 tasks from 4 categories of image restoration problems are used in the pre-training phase. Hereafter, the pre-trained FPformer, FPRformer, and FPAformer are further fine-tuned for the comparison scenarios. Using self-supervised pre-training and supervised fine-tuning, the proposed FPformer, FPRformer, and FPAformer achieve competitive performance with state-of-the-art image restoration methods and better training efficiency. FPAformer employs only 29.82% parameters used in SwinIR models, and provides superior performance after fine-tuning. To train these comparison models, it takes only 26.9% time used for training SwinIR models. It provides a promising way to introduce the Transformer in low-level vision tasks.



### Don't Play Favorites: Minority Guidance for Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.12334v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.12334v1)
- **Published**: 2023-01-29 03:08:47+00:00
- **Updated**: 2023-01-29 03:08:47+00:00
- **Authors**: Soobin Um, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating sufficient numbers of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves highly ineffective and time-consuming for the task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first provide a new insight on the majority-focused nature of the diffusion models: they denoise in favor of the majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the diffusion models w.r.t. the majority samples, we further develop minority guidance, a sampling technique that can guide the generation process toward regions with desired likelihood levels. Experiments on benchmark real datasets demonstrate that our minority guidance can greatly improve the capability of generating the low-likelihood minority samples over existing generative frameworks including the standard diffusion sampler.



### A novel method using machine learning to integrate features from lung and epicardial adipose tissue for detecting the severity of COVID-19 infection
- **Arxiv ID**: http://arxiv.org/abs/2301.12340v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12340v1)
- **Published**: 2023-01-29 03:31:51+00:00
- **Updated**: 2023-01-29 03:31:51+00:00
- **Authors**: Ni Yao, Yanhui Tian, Daniel Gama das Neves, Chen Zhao, Claudio Tinoco Mesquita, Wolney de Andrade Martins, Alair Augusto Sarmet Moreira Damas dos Santos, Yanting Li, Chuang Han, Fubao Zhu, Neng Dai, Weihua Zhou
- **Comment**: 24 pages, 4 figures
- **Journal**: None
- **Summary**: Objectives: To investigate the value of radiomics features of epicardial adipose tissue (EAT) combined with lung for detecting the severity of Coronavirus Disease 2019 (COVID-19) infection. Methods: The retrospective study included data from 515 COVID-19 patients (Cohort1: 415, cohort2: 100) from the two centers between January 2020 and July 2020. A deep learning method was developed to extract the myocardium and visceral pericardium from chest CTs, and then a threshold was applied for automatic EAT extraction. Lung segmentation was achieved according to a published method. Radiomics features of both EAT and lung were extracted for the severity prediction. In a derivation cohort (290, cohort1), univariate analysis and Pearson correlation analysis were used to identify predictors of the severity of COVID-19. A generalized linear regression model for detecting the severity of COVID-19 was built in a derivation cohort and evaluated in internal (125, cohort1) and external (100, cohort2) validation cohorts. Results: For EAT extraction, the Dice similarity coefficients (DSC) of the two centers were 0.972 (0.011) and 0.968 (0.005), respectively. For severity detection, the AUC, net reclassification improvement (NRI), and integrated discrimination improvement (IDI) of the model with radiomics features of both lung and EAT increased by 0.09 (p<0.001), 22.4%, and 17.0%, respectively, compared with the model with lung radiomics features, in the internal validation cohort. The AUC, NRI, and IDI increased by 0.04 (p<0.001), 11.1%, and 8.0%, respectively, in the external validation cohort. Conclusion: Radiomics features of EAT combined with lung have incremental value in detecting the severity of COVID-19.



### Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.12352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12352v1)
- **Published**: 2023-01-29 04:12:44+00:00
- **Updated**: 2023-01-29 04:12:44+00:00
- **Authors**: Jialin Yuan, Jay Patravali, Hung Nguyen, Chanho Kim, Li Fuxin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Video Object Segmentation (UVOS) aims at discovering objects and tracking them through videos. For accurate UVOS, we observe if one can locate precise segment proposals on key frames, subsequent processes are much simpler. Hence, we propose to reason about key frame proposals using a graph built with the object probability masks initially generated from multiple frames around the key frame and then propagated to the key frame. On this graph, we compute maximal cliques, with each clique representing one candidate object. By making multiple proposals in the clique to vote for the key frame proposal, we obtain refined key frame proposals that could be better than any of the single-frame proposals. A semi-supervised VOS algorithm subsequently tracks these key frame proposals to the entire video. Our algorithm is modular and hence can be used with any instance segmentation and semi-supervised VOS algorithm. We achieve state-of-the-art performance on the DAVIS-2017 validation and test-dev dataset. On the related problem of video instance segmentation, our method shows competitive performance with the previous best algorithm that requires joint training with the VOS algorithm.



### Exploiting High Performance Spiking Neural Networks with Efficient Spiking Patterns
- **Arxiv ID**: http://arxiv.org/abs/2301.12356v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12356v1)
- **Published**: 2023-01-29 04:22:07+00:00
- **Updated**: 2023-01-29 04:22:07+00:00
- **Authors**: Guobin Shen, Dongcheng Zhao, Yi Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) use discrete spike sequences to transmit information, which significantly mimics the information transmission of the brain. Although this binarized form of representation dramatically enhances the energy efficiency and robustness of SNNs, it also leaves a large gap between the performance of SNNs and Artificial Neural Networks based on real values. There are many different spike patterns in the brain, and the dynamic synergy of these spike patterns greatly enriches the representation capability. Inspired by spike patterns in biological neurons, this paper introduces the dynamic Burst pattern and designs the Leaky Integrate and Fire or Burst (LIFB) neuron that can make a trade-off between short-time performance and dynamic temporal performance from the perspective of network information capacity. LIFB neuron exhibits three modes, resting, Regular spike, and Burst spike. The burst density of the neuron can be adaptively adjusted, which significantly enriches the characterization capability. We also propose a decoupling method that can losslessly decouple LIFB neurons into equivalent LIF neurons, which demonstrates that LIFB neurons can be efficiently implemented on neuromorphic hardware. We conducted experiments on the static datasets CIFAR10, CIFAR100, and ImageNet, which showed that we greatly improved the performance of the SNNs while significantly reducing the network latency. We also conducted experiments on neuromorphic datasets DVS-CIFAR10 and NCALTECH101 and showed that we achieved state-of-the-art with a small network structure.



### Towards Inference Efficient Deep Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.12378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12378v1)
- **Published**: 2023-01-29 06:48:53+00:00
- **Updated**: 2023-01-29 06:48:53+00:00
- **Authors**: Ziyue Li, Kan Ren, Yifan Yang, Xinyang Jiang, Yuqing Yang, Dongsheng Li
- **Comment**: 11 pages, accepted in AAAI 2023
- **Journal**: None
- **Summary**: Ensemble methods can deliver surprising performance gains but also bring significantly higher computational costs, e.g., can be up to 2048X in large-scale ensemble tasks. However, we found that the majority of computations in ensemble methods are redundant. For instance, over 77% of samples in CIFAR-100 dataset can be correctly classified with only a single ResNet-18 model, which indicates that only around 23% of the samples need an ensemble of extra models. To this end, we propose an inference efficient ensemble learning method, to simultaneously optimize for effectiveness and efficiency in ensemble learning. More specifically, we regard ensemble of models as a sequential inference process and learn the optimal halting event for inference on a specific sample. At each timestep of the inference process, a common selector judges if the current ensemble has reached ensemble effectiveness and halt further inference, otherwise filters this challenging sample for the subsequent models to conduct more powerful ensemble. Both the base models and common selector are jointly optimized to dynamically adjust ensemble inference for different samples with various hardness, through the novel optimization goals including sequential ensemble boosting and computation saving. The experiments with different backbones on real-world datasets illustrate our method can bring up to 56\% inference cost reduction while maintaining comparable performance to full ensemble, achieving significantly better ensemble utility than other baselines. Code and supplemental materials are available at https://seqml.github.io/irene.



### Deep Learning for Human Parsing: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2301.12416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12416v1)
- **Published**: 2023-01-29 10:54:56+00:00
- **Updated**: 2023-01-29 10:54:56+00:00
- **Authors**: Xiaomei Zhang, Xiangyu Zhu, Ming Tang, Zhen Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Human parsing is a key topic in image processing with many applications, such as surveillance analysis, human-robot interaction, person search, and clothing category classification, among many others. Recently, due to the success of deep learning in computer vision, there are a number of works aimed at developing human parsing algorithms using deep learning models. As methods have been proposed, a comprehensive survey of this topic is of great importance. In this survey, we provide an analysis of state-of-the-art human parsing methods, covering a broad spectrum of pioneering works for semantic human parsing. We introduce five insightful categories: (1) structure-driven architectures exploit the relationship of different human parts and the inherent hierarchical structure of a human body, (2) graph-based networks capture the global information to achieve an efficient and complete human body analysis, (3) context-aware networks explore useful contexts across all pixel to characterize a pixel of the corresponding class, (4) LSTM-based methods can combine short-distance and long-distance spatial dependencies to better exploit abundant local and global contexts, and (5) combined auxiliary information approaches use related tasks or supervision to improve network performance. We also discuss the advantages/disadvantages of the methods in each category and the relationships between methods in different categories, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.



### Debiased Fine-Tuning for Vision-language Models by Prompt Regularization
- **Arxiv ID**: http://arxiv.org/abs/2301.12429v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12429v2)
- **Published**: 2023-01-29 11:53:55+00:00
- **Updated**: 2023-03-31 07:05:35+00:00
- **Authors**: Beier Zhu, Yulei Niu, Saeil Lee, Minhoe Hur, Hanwang Zhang
- **Comment**: AAAI2023 accepted
- **Journal**: None
- **Summary**: We present a new paradigm for fine-tuning large-scale visionlanguage pre-trained models on downstream task, dubbed Prompt Regularization (ProReg). Different from traditional fine-tuning which easily overfits to the downstream task data, ProReg uses the prediction by prompting the pretrained model to regularize the fine-tuning. The motivation is: by prompting the large model "a photo of a [CLASS]", the fil-lin answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased. Specifically, given a training sample prediction during fine-tuning, we first calculate its KullbackLeibler loss of the prompt prediction and Cross-Entropy loss of the ground-truth label, and then combine them with a proposed sample-wise adaptive trade-off weight, which automatically adjusts the transfer between the pretrained and downstream domains. On various out-of-distribution benchmarks, we show the consistently strong performance of ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning, and other state-of-the-art methods.



### Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022
- **Arxiv ID**: http://arxiv.org/abs/2301.12436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12436v1)
- **Published**: 2023-01-29 12:29:24+00:00
- **Updated**: 2023-01-29 12:29:24+00:00
- **Authors**: Yi Cheng, Dongyun Lin, Fen Fang, Hao Xuan Woon, Qianli Xu, Ying Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we present the technical details of our submission to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation (UDA) Challenge for Action Recognition 2022. This task aims to adapt an action recognition model trained on a labeled source domain to an unlabeled target domain. To achieve this goal, we propose an action-aware domain adaptation framework that leverages the prior knowledge induced from the action recognition task during the adaptation. Specifically, we disentangle the source features into action-relevant features and action-irrelevant features using the learned action classifier and then align the target features with the action-relevant features. To further improve the action prediction performance, we exploit the verb-noun co-occurrence matrix to constrain and refine the action predictions. Our final submission achieved the first place in terms of top-1 action recognition accuracy.



### Unsupervised Domain Adaptation on Person Re-Identification via Dual-level Asymmetric Mutual Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.12439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12439v1)
- **Published**: 2023-01-29 12:36:17+00:00
- **Updated**: 2023-01-29 12:36:17+00:00
- **Authors**: Qiong Wu, Jiahan Li, Pingyang Dai, Qixiang Ye, Liujuan Cao, Yongjian Wu, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation person re-identification (Re-ID) aims to identify pedestrian images within an unlabeled target domain with an auxiliary labeled source-domain dataset. Many existing works attempt to recover reliable identity information by considering multiple homogeneous networks. And take these generated labels to train the model in the target domain. However, these homogeneous networks identify people in approximate subspaces and equally exchange their knowledge with others or their mean net to improve their ability, inevitably limiting the scope of available knowledge and putting them into the same mistake. This paper proposes a Dual-level Asymmetric Mutual Learning method (DAML) to learn discriminative representations from a broader knowledge scope with diverse embedding spaces. Specifically, two heterogeneous networks mutually learn knowledge from asymmetric subspaces through the pseudo label generation in a hard distillation manner. The knowledge transfer between two networks is based on an asymmetric mutual learning manner. The teacher network learns to identify both the target and source domain while adapting to the target domain distribution based on the knowledge of the student. Meanwhile, the student network is trained on the target dataset and employs the ground-truth label through the knowledge of the teacher. Extensive experiments in Market-1501, CUHK-SYSU, and MSMT17 public datasets verified the superiority of DAML over state-of-the-arts.



### Towards Verifying the Geometric Robustness of Large-scale Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.12456v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12456v2)
- **Published**: 2023-01-29 14:44:27+00:00
- **Updated**: 2023-03-30 21:20:35+00:00
- **Authors**: Fu Wang, Peipei Xu, Wenjie Ruan, Xiaowei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to adversarial geometric transformation. This paper aims to verify the robustness of large-scale DNNs against the combination of multiple geometric transformations with a provable guarantee. Given a set of transformations (e.g., rotation, scaling, etc.), we develop GeoRobust, a black-box robustness analyser built upon a novel global optimisation strategy, for locating the worst-case combination of transformations that affect and even alter a network's output. GeoRobust can provide provable guarantees on finding the worst-case combination based on recent advances in Lipschitzian theory. Due to its black-box nature, GeoRobust can be deployed on large-scale DNNs regardless of their architectures, activation functions, and the number of neurons. In practice, GeoRobust can locate the worst-case geometric transformation with high precision for the ResNet50 model on ImageNet in a few seconds on average. We examined 18 ImageNet classifiers, including the ResNet family and vision transformers, and found a positive correlation between the geometric robustness of the networks and the parameter numbers. We also observe that increasing the depth of DNN is more beneficial than increasing its width in terms of improving its geometric robustness. Our tool GeoRobust is available at https://github.com/TrustAI/GeoRobust.



### The Influences of Color and Shape Features in Visual Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.12459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12459v1)
- **Published**: 2023-01-29 15:10:14+00:00
- **Updated**: 2023-01-29 15:10:14+00:00
- **Authors**: Xiaoqi Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of visual representation learning, performance of contrastive learning has been catching up with the supervised method which is commonly a classification convolutional neural network. However, most of the research work focuses on improving the accuracy of downstream tasks such as image classification and object detection. For visual contrastive learning, the influences of individual image features (e.g., color and shape) to model performance remain ambiguous.   This paper investigates such influences by designing various ablation experiments, the results of which are evaluated by specifically designed metrics. While these metrics are not invented by us, we first use them in the field of representation evaluation. Specifically, we assess the contribution of two primary image features (i.e., color and shape) in a quantitative way. Experimental results show that compared with supervised representations, contrastive representations tend to cluster with objects of similar color in the representation space, and contain less shape information than supervised representations. Finally, we discuss that the current data augmentation is responsible for these results. We believe that exploring an unsupervised augmentation method that



### Gesture Control of Micro-drone: A Lightweight-Net with Domain Randomization and Trajectory Generators
- **Arxiv ID**: http://arxiv.org/abs/2301.12470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12470v1)
- **Published**: 2023-01-29 15:38:15+00:00
- **Updated**: 2023-01-29 15:38:15+00:00
- **Authors**: Isaac Osei Agyemang, Isaac Adjei Mensah, Sophyani Banaamwini Yussif, Fiasam Linda Delali, Bernard Cobinnah Mawuli, Bless Lord Y. Agbley, Collins Sey, Joshua Berkohd
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-drones can be integrated into various industrial applications but are constrained by their computing power and expert pilots, a secondary challenge. This study presents a computationally-efficient deep convolutional neural network that utilizes Gabor filters and spatial separable convolutions with low computational complexities. An attention module is integrated with the model to complement the performance. Further, perception-based action space and trajectory generators are integrated with the model's predictions for intuitive navigation. The computationally-efficient model aids a human operator in controlling a micro-drone via gestures. Nearly 18% of computational resources are conserved using the NVIDIA GPU profiler during training. Using a low-cost DJI Tello drone for experiment verification, the computationally-efficient model shows promising results compared to a state-of-the-art and conventional computer vision-based technique.



### Multi-video Moment Ranking with Multimodal Clue
- **Arxiv ID**: http://arxiv.org/abs/2301.13606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13606v1)
- **Published**: 2023-01-29 18:38:13+00:00
- **Updated**: 2023-01-29 18:38:13+00:00
- **Authors**: Danyang Hou, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng
- **Comment**: 9 pages,6 figures
- **Journal**: None
- **Summary**: Video corpus moment retrieval~(VCMR) is the task of retrieving a relevant video moment from a large corpus of untrimmed videos via a natural language query. State-of-the-art work for VCMR is based on two-stage method. In this paper, we focus on improving two problems of two-stage method: (1) Moment prediction bias: The predicted moments for most queries come from the top retrieved videos, ignoring the possibility that the target moment is in the bottom retrieved videos, which is caused by the inconsistency of Shared Normalization during training and inference. (2) Latent key content: Different modalities of video have different key information for moment localization. To this end, we propose a two-stage model \textbf{M}ult\textbf{I}-video ra\textbf{N}king with m\textbf{U}l\textbf{T}imodal clu\textbf{E}~(MINUTE). MINUTE uses Shared Normalization during both training and inference to rank candidate moments from multiple videos to solve moment predict bias, making it more efficient to predict target moment. In addition, Mutilmdaol Clue Mining~(MCM) of MINUTE can discover key content of different modalities in video to localize moment more accurately. MINUTE outperforms the baselines on TVR and DiDeMo datasets, achieving a new state-of-the-art of VCMR. Our code will be available at GitHub.



### Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline
- **Arxiv ID**: http://arxiv.org/abs/2301.12511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12511v1)
- **Published**: 2023-01-29 18:43:31+00:00
- **Updated**: 2023-01-29 18:43:31+00:00
- **Authors**: Yangguang Li, Bin Huang, Zeren Chen, Yufeng Cui, Feng Liang, Mingzhu Shen, Fenggang Liu, Enze Xie, Lu Sheng, Wanli Ouyang, Jing Shao
- **Comment**: submitted to TPAMI. arXiv admin note: substantial text overlap with
  arXiv:2301.07870
- **Journal**: None
- **Summary**: Recently, perception task based on Bird's-Eye View (BEV) representation has drawn more and more attention, and BEV representation is promising as the foundation for next-generation Autonomous Vehicle (AV) perception. However, most existing BEV solutions either require considerable resources to execute on-vehicle inference or suffer from modest performance. This paper proposes a simple yet effective framework, termed Fast-BEV , which is capable of performing faster BEV perception on the on-vehicle chips. Towards this goal, we first empirically find that the BEV representation can be sufficiently powerful without expensive transformer based transformation nor depth representation. Our Fast-BEV consists of five parts, We novelly propose (1) a lightweight deployment-friendly view transformation which fast transfers 2D image feature to 3D voxel space, (2) an multi-scale image encoder which leverages multi-scale information for better performance, (3) an efficient BEV encoder which is particularly designed to speed up on-vehicle inference. We further introduce (4) a strong data augmentation strategy for both image and BEV space to avoid over-fitting, (5) a multi-frame feature fusion mechanism to leverage the temporal information. Through experiments, on 2080Ti platform, our R50 model can run 52.6 FPS with 47.3% NDS on the nuScenes validation set, exceeding the 41.3 FPS and 47.5% NDS of the BEVDepth-R50 model and 30.2 FPS and 45.7% NDS of the BEVDet4D-R50 model. Our largest model (R101@900x1600) establishes a competitive 53.5% NDS on the nuScenes validation set. We further develop a benchmark with considerable accuracy and efficiency on current popular on-vehicle chips. The code is released at: https://github.com/Sense-GVT/Fast-BEV.



### LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.12515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12515v1)
- **Published**: 2023-01-29 19:10:35+00:00
- **Updated**: 2023-01-29 19:10:35+00:00
- **Authors**: Jin Fang, Dingfu Zhou, Jingjing Zhao, Chulin Tang, Cheng-Zhong Xu, Liangjun Zhang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: LiDAR devices are widely used in autonomous driving scenarios and researches on 3D point cloud achieve remarkable progress over the past years. However, deep learning-based methods heavily rely on the annotation data and often face the domain generalization problem. Unlike 2D images whose domains are usually related to the texture information, the feature extracted from the 3D point cloud is affected by the distribution of the points. Due to the lack of a 3D domain adaptation benchmark, the common practice is to train the model on one benchmark (e.g, Waymo) and evaluate it on another dataset (e.g. KITTI). However, in this setting, there are two types of domain gaps, the scenarios domain, and sensors domain, making the evaluation and analysis complicated and difficult. To handle this situation, we propose LiDAR Dataset with Cross-Sensors (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under 6 groups of different sensors but with same corresponding scenarios, captured from hybrid realistic LiDAR simulator. As far as we know, LiDAR-CS Dataset is the first dataset focused on the sensor (e.g., the points distribution) domain gaps for 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance with several baseline detectors on the LiDAR-CS benchmark and show its applications.



### 3D Object Detection in LiDAR Point Clouds using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.12519v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.12519v2)
- **Published**: 2023-01-29 19:23:01+00:00
- **Updated**: 2023-02-08 06:11:07+00:00
- **Authors**: Shreelakshmi C R, Surya S. Durbha, Gaganpreet Singh
- **Comment**: Errors in the results section. Experiments are carried out to rectify
  the results
- **Journal**: None
- **Summary**: LiDAR (Light Detection and Ranging) is an advanced active remote sensing technique working on the principle of time of travel (ToT) for capturing highly accurate 3D information of the surroundings. LiDAR has gained wide attention in research and development with the LiDAR industry expected to reach 2.8 billion $ by 2025. Although the LiDAR dataset is of rich density and high spatial resolution, it is challenging to process LiDAR data due to its inherent 3D geometry and massive volume. But such a high-resolution dataset possesses immense potential in many applications and has great potential in 3D object detection and recognition. In this research we propose Graph Neural Network (GNN) based framework to learn and identify the objects in the 3D LiDAR point clouds. GNNs are class of deep learning which learns the patterns and objects based on the principle of graph learning which have shown success in various 3D computer vision tasks.



### Diverse, Difficult, and Odd Instances (D2O): A New Test Set for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.12527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.12527v1)
- **Published**: 2023-01-29 19:58:32+00:00
- **Updated**: 2023-01-29 19:58:32+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Test sets are an integral part of evaluating models and gauging progress in object recognition, and more broadly in computer vision and AI. Existing test sets for object recognition, however, suffer from shortcomings such as bias towards the ImageNet characteristics and idiosyncrasies (e.g., ImageNet-V2), being limited to certain types of stimuli (e.g., indoor scenes in ObjectNet), and underestimating the model performance (e.g., ImageNet-A). To mitigate these problems, we introduce a new test set, called D2O, which is sufficiently different from existing test sets. Images are a mix of generated images as well as images crawled from the web. They are diverse, unmodified, and representative of real-world scenarios and cause state-of-the-art models to misclassify them with high confidence. To emphasize generalization, our dataset by design does not come paired with a training set. It contains 8,060 images spread across 36 categories, out of which 29 appear in ImageNet. The best Top-1 accuracy on our dataset is around 60% which is much lower than 91% best Top-1 accuracy on ImageNet. We find that popular vision APIs perform very poorly in detecting objects over D2O categories such as ``faces'', ``cars'', and ``cats''. Our dataset also comes with a ``miscellaneous'' category, over which we test the image tagging models. Overall, our investigations demonstrate that the D2O test set contain a mix of images with varied levels of difficulty and is predictive of the average-case performance of models. It can challenge object recognition models for years to come and can spur more research in this fundamental area.



### PhyCV: The First Physics-inspired Computer Vision Library
- **Arxiv ID**: http://arxiv.org/abs/2301.12531v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12531v2)
- **Published**: 2023-01-29 20:16:15+00:00
- **Updated**: 2023-04-27 17:51:30+00:00
- **Authors**: Yiming Zhou, Callen MacPhee, Madhuri Suthar, Bahram Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: PhyCV is the first computer vision library which utilizes algorithms directly derived from the equations of physics governing physical phenomena. The algorithms appearing in the current release emulate, in a metaphoric sense, the propagation of light through a physical medium with natural and engineered diffractive properties followed by coherent detection. Unlike traditional algorithms that are a sequence of hand-crafted empirical rules or deep learning algorithms that are usually data-driven and computationally heavy, physics-inspired algorithms leverage physical laws of nature as blueprints for inventing algorithms. PhyCV features low-dimensionality and high- efficiency, making it ideal for edge computing applications. We demonstrate real-time video processing on NVIDIA Jetson Nano using PhyCV. In addition, these algorithms have the potential to be implemented in real physical devices for fast and efficient computation in the form of analog computing. The open-sourced code is available at https://github.com/JalaliLabUCLA/phycv



### Supervised and Contrastive Self-Supervised In-Domain Representation Learning for Dense Prediction Problems in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2301.12541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12541v1)
- **Published**: 2023-01-29 20:56:51+00:00
- **Updated**: 2023-01-29 20:56:51+00:00
- **Authors**: Ali Ghanbarzade, Dr. Hossein Soleimani
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years Convolutional neural networks (CNN) have made significant progress in computer vision. These advancements have been applied to other areas, such as remote sensing and have shown satisfactory results. However, the lack of large labeled datasets and the inherent complexity of remote sensing problems have made it difficult to train deep CNNs for dense prediction problems. To solve this issue, ImageNet pretrained weights have been used as a starting point in various dense predictions tasks. Although this type of transfer learning has led to improvements, the domain difference between natural and remote sensing images has also limited the performance of deep CNNs. On the other hand, self-supervised learning methods for learning visual representations from large unlabeled images have grown substantially over the past two years. Accordingly, in this paper we have explored the effectiveness of in-domain representations in both supervised and self-supervised forms to solve the domain difference between remote sensing and the ImageNet dataset. The obtained weights from remote sensing images are utilized as initial weights for solving semantic segmentation and object detection tasks and state-of-the-art results are obtained. For self-supervised pre-training, we have utilized the SimSiam algorithm as it is simple and does not need huge computational resources. One of the most influential factors in acquiring general visual representations from remote sensing images is the pre-training dataset. To examine the effect of the pre-training dataset, equal-sized remote sensing datasets are used for pre-training. Our results have demonstrated that using datasets with a high spatial resolution for self-supervised representation learning leads to high performance in downstream tasks.



### Scaling in Depth: Unlocking Robustness Certification on ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2301.12549v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.12549v2)
- **Published**: 2023-01-29 21:40:04+00:00
- **Updated**: 2023-06-02 17:39:16+00:00
- **Authors**: Kai Hu, Andy Zou, Zifan Wang, Klas Leino, Matt Fredrikson
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \emph{Linear ResNet} (LiResNet) architecture. We then introduce \emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \emph{all} classes. Together, these contributions yield new \emph{state-of-the-art} robust accuracy on CIFAR-10/100 and Tiny-ImageNet under $\ell_2$ perturbations. Moreover, for the first time, we are able to scale up fast deterministic robustness guarantees to ImageNet, demonstrating that this approach to robust learning can be applied to real-world applications.   We release our code on Github: \url{https://github.com/klasleino/gloro}.



### Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2301.12554v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2301.12554v2)
- **Published**: 2023-01-29 22:05:28+00:00
- **Updated**: 2023-05-23 15:48:21+00:00
- **Authors**: Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi
- **Comment**: None
- **Journal**: None
- **Summary**: While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The proposed flexible method, termed "adaptive smoothing", can work in conjunction with existing or even future methods that improve clean accuracy, robustness, or adversary detection. Our empirical evaluation considers strong attack methods, including AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves an 85.21% clean accuracy while maintaining a 38.72% $\ell_\infty$-AutoAttacked ($\epsilon$=8/255) accuracy, becoming the second most robust method on the RobustBench CIFAR-100 benchmark as of submission, while improving the clean accuracy by ten percentage points compared with all listed models. The code that implements our method is available at https://github.com/Bai-YT/AdaptiveSmoothing.



### Development of Machine learning algorithms to identify the Cobb angle in adolescents with idiopathic scoliosis based on lumbosacral joint efforts during gait (Case study)
- **Arxiv ID**: http://arxiv.org/abs/2301.12588v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.12588v1)
- **Published**: 2023-01-29 23:58:16+00:00
- **Updated**: 2023-01-29 23:58:16+00:00
- **Authors**: Bahare Samadi, Maxime Raison, Philippe Mahaudens, Christine Detrembleur, Sofiane Achiche
- **Comment**: 30 pages, 2 Figures, 4 tables
- **Journal**: None
- **Summary**: Objectives: To quantify the magnitude of spinal deformity in adolescent idiopathic scoliosis (AIS), the Cobb angle is measured on X-ray images of the spine. Continuous exposure to X-ray radiation to follow-up the progression of scoliosis may lead to negative side effects on patients. Furthermore, manual measurement of the Cobb angle could lead to up to 10{\deg} or more of a difference due to intra/inter observer variation. Therefore, the objective of this study is to identify the Cobb angle by developing an automated radiation-free model, using Machine learning algorithms. Methods: Thirty participants with lumbar/thoracolumbar AIS (15{\deg} < Cobb angle < 66{\deg}) performed gait cycles. The lumbosacral (L5-S1) joint efforts during six gait cycles of participants were used as features to feed training algorithms. Various regression algorithms were implemented and run. Results: The decision tree regression algorithm achieved the best result with the mean absolute error equal to 4.6{\deg} of averaged 10-fold cross-validation. Conclusions: This study shows that the lumbosacral joint efforts during gait as radiation-free data are capable to identify the Cobb angle by using Machine learning algorithms. The proposed model can be considered as an alternative, radiation-free method to X-ray radiography to assist clinicians in following-up the progression of AIS.



### Confidence-Aware Calibration and Scoring Functions for Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.12589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.12589v1)
- **Published**: 2023-01-29 23:59:40+00:00
- **Updated**: 2023-01-29 23:59:40+00:00
- **Authors**: Shuang Ao, Stefan Rueger, Advaith Siddharthan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success of state-of-the-art deep neural networks, several studies have reported models to be over-confident in predictions, indicating miscalibration. Label Smoothing has been proposed as a solution to the over-confidence problem and works by softening hard targets during training, typically by distributing part of the probability mass from a `one-hot' label uniformly to all other labels. However, neither model nor human confidence in a label are likely to be uniformly distributed in this manner, with some labels more likely to be confused than others. In this paper we integrate notions of model confidence and human confidence with label smoothing, respectively \textit{Model Confidence LS} and \textit{Human Confidence LS}, to achieve better model calibration and generalization. To enhance model generalization, we show how our model and human confidence scores can be successfully applied to curriculum learning, a training strategy inspired by learning of `easier to harder' tasks. A higher model or human confidence score indicates a more recognisable and therefore easier sample, and can therefore be used as a scoring function to rank samples in curriculum learning. We evaluate our proposed methods with four state-of-the-art architectures for image and text classification task, using datasets with multi-rater label annotations by humans. We report that integrating model or human confidence information in label smoothing and curriculum learning improves both model performance and model calibration. The code are available at \url{https://github.com/AoShuang92/Confidence_Calibration_CL}.



