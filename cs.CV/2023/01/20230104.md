# Arxiv Papers in cs.CV on 2023-01-04
### Object Segmentation with Audio Context
- **Arxiv ID**: http://arxiv.org/abs/2301.10295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.10295v1)
- **Published**: 2023-01-04 01:33:42+00:00
- **Updated**: 2023-01-04 01:33:42+00:00
- **Authors**: Kaihui Zheng, Yuqing Ren, Zixin Shen, Tianxu Qin
- **Comment**: Research project for Introduction to Deep Learning (11785) at
  Carnegie Mellon University
- **Journal**: None
- **Summary**: Visual objects often have acoustic signatures that are naturally synchronized with them in audio-bearing video recordings. For this project, we explore the multimodal feature aggregation for video instance segmentation task, in which we integrate audio features into our video segmentation model to conduct an audio-visual learning scheme. Our method is based on existing video instance segmentation method which leverages rich contextual information across video frames. Since this is the first attempt to investigate the audio-visual instance segmentation, a novel dataset, including 20 vocal classes with synchronized video and audio recordings, is collected. By utilizing combined decoder to fuse both video and audio features, our model shows a slight improvements compared to the base model. Additionally, we managed to show the effectiveness of different modules by conducting extensive ablations.



### Attribute-Centric Compositional Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.01413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01413v1)
- **Published**: 2023-01-04 03:03:08+00:00
- **Updated**: 2023-01-04 03:03:08+00:00
- **Authors**: Yuren Cong, Martin Renqiang Min, Li Erran Li, Bodo Rosenhahn, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent impressive breakthroughs in text-to-image generation, generative models have difficulty in capturing the data distribution of underrepresented attribute compositions while over-memorizing overrepresented attribute compositions, which raises public concerns about their robustness and fairness. To tackle this challenge, we propose ACTIG, an attribute-centric compositional text-to-image generation framework. We present an attribute-centric feature augmentation and a novel image-free training scheme, which greatly improves model's ability to generate images with underrepresented attributes. We further propose an attribute-centric contrastive loss to avoid overfitting to overrepresented attribute compositions. We validate our framework on the CelebA-HQ and CUB datasets. Extensive experiments show that the compositional generalization of ACTIG is outstanding, and our framework outperforms previous works in terms of image quality and text-image consistency.



### Scene Synthesis from Human Motion
- **Arxiv ID**: http://arxiv.org/abs/2301.01424v1
- **DOI**: 10.1145/3550469.3555426
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01424v1)
- **Published**: 2023-01-04 03:30:46+00:00
- **Updated**: 2023-01-04 03:30:46+00:00
- **Authors**: Sifan Ye, Yixing Wang, Jiaman Li, Dennis Park, C. Karen Liu, Huazhe Xu, Jiajun Wu
- **Comment**: 9 pages, 8 figures. Published in SIGGRAPH Asia 2022. Sifan Ye and
  Yixing Wang share equal contribution. Huazhe Xu and Jiajun Wu share equal
  contribution
- **Journal**: None
- **Summary**: Large-scale capture of human motion with diverse, complex scenes, while immensely useful, is often considered prohibitively costly. Meanwhile, human motion alone contains rich information about the scene they reside in and interact with. For example, a sitting human suggests the existence of a chair, and their leg position further implies the chair's pose. In this paper, we propose to synthesize diverse, semantically reasonable, and physically plausible scenes based on human motion. Our framework, Scene Synthesis from HUMan MotiON (SUMMON), includes two steps. It first uses ContactFormer, our newly introduced contact predictor, to obtain temporally consistent contact labels from human motion. Based on these predictions, SUMMON then chooses interacting objects and optimizes physical plausibility losses; it further populates the scene with objects that do not interact with humans. Experimental results demonstrate that SUMMON synthesizes feasible, plausible, and diverse scenes and has the potential to generate extensive human-scene interaction data for the community.



### Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.01431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01431v1)
- **Published**: 2023-01-04 03:59:17+00:00
- **Updated**: 2023-01-04 03:59:17+00:00
- **Authors**: Haojie Yu, Kang Zhao, Xiaoming Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformer (ViT) suffers from data scarcity in semi-supervised learning (SSL). To alleviate this issue, inspired by masked autoencoder (MAE), which is a data-efficient self-supervised learner, we propose Semi-MAE, a pure ViT-based SSL framework consisting of a parallel MAE branch to assist the visual representation learning and make the pseudo labels more accurate. The MAE branch is designed as an asymmetric architecture consisting of a lightweight decoder and a shared-weights encoder. We feed the weakly-augmented unlabeled data with a high masking ratio to the MAE branch and reconstruct the missing pixels. Semi-MAE achieves 75.9% top-1 accuracy on ImageNet with 10% labels, surpassing prior state-of-the-art in semi-supervised image classification. In addition, extensive experiments demonstrate that Semi-MAE can be readily used for other ViT models and masked image modeling methods.



### Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.01441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.01441v1)
- **Published**: 2023-01-04 04:20:08+00:00
- **Updated**: 2023-01-04 04:20:08+00:00
- **Authors**: Hao Chen, Weiwei Wan, Masaki Matsushita, Takeyuki Kotaka, Kensuke Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have recently exhibited impressive performance in object detection. However, such methods needed much training data to achieve high recognition accuracy, which was time-consuming and required considerable manual work like labeling images. In this paper, we automatically prepare training data using robots. Considering the low efficiency and high energy consumption in robot motion, we proposed combining robotic in-hand observation and data synthesis to enlarge the limited data set collected by the robot. We first used a robot with a depth sensor to collect images of objects held in the robot's hands and segment the object pictures. Then, we used a copy-paste method to synthesize the segmented objects with rack backgrounds. The collected and synthetic images are combined to train a deep detection neural network. We conducted experiments to compare YOLOv5x detectors trained with images collected using the proposed method and several other methods. The results showed that combined observation and synthetic images led to comparable performance to manual data preparation. They provided a good guide on optimizing data configurations and parameter settings for training detectors. The proposed method required only a single process and was a low-cost way to produce the combined data. Interested readers may find the data sets and trained models from the following GitHub repository: github.com/wrslab/tubedet



### A deep local attention network for pre-operative lymph node metastasis prediction in pancreatic cancer via multiphase CT imaging
- **Arxiv ID**: http://arxiv.org/abs/2301.01448v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01448v1)
- **Published**: 2023-01-04 05:14:31+00:00
- **Updated**: 2023-01-04 05:14:31+00:00
- **Authors**: Zhilin Zheng, Xu Fang, Jiawen Yao, Mengmeng Zhu, Le Lu, Lingyun Huang, Jing Xiao, Yu Shi, Hong Lu, Jianping Lu, Ling Zhang, Chengwei Shao, Yun Bian
- **Comment**: 14 pages,5 figures
- **Journal**: None
- **Summary**: Lymph node (LN) metastasis status is one of the most critical prognostic and cancer staging factors for patients with resectable pancreatic ductal adenocarcinoma (PDAC), or in general, for any types of solid malignant tumors. Preoperative prediction of LN metastasis from non-invasive CT imaging is highly desired, as it might be straightforwardly used to guide the following neoadjuvant treatment decision and surgical planning. Most studies only capture the tumor characteristics in CT imaging to implicitly infer LN metastasis and very few work exploit direct LN's CT imaging information. To the best of our knowledge, this is the first work to propose a fully-automated LN segmentation and identification network to directly facilitate the LN metastasis status prediction task. Nevertheless LN segmentation/detection is very challenging since LN can be easily confused with other hard negative anatomic structures (e.g., vessels) from radiological images. We explore the anatomical spatial context priors of pancreatic LN locations by generating a guiding attention map from related organs and vessels to assist segmentation and infer LN status. As such, LN segmentation is impelled to focus on regions that are anatomically adjacent or plausible with respect to the specific organs and vessels. The metastasized LN identification network is trained to classify the segmented LN instances into positives or negatives by reusing the segmentation network as a pre-trained backbone and padding a new classification head. More importantly, we develop a LN metastasis status prediction network that combines the patient-wise aggregation results of LN segmentation/identification and deep imaging features extracted from the tumor region. Extensive quantitative nested five-fold cross-validation is conducted on a discovery dataset of 749 patients with PDAC.



### Building Coverage Estimation with Low-resolution Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2301.01449v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01449v2)
- **Published**: 2023-01-04 05:19:33+00:00
- **Updated**: 2023-01-05 04:39:49+00:00
- **Authors**: Enci Liu, Chenlin Meng, Matthew Kolodner, Eun Jee Sung, Sihang Chen, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Building coverage statistics provide crucial insights into the urbanization, infrastructure, and poverty level of a region, facilitating efforts towards alleviating poverty, building sustainable cities, and allocating infrastructure investments and public service provision. Global mapping of buildings has been made more efficient with the incorporation of deep learning models into the pipeline. However, these models typically rely on high-resolution satellite imagery which are expensive to collect and infrequently updated. As a result, building coverage data are not updated timely especially in developing regions where the built environment is changing quickly. In this paper, we propose a method for estimating building coverage using only publicly available low-resolution satellite imagery that is more frequently updated. We show that having a multi-node quantile regression layer greatly improves the model's spatial and temporal generalization. Our model achieves a coefficient of determination ($R^2$) as high as 0.968 on predicting building coverage in regions of different levels of development around the world. We demonstrate that the proposed model accurately predicts the building coverage from raw input images and generalizes well to unseen countries and continents, suggesting the possibility of estimating global building coverage using only low-resolution remote sensing data.



### Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA
- **Arxiv ID**: http://arxiv.org/abs/2301.01454v1
- **DOI**: 10.1109/FPL57034.2022.00013
- **Categories**: **cs.AR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01454v1)
- **Published**: 2023-01-04 05:35:30+00:00
- **Updated**: 2023-01-04 05:35:30+00:00
- **Authors**: Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic aperture radar (SAR) automatic target recognition (ATR) is the key technique for remote-sensing image recognition. The state-of-the-art convolutional neural networks (CNNs) for SAR ATR suffer from \emph{high computation cost} and \emph{large memory footprint}, making them unsuitable to be deployed on resource-limited platforms, such as small/micro satellites. In this paper, we propose a comprehensive GNN-based model-architecture {co-design} on FPGA to address the above issues. \emph{Model design}: we design a novel graph neural network (GNN) for SAR ATR. The proposed GNN model incorporates GraphSAGE layer operators and attention mechanism, achieving comparable accuracy as the state-of-the-art work with near $1/100$ computation cost. Then, we propose a pruning approach including weight pruning and input pruning. While weight pruning through lasso regression reduces most parameters without accuracy drop, input pruning eliminates most input pixels with negligible accuracy drop. \emph{Architecture design}: to fully unleash the computation parallelism within the proposed model, we develop a novel unified hardware architecture that can execute various computation kernels (feature aggregation, feature transformation, graph pooling). The proposed hardware design adopts the Scatter-Gather paradigm to efficiently handle the irregular computation {patterns} of various computation kernels. We deploy the proposed design on an embedded FPGA (AMD Xilinx ZCU104) and evaluate the performance using MSTAR dataset. Compared with the state-of-the-art CNNs, the proposed GNN achieves comparable accuracy with $1/3258$ computation cost and $1/83$ model size. Compared with the state-of-the-art CPU/GPU, our FPGA accelerator achieves $14.8\times$/$2.5\times$ speedup (latency) and is $62\times$/$39\times$ more energy efficient.



### Audio-Visual Efficient Conformer for Robust Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.01456v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.01456v1)
- **Published**: 2023-01-04 05:36:56+00:00
- **Updated**: 2023-01-04 05:36:56+00:00
- **Authors**: Maxime Burchi, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end Automatic Speech Recognition (ASR) systems based on neural networks have seen large improvements in recent years. The availability of large scale hand-labeled datasets and sufficient computing resources made it possible to train powerful deep neural networks, reaching very low Word Error Rate (WER) on academic benchmarks. However, despite impressive performance on clean audio samples, a drop of performance is often observed on noisy speech. In this work, we propose to improve the noise robustness of the recently proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based architecture by processing both audio and visual modalities. We improve previous lip reading methods using an Efficient Conformer back-end on top of a ResNet-18 visual front-end and by adding intermediate CTC losses between blocks. We condition intermediate block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC-based models. We also replace the Efficient Conformer grouped attention by a more efficient and simpler attention mechanism that we call patch attention. We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and visual modalities allows to better recognize speech in the presence of environmental noise and significantly accelerate training, reaching lower WER with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC) model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on LRS2 and LRS3 test sets. Code and pretrained models are available at https://github.com/burchim/AVEC.



### Biomedical Image Reconstruction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2301.11813v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11813v1)
- **Published**: 2023-01-04 05:57:39+00:00
- **Updated**: 2023-01-04 05:57:39+00:00
- **Authors**: Samuel Cahyawijaya
- **Comment**: None
- **Journal**: None
- **Summary**: Biomedical image reconstruction research has been developed for more than five decades, giving rise to various techniques such as central and filtered back projection. With the rise of deep learning technology, biomedical image reconstruction field has undergone a massive paradigm shift from analytical and iterative methods to deep learning methods To drive scientific discussion on advanced deep learning techniques for biomedical image reconstruction, a workshop focusing on deep biomedical image reconstruction, MLMIR, is introduced and is being held yearly since 2018. This survey paper is aimed to provide basic knowledge in biomedical image reconstruction and the current research trend in biomedical image reconstruction based on the publications in MLMIR. This survey paper is intended for machine learning researchers to grasp a general understanding of the biomedical image reconstruction field and the current research trend in deep biomedical image reconstruction.



### On Fairness of Medical Image Classification with Multiple Sensitive Attributes via Learning Orthogonal Representations
- **Arxiv ID**: http://arxiv.org/abs/2301.01481v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.01481v3)
- **Published**: 2023-01-04 08:11:11+00:00
- **Updated**: 2023-03-08 22:08:13+00:00
- **Authors**: Wenlong Deng, Yuan Zhong, Qi Dou, Xiaoxiao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Mitigating the discrimination of machine learning models has gained increasing attention in medical image analysis. However, rare works focus on fair treatments for patients with multiple sensitive demographic ones, which is a crucial yet challenging problem for real-world clinical applications. In this paper, we propose a novel method for fair representation learning with respect to multi-sensitive attributes. We pursue the independence between target and multi-sensitive representations by achieving orthogonality in the representation space. Concretely, we enforce the column space orthogonality by keeping target information on the complement of a low-rank sensitive space. Furthermore, in the row space, we encourage feature dimensions between target and sensitive representations to be orthogonal. The effectiveness of the proposed method is demonstrated with extensive experiments on the CheXpert dataset. To our best knowledge, this is the first work to mitigate unfairness with respect to multiple sensitive attributes in the field of medical imaging.



### Underwater Object Tracker: UOSTrack for Marine Organism Grasping of Underwater Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2301.01482v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01482v5)
- **Published**: 2023-01-04 08:22:34+00:00
- **Updated**: 2023-07-24 06:31:58+00:00
- **Authors**: Yunfeng Li, Bo Wang, Ye Li, Zhuoyan Liu, Wei Huo, Yueming Li, Jian Cao
- **Comment**: None
- **Journal**: None
- **Summary**: A visual single-object tracker is an indispensable component of underwater vehicles (UVs) in marine organism grasping tasks. Its accuracy and stability are imperative to guide the UVs to perform grasping behavior. Although single-object trackers show competitive performance in the challenge of underwater image degradation, there are still issues with sample imbalance and exclusion of similar objects that need to be addressed for application in marine organism grasping. This paper proposes Underwater OSTrack (UOSTrack), which consists of underwater image and open-air sequence hybrid training (UOHT), and motion-based post-processing (MBPP). The UOHT training paradigm is designed to train the sample-imbalanced underwater tracker so that the tracker is exposed to a great number of underwater domain training samples and learns the feature expressions. The MBPP paradigm is proposed to exclude similar objects. It uses the estimation box predicted with a Kalman filter and the candidate boxes in the response map to relocate the lost tracked object in the candidate area. UOSTrack achieves an average performance improvement of 4.41% and 7.98% maximum compared to state-of-the-art methods on various benchmarks, respectively. Field experiments have verified the accuracy and stability of our proposed UOSTrack for UVs in marine organism grasping tasks. More details can be found at https://github.com/LiYunfengLYF/UOSTrack.



### Towards a Pipeline for Real-Time Visualization of Faces for VR-based Telepresence and Live Broadcasting Utilizing Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2301.01490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01490v1)
- **Published**: 2023-01-04 08:49:51+00:00
- **Updated**: 2023-01-04 08:49:51+00:00
- **Authors**: Philipp Ladwig, Rene Ebertowski, Alexander Pech, Ralf Dörner, Christian Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: While head-mounted displays (HMDs) for Virtual Reality (VR) have become widely available in the consumer market, they pose a considerable obstacle for a realistic face-to-face conversation in VR since HMDs hide a significant portion of the participants faces. Even with image streams from cameras directly attached to an HMD, stitching together a convincing image of an entire face remains a challenging task because of extreme capture angles and strong lens distortions due to a wide field of view. Compared to the long line of research in VR, reconstruction of faces hidden beneath an HMD is a very recent topic of research. While the current state-of-the-art solutions demonstrate photo-realistic 3D reconstruction results, they require high-cost laboratory equipment and large computational costs. We present an approach that focuses on low-cost hardware and can be used on a commodity gaming computer with a single GPU. We leverage the benefits of an end-to-end pipeline by means of Generative Adversarial Networks (GAN). Our GAN produces a frontal-facing 2.5D point cloud based on a training dataset captured with an RGBD camera. In our approach, the training process is offline, while the reconstruction runs in real-time. Our results show adequate reconstruction quality within the 'learned' expressions. Expressions not learned by the network produce artifacts and can trigger the Uncanny Valley effect.



### Beckman Defense
- **Arxiv ID**: http://arxiv.org/abs/2301.01495v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01495v2)
- **Published**: 2023-01-04 09:04:34+00:00
- **Updated**: 2023-01-05 06:27:38+00:00
- **Authors**: A. V. Subramanyam
- **Comment**: None
- **Journal**: None
- **Summary**: Optimal transport (OT) based distributional robust optimisation (DRO) has received some traction in the recent past. However, it is at a nascent stage but has a sound potential in robustifying the deep learning models. Interestingly, OT barycenters demonstrate a good robustness against adversarial attacks. Owing to the computationally expensive nature of OT barycenters, they have not been investigated under DRO framework. In this work, we propose a new barycenter, namely Beckman barycenter, which can be computed efficiently and used for training the network to defend against adversarial attacks in conjunction with adversarial training. We propose a novel formulation of Beckman barycenter and analytically obtain the barycenter using the marginals of the input image. We show that the Beckman barycenter can be used to train adversarially trained networks to improve the robustness. Our training is extremely efficient as it requires only a single epoch of training. Elaborate experiments on CIFAR-10, CIFAR-100 and Tiny ImageNet demonstrate that training an adversarially robust network with Beckman barycenter can significantly increase the performance. Under auto attack, we get a a maximum boost of 10\% in CIFAR-10, 8.34\% in CIFAR-100 and 11.51\% in Tiny ImageNet. Our code is available at https://github.com/Visual-Conception-Group/test-barycentric-defense.



### Towards Edge-Cloud Architectures for Personal Protective Equipment Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.01501v1
- **DOI**: 10.1145/3590837.3590921
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01501v1)
- **Published**: 2023-01-04 09:17:34+00:00
- **Updated**: 2023-01-04 09:17:34+00:00
- **Authors**: Jaroslaw Legierski, Kajetan Rachwal, Piotr Sowinski, Wojciech Niewolski, Przemyslaw Ratuszek, Zbigniew Kopertowski, Marcin Paprzycki, Maria Ganzha
- **Comment**: Presented on the 4th International Conference on Information
  Management and Machine Intelligence (ICIMMI 2022). In print
- **Journal**: ICIMMI 2022: Proceedings of the 4th International Conference on
  Information Management & Machine Intelligence
- **Summary**: Detecting Personal Protective Equipment in images and video streams is a relevant problem in ensuring the safety of construction workers. In this contribution, an architecture enabling live image recognition of such equipment is proposed. The solution is deployable in two settings -- edge-cloud and edge-only. The system was tested on an active construction site, as a part of a larger scenario, within the scope of the ASSIST-IoT H2020 project. To determine the feasibility of the edge-only variant, a model for counting people wearing safety helmets was developed using the YOLOX method. It was found that an edge-only deployment is possible for this use case, given the hardware infrastructure available on site. In the preliminary evaluation, several important observations were made, that are crucial to the further development and deployment of the system. Future work will include an in-depth investigation of performance aspects of the two architecture variants.



### Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy
- **Arxiv ID**: http://arxiv.org/abs/2301.01520v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.01520v2)
- **Published**: 2023-01-04 10:17:16+00:00
- **Updated**: 2023-08-20 20:37:31+00:00
- **Authors**: Cassio F. Dantas, Diego Marcos, Dino Ienco
- **Comment**: None
- **Journal**: None
- **Summary**: Counterfactual explanations are an emerging tool to enhance interpretability of deep learning models. Given a sample, these methods seek to find and display to the user similar samples across the decision boundary. In this paper, we propose a generative adversarial counterfactual approach for satellite image time series in a multi-class setting for the land cover classification task. One of the distinctive features of the proposed approach is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the discovery of interesting information on the relationship between land cover classes. The other feature consists of encouraging the counterfactual to differ from the original sample only in a small and compact temporal segment. These time-contiguous perturbations allow for a much sparser and, thus, interpretable solution. Furthermore, plausibility/realism of the generated counterfactual explanations is enforced via the proposed adversarial learning strategy.



### MoBYv2AL: Self-supervised Active Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.01531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01531v1)
- **Published**: 2023-01-04 10:52:02+00:00
- **Updated**: 2023-01-04 10:52:02+00:00
- **Authors**: Razvan Caramalau, Binod Bhattarai, Danail Stoyanov, Tae-Kyun Kim
- **Comment**: Poster accepted at BMVC 2022
- **Journal**: None
- **Summary**: Active learning(AL) has recently gained popularity for deep learning(DL) models. This is due to efficient and informative sampling, especially when the learner requires large-scale labelled datasets. Commonly, the sampling and training happen in stages while more batches are added. One main bottleneck in this strategy is the narrow representation learned by the model that affects the overall AL selection.   We present MoBYv2AL, a novel self-supervised active learning framework for image classification. Our contribution lies in lifting MoBY, one of the most successful self-supervised learning algorithms, to the AL pipeline. Thus, we add the downstream task-aware objective function and optimize it jointly with contrastive loss. Further, we derive a data-distribution selection function from labelling the new examples. Finally, we test and study our pipeline robustness and performance for image classification tasks. We successfully achieved state-of-the-art results when compared to recent AL methods. Code available: https://github.com/razvancaramalau/MoBYv2AL



### Learning Decorrelated Representations Efficiently Using Fast Fourier Transform
- **Arxiv ID**: http://arxiv.org/abs/2301.01569v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01569v2)
- **Published**: 2023-01-04 12:38:08+00:00
- **Updated**: 2023-06-01 06:51:57+00:00
- **Authors**: Yutaro Shigeto, Masashi Shimbo, Yuya Yoshikawa, Akikazu Takeuchi
- **Comment**: Accepted for CVPR 2023
- **Journal**: None
- **Summary**: Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.



### Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption
- **Arxiv ID**: http://arxiv.org/abs/2301.01583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.01583v1)
- **Published**: 2023-01-04 12:59:51+00:00
- **Updated**: 2023-01-04 12:59:51+00:00
- **Authors**: Matthias Mitterreiter, Marcel Koch, Joachim Giesen, Sören Laue
- **Comment**: To appear in AAAI 2023
- **Journal**: None
- **Summary**: Capsule neural networks replace simple, scalar-valued neurons with vector-valued capsules. They are motivated by the pattern recognition system in the human brain, where complex objects are decomposed into a hierarchy of simpler object parts. Such a hierarchy is referred to as a parse-tree. Conceptually, capsule neural networks have been defined to realize such parse-trees. The capsule neural network (CapsNet), by Sabour, Frosst, and Hinton, is the first actual implementation of the conceptual idea of capsule neural networks. CapsNets achieved state-of-the-art performance on simple image recognition tasks with fewer parameters and greater robustness to affine transformations than comparable approaches. This sparked extensive follow-up research. However, despite major efforts, no work was able to scale the CapsNet architecture to more reasonable-sized datasets. Here, we provide a reason for this failure and argue that it is most likely not possible to scale CapsNets beyond toy examples. In particular, we show that the concept of a parse-tree, the main idea behind capsule neuronal networks, is not present in CapsNets. We also show theoretically and experimentally that CapsNets suffer from a vanishing gradient problem that results in the starvation of many capsules during training.



### Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2302.05289v1
- **DOI**: 10.1007/s10796-022-10315-z
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2302.05289v1)
- **Published**: 2023-01-04 13:15:08+00:00
- **Updated**: 2023-01-04 13:15:08+00:00
- **Authors**: Abderrazek Azri, Cécile Favre, Nouria Harbi, Jérôme Darmont, Camille Noûs
- **Comment**: Information Systems Frontiers, 2022
- **Journal**: None
- **Summary**: The proliferation of rumors on social media has become a major concern due to its ability to create a devastating impact. Manually assessing the veracity of social media messages is a very time-consuming task that can be much helped by machine learning. Most message veracity verification methods only exploit textual contents and metadata. Very few take both textual and visual contents, and more particularly images, into account. Moreover, prior works have used many classical machine learning models to detect rumors. However, although recent studies have proven the effectiveness of ensemble machine learning approaches, such models have seldom been applied. Thus, in this paper, we propose a set of advanced image features that are inspired from the field of image quality assessment, and introduce the Multimodal fusiON framework to assess message veracIty in social neTwORks (MONITOR), which exploits all message features by exploring various machine learning models. Moreover, we demonstrate the effectiveness of ensemble learning algorithms for rumor detection by using five metalearning models. Eventually, we conduct extensive experiments on two real-world datasets. Results show that MONITOR outperforms state-of-the-art machine learning baselines and that ensemble models significantly increase MONITOR's performance.



### StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.01615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01615v2)
- **Published**: 2023-01-04 13:38:48+00:00
- **Updated**: 2023-01-07 15:12:33+00:00
- **Authors**: Zhe Liu, Xiaoqing Ye, Xiao Tan, Errui Ding, Xiang Bai
- **Comment**: Accepted by AAAI-2023
- **Journal**: None
- **Summary**: In this paper, we propose a cross-modal distillation method named StereoDistill to narrow the gap between the stereo and LiDAR-based approaches via distilling the stereo detectors from the superior LiDAR model at the response level, which is usually overlooked in 3D object detection distillation. The key designs of StereoDistill are: the X-component Guided Distillation~(XGD) for regression and the Cross-anchor Logit Distillation~(CLD) for classification. In XGD, instead of empirically adopting a threshold to select the high-quality teacher predictions as soft targets, we decompose the predicted 3D box into sub-components and retain the corresponding part for distillation if the teacher component pilot is consistent with ground truth to largely boost the number of positive predictions and alleviate the mimicking difficulty of the student model. For CLD, we aggregate the probability distribution of all anchors at the same position to encourage the highest probability anchor rather than individually distill the distribution at the anchor level. Finally, our StereoDistill achieves state-of-the-art results for stereo-based 3D detection on the KITTI test benchmark and extensive experiments on KITTI and Argoverse Dataset validate the effectiveness.



### SPTS v2: Single-Point Scene Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2301.01635v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.01635v3)
- **Published**: 2023-01-04 14:20:14+00:00
- **Updated**: 2023-08-08 01:45:37+00:00
- **Authors**: Yuliang Liu, Jiaxin Zhang, Dezhi Peng, Mingxin Huang, Xinyu Wang, Jingqun Tang, Can Huang, Dahua Lin, Chunhua Shen, Xiang Bai, Lianwen Jin
- **Comment**: arXiv admin note: text overlap with arXiv:2112.07917
- **Journal**: None
- **Summary**: End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. Our new framework, SPTS v2, allows us to train high-performing text-spotting models using a single-point annotation. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Comprehensive experiments on various existing benchmark datasets demonstrate the SPTS v2 can outperform previous state-of-the-art single-point text spotters with fewer parameters while achieving 19$\times$ faster inference speed. Within the context of our SPTS v2 framework, our experiments suggest a potential preference for single-point representation in scene text spotting when compared to other representations. Such an attempt provides a significant opportunity for scene text spotting applications beyond the realms of existing paradigms. Code is available at https://github.com/Yuliang-Liu/SPTSv2.



### RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.01661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01661v1)
- **Published**: 2023-01-04 15:12:57+00:00
- **Updated**: 2023-01-04 15:12:57+00:00
- **Authors**: Kang Liao, Lang Nie, Chunyu Lin, Zishuo Zheng, Yao Zhao
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: The wide-angle lens shows appealing applications in VR technologies, but it introduces severe radial distortion into its captured image. To recover the realistic scene, previous works devote to rectifying the content of the wide-angle image. However, such a rectification solution inevitably distorts the image boundary, which potentially changes related geometric distributions and misleads the current vision perception models. In this work, we explore constructing a win-win representation on both content and boundary by contributing a new learning model, i.e., Rectangling Rectification Network (RecRecNet). In particular, we propose a thin-plate spline (TPS) module to formulate the non-linear and non-rigid transformation for rectangling images. By learning the control points on the rectified image, our model can flexibly warp the source structure to the target domain and achieves an end-to-end unsupervised deformation. To relieve the complexity of structure approximation, we then inspire our RecRecNet to learn the gradual deformation rules with a DoF (Degree of Freedom)-based curriculum learning. By increasing the DoF in each curriculum stage, namely, from similarity transformation (4-DoF) to homography transformation (8-DoF), the network is capable of investigating more detailed deformations, offering fast convergence on the final rectangling task. Experiments show the superiority of our solution over the compared methods on both quantitative and qualitative evaluations. The code and dataset will be made available.



### COVID-Net USPro: An Open-Source Explainable Few-Shot Deep Prototypical Network to Monitor and Detect COVID-19 Infection from Point-of-Care Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2301.01679v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.01679v1)
- **Published**: 2023-01-04 16:05:51+00:00
- **Updated**: 2023-01-04 16:05:51+00:00
- **Authors**: Jessy Song, Ashkan Ebadi, Adrian Florea, Pengcheng Xi, Stéphane Tremblay, Alexander Wong
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: As the Coronavirus Disease 2019 (COVID-19) continues to impact many aspects of life and the global healthcare systems, the adoption of rapid and effective screening methods to prevent further spread of the virus and lessen the burden on healthcare providers is a necessity. As a cheap and widely accessible medical image modality, point-of-care ultrasound (POCUS) imaging allows radiologists to identify symptoms and assess severity through visual inspection of the chest ultrasound images. Combined with the recent advancements in computer science, applications of deep learning techniques in medical image analysis have shown promising results, demonstrating that artificial intelligence-based solutions can accelerate the diagnosis of COVID-19 and lower the burden on healthcare professionals. However, the lack of a huge amount of well-annotated data poses a challenge in building effective deep neural networks in the case of novel diseases and pandemics. Motivated by this, we present COVID-Net USPro, an explainable few-shot deep prototypical network, that monitors and detects COVID-19 positive cases with high precision and recall from minimal ultrasound images. COVID-Net USPro achieves 99.65% overall accuracy, 99.7% recall and 99.67% precision for COVID-19 positive cases when trained with only 5 shots. The analytic pipeline and results were verified by our contributing clinician with extensive experience in POCUS interpretation, ensuring that the network makes decisions based on actual patterns.



### UNAEN: Unsupervised Abnormality Extraction Network for MRI Motion Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/2301.01732v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2301.01732v4)
- **Published**: 2023-01-04 18:02:59+00:00
- **Updated**: 2023-08-11 11:55:50+00:00
- **Authors**: Yusheng Zhou, Hao Li, Jianan Liu, Zhengmin Kong, Tao Huang, Euijoon Ahn, Zhihan Lv, Jinman Kim, David Dagan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Motion artifacts compromise the quality of magnetic resonance imaging (MRI) and pose challenges to achieving diagnostic outcomes and image-guided therapies. In recent years, supervised deep learning approaches have emerged as successful solutions for motion artifact reduction (MAR). One disadvantage of these methods is their dependency on acquiring paired sets of motion artifact-corrupted (MA-corrupted) and motion artifact-free (MA-free) MR images for training purposes. Obtaining such image pairs is difficult and therefore limits the application of supervised training. In this paper, we propose a novel UNsupervised Abnormality Extraction Network (UNAEN) to alleviate this problem. Our network is capable of working with unpaired MA-corrupted and MA-free images. It converts the MA-corrupted images to MA-reduced images by extracting abnormalities from the MA-corrupted images using a proposed artifact extractor, which intercepts the residual artifact maps from the MA-corrupted MR images explicitly, and a reconstructor to restore the original input from the MA-reduced images. The performance of UNAEN was assessed by experimenting on various publicly available MRI datasets and comparing them with state-of-the-art methods. The quantitative evaluation demonstrates the superiority of UNAEN over alternative MAR methods and visually exhibits fewer residual artifacts. Our results substantiate the potential of UNAEN as a promising solution applicable in real-world clinical environments, with the capability to enhance diagnostic accuracy and facilitate image-guided therapies.



### Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations
- **Arxiv ID**: http://arxiv.org/abs/2301.02184v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.02184v2)
- **Published**: 2023-01-04 18:47:32+00:00
- **Updated**: 2023-04-20 05:26:28+00:00
- **Authors**: Sagnik Majumder, Hao Jiang, Pierre Moulon, Ethan Henderson, Paul Calamia, Kristen Grauman, Vamsi Krishna Ithapu
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Can conversational videos captured from multiple egocentric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously unseen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multiple people ("egos") move in a scene and talk among themselves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and reduce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to efficiently chart out the space. We evaluate the approach using a state-of-the-art audio-visual simulator for 3D scenes as well as real-world video. Our model outperforms previous state-of-the-art mapping methods, and achieves an excellent cost-accuracy tradeoff. Project: http://vision.cs.utexas.edu/projects/chat2map.



### An Ensemble Mobile-Cloud Computing Method for Affordable and Accurate Glucometer Readout
- **Arxiv ID**: http://arxiv.org/abs/2301.01758v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01758v1)
- **Published**: 2023-01-04 18:48:53+00:00
- **Updated**: 2023-01-04 18:48:53+00:00
- **Authors**: Navidreza Asadi, Maziar Goudarzi
- **Comment**: 12 pages, 12 figures, 8 tables
- **Journal**: None
- **Summary**: Despite essential efforts towards advanced wireless medical devices for regular monitoring of blood properties, many such devices are not available or not affordable for everyone in many countries. Alternatively using ordinary devices, patients ought to log data into a mobile health-monitoring manually. It causes several issues: (1) clients reportedly tend to enter unrealistic data; (2) typing values several times a day is bothersome and causes clients to leave the mobile app. Thus, there is a strong need to use now-ubiquitous smartphones, reducing error by capturing images from the screen of medical devices and extracting useful information automatically. Nevertheless, there are a few challenges in its development: (1) data scarcity has led to impractical methods with very low accuracy: to our knowledge, only small datasets are available in this case; (2) accuracy-availability tradeoff: one can execute a less accurate algorithm on a mobile phone to maintain higher availability, or alternatively deploy a more accurate and more compute-intensive algorithm on the cloud, however, at the cost of lower availability in poor/no connectivity situations. We present an ensemble learning algorithm, a mobile-cloud computing service architecture, and a simple compression technique to achieve higher availability and faster response time while providing higher accuracy by integrating cloud- and mobile-side predictions. Additionally, we propose an algorithm to generate synthetic training data which facilitates utilizing deep learning models to improve accuracy. Our proposed method achieves three main objectives: (1) 92.1% and 97.7% accuracy on two different datasets, improving previous methods by 40%, (2) reducing required bandwidth by 45x with 1% drop in accuracy, (3) and providing better availability compared to mobile-only, cloud-only, split computing, and early exit service models.



### Self-Supervised Video Forensics by Audio-Visual Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.01767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01767v2)
- **Published**: 2023-01-04 18:59:49+00:00
- **Updated**: 2023-03-27 18:53:32+00:00
- **Authors**: Chao Feng, Ziyang Chen, Andrew Owens
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics



### Fully Automated Artery-Vein ratio and vascular tortuosity measurement in retinal fundus images
- **Arxiv ID**: http://arxiv.org/abs/2301.01791v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01791v1)
- **Published**: 2023-01-04 19:13:21+00:00
- **Updated**: 2023-01-04 19:13:21+00:00
- **Authors**: Aashis Khanal, Rolando Estrada
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate measurements of abnormalities like Artery-Vein ratio and tortuosity in fundus images is an actively researched task. Most of the research seems to compute such features independently. However, in this work, we have devised a fully automated technique to measure any vascular abnormalities. This paper is a follow-up paper on vessel topology estimation and extraction, we use the extracted topology to perform A-V state-of-the-art Artery-Vein classification, AV ratio calculation, and vessel tortuosity measurement, all fully automated. Existing techniques tend to only work on the partial region, but we extract the complete vascular structure. We have shown the usability of this topology by extracting two of the most important vascular features; Artery-Vein ratio, and vessel tortuosity.



### PACO: Parts and Attributes of Common Objects
- **Arxiv ID**: http://arxiv.org/abs/2301.01795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01795v1)
- **Published**: 2023-01-04 19:28:03+00:00
- **Updated**: 2023-01-04 19:28:03+00:00
- **Authors**: Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, Dhruv Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: Object models are gradually progressing from predicting just category labels to providing detailed descriptions of object instances. This motivates the need for large datasets which go beyond traditional object masks and provide richer annotations such as part masks and attributes. Hence, we introduce PACO: Parts and Attributes of Common Objects. It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. We provide 641K part masks annotated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well. We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, object and part attribute prediction and zero-shot instance detection. Dataset, models, and code are open-sourced at https://github.com/facebookresearch/paco.



### MonoEdge: Monocular 3D Object Detection Using Local Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2301.01802v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.01802v1)
- **Published**: 2023-01-04 19:51:53+00:00
- **Updated**: 2023-01-04 19:51:53+00:00
- **Authors**: Minghan Zhu, Lingting Ge, Panqu Wang, Huei Peng
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: We propose a novel approach for monocular 3D object detection by leveraging local perspective effects of each object. While the global perspective effect shown as size and position variations has been exploited for monocular 3D detection extensively, the local perspectives has long been overlooked. We design a local perspective module to regress a newly defined variable named keyedge-ratios as the parameterization of the local shape distortion to account for the local perspective, and derive the object depth and yaw angle from it. Theoretically, this module does not rely on the pixel-wise size or position in the image of the objects, therefore independent of the camera intrinsic parameters. By plugging this module in existing monocular 3D object detection frameworks, we incorporate the local perspective distortion with global perspective effect for monocular 3D reasoning, and we demonstrate the effectiveness and superior performance over strong baseline methods in multiple datasets.



### Unsupervised Manifold Linearizing and Clustering
- **Arxiv ID**: http://arxiv.org/abs/2301.01805v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01805v2)
- **Published**: 2023-01-04 20:08:23+00:00
- **Updated**: 2023-08-24 06:28:02+00:00
- **Authors**: Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin D. Haeffele
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the classical problem of subspace clustering, which has been studied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appropriate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold being mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial representations. In this paper, we propose to optimize the Maximal Coding Rate Reduction metric with respect to both the data representation and a novel doubly stochastic cluster membership, inspired by state-of-the-art subspace clustering results. We give a parameterization of such a representation and membership, allowing efficient mini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100, and TinyImageNet-200 datasets show that the proposed method is much more accurate and scalable than state-of-the-art deep clustering methods, and further learns a latent linear representation of the data.



### Living Images: A Recursive Approach to Computing the Structural Beauty of Images or the Livingness of Space
- **Arxiv ID**: http://arxiv.org/abs/2301.01814v1
- **DOI**: None
- **Categories**: **physics.soc-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01814v1)
- **Published**: 2023-01-04 20:27:32+00:00
- **Updated**: 2023-01-04 20:27:32+00:00
- **Authors**: Bin Jiang, Chris de Rijke
- **Comment**: 19 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Any image is perceived subconsciously as a coherent structure (or whole) with two contrast substructures: figure and ground. The figure consists of numerous auto-generated substructures with an inherent hierarchy of far more smalls than larges. Through these substructures, the structural beauty of an image (L) can be computed by the multiplication of the number of substructures (S) and their inherent hierarchy (H). This definition implies that the more substructures, the more living or more structurally beautiful, and the higher hierarchy of the substructures, the more living or more structurally beautiful. This is the non-recursive approach to the structural beauty of images or the livingness of space. In this paper we develop a recursive approach, which derives all substructures of an image (instead of its figure) and continues the deriving process for those decomposable substructures until none of them are decomposable. All of the substructures derived at different iterations (or recursive levels) together constitute a living structure; hence the notion of living images. We applied the recursive approach to a set of images and found that (1) the number of substructures of an image is far lower (3 percent on average) than the number of pixels and the centroids of the substructures can effectively capture the skeleton or saliency of the image; (2) all the images have the recursive levels more than three, indicating that they are indeed living images; (3) no more than 2 percent of the substructures are decomposable; (4) structural beauty can be measured by the recursively defined substructures, as well as their decomposable subsets. The recursive approach is proved to be more robust than the non-recursive approach. The recursive approach and the non-recursive approach both provide a powerful means to study the livingness or vitality of space in cities and communities.



### Automatic Classification of Single Tree Decay Stages from Combined ALS Data and Aerial CIR Imagery using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.01841v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.01841v2)
- **Published**: 2023-01-04 22:20:16+00:00
- **Updated**: 2023-05-01 08:17:35+00:00
- **Authors**: Tsz Chung Wong, Abubakar Sani-Mohammed, Wei Yao, Marco Heurich
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding forest health is of great importance for the conservation of the integrity of forest ecosystems. The monitoring of forest health is, therefore, indispensable for the long-term conservation of forests and their sustainable management. In this regard, evaluating the amount and quality of dead wood is of utmost interest as they are favorable indicators of biodiversity. Apparently, remote sensing-based techniques have proven to be more efficient and sustainable with unprecedented accuracy in forest inventory. However, the application of these techniques is still in its infancy with respect to dead wood mapping. This study investigates for the first time the automatic classification of individual coniferous trees into five decay stages (live, declining, dead, loose bark, and clean) from combined airborne laser scanning data and color infrared images using Machine Learning methods. First, CIR colorized point clouds are created by fusing the ALS point clouds and the color infrared images. Then, with the colorized point cloud, individual tree segmentation is conducted using a semi-automatic approach, which are further projected onto four orthogonal planes displaying the side views of the trees in 2D. Finally, the classification is conducted on the multispectral point clouds and projected images using the three Machine Learning algorithms. All models achieved promising results, reaching overall accuracy (OA) of up to 90.9%, 90.6%, and 80.6% for CNN, RF, and PointNet, respectively. The experimental results reveal that the image-based approach notably outperformed the point cloud-based one, while spectral image texture is of the highest relevance to the success of categorizing tree decay. Our models could therefore be used for automatic determination of single tree decay stages and landscape-wide assessment of dead wood amount and quality using modern airborne remote sensing.



### Detecting Neighborhood Gentrification at Scale via Street-level Visual Data
- **Arxiv ID**: http://arxiv.org/abs/2301.01842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2301.01842v1)
- **Published**: 2023-01-04 22:52:29+00:00
- **Updated**: 2023-01-04 22:52:29+00:00
- **Authors**: Tianyuan Huang, Timothy Dai, Zhecheng Wang, Hesu Yoon, Hao Sheng, Andrew Y. Ng, Ram Rajagopal, Jackelyn Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Neighborhood gentrification plays a significant role in shaping the social and economic well-being of both individuals and communities at large. While some efforts have been made to detect gentrification in cities, existing approaches rely mainly on estimated measures from survey data, require substantial work of human labeling, and are limited in characterizing the neighborhood as a whole. We propose a novel approach to detecting neighborhood gentrification at a large-scale based on the physical appearance of neighborhoods by incorporating historical street-level visual data. We show the effectiveness of the proposed method by comparing results from our approach with gentrification measures from previous literature and case studies. Our approach has the potential to supplement existing indicators of gentrification and become a valid resource for urban researchers and policy makers.



