# Arxiv Papers in cs.CV on 2023-01-14
### RMM: Reinforced Memory Management for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.05792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05792v1)
- **Published**: 2023-01-14 00:07:47+00:00
- **Updated**: 2023-01-14 00:07:47+00:00
- **Authors**: Yaoyao Liu, Bernt Schiele, Qianru Sun
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Class-Incremental Learning (CIL) [40] trains classifiers under a strict memory budget: in each incremental phase, learning is done for new data, most of which is abandoned to free space for the next phase. The preserved data are exemplars used for replaying. However, existing methods use a static and ad hoc strategy for memory allocation, which is often sub-optimal. In this work, we propose a dynamic memory management strategy that is optimized for the incremental phases and different object classes. We call our method reinforced memory management (RMM), leveraging reinforcement learning. RMM training is not naturally compatible with CIL as the past, and future data are strictly non-accessible during the incremental phases. We solve this by training the policy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data of the 0-th phase, and then applying it to target tasks. RMM propagates two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. In essence, it is an optimizable and general method for memory management that can be used in any replaying-based CIL method. For evaluation, we plug RMM into two top-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct experiments on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets by 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks, respectively.



### Learning Trajectory-Conditioned Relations to Predict Pedestrian Crossing Behavior
- **Arxiv ID**: http://arxiv.org/abs/2301.05796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05796v1)
- **Published**: 2023-01-14 00:42:05+00:00
- **Updated**: 2023-01-14 00:42:05+00:00
- **Authors**: Chen Zhou, Ghassan AlRegib, Armin Parchami, Kunjan Singh
- **Comment**: None
- **Journal**: None
- **Summary**: In smart transportation, intelligent systems avoid potential collisions by predicting the intent of traffic agents, especially pedestrians. Pedestrian intent, defined as future action, e.g., start crossing, can be dependent on traffic surroundings. In this paper, we develop a framework to incorporate such dependency given observed pedestrian trajectory and scene frames. Our framework first encodes regional joint information between a pedestrian and surroundings over time into feature-map vectors. The global relation representations are then extracted from pairwise feature-map vectors to estimate intent with past trajectory condition. We evaluate our approach on two public datasets and compare against two state-of-the-art approaches. The experimental results demonstrate that our method helps to inform potential risks during crossing events with 0.04 improvement in F1-score on JAAD dataset and 0.01 improvement in recall on PIE dataset. Furthermore, we conduct ablation experiments to confirm the contribution of the relation extraction in our framework.



### Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over Full Visual Context
- **Arxiv ID**: http://arxiv.org/abs/2301.05804v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.05804v2)
- **Published**: 2023-01-14 01:47:09+00:00
- **Updated**: 2023-01-18 19:48:16+00:00
- **Authors**: Ross Greer, Akshay Gopalkrishnan, Nachiket Deo, Akshay Rangesh, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting road traffic signs and accurately determining how they can affect the driver's future actions is a critical task for safe autonomous driving systems. However, various traffic signs in a driving scene have an unequal impact on the driver's decisions, making detecting the salient traffic signs a more important task. Our research addresses this issue, constructing a traffic sign detection model which emphasizes performance on salient signs, or signs that influence the decisions of a driver. We define a traffic sign salience property and use it to construct the LAVA Salient Signs Dataset, the first traffic sign dataset that includes an annotated salience property. Next, we use a custom salience loss function, Salience-Sensitive Focal Loss, to train a Deformable DETR object detection model in order to emphasize stronger performance on salient signs. Results show that a model trained with Salience-Sensitive Focal Loss outperforms a model trained without, with regards to recall of both salient signs and all signs combined. Further, the performance margin on salient signs compared to all signs is largest for the model trained with Salience-Sensitive Focal Loss.



### Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.05805v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2301.05805v2)
- **Published**: 2023-01-14 01:53:48+00:00
- **Updated**: 2023-01-18 19:47:34+00:00
- **Authors**: Ross Greer, Nachiket Deo, Akshay Rangesh, Pujitha Gunaratne, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: To make safe transitions from autonomous to manual control, a vehicle must have a representation of the awareness of driver state; two metrics which quantify this state are the Observable Readiness Index and Takeover Time. In this work, we show that machine learning models which predict these two metrics are robust to multiple camera views, expanding from the limited view angles in prior research. Importantly, these models take as input feature vectors corresponding to hand location and activity as well as gaze location, and we explore the tradeoffs of different views in generating these feature vectors. Further, we introduce two metrics to evaluate the quality of control transitions following the takeover event (the maximal lateral deviation and velocity deviation) and compute correlations of these post-takeover metrics to the pre-takeover predictive metrics.



### Deepfake Detection using Biological Features: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2301.05819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2301.05819v1)
- **Published**: 2023-01-14 05:07:46+00:00
- **Updated**: 2023-01-14 05:07:46+00:00
- **Authors**: Kundan Patil, Shrushti Kale, Jaivanti Dhokey, Abhishek Gulhane
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake is a deep learning-based technique that makes it easy to change or modify images and videos. In investigations and court, visual evidence is commonly employed, but these pieces of evidence may now be suspect due to technological advancements in deepfake. Deepfakes have been used to blackmail individuals, plan terrorist attacks, disseminate false information, defame individuals, and foment political turmoil. This study describes the history of deepfake, its development and detection, and the challenges based on physiological measurements such as eyebrow recognition, eye blinking detection, eye movement detection, ear and mouth detection, and heartbeat detection. The study also proposes a scope in this field and compares the different biological features and their classifiers. Deepfakes are created using the generative adversarial network (GANs) model, and were once easy to detect by humans due to visible artifacts. However, as technology has advanced, deepfakes have become highly indistinguishable from natural images, making it important to review detection methods.



### (Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using a Multi-Camera Framework
- **Arxiv ID**: http://arxiv.org/abs/2301.05838v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2301.05838v3)
- **Published**: 2023-01-14 07:22:12+00:00
- **Updated**: 2023-01-30 02:02:03+00:00
- **Authors**: Ross Greer, Lulua Rakla, Anish Gopalan, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Manual (hand-related) activity is a significant source of crash risk while driving. Accordingly, analysis of hand position and hand activity occupation is a useful component to understanding a driver's readiness to take control of a vehicle. Visual sensing through cameras provides a passive means of observing the hands, but its effectiveness varies depending on camera location. We introduce an algorithmic framework, SMART Hands, for accurate hand classification with an ensemble of camera views using machine learning. We illustrate the effectiveness of this framework in a 4-camera setup, reaching 98% classification accuracy on a variety of locations and held objects for both of the driver's hands. We conclude that this multi-camera framework can be extended to additional tasks such as gaze and pose analysis, with further applications in driver and passenger safety.



### NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2301.05839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2301.05839v1)
- **Published**: 2023-01-14 07:22:18+00:00
- **Updated**: 2023-01-14 07:22:18+00:00
- **Authors**: Souhaib Attaiki, Maks Ovsjanikov
- **Comment**: NeurIPS 2022, 10 pages, 9 figures
- **Journal**: 2022 Advances in Neural Information Processing Systems (NeurIPS)
- **Summary**: We present Neural Correspondence Prior (NCP), a new paradigm for computing correspondences between 3D shapes. Our approach is fully unsupervised and can lead to high-quality correspondences even in challenging cases such as sparse point clouds or non-isometric meshes, where current methods fail. Our first key observation is that, in line with neural priors observed in other domains, recent network architectures on 3D data, even without training, tend to produce pointwise features that induce plausible maps between rigid or non-rigid shapes. Secondly, we show that given a noisy map as input, training a feature extraction network with the input map as supervision tends to remove artifacts from the input and can act as a powerful correspondence denoising mechanism, both between individual pairs and within a collection. With these observations in hand, we propose a two-stage unsupervised paradigm for shape matching by (i) performing unsupervised training by adapting an existing approach to obtain an initial set of noisy matches, and (ii) using these matches to train a network in a supervised manner. We demonstrate that this approach significantly improves the accuracy of the maps, especially when trained within a collection. We show that NCP is data-efficient, fast, and achieves state-of-the-art results on many tasks. Our code can be found online: https://github.com/pvnieo/NCP.



### CHAMP: Crowdsourced, History-Based Advisory of Mapped Pedestrians for Safer Driver Assistance Systems
- **Arxiv ID**: http://arxiv.org/abs/2301.05842v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05842v3)
- **Published**: 2023-01-14 07:28:05+00:00
- **Updated**: 2023-01-30 02:00:31+00:00
- **Authors**: Ross Greer, Lulua Rakla, Samveed Desai, Afnan Alofi, Akshay Gopalkrishnan, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicles are constantly approaching and sharing the road with pedestrians, and as a result it is critical for vehicles to prevent any collisions with pedestrians. Current methods for pedestrian collision prevention focus on integrating visual pedestrian detectors with Automatic Emergency Braking (AEB) systems which can trigger warnings and apply brakes as a pedestrian enters a vehicle's path. Unfortunately, pedestrian-detection-based systems can be hindered in certain situations such as nighttime or when pedestrians are occluded. Our system, CHAMP (Crowdsourced, History-based Advisories of Mapped Pedestrians), addresses such issues using an online, map-based pedestrian detection system where pedestrian locations are aggregated into a dataset after repeated passes of locations. Using this dataset, we are able to learn pedestrian zones and generate advisory notices when a vehicle is approaching a pedestrian despite challenges like dark lighting or pedestrian occlusion. We collected and carefully annotated pedestrian data in La Jolla, CA to construct training and test sets of pedestrian locations. Moreover, we use the number of correct advisories, false advisories, and missed advisories to define precision and recall performance metrics to evaluate CHAMP. This approach can be tuned such that we achieve a maximum of 100% precision and 75% recall on the experimental dataset, with performance enhancement options through further data collection.



### ${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface
- **Arxiv ID**: http://arxiv.org/abs/2301.05845v1
- **DOI**: 10.1109/LRA.2023.3234820
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.05845v1)
- **Published**: 2023-01-14 07:39:15+00:00
- **Updated**: 2023-01-14 07:39:15+00:00
- **Authors**: Meng Li, Senbo Wang, Weihao Yuan, Weichao Shen, Zhe Sheng, Zilong Dong
- **Comment**: Accepted by IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Monocular depth estimation is an ambiguous problem, thus global structural cues play an important role in current data-driven single-view depth estimation methods. Panorama images capture the complete spatial information of their surroundings utilizing the equirectangular projection which introduces large distortion. This requires the depth estimation method to be able to handle the distortion and extract global context information from the image. In this paper, we propose an end-to-end deep network for monocular panorama depth estimation on a unit spherical surface. Specifically, we project the feature maps extracted from equirectangular images onto unit spherical surface sampled by uniformly distributed grids, where the decoder network can aggregate the information from the distortion-reduced feature maps. Meanwhile, we propose a global cross-attention-based fusion module to fuse the feature maps from skip connection and enhance the ability to obtain global context. Experiments are conducted on five panorama depth estimation datasets, and the results demonstrate that the proposed method substantially outperforms previous state-of-the-art methods. All related codes will be open-sourced in the upcoming days.



### EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2301.05856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05856v1)
- **Published**: 2023-01-14 08:32:16+00:00
- **Updated**: 2023-01-14 08:32:16+00:00
- **Authors**: Jian Guan, Mingjie Xie, Youtian Lin, Guangjun He, Pengming Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Label assignment is often employed in recent convolutional neural network (CNN) based detectors to determine positive or negative samples during training process. However, we note that current label assignment strategies barely consider the characteristics of targets in remote sensing images thoroughly, such as large variations in orientations, aspect ratios and scales, which lead to insufficient sampling. In this paper, an Elliptical Distribution aided Adaptive Rotation Label Assignment (EARL) is proposed to select positive samples with higher quality in orientation detectors, and yields better performance. Concretely, to avoid inadequate sampling of targets with extreme scales, an adaptive scale sampling (ADS) strategy is proposed to dynamically select samples on different feature levels according to the scales of targets. To enhance ADS, positive samples are selected following a dynamic elliptical distribution (DED), which can further exploit the orientation and shape properties of targets. Moreover, a spatial distance weighting (SDW) module is introduced to mitigate the influence from low-quality samples on detection performance. Extensive experiments on popular remote sensing datasets, such as DOTA and HRSC2016, demonstrate the effectiveness and the superiority of our proposed EARL, where without bells and whistles, it achieves 72.87 of mAP on DOTA dataset by being integrated with simple structure, which outperforms current state-of-the-art anchor-free detectors and provides comparable performance as anchor-based methods. The source code will be available at https://github.com/Justlovesmile/EARL



### Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking
- **Arxiv ID**: http://arxiv.org/abs/2301.05858v1
- **DOI**: 10.1007/978-3-031-20096-0_7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05858v1)
- **Published**: 2023-01-14 08:49:33+00:00
- **Updated**: 2023-01-14 08:49:33+00:00
- **Authors**: Jinyang Wang, Tao Wang, Min Gan, George Hadjichristofi
- **Comment**: Paper accepted by the 4th International Conference on Machine
  Learning for Cyber Security (ML4CS 2022), Guangzhou, China
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been widely used in scene classification of remotely sensed images. In this work, we propose a robust learning method for the task that is secure against partially incorrect categorization of images. Specifically, we remove and correct errors in the labels progressively by iterative multi-view voting and entropy ranking. At each time step, we first divide the training data into disjoint parts for separate training and voting. The unanimity in the voting reveals the correctness of the labels, so that we can train a strong model with only the images with unanimous votes. In addition, we adopt entropy as an effective measure for prediction uncertainty, in order to partially recover labeling errors by ranking and selection. We empirically demonstrate the superiority of the proposed method on the WHU-RS19 dataset and the AID dataset.



### Gated Self-supervised Learning For Improving Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.05865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05865v1)
- **Published**: 2023-01-14 09:32:12+00:00
- **Updated**: 2023-01-14 09:32:12+00:00
- **Authors**: Erland Hilman Fuadi, Aristo Renaldo Ruslim, Putu Wahyu Kusuma Wardhana, Novanto Yudistira
- **Comment**: None
- **Journal**: None
- **Summary**: In past research on self-supervised learning for image classification, the use of rotation as an augmentation has been common. However, relying solely on rotation as a self-supervised transformation can limit the ability of the model to learn rich features from the data. In this paper, we propose a novel approach to self-supervised learning for image classification using several localizable augmentations with the combination of the gating method. Our approach uses flip and shuffle channel augmentations in addition to the rotation, allowing the model to learn rich features from the data. Furthermore, the gated mixture network is used to weigh the effects of each self-supervised learning on the loss function, allowing the model to focus on the most relevant transformations for classification.



### Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2301.05871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05871v2)
- **Published**: 2023-01-14 09:43:23+00:00
- **Updated**: 2023-03-20 09:12:21+00:00
- **Authors**: Songchun Zhang, Chunhui Zhao
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: Self-supervised methods have showed promising results on depth estimation task. However, previous methods estimate the target depth map and camera ego-motion simultaneously, underusing multi-frame correlation information and ignoring the motion of dynamic objects. In this paper, we propose a novel Dyna-Depthformer framework, which predicts scene depth and 3D motion field jointly and aggregates multi-frame information with transformer. Our contributions are two-fold. First, we leverage multi-view correlation through a series of self- and cross-attention layers in order to obtain enhanced depth feature representation. Specifically, we use the perspective transformation to acquire the initial reference point, and use deformable attention to reduce the computational cost. Second, we propose a warping-based Motion Network to estimate the motion field of dynamic objects without using semantic prior. To improve the motion field predictions, we propose an iterative optimization strategy, together with a sparsity-regularized loss. The entire pipeline achieves end-to-end self-supervised training by constructing a minimum reprojection loss. Extensive experiments on the KITTI and Cityscapes benchmarks demonstrate the effectiveness of our method and show that our method outperforms state-of-the-art algorithms.



### Object Detection performance variation on compressed satellite image datasets with iquaflow
- **Arxiv ID**: http://arxiv.org/abs/2301.05892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05892v2)
- **Published**: 2023-01-14 11:20:27+00:00
- **Updated**: 2023-01-18 14:21:07+00:00
- **Authors**: Pau Gallés, Katalin Takats, Javier Marin
- **Comment**: None
- **Journal**: None
- **Summary**: A lot of work has been done to reach the best possible performance of predictive models on images. There are fewer studies about the resilience of these models when they are trained on image datasets that suffer modifications altering their original quality. Yet this is a common problem that is often encountered in the industry. A good example of that is with earth observation satellites that are capturing many images. The energy and time of connection to the earth of an orbiting satellite are limited and must be carefully used. An approach to mitigate that is to compress the images on board before downloading. The compression can be regulated depending on the intended usage of the image and the requirements of this application. We present a new software tool with the name iquaflow that is designed to study image quality and model performance variation given an alteration of the image dataset. Furthermore, we do a showcase study about oriented object detection models adoption on a public image dataset DOTA Xia_2018_CVPR given different compression levels. The optimal compression point is found and the usefulness of iquaflow becomes evident.



### Model-based Transfer Learning for Automatic Optical Inspection based on domain discrepancy
- **Arxiv ID**: http://arxiv.org/abs/2301.05897v1
- **DOI**: 10.1117/12.2644087 10.1117/12.2644087
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05897v1)
- **Published**: 2023-01-14 11:32:39+00:00
- **Updated**: 2023-01-14 11:32:39+00:00
- **Authors**: Erik Isai Valle Salgado, Haoxin Yan, Yue Hong, Peiyuan Zhu, Shidong Zhu, Chengwei Liao, Yanxiang Wen, Xiu Li, Xiang Qian, Xiaohao Wang, Xinghui Li
- **Comment**: This is a fix of the published paper "Relational-based transfer
  learning for automatic optical inspection based on domain discrepancy"
- **Journal**: Proc. SPIE 12317, Optoelectronic Imaging and Multimedia Technology
  IXMultimedia Technology IX, 2023
- **Summary**: Transfer learning is a promising method for AOI applications since it can significantly shorten sample collection time and improve efficiency in today's smart manufacturing. However, related research enhanced the network models by applying TL without considering the domain similarity among datasets, the data long-tailedness of a source dataset, and mainly used linear transformations to mitigate the lack of samples. This research applies model-based TL via domain similarity to improve the overall performance and data augmentation in both target and source domains to enrich the data quality and reduce the imbalance. Given a group of source datasets from similar industrial processes, we define which group is the most related to the target through the domain discrepancy score and the number of samples each has. Then, we transfer the chosen pre-trained backbone weights to train and fine-tune the target network. Our research suggests increases in the F1 score and the PR curve up to 20% compared with TL using benchmark datasets.



### An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores
- **Arxiv ID**: http://arxiv.org/abs/2301.05908v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.05908v1)
- **Published**: 2023-01-14 12:30:16+00:00
- **Updated**: 2023-01-14 12:30:16+00:00
- **Authors**: Xin Jin, Wu Zhou, Jinyu Wang, Duo Xu, Yiqing Rong, Shuai Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Computational aesthetics evaluation has made great achievements in the field of visual arts, but the research work on music still needs to be explored. Although the existing work of music generation is very substantial, the quality of music score generated by AI is relatively poor compared with that created by human composers. The music scores created by AI are usually monotonous and devoid of emotion. Based on Birkhoff's aesthetic measure, this paper proposes an objective quantitative evaluation method for homophony music score aesthetic quality assessment. The main contributions of our work are as follows: first, we put forward a homophony music score aesthetic model to objectively evaluate the quality of music score as a baseline model; second, we put forward eight basic music features and four music aesthetic features.



### End-to-End Page-Level Assessment of Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.05935v2
- **DOI**: 10.1016/j.patcog.2023.109695
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2301.05935v2)
- **Published**: 2023-01-14 15:43:07+00:00
- **Updated**: 2023-05-21 07:41:53+00:00
- **Authors**: Enrique Vidal, Alejandro H. Toselli, Antonio Ríos-Vila, Jorge Calvo-Zaragoza
- **Comment**: Published in Pattern Recognition
- **Journal**: None
- **Summary**: The evaluation of Handwritten Text Recognition (HTR) systems has traditionally used metrics based on the edit distance between HTR and ground truth (GT) transcripts, at both the character and word levels. This is very adequate when the experimental protocol assumes that both GT and HTR text lines are the same, which allows edit distances to be independently computed to each given line. Driven by recent advances in pattern recognition, HTR systems increasingly face the end-to-end page-level transcription of a document, where the precision of locating the different text lines and their corresponding reading order (RO) play a key role. In such a case, the standard metrics do not take into account the inconsistencies that might appear. In this paper, the problem of evaluating HTR systems at the page level is introduced in detail. We analyse the convenience of using a two-fold evaluation, where the transcription accuracy and the RO goodness are considered separately. Different alternatives are proposed, analysed and empirically compared both through partially simulated and through real, full end-to-end experiments. Results support the validity of the proposed two-fold evaluation approach. An important conclusion is that such an evaluation can be adequately achieved by just two simple and well-known metrics: the Word Error Rate (WER), that takes transcription sequentiality into account, and the here re-formulated Bag of Words Word Error Rate (bWER), that ignores order. While the latter directly and very accurately assess intrinsic word recognition errors, the difference between both metrics gracefully correlates with the Normalised Spearman's Foot Rule Distance (NSFD), a metric which explicitly measures RO errors associated with layout analysis flaws.



### Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes
- **Arxiv ID**: http://arxiv.org/abs/2301.05938v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2301.05938v1)
- **Published**: 2023-01-14 15:57:00+00:00
- **Updated**: 2023-01-14 15:57:00+00:00
- **Authors**: Kareem Allam, Xiaohong Iris Wang, Songlin Zhang, Jianmin Ding, Kevin Chiu, Karan Saluja, Amer Wahed, Hongxia Sun, Andy N. D. Nguyen
- **Comment**: 9 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Deep learning has been shown to be useful to detect breast cancer metastases by analyzing whole slide images of sentinel lymph nodes. However, it requires extensive scanning and analysis of all the lymph nodes slides for each case. Our deep learning study focuses on breast cancer screening with only a small set of image patches from any sentinel lymph node, positive or negative for metastasis, to detect changes in tumor environment and not in the tumor itself. We design a convolutional neural network in the Python language to build a diagnostic model for this purpose. The excellent results from this preliminary study provided a proof of concept for incorporating automated metastatic screen into the digital pathology workflow to augment the pathologists' productivity. Our approach is unique since it provides a very rapid screen rather than an exhaustive search for tumor in all fields of all sentinel lymph nodes.



### Towards Spatial Equilibrium Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.05957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05957v1)
- **Published**: 2023-01-14 17:33:26+00:00
- **Updated**: 2023-01-14 17:33:26+00:00
- **Authors**: Zhaohui Zheng, Yuming Chen, Qibin Hou, Xiang Li, Ming-Ming Cheng
- **Comment**: Our source codes are publicly available at
  https://github.com/Zzh-tju/ZoneEval
- **Journal**: None
- **Summary**: Semantic objects are unevenly distributed over images. In this paper, we study the spatial disequilibrium problem of modern object detectors and propose to quantify this ``spatial bias'' by measuring the detection performance over zones. Our analysis surprisingly shows that the spatial imbalance of objects has a great impact on the detection performance, limiting the robustness of detection applications. This motivates us to design a more generalized measurement, termed Spatial equilibrium Precision (SP), to better characterize the detection performance of object detectors. Furthermore, we also present a spatial equilibrium label assignment (SELA) to alleviate the spatial disequilibrium problem by injecting the prior spatial weight into the optimization process of detectors. Extensive experiments on PASCAL VOC, MS COCO, and 3 application datasets on face mask/fruit/helmet images demonstrate the advantages of our method. Our findings challenge the conventional sense of object detectors and show the indispensability of spatial equilibrium. We hope these discoveries would stimulate the community to rethink how an excellent object detector should be. All the source code, evaluation protocols, and the tutorials are publicly available at https://github.com/Zzh-tju/ZoneEval



