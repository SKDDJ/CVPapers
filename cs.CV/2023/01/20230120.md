# Arxiv Papers in cs.CV on 2023-01-20
### On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2301.08365v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2301.08365v5)
- **Published**: 2023-01-20 00:05:18+00:00
- **Updated**: 2023-08-09 21:49:44+00:00
- **Authors**: George Yiasemis, Clara I. Sánchez, Jan-Jakob Sonke, Jonas Teuwen
- **Comment**: 22 pages, 12 figures, 5 tables
- **Journal**: None
- **Summary**: Acquiring fully-sampled MRI $k$-space data is time-consuming, and collecting accelerated data can reduce the acquisition time. Employing 2D Cartesian-rectilinear subsampling schemes is a conventional approach for accelerated acquisitions; however, this often results in imprecise reconstructions, even with the use of Deep Learning (DL), especially at high acceleration factors. Non-rectilinear or non-Cartesian trajectories can be implemented in MRI scanners as alternative subsampling options. This work investigates the impact of the $k$-space subsampling scheme on the quality of reconstructed accelerated MRI measurements produced by trained DL models. The Recurrent Variational Network (RecurrentVarNet) was used as the DL-based MRI-reconstruction architecture. Cartesian, fully-sampled multi-coil $k$-space measurements from three datasets were retrospectively subsampled with different accelerations using eight distinct subsampling schemes: four Cartesian-rectilinear, two Cartesian non-rectilinear, and two non-Cartesian. Experiments were conducted in two frameworks: scheme-specific, where a distinct model was trained and evaluated for each dataset-subsampling scheme pair, and multi-scheme, where for each dataset a single model was trained on data randomly subsampled by any of the eight schemes and evaluated on data subsampled by all schemes. In both frameworks, RecurrentVarNets trained and evaluated on non-rectilinearly subsampled data demonstrated superior performance, particularly for high accelerations. In the multi-scheme setting, reconstruction performance on rectilinearly subsampled data improved when compared to the scheme-specific experiments. Our findings demonstrate the potential for using DL-based methods, trained on non-rectilinearly subsampled measurements, to optimize scan time and image quality.



### Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree Canopies
- **Arxiv ID**: http://arxiv.org/abs/2301.08387v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08387v1)
- **Published**: 2023-01-20 01:46:07+00:00
- **Updated**: 2023-01-20 01:46:07+00:00
- **Authors**: Chung Hee Kim, George Kantor
- **Comment**: 7 pages, 10 figures, submitted to ICRA 2023
- **Journal**: None
- **Summary**: In this work, we present a method to extract the skeleton of a self-occluded tree canopy by estimating the unobserved structures of the tree. A tree skeleton compactly describes the topological structure and contains useful information such as branch geometry, positions and hierarchy. This can be critical to planning contact interactions for agricultural manipulation, yet is difficult to gain due to occlusion by leaves, fruits and other branches. Our method uses an instance segmentation network to detect visible trunk, branches, and twigs. Then, based on the observed tree structures, we build a custom 3D likelihood map in the form of an occupancy grid to hypothesize on the presence of occluded skeletons through a series of minimum cost path searches. We show that our method outperforms baseline methods in highly occluded scenes, demonstrated through a set of experiments on a synthetic tree dataset. Qualitative results are also presented on a real tree dataset collected from the field.



### Open-Set Likelihood Maximization for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.08390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08390v2)
- **Published**: 2023-01-20 01:56:19+00:00
- **Updated**: 2023-05-19 13:51:47+00:00
- **Authors**: Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, Céline Hudelot, Ismail Ben Ayed
- **Comment**: CVPR 2023. Supercedes arXiv:2206.09236
- **Journal**: None
- **Summary**: We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation \textit{Open-Set Likelihood Optimization} (OSLO). OSLO is interpretable and fully modular; it can be applied on top of any pre-trained model seamlessly. Through extensive experiments, we show that our method surpasses existing inductive and transductive methods on both aspects of open-set recognition, namely inlier classification and outlier detection.



### HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.09422v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09422v2)
- **Published**: 2023-01-20 01:57:34+00:00
- **Updated**: 2023-02-02 01:31:37+00:00
- **Authors**: Jinqi Xiao, Chengming Zhang, Yu Gong, Miao Yin, Yang Sui, Lizhi Xiang, Dingwen Tao, Bo Yuan
- **Comment**: AAAI-23
- **Journal**: None
- **Summary**: Low-rank compression is an important model compression strategy for obtaining compact neural network models. In general, because the rank values directly determine the model complexity and model accuracy, proper selection of layer-wise rank is very critical and desired. To date, though many low-rank compression approaches, either selecting the ranks in a manual or automatic way, have been proposed, they suffer from costly manual trials or unsatisfied compression performance. In addition, all of the existing works are not designed in a hardware-aware way, limiting the practical performance of the compressed models on real-world hardware platforms.   To address these challenges, in this paper we propose HALOC, a hardware-aware automatic low-rank compression framework. By interpreting automatic rank selection from an architecture search perspective, we develop an end-to-end solution to determine the suitable layer-wise ranks in a differentiable and hardware-aware way. We further propose design principles and mitigation strategy to efficiently explore the rank space and reduce the potential interference problem.   Experimental results on different datasets and hardware platforms demonstrate the effectiveness of our proposed approach. On CIFAR-10 dataset, HALOC enables 0.07% and 0.38% accuracy increase over the uncompressed ResNet-20 and VGG-16 models with 72.20% and 86.44% fewer FLOPs, respectively. On ImageNet dataset, HALOC achieves 0.9% higher top-1 accuracy than the original ResNet-18 model with 66.16% fewer FLOPs. HALOC also shows 0.66% higher top-1 accuracy increase than the state-of-the-art automatic low-rank compression solution with fewer computational and memory costs. In addition, HALOC demonstrates the practical speedups on different hardware platforms, verified by the measurement results on desktop GPU, embedded GPU and ASIC accelerator.



### Identity masking effectiveness and gesture recognition: Effects of eye enhancement in seeing through the mask
- **Arxiv ID**: http://arxiv.org/abs/2301.08408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08408v1)
- **Published**: 2023-01-20 03:10:19+00:00
- **Updated**: 2023-01-20 03:10:19+00:00
- **Authors**: Madeline Rachow, Thomas Karnowski, Alice J. O'Toole
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Face identity masking algorithms developed in recent years aim to protect the privacy of people in video recordings. These algorithms are designed to interfere with identification, while preserving information about facial actions. An important challenge is to preserve subtle actions in the eye region, while obscuring the salient identity cues from the eyes. We evaluated the effectiveness of identity-masking algorithms based on Canny filters, applied with and without eye enhancement, for interfering with identification and preserving facial actions. In Experiments 1 and 2, we tested human participants' ability to match the facial identity of a driver in a low resolution video to a high resolution facial image. Results showed that both masking methods impaired identification, and that eye enhancement did not alter the effectiveness of the Canny filter mask. In Experiment 3, we tested action preservation and found that neither method interfered significantly with driver action perception. We conclude that relatively simple, filter-based masking algorithms, which are suitable for application to low quality video, can be used in privacy protection without compromising action perception.



### On Multi-Agent Deep Deterministic Policy Gradients and their Explainability for SMARTS Environment
- **Arxiv ID**: http://arxiv.org/abs/2301.09420v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09420v1)
- **Published**: 2023-01-20 03:17:16+00:00
- **Updated**: 2023-01-20 03:17:16+00:00
- **Authors**: Ansh Mittal, Aditya Malte
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-Agent RL or MARL is one of the complex problems in Autonomous Driving literature that hampers the release of fully-autonomous vehicles today. Several simulators have been in iteration after their inception to mitigate the problem of complex scenarios with multiple agents in Autonomous Driving. One such simulator--SMARTS, discusses the importance of cooperative multi-agent learning. For this problem, we discuss two approaches--MAPPO and MADDPG, which are based on-policy and off-policy RL approaches. We compare our results with the state-of-the-art results for this challenge and discuss the potential areas of improvement while discussing the explainability of these approaches in conjunction with waypoints in the SMARTS environment.



### Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2301.08413v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08413v3)
- **Published**: 2023-01-20 03:39:35+00:00
- **Updated**: 2023-08-14 07:25:27+00:00
- **Authors**: Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang
- **Comment**: Accepted by ACM MM2023
- **Journal**: None
- **Summary**: Source-free domain adaptation (SFDA), where only a pre-trained source model is used to adapt to the target distribution, is a more general approach to achieving domain adaptation in the real world. However, it can be challenging to capture the inherent structure of the target features accurately due to the lack of supervised information on the target domain. By analyzing the clustering performance of the target features, we show that they still contain core features related to discriminative attributes but lack the collation of semantic information. Inspired by this insight, we present Chaos to Order (CtO), a novel approach for SFDA that strives to constrain semantic credibility and propagate label information among target subpopulations. CtO divides the target data into inner and outlier samples based on the adaptive threshold of the learning state, customizing the learning strategy to fit the data properties best. Specifically, inner samples are utilized for learning intra-class structure thanks to their relatively well-clustered properties. The low-density outlier samples are regularized by input consistency to achieve high accuracy with respect to the ground truth labels. In CtO, by employing different learning strategies to propagate the labels from the inner local to outlier instances, it clusters the global samples from chaos to order. We further adaptively regulate the neighborhood affinity of the inner samples to constrain the local semantic credibility. In theoretical and empirical analyses, we demonstrate that our algorithm not only propagates from inner to outlier but also prevents local clustering from forming spurious clusters. Empirical evidence demonstrates that CtO outperforms the state of the arts on three public benchmarks: Office-31, Office-Home, and VisDA.



### FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.08414v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.08414v2)
- **Published**: 2023-01-20 04:02:13+00:00
- **Updated**: 2023-02-07 09:48:34+00:00
- **Authors**: Junyu Zhu, Lina Liu, Yong Liu, Wanlong Li, Feng Wen, Hongbo Zhang
- **Comment**: Accepted by ICRA2023
- **Journal**: None
- **Summary**: The great potential of unsupervised monocular depth estimation has been demonstrated by many works due to low annotation cost and impressive accuracy comparable to supervised methods. To further improve the performance, recent works mainly focus on designing more complex network structures and exploiting extra supervised information, e.g., semantic segmentation. These methods optimize the models by exploiting the reconstructed relationship between the target and reference images in varying degrees. However, previous methods prove that this image reconstruction optimization is prone to get trapped in local minima. In this paper, our core idea is to guide the optimization with prior knowledge from pretrained Flow-Net. And we show that the bottleneck of unsupervised monocular depth estimation can be broken with our simple but effective framework named FG-Depth. In particular, we propose (i) a flow distillation loss to replace the typical photometric loss that limits the capacity of the model and (ii) a prior flow based mask to remove invalid pixels that bring the noise in training loss. Extensive experiments demonstrate the effectiveness of each component, and our approach achieves state-of-the-art results on both KITTI and NYU-Depth-v2 datasets.



### Towards Robust Video Instance Segmentation with Temporal-Aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.09416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09416v1)
- **Published**: 2023-01-20 05:22:16+00:00
- **Updated**: 2023-01-20 05:22:16+00:00
- **Authors**: Zhenghao Zhang, Fangtao Shao, Zuozhuo Dai, Siyu Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing transformer based video instance segmentation methods extract per frame features independently, hence it is challenging to solve the appearance deformation problem. In this paper, we observe the temporal information is important as well and we propose TAFormer to aggregate spatio-temporal features both in transformer encoder and decoder. Specifically, in transformer encoder, we propose a novel spatio-temporal joint multi-scale deformable attention module which dynamically integrates the spatial and temporal information to obtain enriched spatio-temporal features. In transformer decoder, we introduce a temporal self-attention module to enhance the frame level box queries with the temporal relation. Moreover, TAFormer adopts an instance level contrastive loss to increase the discriminability of instance query embeddings. Therefore the tracking error caused by visually similar instances can be decreased. Experimental results show that TAFormer effectively leverages the spatial and temporal information to obtain context-aware feature representation and outperforms state-of-the-art methods.



### Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.08433v2
- **DOI**: 10.1109/TCSVT.2023.3305978
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08433v2)
- **Published**: 2023-01-20 06:11:17+00:00
- **Updated**: 2023-08-18 08:11:15+00:00
- **Authors**: Shansi Zhang, Nan Meng, Edmund Y. Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation from light field (LF) images is a fundamental step for numerous applications. Recently, learning-based methods have achieved higher accuracy and efficiency than the traditional methods. However, it is costly to obtain sufficient depth labels for supervised training. In this paper, we propose an unsupervised framework to estimate depth from LF images. First, we design a disparity estimation network (DispNet) with a coarse-to-fine structure to predict disparity maps from different view combinations. It explicitly performs multi-view feature matching to learn the correspondences effectively. As occlusions may cause the violation of photo-consistency, we introduce an occlusion prediction network (OccNet) to predict the occlusion maps, which are used as the element-wise weights of photometric loss to solve the occlusion issue and assist the disparity learning. With the disparity maps estimated by multiple input combinations, we then propose a disparity fusion strategy based on the estimated errors with effective occlusion handling to obtain the final disparity map with higher accuracy. Experimental results demonstrate that our method achieves superior performance on both the dense and sparse LF images, and also shows better robustness and generalization on the real-world LF images compared to the other methods.



### DIFAI: Diverse Facial Inpainting using StyleGAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2301.08443v1
- **DOI**: 10.1109/ICIP46576.2022.9898012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08443v1)
- **Published**: 2023-01-20 06:51:34+00:00
- **Updated**: 2023-01-20 06:51:34+00:00
- **Authors**: Dongsik Yoon, Jeong-gi Kwak, Yuanming Li, David Han, Hanseok Ko
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: Image inpainting is an old problem in computer vision that restores occluded regions and completes damaged images. In the case of facial image inpainting, most of the methods generate only one result for each masked image, even though there are other reasonable possibilities. To prevent any potential biases and unnatural constraints stemming from generating only one image, we propose a novel framework for diverse facial inpainting exploiting the embedding space of StyleGAN. Our framework employs pSp encoder and SeFa algorithm to identify semantic components of the StyleGAN embeddings and feed them into our proposed SPARN decoder that adopts region normalization for plausible inpainting. We demonstrate that our proposed method outperforms several state-of-the-art methods.



### Source-free Subject Adaptation for EEG-based Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.08448v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08448v1)
- **Published**: 2023-01-20 07:01:01+00:00
- **Updated**: 2023-01-20 07:01:01+00:00
- **Authors**: Pilhyeon Lee, Seogkyu Jeon, Sunhee Hwang, Minjung Shin, Hyeran Byun
- **Comment**: Accepted by the 11th IEEE International Winter Conference on
  Brain-Computer Interface (BCI 2023). Code is available at
  https://github.com/DeepBCI/Deep-BCI
- **Journal**: None
- **Summary**: This paper focuses on subject adaptation for EEG-based visual recognition. It aims at building a visual stimuli recognition system customized for the target subject whose EEG samples are limited, by transferring knowledge from abundant data of source subjects. Existing approaches consider the scenario that samples of source subjects are accessible during training. However, it is often infeasible and problematic to access personal biological data like EEG signals due to privacy issues. In this paper, we introduce a novel and practical problem setup, namely source-free subject adaptation, where the source subject data are unavailable and only the pre-trained model parameters are provided for subject adaptation. To tackle this challenging problem, we propose classifier-based data generation to simulate EEG samples from source subjects using classifier responses. Using the generated samples and target subject data, we perform subject-independent feature learning to exploit the common knowledge shared across different subjects. Notably, our framework is generalizable and can adopt any subject-independent learning method. In the experiments on the EEG-ImageNet40 benchmark, our model brings consistent improvements regardless of the choice of subject-independent learning. Also, our method shows promising performance, recording top-1 test accuracy of 74.6% under the 5-shot setting even without relying on source data. Our code can be found at https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Source_Free_Subject_Adaptation_for_EEG.



### Spatial Steerability of GANs via Self-Supervision from Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2301.08455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08455v1)
- **Published**: 2023-01-20 07:36:29+00:00
- **Updated**: 2023-01-20 07:36:29+00:00
- **Authors**: Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou
- **Comment**: This manuscript is a journal extension of our previous conference
  work (arXiv:2112.00718), submitted to TPAMI
- **Journal**: None
- **Summary**: Generative models make huge progress to the photorealistic image synthesis in recent years. To enable human to steer the image generation process and customize the output, many works explore the interpretable dimensions of the latent space in GANs. Existing methods edit the attributes of the output image such as orientation or color scheme by varying the latent code along certain directions. However, these methods usually require additional human annotations for each pretrained model, and they mostly focus on editing global attributes. In this work, we propose a self-supervised approach to improve the spatial steerability of GANs without searching for steerable directions in the latent space or requiring extra annotations. Specifically, we design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GAN's discriminator in a self-supervised learning manner. During inference, human users can intuitively interact with the spatial heatmaps to edit the output image, such as varying the scene layout or moving objects in the scene. Extensive experiments show that the proposed method not only enables spatial editing over human faces, animal faces, outdoor scenes, and complicated indoor scenes, but also brings improvement in synthesis quality.



### Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance
- **Arxiv ID**: http://arxiv.org/abs/2301.08479v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08479v1)
- **Published**: 2023-01-20 09:17:39+00:00
- **Updated**: 2023-01-20 09:17:39+00:00
- **Authors**: Wardah Ali, Eesha Qureshi, Omama Ahmed Farooqi, Rizwan Ahmed Khan
- **Comment**: None
- **Journal**: None
- **Summary**: People all over the globe are affected by pneumonia but deaths due to it are highest in Sub-Saharan Asia and South Asia. In recent years, the overall incidence and mortality rate of pneumonia regardless of the utilization of effective vaccines and compelling antibiotics has escalated. Thus, pneumonia remains a disease that needs spry prevention and treatment. The widespread prevalence of pneumonia has caused the research community to come up with a framework that helps detect, diagnose and analyze diseases accurately and promptly. One of the major hurdles faced by the Artificial Intelligence (AI) research community is the lack of publicly available datasets for chest diseases, including pneumonia . Secondly, few of the available datasets are highly imbalanced (normal examples are over sampled, while samples with ailment are in severe minority) making the problem even more challenging. In this article we present a novel framework for the detection of pneumonia. The novelty of the proposed methodology lies in the tackling of class imbalance problem. The Generative Adversarial Network (GAN), specifically a combination of Deep Convolutional Generative Adversarial Network (DCGAN) and Wasserstein GAN gradient penalty (WGAN-GP) was applied on the minority class ``Pneumonia'' for augmentation, whereas Random Under-Sampling (RUS) was done on the majority class ``No Findings'' to deal with the imbalance problem. The ChestX-Ray8 dataset, one of the biggest datasets, is used to validate the performance of the proposed framework. The learning phase is completed using transfer learning on state-of-the-art deep learning models i.e. ResNet-50, Xception, and VGG-16. Results obtained exceed state-of-the-art.



### Exploration of Various Fractional Order Derivatives in Parkinson's Disease Dysgraphia Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.08529v1
- **DOI**: 10.1007/978-3-031-19745-1_23
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2301.08529v1)
- **Published**: 2023-01-20 12:18:05+00:00
- **Updated**: 2023-01-20 12:18:05+00:00
- **Authors**: Jan Mucha, Zoltan Galaz, Jiri Mekyska, Marcos Faundez-Zanuy, Vojtech Zvoncak, Zdenek Smekal, Lubos Brabenec, Irena Rektorova
- **Comment**: Print ISBN 978-3-031-19744-4
- **Journal**: In: Carmona-Duarte, C., Diaz, M., Ferrer, M.A., Morales, A. (eds)
  Intertwining Graphonomics with Human Movements. IGS 2022. Lecture Notes in
  Computer Science, vol 13424. Springer, Cham
- **Summary**: Parkinson's disease (PD) is a common neurodegenerative disorder with a prevalence rate estimated to 2.0% for people aged over 65 years. Cardinal motor symptoms of PD such as rigidity and bradykinesia affect the muscles involved in the handwriting process resulting in handwriting abnormalities called PD dysgraphia. Nowadays, online handwritten signal (signal with temporal information) acquired by the digitizing tablets is the most advanced approach of graphomotor difficulties analysis. Although the basic kinematic features were proved to effectively quantify the symptoms of PD dysgraphia, a recent research identified that the theory of fractional calculus can be used to improve the graphomotor difficulties analysis. Therefore, in this study, we follow up on our previous research, and we aim to explore the utilization of various approaches of fractional order derivative (FD) in the analysis of PD dysgraphia. For this purpose, we used the repetitive loops task from the Parkinson's disease handwriting database (PaHaW). Handwritten signals were parametrized by the kinematic features employing three FD approximations: Gr\"unwald-Letnikov's, Riemann-Liouville's, and Caputo's. Results of the correlation analysis revealed a significant relationship between the clinical state and the handwriting features based on the velocity. The extracted features by Caputo's FD approximation outperformed the rest of the analyzed FD approaches. This was also confirmed by the results of the classification analysis, where the best model trained by Caputo's handwriting features resulted in a balanced accuracy of 79.73% with a sensitivity of 83.78% and a specificity of 75.68%.



### Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties
- **Arxiv ID**: http://arxiv.org/abs/2301.08534v1
- **DOI**: 10.1007/978-3-031-19745-1_19
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08534v1)
- **Published**: 2023-01-20 12:30:28+00:00
- **Updated**: 2023-01-20 12:30:28+00:00
- **Authors**: Zoltan Galaz, Jiri Mekyska, Jan Mucha, Vojtech Zvoncak, Zdenek Smekal, Marcos Faundez-Zanuy, Lubos Brabenec, Ivona Moravkova, Irena Rektorova
- **Comment**: Print ISBN 978-3-031-19744-4
- **Journal**: In: Carmona-Duarte, C., Diaz, M., Ferrer, M.A., Morales, A. (eds)
  Intertwining Graphonomics with Human Movements. IGS 2022. Lecture Notes in
  Computer Science, vol 13424. Springer, Cham
- **Summary**: To this date, studies focusing on the prodromal diagnosis of Lewy body diseases (LBDs) based on quantitative analysis of graphomotor and handwriting difficulties are missing. In this work, we enrolled 18 subjects diagnosed with possible or probable mild cognitive impairment with Lewy bodies (MCI-LB), 7 subjects having more than 50% probability of developing Parkinson's disease (PD), 21 subjects with both possible/probable MCI-LB and probability of PD > 50%, and 37 age- and gender-matched healthy controls (HC). Each participant performed three tasks: Archimedean spiral drawing (to quantify graphomotor difficulties), sentence writing task (to quantify handwriting difficulties), and pentagon copying test (to quantify cognitive decline). Next, we parameterized the acquired data by various temporal, kinematic, dynamic, spatial, and task-specific features. And finally, we trained classification models for each task separately as well as a model for their combination to estimate the predictive power of the features for the identification of LBDs. Using this approach we were able to identify prodromal LBDs with 74% accuracy and showed the promising potential of computerized objective and non-invasive diagnosis of LBDs based on the assessment of graphomotor and handwriting difficulties.



### Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/2301.08571v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08571v1)
- **Published**: 2023-01-20 13:38:24+00:00
- **Updated**: 2023-01-20 13:38:24+00:00
- **Authors**: Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
- **Comment**: Paper accepted by Transactions of the Association for Computational
  Linguistics (TACL). This is a pre-MIT Press publication version. 15 pages, 6
  figures
- **Journal**: None
- **Summary**: Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent and have more narrativity compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and have more narrativity than stories generated with the current state-of-the-art model.



### Improving Sketch Colorization using Adversarial Segmentation Consistency
- **Arxiv ID**: http://arxiv.org/abs/2301.08590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08590v1)
- **Published**: 2023-01-20 14:07:30+00:00
- **Updated**: 2023-01-20 14:07:30+00:00
- **Authors**: Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu
- **Comment**: Under review at Pattern Recognition Letters. arXiv admin note:
  substantial text overlap with arXiv:2102.06192
- **Journal**: None
- **Summary**: We propose a new method for producing color images from sketches. Current solutions in sketch colorization either necessitate additional user instruction or are restricted to the "paired" translation strategy. We leverage semantic image segmentation from a general-purpose panoptic segmentation network to generate an additional adversarial loss function. The proposed loss function is compatible with any GAN model. Our method is not restricted to datasets with segmentation labels and can be applied to unpaired translation tasks as well. Using qualitative, and quantitative analysis, and based on a user study, we demonstrate the efficacy of our method on four distinct image datasets. On the FID metric, our model improves the baseline by up to 35 points. Our code, pretrained models, scripts to produce newly introduced datasets and corresponding sketch images are available at https://github.com/giddyyupp/AdvSegLoss.



### A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas
- **Arxiv ID**: http://arxiv.org/abs/2301.08605v1
- **DOI**: 10.1109/LGRS.2023.3293470
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08605v1)
- **Published**: 2023-01-20 14:34:03+00:00
- **Updated**: 2023-01-20 14:34:03+00:00
- **Authors**: Zoé Berenger, Loïc Denis, Florence Tupin, Laurent Ferro-Famil, Yue Huang
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters, January 2023
- **Journal**: None
- **Summary**: Synthetic aperture radar tomographic imaging reconstructs the three-dimensional reflectivity of a scene from a set of coherent acquisitions performed in an interferometric configuration. In forest areas, a large number of elements backscatter the radar signal within each resolution cell. To reconstruct the vertical reflectivity profile, state-of-the-art techniques perform a regularized inversion implemented in the form of iterative minimization algorithms. We show that light-weight neural networks can be trained to perform the tomographic inversion with a single feed-forward pass, leading to fast reconstructions that could better scale to the amount of data provided by the future BIOMASS mission. We train our encoder-decoder network using simulated data and validate our technique on real L-band and P-band data.



### Sanity checks and improvements for patch visualisation in prototype-based image classification
- **Arxiv ID**: http://arxiv.org/abs/2302.08508v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.08508v2)
- **Published**: 2023-01-20 15:13:04+00:00
- **Updated**: 2023-05-15 09:09:19+00:00
- **Authors**: Romain Xu-Darme, Georges Quénot, Zakaria Chihani, Marie-Christine Rousset
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.



### Image Memorability Prediction with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.08647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08647v1)
- **Published**: 2023-01-20 15:55:35+00:00
- **Updated**: 2023-01-20 15:55:35+00:00
- **Authors**: Thomas Hagen, Thomas Espeseth
- **Comment**: None
- **Journal**: None
- **Summary**: Behavioral studies have shown that the memorability of images is similar across groups of people, suggesting that memorability is a function of the intrinsic properties of images, and is unrelated to people's individual experiences and traits. Deep learning networks can be trained on such properties and be used to predict memorability in new data sets. Convolutional neural networks (CNN) have pioneered image memorability prediction, but more recently developed vision transformer (ViT) models may have the potential to yield even better predictions. In this paper, we present the ViTMem, a new memorability model based on ViT, and evaluate memorability predictions obtained by it with state-of-the-art CNN-derived models. Results showed that ViTMem performed equal to or better than state-of-the-art models on all data sets. Additional semantic level analyses revealed that ViTMem is particularly sensitive to the semantic content that drives memorability in images. We conclude that ViTMem provides a new step forward, and propose that ViT-derived models can replace CNNs for computational prediction of image memorability. Researchers, educators, advertisers, visual designers and other interested parties can leverage the model to improve the memorability of their image material.



### Automated extraction of capacitive coupling for quantum dot systems
- **Arxiv ID**: http://arxiv.org/abs/2301.08654v2
- **DOI**: 10.1103/PhysRevApplied.19.054077
- **Categories**: **cond-mat.mes-hall**, cs.CV, cs.LG, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2301.08654v2)
- **Published**: 2023-01-20 16:03:30+00:00
- **Updated**: 2023-05-25 14:56:05+00:00
- **Authors**: Joshua Ziegler, Florian Luthi, Mick Ramsey, Felix Borjans, Guoji Zheng, Justyna P. Zwolak
- **Comment**: 9 pages, 5 figures
- **Journal**: Phys. Rev. Applied 19, 054077 (2023)
- **Summary**: Gate-defined quantum dots (QDs) have appealing attributes as a quantum computing platform. However, near-term devices possess a range of possible imperfections that need to be accounted for during the tuning and operation of QD devices. One such problem is the capacitive cross-talk between the metallic gates that define and control QD qubits. A way to compensate for the capacitive cross-talk and enable targeted control of specific QDs independent of coupling is by the use of virtual gates. Here, we demonstrate a reliable automated capacitive coupling identification method that combines machine learning with traditional fitting to take advantage of the desirable properties of each. We also show how the cross-capacitance measurement may be used for the identification of spurious QDs sometimes formed during tuning experimental devices. Our systems can autonomously flag devices with spurious dots near the operating regime, which is crucial information for reliable tuning to a regime suitable for qubit operations.



### AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2301.08664v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.08664v2)
- **Published**: 2023-01-20 16:30:44+00:00
- **Updated**: 2023-01-24 11:04:47+00:00
- **Authors**: Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu
- **Comment**: Accepted by 2023 IEEE INFOCOM
- **Journal**: None
- **Summary**: The quality of the video stream is key to neural network-based video analytics. However, low-quality video is inevitably collected by existing surveillance systems because of poor quality cameras or over-compressed/pruned video streaming protocols, e.g., as a result of upstream bandwidth limit. To address this issue, existing studies use quality enhancers (e.g., neural super-resolution) to improve the quality of videos (e.g., resolution) and eventually ensure inference accuracy. Nevertheless, directly applying quality enhancers does not work in practice because it will introduce unacceptable latency. In this paper, we present AccDecoder, a novel accelerated decoder for real-time and neural-enhanced video analytics. AccDecoder can select a few frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality by neural super-resolution and then up-scale the unselected frames that reference them, which leads to 6-21% accuracy improvement. AccDecoder provides efficient inference capability via filtering important frames using DRL for DNN-based inference and reusing the results for the other frames via extracting the reference relationship among frames and blocks, which results in a latency reduction of 20-80% than baselines.



### Holistically Explainable Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.08669v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.08669v1)
- **Published**: 2023-01-20 16:45:34+00:00
- **Updated**: 2023-01-20 16:45:34+00:00
- **Authors**: Moritz Böhle, Mario Fritz, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon.



### Deep-Learning Quantitative Structural Characterization in Additive Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2302.06389v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/2302.06389v1)
- **Published**: 2023-01-20 17:59:45+00:00
- **Updated**: 2023-01-20 17:59:45+00:00
- **Authors**: Amra Peles, Vincent C. Paquit, Ryan R. Dehoff
- **Comment**: None
- **Journal**: None
- **Summary**: With a goal of accelerating fabrication of additively manufactured components with precise microstructures, we developed a method for structural characterization of key features in additively manufactured materials and parts. The method utilizes deep learning based on an image-to-image translation conditional Generative Adversarial Neural Network architecture and enables fast and incrementally more accurate predictions of the prevalent geometric features, including melt pool boundaries and printing induced defects visible in etched optical images. These structural details are heterogeneous in nature. Our method specifies the microstructure state of an additive built via statistical distribution of structural details, based on an ensemble of collected images. Extensions of the method are proposed to address Artificial Intelligence implementation of developed machine learning model for in real time control of additive manufacturing.



### A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox
- **Arxiv ID**: http://arxiv.org/abs/2302.10277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10277v1)
- **Published**: 2023-01-20 18:11:43+00:00
- **Updated**: 2023-01-20 18:11:43+00:00
- **Authors**: Sourav Saha, Trina Chakraborty, Rejwan Bin Sulaiman, Tithi Paul
- **Comment**: None
- **Journal**: None
- **Summary**: Monkeypox is a rare disease that raised concern among medical specialists following the convi-19 pandemic. It's concerning since monkeypox is difficult to diagnose early on because of symptoms that are similar to chickenpox and measles. Furthermore, because this is a rare condition, there is a knowledge gap among healthcare professionals. As a result, there is an urgent need for a novel technique to combat and anticipate the disease in the early phases of individual virus infection. Multiple CNN-based pre-trained models, including VGG-16, VGG-19, Restnet50, Inception-V3, Densnet, Xception, MobileNetV2, Alexnet, Lenet, and majority Voting, were employed in classification in this study. For this study, multiple data sets were combined, such as monkeypox vs chickenpox, monkeypox versus measles, monkeypox versus normal, and monkeypox versus all diseases. Majority voting performed 97% in monkeypox vs chickenpox, Xception achieved 79% in monkeypox against measles, MobileNetV2 scored 96% in monkeypox vs normal, and Lenet performed 80% in monkeypox versus all.



### Novel-View Acoustic Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.08730v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.08730v2)
- **Published**: 2023-01-20 18:49:58+00:00
- **Updated**: 2023-01-23 17:11:30+00:00
- **Authors**: Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi
- **Comment**: Project page: https://vision.cs.utexas.edu/projects/nvas
- **Journal**: None
- **Summary**: We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source viewpoint, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues. To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel-view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Unlocked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos.



### FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.08739v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.08739v3)
- **Published**: 2023-01-20 18:59:57+00:00
- **Updated**: 2023-07-14 18:57:30+00:00
- **Authors**: Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, Song Han
- **Comment**: CVPR 2023. The first two authors contributed equally to this work.
  Project page: https://flatformer.mit.edu
- **Journal**: None
- **Summary**: Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks.



### Estimation of mitral valve hinge point coordinates -- deep neural net for echocardiogram segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.08782v1
- **DOI**: 10.24132/CSRN.3201.21
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08782v1)
- **Published**: 2023-01-20 19:46:16+00:00
- **Updated**: 2023-01-20 19:46:16+00:00
- **Authors**: Christian Schmidt, Heinrich Martin Overhoff
- **Comment**: 8 Pages, 11 figures Presented at WSCG 2022
- **Journal**: Computer Science Research Notes , CSRN 3201, 2022, ISSN 2464-4617
- **Summary**: Cardiac image segmentation is a powerful tool in regard to diagnostics and treatment of cardiovascular diseases. Purely feature-based detection of anatomical structures like the mitral valve is a laborious task due to specifically required feature engineering and is especially challenging in echocardiograms, because of their inherently low contrast and blurry boundaries between some anatomical structures. With the publication of further annotated medical datasets and the increase in GPU processing power, deep learning-based methods in medical image segmentation became more feasible in the past years. We propose a fully automatic detection method for mitral valve hinge points, which uses a U-Net based deep neural net to segment cardiac chambers in echocardiograms in a first step, and subsequently extracts the mitral valve hinge points from the resulting segmentations in a second step. Results measured with this automatic detection method were compared to reference coordinate values, which with median absolute hinge point coordinate errors of 1.35 mm for the x- (15-85 percentile range: [0.3 mm; 3.15 mm]) and 0.75 mm for the y- coordinate (15-85 percentile range: [0.15 mm; 1.88 mm]).



### An Asynchronous Intensity Representation for Framed and Event Video Sources
- **Arxiv ID**: http://arxiv.org/abs/2301.08783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.08783v1)
- **Published**: 2023-01-20 19:46:23+00:00
- **Updated**: 2023-01-20 19:46:23+00:00
- **Authors**: Andrew C. Freeman, Montek Singh, Ketan Mayer-Patel
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Neuromorphic "event" cameras, designed to mimic the human vision system with asynchronous sensing, unlock a new realm of high-speed and high dynamic range applications. However, researchers often either revert to a framed representation of event data for applications, or build bespoke applications for a particular camera's event data type. To usher in the next era of video systems, accommodate new event camera designs, and explore the benefits to asynchronous video in classical applications, we argue that there is a need for an asynchronous, source-agnostic video representation. In this paper, we introduce a novel, asynchronous intensity representation for both framed and non-framed data sources. We show that our representation can increase intensity precision and greatly reduce the number of samples per pixel compared to grid-based representations. With framed sources, we demonstrate that by permitting a small amount of loss through the temporal averaging of similar pixel values, we can reduce our representational sample rate by more than half, while incurring a drop in VMAF quality score of only 4.5. We also demonstrate lower latency than the state-of-the-art method for fusing and transcoding framed and event camera data to an intensity representation, while maintaining $2000\times$ the temporal resolution. We argue that our method provides the computational efficiency and temporal granularity necessary to build real-time intensity-based applications for event cameras.



### Visual Semantic Relatedness Dataset for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2301.08784v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08784v2)
- **Published**: 2023-01-20 20:04:35+00:00
- **Updated**: 2023-04-30 20:23:09+00:00
- **Authors**: Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
- **Comment**: Project Page: bit.ly/project-page-paper
- **Journal**: None
- **Summary**: Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.



### Robot Skill Learning Via Classical Robotics-Based Generated Datasets: Advantages, Disadvantages, and Future Improvement
- **Arxiv ID**: http://arxiv.org/abs/2301.08794v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.IR, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2301.08794v1)
- **Published**: 2023-01-20 20:37:46+00:00
- **Updated**: 2023-01-20 20:37:46+00:00
- **Authors**: Batu Kaan Oezen
- **Comment**: None
- **Journal**: None
- **Summary**: Why do we not profit from our long-existing classical robotics knowledge and look for some alternative way for data collection? The situation ignoring all existing methods might be such a waste. This article argues that a dataset created using a classical robotics algorithm is a crucial part of future development. This developed classic algorithm has a perfect domain adaptation and generalization property, and most importantly, collecting datasets based on them is quite easy. It is well known that current robot skill-learning approaches perform exceptionally badly in the unseen domain, and their performance against adversarial attacks is quite limited as long as they do not have a very exclusive big dataset. Our experiment is the initial steps of using a dataset created by classical robotics codes. Our experiment investigated possible trajectory collection based on classical robotics. It addressed some advantages and disadvantages and pointed out other future development ideas.



### DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels
- **Arxiv ID**: http://arxiv.org/abs/2301.08798v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08798v1)
- **Published**: 2023-01-20 20:54:25+00:00
- **Updated**: 2023-01-20 20:54:25+00:00
- **Authors**: Yunan Wu, Amil Dravid, Ramsey Michael Wehbe, Aggelos K. Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: Propose: To present DeepCOVID-Fuse, a deep learning fusion model to predict risk levels in patients with confirmed coronavirus disease 2019 (COVID-19) and to evaluate the performance of pre-trained fusion models on full or partial combination of chest x-ray (CXRs) or chest radiograph and clinical variables.   Materials and Methods: The initial CXRs, clinical variables and outcomes (i.e., mortality, intubation, hospital length of stay, ICU admission) were collected from February 2020 to April 2020 with reverse-transcription polymerase chain reaction (RT-PCR) test results as the reference standard. The risk level was determined by the outcome. The fusion model was trained on 1657 patients (Age: 58.30 +/- 17.74; Female: 807) and validated on 428 patients (56.41 +/- 17.03; 190) from Northwestern Memorial HealthCare system and was tested on 439 patients (56.51 +/- 17.78; 205) from a single holdout hospital. Performance of pre-trained fusion models on full or partial modalities were compared on the test set using the DeLong test for the area under the receiver operating characteristic curve (AUC) and the McNemar test for accuracy, precision, recall and F1.   Results: The accuracy of DeepCOVID-Fuse trained on CXRs and clinical variables is 0.658, with an AUC of 0.842, which significantly outperformed (p < 0.05) models trained only on CXRs with an accuracy of 0.621 and AUC of 0.807 and only on clinical variables with an accuracy of 0.440 and AUC of 0.502. The pre-trained fusion model with only CXRs as input increases accuracy to 0.632 and AUC to 0.813 and with only clinical variables as input increases accuracy to 0.539 and AUC to 0.733.   Conclusion: The fusion model learns better feature representations across different modalities during training and achieves good outcome predictions even when only some of the modalities are used in testing.



### In-situ Water quality monitoring in Oil and Gas operations
- **Arxiv ID**: http://arxiv.org/abs/2301.08800v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.CO, stat.ME, G.1; I.1; I.2; I.4; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2301.08800v2)
- **Published**: 2023-01-20 20:56:52+00:00
- **Updated**: 2023-07-23 02:04:40+00:00
- **Authors**: Satish Kumar, Rui Kou, Henry Hill, Jake Lempges, Eric Qian, Vikram Jayaram
- **Comment**: 15 pages, 8 figures, SPIE Defense + Commercial: Algorithms,
  Technologies, and Applications for Multispectral and Hyperspectral Imaging
  XXIX
- **Journal**: None
- **Summary**: From agriculture to mining, to energy, surface water quality monitoring is an essential task. As oil and gas operators work to reduce the consumption of freshwater, it is increasingly important to actively manage fresh and non-fresh water resources over the long term. For large-scale monitoring, manual sampling at many sites has become too time-consuming and unsustainable, given the sheer number of dispersed ponds, small lakes, playas, and wetlands over a large area. Therefore, satellite-based environmental monitoring presents great potential. Many existing satellite-based monitoring studies utilize index-based methods to monitor large water bodies such as rivers and oceans. However, these existing methods fail when monitoring small ponds-the reflectance signal received from small water bodies is too weak to detect. To address this challenge, we propose a new Water Quality Enhanced Index (WQEI) Model, which is designed to enable users to determine contamination levels in water bodies with weak reflectance patterns. Our results show that 1) WQEI is a good indicator of water turbidity validated with 1200 water samples measured in the laboratory, and 2) by applying our method to commonly available satellite data (e.g. LandSat8), one can achieve high accuracy water quality monitoring efficiently in large regions. This provides a tool for operators to optimize the quality of water stored within surface storage ponds and increasing the readiness and availability of non-fresh water.



### Impact of PCA-based preprocessing and different CNN structures on deformable registration of sonograms
- **Arxiv ID**: http://arxiv.org/abs/2301.08802v1
- **DOI**: 10.24132/CSRN.3201.23
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08802v1)
- **Published**: 2023-01-20 21:01:39+00:00
- **Updated**: 2023-01-20 21:01:39+00:00
- **Authors**: Christian Schmidt, Heinrich Martin Overhoff
- **Comment**: 8 pages, 7 figures Presented at WSCG 2022
- **Journal**: Computer Science Research Notes , CSRN 3201, 2022
- **Summary**: Central venous catheters (CVC) are commonly inserted into the large veins of the neck, e.g. the internal jugular vein (IJV). CVC insertion may cause serious complications like misplacement into an artery or perforation of cervical vessels. Placing a CVC under sonographic guidance is an appropriate method to reduce such adverse events, if anatomical landmarks like venous and arterial vessels can be detected reliably. This task shall be solved by registration of patient individual images vs. an anatomically labelled reference image. In this work, a linear, affine transformation is performed on cervical sonograms, followed by a non-linear transformation to achieve a more precise registration. Voxelmorph (VM), a learning-based library for deformable image registration using a convolutional neural network (CNN) with U-Net structure was used for non-linear transformation. The impact of principal component analysis (PCA)-based pre-denoising of patient individual images, as well as the impact of modified net structures with differing complexities on registration results were examined visually and quantitatively, the latter using metrics for deformation and image similarity. Using the PCA-approximated cervical sonograms resulted in decreased mean deformation lengths between 18% and 66% compared to their original image counterparts, depending on net structure. In addition, reducing the number of convolutional layers led to improved image similarity with PCA images, while worsening in original images. Despite a large reduction of network parameters, no overall decrease in registration quality was observed, leading to the conclusion that the original net structure is oversized for the task at hand.



### DiffusionCT: Latent Diffusion Model for CT Image Standardization
- **Arxiv ID**: http://arxiv.org/abs/2301.08815v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.08815v2)
- **Published**: 2023-01-20 22:13:48+00:00
- **Updated**: 2023-03-25 21:29:16+00:00
- **Authors**: Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen
- **Comment**: 6 pages, 03 figures and 01 tables
- **Journal**: None
- **Summary**: Computed tomography (CT) is one of the modalities for effective lung cancer screening, diagnosis, treatment, and prognosis. The features extracted from CT images are now used to quantify spatial and temporal variations in tumors. However, CT images obtained from various scanners with customized acquisition protocols may introduce considerable variations in texture features, even for the same patient. This presents a fundamental challenge to downstream studies that require consistent and reliable feature analysis. Existing CT image harmonization models rely on GAN-based supervised or semi-supervised learning, with limited performance. This work addresses the issue of CT image harmonization using a new diffusion-based model, named DiffusionCT, to standardize CT images acquired from different vendors and protocols. DiffusionCT operates in the latent space by mapping a latent non-standard distribution into a standard one. DiffusionCT incorporates an Unet-based encoder-decoder, augmented by a diffusion model integrated into the bottleneck part. The model is designed in two training phases. The encoder-decoder is first trained, without embedding the diffusion model, to learn the latent representation of the input data. The latent diffusion model is then trained in the next training phase while fixing the encoder-decoder. Finally, the decoder synthesizes a standardized image with the transformed latent representation. The experimental results demonstrate a significant improvement in the performance of the standardization task using DiffusionCT.



