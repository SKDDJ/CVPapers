# Arxiv Papers in cs.CV on 2023-01-25
### Data Consistent Deep Rigid MRI Motion Correction
- **Arxiv ID**: http://arxiv.org/abs/2301.10365v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10365v1)
- **Published**: 2023-01-25 00:21:31+00:00
- **Updated**: 2023-01-25 00:21:31+00:00
- **Authors**: Nalini M. Singh, Neel Dey, Malte Hoffmann, Bruce Fischl, Elfar Adalsteinsson, Robert Frost, Adrian V. Dalca, Polina Golland
- **Comment**: 13 pages, 5 figures, motion correction, magnetic resonance imaging,
  deep learning
- **Journal**: None
- **Summary**: Motion artifacts are a pervasive problem in MRI, leading to misdiagnosis or mischaracterization in population-level imaging studies. Current retrospective rigid intra-slice motion correction techniques jointly optimize estimates of the image and the motion parameters. In this paper, we use a deep network to reduce the joint image-motion parameter search to a search over rigid motion parameters alone. Our network produces a reconstruction as a function of two inputs: corrupted k-space data and motion parameters. We train the network using simulated, motion-corrupted k-space data generated from known motion parameters. At test-time, we estimate unknown motion parameters by minimizing a data consistency loss between the motion parameters, the network-based image reconstruction given those parameters, and the acquired measurements. Intra-slice motion correction experiments on simulated and realistic 2D fast spin echo brain MRI achieve high reconstruction fidelity while retaining the benefits of explicit data consistency-based optimization. Our code is publicly available at https://www.github.com/nalinimsingh/neuroMoCo.



### Local Feature Extraction from Salient Regions by Feature Map Transformation
- **Arxiv ID**: http://arxiv.org/abs/2301.10413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10413v1)
- **Published**: 2023-01-25 05:31:20+00:00
- **Updated**: 2023-01-25 05:31:20+00:00
- **Authors**: Yerim Jung, Nur Suriza Syazwany Binti Ahmad Nizam, Sang-Chul Lee
- **Comment**: British Machine Vision Conference (BMVC) 2022
- **Journal**: None
- **Summary**: Local feature matching is essential for many applications, such as localization and 3D reconstruction. However, it is challenging to match feature points accurately in various camera viewpoints and illumination conditions. In this paper, we propose a framework that robustly extracts and describes salient local features regardless of changing light and viewpoints. The framework suppresses illumination variations and encourages structural information to ignore the noise from light and to focus on edges. We classify the elements in the feature covariance matrix, an implicit feature map information, into two components. Our model extracts feature points from salient regions leading to reduced incorrect matches. In our experiments, the proposed method achieved higher accuracy than the state-of-the-art methods in the public dataset, such as HPatches, Aachen Day-Night, and ETH, which especially show highly variant viewpoints and illumination.



### DEJA VU: Continual Model Generalization For Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2301.10418v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10418v2)
- **Published**: 2023-01-25 05:56:53+00:00
- **Updated**: 2023-03-10 18:17:34+00:00
- **Authors**: Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao Wang, Qi Zhu
- **Comment**: Published as a conference paper at ICLR 2023
- **Journal**: None
- **Summary**: In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation - in what we call the "Unfamiliar Period", especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.



### Bias-Compensated Integral Regression for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.10431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10431v1)
- **Published**: 2023-01-25 06:54:04+00:00
- **Updated**: 2023-01-25 06:54:04+00:00
- **Authors**: Kerui Gu, Linlin Yang, Michael Bi Mi, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: In human and hand pose estimation, heatmaps are a crucial intermediate representation for a body or hand keypoint. Two popular methods to decode the heatmap into a final joint coordinate are via an argmax, as done in heatmap detection, or via softmax and expectation, as done in integral regression. Integral regression is learnable end-to-end, but has lower accuracy than detection. This paper uncovers an induced bias from integral regression that results from combining the softmax and the expectation operation. This bias often forces the network to learn degenerately localized heatmaps, obscuring the keypoint's true underlying distribution and leads to lower accuracies. Training-wise, by investigating the gradients of integral regression, we show that the implicit guidance of integral regression to update the heatmap makes it slower to converge than detection. To counter the above two limitations, we propose Bias Compensated Integral Regression (BCIR), an integral regression-based framework that compensates for the bias. BCIR also incorporates a Gaussian prior loss to speed up training and improve prediction accuracy. Experimental results on both the human body and hand benchmarks show that BCIR is faster to train and more accurate than the original integral regression, making it competitive with state-of-the-art detection methods.



### Learning Trustworthy Model from Noisy Labels based on Rough Set for Surface Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.10441v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10441v1)
- **Published**: 2023-01-25 07:29:22+00:00
- **Updated**: 2023-01-25 07:29:22+00:00
- **Authors**: Tongzhi Niu, Bin Li, Kai Li, Yufeng Lin, Yuwei Li, Weifeng Li, Zhenrong Wang
- **Comment**: 12 pages, 8figures
- **Journal**: None
- **Summary**: In the surface defect detection, there are some suspicious regions that cannot be uniquely classified as abnormal or normal. The annotating of suspicious regions is easily affected by factors such as workers' emotional fluctuations and judgment standard, resulting in noisy labels, which in turn leads to missing and false detections, and ultimately leads to inconsistent judgments of product quality. Unlike the usual noisy labels, the ones used for surface defect detection appear to be inconsistent rather than mislabeled. The noise occurs in almost every label and is difficult to correct or evaluate. In this paper, we proposed a framework that learns trustworthy models from noisy labels for surface defect defection. At first, to avoid the negative impact of noisy labels on the model, we represent the suspicious regions with consistent and precise elements at the pixel-level and redesign the loss function. Secondly, without changing network structure and adding any extra labels, pluggable spatially correlated Bayesian module is proposed. Finally, the defect discrimination confidence is proposed to measure the uncertainty, with which anomalies can be identified as defects. Our results indicate not only the effectiveness of the proposed method in learning from noisy labels, but also robustness and real-time performance.



### A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.10454v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10454v1)
- **Published**: 2023-01-25 08:13:50+00:00
- **Updated**: 2023-01-25 08:13:50+00:00
- **Authors**: Mohammad Azizmalayeri, Arman Zarei, Alireza Isavand, Mohammad Taghi Manzuri, Mohammad Hossein Rohban
- **Comment**: Accepted to CSICC 2023
- **Journal**: None
- **Summary**: Current machine learning models achieve super-human performance in many real-world applications. Still, they are susceptible against imperceptible adversarial perturbations. The most effective solution for this problem is adversarial training that trains the model with adversarially perturbed samples instead of original ones. Various methods have been developed over recent years to improve adversarial training such as data augmentation or modifying training attacks. In this work, we examine the same problem from a new data-centric perspective. For this purpose, we first demonstrate that the existing model-based methods can be equivalent to applying smaller perturbation or optimization weights to the hard training examples. By using this finding, we propose detecting and removing these hard samples directly from the training procedure rather than applying complicated algorithms to mitigate their effects. For detection, we use maximum softmax probability as an effective method in out-of-distribution detection since we can consider the hard samples as the out-of-distribution samples for the whole data distribution. Our results on SVHN and CIFAR-10 datasets show the effectiveness of this method in improving the adversarial training without adding too much computational cost.



### Rate-Perception Optimized Preprocessing for Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2301.10455v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.10455v1)
- **Published**: 2023-01-25 08:21:52+00:00
- **Updated**: 2023-01-25 08:21:52+00:00
- **Authors**: Chengqian Ma, Zhiqiang Wu, Chunlei Cai, Pengwei Zhang, Yi Wang, Long Zheng, Chao Chen, Quan Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decades, lots of progress have been done in the video compression field including traditional video codec and learning-based video codec. However, few studies focus on using preprocessing techniques to improve the rate-distortion performance. In this paper, we propose a rate-perception optimized preprocessing (RPP) method. We first introduce an adaptive Discrete Cosine Transform loss function which can save the bitrate and keep essential high frequency components as well. Furthermore, we also combine several state-of-the-art techniques from low-level vision fields into our approach, such as the high-order degradation model, efficient lightweight network design, and Image Quality Assessment model. By jointly using these powerful techniques, our RPP approach can achieve on average, 16.27% bitrate saving with different video encoders like AVC, HEVC, and VVC under multiple quality metrics. In the deployment stage, our RPP method is very simple and efficient which is not required any changes in the setting of video encoding, streaming, and decoding. Each input frame only needs to make a single pass through RPP before sending into video encoders. In addition, in our subjective visual quality test, 87% of users think videos with RPP are better or equal to videos by only using the codec to compress, while these videos with RPP save about 12% bitrate on average. Our RPP framework has been integrated into the production environment of our video transcoding services which serve millions of users every day.



### HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling
- **Arxiv ID**: http://arxiv.org/abs/2301.10460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10460v1)
- **Published**: 2023-01-25 08:40:34+00:00
- **Updated**: 2023-01-25 08:40:34+00:00
- **Authors**: Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first active learning tool for fine-grained 3D part labeling, a problem which challenges even the most advanced deep learning (DL) methods due to the significant structural variations among the small and intricate parts. For the same reason, the necessary data annotation effort is tremendous, motivating approaches to minimize human involvement. Our labeling tool iteratively verifies or modifies part labels predicted by a deep neural network, with human feedback continually improving the network prediction. To effectively reduce human efforts, we develop two novel features in our tool, hierarchical and symmetry-aware active labeling. Our human-in-the-loop approach, coined HAL3D, achieves 100% accuracy (barring human errors) on any test set with pre-defined hierarchical part labels, with 80% time-saving over manual effort.



### Aircraft Skin Inspections: Towards a New Model for Dent Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2301.10473v2
- **DOI**: 10.1784/insi.2023.65.7.378
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10473v2)
- **Published**: 2023-01-25 09:20:19+00:00
- **Updated**: 2023-07-11 06:40:45+00:00
- **Authors**: Pasquale Lafiosca, Ip-Shing Fan, Nicolas P. Avdelidis
- **Comment**: None
- **Journal**: None
- **Summary**: Aircraft maintenance, repair and overhaul industry is gradually switching to 3D scanning for dent inspection. High-accuracy devices allow quick and repeatable measurements, which translate into efficient reporting and more objective damage evaluations. However, the potential of 3D scanners is far from being exploited. This is due to the traditional way in which the structural repair manual deals with dents, that is, considering length, width and depth as the only relevant measures. Being equivalent to describing a dent similarly to a box, the current approach discards any information about the actual shape. This causes high degrees of ambiguity, with very different shapes (and corresponding fatigue life) being classified as the same, and nullifies the effort of acquiring such great amount of information from high-accuracy 3D scanners. In this paper a 7-parameter model is proposed to describe the actual dent shape, thus enabling the exploitation of the high fidelity data produced by 3D scanners. The compact set of values can then be compared against historical data and structural evaluations based on the same model. The proposed approach has been evaluated in both simulations and point cloud data generated by 8tree's dentCHECK tool, suggesting increased capability to evaluate damage, enabling more targeted interventions and, ultimately, saving costs.



### Flow-guided Semi-supervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.10492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10492v1)
- **Published**: 2023-01-25 10:02:31+00:00
- **Updated**: 2023-01-25 10:02:31+00:00
- **Authors**: Yushan Zhang, Andreas Robinson, Maria Magnusson, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an optical flow-guided approach for semi-supervised video object segmentation. Optical flow is usually exploited as additional guidance information in unsupervised video object segmentation. However, its relevance in semi-supervised video object segmentation has not been fully explored. In this work, we follow an encoder-decoder approach to address the segmentation task. A model to extract the combined information from optical flow and the image is proposed, which is then used as input to the target model and the decoder network. Unlike previous methods where concatenation is used to integrate information from image data and optical flow, a simple yet effective attention mechanism is exploited in our work. Experiments on DAVIS 2017 and YouTube-VOS 2019 show that by integrating the information extracted from optical flow into the original image branch results in a strong performance gain and our method achieves state-of-the-art performance.



### Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration
- **Arxiv ID**: http://arxiv.org/abs/2302.10306v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.10306v1)
- **Published**: 2023-01-25 11:00:32+00:00
- **Updated**: 2023-01-25 11:00:32+00:00
- **Authors**: Masoud Shahraki Mohammadi, Seyed Javad Seyed Mahdavi Chabok
- **Comment**: None
- **Journal**: None
- **Summary**: Enhancing quality and removing noise during preprocessing is one of the most critical steps in image processing. X-ray images are created by photons colliding with atoms and the variation in scattered noise absorption. This noise causes the graph's quality of medical to decline and, occasionally, causes it to repeat itself, causing an elevation in the patient's effective dose. One of the most critical challenges in this area has consistently been lowering the image noise. Techniques like BM3d, low-pass filters, and Autoencoder have taken this step. Due to the algorithm's structure and high repetition rate, neural networks using various architectures have reduced noise with acceptable results over the past ten years compared to the traditional BM3D and low-pass filters. The Hankel matrix combined with neural networks is one of these configurations. The Hankel matrix seeks a local circle by splitting up individual values into local and non-local components using a non-local matrix. A non-local matrix can be created using the wave or DCT. This paper proposes combining the waveform with the Daubechies (D4) wavelength because it has more energy and uses the u-Net neural network structure, which uses the waveform alone at each stage. The outcomes were evaluated using the PSNR and SSIM criteria, and the outcomes were verified by using various waves. The effectiveness of a one-wave network has increased from 0.5% to 1.2%, according to studies done on other datasets.



### Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2301.10520v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10520v2)
- **Published**: 2023-01-25 11:02:09+00:00
- **Updated**: 2023-04-11 08:16:55+00:00
- **Authors**: Magdalena Wysocki, Mohammad Farid Azampour, Christine Eilers, Benjamin Busam, Mehrdad Salehi, Nassir Navab
- **Comment**: accepted for oral presentation at MIDL 2023
  (https://openreview.net/forum?id=x4McMBwVyi)
- **Journal**: None
- **Summary**: We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tracked with a robotic arm. The experiments corroborate that our method generates US frames that enable consistent volume compounding from previously unseen views. To the best of our knowledge, the presented work is the first to address view-dependent US image synthesis using INR.



### 3D Tooth Mesh Segmentation with Simplified Mesh Cell Representation
- **Arxiv ID**: http://arxiv.org/abs/2301.10531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.10531v1)
- **Published**: 2023-01-25 11:43:56+00:00
- **Updated**: 2023-01-25 11:43:56+00:00
- **Authors**: Ananya Jana, Hrebesh Molly Subhash, Dimitris N. Metaxas
- **Comment**: accepted at IEEE ISBI 2023 International Symposium on Biomedical
  Imaging
- **Journal**: None
- **Summary**: Manual tooth segmentation of 3D tooth meshes is tedious and there is variations among dentists. %Manual tooth annotation of 3D tooth meshes is a tedious task. Several deep learning based methods have been proposed to perform automatic tooth mesh segmentation. Many of the proposed tooth mesh segmentation algorithms summarize the mesh cell as - the cell center or barycenter, the normal at barycenter, the cell vertices and the normals at the cell vertices. Summarizing of the mesh cell/triangle in this manner imposes an implicit structural constraint and makes it difficult to work with multiple resolutions which is done in many point cloud based deep learning algorithms. We propose a novel segmentation method which utilizes only the barycenter and the normal at the barycenter information of the mesh cell and yet achieves competitive performance. We are the first to demonstrate that it is possible to relax the implicit structural constraint and yet achieve superior segmentation performance



### Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN
- **Arxiv ID**: http://arxiv.org/abs/2301.10540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10540v2)
- **Published**: 2023-01-25 12:12:47+00:00
- **Updated**: 2023-04-16 08:55:36+00:00
- **Authors**: David M. Knigge, David W. Romero, Albert Gu, Efstratios Gavves, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn, Jan-Jakob Sonke
- **Comment**: None
- **Journal**: None
- **Summary**: Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\rm D}$), visual ($2{\rm D}$) and point-cloud ($3{\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.



### Variation-Aware Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.10551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10551v1)
- **Published**: 2023-01-25 12:35:17+00:00
- **Updated**: 2023-01-25 12:35:17+00:00
- **Authors**: Mingle Xu, Jaehwan Lee, Sook Yoon, Hyongsuk Kim, Dong Sun Park
- **Comment**: 12 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Semantic image synthesis (SIS) aims to produce photorealistic images aligning to given conditional semantic layout and has witnessed a significant improvement in recent years. Although the diversity in image-level has been discussed heavily, class-level mode collapse widely exists in current algorithms. Therefore, we declare a new requirement for SIS to achieve more photorealistic images, variation-aware, which consists of inter- and intra-class variation. The inter-class variation is the diversity between different semantic classes while the intra-class variation stresses the diversity inside one class. Through analysis, we find that current algorithms elusively embrace the inter-class variation but the intra-class variation is still not enough. Further, we introduce two simple methods to achieve variation-aware semantic image synthesis (VASIS) with a higher intra-class variation, semantic noise and position code. We combine our method with several state-of-the-art algorithms and the experimental result shows that our models generate more natural images and achieves slightly better FIDs and/or mIoUs than the counterparts. Our codes and models will be publicly available.



### Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2301.10559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10559v2)
- **Published**: 2023-01-25 13:00:16+00:00
- **Updated**: 2023-05-16 09:46:02+00:00
- **Authors**: Chamath Abeysinghe, Chris Reid, Hamid Rezatofighi, Bernd Meyer
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking individuals is a vital part of many experiments conducted to understand collective behaviour. Ants are the paradigmatic model system for such experiments but their lack of individually distinguishing visual features and their high colony densities make it extremely difficult to perform reliable tracking automatically. Additionally, the wide diversity of their species' appearances makes a generalized approach even harder. In this paper, we propose a data-driven multi-object tracker that, for the first time, employs domain adaptation to achieve the required generalisation. This approach is built upon a joint-detection-and-tracking framework that is extended by a set of domain discriminator modules integrating an adversarial training strategy in addition to the tracking loss. In addition to this novel domain-adaptive tracking framework, we present a new dataset and a benchmark for the ant tracking problem. The dataset contains 57 video sequences with full trajectory annotation, including 30k frames captured from two different ant species moving on different background patterns. It comprises 33 and 24 sequences for source and target domains, respectively. We compare our proposed framework against other domain-adaptive and non-domain-adaptive multi-object tracking baselines using this dataset and show that incorporating domain adaptation at multiple levels of the tracking pipeline yields significant improvements. The code and the dataset are available at https://github.com/chamathabeysinghe/da-tracker.



### Trainable Loss Weights in Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2301.10575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07 (Primary) 68T45 (Secondary), I.4
- **Links**: [PDF](http://arxiv.org/pdf/2301.10575v1)
- **Published**: 2023-01-25 13:27:27+00:00
- **Updated**: 2023-01-25 13:27:27+00:00
- **Authors**: Arash Chaichi Mellatshahi, Shohreh Kasaei
- **Comment**: 7 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: In recent years, research on super-resolution has primarily focused on the development of unsupervised models, blind networks, and the use of optimization methods in non-blind models. But, limited research has discussed the loss function in the super-resolution process. The majority of those studies have only used perceptual similarity in a conventional way. This is while the development of appropriate loss can improve the quality of other methods as well. In this article, a new weighting method for pixel-wise loss is proposed. With the help of this method, it is possible to use trainable weights based on the general structure of the image and its perceptual features while maintaining the advantages of pixel-wise loss. Also, a criterion for comparing weights of loss is introduced so that the weights can be estimated directly by a convolutional neural network using this criterion. In addition, in this article, the expectation-maximization method is used for the simultaneous estimation super-resolution network and weighting network. In addition, a new activation function, called "FixedSum", is introduced which can keep the sum of all components of vector constants while keeping the output components between zero and one. As shown in the experimental results section, weighted loss by the proposed method leads to better results than the unweighted loss in both signal-to-noise and perceptual similarity senses.



### An Efficient Approximate Method for Online Convolutional Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.10583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10583v1)
- **Published**: 2023-01-25 13:40:18+00:00
- **Updated**: 2023-01-25 13:40:18+00:00
- **Authors**: Farshad G. Veshki, Sergiy A. Vorobyov
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing convolutional dictionary learning (CDL) algorithms are based on batch learning, where the dictionary filters and the convolutional sparse representations are optimized in an alternating manner using a training dataset. When large training datasets are used, batch CDL algorithms become prohibitively memory-intensive. An online-learning technique is used to reduce the memory requirements of CDL by optimizing the dictionary incrementally after finding the sparse representations of each training sample. Nevertheless, learning large dictionaries using the existing online CDL (OCDL) algorithms remains highly computationally expensive. In this paper, we present a novel approximate OCDL method that incorporates sparse decomposition of the training samples. The resulting optimization problems are addressed using the alternating direction method of multipliers. Extensive experimental evaluations using several image datasets show that the proposed method substantially reduces computational costs while preserving the effectiveness of the state-of-the-art OCDL algorithms.



### A Method For Eliminating Contour Errors In Self-Encoder Reconstructed Images
- **Arxiv ID**: http://arxiv.org/abs/2301.10584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.10584v1)
- **Published**: 2023-01-25 13:40:29+00:00
- **Updated**: 2023-01-25 13:40:29+00:00
- **Authors**: Yonggang Li, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a self-supervised twin network approach based on this a priori. The method of generating the approximate10 edge information of an image and then differentially eliminating the edge errors11 in the reconstructed image with a dilate algorithm. This is used to improve the12 accuracy of the reconstructed image and to separate foreign matter and noise from13 the original image, so that it can be visualized in a more practical scene



### Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.10593v1
- **DOI**: 10.1007/978-3-031-41685-9_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10593v1)
- **Published**: 2023-01-25 13:55:14+00:00
- **Updated**: 2023-01-25 13:55:14+00:00
- **Authors**: Denis Coquenet, Clément Chatelain, Thierry Paquet
- **Comment**: None
- **Journal**: International Conference on Document Analysis and Recognition -
  ICDAR 2023
- **Summary**: Recent advances in handwritten text recognition enabled to recognize whole documents in an end-to-end way: the Document Attention Network (DAN) recognizes the characters one after the other through an attention-based prediction process until reaching the end of the document. However, this autoregressive process leads to inference that cannot benefit from any parallelization optimization. In this paper, we propose Faster DAN, a two-step strategy to speed up the recognition process at prediction time: the model predicts the first character of each text line in the document, and then completes all the text lines in parallel through multi-target queries and a specific document positional encoding scheme. Faster DAN reaches competitive results compared to standard DAN, while being at least 4 times faster on whole single-page and double-page images of the RIMES 2009, READ 2016 and MAURDOR datasets. Source code and trained model weights are available at https://github.com/FactoDeepLearning/FasterDAN.



### Deep Generative Neural Embeddings for High Dimensional Data Visualization
- **Arxiv ID**: http://arxiv.org/abs/2302.10801v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2302.10801v1)
- **Published**: 2023-01-25 14:18:09+00:00
- **Updated**: 2023-01-25 14:18:09+00:00
- **Authors**: Halid Ziya Yerebakan, Gerardo Hermosillo Valadez
- **Comment**: High Dimensional Data Visualization
- **Journal**: None
- **Summary**: We propose a visualization technique that utilizes neural network embeddings and a generative network to reconstruct original data. This method allows for independent manipulation of individual image embeddings through its non-parametric structure, providing more flexibility than traditional autoencoder approaches. We have evaluated the effectiveness of this technique in data visualization and compared it to t-SNE and VAE methods. Furthermore, we have demonstrated the scalability of our method through visualizations on the ImageNet dataset. Our technique has potential applications in human-in-the-loop training, as it allows for independent editing of embedding locations without affecting the optimization process.



### Connecting metrics for shape-texture knowledge in computer vision
- **Arxiv ID**: http://arxiv.org/abs/2301.10608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10608v1)
- **Published**: 2023-01-25 14:37:42+00:00
- **Updated**: 2023-01-25 14:37:42+00:00
- **Authors**: Tiago Oliveira, Tiago Marques, Arlindo L. Oliveira
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Modern artificial neural networks, including convolutional neural networks and vision transformers, have mastered several computer vision tasks, including object recognition. However, there are many significant differences between the behavior and robustness of these systems and of the human visual system. Deep neural networks remain brittle and susceptible to many changes in the image that do not cause humans to misclassify images. Part of this different behavior may be explained by the type of features humans and deep neural networks use in vision tasks. Humans tend to classify objects according to their shape while deep neural networks seem to rely mostly on texture. Exploring this question is relevant, since it may lead to better performing neural network architectures and to a better understanding of the workings of the vision system of primates. In this work, we advance the state of the art in our understanding of this phenomenon, by extending previous analyses to a much larger set of deep neural network architectures. We found that the performance of models in image classification tasks is highly correlated with their shape bias measured at the output and penultimate layer. Furthermore, our results showed that the number of neurons that represent shape and texture are strongly anti-correlated, thus providing evidence that there is competition between these two types of features. Finally, we observed that while in general there is a correlation between performance and shape bias, there are significant variations between architecture families.



### Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.10611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10611v2)
- **Published**: 2023-01-25 14:45:13+00:00
- **Updated**: 2023-08-30 13:50:25+00:00
- **Authors**: Indel Pal Singh, Enjie Ghorbel, Anis Kacem, Arunkumar Rathinam, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a discriminator-free adversarial-based Unsupervised Domain Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as DDA-MLIC is proposed. Recently, some attempts have been made for introducing adversarial-based UDA methods in the context of MLIC. However, these methods which rely on an additional discriminator subnet present one major shortcoming. The learning of domain-invariant features may harm their task-specific discriminative power, since the classification and discrimination tasks are decoupled. Herein, we propose to overcome this issue by introducing a novel adversarial critic that is directly deduced from the task-specific classifier. Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the source and target predictions in order to distinguish between two clusters. This allows extracting a Gaussian distribution for each component. The resulting Gaussian distributions are then used for formulating an adversarial loss based on a Frechet distance. The proposed method is evaluated on several multi-label image datasets covering three different types of domain shift. The obtained results demonstrate that DDA-MLIC outperforms existing state-of-the-art methods in terms of precision while requiring a lower number of parameters. The code will be made publicly available online.



### Toward Realistic Evaluation of Deep Active Learning Algorithms in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.10625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10625v1)
- **Published**: 2023-01-25 15:07:44+00:00
- **Updated**: 2023-01-25 15:07:44+00:00
- **Authors**: Carsten T. Lüth, Till J. Bungert, Lukas Klein, Paul F. Jaeger
- **Comment**: None
- **Journal**: None
- **Summary**: Active Learning (AL) aims to reduce the labeling burden by interactively querying the most informative observations from a data pool. Despite extensive research on improving AL query methods in the past years, recent studies have questioned the advantages of AL, especially in the light of emerging alternative training paradigms such as semi-supervised (Semi-SL) and self-supervised learning (Self-SL). Thus, today's AL literature paints an inconsistent picture and leaves practitioners wondering whether and how to employ AL in their tasks. We argue that this heterogeneous landscape is caused by a lack of a systematic and realistic evaluation of AL algorithms, including key parameters such as complex and imbalanced datasets, realistic labeling scenarios, systematic method configuration, and integration of Semi-SL and Self-SL. To this end, we present an AL benchmarking suite and run extensive experiments on five datasets shedding light on the questions: when and how to apply AL?



### Towards Arbitrary Text-driven Image Manipulation via Space Alignment
- **Arxiv ID**: http://arxiv.org/abs/2301.10670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10670v1)
- **Published**: 2023-01-25 16:20:01+00:00
- **Updated**: 2023-01-25 16:20:01+00:00
- **Authors**: Yunpeng Bai, Zihan Zhong, Chao Dong, Weichen Zhang, Guowei Xu, Chun Yuan
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: The recent GAN inversion methods have been able to successfully invert the real image input to the corresponding editable latent code in StyleGAN. By combining with the language-vision model (CLIP), some text-driven image manipulation methods are proposed. However, these methods require extra costs to perform optimization for a certain image or a new attribute editing mode. To achieve a more efficient editing method, we propose a new Text-driven image Manipulation framework via Space Alignment (TMSA). The Space Alignment module aims to align the same semantic regions in CLIP and StyleGAN spaces. Then, the text input can be directly accessed into the StyleGAN space and be used to find the semantic shift according to the text description. The framework can support arbitrary image editing mode without additional cost. Our work provides the user with an interface to control the attributes of a given image according to text input and get the result in real time. Ex tensive experiments demonstrate our superior performance over prior works.



### Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.10687v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10687v1)
- **Published**: 2023-01-25 16:45:13+00:00
- **Updated**: 2023-01-25 16:45:13+00:00
- **Authors**: Iván de Andrés Tamé, Kirill Sirotkin, Pablo Carballeira, Marcos Escudero-Viñolo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning technologies have already demonstrated a high potential to build diagnosis support systems from medical imaging data, such as Chest X-Ray images. However, the shortage of labeled data in the medical field represents one key obstacle to narrow down the performance gap with respect to applications in other image domains. In this work, we investigate the benefits of a curricular Self-Supervised Learning (SSL) pretraining scheme with respect to fully-supervised training regimes for pneumonia recognition on Chest X-Ray images of Covid-19 patients. We show that curricular SSL pretraining, which leverages unlabeled data, outperforms models trained from scratch, or pretrained on ImageNet, indicating the potential of performance gains by SSL pretraining on massive unlabeled datasets. Finally, we demonstrate that top-performing SSLpretrained models show a higher degree of attention in the lung regions, embodying models that may be more robust to possible external confounding factors in the training datasets, identified by previous works.



### An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation
- **Arxiv ID**: http://arxiv.org/abs/2301.10732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10732v1)
- **Published**: 2023-01-25 17:42:15+00:00
- **Updated**: 2023-01-25 17:42:15+00:00
- **Authors**: Aotian Wu, Pan He, Xiao Li, Ke Chen, Sanjay Ranka, Anand Rangarajan
- **Comment**: Submitted to IEEE Intelligent Transportation Systems Transactions
- **Journal**: None
- **Summary**: Most existing perception systems rely on sensory data acquired from cameras, which perform poorly in low light and adverse weather conditions. To resolve this limitation, we have witnessed advanced LiDAR sensors become popular in perception tasks in autonomous driving applications. Nevertheless, their usage in traffic monitoring systems is less ubiquitous. We identify two significant obstacles in cost-effectively and efficiently developing such a LiDAR-based traffic monitoring system: (i) public LiDAR datasets are insufficient for supporting perception tasks in infrastructure systems, and (ii) 3D annotations on LiDAR point clouds are time-consuming and expensive. To fill this gap, we present an efficient semi-automated annotation tool that automatically annotates LiDAR sequences with tracking algorithms while offering a fully annotated infrastructure LiDAR dataset -- FLORIDA (Florida LiDAR-based Object Recognition and Intelligent Data Annotation) -- which will be made publicly available. Our advanced annotation tool seamlessly integrates multi-object tracking (MOT), single-object tracking (SOT), and suitable trajectory post-processing techniques. Specifically, we introduce a human-in-the-loop schema in which annotators recursively fix and refine annotations imperfectly predicted by our tool and incrementally add them to the training dataset to obtain better SOT and MOT models. By repeating the process, we significantly increase the overall annotation speed by three to four times and obtain better qualitative annotations than a state-of-the-art annotation tool. The human annotation experiments verify the effectiveness of our annotation tool. In addition, we provide detailed statistics and object detection evaluation results for our dataset in serving as a benchmark for perception tasks at traffic intersections.



### Out of Distribution Performance of State of Art Vision Model
- **Arxiv ID**: http://arxiv.org/abs/2301.10750v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10750v2)
- **Published**: 2023-01-25 18:14:49+00:00
- **Updated**: 2023-07-30 00:55:57+00:00
- **Authors**: Md Salman Rahman, Wonkwon Lee
- **Comment**: incomplete work - need to complete it
- **Journal**: None
- **Summary**: The vision transformer (ViT) has advanced to the cutting edge in the visual recognition task. Transformers are more robust than CNN, according to the latest research. ViT's self-attention mechanism, according to the claim, makes it more robust than CNN. Even with this, we discover that these conclusions are based on unfair experimental conditions and just comparing a few models, which did not allow us to depict the entire scenario of robustness performance. In this study, we investigate the performance of 58 state-of-the-art computer vision models in a unified training setup based not only on attention and convolution mechanisms but also on neural networks based on a combination of convolution and attention mechanisms, sequence-based model, complementary search, and network-based method. Our research demonstrates that robustness depends on the training setup and model types, and performance varies based on out-of-distribution type. Our research will aid the community in better understanding and benchmarking the robustness of computer vision models.



### Efficient Flow-Guided Multi-frame De-fencing
- **Arxiv ID**: http://arxiv.org/abs/2301.10759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10759v1)
- **Published**: 2023-01-25 18:42:59+00:00
- **Updated**: 2023-01-25 18:42:59+00:00
- **Authors**: Stavros Tsogkas, Fengjia Zhang, Allan Jepson, Alex Levinshtein
- **Comment**: 16 pages, 12 figures. Published at the Winter Conference on
  Application of Computer Vision (WACV) 2023
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2023, pp. 1838-1847
- **Summary**: Taking photographs ''in-the-wild'' is often hindered by fence obstructions that stand between the camera user and the scene of interest, and which are hard or impossible to avoid. De-fencing is the algorithmic process of automatically removing such obstructions from images, revealing the invisible parts of the scene. While this problem can be formulated as a combination of fence segmentation and image inpainting, this often leads to implausible hallucinations of the occluded regions. Existing multi-frame approaches rely on propagating information to a selected keyframe from its temporal neighbors, but they are often inefficient and struggle with alignment of severely obstructed images. In this work we draw inspiration from the video completion literature and develop a simplified framework for multi-frame de-fencing that computes high quality flow maps directly from obstructed frames and uses them to accurately align frames. Our primary focus is efficiency and practicality in a real-world setting: the input to our algorithm is a short image burst (5 frames) - a data modality commonly available in modern smartphones - and the output is a single reconstructed keyframe, with the fence removed. Our approach leverages simple yet effective CNN modules, trained on carefully generated synthetic data, and outperforms more complicated alternatives real bursts, both quantitatively and qualitatively, while running real-time.



### On the Adversarial Robustness of Camera-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.10766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.10766v1)
- **Published**: 2023-01-25 18:59:15+00:00
- **Updated**: 2023-01-25 18:59:15+00:00
- **Authors**: Shaoyuan Xie, Zichao Li, Zeyu Wang, Cihang Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection methods under various adversarial conditions. Our experiments reveal five interesting findings: (a) the use of accurate depth estimation effectively improves robustness; (b) depth-estimation-free approaches do not show superior robustness; (c) bird's-eye-view-based representations exhibit greater robustness against localization attacks; (d) incorporating multi-frame benign inputs can effectively mitigate adversarial attacks; and (e) addressing long-tail problems can enhance robustness. We hope our work can provide guidance for the design of future camera-based object detection modules with improved adversarial robustness.



### TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.10829v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10829v1)
- **Published**: 2023-01-25 21:05:10+00:00
- **Updated**: 2023-01-25 21:05:10+00:00
- **Authors**: Zeynel A. Samak, Philip Clatworthy, Majid Mirmehdi
- **Comment**: Accepted at IEEE ISBI 2023, 5 pages
- **Journal**: None
- **Summary**: Acute ischaemic stroke, caused by an interruption in blood flow to brain tissue, is a leading cause of disability and mortality worldwide. The selection of patients for the most optimal ischaemic stroke treatment is a crucial step for a successful outcome, as the effect of treatment highly depends on the time to treatment. We propose a transformer-based multimodal network (TranSOP) for a classification approach that employs clinical metadata and imaging information, acquired on hospital admission, to predict the functional outcome of stroke treatment based on the modified Rankin Scale (mRS). This includes a fusion module to efficiently combine 3D non-contrast computed tomography (NCCT) features and clinical information. In comparative experiments using unimodal and multimodal data on the MRCLEAN dataset, we achieve a state-of-the-art AUC score of 0.85.



### Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach
- **Arxiv ID**: http://arxiv.org/abs/2301.10847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10847v1)
- **Published**: 2023-01-25 22:09:07+00:00
- **Updated**: 2023-01-25 22:09:07+00:00
- **Authors**: Reza Azad, Yiwei Jia, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, Dorit Merhof
- **Comment**: Submitted to IEEE TMI Journal
- **Journal**: None
- **Summary**: While CNN-based methods have been the cornerstone of medical image segmentation due to their promising performance and robustness, they suffer from limitations in capturing long-range dependencies. Transformer-based approaches are currently prevailing since they enlarge the reception field to model global contextual correlation. To further extract rich representations, some extensions of the U-Net employ multi-scale feature extraction and fusion modules and obtain improved performance. Inspired by this idea, we propose TransCeption for medical image segmentation, a pure transformer-based U-shape network featured by incorporating the inception-like module into the encoder and adopting a contextual bridge for better feature fusion. The design proposed in this work is based on three core principles: (1) The patch merging module in the encoder is redesigned with ResInception Patch Merging (RIPM). Multi-branch transformer (MB transformer) adopts the same number of branches as the outputs of RIPM. Combining the two modules enables the model to capture a multi-scale representation within a single stage. (2) We construct an Intra-stage Feature Fusion (IFF) module following the MB transformer to enhance the aggregation of feature maps from all the branches and particularly focus on the interaction between the different channels of all the scales. (3) In contrast to a bridge that only contains token-wise self-attention, we propose a Dual Transformer Bridge that also includes channel-wise self-attention to exploit correlations between scales at different stages from a dual perspective. Extensive experiments on multi-organ and skin lesion segmentation tasks present the superior performance of TransCeption compared to previous work. The code is publicly available at \url{https://github.com/mindflow-institue/TransCeption}.



### Shape Reconstruction from Thoracoscopic Images using Self-supervised Virtual Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.10863v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10863v1)
- **Published**: 2023-01-25 23:08:41+00:00
- **Updated**: 2023-01-25 23:08:41+00:00
- **Authors**: Tomoki Oya, Megumi Nakao, Tetsuya Matsuda
- **Comment**: None
- **Journal**: None
- **Summary**: Intraoperative shape reconstruction of organs from endoscopic camera images is a complex yet indispensable technique for image-guided surgery. To address the uncertainty in reconstructing entire shapes from single-viewpoint occluded images, we propose a framework for generative virtual learning of shape reconstruction using image translation with common latent variables between simulated and real images. As it is difficult to prepare sufficient amount of data to learn the relationship between endoscopic images and organ shapes, self-supervised virtual learning is performed using simulated images generated from statistical shape models. However, small differences between virtual and real images can degrade the estimation performance even if the simulated images are regarded as equivalent by humans. To address this issue, a Variational Autoencoder is used to convert real and simulated images into identical synthetic images. In this study, we targeted the shape reconstruction of collapsed lungs from thoracoscopic images and confirmed that virtual learning could improve the similarity between real and simulated images. Furthermore, shape reconstruction error could be improved by 16.9%.



