# Arxiv Papers in cs.CV on 2023-01-09
### Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture
- **Arxiv ID**: http://arxiv.org/abs/2301.03127v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.03127v2)
- **Published**: 2023-01-09 00:19:11+00:00
- **Updated**: 2023-02-03 22:55:12+00:00
- **Authors**: Pim Jordi Verschuuren, Jie Gao, Adelize van Eeden, Stylianos Oikonomou, Anil Bandhakavi
- **Comment**: Accepted in AAAI'23: Second Workshop on Multimodal Fact-Checking and
  Hate Speech Detection, February 2023, Washington, DC, USA
- **Journal**: None
- **Summary**: In this paper, we present the Logically submissions to De-Factify 2 challenge (DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our submissions to this challenge including explored evidence retrieval and selection techniques, pre-trained cross-modal and unimodal models, and a cross-modal veracity model based on the well established Transformer Encoder (TE) architecture which is heavily relies on the concept of self-attention. Exploratory analysis is also conducted on this Factify 2 data set that uncovers the salient multi-modal patterns and hypothesis motivating the architecture proposed in this work. A series of preliminary experiments were done to investigate and benchmarking different pre-trained embedding models, evidence retrieval settings and thresholds. The final system, a standard two-stage evidence based veracity detection system, yields weighted avg. 0.79 on both val set and final blind test set on the task 1, which achieves 3rd place with a small margin to the top performing system on the leaderboard among 9 participants.



### SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions
- **Arxiv ID**: http://arxiv.org/abs/2301.03130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03130v1)
- **Published**: 2023-01-09 00:56:51+00:00
- **Updated**: 2023-01-09 00:56:51+00:00
- **Authors**: MohammadReza Naderi, MohammadHossein Givkashi, Nader Karimi, Shahram Shirani, Shadrokh Samavi
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Image inpainting consists of filling holes or missing parts of an image. Inpainting face images with symmetric characteristics is more challenging than inpainting a natural scene. None of the powerful existing models can fill out the missing parts of an image while considering the symmetry and homogeneity of the picture. Moreover, the metrics that assess a repaired face image quality cannot measure the preservation of symmetry between the rebuilt and existing parts of a face. In this paper, we intend to solve the symmetry problem in the face inpainting task by using multiple discriminators that check each face organ's reality separately and a transformer-based network. We also propose "symmetry concentration score" as a new metric for measuring the symmetry of a repaired face image. The quantitative and qualitative results show the superiority of our proposed method compared to some of the recently proposed algorithms in terms of the reality, symmetry, and homogeneity of the inpainted parts.



### Instance Segmentation Based Graph Extraction for Handwritten Circuit Diagram Images
- **Arxiv ID**: http://arxiv.org/abs/2301.03155v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03155v2)
- **Published**: 2023-01-09 03:00:20+00:00
- **Updated**: 2023-01-18 15:25:22+00:00
- **Authors**: Johannes Bayer, Amit Kumar Roy, Andreas Dengel
- **Comment**: As submitted to ICPRAM23
- **Journal**: None
- **Summary**: Handwritten circuit diagrams from educational scenarios or historic sources usually exist on analogue media. For deriving their functional principles or flaws automatically, they need to be digitized, extracting their electrical graph. Recently, the base technologies for automated pipelines facilitating this process shifted from computer vision to machine learning. This paper describes an approach for extracting both the electrical components (including their terminals and describing texts) as well their interconnections (including junctions and wire hops) by the means of instance segmentation and keypoint extraction. Consequently, the resulting graph extraction process consists of a simple two-step process of model inference and trivial geometric keypoint matching. The dataset itself, its preparation, model training and post-processing are described and publicly available.



### Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network
- **Arxiv ID**: http://arxiv.org/abs/2301.03160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03160v1)
- **Published**: 2023-01-09 03:57:14+00:00
- **Updated**: 2023-01-09 03:57:14+00:00
- **Authors**: Haowei Wang, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Xiaoshuai Sun
- **Comment**: 9 pages, 5 figures, accepted by AAAI23
- **Journal**: None
- **Summary**: Panoptic Narrative Grounding (PNG) is an emerging cross-modal grounding task, which locates the target regions of an image corresponding to the text description. Existing approaches for PNG are mainly based on a two-stage paradigm, which is computationally expensive. In this paper, we propose a one-stage network for real-time PNG, termed End-to-End Panoptic Narrative Grounding network (EPNG), which directly generates masks for referents. Specifically, we propose two innovative designs, i.e., Locality-Perceptive Attention (LPA) and a bidirectional Semantic Alignment Loss (SAL), to properly handle the many-to-many relationship between textual expressions and visual objects. LPA embeds the local spatial priors into attention modeling, i.e., a pixel may belong to multiple masks at different scales, thereby improving segmentation. To help understand the complex semantic relationships, SAL proposes a bidirectional contrastive objective to regularize the semantic consistency inter modalities. Extensive experiments on the PNG benchmark dataset demonstrate the effectiveness and efficiency of our method. Compared to the single-stage baseline, our method achieves a significant improvement of up to 9.4% accuracy. More importantly, our EPNG is 10 times faster than the two-stage model. Meanwhile, the generalization ability of EPNG is also validated by zero-shot experiments on other grounding tasks.



### eFIN: Enhanced Fourier Imager Network for generalizable autofocusing and pixel super-resolution in holographic imaging
- **Arxiv ID**: http://arxiv.org/abs/2301.03162v1
- **DOI**: 10.1109/JSTQE.2023.3248684
- **Categories**: **physics.optics**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03162v1)
- **Published**: 2023-01-09 04:12:10+00:00
- **Updated**: 2023-01-09 04:12:10+00:00
- **Authors**: Hanlong Chen, Luzhe Huang, Tairan Liu, Aydogan Ozcan
- **Comment**: 10 Pages, 4 Figures
- **Journal**: IEEE Journal of Selected Topics in Quantum Electronics (2023)
- **Summary**: The application of deep learning techniques has greatly enhanced holographic imaging capabilities, leading to improved phase recovery and image reconstruction. Here, we introduce a deep neural network termed enhanced Fourier Imager Network (eFIN) as a highly generalizable framework for hologram reconstruction with pixel super-resolution and image autofocusing. Through holographic microscopy experiments involving lung, prostate and salivary gland tissue sections and Papanicolau (Pap) smears, we demonstrate that eFIN has a superior image reconstruction quality and exhibits external generalization to new types of samples never seen during the training phase. This network achieves a wide autofocusing axial range of 0.35 mm, with the capability to accurately predict the hologram axial distances by physics-informed learning. eFIN enables 3x pixel super-resolution imaging and increases the space-bandwidth product of the reconstructed images by 9-fold with almost no performance loss, which allows for significant time savings in holographic imaging and data processing steps. Our results showcase the advancements of eFIN in pushing the boundaries of holographic imaging for various applications in e.g., quantitative phase imaging and label-free microscopy.



### Cursive Caption Text Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2301.03164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03164v1)
- **Published**: 2023-01-09 04:30:48+00:00
- **Updated**: 2023-01-09 04:30:48+00:00
- **Authors**: Ali Mirza, Imran Siddiqi
- **Comment**: 19 pages, 16 figures
- **Journal**: None
- **Summary**: Textual content appearing in videos represents an interesting index for semantic retrieval of videos (from archives), generation of alerts (live streams) as well as high level applications like opinion mining and content summarization. One of the key components of such systems is the detection of textual content in video frames and the same makes the subject of our present study. This paper presents a robust technique for detection of textual content appearing in video frames. More specifically we target text in cursive script taking Urdu text as a case study. Detection of textual regions in video frames is carried out by fine-tuning object detectors based on deep convolutional neural networks for the specific case of text detection. Since it is common to have videos with caption text in multiple-scripts, cursive text is distinguished from Latin text using a script-identification module. Finally, detection and script identification are combined in a single end-to-end trainable system. Experiments on a comprehensive dataset of around 11,000 video frames report an F-measure of 0.91.



### Machining feature recognition using descriptors with range constraints for mechanical 3D models
- **Arxiv ID**: http://arxiv.org/abs/2301.03167v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03167v1)
- **Published**: 2023-01-09 04:50:06+00:00
- **Updated**: 2023-01-09 04:50:06+00:00
- **Authors**: Seungeun Lim, Changmo Yeo, Fazhi He, Jinwon Lee, Duhwan Mun
- **Comment**: None
- **Journal**: None
- **Summary**: In machining feature recognition, geometric elements generated in a three-dimensional computer-aided design model are identified. This technique is used in manufacturability evaluation, process planning, and tool path generation. Here, we propose a method of recognizing 16 types of machining features using descriptors, often used in shape-based part retrieval studies. The base face is selected for each feature type, and descriptors express the base face's minimum, maximum, and equal conditions. Furthermore, the similarity in the three conditions between the descriptors extracted from the target face and those from the base face is calculated. If the similarity is greater than or equal to the threshold, the target face is determined as the base face of the feature. Machining feature recognition tests were conducted for two test cases using the proposed method, and all machining features included in the test cases were successfully recognized. Also, it was confirmed through an additional test that the proposed method in this study showed better feature recognition performance than the latest artificial neural network.



### A Study on the Generality of Neural Network Structures for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.03169v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.03169v2)
- **Published**: 2023-01-09 04:58:12+00:00
- **Updated**: 2023-01-10 02:30:25+00:00
- **Authors**: Jinwoo Bae, Kyumin Hwang, Sunghoon Im
- **Comment**: under review. This paper extends our previous work in
  arXiv:2205.11083
- **Journal**: None
- **Summary**: Monocular depth estimation has been widely studied, and significant improvements in performance have been recently reported. However, most previous works are evaluated on a few benchmark datasets, such as KITTI datasets, and none of the works provide an in-depth analysis of the generalization performance of monocular depth estimation. In this paper, we deeply investigate the various backbone networks (e.g.CNN and Transformer models) toward the generalization of monocular depth estimation. First, we evaluate state-of-the-art models on both in-distribution and out-of-distribution datasets, which have never been seen during network training. Then, we investigate the internal properties of the representations from the intermediate layers of CNN-/Transformer-based models using synthetic texture-shifted datasets. Through extensive experiments, we observe that the Transformers exhibit a strong shape-bias rather than CNNs, which have a strong texture-bias. We also discover that texture-biased models exhibit worse generalization performance for monocular depth estimation than shape-biased models. We demonstrate that similar aspects are observed in real-world driving datasets captured under diverse environments. Lastly, we conduct a dense ablation study with various backbone networks which are utilized in modern strategies. The experiments demonstrate that the intrinsic locality of the CNNs and the self-attention of the Transformers induce texture-bias and shape-bias, respectively.



### Deep Planar Parallax for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.03178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03178v1)
- **Published**: 2023-01-09 06:02:36+00:00
- **Updated**: 2023-01-09 06:02:36+00:00
- **Authors**: Haoqian Liang, Zhichao Li, Ya Yang, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is a fundamental problem in the perception system of autonomous driving scenes. Although autonomous driving is challenging, much prior knowledge can still be utilized, by which the sophistication of the problem can be effectively restricted. Some previous works introduce the road plane prior to the depth estimation problem according to the Planar Parallax Geometry. However, we find that their usages are not effective, leaving the network cannot learn the geometric information. To this end, we analyze this problem in detail and reveal that explicit warping of consecutive frames and flow pre-training can effectively bring the geometric prior into learning. Furthermore, we propose Planar Position Embedding to deal with the intrinsic weakness of plane parallax geometry. Comprehensive experimental results on autonomous driving datasets like KITTI and Waymo Open Dataset (WOD) demonstrate that our Planar Parallax Network(PPNet) dramatically outperforms existing learning-based methods.



### Structure-Informed Shadow Removal Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.03182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03182v1)
- **Published**: 2023-01-09 06:31:52+00:00
- **Updated**: 2023-01-09 06:31:52+00:00
- **Authors**: Yuhao Liu, Qing Guo, Lan Fu, Zhanghan Ke, Ke Xu, Wei Feng, Ivor W. Tsang, Rynson W. H. Lau
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Shadow removal is a fundamental task in computer vision. Despite the success, existing deep learning-based shadow removal methods still produce images with shadow remnants. These shadow remnants typically exist in homogeneous regions with low intensity values, making them untraceable in the existing image-to-image mapping paradigm. We observe from our experiments that shadows mainly degrade object colors at the image structure level (in which humans perceive object outlines filled with continuous colors). Hence, in this paper, we propose to remove shadows at the image structure level. Based on this idea, we propose a novel structure-informed shadow removal network (StructNet) to leverage the image structure information to address the shadow remnant problem. Specifically, StructNet first reconstructs the structure information of the input image without shadows and then uses the restored shadow-free structure prior to guiding the image-level shadow removal. StructNet contains two main novel modules: (1) a mask-guided shadow-free extraction (MSFE) module to extract image structural features in a non-shadow to shadow directional manner, and (2) a multi-scale feature & residual aggregation (MFRA) module to leverage the shadow-free structure information to regularize feature consistency. In addition, we also propose to extend StructNet to exploit multi-level structure information (MStructNet), to further boost the shadow removal performance with minimum computational overheads. Extensive experiments on three shadow removal benchmarks demonstrate that our method outperforms existing shadow removal methods, and our StructNet can be integrated with existing methods to boost their performances further.



### Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2301.03194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03194v2)
- **Published**: 2023-01-09 08:00:01+00:00
- **Updated**: 2023-03-15 09:06:48+00:00
- **Authors**: Jie Liu, Yanqi Bao, Wenzhe Yin, Haochen Wang, Yang Gao, Jan-Jakob Sonke, Efstratios Gavves
- **Comment**: Accepted in BMVC2022 as oral presentation
- **Journal**: None
- **Summary**: Few-shot semantic segmentation (FSS) aims to achieve novel objects segmentation with only a few annotated samples and has made great progress recently. Most of the existing FSS models focus on the feature matching between support and query to tackle FSS. However, the appearance variations between objects from the same category could be extremely large, leading to unreliable feature matching and query mask prediction. To this end, we propose a Support-induced Graph Convolutional Network (SiGCN) to explicitly excavate latent context structure in query images. Specifically, we propose a Support-induced Graph Reasoning (SiGR) module to capture salient query object parts at different semantic levels with a Support-induced GCN. Furthermore, an instance association (IA) module is designed to capture high-order instance context from both support and query instances. By integrating the proposed two modules, SiGCN can learn rich query context representation, and thus being more robust to appearance variations. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our SiGCN achieves state-of-the-art performance.



### The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2301.03198v4
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2301.03198v4)
- **Published**: 2023-01-09 08:27:36+00:00
- **Updated**: 2023-07-11 20:27:04+00:00
- **Authors**: A. T. Gifford, B. Lahner, S. Saba-Sadiya, M. G. Vilas, A. Lascelles, A. Oliva, K. Kay, G. Roig, R. M. Cichy
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to ~73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems.



### Integrating features from lymph node stations for metastatic lymph node detection
- **Arxiv ID**: http://arxiv.org/abs/2301.03202v1
- **DOI**: 10.1016/j.compmedimag.2022.102108
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03202v1)
- **Published**: 2023-01-09 08:35:58+00:00
- **Updated**: 2023-01-09 08:35:58+00:00
- **Authors**: Chaoyi Wu, Feng Chang, Xiao Su, Zhihan Wu, Yanfeng Wang, Ling Zhu, Ya Zhang
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics, Volume 101, 2022,
  102108, ISSN 0895-6111
- **Summary**: Metastasis on lymph nodes (LNs), the most common way of spread for primary tumor cells, is a sign of increased mortality. However, metastatic LNs are time-consuming and challenging to detect even for professional radiologists due to their small sizes, high sparsity, and ambiguity in appearance. It is desired to leverage recent development in deep learning to automatically detect metastatic LNs. Besides a two-stage detection network, we here introduce an additional branch to leverage information about LN stations, an important reference for radiologists during metastatic LN diagnosis, as supplementary information for metastatic LN detection. The branch targets to solve a closely related task on the LN station level, i.e., classifying whether an LN station contains metastatic LN or not, so as to learn representations for LN stations. Considering that a metastatic LN station is expected to significantly affect the nearby ones, a GCN-based structure is adopted by the branch to model the relationship among different LN stations. At the classification stage of metastatic LN detection, the above learned LN station features, as well as the features reflecting the distance between the LN candidate and the LN stations, are integrated with the LN features. We validate our method on a dataset containing 114 intravenous contrast-enhanced Computed Tomography (CT) images of oral squamous cell carcinoma (OSCC) patients and show that it outperforms several state-of-the-art methods on the mFROC, maxF1, and AUC scores,respectively.



### EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.03213v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03213v4)
- **Published**: 2023-01-09 09:10:35+00:00
- **Updated**: 2023-03-14 18:48:15+00:00
- **Authors**: Hao Tang, Kevin Liang, Matt Feiszli, Weiyao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their "framed" nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.



### Explainable, Physics Aware, Trustworthy AI Paradigm Shift for Synthetic Aperture Radar
- **Arxiv ID**: http://arxiv.org/abs/2301.03589v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03589v1)
- **Published**: 2023-01-09 09:22:13+00:00
- **Updated**: 2023-01-09 09:22:13+00:00
- **Authors**: Mihai Datcu, Zhongling Huang, Andrei Anghel, Juanping Zhao, Remus Cacoveanu
- **Comment**: None
- **Journal**: None
- **Summary**: The recognition or understanding of the scenes observed with a SAR system requires a broader range of cues, beyond the spatial context. These encompass but are not limited to: imaging geometry, imaging mode, properties of the Fourier spectrum of the images or the behavior of the polarimetric signatures. In this paper, we propose a change of paradigm for explainability in data science for the case of Synthetic Aperture Radar (SAR) data to ground the explainable AI for SAR. It aims to use explainable data transformations based on well-established models to generate inputs for AI methods, to provide knowledgeable feedback for training process, and to learn or improve high-complexity unknown or un-formalized models from the data. At first, we introduce a representation of the SAR system with physical layers: i) instrument and platform, ii) imaging formation, iii) scattering signatures and objects, that can be integrated with an AI model for hybrid modeling. Successively, some illustrative examples are presented to demonstrate how to achieve hybrid modeling for SAR image understanding. The perspective of trustworthy model and supplementary explanations are discussed later. Finally, we draw the conclusion and we deem the proposed concept has applicability to the entire class of coherent imaging sensors and other computational imaging systems.



### The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge
- **Arxiv ID**: http://arxiv.org/abs/2301.03281v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03281v2)
- **Published**: 2023-01-09 11:48:05+00:00
- **Updated**: 2023-01-12 15:16:59+00:00
- **Authors**: Xiangyu Li, Gongning Luo, Kuanquan Wang, Hongyu Wang, Jun Liu, Xinjie Liang, Jie Jiang, Zhenghao Song, Chunyue Zheng, Haokai Chi, Mingwang Xu, Yingte He, Xinghua Ma, Jingwen Guo, Yifan Liu, Chuanpu Li, Zeli Chen, Md Mahfuzur Rahman Siddiquee, Andriy Myronenko, Antoine P. Sanner, Anirban Mukhopadhyay, Ahmed E. Othman, Xingyu Zhao, Weiping Liu, Jinhuang Zhang, Xiangyuan Ma, Qinghui Liu, Bradley J. MacIntosh, Wei Liang, Moona Mazher, Abdul Qayyum, Valeriia Abramova, Xavier Lladó, Shuo Li
- **Comment**: Summarized paper for the MICCAI INSTANCE 2022 Challenge
- **Journal**: None
- **Summary**: Automatic intracranial hemorrhage segmentation in 3D non-contrast head CT (NCCT) scans is significant in clinical practice. Existing hemorrhage segmentation methods usually ignores the anisotropic nature of the NCCT, and are evaluated on different in-house datasets with distinct metrics, making it highly challenging to improve segmentation performance and perform objective comparisons among different methods. The INSTANCE 2022 was a grand challenge held in conjunction with the 2022 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI). It is intended to resolve the above-mentioned problems and promote the development of both intracranial hemorrhage segmentation and anisotropic data processing. The INSTANCE released a training set of 100 cases with ground-truth and a validation set with 30 cases without ground-truth labels that were available to the participants. A held-out testing set with 70 cases is utilized for the final evaluation and ranking. The methods from different participants are ranked based on four metrics, including Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), Relative Volume Difference (RVD) and Normalized Surface Dice (NSD). A total of 13 teams submitted distinct solutions to resolve the challenges, making several baseline models, pre-processing strategies and anisotropic data processing techniques available to future researchers. The winner method achieved an average DSC of 0.6925, demonstrating a significant growth over our proposed baseline method. To the best of our knowledge, the proposed INSTANCE challenge releases the first intracranial hemorrhage segmentation benchmark, and is also the first challenge that intended to resolve the anisotropic problem in 3D medical image segmentation, which provides new alternatives in these research fields.



### Simplifying Open-Set Video Domain Adaptation with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.03322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03322v1)
- **Published**: 2023-01-09 13:16:50+00:00
- **Updated**: 2023-01-09 13:16:50+00:00
- **Authors**: Giacomo Zara, Victor Guilherme Turrisi da Costa, Subhankar Roy, Paolo Rota, Elisa Ricci
- **Comment**: Currently under review at Computer Vision and Image Understanding
  (CVIU) journal
- **Journal**: None
- **Summary**: In an effort to reduce annotation costs in action recognition, unsupervised video domain adaptation methods have been proposed that aim to adapt a predictive model from a labelled dataset (i.e., source domain) to an unlabelled dataset (i.e., target domain). In this work we address a more realistic scenario, called open-set video domain adaptation (OUVDA), where the target dataset contains "unknown" semantic categories that are not shared with the source. The challenge lies in aligning the shared classes of the two domains while separating the shared classes from the unknown ones. In this work we propose to address OUVDA with an unified contrastive learning framework that learns discriminative and well-clustered features. We also propose a video-oriented temporal contrastive loss that enables our method to better cluster the feature space by exploiting the freely available temporal information in video data. We show that discriminative feature space facilitates better separation of the unknown classes, and thereby allows us to use a simple similarity based score to identify them. We conduct thorough experimental evaluation on multiple OUVDA benchmarks and show the effectiveness of our proposed method against the prior art.



### HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.03330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03330v1)
- **Published**: 2023-01-09 13:32:50+00:00
- **Updated**: 2023-01-09 13:32:50+00:00
- **Authors**: Xiang Wang, Shiwei Zhang, Zhiwu Qing, Zhengrong Zuo, Changxin Gao, Rong Jin, Nong Sang
- **Comment**: An extended version of a paper arXiv:2204.13423 published in CVPR
  2022. This work has been submitted to the Springer for possible publication
- **Journal**: None
- **Summary**: Recent attempts mainly focus on learning deep representations for each video individually under the episodic meta-learning regime and then performing temporal alignment to match query and support videos. However, they still suffer from two drawbacks: (i) learning individual features without considering the entire task may result in limited representation capability, and (ii) existing alignment strategies are sensitive to noises and misaligned instances. To handle the two limitations, we propose a novel Hybrid Relation guided temporal Set Matching (HyRSM++) approach for few-shot action recognition. The core idea of HyRSM++ is to integrate all videos within the task to learn discriminative representations and involve a robust matching technique. To be specific, HyRSM++ consists of two key components, a hybrid relation module and a temporal set matching metric. Given the basic representations from the feature extractor, the hybrid relation module is introduced to fully exploit associated relations within and cross videos in an episodic task and thus can learn task-specific embeddings. Subsequently, in the temporal set matching metric, we carry out the distance measure between query and support videos from a set matching perspective and design a Bi-MHM to improve the resilience to misaligned instances. In addition, we explicitly exploit the temporal coherence in videos to regularize the matching process. Furthermore, we extend the proposed HyRSM++ to deal with the more challenging semi-supervised few-shot action recognition and unsupervised few-shot action recognition tasks. Experimental results on multiple benchmarks demonstrate that our method achieves state-of-the-art performance under various few-shot settings. The source code is available at https://github.com/alibaba-mmai-research/HyRSMPlusPlus.



### A Specific Task-oriented Semantic Image Communication System for substation patrol inspection
- **Arxiv ID**: http://arxiv.org/abs/2301.03331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03331v1)
- **Published**: 2023-01-09 13:35:03+00:00
- **Updated**: 2023-01-09 13:35:03+00:00
- **Authors**: Senran Fan, Haotai Liang, Chen Dong, Xiaodong Xu, Geng Liu
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Intelligent inspection robots are widely used in substation patrol inspection, which can help check potential safety hazards by patrolling the substation and sending back scene images. However, when patrolling some marginal areas with weak signal, the scene images cannot be sucessfully transmissted to be used for hidden danger elimination, which greatly reduces the quality of robots'daily work. To solve such problem, a Specific Task-oriented Semantic Communication System for Imag-STSCI is designed, which involves the semantic features extraction, transmission, restoration and enhancement to get clearer images sent by intelligent robots under weak signals. Inspired by that only some specific details of the image are needed in such substation patrol inspection task, we proposed a new paradigm of semantic enhancement in such specific task to ensure the clarity of key semantic information when facing a lower bit rate or a low signal-to-noise ratio situation. Across the reality-based simulation, experiments show our STSCI can generally surpass traditional image-compression-based and channel-codingbased or other semantic communication system in the substation patrol inspection task with a lower bit rate even under a low signal-to-noise ratio situation.



### Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.03335v1
- **DOI**: 10.1109/TGRS.2023.3236154
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03335v1)
- **Published**: 2023-01-09 13:43:54+00:00
- **Updated**: 2023-01-09 13:43:54+00:00
- **Authors**: Meng Wang, Feng Gao, Junyu Dong, Heng-Chao Li, Qian Du
- **Comment**: IEEE TGRS 2023
- **Journal**: None
- **Summary**: The joint hyperspectral image (HSI) and LiDAR data classification aims to interpret ground objects at more detailed and precise level. Although deep learning methods have shown remarkable success in the multisource data classification task, self-supervised learning has rarely been explored. It is commonly nontrivial to build a robust self-supervised learning model for multisource data classification, due to the fact that the semantic similarities of neighborhood regions are not exploited in existing contrastive learning framework. Furthermore, the heterogeneous gap induced by the inconsistent distribution of multisource data impedes the classification performance. To overcome these disadvantages, we propose a Nearest Neighbor-based Contrastive Learning Network (NNCNet), which takes full advantage of large amounts of unlabeled data to learn discriminative feature representations. Specifically, we propose a nearest neighbor-based data augmentation scheme to use enhanced semantic relationships among nearby regions. The intermodal semantic alignments can be captured more accurately. In addition, we design a bilinear attention module to exploit the second-order and even high-order feature interactions between the HSI and LiDAR data. Extensive experiments on four public datasets demonstrate the superiority of our NNCNet over state-of-the-art methods. The source codes are available at \url{https://github.com/summitgao/NNCNet}.



### Universal Multimodal Representation for Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2301.03344v1
- **DOI**: 10.1109/TPAMI.2023.3234170
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03344v1)
- **Published**: 2023-01-09 13:54:11+00:00
- **Updated**: 2023-01-09 13:54:11+00:00
- **Authors**: Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Representation learning is the foundation of natural language processing (NLP). This work presents new methods to employ visual information as assistant signals to general NLP tasks. For each sentence, we first retrieve a flexible number of images either from a light topic-image lookup table extracted over the existing sentence-image pairs or a shared cross-modal embedding space that is pre-trained on out-of-shelf text-image pairs. Then, the text and images are encoded by a Transformer encoder and convolutional neural network, respectively. The two sequences of representations are further fused by an attention layer for the interaction of the two modalities. In this study, the retrieval process is controllable and flexible. The universal visual representation overcomes the lack of large-scale bilingual sentence-image pairs. Our method can be easily applied to text-only tasks without manually annotated multimodal parallel corpora. We apply the proposed method to a wide range of natural language generation and understanding tasks, including neural machine translation, natural language inference, and semantic similarity. Experimental results show that our method is generally effective for different tasks and languages. Analysis indicates that the visual signals enrich textual representations of content words, provide fine-grained grounding information about the relationship between concepts and events, and potentially conduce to disambiguation.



### Image Denoising: The Deep Learning Revolution and Beyond -- A Survey Paper --
- **Arxiv ID**: http://arxiv.org/abs/2301.03362v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03362v1)
- **Published**: 2023-01-09 14:16:40+00:00
- **Updated**: 2023-01-09 14:16:40+00:00
- **Authors**: Michael Elad, Bahjat Kawar, Gregory Vaksman
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising (removal of additive white Gaussian noise from an image) is one of the oldest and most studied problems in image processing. An extensive work over several decades has led to thousands of papers on this subject, and to many well-performing algorithms for this task. Indeed, 10 years ago, these achievements have led some researchers to suspect that "Denoising is Dead", in the sense that all that can be achieved in this domain has already been obtained. However, this turned out to be far from the truth, with the penetration of deep learning (DL) into image processing. The era of DL brought a revolution to image denoising, both by taking the lead in today's ability for noise removal in images, and by broadening the scope of denoising problems being treated. Our paper starts by describing this evolution, highlighting in particular the tension and synergy that exist between classical approaches and modern DL-based alternatives in design of image denoisers.   The recent transitions in the field of image denoising go far beyond the ability to design better denoisers. In the 2nd part of this paper we focus on recently discovered abilities and prospects of image denoisers. We expose the possibility of using denoisers to serve other problems, such as regularizing general inverse problems and serving as the prime engine in diffusion-based image synthesis. We also unveil the idea that denoising and other inverse problems might not have a unique solution as common algorithms would have us believe. Instead, we describe constructive ways to produce randomized and diverse high quality results for inverse problems, all fueled by the progress that DL brought to image denoising.   This survey paper aims to provide a broad view of the history of image denoising and closely related topics. Our aim is to give a better context to recent discoveries, and to the influence of DL in our domain.



### On Advantages of Mask-level Recognition for Outlier-aware Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.03407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03407v2)
- **Published**: 2023-01-09 14:59:44+00:00
- **Updated**: 2023-04-05 09:01:50+00:00
- **Authors**: Matej Grcić, Josip Šarić, Siniša Šegvić
- **Comment**: Accepted to CVPR 2023 workshop on Visual Anomaly and Novelty
  Detection (VAND)
- **Journal**: None
- **Summary**: Most dense recognition approaches bring a separate decision in each particular pixel. These approaches deliver competitive performance in usual closed-set setups. However, important applications in the wild typically require strong performance in presence of outliers. We show that this demanding setup greatly benefit from mask-level predictions, even in the case of non-finetuned baseline models. Moreover, we propose an alternative formulation of dense recognition uncertainty that effectively reduces false positive responses at semantic borders. The proposed formulation produces a further improvement over a very strong baseline and sets the new state of the art in outlier-aware semantic segmentation with and without training on negative data. Our contributions also lead to performance improvement in a recent panoptic setup. In-depth experiments confirm that our approach succeeds due to implicit aggregation of pixel-level cues into mask-level predictions.



### Nuclear Segmentation and Classification: On Color & Compression Generalization
- **Arxiv ID**: http://arxiv.org/abs/2301.03418v1
- **DOI**: 10.1007/978-3-031-21014-3_26
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03418v1)
- **Published**: 2023-01-09 15:14:48+00:00
- **Updated**: 2023-01-09 15:14:48+00:00
- **Authors**: Quoc Dang Vu, Robert Jewsbury, Simon Graham, Mostafa Jahanifar, Shan E Ahmed Raza, Fayyaz Minhas, Abhir Bhalerao, Nasir Rajpoot
- **Comment**: Oral presentation at MICCAI MLMI 2022, 7 pages, 6 figures
- **Journal**: None
- **Summary**: Since the introduction of digital and computational pathology as a field, one of the major problems in the clinical application of algorithms has been the struggle to generalize well to examples outside the distribution of the training data. Existing work to address this in both pathology and natural images has focused almost exclusively on classification tasks. We explore and evaluate the robustness of the 7 best performing nuclear segmentation and classification models from the largest computational pathology challenge for this problem to date, the CoNIC challenge. We demonstrate that existing state-of-the-art (SoTA) models are robust towards compression artifacts but suffer substantial performance reduction when subjected to shifts in the color domain. We find that using stain normalization to address the domain shift problem can be detrimental to the model performance. On the other hand, neural style transfer is more consistent in improving test performance when presented with large color variations in the wild.



### LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects
- **Arxiv ID**: http://arxiv.org/abs/2301.03426v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.03426v3)
- **Published**: 2023-01-09 15:24:19+00:00
- **Updated**: 2023-06-12 07:18:39+00:00
- **Authors**: Ibrahim Hroob, Sergi Molina, Riccardo Polvara, Grzegorz Cielniak, Marc Hanheide
- **Comment**: None
- **Journal**: None
- **Summary**: In this research, we present an end-to-end data-driven pipeline for determining the long-term stability status of objects within a given environment, specifically distinguishing between static and dynamic objects. Understanding object stability is key for mobile robots since long-term stable objects can be exploited as landmarks for long-term localisation. Our pipeline includes a labelling method that utilizes historical data from the environment to generate training data for a neural network. Rather than utilizing discrete labels, we propose the use of point-wise continuous label values, indicating the spatio-temporal stability of individual points, to train a point cloud regression network named LTS-NET. Our approach is evaluated on point cloud data from two parking lots in the NCLT dataset, and the results show that our proposed solution, outperforms direct training of a classification model for static vs dynamic object classification.



### High-Resolution Cloud Removal with Multi-Modal and Multi-Resolution Data Fusion: A New Baseline and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2301.03432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03432v1)
- **Published**: 2023-01-09 15:31:28+00:00
- **Updated**: 2023-01-09 15:31:28+00:00
- **Authors**: Fang Xu, Yilei Shi, Patrick Ebel, Wen Yang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce Planet-CR, a benchmark dataset for high-resolution cloud removal with multi-modal and multi-resolution data fusion. Planet-CR is the first public dataset for cloud removal to feature globally sampled high resolution optical observations, in combination with paired radar measurements as well as pixel-level land cover annotations. It provides solid basis for exhaustive evaluation in terms of generating visually pleasing textures and semantically meaningful structures. With this dataset, we consider the problem of cloud removal in high resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. Existing multi-modal data fusion based methods, which assume the image pairs are aligned pixel-to-pixel, are hence not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution SAR image guided high-resolution optical image cloud removal. It implicitly aligns the multi-modal and multi-resolution data during the reconstruction process to promote the cloud removal performance. The experimental results demonstrate that the proposed Align-CR method gives the best performance in both visual recovery quality and semantic recovery quality. The project is available at https://github.com/zhu-xlab/Planet-CR, and hope this will inspire future research.



### Generalized adaptive smoothing based neural network architecture for traffic state estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.03439v1
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2301.03439v1)
- **Published**: 2023-01-09 15:40:45+00:00
- **Updated**: 2023-01-09 15:40:45+00:00
- **Authors**: Chuhan Yang, Sai Venkata Ramana Ambadipudi, Saif Eddin Jabari
- **Comment**: None
- **Journal**: None
- **Summary**: The adaptive smoothing method (ASM) is a standard data-driven technique used in traffic state estimation. The ASM has free parameters which, in practice, are chosen to be some generally acceptable values based on intuition. However, we note that the heuristically chosen values often result in un-physical predictions by the ASM. In this work, we propose a neural network based on the ASM which tunes those parameters automatically by learning from sparse data from road sensors. We refer to it as the adaptive smoothing neural network (ASNN). We also propose a modified ASNN (MASNN), which makes it a strong learner by using ensemble averaging. The ASNN and MASNN are trained and tested two real-world datasets. Our experiments reveal that the ASNN and the MASNN outperform the conventional ASM.



### DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2301.03461v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.03461v3)
- **Published**: 2023-01-09 16:00:15+00:00
- **Updated**: 2023-03-04 14:22:29+00:00
- **Authors**: Yangyang Xu, Yibo Yang, Lefei Zhang
- **Comment**: Accepted by AAAI2023
- **Journal**: None
- **Summary**: Convolution neural networks (CNNs) and Transformers have their own advantages and both have been widely used for dense prediction in multi-task learning (MTL). Most of the current studies on MTL solely rely on CNN or Transformer. In this work, we present a novel MTL model by combining both merits of deformable CNN and query-based Transformer for multi-task learning of dense prediction. Our method, named DeMT, is based on a simple and effective encoder-decoder architecture (i.e., deformable mixer encoder and task-aware transformer decoder). First, the deformable mixer encoder contains two types of operators: the channel-aware mixing operator leveraged to allow communication among different channels ($i.e.,$ efficient channel location mixing), and the spatial-aware deformable operator with deformable convolution applied to efficiently sample more informative spatial locations (i.e., deformed features). Second, the task-aware transformer decoder consists of the task interaction block and task query block. The former is applied to capture task interaction features via self-attention. The latter leverages the deformed features and task-interacted features to generate the corresponding task-specific feature through a query-based Transformer for corresponding task predictions. Extensive experiments on two dense image prediction datasets, NYUD-v2 and PASCAL-Context, demonstrate that our model uses fewer GFLOPs and significantly outperforms current Transformer- and CNN-based competitive models on a variety of metrics. The code are available at https://github.com/yangyangxu0/DeMT .



### On the challenges to learn from Natural Data Streams
- **Arxiv ID**: http://arxiv.org/abs/2301.03495v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03495v1)
- **Published**: 2023-01-09 16:32:02+00:00
- **Updated**: 2023-01-09 16:32:02+00:00
- **Authors**: Guido Borghi, Gabriele Graffieti, Davide Maltoni
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world contexts, sometimes data are available in form of Natural Data Streams, i.e. data characterized by a streaming nature, unbalanced distribution, data drift over a long time frame and strong correlation of samples in short time ranges. Moreover, a clear separation between the traditional training and deployment phases is usually lacking. This data organization and fruition represents an interesting and challenging scenario for both traditional Machine and Deep Learning algorithms and incremental learning agents, i.e. agents that have the ability to incrementally improve their knowledge through the past experience. In this paper, we investigate the classification performance of a variety of algorithms that belong to various research field, i.e. Continual, Streaming and Online Learning, that receives as training input Natural Data Streams. The experimental validation is carried out on three different datasets, expressly organized to replicate this challenging setting.



### Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2301.03505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03505v2)
- **Published**: 2023-01-09 16:56:23+00:00
- **Updated**: 2023-01-10 16:00:52+00:00
- **Authors**: Reza Azad, Amirhossein Kazerouni, Moein Heidari, Ehsan Khodapanah Aghdam, Amirali Molaei, Yiwei Jia, Abin Jose, Rijo Roy, Dorit Merhof
- **Comment**: typo correction applied
- **Journal**: None
- **Summary**: The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.



### Parallel Reasoning Network for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.03510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03510v1)
- **Published**: 2023-01-09 17:00:34+00:00
- **Updated**: 2023-01-09 17:00:34+00:00
- **Authors**: Huan Peng, Fenggang Liu, Yangguang Li, Bin Huang, Jing Shao, Nong Sang, Changxin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection aims to learn how human interacts with surrounding objects. Previous HOI detection frameworks simultaneously detect human, objects and their corresponding interactions by using a predictor. Using only one shared predictor cannot differentiate the attentive field of instance-level prediction and relation-level prediction. To solve this problem, we propose a new transformer-based method named Parallel Reasoning Network(PR-Net), which constructs two independent predictors for instance-level localization and relation-level understanding. The former predictor concentrates on instance-level localization by perceiving instances' extremity regions. The latter broadens the scope of relation region to reach a better relation-level semantic understanding. Extensive experiments and analysis on HICO-DET benchmark exhibit that our PR-Net effectively alleviated this problem. Our PR-Net has achieved competitive results on HICO-DET and V-COCO benchmarks.



### SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.03512v1
- **DOI**: 10.1109/LRA.2023.3234771
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.03512v1)
- **Published**: 2023-01-09 17:05:28+00:00
- **Updated**: 2023-01-09 17:05:28+00:00
- **Authors**: Thomas Monninger, Julian Schmidt, Jan Rupprecht, David Raba, Julian Jordan, Daniel Frank, Steffen Staab, Klaus Dietmayer
- **Comment**: Thomas Monninger and Julian Schmidt are co-first authors. The order
  was determined alphabetically
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 2023
- **Summary**: Understanding traffic scenes requires considering heterogeneous information about dynamic agents and the static infrastructure. In this work we propose SCENE, a methodology to encode diverse traffic scenes in heterogeneous graphs and to reason about these graphs using a heterogeneous Graph Neural Network encoder and task-specific decoders. The heterogeneous graphs, whose structures are defined by an ontology, consist of different nodes with type-specific node features and different relations with type-specific edge features. In order to exploit all the information given by these graphs, we propose to use cascaded layers of graph convolution. The result is an encoding of the scene. Task-specific decoders can be applied to predict desired attributes of the scene. Extensive evaluation on two diverse binary node classification tasks show the main strength of this methodology: despite being generic, it even manages to outperform task-specific baselines. The further application of our methodology to the task of node classification in various knowledge graphs shows its transferability to other domains.



### FedDebug: Systematic Debugging for Federated Learning Applications
- **Arxiv ID**: http://arxiv.org/abs/2301.03553v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03553v1)
- **Published**: 2023-01-09 18:03:42+00:00
- **Updated**: 2023-01-09 18:03:42+00:00
- **Authors**: Waris Gill, Ali Anwar, Muhammad Ali Gulzar
- **Comment**: None
- **Journal**: None
- **Summary**: In Federated Learning (FL), clients train a model locally and share it with a central aggregator to build a global model. Impermissibility to access client's data and collaborative training makes FL appealing for applications with data-privacy concerns such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, finding the round and the clients responsible is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the accuracy or let future FL rounds retune the model, which are time-consuming and costly.   We design a systematic fault localization framework, FedDebug, that advances the FL debugging on two novel fronts. First, FedDebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to construct a simulation that mirrors live FL. FedDebug's {\em breakpoint} can help inspect an FL state (round, client, and global model) and seamlessly move between rounds and clients' models, enabling a fine-grained step-by-step inspection. Second, FedDebug automatically identifies the client responsible for lowering global model's performance without any testing data and labels--both are essential for existing debugging techniques. FedDebug's strengths come from adapting differential testing in conjunction with neurons activations to determine the precise client deviating from normal behavior. FedDebug achieves 100\% to find a single client and 90.3\% accuracy to find multiple faulty clients. FedDebug's interactive debugging incurs 1.2\% overhead during training, while it localizes a faulty client in only 2.1\% of a round's training time. With FedDebug, we bring effective debugging practices to federated learning, improving the quality and productivity of FL application developers.



### Ancilia: Scalable Intelligent Video Surveillance for the Artificial Intelligence of Things
- **Arxiv ID**: http://arxiv.org/abs/2301.03561v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2301.03561v2)
- **Published**: 2023-01-09 18:21:22+00:00
- **Updated**: 2023-03-09 18:55:02+00:00
- **Authors**: Armin Danesh Pazho, Christopher Neff, Ghazal Alinezhad Noghre, Babak Rahimi Ardabili, Shanle Yao, Mohammadreza Baharani, Hamed Tabkhi
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of vision-based artificial intelligence, the proliferation of the Internet of Things connected cameras, and the increasing societal need for rapid and equitable security, the demand for accurate real-time intelligent surveillance has never been higher. This article presents Ancilia, an end-to-end scalable, intelligent video surveillance system for the Artificial Intelligence of Things. Ancilia brings state-of-the-art artificial intelligence to real-world surveillance applications while respecting ethical concerns and performing high-level cognitive tasks in real-time. Ancilia aims to revolutionize the surveillance landscape, to bring more effective, intelligent, and equitable security to the field, resulting in safer and more secure communities without requiring people to compromise their right to privacy.



### An Impartial Transformer for Story Visualization
- **Arxiv ID**: http://arxiv.org/abs/2301.03563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03563v1)
- **Published**: 2023-01-09 18:29:01+00:00
- **Updated**: 2023-01-09 18:29:01+00:00
- **Authors**: Nikolaos Tsakas, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou
- **Comment**: None
- **Journal**: None
- **Summary**: Story Visualization is an advanced task of computed vision that targets sequential image synthesis, where the generated samples need to be realistic, faithful to their conditioning and sequentially consistent. Our work proposes a novel architectural and training approach: the Impartial Transformer achieves both text-relevant plausible scenes and sequential consistency utilizing as few trainable parameters as possible. This enhancement is even able to handle synthesis of 'hard' samples with occluded objects, achieving improved evaluation metrics comparing to past approaches.



### Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction
- **Arxiv ID**: http://arxiv.org/abs/2301.03573v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03573v1)
- **Published**: 2023-01-09 18:50:03+00:00
- **Updated**: 2023-01-09 18:50:03+00:00
- **Authors**: Bowen Lei, Dongkuan Xu, Ruqi Zhang, Shuren He, Bani K. Mallick
- **Comment**: None
- **Journal**: None
- **Summary**: Despite impressive performance on a wide variety of tasks, deep neural networks require significant memory and computation costs, prohibiting their application in resource-constrained scenarios. Sparse training is one of the most common techniques to reduce these costs, however, the sparsity constraints add difficulty to the optimization, resulting in an increase in training time and instability. In this work, we aim to overcome this problem and achieve space-time co-efficiency. To accelerate and stabilize the convergence of sparse training, we analyze the gradient changes and develop an adaptive gradient correction method. Specifically, we approximate the correlation between the current and previous gradients, which is used to balance the two gradients to obtain a corrected gradient. Our method can be used with most popular sparse training pipelines under both standard and adversarial setups. Theoretically, we prove that our method can accelerate the convergence rate of sparse training. Extensive experiments on multiple datasets, model architectures, and sparsities demonstrate that our method outperforms leading sparse training methods by up to \textbf{5.0\%} in accuracy given the same number of training epochs, and reduces the number of training epochs by up to \textbf{52.1\%} to achieve the same accuracy.



### Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2301.02667v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.02667v1)
- **Published**: 2023-01-09 18:59:16+00:00
- **Updated**: 2023-01-09 18:59:16+00:00
- **Authors**: Jiye Lee, Hanbyul Joo
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing interaction-involved human motions has been challenging due to the high complexity of 3D environments and the diversity of possible human behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long term human movements in complex indoor environments. The key motivation of LAMA is to build a unified framework to encompass a series of motions commonly observable in our daily lives, including locomotion, interactions with 3D scenes, and manipulations of 3D objects. LAMA is based on a reinforcement learning framework coupled with a motion matching algorithm to synthesize locomotion and scene interaction seamlessly under common constraints and collision avoidance handling. LAMA also exploits a motion editing framework via manifold learning to cover possible variations in interaction and manipulation motions. We quantitatively and qualitatively demonstrate that LAMA outperforms existing approaches in various challenging scenarios. Project webpage: https://lama-www.github.io/ .



### Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling
- **Arxiv ID**: http://arxiv.org/abs/2301.03580v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03580v2)
- **Published**: 2023-01-09 18:59:50+00:00
- **Updated**: 2023-01-10 08:02:09+00:00
- **Authors**: Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, Zehuan Yuan
- **Comment**: v2: fixed some formatting errors
- **Journal**: None
- **Summary**: We identify and overcome two key obstacles in extending the success of BERT-style pre-training, or the masked image modeling, to convolutional networks (convnets): (i) convolution operation cannot handle irregular, random-masked input images; (ii) the single-scale nature of BERT pre-training is inconsistent with convnet's hierarchical structure. For (i), we treat unmasked pixels as sparse voxels of 3D point clouds and use sparse convolution to encode. This is the first use of sparse convolution for 2D masked modeling. For (ii), we develop a hierarchical decoder to reconstruct images from multi-scale encoded features. Our method called Sparse masKed modeling (SparK) is general: it can be used directly on any convolutional model without backbone modifications. We validate it on both classical (ResNet) and modern (ConvNeXt) models: on three downstream tasks, it surpasses both state-of-the-art contrastive learning and transformer-based masked modeling by similarly large margins (around +1.0%). Improvements on object detection and instance segmentation are more substantial (up to +3.5%), verifying the strong transferability of features learned. We also find its favorable scaling behavior by observing more gains on larger models. All this evidence reveals a promising future of generative pre-training on convnets. Codes and models are released at https://github.com/keyu-tian/SparK.



### MOC-AE: An Anatomically-Pathological-Based model for Clinical Decision Support System of tumoural brain images
- **Arxiv ID**: http://arxiv.org/abs/2301.03701v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2301.03701v1)
- **Published**: 2023-01-09 22:15:18+00:00
- **Updated**: 2023-01-09 22:15:18+00:00
- **Authors**: Guillermo Iglesias, Edgar Talavera, Alberto Díaz-Álvarez, Miguel Gracía-Remesal
- **Comment**: 10 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: The present work proposes a Multi-Output Classification Autoencoder (MOC-AE) algorithm to extract features from brain tumour images. The proposed algorithm is able to focus on both the normal features of the patient and the pathological features present in the case, resulting in a compact and significant representation of each image. The architecture of MOC-AE combines anatomical information from the patients scan using an Autoencoder (AE) with information related to a specific pathology using a classification output with the same image descriptor. This combination of goals forces the network to maintain a balance between anatomical and pathological features of the case while maintaining the low cost of the labels being used. The results obtained are compared with those of similar studies and the strengths and limitations of each approach are discussed. The results demonstrate that the proposed algorithm is capable of achieving state-of-the-art results in terms of both the anatomical and tumor characteristics of the recommended cases.



### 3D Shape Perception Integrates Intuitive Physics and Analysis-by-Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.03711v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03711v1)
- **Published**: 2023-01-09 23:11:41+00:00
- **Updated**: 2023-01-09 23:11:41+00:00
- **Authors**: Ilker Yildirim, Max H. Siegel, Amir A. Soltani, Shraman Ray Chaudhari, Joshua B. Tenenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Many surface cues support three-dimensional shape perception, but people can sometimes still see shape when these features are missing -- in extreme cases, even when an object is completely occluded, as when covered with a draped cloth. We propose a framework for 3D shape perception that explains perception in both typical and atypical cases as analysis-by-synthesis, or inference in a generative model of image formation: the model integrates intuitive physics to explain how shape can be inferred from deformations it causes to other objects, as in cloth-draping. Behavioral and computational studies comparing this account with several alternatives show that it best matches human observers in both accuracy and response times, and is the only model that correlates significantly with human performance on difficult discriminations. Our results suggest that bottom-up deep neural network models are not fully adequate accounts of human shape perception, and point to how machine vision systems might achieve more human-like robustness.



