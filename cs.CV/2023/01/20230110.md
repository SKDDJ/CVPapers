# Arxiv Papers in cs.CV on 2023-01-10
### Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video
- **Arxiv ID**: http://arxiv.org/abs/2302.01923v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.01923v1)
- **Published**: 2023-01-10 00:22:30+00:00
- **Updated**: 2023-01-10 00:22:30+00:00
- **Authors**: Russ Messenger, Md Zobaer Islam, Matthew Whitlock, Erik Spong, Nate Morton, Layne Claggett, Chris Matthews, Jordan Fox, Leland Palmer, Dane C. Johnson, John F. O'Hara, Christopher J. Crick, Jamey D. Jacob, Sabit Ekin
- **Comment**: 13 pages, 21 figures, submitted to International Journal of
  Intelligent Transportation Systems Research
- **Journal**: None
- **Summary**: Highway work zones are susceptible to undue accumulation of motorized vehicles which calls for dynamic work zone warning signs to prevent accidents. The work zone signs are placed according to the location of the end-of-queue of vehicles which usually changes rapidly. The detection of moving objects in video captured by Unmanned Aerial Vehicles (UAV) has been extensively researched so far, and is used in a wide array of applications including traffic monitoring. Unlike the fixed traffic cameras, UAVs can be used to monitor the traffic at work zones in real-time and also in a more cost-effective way. This study presents a method as a proof of concept for detecting End-of-Queue (EOQ) of traffic by processing the real-time video footage of a highway work zone captured by UAV. EOQ is detected in the video by image processing which includes background subtraction and blob detection methods. This dynamic localization of EOQ of vehicles will enable faster and more accurate relocation of work zone warning signs for drivers and thus will reduce work zone fatalities. The method can be applied to detect EOQ of vehicles and notify drivers in any other roads or intersections too where vehicles are rapidly accumulating due to special events, traffic jams, construction, or accidents.



### Learning to Perceive in Deep Model-Free Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.03730v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03730v2)
- **Published**: 2023-01-10 00:31:57+00:00
- **Updated**: 2023-01-13 01:04:54+00:00
- **Authors**: Gonçalo Querido, Alberto Sardinha, Francisco S. Melo
- **Comment**: 8 pages; 7 figures; fixed author name; added link for code
- **Journal**: None
- **Summary**: This work proposes a novel model-free Reinforcement Learning (RL) agent that is able to learn how to complete an unknown task having access to only a part of the input observation. We take inspiration from the concepts of visual attention and active perception that are characteristic of humans and tried to apply them to our agent, creating a hard attention mechanism. In this mechanism, the model decides first which region of the input image it should look at, and only after that it has access to the pixels of that region. Current RL agents do not follow this principle and we have not seen these mechanisms applied to the same purpose as this work. In our architecture, we adapt an existing model called recurrent attention model (RAM) and combine it with the proximal policy optimization (PPO) algorithm. We investigate whether a model with these characteristics is capable of achieving similar performance to state-of-the-art model-free RL agents that access the full input observation. This analysis is made in two Atari games, Pong and SpaceInvaders, which have a discrete action space, and in CarRacing, which has a continuous action space. Besides assessing its performance, we also analyze the movement of the attention of our model and compare it with what would be an example of the human behavior. Even with such visual limitation, we show that our model matches the performance of PPO+LSTM in two of the three games tested.



### Online Backfilling with No Regret for Large-Scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2301.03767v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03767v1)
- **Published**: 2023-01-10 03:10:32+00:00
- **Updated**: 2023-01-10 03:10:32+00:00
- **Authors**: Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Joena Zhang, Taipeng Tian, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: Backfilling is the process of re-extracting all gallery embeddings from upgraded models in image retrieval systems. It inevitably requires a prohibitively large amount of computational cost and even entails the downtime of the service. Although backward-compatible learning sidesteps this challenge by tackling query-side representations, this leads to suboptimal solutions in principle because gallery embeddings cannot benefit from model upgrades. We address this dilemma by introducing an online backfilling algorithm, which enables us to achieve a progressive performance improvement during the backfilling process while not sacrificing the final performance of new model after the completion of backfilling. To this end, we first propose a simple distance rank merge technique for online backfilling. Then, we incorporate a reverse transformation module for more effective and efficient merging, which is further enhanced by adopting a metric-compatible contrastive learning approach. These two components help to make the distances of old and new models compatible, resulting in desirable merge results during backfilling with no extra computational overhead. Extensive experiments show the effectiveness of our framework on four standard benchmarks in various settings.



### Learning from What is Already Out There: Few-shot Sign Language Recognition with Online Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/2301.03769v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2301.03769v1)
- **Published**: 2023-01-10 03:21:01+00:00
- **Updated**: 2023-01-10 03:21:01+00:00
- **Authors**: Matyáš Boháček, Marek Hrúz
- **Comment**: 6 pages, 2 figures, IEEE Face & Gestures 2023
- **Journal**: None
- **Summary**: Today's sign language recognition models require large training corpora of laboratory-like videos, whose collection involves an extensive workforce and financial resources. As a result, only a handful of such systems are publicly available, not to mention their limited localization capabilities for less-populated sign languages. Utilizing online text-to-video dictionaries, which inherently hold annotated data of various attributes and sign languages, and training models in a few-shot fashion hence poses a promising path for the democratization of this technology. In this work, we collect and open-source the UWB-SL-Wild few-shot dataset, the first of its kind training resource consisting of dictionary-scraped videos. This dataset represents the actual distribution and characteristics of available online sign language data. We select glosses that directly overlap with the already existing datasets WLASL100 and ASLLVD and share their class mappings to allow for transfer learning experiments. Apart from providing baseline results on a pose-based architecture, we introduce a novel approach to training sign language recognition models in a few-shot scenario, resulting in state-of-the-art results on ASLLVD-Skeleton and ASLLVD-Skeleton-20 datasets with top-1 accuracy of $30.97~\%$ and $95.45~\%$, respectively.



### DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation
- **Arxiv ID**: http://arxiv.org/abs/2301.03786v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03786v2)
- **Published**: 2023-01-10 05:11:25+00:00
- **Updated**: 2023-04-20 08:51:11+00:00
- **Authors**: Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu
- **Comment**: Project page https://sstzal.github.io/DiffTalk/
- **Journal**: None
- **Summary**: Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally generalized across different identities without any further fine-tuning. Additionally, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identities. For more video results, please refer to \url{https://sstzal.github.io/DiffTalk/}.



### Assessing the applicability of common performance metrics for real-world infrared small-target detection
- **Arxiv ID**: http://arxiv.org/abs/2301.03796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03796v1)
- **Published**: 2023-01-10 05:40:28+00:00
- **Updated**: 2023-01-10 05:40:28+00:00
- **Authors**: Saed Moradi, Alireza Memarmoghadam, Payman Moallem, Mohamad Farzan Sabahi
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared small target detection (IRSTD) is a challenging task in computer vision. During the last two decades, researchers' efforts are devoted to improving detection ability of IRSTDs. Despite the huge improvement in designing new algorithms, lack of extensive investigation of the evaluation metrics are evident. Therefore, in this paper, a systematic approach is utilized to: First, investigate the evaluation ability of current metrics; Second, propose new evaluation metrics to address shortcoming of common metrics. To this end, after carefully reviewing the problem, the required conditions to have a successful detection are analyzed. Then, the shortcomings of current evaluation metrics which include pre-thresholding as well as post-thresholding metrics are determined. Based on the requirements of real-world systems, new metrics are proposed. Finally, the proposed metrics are used to compare and evaluate four well-known small infrared target detection algorithms. The results show that new metrics are consistent with qualitative results.



### CDA: Contrastive-adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2301.03826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03826v1)
- **Published**: 2023-01-10 07:43:21+00:00
- **Updated**: 2023-01-10 07:43:21+00:00
- **Authors**: Nishant Yadav, Mahbubul Alam, Ahmed Farahat, Dipanjan Ghosh, Chetan Gupta, Auroop R. Ganguly
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in domain adaptation reveal that adversarial learning on deep neural networks can learn domain invariant features to reduce the shift between source and target domains. While such adversarial approaches achieve domain-level alignment, they ignore the class (label) shift. When class-conditional data distributions are significantly different between the source and target domain, it can generate ambiguous features near class boundaries that are more likely to be misclassified. In this work, we propose a two-stage model for domain adaptation called \textbf{C}ontrastive-adversarial \textbf{D}omain \textbf{A}daptation \textbf{(CDA)}. While the adversarial component facilitates domain-level alignment, two-stage contrastive learning exploits class information to achieve higher intra-class compactness across domains resulting in well-separated decision boundaries. Furthermore, the proposed contrastive framework is designed as a plug-and-play module that can be easily embedded with existing adversarial methods for domain adaptation. We conduct experiments on two widely used benchmark datasets for domain adaptation, namely, \textit{Office-31} and \textit{Digits-5}, and demonstrate that CDA achieves state-of-the-art results on both datasets.



### From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore
- **Arxiv ID**: http://arxiv.org/abs/2301.03829v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DB, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.03829v2)
- **Published**: 2023-01-10 07:51:36+00:00
- **Updated**: 2023-03-28 15:54:39+00:00
- **Authors**: Kaiping Zheng, Thao Nguyen, Jesslyn Hwei Sing Chong, Charlene Enhui Goh, Melanie Herschel, Hee Hoon Lee, Changshuo Liu, Beng Chin Ooi, Wei Wang, James Yip
- **Comment**: None
- **Journal**: None
- **Summary**: Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learning into our food recognition model FoodSG-SCL for the intrinsic capability to mine hard positive/negative samples and therefore boost the accuracy. Through a comprehensive evaluation, we present performance results of the proposed model and insights on food-related healthcare applications. The FoodSG-233 dataset has been released in https://foodlg.comp.nus.edu.sg/.



### Dynamic Grained Encoder for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.03831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03831v1)
- **Published**: 2023-01-10 07:55:29+00:00
- **Updated**: 2023-01-10 07:55:29+00:00
- **Authors**: Lin Song, Songyang Zhang, Songtao Liu, Zeming Li, Xuming He, Hongbin Sun, Jian Sun, Nanning Zheng
- **Comment**: Accepted by NeurIPS2021
- **Journal**: None
- **Summary**: Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40%-60% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.



### Video Semantic Segmentation with Inter-Frame Feature Fusion and Inner-Frame Feature Refinement
- **Arxiv ID**: http://arxiv.org/abs/2301.03832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03832v1)
- **Published**: 2023-01-10 07:57:05+00:00
- **Updated**: 2023-01-10 07:57:05+00:00
- **Authors**: Jiafan Zhuang, Zilei Wang, Junjie Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video semantic segmentation aims to generate accurate semantic maps for each video frame. To this end, many works dedicate to integrate diverse information from consecutive frames to enhance the features for prediction, where a feature alignment procedure via estimated optical flow is usually required. However, the optical flow would inevitably suffer from inaccuracy, and then introduce noises in feature fusion and further result in unsatisfactory segmentation results. In this paper, to tackle the misalignment issue, we propose a spatial-temporal fusion (STF) module to model dense pairwise relationships among multi-frame features. Different from previous methods, STF uniformly and adaptively fuses features at different spatial and temporal positions, and avoids error-prone optical flow estimation. Besides, we further exploit feature refinement within a single frame and propose a novel memory-augmented refinement (MAR) module to tackle difficult predictions among semantic boundaries. Specifically, MAR can store the boundary features and prototypes extracted from the training samples, which together form the task-specific memory, and then use them to refine the features during inference. Essentially, MAR can move the hard features closer to the most likely category and thus make them more discriminative. We conduct extensive experiments on Cityscapes and CamVid, and the results show that our proposed methods significantly outperform previous methods and achieves the state-of-the-art performance. Code and pretrained models are available at https://github.com/jfzhuang/ST_Memory.



### InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.04470v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04470v2)
- **Published**: 2023-01-10 08:15:35+00:00
- **Updated**: 2023-06-22 10:12:01+00:00
- **Authors**: Juyeb Shin, Francois Rameau, Hyeonjun Jeong, Dongsuk Kum
- **Comment**: Workshop on Vision-Centric Autonomous Driving (VCAD) at Conference on
  Computer Vision and Pattern Recognition (CVPR) 2023
- **Journal**: None
- **Summary**: Inferring traffic object such as lane information is of foremost importance for deployment of autonomous driving. Previous approaches focus on offline construction of HD map inferred with GPS localization, which is insufficient for globally scalable autonomous driving. To alleviate these issues, we propose online HD map learning framework that detects HD map elements from onboard sensor observations. We represent the map elements as a graph; we propose InstaGraM, instance-level graph modeling of HD map that brings accurate and fast end-to-end vectorized HD map learning. Along with the graph modeling strategy, we propose end-to-end neural network composed of three stages: a unified BEV feature extraction, map graph component detection, and association via graph neural networks. Comprehensive experiments on public open dataset show that our proposed network outperforms previous models by up to 13.7 mAP with up to 33.8X faster computation time.



### A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models
- **Arxiv ID**: http://arxiv.org/abs/2301.03843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03843v2)
- **Published**: 2023-01-10 08:21:19+00:00
- **Updated**: 2023-01-17 05:40:33+00:00
- **Authors**: Rei Aso, Tatsuya Chuman, Hitoshi Kiya
- **Comment**: To appear in 2023 RISP International Workshop on Nonlinear Circuits,
  Communications and Signal Processing
- **Journal**: None
- **Summary**: In this paper, a privacy preserving image classification method is proposed under the use of ConvMixer models. To protect the visual information of test images, a test image is divided into blocks, and then every block is encrypted by using a random orthogonal matrix. Moreover, a ConvMixer model trained with plain images is transformed by the random orthogonal matrix used for encrypting test images, on the basis of the embedding structure of ConvMixer. The proposed method allows us not only to use the same classification accuracy as that of ConvMixer models without considering privacy protection but to also enhance robustness against various attacks compared to conventional privacy-preserving learning.



### Look Beyond Bias with Entropic Adversarial Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.03844v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.03844v1)
- **Published**: 2023-01-10 08:25:24+00:00
- **Updated**: 2023-01-10 08:25:24+00:00
- **Authors**: Thomas Duboudin, Emmanuel Dellandréa, Corentin Abgrall, Gilles Hénaff, Liming Chen
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition 2022, Aug 2022,
  Montr{\'e}al, Canada
- **Summary**: Deep neural networks do not discriminate between spurious and causal patterns, and will only learn the most predictive ones while ignoring the others. This shortcut learning behaviour is detrimental to a network's ability to generalize to an unknown test-time distribution in which the spurious correlations do not hold anymore. Debiasing methods were developed to make networks robust to such spurious biases but require to know in advance if a dataset is biased and make heavy use of minority counterexamples that do not display the majority bias of their class. In this paper, we argue that such samples should not be necessarily needed because the ''hidden'' causal information is often also contained in biased images. To study this idea, we propose 3 publicly released synthetic classification benchmarks, exhibiting predictive classification shortcuts, each of a different and challenging nature, without any minority samples acting as counterexamples. First, we investigate the effectiveness of several state-of-the-art strategies on our benchmarks and show that they do not yield satisfying results on them. Then, we propose an architecture able to succeed on our benchmarks, despite their unusual properties, using an entropic adversarial data augmentation training scheme. An encoder-decoder architecture is tasked to produce images that are not recognized by a classifier, by maximizing the conditional entropy of its outputs, and keep as much as possible of the initial content. A precise control of the information destroyed, via a disentangling process, enables us to remove the shortcut and leave everything else intact. Furthermore, results competitive with the state-of-the-art on the BAR dataset ensure the applicability of our method in real-life situations.



### Deep Multi-stream Network for Video-based Calving Sign Detection
- **Arxiv ID**: http://arxiv.org/abs/2302.08493v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.08493v1)
- **Published**: 2023-01-10 09:23:27+00:00
- **Updated**: 2023-01-10 09:23:27+00:00
- **Authors**: Ryosuke Hyodo, Teppei Nakano, Tetsuji Ogawa
- **Comment**: None
- **Journal**: None
- **Summary**: We have designed a deep multi-stream network for automatically detecting calving signs from video. Calving sign detection from a camera, which is a non-contact sensor, is expected to enable more efficient livestock management. As large-scale, well-developed data cannot generally be assumed when establishing calving detection systems, the basis for making the prediction needs to be presented to farmers during operation, so black-box modeling (also known as end-to-end modeling) is not appropriate. For practical operation of calving detection systems, the present study aims to incorporate expert knowledge into a deep neural network. To this end, we propose a multi-stream calving sign detection network in which multiple calving-related features are extracted from the corresponding feature extraction networks designed for each attribute with different characteristics, such as a cow's posture, rotation, and movement, known as calving signs, and are then integrated appropriately depending on the cow's situation. Experimental comparisons conducted using videos of 15 cows demonstrated that our multi-stream system yielded a significant improvement over the end-to-end system, and the multi-stream architecture significantly contributed to a reduction in detection errors. In addition, the distinctive mixture weights we observed helped provide interpretability of the system's behavior.



### Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/2301.03867v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.03867v1)
- **Published**: 2023-01-10 09:38:43+00:00
- **Updated**: 2023-01-10 09:38:43+00:00
- **Authors**: Thorsten Hempel, Laslo Dinges, Ayoub Al-Hamadi
- **Comment**: Camera ready version - 18th International Conference on Computer
  Vision Theory and Applications (VISAPP 2023)
- **Journal**: None
- **Summary**: Emotion expressions serve as important communicative signals and are crucial cues in intuitive interactions between humans. Hence, it is essential to include these fundamentals in robotic behavior strategies when interacting with humans to promote mutual understanding and to reduce misjudgements. We tackle this challenge by detecting and using the emotional state and attention for a sentiment analysis of potential human interaction partners to select well-adjusted engagement strategies. This way, we pave the way for more intuitive human-robot interactions, as the robot's action conforms to the person's mood and expectation. We propose four different engagement strategies with implicit and explicit communication techniques that we implement on a mobile robot platform for initial experiments.



### Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.03914v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03914v1)
- **Published**: 2023-01-10 11:35:14+00:00
- **Updated**: 2023-01-10 11:35:14+00:00
- **Authors**: Thomas Bonte, Maxence Philbert, Emeline Coleno, Edouard Bertrand, Arthur Imbert, Thomas Walter
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning provides us with powerful methods to perform nucleus or cell segmentation with unprecedented quality. However, these methods usually require large training sets of manually annotated images, which are tedious and expensive to generate. In this paper we propose to use In Silico Labeling (ISL) as a pretraining scheme for segmentation tasks. The strategy is to acquire label-free microscopy images (such as bright-field or phase contrast) along fluorescently labeled images (such as DAPI or CellMask). We then train a model to predict the fluorescently labeled images from the label-free microscopy images. By comparing segmentation performance across several training set sizes, we show that such a scheme can dramatically reduce the number of required annotations.



### Speech Driven Video Editing via an Audio-Conditioned Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2301.04474v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.04474v3)
- **Published**: 2023-01-10 12:01:20+00:00
- **Updated**: 2023-05-11 11:56:42+00:00
- **Authors**: Dan Bigioi, Shubhajit Basak, Michał Stypułkowski, Maciej Zięba, Hugh Jordan, Rachel McDonnell, Peter Corcoran
- **Comment**: 8 Pages, code and project page available here:
  https://danbigioi.github.io/DiffusionVideoEditing/
- **Journal**: None
- **Summary**: Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.



### Video Surveillance System Incorporating Expert Decision-making Process: A Case Study on Detecting Calving Signs in Cattle
- **Arxiv ID**: http://arxiv.org/abs/2301.03926v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03926v1)
- **Published**: 2023-01-10 12:06:49+00:00
- **Updated**: 2023-01-10 12:06:49+00:00
- **Authors**: Ryosuke Hyodo, Susumu Saito, Teppei Nakano, Makoto Akabane, Ryoichi Kasuga, Tetsuji Ogawa
- **Comment**: None
- **Journal**: None
- **Summary**: Through a user study in the field of livestock farming, we verify the effectiveness of an XAI framework for video surveillance systems. The systems can be made interpretable by incorporating experts' decision-making processes. AI systems are becoming increasingly common in real-world applications, especially in fields related to human decision-making, and its interpretability is necessary. However, there are still relatively few standard methods for assessing and addressing the interpretability of machine learning-based systems in real-world applications. In this study, we examine the framework of a video surveillance AI system that presents the reasoning behind predictions by incorporating experts' decision-making processes with rich domain knowledge of the notification target. While general black-box AI systems can only present final probability values, the proposed framework can present information relevant to experts' decisions, which is expected to be more helpful for their decision-making. In our case study, we designed a system for detecting signs of calving in cattle based on the proposed framework and evaluated the system through a user study (N=6) with people involved in livestock farming. A comparison with the black-box AI system revealed that many participants referred to the presented reasons for the prediction results, and five out of six participants selected the proposed system as the system they would like to use in the future. It became clear that we need to design a user interface that considers the reasons for the prediction results.



### Autonomous Strawberry Picking Robotic System (Robofruit)
- **Arxiv ID**: http://arxiv.org/abs/2301.03947v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.03947v1)
- **Published**: 2023-01-10 13:02:23+00:00
- **Updated**: 2023-01-10 13:02:23+00:00
- **Authors**: Soran Parsa, Bappaditya Debnath, Muhammad Arshad Khan, Amir Ghalamzan E.
- **Comment**: To appear in the Journal of Field Robotics (Accepted) Please watch
  the video at https://www.youtube.com/watch?v=v8gGAvsISXU
- **Journal**: None
- **Summary**: Challenges in strawberry picking made selective harvesting robotic technology demanding. However, selective harvesting of strawberries is complicated forming a few scientific research questions. Most available solutions only deal with a specific picking scenario, e.g., picking only a single variety of fruit in isolation. Nonetheless, most economically viable (e.g. high-yielding and/or disease-resistant) varieties of strawberry are grown in dense clusters. The current perception technology in such use cases is inefficient. In this work, we developed a novel system capable of harvesting strawberries with several unique features. The features allow the system to deal with very complex picking scenarios, e.g. dense clusters. Our concept of a modular system makes our system reconfigurable to adapt to different picking scenarios. We designed, manufactured, and tested a picking head with 2.5 DOF (2 independent mechanisms and 1 dependent cutting system) capable of removing possible occlusions and harvesting targeted strawberries without contacting fruit flesh to avoid damage and bruising. In addition, we developed a novel perception system to localise strawberries and detect their key points, picking points, and determine their ripeness. For this purpose, we introduced two new datasets. Finally, we tested the system in a commercial strawberry growing field and our research farm with three different strawberry varieties. The results show the effectiveness and reliability of the proposed system. The designed picking head was able to remove occlusions and harvest strawberries effectively. The perception system was able to detect and determine the ripeness of strawberries with 95% accuracy. In total, the system was able to harvest 87% of all detected strawberries with a success rate of 83% for all pluckable fruits. We also discuss a series of open research questions in the discussion section.



### Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2301.03949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03949v2)
- **Published**: 2023-01-10 13:15:42+00:00
- **Updated**: 2023-03-28 08:26:30+00:00
- **Authors**: Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion-based generative models have recently emerged as powerful solutions for high-quality synthesis in multiple domains. Leveraging the bidirectional Markov chains, diffusion probabilistic models generate samples by inferring the reversed Markov chain based on the learned distribution mapping at the forward diffusion process. In this work, we propose Modiff, a conditional paradigm that benefits from the denoising diffusion probabilistic model (DDPM) to tackle the problem of realistic and diverse action-conditioned 3D skeleton-based motion generation. We are a pioneering attempt that uses DDPM to synthesize a variable number of motion sequences conditioned on a categorical action. We evaluate our approach on the large-scale NTU RGB+D dataset and show improvements over state-of-the-art motion generation methods.



### AdvBiom: Adversarial Attacks on Biometric Matchers
- **Arxiv ID**: http://arxiv.org/abs/2301.03966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03966v1)
- **Published**: 2023-01-10 14:01:11+00:00
- **Updated**: 2023-01-10 14:01:11+00:00
- **Authors**: Debayan Deb, Vishesh Mistry, Rahul Parthe
- **Comment**: arXiv admin note: text overlap with arXiv:1908.05008
- **Journal**: None
- **Summary**: With the advent of deep learning models, face recognition systems have achieved impressive recognition rates. The workhorses behind this success are Convolutional Neural Networks (CNNs) and the availability of large training datasets. However, we show that small human-imperceptible changes to face samples can evade most prevailing face recognition systems. Even more alarming is the fact that the same generator can be extended to other traits in the future. In this work, we present how such a generator can be trained and also extended to other biometric modalities, such as fingerprint recognition systems.



### Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2301.03976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.03976v1)
- **Published**: 2023-01-10 14:15:17+00:00
- **Updated**: 2023-01-10 14:15:17+00:00
- **Authors**: Hao Xu, Hui Xiao, Huazheng Hao, Li Dong, Xiaojie Qiu, Chengbin Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning frameworks usually adopt mutual learning approaches with multiple submodels to learn from different perspectives. To avoid transferring erroneous pseudo labels between these submodels, a high threshold is usually used to filter out a large number of low-confidence predictions for unlabeled data. However, such filtering can not fully exploit unlabeled data with low prediction confidence. To overcome this problem, in this work, we propose a mutual learning framework based on pseudo-negative labels. Negative labels are those that a corresponding data item does not belong. In each iteration, one submodel generates pseudo-negative labels for each data item, and the other submodel learns from these labels. The role of the two submodels exchanges after each iteration until convergence. By reducing the prediction probability on pseudo-negative labels, the dual model can improve its prediction ability. We also propose a mechanism to select a few pseudo-negative labels to feed into submodels. In the experiments, our framework achieves state-of-the-art results on several main benchmarks. Specifically, with our framework, the error rates of the 13-layer CNN model are 9.35% and 7.94% for CIFAR-10 with 1000 and 4000 labels, respectively. In addition, for the non-augmented MNIST with only 20 labels, the error rate is 0.81% by our framework, which is much smaller than that of other approaches. Our approach also demonstrates a significant performance improvement in domain adaptation.



### Objective Evaluation-based High-efficiency Learning Framework for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2302.05297v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2302.05297v1)
- **Published**: 2023-01-10 15:15:55+00:00
- **Updated**: 2023-01-10 15:15:55+00:00
- **Authors**: Xuming Zhang, Jian Yan, Jia Tian, Wei Li, Xingfa Gu, Qingjiu Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have been successfully applied to hyperspectral image (HSI) classification with remarkable performance. Because of limited labelled HSI data, earlier studies primarily adopted a patch-based classification framework, which divides images into overlapping patches for training and testing. However, this approach results in redundant computations and possible information leakage. In this study, we propose an objective evaluation-based high-efficiency learning framework for tiny HSI classification. This framework comprises two main parts: (i) a leakage-free balanced sampling strategy, and (ii) a modified end-to-end fully convolutional network (FCN) architecture that optimizes the trade-off between accuracy and efficiency. The leakage-free balanced sampling strategy generates balanced and non-overlapping training and testing data by partitioning an HSI and the ground truth image into small windows, each of which corresponds to one training or testing sample. The proposed high-efficiency FCN exhibits a pixel-to-pixel architecture with modifications aimed at faster inference speed and improved parameter efficiency. Experiments conducted on four representative datasets demonstrated that the proposed sampling strategy can provide objective performance evaluation and that the proposed network outperformed many state-of-the-art approaches with respect to the speed/accuracy tradeoff. Our source code is available at https://github.com/xmzhang2018.



### Does image resolution impact chest X-ray based fine-grained Tuberculosis-consistent lesion segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2301.04032v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04032v2)
- **Published**: 2023-01-10 15:34:39+00:00
- **Updated**: 2023-01-27 15:40:48+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Feng Yang, Ghada Zamzmi, Zhiyun Xue, Sameer Antani
- **Comment**: 17 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Deep learning (DL) models are state-of-the-art in segmenting anatomical and disease regions of interest (ROIs) in medical images. Particularly, a large number of DL-based techniques have been reported using chest X-rays (CXRs). However, these models are reportedly trained on reduced image resolutions for reasons related to the lack of computational resources. Literature is sparse in discussing the optimal image resolution to train these models for segmenting the Tuberculosis (TB)-consistent lesions in CXRs. In this study, we investigated the performance variations using an Inception-V3 UNet model using various image resolutions with/without lung ROI cropping and aspect ratio adjustments, and (ii) identified the optimal image resolution through extensive empirical evaluations to improve TB-consistent lesion segmentation performance. We used the Shenzhen CXR dataset for the study which includes 326 normal patients and 336 TB patients. We proposed a combinatorial approach consisting of storing model snapshots, optimizing segmentation threshold and test-time augmentation (TTA), and averaging the snapshot predictions, to further improve performance with the optimal resolution. Our experimental results demonstrate that higher image resolutions are not always necessary, however, identifying the optimal image resolution is critical to achieving superior performance.



### ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library
- **Arxiv ID**: http://arxiv.org/abs/2301.04037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04037v1)
- **Published**: 2023-01-10 15:39:02+00:00
- **Updated**: 2023-01-10 15:39:02+00:00
- **Authors**: Mohammadreza Shetab-Bushehri, Miguel Aranda, Youcef Mezouar, Adrien Bartoli, Erol Ozgur
- **Comment**: 19 Pages
- **Journal**: None
- **Summary**: Tracking the 3D shape of a deforming object using only monocular 2D vision is a challenging problem. This is because one should (i) infer the 3D shape from a 2D image, which is a severely underconstrained problem, and (ii) implement the whole solution pipeline in real-time. The pipeline typically requires feature detection and matching, mismatch filtering, 3D shape inference and feature tracking algorithms. We propose ROBUSfT, a conventional pipeline based on a template containing the object's rest shape, texturemap and deformation law. ROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations, fast up to 30 fps, free of training, and robust against partial occlusions and discontinuity in video frames. It outperforms the state-of-the-art methods in challenging datasets. ROBUSfT is implemented as a publicly available C++ library and we provide a tutorial on how to use it in https://github.com/mrshetab/ROBUSfT



### Rethinking Voxelization and Classification for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.04058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04058v1)
- **Published**: 2023-01-10 16:22:04+00:00
- **Updated**: 2023-01-10 16:22:04+00:00
- **Authors**: Youshaa Murhij, Alexander Golodkov, Dmitry Yudin
- **Comment**: Accepted in ICONIP 2022. arXiv admin note: text overlap with
  arXiv:1902.06326 by other authors
- **Journal**: None
- **Summary**: The main challenge in 3D object detection from LiDAR point clouds is achieving real-time performance without affecting the reliability of the network. In other words, the detecting network must be confident enough about its predictions. In this paper, we present a solution to improve network inference speed and precision at the same time by implementing a fast dynamic voxelizer that works on fast pillar-based models in the same way a voxelizer works on slow voxel-based models. In addition, we propose a lightweight detection sub-head model for classifying predicted objects and filter out false detected objects that significantly improves model precision in a negligible time and computing cost. The developed code is publicly available at: https://github.com/YoushaaMurhij/RVCDet.



### Benchmarking Robustness in Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2301.04075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04075v1)
- **Published**: 2023-01-10 17:01:12+00:00
- **Updated**: 2023-01-10 17:01:12+00:00
- **Authors**: Chen Wang, Angtian Wang, Junbo Li, Alan Yuille, Cihang Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view synthesis, thanks to its ability to model 3D object geometries in a concise formulation. However, current approaches to NeRF-based models rely on clean images with accurate camera calibration, which can be difficult to obtain in the real world, where data is often subject to corruption and distortion. In this work, we provide the first comprehensive analysis of the robustness of NeRF-based novel view synthesis algorithms in the presence of different types of corruptions.   We find that NeRF-based models are significantly degraded in the presence of corruption, and are more sensitive to a different set of corruptions than image recognition models. Furthermore, we analyze the robustness of the feature encoder in generalizable methods, which synthesize images using neural features extracted via convolutional neural networks or transformers, and find that it only contributes marginally to robustness. Finally, we reveal that standard data augmentation techniques, which can significantly improve the robustness of recognition models, do not help the robustness of NeRF-based models. We hope that our findings will attract more researchers to study the robustness of NeRF-based approaches and help to improve their performance in the real world.



### FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.04467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04467v2)
- **Published**: 2023-01-10 17:51:55+00:00
- **Updated**: 2023-03-24 09:18:39+00:00
- **Authors**: Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: The transformation of features from 2D perspective space to 3D space is essential to multi-view 3D object detection. Recent approaches mainly focus on the design of view transformation, either pixel-wisely lifting perspective view features into 3D space with estimated depth or grid-wisely constructing BEV features via 3D projection, treating all pixels or grids equally. However, choosing what to transform is also important but has rarely been discussed before. The pixels of a moving car are more informative than the pixels of the sky. To fully utilize the information contained in images, the view transformation should be able to adapt to different image regions according to their contents. In this paper, we propose a novel framework named FrustumFormer, which pays more attention to the features in instance regions via adaptive instance-aware resampling. Specifically, the model obtains instance frustums on the bird's eye view by leveraging image view object proposals. An adaptive occupancy mask within the instance frustum is learned to refine the instance location. Moreover, the temporal frustum intersection could further reduce the localization uncertainty of objects. Comprehensive experiments on the nuScenes dataset demonstrate the effectiveness of FrustumFormer, and we achieve a new state-of-the-art performance on the benchmark. Codes and models will be made available at https://github.com/Robertwyq/Frustum.



### Neural Radiance Field Codebooks
- **Arxiv ID**: http://arxiv.org/abs/2301.04101v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04101v2)
- **Published**: 2023-01-10 18:03:48+00:00
- **Updated**: 2023-04-30 09:25:38+00:00
- **Authors**: Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: 19 pages, 8 figures, 9 tables
- **Journal**: International Conference on Learning Representations 2023
- **Summary**: Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% relative improvement). Finally, we show that NRC improves on the task of depth ordering by 5.5% accuracy in THOR.



### Vision Transformers Are Good Mask Auto-Labelers
- **Arxiv ID**: http://arxiv.org/abs/2301.03992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.03992v1)
- **Published**: 2023-01-10 18:59:00+00:00
- **Updated**: 2023-01-10 18:59:00+00:00
- **Authors**: Shiyi Lan, Xitong Yang, Zhiding Yu, Zuxuan Wu, Jose M. Alvarez, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Mask Auto-Labeler (MAL), a high-quality Transformer-based mask auto-labeling framework for instance segmentation using only box annotations. MAL takes box-cropped images as inputs and conditionally generates their mask pseudo-labels.We show that Vision Transformers are good mask auto-labelers. Our method significantly reduces the gap between auto-labeling and human annotation regarding mask quality. Instance segmentation models trained using the MAL-generated masks can nearly match the performance of their fully-supervised counterparts, retaining up to 97.4\% performance of fully supervised models. The best model achieves 44.1\% mAP on COCO instance segmentation (test-dev 2017), outperforming state-of-the-art box-supervised methods by significant margins. Qualitative results indicate that masks produced by MAL are, in some cases, even better than human annotations.



### Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines
- **Arxiv ID**: http://arxiv.org/abs/2301.04168v2
- **DOI**: 10.3847/1538-4357/accf84
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04168v2)
- **Published**: 2023-01-10 19:00:12+00:00
- **Updated**: 2023-04-24 14:57:12+00:00
- **Authors**: Alexandre Adam, Laurence Perreault-Levasseur, Yashar Hezaveh, Max Welling
- **Comment**: 13+7 pages, 13 figures; Accepted by The Astrophysical Journal. arXiv
  admin note: text overlap with arXiv:2207.01073
- **Journal**: None
- **Summary**: Modeling strong gravitational lenses in order to quantify the distortions in the images of background sources and to reconstruct the mass density in the foreground lenses has been a difficult computational challenge. As the quality of gravitational lens images increases, the task of fully exploiting the information they contain becomes computationally and algorithmically more difficult. In this work, we use a neural network based on the Recurrent Inference Machine (RIM) to simultaneously reconstruct an undistorted image of the background source and the lens mass density distribution as pixelated maps. The method iteratively reconstructs the model parameters (the image of the source and a pixelated density map) by learning the process of optimizing the likelihood given the data using the physical model (a ray-tracing simulation), regularized by a prior implicitly learned by the neural network through its training data. When compared to more traditional parametric models, the proposed method is significantly more expressive and can reconstruct complex mass distributions, which we demonstrate by using realistic lensing galaxies taken from the IllustrisTNG cosmological hydrodynamic simulation.



### Deep Learning based Multi-Label Image Classification of Protest Activities
- **Arxiv ID**: http://arxiv.org/abs/2301.04212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04212v1)
- **Published**: 2023-01-10 21:25:53+00:00
- **Updated**: 2023-01-10 21:25:53+00:00
- **Authors**: Yingzhou Lu, Kosaku Sato, Jialu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: With the rise of internet technology amidst increasing rates of urbanization, sharing information has never been easier thanks to globally-adopted platforms for digital communication. The resulting output of massive amounts of user-generated data can be used to enhance our understanding of significant societal issues particularly for urbanizing areas. In order to better analyze protest behavior, we enhanced the GSR dataset and manually labeled all the images. We used deep learning techniques to analyze social media data to detect social unrest through image classification, which performed good in predict multi-attributes, then also used map visualization to display protest behaviors across the country.



### Leveraging Diffusion For Strong and High Quality Face Morphing Attacks
- **Arxiv ID**: http://arxiv.org/abs/2301.04218v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04218v3)
- **Published**: 2023-01-10 21:50:26+00:00
- **Updated**: 2023-06-08 17:13:55+00:00
- **Authors**: Zander Blasingame, Chen Liu
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Face morphing attacks seek to deceive a Face Recognition (FR) system by presenting a morphed image consisting of the biometric qualities from two different identities with the aim of triggering a false acceptance with one of the two identities, thereby presenting a significant threat to biometric systems. The success of a morphing attack is dependent on the ability of the morphed image to represent the biometric characteristics of both identities that were used to create the image. We present a novel morphing attack that uses a Diffusion-based architecture to improve the visual fidelity of the image and the ability of the morphing attack to represent characteristics from both identities. We demonstrate the effectiveness of the proposed attack by evaluating its visual fidelity via the Frechet Inception Distance (FID). Also, extensive experiments are conducted to measure the vulnerability of FR systems to the proposed attack. The ability of a morphing attack detector to detect the proposed attack is measured and compared against two state-of-the-art GAN-based morphing attacks along with two Landmark-based attacks. Additionally, a novel metric to measure the relative strength between different morphing attacks is introduced and evaluated.



### Explaining Deep Models through Forgettable Learning Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2301.04221v1
- **DOI**: 10.1109/ICIP42928.2021.9506644
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04221v1)
- **Published**: 2023-01-10 21:59:20+00:00
- **Updated**: 2023-01-10 21:59:20+00:00
- **Authors**: Ryan Benkert, Oluwaseun Joseph Aribido, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: Even though deep neural networks have shown tremendous success in countless applications, explaining model behaviour or predictions is an open research problem. In this paper, we address this issue by employing a simple yet effective method by analysing the learning dynamics of deep neural networks in semantic segmentation tasks. Specifically, we visualize the learning behaviour during training by tracking how often samples are learned and forgotten in subsequent training epochs. This further allows us to derive important information about the proximity to the class decision boundary and identify regions that pose a particular challenge to the model. Inspired by this phenomenon, we present a novel segmentation method that actively uses this information to alter the data representation within the model by increasing the variety of difficult regions. Finally, we show that our method consistently reduces the amount of regions that are forgotten frequently. We further evaluate our method in light of the segmentation performance.



### Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images
- **Arxiv ID**: http://arxiv.org/abs/2301.04224v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04224v2)
- **Published**: 2023-01-10 22:05:35+00:00
- **Updated**: 2023-04-09 21:30:05+00:00
- **Authors**: Xindi Wu, KwunFung Lau, Francesco Ferroni, Aljoša Ošep, Deva Ramanan
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Self-driving vehicles rely on urban street maps for autonomous navigation. In this paper, we introduce Pix2Map, a method for inferring urban street map topology directly from ego-view images, as needed to continually update and expand existing maps. This is a challenging task, as we need to infer a complex urban road topology directly from raw image data. The main insight of this paper is that this problem can be posed as cross-modal retrieval by learning a joint, cross-modal embedding space for images and existing maps, represented as discrete graphs that encode the topological layout of the visual surroundings. We conduct our experimental evaluation using the Argoverse dataset and show that it is indeed possible to accurately retrieve street maps corresponding to both seen and unseen roads solely from image data. Moreover, we show that our retrieved maps can be used to update or expand existing maps and even show proof-of-concept results for visual localization and image retrieval from spatial graphs.



### Adapting to Skew: Imputing Spatiotemporal Urban Data with 3D Partial Convolutions and Biased Masking
- **Arxiv ID**: http://arxiv.org/abs/2301.04233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2301.04233v1)
- **Published**: 2023-01-10 22:44:22+00:00
- **Updated**: 2023-01-10 22:44:22+00:00
- **Authors**: Bin Han, Bill Howe
- **Comment**: None
- **Journal**: None
- **Summary**: We adapt image inpainting techniques to impute large, irregular missing regions in urban settings characterized by sparsity, variance in both space and time, and anomalous events. Missing regions in urban data can be caused by sensor or software failures, data quality issues, interference from weather events, incomplete data collection, or varying data use regulations; any missing data can render the entire dataset unusable for downstream applications. To ensure coverage and utility, we adapt computer vision techniques for image inpainting to operate on 3D histograms (2D space + 1D time) commonly used for data exchange in urban settings.   Adapting these techniques to the spatiotemporal setting requires handling skew: urban data tend to follow population density patterns (small dense regions surrounded by large sparse areas); these patterns can dominate the learning process and fool the model into ignoring local or transient effects. To combat skew, we 1) train simultaneously in space and time, and 2) focus attention on dense regions by biasing the masks used for training to the skew in the data. We evaluate the core model and these two extensions using the NYC taxi data and the NYC bikeshare data, simulating different conditions for missing data. We show that the core model is effective qualitatively and quantitatively, and that biased masking during training reduces error in a variety of scenarios. We also articulate a tradeoff in varying the number of timesteps per training sample: too few timesteps and the model ignores transient events; too many timesteps and the model is slow to train with limited performance gain.



### Robust Human Identity Anonymization using Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2301.04243v1
- **DOI**: 10.1109/CASE49997.2022.9926568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04243v1)
- **Published**: 2023-01-10 23:35:42+00:00
- **Updated**: 2023-01-10 23:35:42+00:00
- **Authors**: Hengyuan Zhang, Jing-Yan Liao, David Paz, Henrik I. Christensen
- **Comment**: Source code will be available at
  https://github.com/AutonomousVehicleLaboratory/anonymization
- **Journal**: 2022 IEEE 18th International Conference on Automation Science and
  Engineering (CASE), Mexico City, Mexico, 2022, pp. 619-626
- **Summary**: Many outdoor autonomous mobile platforms require more human identity anonymized data to power their data-driven algorithms. The human identity anonymization should be robust so that less manual intervention is needed, which remains a challenge for current face detection and anonymization systems. In this paper, we propose to use the skeleton generated from the state-of-the-art human pose estimation model to help localize human heads. We develop criteria to evaluate the performance and compare it with the face detection approach. We demonstrate that the proposed algorithm can reduce missed faces and thus better protect the identity information for the pedestrians. We also develop a confidence-based fusion method to further improve the performance.



