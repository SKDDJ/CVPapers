# Arxiv Papers in cs.CV on 2023-01-31
### Continuous Spatiotemporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.13338v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13338v2)
- **Published**: 2023-01-31 00:06:56+00:00
- **Updated**: 2023-07-28 21:38:47+00:00
- **Authors**: Antonio H. de O. Fonseca, Emanuele Zappala, Josue Ortega Caro, David van Dijk
- **Comment**: Updated version, after reviews
- **Journal**: None
- **Summary**: Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.



### Few-Shot Image-to-Semantics Translation for Policy Transfer in Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.13343v1
- **DOI**: 10.1109/IJCNN55064.2022.9892464
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13343v1)
- **Published**: 2023-01-31 00:28:18+00:00
- **Updated**: 2023-01-31 00:28:18+00:00
- **Authors**: Rei Sato, Kazuto Fukuchi, Jun Sakuma, Youhei Akimoto
- **Comment**: The 2022 International Joint Conference on Neural Networks
  (IJCNN2022)
- **Journal**: None
- **Summary**: We investigate policy transfer using image-to-semantics translation to mitigate learning difficulties in vision-based robotics control agents. This problem assumes two environments: a simulator environment with semantics, that is, low-dimensional and essential information, as the state space, and a real-world environment with images as the state space. By learning mapping from images to semantics, we can transfer a policy, pre-trained in the simulator, to the real world, thereby eliminating real-world on-policy agent interactions to learn, which are costly and risky. In addition, using image-to-semantics mapping is advantageous in terms of the computational efficiency to train the policy and the interpretability of the obtained policy over other types of sim-to-real transfer strategies. To tackle the main difficulty in learning image-to-semantics mapping, namely the human annotation cost for producing a training dataset, we propose two techniques: pair augmentation with the transition function in the simulator environment and active learning. We observed a reduction in the annotation cost without a decline in the performance of the transfer, and the proposed approach outperformed the existing approach without annotation.



### Inference Time Evidences of Adversarial Attacks for Forensic on Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.13356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13356v1)
- **Published**: 2023-01-31 01:17:03+00:00
- **Updated**: 2023-01-31 01:17:03+00:00
- **Authors**: Hugo Lemarchant, Liangzi Li, Yiming Qian, Yuta Nakashima, Hajime Nagahara
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) are becoming a very popular paradigm for vision tasks as they achieve state-of-the-art performance on image classification. However, although early works implied that this network structure had increased robustness against adversarial attacks, some works argue ViTs are still vulnerable. This paper presents our first attempt toward detecting adversarial attacks during inference time using the network's input and outputs as well as latent features. We design four quantifications (or derivatives) of input, output, and latent vectors of ViT-based models that provide a signature of the inference, which could be beneficial for the attack detection, and empirically study their behavior over clean samples and adversarial samples. The results demonstrate that the quantifications from input (images) and output (posterior probabilities) are promising for distinguishing clean and adversarial samples, while latent vectors offer less discriminative power, though they give some insights on how adversarial perturbations work.



### Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2301.13358v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13358v1)
- **Published**: 2023-01-31 01:24:34+00:00
- **Updated**: 2023-01-31 01:24:34+00:00
- **Authors**: Wenchao Du, Hu Chen, Yi Zhang, H. Yang
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Image denoising is a typical ill-posed problem due to complex degradation. Leading methods based on normalizing flows have tried to solve this problem with an invertible transformation instead of a deterministic mapping. However, the implicit bijective mapping is not explored well. Inspired by a latent observation that noise tends to appear in the high-frequency part of the image, we propose a fully invertible denoising method that injects the idea of disentangled learning into a general invertible neural network to split noise from the high-frequency part. More specifically, we decompose the noisy image into clean low-frequency and hybrid high-frequency parts with an invertible transformation and then disentangle case-specific noise and high-frequency components in the latent space. In this way, denoising is made tractable by inversely merging noiseless low and high-frequency parts. Furthermore, we construct a flexible hierarchical disentangling framework, which aims to decompose most of the low-frequency image information while disentangling noise from the high-frequency part in a coarse-to-fine manner. Extensive experiments on real image denoising, JPEG compressed artifact removal, and medical low-dose CT image restoration have demonstrated that the proposed method achieves competing performance on both quantitative metrics and visual quality, with significantly less computational cost.



### IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2301.13359v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13359v2)
- **Published**: 2023-01-31 01:24:45+00:00
- **Updated**: 2023-07-10 02:21:41+00:00
- **Authors**: Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, Yaochu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our extensive experiments (17,017 in total) provide in-depth insights for IAD algorithm redesign or selection under the IM setting. Next, the proposed benchmark IM-IAD gives challenges as well as directions for the future. To foster reproducibility and accessibility, the source code of IM-IAD is uploaded on the website, https://github.com/M-3LAB/IM-IAD.



### Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN)
- **Arxiv ID**: http://arxiv.org/abs/2301.13360v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13360v1)
- **Published**: 2023-01-31 01:26:17+00:00
- **Updated**: 2023-01-31 01:26:17+00:00
- **Authors**: Ayman Ali, Ekkasit Pinyoanuntapong, Pu Wang, Mohsen Dorodchi
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been a remarkable increase in the interest towards skeleton-based action recognition within the research community, owing to its various advantageous features, including computational efficiency, representative features, and illumination invariance. Despite this, researchers continue to explore and investigate the most optimal way to represent human actions through skeleton representation and the extracted features. As a result, the growth and availability of human action recognition datasets have risen substantially. In addition, deep learning-based algorithms have gained widespread popularity due to the remarkable advancements in various computer vision tasks. Most state-of-the-art contributions in skeleton-based action recognition incorporate a Graph Neural Network (GCN) architecture for representing the human body and extracting features. Our research demonstrates that Convolutional Neural Networks (CNNs) can attain comparable results to GCN, provided that the proper training techniques, augmentations, and optimizers are applied. Our approach has been rigorously validated, and we have achieved a score of 95% on the NTU-60 dataset



### Iterative Loop Method Combining Active and Semi-Supervised Learning for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.13361v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13361v4)
- **Published**: 2023-01-31 01:31:43+00:00
- **Updated**: 2023-03-14 01:28:35+00:00
- **Authors**: Licong Guan, Xue Yuan
- **Comment**: 10 pages,5 figures
- **Journal**: None
- **Summary**: Semantic segmentation is an important technique for environment perception in intelligent transportation systems. With the rapid development of convolutional neural networks (CNNs), road scene analysis can usually achieve satisfactory results in the source domain. However, guaranteeing good generalization to different target domain scenarios remains a significant challenge. Recently, semi-supervised learning and active learning have been proposed to alleviate this problem. Semisupervised learning can improve model accuracy with massive unlabeled data, but some pseudo labels containing noise would be generated with limited or imbalanced training data. And there will be suboptimal models if human guidance is absent. Active learning can select more effective data to intervene, while the model accuracy can not be improved because the massive unlabeled data are not used. And the probability of querying sub-optimal samples will increase when the domain difference is too large, increasing annotation cost. This paper proposes an iterative loop method combining active and semisupervised learning for domain adaptive semantic segmentation. The method first uses semi-supervised to learn massive unlabeled data to improve model accuracy and provide more accurate selection models for active learning. Secondly, combined with the predictive uncertainty sample selection strategy of active learning, manual intervention is used to correct the pseudo-labels. Finally, flexible iterative loops achieve the best performance with minimal labeling cost. Extensive experiments show that our method establishes state-of-the-art performance on tasks of GTAV to Cityscapes, SYNTHIA to Cityscapes, improving by 4.9% mIoU and 5.2% mIoU, compared to the previous best method, respectively.



### ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2302.10307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2302.10307v1)
- **Published**: 2023-01-31 01:57:52+00:00
- **Updated**: 2023-01-31 01:57:52+00:00
- **Authors**: Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guangrun Wang, Jianzhuang Liu, Xiaojun Chang, Xiaodan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-\textbf{View} \textbf{Co}nsistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders. The text-to-views consistency benefits the dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9\%, 1.6\%, and 2.4\% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively.



### CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects
- **Arxiv ID**: http://arxiv.org/abs/2301.13366v1
- **DOI**: 10.1117/1.JMI.10.1.014005
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13366v1)
- **Published**: 2023-01-31 02:12:33+00:00
- **Updated**: 2023-01-31 02:12:33+00:00
- **Authors**: Ange Lou, Shuyue Guan, Murray Loew
- **Comment**: arXiv admin note: text overlap with arXiv:2108.07368
- **Journal**: Journal of Medical Imaging 10(1), 014005 (18 February 2023)
- **Summary**: Segmenting medical images accurately and reliably is important for disease diagnosis and treatment. It is a challenging task because of the wide variety of objects' sizes, shapes, and scanning modalities. Recently, many convolutional neural networks (CNN) have been designed for segmentation tasks and achieved great success. Few studies, however, have fully considered the sizes of objects, and thus most demonstrate poor performance for small objects segmentation. This can have a significant impact on the early detection of diseases. This paper proposes a Context Axial Reverse Attention Network (CaraNet) to improve the segmentation performance on small objects compared with several recent state-of-the-art models. CaraNet applies axial reserve attention (ARA) and channel-wise feature pyramid (CFP) module to dig feature information of small medical object. And we evaluate our model by six different measurement metrics. We test our CaraNet on brain tumor (BraTS 2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and ETIS-LaribPolypDB) segmentation datasets. Our CaraNet achieves the top-rank mean Dice segmentation accuracy, and results show a distinct advantage of CaraNet in the segmentation of small medical objects.



### Demystifying Disagreement-on-the-Line in High Dimensions
- **Arxiv ID**: http://arxiv.org/abs/2301.13371v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13371v2)
- **Published**: 2023-01-31 02:31:18+00:00
- **Updated**: 2023-03-01 18:32:04+00:00
- **Authors**: Donghwan Lee, Behrad Moniri, Xinmeng Huang, Edgar Dobriban, Hamed Hassani
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluating the performance of machine learning models under distribution shift is challenging, especially when we only have unlabeled data from the shifted (target) domain, along with labeled data from the original (source) domain. Recent work suggests that the notion of disagreement, the degree to which two models trained with different randomness differ on the same input, is a key to tackle this problem. Experimentally, disagreement and prediction error have been shown to be strongly connected, which has been used to estimate model performance. Experiments have led to the discovery of the disagreement-on-the-line phenomenon, whereby the classification error under the target domain is often a linear function of the classification error under the source domain; and whenever this property holds, disagreement under the source and target domain follow the same linear relation. In this work, we develop a theoretical foundation for analyzing disagreement in high-dimensional random features regression; and study under what conditions the disagreement-on-the-line phenomenon occurs in our setting. Experiments on CIFAR-10-C, Tiny ImageNet-C, and Camelyon17 are consistent with our theory and support the universality of the theoretical findings.



### Quantized Neural Networks for Low-Precision Accumulation with Guaranteed Overflow Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2301.13376v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13376v1)
- **Published**: 2023-01-31 02:46:57+00:00
- **Updated**: 2023-01-31 02:46:57+00:00
- **Authors**: Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a quantization-aware training algorithm that guarantees avoiding numerical overflow when reducing the precision of accumulators during inference. We leverage weight normalization as a means of constraining parameters during training using accumulator bit width bounds that we derive. We evaluate our algorithm across multiple quantized models that we train for different tasks, showing that our approach can reduce the precision of accumulators while maintaining model accuracy with respect to a floating-point baseline. We then show that this reduction translates to increased design efficiency for custom FPGA-based accelerators. Finally, we show that our algorithm not only constrains weights to fit into an accumulator of user-defined bit width, but also increases the sparsity and compressibility of the resulting weights. Across all of our benchmark models trained with 8-bit weights and activations, we observe that constraining the hidden layers of quantized neural networks to fit into 16-bit accumulators yields an average 98.2% sparsity with an estimated compression rate of 46.5x all while maintaining 99.2% of the floating-point performance.



### When Source-Free Domain Adaptation Meets Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2301.13381v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13381v2)
- **Published**: 2023-01-31 03:06:47+00:00
- **Updated**: 2023-02-24 15:56:45+00:00
- **Authors**: Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, A. Ian McLeod, Boyu Wang
- **Comment**: ICLR 2023 camera-ready
- **Journal**: None
- **Summary**: Recent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA.



### GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.13384v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13384v3)
- **Published**: 2023-01-31 03:21:08+00:00
- **Updated**: 2023-05-05 18:01:18+00:00
- **Authors**: Ekkasit Pinyoanuntapong, Ayman Ali, Kalvik Jakkala, Pu Wang, Minwoo Lee, Qucheng Peng, Chen Chen, Zhi Sun
- **Comment**: Submitted to IEEE MASS 2023
- **Journal**: None
- **Summary**: mmWave radar-based gait recognition is a novel user identification method that captures human gait biometrics from mmWave radar return signals. This technology offers privacy protection and is resilient to weather and lighting conditions. However, its generalization performance is yet unknown and limits its practical deployment. To address this problem, in this paper, a non-synthetic dataset is collected and analyzed to reveal the presence of spatial and temporal domain shifts in mmWave gait biometric data, which significantly impacts identification accuracy. To mitigate this issue, a novel self-aligned domain adaptation method called GaitSADA is proposed. GaitSADA improves system generalization performance by using a two-stage semi-supervised model training approach. The first stage employs semi-supervised contrastive learning to learn a compact gait representation from both source and target domain data, aligning source-target domain distributions implicitly. The second stage uses semi-supervised consistency training with centroid alignment to further close source-target domain gap by pseudo-labelling the target-domain samples, clustering together the samples belonging to the same class but from different domains, and pushing the class centroid close to the weight vector of each class. Experiments show that GaitSADA outperforms representative domain adaptation methods with an improvement ranging from 15.41\% to 26.32\% on average accuracy in low data regimes. Code and dataset will be available at https://exitudio.github.io/GaitSADA



### Fisheye traffic data set of point center markers
- **Arxiv ID**: http://arxiv.org/abs/2301.13385v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13385v1)
- **Published**: 2023-01-31 03:31:43+00:00
- **Updated**: 2023-01-31 03:31:43+00:00
- **Authors**: Chung-I Huang, Wei-Yu Chen, Wei Jan Ko, Jih-Sheng Chang, Chen-Kai Sun, Hui Hung Yu, Fang-Pang Lin
- **Comment**: https://youtu.be/sjUQ-Ayxxtk
- **Journal**: None
- **Summary**: This study presents an open data-market platform and a dataset containing 160,000 markers and 18,000 images. We hope that this dataset will bring more new data value and applications In this paper, we introduce the format and usage of the dataset, and we show a demonstration of deep learning vehicle detection trained by this dataset.



### ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2301.13402v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13402v1)
- **Published**: 2023-01-31 04:38:42+00:00
- **Updated**: 2023-01-31 04:38:42+00:00
- **Authors**: Bingchuan Li, Tianxiang Ma, Peng Zhang, Miao Hua, Wei Liu, Qian He, Zili Yi
- **Comment**: None
- **Journal**: None
- **Summary**: The StyleGAN family succeed in high-fidelity image generation and allow for flexible and plausible editing of generated images by manipulating the semantic-rich latent style space.However, projecting a real image into its latent space encounters an inherent trade-off between inversion quality and editability. Existing encoder-based or optimization-based StyleGAN inversion methods attempt to mitigate the trade-off but see limited performance. To fundamentally resolve this problem, we propose a novel two-phase framework by designating two separate networks to tackle editing and reconstruction respectively, instead of balancing the two. Specifically, in Phase I, a W-space-oriented StyleGAN inversion network is trained and used to perform image inversion and editing, which assures the editability but sacrifices reconstruction quality. In Phase II, a carefully designed rectifying network is utilized to rectify the inversion errors and perform ideal reconstruction. Experimental results show that our approach yields near-perfect reconstructions without sacrificing the editability, thus allowing accurate manipulation of real images. Further, we evaluate the performance of our rectifying network, and see great generalizability towards unseen manipulation types and out-of-domain images.



### A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose
- **Arxiv ID**: http://arxiv.org/abs/2301.13403v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13403v1)
- **Published**: 2023-01-31 04:42:47+00:00
- **Updated**: 2023-01-31 04:42:47+00:00
- **Authors**: Ayman Ali, Ekkasit Pinyoanuntapong, Pu Wang, Mohsen Dorodchi
- **Comment**: None
- **Journal**: None
- **Summary**: In this research, we address the challenge faced by existing deep learning-based human mesh reconstruction methods in balancing accuracy and computational efficiency. These methods typically prioritize accuracy, resulting in large network sizes and excessive computational complexity, which may hinder their practical application in real-world scenarios, such as virtual reality systems. To address this issue, we introduce a modular multi-stage lightweight graph-based transformer network for human pose and shape estimation from 2D human pose, a pose-based human mesh reconstruction approach that prioritizes computational efficiency without sacrificing reconstruction accuracy. Our method consists of a 2D-to-3D lifter module that utilizes graph transformers to analyze structured and implicit joint correlations in 2D human poses, and a mesh regression module that combines the extracted pose features with a mesh template to produce the final human mesh parameters.



### Few-Shot Object Detection via Variational Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2301.13411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13411v1)
- **Published**: 2023-01-31 04:58:21+00:00
- **Updated**: 2023-01-31 04:58:21+00:00
- **Authors**: Jiaming Han, Yuqiang Ren, Jian Ding, Ke Yan, Gui-Song Xia
- **Comment**: Accepted by AAAI2023
- **Journal**: None
- **Summary**: As few-shot object detectors are often trained with abundant base samples and fine-tuned on few-shot novel examples,the learned models are usually biased to base classes and sensitive to the variance of novel examples. To address this issue, we propose a meta-learning framework with two novel feature aggregation schemes. More precisely, we first present a Class-Agnostic Aggregation (CAA) method, where the query and support features can be aggregated regardless of their categories. The interactions between different classes encourage class-agnostic representations and reduce confusion between base and novel classes. Based on the CAA, we then propose a Variational Feature Aggregation (VFA) method, which encodes support examples into class-level support features for robust feature aggregation. We use a variational autoencoder to estimate class distributions and sample variational features from distributions that are more robust to the variance of support examples. Besides, we decouple classification and regression tasks so that VFA is performed on the classification branch without affecting object localization. Extensive experiments on PASCAL VOC and COCO demonstrate that our method significantly outperforms a strong baseline (up to 16\%) and previous state-of-the-art methods (4\% in average). Code will be available at: \url{https://github.com/csuhan/VFA}



### Structure Flow-Guided Network for Real Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2301.13416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13416v1)
- **Published**: 2023-01-31 05:13:55+00:00
- **Updated**: 2023-01-31 05:13:55+00:00
- **Authors**: Jiayi Yuan, Haobo Jiang, Xiang Li, Jianjun Qian, Jun Li, Jian Yang
- **Comment**: Accepted by AAAI-2023
- **Journal**: None
- **Summary**: Real depth super-resolution (DSR), unlike synthetic settings, is a challenging task due to the structural distortion and the edge noise caused by the natural degradation in real-world low-resolution (LR) depth maps. These defeats result in significant structure inconsistency between the depth map and the RGB guidance, which potentially confuses the RGB-structure guidance and thereby degrades the DSR quality. In this paper, we propose a novel structure flow-guided DSR framework, where a cross-modality flow map is learned to guide the RGB-structure information transferring for precise depth upsampling. Specifically, our framework consists of a cross-modality flow-guided upsampling network (CFUNet) and a flow-enhanced pyramid edge attention network (PEANet). CFUNet contains a trilateral self-attention module combining both the geometric and semantic correlations for reliable cross-modality flow learning. Then, the learned flow maps are combined with the grid-sampling mechanism for coarse high-resolution (HR) depth prediction. PEANet targets at integrating the learned flow map as the edge attention into a pyramid network to hierarchically learn the edge-focused guidance feature for depth edge refinement. Extensive experiments on real and synthetic DSR datasets verify that our approach achieves excellent performance compared to state-of-the-art methods.



### BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations
- **Arxiv ID**: http://arxiv.org/abs/2301.13418v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13418v2)
- **Published**: 2023-01-31 05:14:49+00:00
- **Updated**: 2023-02-02 08:50:36+00:00
- **Authors**: Yuanhong Chen, Yuyuan Liu, Chong Wang, Michael Elliott, Chun Fung Kwok, Carlos Pena-Solorzano, Yu Tian, Fengbei Liu, Helen Frazer, Davis J. McCarthy, Gustavo Carneiro
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant breast lesion detection with incomplete annotations. To address this problem, our new method comprises two stages, namely: 1) pre-training a multi-view mammogram classifier with weak supervision from the whole dataset, and 2) extending the trained classifier to become a multi-view detector that is trained with semi-supervised student-teacher learning, where the training set contains fully and weakly-annotated mammograms. We provide extensive detection results on two real-world screening mammogram datasets containing incomplete annotations, and show that our proposed approach achieves state-of-the-art results in the detection of malignant breast lesions with incomplete annotations.



### Recurrent Structure Attention Guidance for Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2301.13419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13419v1)
- **Published**: 2023-01-31 05:18:34+00:00
- **Updated**: 2023-01-31 05:18:34+00:00
- **Authors**: Jiayi Yuan, Haobo Jiang, Xiang Li, Jianjun Qian, Jun Li, Jian Yang
- **Comment**: Accepted by AAAI-2023
- **Journal**: None
- **Summary**: Image guidance is an effective strategy for depth super-resolution. Generally, most existing methods employ hand-crafted operators to decompose the high-frequency (HF) and low-frequency (LF) ingredients from low-resolution depth maps and guide the HF ingredients by directly concatenating them with image features. However, the hand-designed operators usually cause inferior HF maps (e.g., distorted or structurally missing) due to the diverse appearance of complex depth maps. Moreover, the direct concatenation often results in weak guidance because not all image features have a positive effect on the HF maps. In this paper, we develop a recurrent structure attention guided (RSAG) framework, consisting of two important parts. First, we introduce a deep contrastive network with multi-scale filters for adaptive frequency-domain separation, which adopts contrastive networks from large filters to small ones to calculate the pixel contrasts for adaptive high-quality HF predictions. Second, instead of the coarse concatenation guidance, we propose a recurrent structure attention block, which iteratively utilizes the latest depth estimation and the image features to jointly select clear patterns and boundaries, aiming at providing refined guidance for accurate depth recovery. In addition, we fuse the features of HF maps to enhance the edge structures in the decomposed LF maps. Extensive experiments show that our approach obtains superior performance compared with state-of-the-art depth super-resolution methods.



### Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2301.13422v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2301.13422v2)
- **Published**: 2023-01-31 05:32:34+00:00
- **Updated**: 2023-04-28 11:14:33+00:00
- **Authors**: Jingtao Li, Xinyu Wang, Hengwei Zhao, Shaoyu Wang, Yanfei Zhong
- **Comment**: Accepted in AAAI2023
- **Journal**: None
- **Summary**: Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery is aimed at segmenting anomaly patterns of the earth deviating from normal patterns, which plays an important role in various Earth vision applications. However, it is a challenging task due to the complex distribution and the irregular shapes of objects, and the lack of abnormal samples. To tackle these problems, an anomaly segmentation model based on pixel descriptors (ASD) is proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class classification is introduced for anomaly segmentation in the feature space with discriminative pixel descriptors. The ASD model incorporates the data argument for generating virtual ab-normal samples, which can force the pixel descriptors to be compact for normal data and meanwhile to be diverse to avoid the model collapse problems when only positive samples participated in the training. In addition, the ASD introduced a multi-level and multi-scale feature extraction strategy for learning the low-level and semantic information to make the pixel descriptors feature-rich. The proposed ASD model was validated using four HSR datasets and compared with the recent state-of-the-art models, showing its potential value in Earth vision applications.



### Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2301.13428v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13428v3)
- **Published**: 2023-01-31 05:51:05+00:00
- **Updated**: 2023-03-18 03:02:37+00:00
- **Authors**: Yuqi Chen, Xiangbin Zhu, Yonggang Li, Yingjian Li, Haojie Fang
- **Comment**: Journal articles
- **Journal**: None
- **Summary**: Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. The results demonstrate the superiority of our methods compared with other state-of-the-art works.



### GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.13430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13430v1)
- **Published**: 2023-01-31 05:56:06+00:00
- **Updated**: 2023-01-31 05:56:06+00:00
- **Authors**: Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, JinZheng He, Zhou Zhao
- **Comment**: Accepted by ICLR2023. Project page: https://geneface.github.io/
- **Journal**: None
- **Summary**: Generating photo-realistic video portrait with arbitrary speech audio is a crucial problem in film-making and virtual reality. Recently, several works explore the usage of neural radiance field in this task to improve 3D realness and image fidelity. However, the generalizability of previous NeRF-based methods to out-of-domain audio is limited by the small scale of training data. In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based talking face generation method, which can generate natural results corresponding to various out-of-domain audio. Specifically, we learn a variaitional motion generator on a large lip-reading corpus, and introduce a domain adaptative post-net to calibrate the result. Moreover, we learn a NeRF-based renderer conditioned on the predicted facial motion. A head-aware torso-NeRF is proposed to eliminate the head-torso separation problem. Extensive experiments show that our method achieves more generalized and high-fidelity talking face generation compared to previous methods.



### Rethinking Soft Label in Label Distribution Learning Perspective
- **Arxiv ID**: http://arxiv.org/abs/2301.13444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13444v1)
- **Published**: 2023-01-31 06:47:19+00:00
- **Updated**: 2023-01-31 06:47:19+00:00
- **Authors**: Seungbum Hong, Jihun Yoon, Bogyu Park, Min-Kook Choi
- **Comment**: 11 pages main manuscript + references and 11 pages supplementary
  materials
- **Journal**: None
- **Summary**: The primary goal of training in early convolutional neural networks (CNN) is the higher generalization performance of the model. However, as the expected calibration error (ECE), which quantifies the explanatory power of model inference, was recently introduced, research on training models that can be explained is in progress. We hypothesized that a gap in supervision criteria during training and inference leads to overconfidence, and investigated that performing label distribution learning (LDL) would enhance the model calibration in CNN training. To verify this assumption, we used a simple LDL setting with recent data augmentation techniques. Based on a series of experiments, the following results are obtained: 1) State-of-the-art KD methods significantly impede model calibration. 2) Training using LDL with recent data augmentation can have excellent effects on model calibration and even in generalization performance. 3) Online LDL brings additional improvements in model calibration and accuracy with long training, especially in large-size models. Using the proposed approach, we simultaneously achieved a lower ECE and higher generalization performance for the image classification datasets CIFAR10, 100, STL10, and ImageNet. We performed several visualizations and analyses and witnessed several interesting behaviors in CNN training with the LDL.



### A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics
- **Arxiv ID**: http://arxiv.org/abs/2301.13445v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13445v1)
- **Published**: 2023-01-31 06:49:42+00:00
- **Updated**: 2023-01-31 06:49:42+00:00
- **Authors**: Naveed Akhtar
- **Comment**: Short accessible survey (9pgs)
- **Journal**: None
- **Summary**: Deep visual models have widespread applications in high-stake domains. Hence, their black-box nature is currently attracting a large interest of the research community. We present the first survey in Explainable AI that focuses on the methods and metrics for interpreting deep visual models. Covering the landmark contributions along the state-of-the-art, we not only provide a taxonomic organization of the existing techniques, but also excavate a range of evaluation metrics and collate them as measures of different properties of model explanations. Along the insightful discussion on the current trends, we also discuss the challenges and future avenues for this research direction.



### Image Guidance for Robot-Assisted Ankle Fracture Repair
- **Arxiv ID**: http://arxiv.org/abs/2303.08105v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.08105v2)
- **Published**: 2023-01-31 07:32:13+00:00
- **Updated**: 2023-03-18 08:51:28+00:00
- **Authors**: Asef Islam, Anthony Wu, Jay Mandavilli, Wojtek Zbijewski, Jeff Siewerdsen
- **Comment**: None
- **Journal**: None
- **Summary**: This project concerns developing and validating an image guidance framework for application to a robotic-assisted fibular reduction in ankle fracture surgery. The aim is to produce and demonstrate proper functioning of software for automatic determination of directions for fibular repositioning with the ultimate goal of application to a robotic reduction procedure that can reduce the time and complexity of the procedure as well as provide the benefits of reduced error in ideal final fibular position, improved syndesmosis restoration and reduced incidence of post-traumatic osteoarthritis. The focus of this product will be developing and testing the image guidance software, from the input of preoperative images through the steps of automated segmentation and registration until the output of a final transformation that can be used as instructions to a robot on how to reposition the fibula, but will not involve developing or implementing the hardware of the robot itself.



### Learning Generalized Hybrid Proximity Representation for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.13459v2
- **DOI**: 10.1109/ICTAI56018.2022.00138
- **Categories**: **cs.CV**, cs.LG, math.PR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.13459v2)
- **Published**: 2023-01-31 07:49:25+00:00
- **Updated**: 2023-04-16 03:22:41+00:00
- **Authors**: Zhiyuan Li, Anca Ralescu
- **Comment**: The paper has been accepted by the IEEE ICTAI 2022
- **Journal**: None
- **Summary**: Recently, deep metric learning techniques received attention, as the learned distance representations are useful to capture the similarity relationship among samples and further improve the performance of various of supervised or unsupervised learning tasks. We propose a novel supervised metric learning method that can learn the distance metrics in both geometric and probabilistic space for image recognition. In contrast to the previous metric learning methods which usually focus on learning the distance metrics in Euclidean space, our proposed method is able to learn better distance representation in a hybrid approach. To achieve this, we proposed a Generalized Hybrid Metric Loss (GHM-Loss) to learn the general hybrid proximity features from the image data by controlling the trade-off between geometric proximity and probabilistic proximity. To evaluate the effectiveness of our method, we first provide theoretical derivations and proofs of the proposed loss function, then we perform extensive experiments on two public datasets to show the advantage of our method compared to other state-of-the-art metric learning methods.



### CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.13473v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.13473v2)
- **Published**: 2023-01-31 08:41:18+00:00
- **Updated**: 2023-03-01 04:41:51+00:00
- **Authors**: Darshita Jain, Anima Majumder, Samrat Dutta, Swagat Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of visual feature representation learning with an aim to improve the performance of end-to-end reinforcement learning (RL) models. Specifically, a novel architecture is proposed that uses a heterogeneous loss function, called CRC loss, to learn improved visual features which can then be used for policy learning in RL. The CRC-loss function is a combination of three individual loss functions, namely, contrastive, reconstruction and consistency loss. The feature representation is learned in parallel to the policy learning while sharing the weight updates through a Siamese Twin encoder model. This encoder model is augmented with a decoder network and a feature projection network to facilitate computation of the above loss components. Through empirical analysis involving latent feature visualization, an attempt is made to provide an insight into the role played by this loss function in learning new action-dependent features and how they are linked to the complexity of the problems being solved. The proposed architecture, called CRC-RL, is shown to outperform the existing state-of-the-art methods on the challenging Deep mind control suite environments by a significant margin thereby creating a new benchmark in this field.



### Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks
- **Arxiv ID**: http://arxiv.org/abs/2301.13487v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13487v3)
- **Published**: 2023-01-31 09:12:16+00:00
- **Updated**: 2023-04-02 09:49:17+00:00
- **Authors**: Zhiyuan Cheng, James Liang, Guanhong Tao, Dongfang Liu, Xiangyu Zhang
- **Comment**: Initially accepted at ICLR2023 (Spotlight)
- **Journal**: None
- **Summary**: Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems. Traditional adversarial training method requires ground-truth labels hence cannot be directly applied to self-supervised MDE that does not have ground-truth depth. Some self-supervised model hardening techniques (e.g., contrastive learning) ignore the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using ground-truth depth. We improve adversarial robustness against physical-world attacks using L0-norm-bounded perturbation in training. We compare our method with supervised learning based and contrastive learning based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation.



### Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease
- **Arxiv ID**: http://arxiv.org/abs/2301.13504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13504v1)
- **Published**: 2023-01-31 09:44:52+00:00
- **Updated**: 2023-01-31 09:44:52+00:00
- **Authors**: Maha M. Alwuthaynani, Zahraa S. Abdallah, Raul Santos-Rodriguez
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Early diagnosis of Alzheimer's disease (AD) is essential in preventing the disease's progression. Therefore, detecting AD from neuroimaging data such as structural magnetic resonance imaging (sMRI) has been a topic of intense investigation in recent years. Deep learning has gained considerable attention in Alzheimer's detection. However, training a convolutional neural network from scratch is challenging since it demands more computational time and a significant amount of annotated data. By transferring knowledge learned from other image recognition tasks to medical image classification, transfer learning can provide a promising and effective solution. Irregularities in the dataset distribution present another difficulty. Class decomposition can tackle this issue by simplifying learning a dataset's class boundaries. Motivated by these approaches, this paper proposes a transfer learning method using class decomposition to detect Alzheimer's disease from sMRI images. We use two ImageNet-trained architectures: VGG19 and ResNet50, and an entropy-based technique to determine the most informative images. The proposed model achieved state-of-the-art performance in the Alzheimer's disease (AD) vs mild cognitive impairment (MCI) vs cognitively normal (CN) classification task with a 3\% increase in accuracy from what is reported in the literature.



### 3D Former: Monocular Scene Reconstruction with 3D SDF Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.13510v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.13510v2)
- **Published**: 2023-01-31 09:54:20+00:00
- **Updated**: 2023-03-09 10:08:19+00:00
- **Authors**: Weihao Yuan, Xiaodong Gu, Heng Li, Zilong Dong, Siyu Zhu
- **Comment**: Accepted to ICLR 2023
- **Journal**: None
- **Summary**: Monocular scene reconstruction from posed images is challenging due to the complexity of a large environment. Recent volumetric methods learn to directly predict the TSDF volume and have demonstrated promising results in this task. However, most methods focus on how to extract and fuse the 2D features to a 3D feature volume, but none of them improve the way how the 3D volume is aggregated. In this work, we propose an SDF transformer network, which replaces the role of 3D CNN for better 3D feature aggregation. To reduce the explosive computation complexity of the 3D multi-head attention, we propose a sparse window attention module, where the attention is only calculated between the non-empty voxels within a local window. Then a top-down-bottom-up 3D attention network is built for 3D feature aggregation, where a dilate-attention structure is proposed to prevent geometry degeneration, and two global modules are employed to equip with global receptive fields. The experiments on multiple datasets show that this 3D transformer network generates a more accurate and complete reconstruction, which outperforms previous methods by a large margin. Remarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness is improved by 25.3% on the ScanNet dataset. Project page: https://weihaosky.github.io/sdfformer.



### Fourier Sensitivity and Regularization of Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2301.13514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13514v1)
- **Published**: 2023-01-31 10:05:35+00:00
- **Updated**: 2023-01-31 10:05:35+00:00
- **Authors**: Kiran Krishnamachari, See-Kiong Ng, Chuan-Sheng Foo
- **Comment**: Published in TMLR, https://openreview.net/forum?id=VmTYgjYloM
- **Journal**: TMLR 2022
- **Summary**: Recent work has empirically shown that deep neural networks latch on to the Fourier statistics of training data and show increased sensitivity to Fourier-basis directions in the input. Understanding and modifying this Fourier-sensitivity of computer vision models may help improve their robustness. Hence, in this paper we study the frequency sensitivity characteristics of deep neural networks using a principled approach. We first propose a basis trick, proving that unitary transformations of the input-gradient of a function can be used to compute its gradient in the basis induced by the transformation. Using this result, we propose a general measure of any differentiable model's Fourier-sensitivity using the unitary Fourier-transform of its input-gradient. When applied to deep neural networks, we find that computer vision models are consistently sensitive to particular frequencies dependent on the dataset, training method and architecture. Based on this measure, we further propose a Fourier-regularization framework to modify the Fourier-sensitivities and frequency bias of models. Using our proposed regularizer-family, we demonstrate that deep neural networks obtain improved classification accuracy on robustness evaluations.



### Domain-Generalizable Multiple-Domain Clustering
- **Arxiv ID**: http://arxiv.org/abs/2301.13530v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13530v1)
- **Published**: 2023-01-31 10:24:50+00:00
- **Updated**: 2023-01-31 10:24:50+00:00
- **Authors**: Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Accurately clustering high-dimensional measurements is vital for adequately analyzing scientific data. Deep learning machinery has remarkably improved clustering capabilities in recent years due to its ability to extract meaningful representations. In this work, we are given unlabeled samples from multiple source domains, and we aim to learn a shared classifier that assigns the examples to various clusters. Evaluation is done by using the classifier for predicting cluster assignments in a previously unseen domain. This setting generalizes the problem of unsupervised domain generalization to the case in which no supervised learning samples are given (completely unsupervised). Towards this goal, we present an end-to-end model and evaluate its capabilities on several multi-domain image datasets. Specifically, we demonstrate that our model is more accurate than schemes that require fine-tuning using samples from the target domain or some level of supervision.



### AMD: Adaptive Masked Distillation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.13538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13538v2)
- **Published**: 2023-01-31 10:32:13+00:00
- **Updated**: 2023-02-10 13:34:01+00:00
- **Authors**: Guang Yang, Yin Tang, Jun Li, Jianhua Xu, Xili Wan
- **Comment**: None
- **Journal**: None
- **Summary**: As a general model compression paradigm, feature-based knowledge distillation allows the student model to learn expressive features from the teacher counterpart. In this paper, we mainly focus on designing an effective feature-distillation framework and propose a spatial-channel adaptive masked distillation (AMD) network for object detection. More specifically, in order to accurately reconstruct important feature regions, we first perform attention-guided feature masking on the feature map of the student network, such that we can identify the important features via spatially adaptive feature masking instead of random masking in the previous methods. In addition, we employ a simple and efficient module to allow the student network channel to be adaptive, improving its model capability in object perception and detection. In contrast to the previous methods, more crucial object-aware features can be reconstructed and learned from the proposed network, which is conducive to accurate object detection. The empirical experiments demonstrate the superiority of our method: with the help of our proposed distillation method, the student networks report 41.3%, 42.4%, and 42.7% mAP scores when RetinaNet, Cascade Mask-RCNN and RepPoints are respectively used as the teacher framework for object detection, which outperforms the previous state-of-the-art distillation methods including FGD and MGD.



### Review of methods for automatic cerebral microbleeds detection
- **Arxiv ID**: http://arxiv.org/abs/2301.13549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2301.13549v1)
- **Published**: 2023-01-31 10:59:09+00:00
- **Updated**: 2023-01-31 10:59:09+00:00
- **Authors**: Maria Ferlin, Zuzanna Klawikowska, Michał Grochowski, Małgorzata Grzywińska, Edyta Szurowska
- **Comment**: 32 pages, 6 figures, 3 tables, 174 references
- **Journal**: None
- **Summary**: Cerebral microbleeds detection is an important and challenging task. With the gaining popularity of the MRI, the ability to detect cerebral microbleeds also raises. Unfortunately, for radiologists, it is a time-consuming and laborious procedure. For this reason, various solutions to automate this process have been proposed for several years, but none of them is currently used in medical practice. In this context, the need to systematize the existing knowledge and best practices has been recognized as a factor facilitating the imminent synthesis of a real CMBs detection system practically applicable in medicine. To the best of our knowledge, all available publications regarding automatic cerebral microbleeds detection have been gathered, described, and assessed in this paper in order to distinguish the current research state and provide a starting point for future studies.



### NoiseTransfer: Image Noise Generation with Contrastive Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2301.13554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13554v1)
- **Published**: 2023-01-31 11:09:15+00:00
- **Updated**: 2023-01-31 11:09:15+00:00
- **Authors**: Seunghwan Lee, Tae Hyun Kim
- **Comment**: ACCV 2022 oral
- **Journal**: None
- **Summary**: Deep image denoising networks have achieved impressive success with the help of a considerably large number of synthetic train datasets. However, real-world denoising is a still challenging problem due to the dissimilarity between distributions of real and synthetic noisy datasets. Although several real-world noisy datasets have been presented, the number of train datasets (i.e., pairs of clean and real noisy images) is limited, and acquiring more real noise datasets is laborious and expensive. To mitigate this problem, numerous attempts to simulate real noise models using generative models have been studied. Nevertheless, previous works had to train multiple networks to handle multiple different noise distributions. By contrast, we propose a new generative model that can synthesize noisy images with multiple different noise distributions. Specifically, we adopt recent contrastive learning to learn distinguishable latent features of the noise. Moreover, our model can generate new noisy images by transferring the noise characteristics solely from a single reference noisy image. We demonstrate the accuracy and the effectiveness of our noise model for both known and unknown noise removal.



### Lidar Upsampling with Sliced Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/2301.13558v1
- **DOI**: 10.1109/LRA.2022.3214791
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.13558v1)
- **Published**: 2023-01-31 11:16:21+00:00
- **Updated**: 2023-01-31 11:16:21+00:00
- **Authors**: Artem Savkin, Yida Wang, Sebastian Wirkert, Nassir Navab, Federico Tombar
- **Comment**: None
- **Journal**: in IEEE Robotics and Automation Letters, vol. 8, no. 1, pp.
  392-399, Jan. 2023
- **Summary**: Lidar became an important component of the perception systems in autonomous driving. But challenges of training data acquisition and annotation made emphasized the role of the sensor to sensor domain adaptation. In this work, we address the problem of lidar upsampling. Learning on lidar point clouds is rather a challenging task due to their irregular and sparse structure. Here we propose a method for lidar point cloud upsampling which can reconstruct fine-grained lidar scan patterns. The key idea is to utilize edge-aware dense convolutions for both feature extraction and feature expansion. Additionally applying a more accurate Sliced Wasserstein Distance facilitates learning of the fine lidar sweep structures. This in turn enables our method to employ a one-stage upsampling paradigm without the need for coarse and fine reconstruction. We conduct several experiments to evaluate our method and demonstrate that it provides better upsampling.



### A Comprehensive Survey of Continual Learning: Theory, Method and Application
- **Arxiv ID**: http://arxiv.org/abs/2302.00487v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00487v2)
- **Published**: 2023-01-31 11:34:56+00:00
- **Updated**: 2023-06-11 00:36:51+00:00
- **Authors**: Liyuan Wang, Xingxing Zhang, Hang Su, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.



### NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.13569v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13569v2)
- **Published**: 2023-01-31 11:44:45+00:00
- **Updated**: 2023-06-25 15:09:48+00:00
- **Authors**: Jianfeng Wang, Xiaolin Hu, Thomas Lukasiewicz
- **Comment**: An extended version of our previous ICML 2022 paper arXiv:2207.01066
  with more experiments
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has been widely explored in recent years, and it is an effective way of leveraging unlabeled data to reduce the reliance on labeled data. In this work, we adjust neural processes (NPs) to the semi-supervised image classification task, resulting in a new method named NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match implicitly compares data points when making predictions, and as a result, the prediction of each unlabeled data point is affected by the labeled data points that are similar to it, which improves the quality of pseudo-labels. Secondly, NP-Match is able to estimate uncertainty that can be used as a tool for selecting unlabeled samples with reliable pseudo-labels. Compared with uncertainty-based SSL methods implemented with Monte-Carlo (MC) dropout, NP-Match estimates uncertainty with much less computational overhead, which can save time at both the training and the testing phases. We conducted extensive experiments on five public datasets under three semi-supervised image classification settings, namely, the standard semi-supervised image classification, the imbalanced semi-supervised image classification, and the multi-label semi-supervised image classification, and NP-Match outperforms state-of-the-art (SOTA) approaches or achieves competitive results on them, which shows the effectiveness of NP-Match and its potential for SSL. The codes are at https://github.com/Jianf-Wang/NP-Match



### Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022
- **Arxiv ID**: http://arxiv.org/abs/2301.13576v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.13576v1)
- **Published**: 2023-01-31 12:03:59+00:00
- **Updated**: 2023-01-31 12:03:59+00:00
- **Authors**: Pierre-Etienne Martin, Jordan Calandre, Boris Mansencal, Jenny Benois-Pineau, Renaud Péteri, Laurent Mascarilla, Julien Morlier
- **Comment**: MediaEval 2022 Workshop, Jan 2023, Bergen, Norway. arXiv admin note:
  substantial text overlap with arXiv:2112.11384
- **Journal**: None
- **Summary**: Sports video analysis is a widespread research topic. Its applications are very diverse, like events detection during a match, video summary, or fine-grained movement analysis of athletes. As part of the MediaEval 2022 benchmarking initiative, this task aims at detecting and classifying subtle movements from sport videos. We focus on recordings of table tennis matches. Conducted since 2019, this task provides a classification challenge from untrimmed videos recorded under natural conditions with known temporal boundaries for each stroke. Since 2021, the task also provides a stroke detection challenge from unannotated, untrimmed videos. This year, the training, validation, and test sets are enhanced to ensure that all strokes are represented in each dataset. The dataset is now similar to the one used in [1, 2]. This research is intended to build tools for coaches and athletes who want to further evaluate their sport performances.



### Zero3D: Semantic-Driven Multi-Category 3D Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.13591v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.13591v4)
- **Published**: 2023-01-31 12:43:54+00:00
- **Updated**: 2023-06-13 02:21:59+00:00
- **Authors**: Bo Han, Yitong Fu, Yixuan Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic-driven 3D shape generation aims to generate 3D objects conditioned on text. Previous works face problems with single-category generation, low-frequency 3D details, and requiring a large number of paired datasets for training. To tackle these challenges, we propose a multi-category conditional diffusion model. Specifically, 1) to alleviate the problem of lack of large-scale paired data, we bridge the text, 2D image and 3D shape based on the pre-trained CLIP model, and 2) to obtain the multi-category 3D shape feature, we apply the conditional flow model to generate 3D shape vector conditioned on CLIP embedding. 3) to generate multi-category 3D shape, we employ the hidden-layer diffusion model conditioned on the multi-category shape vector, which greatly reduces the training time and memory consumption.



### Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors
- **Arxiv ID**: http://arxiv.org/abs/2301.13592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.13592v1)
- **Published**: 2023-01-31 12:45:19+00:00
- **Updated**: 2023-01-31 12:45:19+00:00
- **Authors**: Di Feng, Francesco Ferroni
- **Comment**: None
- **Journal**: None
- **Summary**: Transfomer-based approaches advance the recent development of multi-camera 3D detection both in academia and industry. In a vanilla transformer architecture, queries are randomly initialised and optimised for the whole dataset, without considering the differences among input frames. In this work, we propose to leverage the predictions from an image backbone, which is often highly optimised for 2D tasks, as priors to the transformer part of a 3D detection network. The method works by (1). augmenting image feature maps with 2D priors, (2). sampling query locations via ray-casting along 2D box centroids, as well as (3). initialising query features with object-level image features. Experimental results shows that 2D priors not only help the model converge faster, but also largely improve the baseline approach by up to 12% in terms of average precision.



### Learning Data Representations with Joint Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.13622v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.13622v2)
- **Published**: 2023-01-31 13:29:19+00:00
- **Updated**: 2023-04-05 13:09:54+00:00
- **Authors**: Kamil Deja, Tomasz Trzcinski, Jakub M. Tomczak
- **Comment**: Code: https://github.com/KamilDeja/joint_diffusion
- **Journal**: None
- **Summary**: Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations.



### A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2301.13656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13656v1)
- **Published**: 2023-01-31 14:18:19+00:00
- **Updated**: 2023-01-31 14:18:19+00:00
- **Authors**: Raphael Sulzer, Loic Landrieu, Renaud Marlet, Bruno Vallet
- **Comment**: None
- **Journal**: None
- **Summary**: We survey and benchmark traditional and novel learning-based algorithms that address the problem of surface reconstruction from point clouds. Surface reconstruction from point clouds is particularly challenging when applied to real-world acquisitions, due to noise, outliers, non-uniform sampling and missing data. Traditionally, different handcrafted priors of the input points or the output surface have been proposed to make the problem more tractable. However, hyperparameter tuning for adjusting priors to different acquisition defects can be a tedious task. To this end, the deep learning community has recently addressed the surface reconstruction problem. In contrast to traditional approaches, deep surface reconstruction methods can learn priors directly from a training set of point clouds and corresponding true surfaces. In our survey, we detail how different handcrafted and learned priors affect the robustness of methods to defect-laden input and their capability to generate geometric and topologically accurate reconstructions. In our benchmark, we evaluate the reconstructions of several traditional and learning-based methods on the same grounds. We show that learning-based methods can generalize to unseen shape categories, but their training and test sets must share the same point cloud characteristics. We also provide the code and data to compete in our benchmark and to further stimulate the development of learning-based surface reconstruction https://github.com/raphaelsulzer/dsr-benchmark.



### Spyker: High-performance Library for Spiking Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.13659v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2301.13659v1)
- **Published**: 2023-01-31 14:25:03+00:00
- **Updated**: 2023-01-31 14:25:03+00:00
- **Authors**: Shahriar Rezghi Shirsavar, Mohammad-Reza A. Dehaqani
- **Comment**: 11 pages, 6 figures, 6 listings
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) have been recently brought to light due to their promising capabilities. SNNs simulate the brain with higher biological plausibility compared to previous generations of neural networks. Learning with fewer samples and consuming less power are among the key features of these networks. However, the theoretical advantages of SNNs have not been seen in practice due to the slowness of simulation tools and the impracticality of the proposed network structures. In this work, we implement a high-performance library named Spyker using C++/CUDA from scratch that outperforms its predecessor. Several SNNs are implemented in this work with different learning rules (spike-timing-dependent plasticity and reinforcement learning) using Spyker that achieve significantly better runtimes, to prove the practicality of the library in the simulation of large-scale networks. To our knowledge, no such tools have been developed to simulate large-scale spiking neural networks with high performance using a modular structure. Furthermore, a comparison of the represented stimuli extracted from Spyker to recorded electrophysiology data is performed to demonstrate the applicability of SNNs in describing the underlying neural mechanisms of the brain functions. The aim of this library is to take a significant step toward uncovering the true potential of the brain computations using SNNs.



### What Makes Good Examples for Visual In-Context Learning?
- **Arxiv ID**: http://arxiv.org/abs/2301.13670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13670v2)
- **Published**: 2023-01-31 14:40:05+00:00
- **Updated**: 2023-02-01 08:38:14+00:00
- **Authors**: Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu
- **Comment**: code and
  models:https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval
- **Journal**: None
- **Summary**: Large-scale models trained on broad data have recently become the mainstream architecture in computer vision due to their strong generalization performance. In this paper, the main focus is on an emergent ability in large vision models, known as in-context learning, which allows inference on unseen tasks by conditioning on in-context examples (a.k.a.~prompt) without updating the model parameters. This concept has been well-known in natural language processing but has only been studied very recently for large vision models. We for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples. To overcome the problem, we propose a prompt retrieval framework to automate the selection of in-context examples. Specifically, we present (1) an unsupervised prompt retrieval method based on nearest example search using an off-the-shelf model, and (2) a supervised prompt retrieval method, which trains a neural network to choose examples that directly maximize in-context learning performance. The results demonstrate that our methods can bring non-trivial improvements to visual in-context learning in comparison to the commonly-used random selection.



### Improved distinct bone segmentation in upper-body CT through multi-resolution networks
- **Arxiv ID**: http://arxiv.org/abs/2301.13674v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13674v1)
- **Published**: 2023-01-31 14:46:16+00:00
- **Updated**: 2023-01-31 14:46:16+00:00
- **Authors**: Eva Schnider, Julia Wolleb, Antal Huck, Mireille Toranelli, Georg Rauter, Magdalena Müller-Gerbl, Philippe C. Cattin
- **Comment**: Under submission
- **Journal**: None
- **Summary**: Purpose: Automated distinct bone segmentation from CT scans is widely used in planning and navigation workflows. U-Net variants are known to provide excellent results in supervised semantic segmentation. However, in distinct bone segmentation from upper body CTs a large field of view and a computationally taxing 3D architecture are required. This leads to low-resolution results lacking detail or localisation errors due to missing spatial context when using high-resolution inputs.   Methods: We propose to solve this problem by using end-to-end trainable segmentation networks that combine several 3D U-Nets working at different resolutions. Our approach, which extends and generalizes HookNet and MRN, captures spatial information at a lower resolution and skips the encoded information to the target network, which operates on smaller high-resolution inputs. We evaluated our proposed architecture against single resolution networks and performed an ablation study on information concatenation and the number of context networks.   Results: Our proposed best network achieves a median DSC of 0.86 taken over all 125 segmented bone classes and reduces the confusion among similar-looking bones in different locations. These results outperform our previously published 3D U-Net baseline results on the task and distinct-bone segmentation results reported by other groups.   Conclusion: The presented multi-resolution 3D U-Nets address current shortcomings in bone segmentation from upper-body CT scans by allowing for capturing a larger field of view while avoiding the cubic growth of the input pixels and intermediate computations that quickly outgrow the computational capacities in 3D. The approach thus improves the accuracy and efficiency of distinct bone segmentation from upper-body CT.



### DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2301.13721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13721v2)
- **Published**: 2023-01-31 15:58:32+00:00
- **Updated**: 2023-02-01 03:44:24+00:00
- **Authors**: Tao Yang, Yuwang Wang, Yan Lv, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we propose a new task, disentanglement of diffusion probabilistic models (DPMs), to take advantage of the remarkable modeling ability of DPMs. To tackle this task, we further devise an unsupervised approach named DisDiff. For the first time, we achieve disentangled representation learning in the framework of diffusion probabilistic models. Given a pre-trained DPM, DisDiff can automatically discover the inherent factors behind the image data and disentangle the gradient fields of DPM into sub-gradient fields, each conditioned on the representation of each discovered factor. We propose a novel Disentangling Loss for DisDiff to facilitate the disentanglement of the representation and sub-gradients. The extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.



### A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser
- **Arxiv ID**: http://arxiv.org/abs/2301.13731v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2301.13731v2)
- **Published**: 2023-01-31 16:11:47+00:00
- **Updated**: 2023-04-05 12:20:25+00:00
- **Authors**: Samuel Hurault, Antonin Chambolle, Arthur Leclaire, Nicolas Papadakis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP methods are efficient iterative algorithms for solving image inverse problems formulated as the minimization of the sum of a data-fidelity term and a regularization term. PnP methods perform regularization by plugging a pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD). To ensure convergence of PnP schemes, many works study specific parametrizations of deep denoisers. However, existing results require either unverifiable or suboptimal hypotheses on the denoiser, or assume restrictive conditions on the parameters of the inverse problem. Observing that these limitations can be due to the proximal algorithm in use, we study a relaxed version of the PGD algorithm for minimizing the sum of a convex function and a weakly convex one. When plugged with a relaxed proximal denoiser, we show that the proposed PnP-$\alpha$PGD algorithm converges for a wider range of regularization parameters, thus allowing more accurate image restoration.



### UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.13741v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13741v3)
- **Published**: 2023-01-31 16:18:52+00:00
- **Updated**: 2023-06-30 03:25:27+00:00
- **Authors**: Dachuan Shi, Chaofan Tao, Ying Jin, Zhendong Yang, Chun Yuan, Jiaqi Wang
- **Comment**: ICML 2023. Website: https://dachuanshi.com/UPop-Project
- **Journal**: None
- **Summary**: Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model architectures demonstrate the effectiveness and versatility of the proposed UPop framework. The code is available at https://github.com/sdc17/UPop.



### Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2301.13743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.13743v1)
- **Published**: 2023-01-31 16:24:34+00:00
- **Updated**: 2023-01-31 16:24:34+00:00
- **Authors**: Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modality data translation has attracted great interest in image computing. Deep generative models (\textit{e.g.}, GANs) show performance improvement in tackling those problems. Nevertheless, as a fundamental challenge in image translation, the problem of Zero-shot-Learning Cross-Modality Data Translation with fidelity remains unanswered. This paper proposes a new unsupervised zero-shot-learning method named Mutual Information guided Diffusion cross-modality data translation Model (MIDiffusion), which learns to translate the unseen source data to the target domain. The MIDiffusion leverages a score-matching-based generative model, which learns the prior knowledge in the target domain. We propose a differentiable local-wise-MI-Layer ($LMI$) for conditioning the iterative denoising sampling. The $LMI$ captures the identical cross-modality features in the statistical domain for the diffusion guidance; thus, our method does not require retraining when the source domain is changed, as it does not rely on any direct mapping between the source and target domains. This advantage is critical for applying cross-modality data translation methods in practice, as a reasonable amount of source domain dataset is not always available for supervised training. We empirically show the advanced performance of MIDiffusion in comparison with an influential group of generative models, including adversarial-based and other score-matching-based models.



### Deep learning-based lung segmentation and automatic regional template in chest X-ray images for pediatric tuberculosis
- **Arxiv ID**: http://arxiv.org/abs/2301.13786v1
- **DOI**: 10.1117/12.2652626
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07, I.4.0; I.4.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2301.13786v1)
- **Published**: 2023-01-31 17:33:35+00:00
- **Updated**: 2023-01-31 17:33:35+00:00
- **Authors**: Daniel Capellán-Martín, Juan J. Gómez-Valverde, Ramon Sanchez-Jacob, David Bermejo-Peláez, Lara García-Delgado, Elisa López-Varela, Maria J. Ledesma-Carbayo
- **Comment**: This work has been accepted at the SPIE Medical Imaging 2023, Image
  Processing conference
- **Journal**: None
- **Summary**: Tuberculosis (TB) is still considered a leading cause of death and a substantial threat to global child health. Both TB infection and disease are curable using antibiotics. However, most children who die of TB are never diagnosed or treated. In clinical practice, experienced physicians assess TB by examining chest X-rays (CXR). Pediatric CXR has specific challenges compared to adult CXR, which makes TB diagnosis in children more difficult. Computer-aided diagnosis systems supported by Artificial Intelligence have shown performance comparable to experienced radiologist TB readings, which could ease mass TB screening and reduce clinical burden. We propose a multi-view deep learning-based solution which, by following a proposed template, aims to automatically regionalize and extract lung and mediastinal regions of interest from pediatric CXR images where key TB findings may be present. Experimental results have shown accurate region extraction, which can be used for further analysis to confirm TB finding presence and severity assessment. Code publicly available at https://github.com/dani-capellan/pTB_LungRegionExtractor.



### Fairness-aware Vision Transformer via Debiased Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2301.13803v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13803v2)
- **Published**: 2023-01-31 17:44:59+00:00
- **Updated**: 2023-08-29 17:38:45+00:00
- **Authors**: Yao Qiang, Chengyin Li, Prashant Khanduri, Dongxiao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA utilizes an attention weights alignment regularizer in the training objective to encourage learning informative features for target prediction. Importantly, our DSA framework leads to improved fairness guarantees over prior works on multiple prediction tasks without compromising target prediction performance.



### A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control
- **Arxiv ID**: http://arxiv.org/abs/2301.13809v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.13809v3)
- **Published**: 2023-01-31 17:53:16+00:00
- **Updated**: 2023-04-18 10:46:10+00:00
- **Authors**: Ayush Singh, Pisharody Harikrishnan Gopalkrishnan, Mahesh Raveendranatha Panicker
- **Comment**: None
- **Journal**: None
- **Summary**: The creation of unique control methods for a hand prosthesis is still a problem that has to be addressed. The best choice of a human-machine interface (HMI) that should be used to enable natural control is still a challenge. Surface electromyography (sEMG), the most popular option, has a variety of difficult-to-fix issues (electrode displacement, sweat, fatigue). The ultrasound imaging-based methodology offers a means of recognising complex muscle activity and configuration with a greater SNR and less hardware requirements as compared to sEMG. In this study, a prototype system for high frame rate ultrasound imaging for prosthetic arm control is proposed. Using the proposed framework, a virtual robotic hand simulation is developed that can mimic a human hand as illustrated in the link [10]. The proposed classification model simulating four hand gestures has a classification accuracy of more than 90%.



### Patch Gradient Descent: Training Neural Networks on Very Large Images
- **Arxiv ID**: http://arxiv.org/abs/2301.13817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13817v1)
- **Published**: 2023-01-31 18:04:35+00:00
- **Updated**: 2023-01-31 18:04:35+00:00
- **Authors**: Deepak K. Gupta, Gowreesh Mago, Arnav Chavan, Dilip K. Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional CNN models are trained and tested on relatively low resolution images (<300 px), and cannot be directly operated on large-scale images due to compute and memory constraints. We propose Patch Gradient Descent (PatchGD), an effective learning strategy that allows to train the existing CNN architectures on large-scale images in an end-to-end manner. PatchGD is based on the hypothesis that instead of performing gradient-based updates on an entire image at once, it should be possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. PatchGD thus extensively enjoys better memory and compute efficiency when training models on large scale images. PatchGD is thoroughly evaluated on two datasets - PANDA and UltraMNIST with ResNet50 and MobileNetV2 models under different memory constraints. Our evaluation clearly shows that PatchGD is much more stable and efficient than the standard gradient-descent method in handling large images, and especially when the compute memory is limited.



### Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2301.13826v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13826v2)
- **Published**: 2023-01-31 18:10:38+00:00
- **Updated**: 2023-05-31 15:42:00+00:00
- **Authors**: Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or
- **Comment**: Accepted to SIGGRAPH 2023; Project page available at
  https://yuval-alaluf.github.io/Attend-and-Excite/
- **Journal**: None
- **Summary**: Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen - or excite - their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.



### Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression
- **Arxiv ID**: http://arxiv.org/abs/2301.13838v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13838v2)
- **Published**: 2023-01-31 18:31:20+00:00
- **Updated**: 2023-06-23 07:20:52+00:00
- **Authors**: Zhuoran Liu, Zhengyu Zhao, Martha Larson
- **Comment**: To appear at ICML 2023. Our code is available at
  https://github.com/liuzrcc/ImageShortcutSqueezing
- **Journal**: None
- **Summary**: Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods.



### Grounding Language Models to Images for Multimodal Inputs and Outputs
- **Arxiv ID**: http://arxiv.org/abs/2301.13823v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13823v4)
- **Published**: 2023-01-31 18:33:44+00:00
- **Updated**: 2023-06-13 21:54:58+00:00
- **Authors**: Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried
- **Comment**: Published in ICML 2023. Project page: https://jykoh.com/fromage
- **Journal**: None
- **Summary**: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.



### Salient Conditional Diffusion for Defending Against Backdoor Attacks
- **Arxiv ID**: http://arxiv.org/abs/2301.13862v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2301.13862v2)
- **Published**: 2023-01-31 18:56:41+00:00
- **Updated**: 2023-05-19 05:36:30+00:00
- **Authors**: Brandon B. May, N. Joseph Tatro, Dylan Walker, Piyush Kumar, Nathan Shnidman
- **Comment**: 14 pages, 5 figures. Edit: Added new baselines
- **Journal**: None
- **Summary**: We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising diffusion probabilistic model (DDPM) to degrade an image with noise and then recover said image using the learned reverse diffusion. Critically, we compute saliency map-based masks to condition our diffusion, allowing for stronger diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is highly effective at diffusing out triggers in data poisoned by backdoor attacks. At the same time, it reliably recovers salient features when applied to clean data. This performance is achieved without requiring access to the model parameters of the Trojan network, meaning Sancdifi operates as a black-box defense.



### From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2301.13865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.13865v1)
- **Published**: 2023-01-31 18:58:41+00:00
- **Updated**: 2023-01-31 18:58:41+00:00
- **Authors**: Huan-ang Gao, Beiwen Tian, Pengfei Li, Xiaoxue Chen, Hao Zhao, Guyue Zhou, Yurong Chen, Hongbin Zha
- **Comment**: Accepted to ICRA2023. Code: https://github.com/AIR-DISCOVER/Omni-PQ
- **Journal**: None
- **Summary**: Room layout estimation is a long-existing robotic vision task that benefits both environment sensing and motion planning. However, layout estimation using point clouds (PCs) still suffers from data scarcity due to annotation difficulty. As such, we address the semi-supervised setting of this task based upon the idea of model exponential moving averaging. But adapting this scheme to the state-of-the-art (SOTA) solution for PC-based layout estimation is not straightforward. To this end, we define a quad set matching strategy and several consistency losses based upon metrics tailored for layout quads. Besides, we propose a new online pseudo-label harvesting algorithm that decomposes the distribution of a hybrid distance measure between quads and PC into two components. This technique does not need manual threshold selection and intuitively encourages quads to align with reliable layout points. Surprisingly, this framework also works for the fully-supervised setting, achieving a new SOTA on the ScanNet benchmark. Last but not least, we also push the semi-supervised setting to the realistic omni-supervised setting, demonstrating significantly promoted performance on a newly annotated ARKitScenes testing set. Our codes, data and models are released in this repository.



### NASiam: Efficient Representation Learning using Neural Architecture Search for Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2302.00059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00059v1)
- **Published**: 2023-01-31 19:48:37+00:00
- **Updated**: 2023-01-31 19:48:37+00:00
- **Authors**: Alexandre Heuillet, Hedi Tabia, Hichem Arioui
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Siamese networks are one of the most trending methods to achieve self-supervised visual representation learning (SSL). Since hand labeling is costly, SSL can play a crucial part by allowing deep learning to train on large unlabeled datasets. Meanwhile, Neural Architecture Search (NAS) is becoming increasingly important as a technique to discover novel deep learning architectures. However, early NAS methods based on reinforcement learning or evolutionary algorithms suffered from ludicrous computational and memory costs. In contrast, differentiable NAS, a gradient-based approach, has the advantage of being much more efficient and has thus retained most of the attention in the past few years. In this article, we present NASiam, a novel approach that uses for the first time differentiable NAS to improve the multilayer perceptron projector and predictor (encoder/predictor pair) architectures inside siamese-networks-based contrastive learning frameworks (e.g., SimCLR, SimSiam, and MoCo) while preserving the simplicity of previous baselines. We crafted a search space designed explicitly for multilayer perceptrons, inside which we explored several alternatives to the standard ReLU activation function. We show that these new architectures allow ResNet backbone convolutional models to learn strong representations efficiently. NASiam reaches competitive performance in both small-scale (i.e., CIFAR-10/CIFAR-100) and large-scale (i.e., ImageNet) image classification datasets while costing only a few GPU hours. We discuss the composition of the NAS-discovered architectures and emit hypotheses on why they manage to prevent collapsing behavior. Our code is available at https://github.com/aheuillet/NASiam.



### Debiasing Vision-Language Models via Biased Prompts
- **Arxiv ID**: http://arxiv.org/abs/2302.00070v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2302.00070v2)
- **Published**: 2023-01-31 20:09:33+00:00
- **Updated**: 2023-05-15 07:51:14+00:00
- **Authors**: Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.



### Real Estate Property Valuation using Self-Supervised Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2302.00117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, econ.EM
- **Links**: [PDF](http://arxiv.org/pdf/2302.00117v1)
- **Published**: 2023-01-31 21:54:15+00:00
- **Updated**: 2023-01-31 21:54:15+00:00
- **Authors**: Mahdieh Yazdani, Maziar Raissi
- **Comment**: None
- **Journal**: None
- **Summary**: The use of Artificial Intelligence (AI) in the real estate market has been growing in recent years. In this paper, we propose a new method for property valuation that utilizes self-supervised vision transformers, a recent breakthrough in computer vision and deep learning. Our proposed algorithm uses a combination of machine learning, computer vision and hedonic pricing models trained on real estate data to estimate the value of a given property. We collected and pre-processed a data set of real estate properties in the city of Boulder, Colorado and used it to train, validate and test our algorithm. Our data set consisted of qualitative images (including house interiors, exteriors, and street views) as well as quantitative features such as the number of bedrooms, bathrooms, square footage, lot square footage, property age, crime rates, and proximity to amenities. We evaluated the performance of our model using metrics such as Root Mean Squared Error (RMSE). Our findings indicate that these techniques are able to accurately predict the value of properties, with a low RMSE. The proposed algorithm outperforms traditional appraisal methods that do not leverage property images and has the potential to be used in real-world applications.



### Design and Implementation of A Soccer Ball Detection System with Multiple Cameras
- **Arxiv ID**: http://arxiv.org/abs/2302.00123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00123v1)
- **Published**: 2023-01-31 22:04:53+00:00
- **Updated**: 2023-01-31 22:04:53+00:00
- **Authors**: Lei Li, Tianfang Zhang, Zhongfeng Kang, Wenhan Zhang
- **Comment**: 89 pages
- **Journal**: None
- **Summary**: The detection of small and medium-sized objects in three dimensions has always been a frontier exploration problem. This technology has a very wide application in sports analysis, games, virtual reality, human animation and other fields. The traditional three-dimensional small target detection technology has the disadvantages of high cost, low precision and inconvenience, so it is difficult to apply in practice. With the development of machine learning and deep learning, the technology of computer vision algorithms is becoming more mature. Creating an immersive media experience is considered to be a very important research work in sports.   The main work is to explore and solve the problem of football detection under the multiple cameras, aiming at the research and implementation of the live broadcast system of football matches. Using multi cameras detects a target ball and determines its position in three dimension with the occlusion, motion, low illumination of the target object.   This paper designed and implemented football detection system under multiple cameras for the detection and capture of targets in real-time matches. The main work mainly consists of three parts, football detector, single camera detection, and multi-cameras detection. The system used bundle adjustment to obtain the three-dimensional position of the target, and the GPU to accelerates data pre-processing and achieve accurate real-time capture of the target. By testing the system, it shows that the system can accurately detect and capture the moving targets in 3D.   In addition, the solution in this paper is reusable for large-scale competitions, like basketball and soccer. The system framework can be well transplanted into other similar engineering project systems. It has been put into the market.



