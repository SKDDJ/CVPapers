# Arxiv Papers in cs.CV on 2023-01-11
### CARD: Semantic Segmentation with Efficient Class-Aware Regularized Decoder
- **Arxiv ID**: http://arxiv.org/abs/2301.04258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04258v1)
- **Published**: 2023-01-11 01:41:37+00:00
- **Updated**: 2023-01-11 01:41:37+00:00
- **Authors**: Ye Huang, Di Kang, Liang Chen, Wenjing Jia, Xiangjian He, Lixin Duan, Xuefei Zhe, Linchao Bao
- **Comment**: Tech report, text extended from arXiv:2203.07160
- **Journal**: None
- **Summary**: Semantic segmentation has recently achieved notable advances by exploiting "class-level" contextual information during learning. However, these approaches simply concatenate class-level information to pixel features to boost the pixel representation learning, which cannot fully utilize intra-class and inter-class contextual information. Moreover, these approaches learn soft class centers based on coarse mask prediction, which is prone to error accumulation. To better exploit class level information, we propose a universal Class-Aware Regularization (CAR) approach to optimize the intra-class variance and inter-class distance during feature learning, motivated by the fact that humans can recognize an object by itself no matter which other objects it appears with. Moreover, we design a dedicated decoder for CAR (CARD), which consists of a novel spatial token mixer and an upsampling module, to maximize its gain for existing baselines while being highly efficient in terms of computational cost. Specifically, CAR consists of three novel loss functions. The first loss function encourages more compact class representations within each class, the second directly maximizes the distance between different class centers, and the third further pushes the distance between inter-class centers and pixels. Furthermore, the class center in our approach is directly generated from ground truth instead of from the error-prone coarse prediction. CAR can be directly applied to most existing segmentation models during training, and can largely improve their accuracy at no additional inference overhead. Extensive experiments and ablation studies conducted on multiple benchmark datasets demonstrate that the proposed CAR can boost the accuracy of all baseline models by up to 2.23% mIOU with superior generalization ability. CARD outperforms SOTA approaches on multiple benchmarks with a highly efficient architecture.



### Towards Microstructural State Variables in Materials Systems
- **Arxiv ID**: http://arxiv.org/abs/2301.04261v1
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.mtrl-sci, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04261v1)
- **Published**: 2023-01-11 01:49:18+00:00
- **Updated**: 2023-01-11 01:49:18+00:00
- **Authors**: Veera Sundararaghavan, Megna N. Shah, Jeff P. Simmons
- **Comment**: None
- **Journal**: None
- **Summary**: The vast combination of material properties seen in nature are achieved by the complexity of the material microstructure. Advanced characterization and physics based simulation techniques have led to generation of extremely large microstructural datasets. There is a need for machine learning techniques that can manage data complexity by capturing the maximal amount of information about the microstructure using the least number of variables. This paper aims to formulate dimensionality and state variable estimation techniques focused on reducing microstructural image data. It is shown that local dimensionality estimation based on nearest neighbors tend to give consistent dimension estimates for natural images for all p-Minkowski distances. However, it is found that dimensionality estimates have a systematic error for low-bit depth microstructural images. The use of Manhattan distance to alleviate this issue is demonstrated. It is also shown that stacked autoencoders can reconstruct the generator space of high dimensional microstructural data and provide a sparse set of state variables to fully describe the variability in material microstructures.



### Adversarial Alignment for Source Free Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.04265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04265v1)
- **Published**: 2023-01-11 02:08:37+00:00
- **Updated**: 2023-01-11 02:08:37+00:00
- **Authors**: Qiaosong Chu, Shuyan Li, Guangyi Chen, Kai Li, Xiu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Source-free object detection (SFOD) aims to transfer a detector pre-trained on a label-rich source domain to an unlabeled target domain without seeing source data. While most existing SFOD methods generate pseudo labels via a source-pretrained model to guide training, these pseudo labels usually contain high noises due to heavy domain discrepancy. In order to obtain better pseudo supervisions, we divide the target domain into source-similar and source-dissimilar parts and align them in the feature space by adversarial learning. Specifically, we design a detection variance-based criterion to divide the target domain. This criterion is motivated by a finding that larger detection variances denote higher recall and larger similarity to the source domain. Then we incorporate an adversarial module into a mean teacher framework to drive the feature spaces of these two subsets indistinguishable. Extensive experiments on multiple cross-domain object detection datasets demonstrate that our proposed method consistently outperforms the compared SFOD methods.



### Data Distillation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2301.04272v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2301.04272v1)
- **Published**: 2023-01-11 02:25:10+00:00
- **Updated**: 2023-01-11 02:25:10+00:00
- **Authors**: Noveen Sachdeva, Julian McAuley
- **Comment**: Under review. 19 pages, 4 figures
- **Journal**: None
- **Summary**: The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.



### LENet: Lightweight And Efficient LiDAR Semantic Segmentation Using Multi-Scale Convolution Attention
- **Arxiv ID**: http://arxiv.org/abs/2301.04275v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04275v3)
- **Published**: 2023-01-11 02:51:38+00:00
- **Updated**: 2023-06-19 00:35:59+00:00
- **Authors**: Ben Ding
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR-based semantic segmentation is critical in the fields of robotics and autonomous driving as it provides a comprehensive understanding of the scene. This paper proposes a lightweight and efficient projection-based semantic segmentation network called LENet with an encoder-decoder structure for LiDAR-based semantic segmentation. The encoder is composed of a novel multi-scale convolutional attention (MSCA) module with varying receptive field sizes to capture features. The decoder employs an Interpolation And Convolution (IAC) mechanism utilizing bilinear interpolation for upsampling multi-resolution feature maps and integrating previous and current dimensional features through a single convolution layer. This approach significantly reduces the network's complexity while also improving its accuracy. Additionally, we introduce multiple auxiliary segmentation heads to further refine the network's accuracy. Extensive evaluations on publicly available datasets, including SemanticKITTI, SemanticPOSS, and nuScenes, show that our proposed method is lighter, more efficient, and robust compared to state-of-the-art semantic segmentation methods. Full implementation is available at https://github.com/fengluodb/LENet.



### Generic Event Boundary Detection in Video with Pyramid Features
- **Arxiv ID**: http://arxiv.org/abs/2301.04288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04288v1)
- **Published**: 2023-01-11 03:29:27+00:00
- **Updated**: 2023-01-11 03:29:27+00:00
- **Authors**: Van Thong Huynh, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Generic event boundary detection (GEBD) aims to split video into chunks at a broad and diverse set of actions as humans naturally perceive event boundaries. In this study, we present an approach that considers the correlation between neighbor frames with pyramid feature maps in both spatial and temporal dimensions to construct a framework for localizing generic events in video. The features at multiple spatial dimensions of a pre-trained ResNet-50 are exploited with different views in the temporal dimension to form a temporal pyramid feature map. Based on that, the similarity between neighbor frames is calculated and projected to build a temporal pyramid similarity feature vector. A decoder with 1D convolution operations is used to decode these similarities to a new representation that incorporates their temporal relationship for later boundary score estimation. Extensive experiments conducted on the GEBD benchmark dataset show the effectiveness of our system and its variations, in which we outperformed the state-of-the-art approaches. Additional experiments on TAPOS dataset, which contains long-form videos with Olympic sport actions, demonstrated the effectiveness of our study compared to others.



### Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments
- **Arxiv ID**: http://arxiv.org/abs/2301.04352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.04352v1)
- **Published**: 2023-01-11 08:04:18+00:00
- **Updated**: 2023-01-11 08:04:18+00:00
- **Authors**: Ting Wang, Zongkai Wu, Feiyu Yao, Donglin Wang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is a navigation task that requires an agent to follow a language instruction in a realistic environment. The understanding of environments is a crucial part of the VLN-CE task, but existing methods are relatively simple and direct in understanding the environment, without delving into the relationship between language instructions and visual environments. Therefore, we propose a new environment representation in order to solve the above problems. First, we propose an Environment Representation Graph (ERG) through object detection to express the environment in semantic level. This operation enhances the relationship between language and environment. Then, the relational representations of object-object, object-agent in ERG are learned through GCN, so as to obtain a continuous expression about ERG. Sequentially, we combine the ERG expression with object label embeddings to obtain the environment representation. Finally, a new cross-modal attention navigation framework is proposed, incorporating our environment representation and a special loss function dedicated to training ERG. Experimental result shows that our method achieves satisfactory performance in terms of success rate on VLN-CE tasks. Further analysis explains that our method attains better cross-modal matching and strong generalization ability.



### Recognising geometric primitives in 3D point clouds of mechanical CAD objects
- **Arxiv ID**: http://arxiv.org/abs/2301.04371v1
- **DOI**: 10.1016/j.cad.2023.103479
- **Categories**: **cs.GR**, cs.CV, 65D18, 65D17, 68U05, 62H30, G.1.2; I.3.5; I.5; J.6
- **Links**: [PDF](http://arxiv.org/pdf/2301.04371v1)
- **Published**: 2023-01-11 09:33:55+00:00
- **Updated**: 2023-01-11 09:33:55+00:00
- **Authors**: Chiara Romanengo, Andrea Raffo, Silvia Biasotti, Bianca Falcidieno
- **Comment**: None
- **Journal**: Computer-Aided Design 157 (2023) 103479
- **Summary**: The problem faced in this paper concerns the recognition of simple and complex geometric primitives in point clouds resulting from scans of mechanical CAD objects. A large number of points, the presence of noise, outliers, missing or redundant parts and uneven distribution are the main problems to be addressed to meet this need. In this article we propose a solution, based on the Hough transform, that can recognize simple and complex geometric primitives and is robust to noise, outliers, and missing parts. Additionally, we can extract a series of geometric descriptors that uniquely characterize a primitive and, based on them, aggregate the output into maximal or compound primitives, thus reducing oversegmentation. The results presented in the paper demonstrate the robustness of the method and its competitiveness with respect to other solutions proposed in the literature.



### An atrium segmentation network with location guidance and siamese adjustment
- **Arxiv ID**: http://arxiv.org/abs/2301.04401v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2301.04401v1)
- **Published**: 2023-01-11 10:57:19+00:00
- **Updated**: 2023-01-11 10:57:19+00:00
- **Authors**: Yuhan Xie, Zhiyong Zhang, Shaolong Chen, Changzhen Qiu
- **Comment**: 17 pages,9 figures
- **Journal**: None
- **Summary**: The segmentation of atrial scan images is of great significance for the three-dimensional reconstruction of the atrium and the surgical positioning. Most of the existing segmentation networks adopt a 2D structure and only take original images as input, ignoring the context information of 3D images and the role of prior information. In this paper, we propose an atrium segmentation network LGSANet with location guidance and siamese adjustment, which takes adjacent three slices of images as input and adopts an end-to-end approach to achieve coarse-to-fine atrial segmentation. The location guidance(LG) block uses the prior information of the localization map to guide the encoding features of the fine segmentation stage, and the siamese adjustment(SA) block uses the context information to adjust the segmentation edges. On the atrium datasets of ACDC and ASC, sufficient experiments prove that our method can adapt to many classic 2D segmentation networks, so that it can obtain significant performance improvements.



### Secure access system using signature verification over tablet PC
- **Arxiv ID**: http://arxiv.org/abs/2301.04402v1
- **DOI**: 10.1109/MAES.2007.351725
- **Categories**: **cs.CR**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2301.04402v1)
- **Published**: 2023-01-11 11:05:47+00:00
- **Updated**: 2023-01-11 11:05:47+00:00
- **Authors**: Fernando Alonso-Fernandez, Julian Fierrez-Aguilar, Javier Ortega-Garcia, Joaquin Gonzalez-Rodriguez
- **Comment**: Published at IEEE Aerospace and Electronic Systems Magazine
- **Journal**: None
- **Summary**: Low-cost portable devices capable of capturing signature signals are being increasingly used. Additionally, the social and legal acceptance of the written signature for authentication purposes is opening a range of new applications. We describe a highly versatile and scalable prototype for Web-based secure access using signature verification. The proposed architecture can be easily extended to work with different kinds of sensors and large-scale databases. Several remarks are also given on security and privacy of network-based signature verification.



### GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.04410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04410v1)
- **Published**: 2023-01-11 11:38:37+00:00
- **Updated**: 2023-01-11 11:38:37+00:00
- **Authors**: Hong-Yu Zhou, Chixiang Lu, Liansheng Wang, Yizhou Yu
- **Comment**: Accepted by IEEE Transactions on Medical Imaging. The code is
  available at https://bit.ly/3xiFyjx
- **Journal**: None
- **Summary**: Self-supervised representation learning has been extremely successful in medical image analysis, as it requires no human annotations to provide transferable representations for downstream tasks. Recent self-supervised learning methods are dominated by noise-contrastive estimation (NCE, also known as contrastive learning), which aims to learn invariant visual representations by contrasting one homogeneous image pair with a large number of heterogeneous image pairs in each training step. Nonetheless, NCE-based approaches still suffer from one major problem that is one homogeneous pair is not enough to extract robust and invariant semantic information. Inspired by the archetypical triplet loss, we propose GraVIS, which is specifically optimized for learning self-supervised features from dermatology images, to group homogeneous dermatology images while separating heterogeneous ones. In addition, a hardness-aware attention is introduced and incorporated to address the importance of homogeneous image views with similar appearance instead of those dissimilar homogeneous ones. GraVIS significantly outperforms its transfer learning and self-supervised learning counterparts in both lesion segmentation and disease classification tasks, sometimes by 5 percents under extremely limited supervision. More importantly, when equipped with the pre-trained weights provided by GraVIS, a single model could achieve better results than winners that heavily rely on ensemble strategies in the well-known ISIC 2017 challenge.



### How Does Traffic Environment Quantitatively Affect the Autonomous Driving Prediction?
- **Arxiv ID**: http://arxiv.org/abs/2301.04414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04414v1)
- **Published**: 2023-01-11 11:47:54+00:00
- **Updated**: 2023-01-11 11:47:54+00:00
- **Authors**: Wenbo Shao, Yanchao Xu, Jun Li, Chen Lv, Weida Wang, Hong Wang
- **Comment**: 16 pages, 15 figures
- **Journal**: None
- **Summary**: An accurate trajectory prediction is crucial for safe and efficient autonomous driving in complex traffic environments. In recent years, artificial intelligence has shown strong capabilities in improving prediction accuracy. However, its characteristics of inexplicability and uncertainty make it challenging to determine the traffic environmental effect on prediction explicitly, posing significant challenges to safety-critical decision-making. To address these challenges, this study proposes a trajectory prediction framework with the epistemic uncertainty estimation ability that outputs high uncertainty when confronting unforeseeable or unknown scenarios. The proposed framework is used to analyze the environmental effect on the prediction algorithm performance. In the analysis, the traffic environment is considered in terms of scenario features and shifts, respectively, where features are divided into kinematic features of a target agent, features of its surrounding traffic participants, and other features. In addition, feature correlation and importance analyses are performed to study the above features' influence on the prediction error and epistemic uncertainty. Further, a cross-dataset case study is conducted using multiple intersection datasets to investigate the impact of unavoidable distributional shifts in the real world on trajectory prediction. The results indicate that the deep ensemble-based method has advantages in improving prediction robustness and estimating epistemic uncertainty. The consistent conclusions are obtained by the feature correlation and importance analyses, including the conclusion that kinematic features of the target agent have relatively strong effects on the prediction error and epistemic uncertainty. Furthermore, the prediction failure caused by distributional shifts and the potential of the deep ensemble-based method are analyzed.



### pyssam -- a Python library for statistical modelling of biomedical shape and appearance
- **Arxiv ID**: http://arxiv.org/abs/2301.04416v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04416v1)
- **Published**: 2023-01-11 11:50:44+00:00
- **Updated**: 2023-01-11 11:50:44+00:00
- **Authors**: Josh Williams, Ali Ozel, Uwe Wolfram
- **Comment**: 5 pages, 3 figures, Journal of Open Source Software submission
- **Journal**: None
- **Summary**: pyssam is a Python library for creating statistical shape and appearance models (SSAMs) for biological (and other) shapes such as bones, lungs or other organs. A point cloud best describing the anatomical 'landmarks' of the organ are required from each sample in a small population as an input. Additional information such as landmark gray-value can be included to incorporate joint correlations of shape and 'appearance' into the model. Our library performs alignment and scaling of the input data and creates a SSAM based on covariance across the population. The output SSAM can be used to parameterise and quantify shape change across a population. pyssam is a small and low dependency codebase with examples included as Jupyter notebooks for several common SSAM computations. The given examples can easily be extended to alternative datasets, and also alternative tasks such as medical image segmentation by incorporating a SSAM as a constraint for segmented organs.



### Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective
- **Arxiv ID**: http://arxiv.org/abs/2301.04421v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04421v2)
- **Published**: 2023-01-11 12:01:08+00:00
- **Updated**: 2023-05-25 12:06:33+00:00
- **Authors**: Wenbo Shao, Yanchao Xu, Liang Peng, Jun Li, Hong Wang
- **Comment**: Accepted by 2023 IEEE International Conference on Robotics and
  Automation (ICRA 2023)
- **Journal**: None
- **Summary**: Motion prediction is essential for safe and efficient autonomous driving. However, the inexplicability and uncertainty of complex artificial intelligence models may lead to unpredictable failures of the motion prediction module, which may mislead the system to make unsafe decisions. Therefore, it is necessary to develop methods to guarantee reliable autonomous driving, where failure detection is a potential direction. Uncertainty estimates can be used to quantify the degree of confidence a model has in its predictions and may be valuable for failure detection. We propose a framework of failure detection for motion prediction from the uncertainty perspective, considering both motion uncertainty and model uncertainty, and formulate various uncertainty scores according to different prediction stages. The proposed approach is evaluated based on different motion prediction algorithms, uncertainty estimation methods, uncertainty scores, etc., and the results show that uncertainty is promising for failure detection for motion prediction but should be used with caution.



### Optical Flow for Autonomous Driving: Applications, Challenges and Improvements
- **Arxiv ID**: http://arxiv.org/abs/2301.04422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.04422v1)
- **Published**: 2023-01-11 12:01:42+00:00
- **Updated**: 2023-01-11 12:01:42+00:00
- **Authors**: Shihao Shen, Louis Kerofsky, Senthil Yogamani
- **Comment**: Accepted for publication in Electronic Imaging, Autonomous Vehicles
  and Machines 2023
- **Journal**: None
- **Summary**: Optical flow estimation is a well-studied topic for automated driving applications. Many outstanding optical flow estimation methods have been proposed, but they become erroneous when tested in challenging scenarios that are commonly encountered. Despite the increasing use of fisheye cameras for near-field sensing in automated driving, there is very limited literature on optical flow estimation with strong lens distortion. Thus we propose and evaluate training strategies to improve a learning-based optical flow algorithm by leveraging the only existing fisheye dataset with optical flow ground truth. While trained with synthetic data, the model demonstrates strong capabilities to generalize to real world fisheye data. The other challenge neglected by existing state-of-the-art algorithms is low light. We propose a novel, generic semi-supervised framework that significantly boosts performances of existing methods in such conditions. To the best of our knowledge, this is the first approach that explicitly handles optical flow estimation in low light.



### Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset
- **Arxiv ID**: http://arxiv.org/abs/2301.04423v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04423v2)
- **Published**: 2023-01-11 12:02:10+00:00
- **Updated**: 2023-02-27 16:25:17+00:00
- **Authors**: Frauke Wilm, Marco Fragoso, Christof A. Bertram, Nikolas Stathonikos, Mathias Öttl, Jingna Qiu, Robert Klopfleisch, Andreas Maier, Katharina Breininger, Marc Aubreville
- **Comment**: 6 pages, 3 figures, 1 table, accepted at BVM workshop 2023
- **Journal**: None
- **Summary**: In histopathology, scanner-induced domain shifts are known to impede the performance of trained neural networks when tested on unseen data. Multi-domain pre-training or dedicated domain-generalization techniques can help to develop domain-agnostic algorithms. For this, multi-scanner datasets with a high variety of slide scanning systems are highly desirable. We present a publicly available multi-scanner dataset of canine cutaneous squamous cell carcinoma histopathology images, composed of 44 samples digitized with five slide scanners. This dataset provides local correspondences between images and thereby isolates the scanner-induced domain shift from other inherent, e.g. morphology-induced domain shifts. To highlight scanner differences, we present a detailed evaluation of color distributions, sharpness, and contrast of the individual scanner subsets. Additionally, to quantify the inherent scanner-induced domain shift, we train a tumor segmentation network on each scanner subset and evaluate the performance both in- and cross-domain. We achieve a class-averaged in-domain intersection over union coefficient of up to 0.86 and observe a cross-domain performance decrease of up to 0.38, which confirms the inherent domain shift of the presented dataset and its negative impact on the performance of deep neural networks.



### VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.04447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04447v1)
- **Published**: 2023-01-11 13:07:31+00:00
- **Updated**: 2023-01-11 13:07:31+00:00
- **Authors**: Hemraj Singh, Mridula Verma, Ramalingaswamy Cheruku
- **Comment**: None
- **Journal**: https://ictai.computer.org/2022/
- **Summary**: Video Salient Document Detection (VSDD) is an essential task of practical computer vision, which aims to highlight visually salient document regions in video frames. Previous techniques for VSDD focus on learning features without considering the cooperation among and across the appearance and motion cues and thus fail to perform in practical scenarios. Moreover, most of the previous techniques demand high computational resources, which limits the usage of such systems in resource-constrained settings. To handle these issues, we propose VS-Net, which captures multi-scale spatiotemporal information with the help of dilated depth-wise separable convolution and Approximation Rank Pooling. VS-Net extracts the key features locally from each frame across embedding sub-spaces and forwards the features between adjacent and parallel nodes, enhancing model performance globally. Our model generates saliency maps considering both the background and foreground simultaneously, making it perform better in challenging scenarios. The immense experiments regulated on the benchmark MIDV-500 dataset show that the VS-Net model outperforms state-of-the-art approaches in both time and robustness measures.



### Heterogeneous Tri-stream Clustering Network
- **Arxiv ID**: http://arxiv.org/abs/2301.04451v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04451v1)
- **Published**: 2023-01-11 13:15:54+00:00
- **Updated**: 2023-01-11 13:15:54+00:00
- **Authors**: Xiaozhi Deng, Dong Huang, Chang-Dong Wang
- **Comment**: To appear in Neural Processing Letters
- **Journal**: None
- **Summary**: Contrastive deep clustering has recently gained significant attention with its ability of joint contrastive learning and clustering via deep neural networks. Despite the rapid progress, previous works mostly require both positive and negative sample pairs for contrastive clustering, which rely on a relative large batch-size. Moreover, they typically adopt a two-stream architecture with two augmented views, which overlook the possibility and potential benefits of multi-stream architectures (especially with heterogeneous or hybrid networks). In light of this, this paper presents a new end-to-end deep clustering approach termed Heterogeneous Tri-stream Clustering Network (HTCN). The tri-stream architecture in HTCN consists of three main components, including two weight-sharing online networks and a target network, where the parameters of the target network are the exponential moving average of that of the online networks. Notably, the two online networks are trained by simultaneously (i) predicting the instance representations of the target network and (ii) enforcing the consistency between the cluster representations of the target network and that of the two online networks. Experimental results on four challenging image datasets demonstrate the superiority of HTCN over the state-of-the-art deep clustering approaches. The code is available at https://github.com/dengxiaozhi/HTCN.



### Prostate Lesion Estimation using Prostate Masks from Biparametric MRI
- **Arxiv ID**: http://arxiv.org/abs/2301.09673v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09673v1)
- **Published**: 2023-01-11 13:20:24+00:00
- **Updated**: 2023-01-11 13:20:24+00:00
- **Authors**: Ahmet Karagoz, Mustafa Ege Seker, Mert Yergin, Tarkan Atak Kan, Mustafa Said Kartal, Ercan Karaarslan, Deniz Alis, Ilkay Oksuz
- **Comment**: None
- **Journal**: None
- **Summary**: Biparametric MRI has emerged as an alternative to multiparametric prostate MRI, which eliminates the need for the potential harms to the patient due to the contrast medium. One major issue with biparametric MRI is difficulty to detect clinically significant prostate cancer (csPCA). Deep learning algorithms have emerged as an alternative solution to detect csPCA in cohort studies. We present a workflow which predicts csPCA on biparametric prostate MRI PI-CAI 2022 Challenge with over 10,000 carefully-curated prostate MRI exams. We propose to to segment the prostate gland first to the central gland (transition + central zone) and the peripheral gland. Then we utilize these predcitions in combination with T2, ADC and DWI images to train an ensemble nnU-Net model. Finally, we utilize clinical indices PSA and ADC intensity distributions of lesion regions to reduce the false positives. Our method achieves top results on open-validation stage with a AUROC of 0.888 and AP of 0.732.



### Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.04454v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2301.04454v1)
- **Published**: 2023-01-11 13:23:21+00:00
- **Updated**: 2023-01-11 13:23:21+00:00
- **Authors**: Rabbia Asghar, Lukas Rummelhard, Anne Spalanzani, Christian Laugier
- **Comment**: ICARCV 2022-17th International Conference on Control, Automation,
  Robotics and Vision
- **Journal**: None
- **Summary**: Prediction of dynamic environment is crucial to safe navigation of an autonomous vehicle. Urban traffic scenes are particularly challenging to forecast due to complex interactions between various dynamic agents, such as vehicles and vulnerable road users. Previous approaches have used egocentric occupancy grid maps to represent and predict dynamic environments. However, these predictions suffer from blurriness, loss of scene structure at turns, and vanishing of agents over longer prediction horizon. In this work, we propose a novel framework to make long-term predictions by representing the traffic scene in a fixed frame, referred as allo-centric occupancy grid. This allows for the static scene to remain fixed and to represent motion of the ego-vehicle on the grid like other agents'. We study the allo-centric grid prediction with different video prediction networks and validate the approach on the real-world Nuscenes dataset. The results demonstrate that the allo-centric grid representation significantly improves scene prediction, in comparison to the conventional ego-centric grid approach.



### Fast spline detection in high density microscopy data
- **Arxiv ID**: http://arxiv.org/abs/2301.04460v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2301.04460v2)
- **Published**: 2023-01-11 13:40:05+00:00
- **Updated**: 2023-01-13 10:05:00+00:00
- **Authors**: Albert Alonso, Julius B. Kirkegaard
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided analysis of biological microscopy data has seen a massive improvement with the utilization of general-purpose deep learning techniques. Yet, in microscopy studies of multi-organism systems, the problem of collision and overlap remains challenging. This is particularly true for systems composed of slender bodies such as crawling nematodes, swimming spermatozoa, or the beating of eukaryotic or prokaryotic flagella. Here, we develop a novel end-to-end deep learning approach to extract precise shape trajectories of generally motile and overlapping splines. Our method works in low resolution settings where feature keypoints are hard to define and detect. Detection is fast and we demonstrate the ability to track thousands of overlapping organisms simultaneously. While our approach is agnostic to area of application, we present it in the setting of and exemplify its usability on dense experiments of crawling Caenorhabditis elegans. The model training is achieved purely on synthetic data, utilizing a physics-based model for nematode motility, and we demonstrate the model's ability to generalize from simulations to experimental videos.



### Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.04465v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04465v3)
- **Published**: 2023-01-11 13:48:37+00:00
- **Updated**: 2023-05-26 15:14:45+00:00
- **Authors**: Zhiqiang Shen, Peng Cao, Hua Yang, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
- **Comment**: None
- **Journal**: None
- **Summary**: Consistency regularization and pseudo labeling-based semi-supervised methods perform co-training using the pseudo labels from multi-view inputs. However, such co-training models tend to converge early to a consensus, degenerating to the self-training ones, and produce low-confidence pseudo labels from the perturbed inputs during training. To address these issues, we propose an Uncertainty-guided Collaborative Mean-Teacher (UCMT) for semi-supervised semantic segmentation with the high-confidence pseudo labels. Concretely, UCMT consists of two main components: 1) collaborative mean-teacher (CMT) for encouraging model disagreement and performing co-training between the sub-networks, and 2) uncertainty-guided region mix (UMIX) for manipulating the input images according to the uncertainty maps of CMT and facilitating CMT to produce high-confidence pseudo labels. Combining the strengths of UMIX with CMT, UCMT can retain model disagreement and enhance the quality of pseudo labels for the co-training segmentation. Extensive experiments on four public medical image datasets including 2D and 3D modalities demonstrate the superiority of UCMT over the state-of-the-art. Code is available at: https://github.com/Senyh/UCMT.



### Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains
- **Arxiv ID**: http://arxiv.org/abs/2301.04494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04494v1)
- **Published**: 2023-01-11 14:42:47+00:00
- **Updated**: 2023-01-11 14:42:47+00:00
- **Authors**: Indel Pal Singh, Enjie Ghorbel, Oyebade Oyedotun, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an adaptive graph-based approach for multi-label image classification. Graph-based methods have been largely exploited in the field of multi-label classification, given their ability to model label correlations. Specifically, their effectiveness has been proven not only when considering a single domain but also when taking into account multiple domains. However, the topology of the used graph is not optimal as it is pre-defined heuristically. In addition, consecutive Graph Convolutional Network (GCN) aggregations tend to destroy the feature similarity. To overcome these issues, an architecture for learning the graph connectivity in an end-to-end fashion is introduced. This is done by integrating an attention-based mechanism and a similarity-preserving strategy. The proposed framework is then extended to multiple domains using an adversarial training scheme. Numerous experiments are reported on well-known single-domain and multi-domain benchmarks. The results demonstrate that our approach outperforms the state-of-the-art in terms of mean Average Precision (mAP) and model size.



### Dynamic Background Reconstruction via MAE for Infrared Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.04497v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04497v2)
- **Published**: 2023-01-11 14:45:50+00:00
- **Updated**: 2023-04-09 13:51:57+00:00
- **Authors**: Jingchao Peng, Haitao Zhao, Kaijie Zhao, Zhongze Wang, Lujian Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared small target detection (ISTD) under complex backgrounds is a difficult problem, for the differences between targets and backgrounds are not easy to distinguish. Background reconstruction is one of the methods to deal with this problem. This paper proposes an ISTD method based on background reconstruction called Dynamic Background Reconstruction (DBR). DBR consists of three modules: a dynamic shift window module (DSW), a background reconstruction module (BR), and a detection head (DH). BR takes advantage of Vision Transformers in reconstructing missing patches and adopts a grid masking strategy with a masking ratio of 50\% to reconstruct clean backgrounds without targets. To avoid dividing one target into two neighboring patches, resulting in reconstructing failure, DSW is performed before input embedding. DSW calculates offsets, according to which infrared images dynamically shift. To reduce False Positive (FP) cases caused by regarding reconstruction errors as targets, DH utilizes a structure of densely connected Transformer to further improve the detection performance. Experimental results show that DBR achieves the best F1-score on the two ISTD datasets, MFIRST (64.10\%) and SIRST (75.01\%).



### Pruning Compact ConvNets for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2301.04502v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04502v1)
- **Published**: 2023-01-11 14:51:19+00:00
- **Updated**: 2023-01-11 14:51:19+00:00
- **Authors**: Sayan Ghosh, Karthik Prasad, Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Graham Cormode, Peter Vajda
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network pruning is frequently used to compress over-parameterized networks by large amounts, while incurring only marginal drops in generalization performance. However, the impact of pruning on networks that have been highly optimized for efficient inference has not received the same level of attention. In this paper, we analyze the effect of pruning for computer vision, and study state-of-the-art ConvNets, such as the FBNetV3 family of models. We show that model pruning approaches can be used to further optimize networks trained through NAS (Neural Architecture Search). The resulting family of pruned models can consistently obtain better performance than existing FBNetV3 models at the same level of computation, and thus provide state-of-the-art results when trading off between computational complexity and generalization performance on the ImageNet benchmark. In addition to better generalization performance, we also demonstrate that when limited computation resources are available, pruning FBNetV3 models incur only a fraction of GPU-hours involved in running a full-scale NAS.



### A Distinct Unsupervised Reference Model From The Environment Helps Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.04506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04506v1)
- **Published**: 2023-01-11 15:05:36+00:00
- **Updated**: 2023-01-11 15:05:36+00:00
- **Authors**: Seyyed AmirHossein Ameli Kalkhoran, Mohammadamin Banayeeanzade, Mahdi Samiei, Mahdieh Soleymani Baghshah
- **Comment**: 22 pages, 3 figures, and 18 tables
- **Journal**: None
- **Summary**: The existing continual learning methods are mainly focused on fully-supervised scenarios and are still not able to take advantage of unlabeled data available in the environment. Some recent works tried to investigate semi-supervised continual learning (SSCL) settings in which the unlabeled data are available, but it is only from the same distribution as the labeled data. This assumption is still not general enough for real-world applications and restricts the utilization of unsupervised data. In this work, we introduce Open-Set Semi-Supervised Continual Learning (OSSCL), a more realistic semi-supervised continual learning setting in which out-of-distribution (OoD) unlabeled samples in the environment are assumed to coexist with the in-distribution ones. Under this configuration, we present a model with two distinct parts: (i) the reference network captures general-purpose and task-agnostic knowledge in the environment by using a broad spectrum of unlabeled samples, (ii) the learner network is designed to learn task-specific representations by exploiting supervised samples. The reference model both provides a pivotal representation space and also segregates unlabeled data to exploit them more efficiently. By performing a diverse range of experiments, we show the superior performance of our model compared with other competitors and prove the effectiveness of each component of the proposed model.



### Determination of cutting positions of honeycomb blocks using computer vision
- **Arxiv ID**: http://arxiv.org/abs/2304.00001v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.00001v2)
- **Published**: 2023-01-11 15:24:09+00:00
- **Updated**: 2023-04-04 05:52:42+00:00
- **Authors**: Alexander Razumovsky, Yakov Pikalov, Mikhail Saramud
- **Comment**: 5 pages, in Russian language, 5 figures, for ASEDU-III
- **Journal**: None
- **Summary**: The article discusses a method for automating the process of cutting a honeycomb block, and specifically obtaining points and cutting angles for the required faces. The following requirements are taken into account in the calculations: the allowable location of the cut plane is 0.4 of the length of the cell face, the cut plane must be perpendicular to the cell wall. The algorithm itself consists of two main stages: determining the honeycomb structure and searching for cut points. In the absence of significant defects in honeycomb blocks (deformation of the cell profile and a dent on the edges of the cells), the structure determination algorithm works without significant inaccuracies. The results of the cut point search algorithm can be considered satisfactory.



### A new sampling methodology for creating rich, heterogeneous, subsets of samples for training image segmentation algorithms
- **Arxiv ID**: http://arxiv.org/abs/2301.04517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04517v2)
- **Published**: 2023-01-11 15:31:15+00:00
- **Updated**: 2023-01-30 15:42:29+00:00
- **Authors**: Matheus Viana da Silva, Natália de Carvalho Santos, Baptiste Lacoste, Cesar Henrique Comin
- **Comment**: 11 pages, 9 figures. To be published
- **Journal**: None
- **Summary**: Creating a dataset for training supervised machine learning algorithms can be a demanding task. This is especially true for medical image segmentation since this task usually requires one or more specialists for image annotation, and creating ground truth labels for just a single image can take up to several hours. In addition, it is paramount that the annotated samples represent well the different conditions that might affect the imaged tissue as well as possible changes in the image acquisition process. This can only be achieved by considering samples that are typical in the dataset as well as atypical, or even outlier, samples. We introduce a new sampling methodology for selecting relevant images from a larger non-annotated dataset in a way that evenly considers both prototypical as well as atypical samples. The methodology involves the generation of a uniform grid from a feature space representing the samples, which is then used for randomly drawing relevant images. The selected images provide a uniform cover of the original dataset, and thus define a heterogeneous set of images that can be annotated and used for training supervised segmentation algorithms. We provide a case example by creating a dataset containing a representative set of blood vessel microscopy images selected from a larger dataset containing thousands of images.



### Clustering disease trajectories in contrastive feature space for biomarker discovery in age-related macular degeneration
- **Arxiv ID**: http://arxiv.org/abs/2301.04525v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04525v2)
- **Published**: 2023-01-11 15:44:42+00:00
- **Updated**: 2023-03-20 10:18:28+00:00
- **Authors**: Robbie Holland, Oliver Leingang, Christopher Holmes, Philipp Anders, Rebecca Kaye, Sophie Riedl, Johannes C. Paetzold, Ivan Ezhov, Hrvoje Bogunović, Ursula Schmidt-Erfurth, Lars Fritsche, Hendrik P. N. Scholl, Sobha Sivaprasad, Andrew J. Lotery, Daniel Rueckert, Martin J. Menten
- **Comment**: Submitted to MICCAI2023
- **Journal**: None
- **Summary**: Age-related macular degeneration (AMD) is the leading cause of blindness in the elderly. Current grading systems based on imaging biomarkers only coarsely group disease stages into broad categories and are unable to predict future disease progression. It is widely believed that this is due to their focus on a single point in time, disregarding the dynamic nature of the disease. In this work, we present the first method to automatically discover biomarkers that capture temporal dynamics of disease progression. Our method represents patient time series as trajectories in a latent feature space built with contrastive learning. Then, individual trajectories are partitioned into atomic sub-sequences that encode transitions between disease states. These are clustered using a newly introduced distance metric. In quantitative experiments we found our method yields temporal biomarkers that are predictive of conversion to late AMD. Furthermore, these clusters were highly interpretable to ophthalmologists who confirmed that many of the clusters represent dynamics that have previously been linked to the progression of AMD, even though they are currently not included in any clinical grading system.



### AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers
- **Arxiv ID**: http://arxiv.org/abs/2301.04545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04545v1)
- **Published**: 2023-01-11 16:14:12+00:00
- **Updated**: 2023-01-11 16:14:12+00:00
- **Authors**: Xumin Yu, Yongming Rao, Ziyi Wang, Jiwen Lu, Jie Zhou
- **Comment**: Extension of our ICCV 2021 work: arXiv:2108.08839 . Code is available
  at https://github.com/yuxumin/PoinTr
- **Journal**: None
- **Summary**: In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr, which adopts a Transformer encoder-decoder architecture for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the input data to a sequence of point proxies and employ the Transformers for generation. To facilitate Transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of Transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Taking a step towards more complicated and diverse situations, we further propose AdaPoinTr by developing an adaptive query generation mechanism and designing a novel denoising task during completing a point cloud. Coupling these two techniques enables us to train the model efficiently and effectively: we reduce training time (by 15x or more) and improve completion performance (over 20%). We also show our method can be extended to the scene-level point cloud completion scenario by designing a new geometry-enhanced semantic scene completion framework. Extensive experiments on the existing and newly-proposed datasets demonstrate the effectiveness of our method, which attains 6.53 CD on PCN, 0.81 CD on ShapeNet-55 and 0.392 MMD on real-world KITTI, surpassing other work by a large margin and establishing new state-of-the-arts on various benchmarks. Most notably, AdaPoinTr can achieve such promising performance with higher throughputs and fewer FLOPs compared with the previous best methods in practice. The code and datasets are available at https://github.com/yuxumin/PoinTr



### Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis
- **Arxiv ID**: http://arxiv.org/abs/2301.04554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04554v1)
- **Published**: 2023-01-11 16:31:38+00:00
- **Updated**: 2023-01-11 16:31:38+00:00
- **Authors**: Wei Guo, Benedetta Tondi, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Universal Defence based on Clustering and Centroids Analysis (CCA-UD) against backdoor attacks. The goal of the proposed defence is to reveal whether a Deep Neural Network model is subject to a backdoor attack by inspecting the training dataset. CCA-UD first clusters the samples of the training set by means of density-based clustering. Then, it applies a novel strategy to detect the presence of poisoned clusters. The proposed strategy is based on a general misclassification behaviour obtained when the features of a representative example of the analysed cluster are added to benign samples. The capability of inducing a misclassification error is a general characteristic of poisoned samples, hence the proposed defence is attack-agnostic. This mask a significant difference with respect to existing defences, that, either can defend against only some types of backdoor attacks, e.g., when the attacker corrupts the label of the poisoned samples, or are effective only when some conditions on the poisoning ratios adopted by the attacker or the kind of triggering pattern used by the attacker are satisfied. Experiments carried out on several classification tasks, considering different types of backdoor attacks and triggering patterns, including both local and global triggers, reveal that the proposed method is very effective to defend against backdoor attacks in all the cases, always outperforming the state of the art techniques.



### Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing
- **Arxiv ID**: http://arxiv.org/abs/2301.04558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2301.04558v2)
- **Published**: 2023-01-11 16:35:33+00:00
- **Updated**: 2023-03-16 17:12:03+00:00
- **Authors**: Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Pérez-García, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, Ozan Oktay
- **Comment**: To appear in CVPR 2023
- **Journal**: None
- **Summary**: Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks. We release a novel multi-modal temporal benchmark dataset, MS-CXR-T, to quantify the quality of vision-language representations in terms of temporal semantics. Our experimental results show the advantages of incorporating prior images and reports to make most use of the data.



### Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2301.04581v1
- **DOI**: 10.1109/TGRS.2023.3266477
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04581v1)
- **Published**: 2023-01-11 17:20:30+00:00
- **Updated**: 2023-01-11 17:20:30+00:00
- **Authors**: Yongqiang Mao, Kaiqiang Chen, Liangjin Zhao, Wei Chen, Deke Tang, Wenjie Liu, Zhirui Wang, Wenhui Diao, Xian Sun, Kun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Building 3D reconstruction from remote sensing images has a wide range of applications in smart cities, photogrammetry and other fields. Methods for automatic 3D urban building modeling typically employ multi-view images as input to algorithms to recover point clouds and 3D models of buildings. However, such models rely heavily on multi-view images of buildings, which are time-intensive and limit the applicability and practicality of the models. To solve these issues, we focus on designing an efficient DSM estimation-driven reconstruction framework (Building3D), which aims to reconstruct 3D building models from the input single-view remote sensing image. First, we propose a Semantic Flow Field-guided DSM Estimation (SFFDE) network, which utilizes the proposed concept of elevation semantic flow to achieve the registration of local and global features. Specifically, in order to make the network semantics globally aware, we propose an Elevation Semantic Globalization (ESG) module to realize the semantic globalization of instances. Further, in order to alleviate the semantic span of global features and original local features, we propose a Local-to-Global Elevation Semantic Registration (L2G-ESR) module based on elevation semantic flow. Our Building3D is rooted in the SFFDE network for building elevation prediction, synchronized with a building extraction network for building masks, and then sequentially performs point cloud reconstruction, surface reconstruction (or CityGML model reconstruction). On this basis, our Building3D can optionally generate CityGML models or surface mesh models of the buildings. Extensive experiments on ISPRS Vaihingen and DFC2019 datasets on the DSM estimation task show that our SFFDE significantly improves upon state-of-the-arts. Furthermore, our Building3D achieves impressive results in the 3D point cloud and 3D model reconstruction process.



### Continual Few-Shot Learning Using HyperTransformers
- **Arxiv ID**: http://arxiv.org/abs/2301.04584v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04584v2)
- **Published**: 2023-01-11 17:27:47+00:00
- **Updated**: 2023-01-12 19:44:13+00:00
- **Authors**: Max Vladymyrov, Andrey Zhmoginov, Mark Sandler
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the problem of learning without forgetting from multiple tasks arriving sequentially, where each task is defined using a few-shot episode of novel or already seen classes. We approach this problem using the recently published HyperTransformer (HT), a Transformer-based hypernetwork that generates specialized task-specific CNN weights directly from the support set. In order to learn from a continual sequence of tasks, we propose to recursively re-use the generated weights as input to the HT for the next task. This way, the generated CNN weights themselves act as a representation of previously learned tasks, and the HT is trained to update these weights so that the new task can be learned without forgetting past tasks. This approach is different from most continual learning algorithms that typically rely on using replay buffers, weight regularization or task-dependent architectural changes. We demonstrate that our proposed Continual HyperTransformer method equipped with a prototypical loss is capable of learning and retaining knowledge about past tasks for a variety of scenarios, including learning from mini-batches, and task-incremental and class-incremental learning scenarios.



### LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.04604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04604v1)
- **Published**: 2023-01-11 17:56:36+00:00
- **Updated**: 2023-01-11 17:56:36+00:00
- **Authors**: Jiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli Zhao, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents an easy-to-use regularizer for GAN training, which helps explicitly link some axes of the latent space to an image region or a semantic category (e.g., sky) in the synthesis. Establishing such a connection facilitates a more convenient local control of GAN generation, where users can alter image content only within a spatial area simply by partially resampling the latent codes. Experimental results confirm four appealing properties of our regularizer, which we call LinkGAN. (1) Any image region can be linked to the latent space, even if the region is pre-selected before training and fixed for all instances. (2) Two or multiple regions can be independently linked to different latent axes, surprisingly allowing tokenized control of synthesized images. (3) Our regularizer can improve the spatial controllability of both 2D and 3D GAN models, barely sacrificing the synthesis performance. (4) The models trained with our regularizer are compatible with GAN inversion techniques and maintain editability on real images



### Online Hyperparameter Optimization for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.05032v2
- **DOI**: 10.1609/aaai.v37i7.26070
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.05032v2)
- **Published**: 2023-01-11 17:58:51+00:00
- **Updated**: 2023-05-03 20:44:36+00:00
- **Authors**: Yaoyao Liu, Yingying Li, Bernt Schiele, Qianru Sun
- **Comment**: AAAI 2023 Oral. Code is available at
  https://class-il.mpi-inf.mpg.de/online/code/
- **Journal**: None
- **Summary**: Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings--where typically the training-from-half (TFH) setting needs more stability, but the training-from-scratch (TFS) needs more plasticity. To this end, we design an online learning method that can adaptively optimize the tradeoff without knowing the setting as a priori. Specifically, we first introduce the key hyperparameters that influence the trade-off, e.g., knowledge distillation (KD) loss weights, learning rates, and classifier types. Then, we formulate the hyperparameter optimization process as an online Markov Decision Process (MDP) problem and propose a specific algorithm to solve it. We apply local estimated rewards and a classic bandit algorithm Exp3 to address the issues when applying online MDP methods to the CIL protocol. Our method consistently improves top-performing CIL methods in both TFH and TFS settings, e.g., boosting the average accuracy of TFH and TFS by 2.2 percentage points on ImageNet-Full, compared to the state-of-the-art.



### MotorFactory: A Blender Add-on for Large Dataset Generation of Small Electric Motors
- **Arxiv ID**: http://arxiv.org/abs/2301.05028v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.05028v1)
- **Published**: 2023-01-11 18:03:24+00:00
- **Updated**: 2023-01-11 18:03:24+00:00
- **Authors**: Chengzhi Wu, Kanran Zhou, Jan-Philipp Kaiser, Norbert Mitschke, Jan-Felix Klein, Julius Pfrommer, Jürgen Beyerer, Gisela Lanza, Michael Heizmann, Kai Furmans
- **Comment**: None
- **Journal**: None
- **Summary**: To enable automatic disassembly of different product types with uncertain conditions and degrees of wear in remanufacturing, agile production systems that can adapt dynamically to changing requirements are needed. Machine learning algorithms can be employed due to their generalization capabilities of learning from various types and variants of products. However, in reality, datasets with a diversity of samples that can be used to train models are difficult to obtain in the initial period. This may cause bad performances when the system tries to adapt to new unseen input data in the future. In order to generate large datasets for different learning purposes, in our project, we present a Blender add-on named MotorFactory to generate customized mesh models of various motor instances. MotorFactory allows to create mesh models which, complemented with additional add-ons, can be further used to create synthetic RGB images, depth images, normal images, segmentation ground truth masks, and 3D point cloud datasets with point-wise semantic labels. The created synthetic datasets may be used for various tasks including motor type classification, object detection for decentralized material transfer tasks, part segmentation for disassembly and handling tasks, or even reinforcement learning-based robotics control or view-planning.



### Padding Module: Learning the Padding in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.04608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.04608v1)
- **Published**: 2023-01-11 18:03:57+00:00
- **Updated**: 2023-01-11 18:03:57+00:00
- **Authors**: Fahad Alrasheedi, Xin Zhong, Pei-Chi Huang
- **Comment**: This paper has been accepted for publication by the IEEE Access
- **Journal**: None
- **Summary**: During the last decades, many studies have been dedicated to improving the performance of neural networks, for example, the network architectures, initialization, and activation. However, investigating the importance and effects of learnable padding methods in deep learning remains relatively open. To mitigate the gap, this paper proposes a novel trainable Padding Module that can be placed in a deep learning model. The Padding Module can optimize itself without requiring or influencing the model's entire loss function. To train itself, the Padding Module constructs a ground truth and a predictor from the inputs by leveraging the underlying structure in the input data for supervision. As a result, the Padding Module can learn automatically to pad pixels to the border of its input images or feature maps. The padding contents are realistic extensions to its input data and simultaneously facilitate the deep learning model's downstream task. Experiments have shown that the proposed Padding Module outperforms the state-of-the-art competitors and the baseline methods. For example, the Padding Module has 1.23% and 0.44% more classification accuracy than the zero padding when tested on the VGG16 and ResNet50.



### Generative-Contrastive Learning for Self-Supervised Latent Representations of 3D Shapes from Multi-Modal Euclidean Input
- **Arxiv ID**: http://arxiv.org/abs/2301.04612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04612v1)
- **Published**: 2023-01-11 18:14:24+00:00
- **Updated**: 2023-01-11 18:14:24+00:00
- **Authors**: Chengzhi Wu, Julius Pfrommer, Mingyuan Zhou, Jürgen Beyerer
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a combined generative and contrastive neural architecture for learning latent representations of 3D volumetric shapes. The architecture uses two encoder branches for voxel grids and multi-view images from the same underlying shape. The main idea is to combine a contrastive loss between the resulting latent representations with an additional reconstruction loss. That helps to avoid collapsing the latent representations as a trivial solution for minimizing the contrastive loss. A novel switching scheme is used to cross-train two encoders with a shared decoder. The switching scheme also enables the stop gradient operation on a random branch. Further classification experiments show that the latent representations learned with our self-supervised method integrate more useful information from the additional input data implicitly, thus leading to better reconstruction and classification performance.



### Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding
- **Arxiv ID**: http://arxiv.org/abs/2301.04613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04613v1)
- **Published**: 2023-01-11 18:14:47+00:00
- **Updated**: 2023-01-11 18:14:47+00:00
- **Authors**: Chengzhi Wu, Julius Pfrommer, Jürgen Beyerer, Kangning Li, Boris Neubert
- **Comment**: None
- **Journal**: None
- **Summary**: We present an improved approach for 3D object detection in point cloud data based on the Frustum PointNet (F-PointNet). Compared to the original F-PointNet, our newly proposed method considers the point neighborhood when computing point features. The newly introduced local neighborhood embedding operation mimics the convolutional operations in 2D neural networks. Thus features of each point are not only computed with the features of its own or of the whole point cloud but also computed especially with respect to the features of its neighbors. Experiments show that our proposed method achieves better performance than the F-Pointnet baseline on 3D object detection tasks.



### TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation
- **Arxiv ID**: http://arxiv.org/abs/2301.04619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04619v1)
- **Published**: 2023-01-11 18:20:19+00:00
- **Updated**: 2023-01-11 18:20:19+00:00
- **Authors**: Feiyan Hu, Simone Palazzo, Federica Proietto Salanitri, Giovanni Bellitto, Morteza Moradi, Concetto Spampinato, Kevin McGuinness
- **Comment**: WACV2023
- **Journal**: None
- **Summary**: Video saliency prediction has recently attracted attention of the research community, as it is an upstream task for several practical applications. However, current solutions are particularly computationally demanding, especially due to the wide usage of spatio-temporal 3D convolutions. We observe that, while different model architectures achieve similar performance on benchmarks, visual variations between predicted saliency maps are still significant. Inspired by this intuition, we propose a lightweight model that employs multiple simple heterogeneous decoders and adopts several practical approaches to improve accuracy while keeping computational costs low, such as hierarchical multi-map knowledge distillation, multi-output saliency prediction, unlabeled auxiliary datasets and channel reduction with teacher assistant supervision. Our approach achieves saliency prediction accuracy on par or better than state-of-the-art methods on DFH1K, UCF-Sports and Hollywood2 benchmarks, while enhancing significantly the efficiency of the model. Code is on https://github.com/feiyanhu/tinyHD



### Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication
- **Arxiv ID**: http://arxiv.org/abs/2301.04623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04623v1)
- **Published**: 2023-01-11 18:24:07+00:00
- **Updated**: 2023-01-11 18:24:07+00:00
- **Authors**: Nazmul Shahadat, Anthony S. Maida
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many deep networks have introduced hypercomplex and related calculations into their architectures. In regard to convolutional networks for classification, these enhancements have been applied to the convolution operations in the frontend to enhance accuracy and/or reduce the parameter requirements while maintaining accuracy. Although these enhancements have been applied to the convolutional frontend, it has not been studied whether adding hypercomplex calculations improves performance when applied to the densely connected backend. This paper studies ResNet architectures and incorporates parameterized hypercomplex multiplication (PHM) into the backend of residual, quaternion, and vectormap convolutional neural networks to assess the effect. We show that PHM does improve classification accuracy performance on several image datasets, including small, low-resolution CIFAR 10/100 and large high-resolution ImageNet and ASL, and can achieve state-of-the-art accuracy for hypercomplex networks.



### SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.05027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.05027v1)
- **Published**: 2023-01-11 18:27:29+00:00
- **Updated**: 2023-01-11 18:27:29+00:00
- **Authors**: Chengzhi Wu, Linxi Qiu, Kanran Zhou, Julius Pfrommer, Jürgen Beyerer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a novel benchmark suite including both a 2D synthetic image dataset and a 3D synthetic point cloud dataset. Our work is a sub-task in the framework of a remanufacturing project, in which small electric motors are used as fundamental objects. Apart from the given detection, classification, and segmentation annotations, the key objects also have multiple learnable attributes with ground truth provided. This benchmark can be used for computer vision tasks including 2D/3D detection, classification, segmentation, and multi-attribute learning. It is worth mentioning that most attributes of the motors are quantified as continuously variable rather than binary, which makes our benchmark well-suited for the less explored regression tasks. In addition, appropriate evaluation metrics are adopted or developed for each task and promising baseline results are provided. We hope this benchmark can stimulate more research efforts on the sub-domain of object attribute learning and multi-task learning in the future.



### Deep Axial Hypercomplex Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.04626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04626v1)
- **Published**: 2023-01-11 18:31:00+00:00
- **Updated**: 2023-01-11 18:31:00+00:00
- **Authors**: Nazmul Shahadat, Anthony S. Maida
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decade, deep hypercomplex-inspired networks have enhanced feature extraction for image classification by enabling weight sharing across input channels. Recent works make it possible to improve representational capabilities by using hypercomplex-inspired networks which consume high computational costs. This paper reduces this cost by factorizing a quaternion 2D convolutional module into two consecutive vectormap 1D convolutional modules. Also, we use 5D parameterized hypercomplex multiplication based fully connected layers. Incorporating both yields our proposed hypercomplex network, a novel architecture that can be assembled to construct deep axial-hypercomplex networks (DANs) for image classifications. We conduct experiments on CIFAR benchmarks, SVHN, and Tiny ImageNet datasets and achieve better performance with fewer trainable parameters and FLOPS. Our proposed model achieves almost 2% higher performance for CIFAR and SVHN datasets, and more than 3% for the ImageNet-Tiny dataset and takes six times fewer parameters than the real-valued ResNets. Also, it shows state-of-the-art performance on CIFAR benchmarks in hypercomplex space.



### Face Attribute Editing with Disentangled Latent Vectors
- **Arxiv ID**: http://arxiv.org/abs/2301.04628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04628v1)
- **Published**: 2023-01-11 18:32:13+00:00
- **Updated**: 2023-01-11 18:32:13+00:00
- **Authors**: Yusuf Dalva, Hamza Pehlivan, Cansu Moran, Öykü Irmak Hatipoğlu, Ayşegül Dündar
- **Comment**: See https://yusufdalva.github.io/vecgan for the project webpage.
  arXiv admin note: substantial text overlap with arXiv:2207.03411
- **Journal**: None
- **Summary**: We propose an image-to-image translation framework for facial attribute editing with disentangled interpretable latent directions. Facial attribute editing task faces the challenges of targeted attribute editing with controllable strength and disentanglement in the representations of attributes to preserve the other attributes during edits. For this goal, inspired by the latent space factorization works of fixed pretrained GANs, we design the attribute editing by latent space factorization, and for each attribute, we learn a linear direction that is orthogonal to the others. We train these directions with orthogonality constraints and disentanglement losses. To project images to semantically organized latent spaces, we set an encoder-decoder architecture with attention-based skip connections. We extensively compare with previous image translation algorithms and editing with pretrained GAN works. Our extensive experiments show that our method significantly improves over the state-of-the-arts. Project page: https://yusufdalva.github.io/vecgan



### ShadowNav: Crater-Based Localization for Nighttime and Permanently Shadowed Region Lunar Navigation
- **Arxiv ID**: http://arxiv.org/abs/2301.04630v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04630v1)
- **Published**: 2023-01-11 18:35:31+00:00
- **Updated**: 2023-01-11 18:35:31+00:00
- **Authors**: Abhishek Cauligi, R. Michael Swan, Masahiro Ono, Shreyansh Daftry, John Elliott, Larry Matthies, Deegan Atha
- **Comment**: IEEE Aerospace Conference 2023
- **Journal**: None
- **Summary**: There has been an increase in interest in missions that drive significantly longer distances per day than what has currently been performed. Further, some of these proposed missions require autonomous driving and absolute localization in darkness. For example, the Endurance A mission proposes to drive 1200km of its total traverse at night. The lack of natural light available during such missions limits what can be used as visual landmarks and the range at which landmarks can be observed. In order for planetary rovers to traverse long ranges, onboard absolute localization is critical to the ability of the rover to maintain its planned trajectory and avoid known hazardous regions. Currently, to accomplish absolute localization, a ground in the loop (GITL) operation is performed wherein a human operator matches local maps or images from onboard with orbital images and maps. This GITL operation limits the distance that can be driven in a day to a few hundred meters, which is the distance that the rover can maintain acceptable localization error via relative methods. Previous work has shown that using craters as landmarks is a promising approach for performing absolute localization on the moon during the day. In this work we present a method of absolute localization that utilizes craters as landmarks and matches detected crater edges on the surface with known craters in orbital maps. We focus on a localization method based on a perception system which has an external illuminator and a stereo camera. We evaluate (1) both monocular and stereo based surface crater edge detection techniques, (2) methods of scoring the crater edge matches for optimal localization, and (3) localization performance on simulated Lunar surface imagery at night. We demonstrate that this technique shows promise for maintaining absolute localization error of less than 10m required for most planetary rover missions.



### Deep Residual Axial Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.04631v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04631v2)
- **Published**: 2023-01-11 18:36:54+00:00
- **Updated**: 2023-03-18 01:48:48+00:00
- **Authors**: Nazmul Shahadat, Anthony S. Maida
- **Comment**: None
- **Journal**: None
- **Summary**: While convolutional neural networks (CNNs) demonstrate outstanding performance on computer vision tasks, their computational costs remain high. Several techniques are used to reduce these costs, like reducing channel count, and using separable and depthwise separable convolutions. This paper reduces computational costs by introducing a novel architecture, axial CNNs, which replaces spatial 2D convolution operations with two consecutive depthwise separable 1D operations. The axial CNNs are predicated on the assumption that the dataset supports approximately separable convolution operations with little or no loss of training accuracy. Deep axial separable CNNs still suffer from gradient problems when training deep networks. We modify the construction of axial separable CNNs with residual connections to improve the performance of deep axial architectures and introduce our final novel architecture namely residual axial networks (RANs). Extensive benchmark evaluation shows that RANs achieve at least 1% higher performance with about 77%, 86%, 75%, and 34% fewer parameters and about 75%, 80%, 67%, and 26% fewer flops than ResNets, wide ResNets, MobileNets, and SqueezeNexts on CIFAR benchmarks, SVHN, and Tiny ImageNet image classification datasets. Moreover, our proposed RANs improve deep recursive residual networks performance with 94% fewer parameters on the image super-resolution dataset.



### Street-View Image Generation from a Bird's-Eye View Layout
- **Arxiv ID**: http://arxiv.org/abs/2301.04634v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04634v3)
- **Published**: 2023-01-11 18:39:34+00:00
- **Updated**: 2023-08-23 23:40:59+00:00
- **Authors**: Alexander Swerdlow, Runsheng Xu, Bolei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Bird's-Eye View (BEV) Perception has received increasing attention in recent years as it provides a concise and unified spatial representation across views and benefits a diverse set of downstream driving applications. While the focus has been placed on discriminative tasks such as BEV segmentation, the dual generative task of creating street-view images from a BEV layout has rarely been explored. The ability to generate realistic street-view images that align with a given HD map and traffic layout is critical for visualizing complex traffic scenarios and developing robust perception models for autonomous driving. In this paper, we propose BEVGen, a conditional generative model that synthesizes a set of realistic and spatially consistent surrounding images that match the BEV layout of a traffic scenario. BEVGen incorporates a novel cross-view transformation and spatial attention design which learn the relationship between cameras and map views to ensure their consistency. Our model can accurately render road and lane lines, as well as generate traffic scenes under different weather conditions and times of day. The code will be made publicly available.



### Does progress on ImageNet transfer to real-world datasets?
- **Arxiv ID**: http://arxiv.org/abs/2301.04644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04644v1)
- **Published**: 2023-01-11 18:55:53+00:00
- **Updated**: 2023-01-11 18:55:53+00:00
- **Authors**: Alex Fang, Simon Kornblith, Ludwig Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.



### EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata
- **Arxiv ID**: http://arxiv.org/abs/2301.04647v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2301.04647v4)
- **Published**: 2023-01-11 18:59:16+00:00
- **Updated**: 2023-06-17 12:46:01+00:00
- **Authors**: Chenhao Zheng, Ayush Shrivastava, Andrew Owens
- **Comment**: CVPR 2023 (Highlight). Project link:
  http://hellomuffin.github.io/exif-as-language
- **Journal**: None
- **Summary**: We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.



### Head-Free Lightweight Semantic Segmentation with Linear Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.04648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04648v1)
- **Published**: 2023-01-11 18:59:46+00:00
- **Updated**: 2023-01-11 18:59:46+00:00
- **Authors**: Bo Dong, Pichao Wang, Fan Wang
- **Comment**: Accepted by AAAI2023; codes and models are available at
  https://github.com/dongbo811/AFFormer
- **Journal**: None
- **Summary**: Existing semantic segmentation works have been mainly focused on designing effective decoders; however, the computational load introduced by the overall structure has long been ignored, which hinders their applications on resource-constrained hardwares. In this paper, we propose a head-free lightweight architecture specifically for semantic segmentation, named Adaptive Frequency Transformer. It adopts a parallel architecture to leverage prototype representations as specific learnable local descriptions which replaces the decoder and preserves the rich image semantics on high-resolution features. Although removing the decoder compresses most of the computation, the accuracy of the parallel structure is still limited by low computational resources. Therefore, we employ heterogeneous operators (CNN and Vision Transformer) for pixel embedding and prototype representations to further save computational costs. Moreover, it is very difficult to linearize the complexity of the vision Transformer from the perspective of spatial domain. Due to the fact that semantic segmentation is very sensitive to frequency information, we construct a lightweight prototype learning block with adaptive frequency filter of complexity $O(n)$ to replace standard self attention with $O(n^{2})$. Extensive experiments on widely adopted datasets demonstrate that our model achieves superior accuracy while retaining only 3M parameters. On the ADE20K dataset, our model achieves 41.8 mIoU and 4.6 GFLOPs, which is 4.4 mIoU higher than Segformer, with 45% less GFLOPs. On the Cityscapes dataset, our model achieves 78.7 mIoU and 34.4 GFLOPs, which is 2.5 mIoU higher than Segformer with 72.5% less GFLOPs. Code is available at https://github.com/dongbo811/AFFormer.



### Geometry-biased Transformers for Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.04650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04650v1)
- **Published**: 2023-01-11 18:59:56+00:00
- **Updated**: 2023-01-11 18:59:56+00:00
- **Authors**: Naveen Venkat, Mayank Agarwal, Maneesh Singh, Shubham Tulsiani
- **Comment**: Project page: https://mayankgrwl97.github.io/gbt
- **Journal**: None
- **Summary**: We tackle the task of synthesizing novel views of an object given a few input images and associated camera viewpoints. Our work is inspired by recent 'geometry-free' approaches where multi-view images are encoded as a (global) set-latent representation, which is then used to predict the color for arbitrary query rays. While this representation yields (coarsely) accurate images corresponding to novel viewpoints, the lack of geometric reasoning limits the quality of these outputs. To overcome this limitation, we propose 'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive biases in the set-latent representation-based inference to encourage multi-view geometric consistency. We induce the geometric bias by augmenting the dot-product attention mechanism to also incorporate 3D distances between rays associated with tokens as a learnable bias. We find that this, along with camera-aware embeddings as input, allows our models to generate significantly more accurate outputs. We validate our approach on the real-world CO3D dataset, where we train our system over 10 categories and evaluate its view-synthesis ability for novel objects as well as unseen categories. We empirically validate the benefits of the proposed geometric biases and show that our approach significantly improves over prior works.



### SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2301.04685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04685v1)
- **Published**: 2023-01-11 19:24:03+00:00
- **Updated**: 2023-01-11 19:24:03+00:00
- **Authors**: Seokbeom Song, Suhyeon Lee, Hongje Seong, Kyoungwon Min, Euntai Kim
- **Comment**: Accepted to AAAI 2023
- **Journal**: None
- **Summary**: We propose a novel solution for unpaired image-to-image (I2I) translation. To translate complex images with a wide range of objects to a different domain, recent approaches often use the object annotations to perform per-class source-to-target style mapping. However, there remains a point for us to exploit in the I2I. An object in each class consists of multiple components, and all the sub-object components have different characteristics. For example, a car in CAR class consists of a car body, tires, windows and head and tail lamps, etc., and they should be handled separately for realistic I2I translation. The simplest solution to the problem will be to use more detailed annotations with sub-object component annotations than the simple object annotations, but it is not possible. The key idea of this paper is to bypass the sub-object component annotations by leveraging the original style of the input image because the original style will include the information about the characteristics of the sub-object components. Specifically, for each pixel, we use not only the per-class style gap between the source and target domains but also the pixel's original style to determine the target style of a pixel. To this end, we present Style Harmonization for unpaired I2I translation (SHUNIT). Our SHUNIT generates a new style by harmonizing the target domain style retrieved from a class memory and an original source image style. Instead of direct source-to-target style mapping, we aim for source and target styles harmonization. We validate our method with extensive experiments and achieve state-of-the-art performance on the latest benchmark sets. The source code is available online: https://github.com/bluejangbaljang/SHUNIT.



### Learning Continuous Mesh Representation with Spherical Implicit Surface
- **Arxiv ID**: http://arxiv.org/abs/2301.04695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04695v1)
- **Published**: 2023-01-11 20:00:17+00:00
- **Updated**: 2023-01-11 20:00:17+00:00
- **Authors**: Zhongpai Gao
- **Comment**: https://github.com/Gaozhongpai/SIS-Implicit
- **Journal**: None
- **Summary**: As the most common representation for 3D shapes, mesh is often stored discretely with arrays of vertices and faces. However, 3D shapes in the real world are presented continuously. In this paper, we propose to learn a continuous representation for meshes with fixed topology, a common and practical setting in many faces-, hand-, and body-related applications. First, we split the template into multiple closed manifold genus-0 meshes so that each genus-0 mesh can be parameterized onto the unit sphere. Then we learn spherical implicit surface (SIS), which takes a spherical coordinate and a global feature or a set of local features around the coordinate as inputs, predicting the vertex corresponding to the coordinate as an output. Since the spherical coordinates are continuous, SIS can depict a mesh in an arbitrary resolution. SIS representation builds a bridge between discrete and continuous representation in 3D shapes. Specifically, we train SIS networks in a self-supervised manner for two tasks: a reconstruction task and a super-resolution task. Experiments show that our SIS representation is comparable with state-of-the-art methods that are specifically designed for meshes with a fixed resolution and significantly outperforms methods that work in arbitrary resolutions.



### Inverse Quantum Fourier Transform Inspired Algorithm for Unsupervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.04705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, quant-ph, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2301.04705v1)
- **Published**: 2023-01-11 20:28:44+00:00
- **Updated**: 2023-01-11 20:28:44+00:00
- **Authors**: Taoreed Akinola, Xiangfang Li, Richard Wilkins, Pamela Obiomon, Lijun Qian
- **Comment**: 8 pages, 10 figures, conference
- **Journal**: None
- **Summary**: Image segmentation is a very popular and important task in computer vision. In this paper, inverse quantum Fourier transform (IQFT) for image segmentation has been explored and a novel IQFT-inspired algorithm is proposed and implemented by leveraging the underlying mathematical structure of the IQFT. Specifically, the proposed method takes advantage of the phase information of the pixels in the image by encoding the pixels' intensity into qubit relative phases and applying IQFT to classify the pixels into different segments automatically and efficiently. To the best of our knowledge, this is the first attempt of using IQFT for unsupervised image segmentation. The proposed method has low computational cost comparing to the deep learning-based methods and more importantly it does not require training, thus make it suitable for real-time applications. The performance of the proposed method is compared with K-means and Otsu-thresholding. The proposed method outperforms both of them on the PASCAL VOC 2012 segmentation benchmark and the xVIEW2 challenge dataset by as much as 50% in terms of mean Intersection-Over-Union (mIOU).



### Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks
- **Arxiv ID**: http://arxiv.org/abs/2302.05330v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.05330v1)
- **Published**: 2023-01-11 21:44:37+00:00
- **Updated**: 2023-01-11 21:44:37+00:00
- **Authors**: Weichao Mao, Ruta Desai, Michael Louis Iuzzolino, Nitin Kamra
- **Comment**: AAAI 2023 Workshop on User-Centric Artificial Intelligence for
  Assistance in At-Home Tasks
- **Journal**: None
- **Summary**: Given video demonstrations and paired narrations of an at-home procedural task such as changing a tire, we present an approach to extract the underlying task structure -- relevant actions and their temporal dependencies -- via action-centric task graphs. Learnt structured representations from our method, Action Dynamics Task Graphs (ADTG), can then be used for understanding such tasks in unseen videos of humans performing them. Furthermore, ADTG can enable providing user-centric guidance to humans in these tasks, either for performing them better or for learning new tasks. Specifically, we show how ADTG can be used for: (1) tracking an ongoing task, (2) recommending next actions, and (3) planning a sequence of actions to accomplish a procedural task. We compare against state-of-the-art Neural Task Graph method and demonstrate substantial gains on 18 procedural tasks from the CrossTask dataset, including 30.1% improvement in task tracking accuracy and 20.3% accuracy gain in next action prediction.



### AGMN: Association Graph-based Graph Matching Network for Coronary Artery Semantic Labeling on Invasive Coronary Angiograms
- **Arxiv ID**: http://arxiv.org/abs/2301.04733v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.04733v1)
- **Published**: 2023-01-11 21:54:28+00:00
- **Updated**: 2023-01-11 21:54:28+00:00
- **Authors**: Chen Zhao, Zhihui Xu, Jingfeng Jiang, Michele Esposito, Drew Pienta, Guang-Uei Hung, Weihua Zhou
- **Comment**: 26 pages, 7 figures
- **Journal**: None
- **Summary**: Semantic labeling of coronary arterial segments in invasive coronary angiography (ICA) is important for automated assessment and report generation of coronary artery stenosis in the computer-aided diagnosis of coronary artery disease (CAD). Inspired by the training procedure of interventional cardiologists for interpreting the structure of coronary arteries, we propose an association graph-based graph matching network (AGMN) for coronary arterial semantic labeling. We first extract the vascular tree from invasive coronary angiography (ICA) and convert it into multiple individual graphs. Then, an association graph is constructed from two individual graphs where each vertex represents the relationship between two arterial segments. Using the association graph, the AGMN extracts the vertex features by the embedding module, aggregates the features from adjacent vertices and edges by graph convolution network, and decodes the features to generate the semantic mappings between arteries. By learning the mapping of arterial branches between two individual graphs, the unlabeled arterial segments are classified by the labeled segments to achieve semantic labeling. A dataset containing 263 ICAs was employed to train and validate the proposed model, and a five-fold cross-validation scheme was performed. Our AGMN model achieved an average accuracy of 0.8264, an average precision of 0.8276, an average recall of 0.8264, and an average F1-score of 0.8262, which significantly outperformed existing coronary artery semantic labeling methods. In conclusion, we have developed and validated a new algorithm with high accuracy, interpretability, and robustness for coronary artery semantic labeling on ICAs.



### HADA: A Graph-based Amalgamation Framework in Image-text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2301.04742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04742v1)
- **Published**: 2023-01-11 22:25:20+00:00
- **Updated**: 2023-01-11 22:25:20+00:00
- **Authors**: Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin
- **Comment**: None
- **Journal**: None
- **Summary**: Many models have been proposed for vision and language tasks, especially the image-text retrieval task. All state-of-the-art (SOTA) models in this challenge contained hundreds of millions of parameters. They also were pretrained on a large external dataset that has been proven to make a big improvement in overall performance. It is not easy to propose a new model with a novel architecture and intensively train it on a massive dataset with many GPUs to surpass many SOTA models, which are already available to use on the Internet. In this paper, we proposed a compact graph-based framework, named HADA, which can combine pretrained models to produce a better result, rather than building from scratch. First, we created a graph structure in which the nodes were the features extracted from the pretrained models and the edges connecting them. The graph structure was employed to capture and fuse the information from every pretrained model with each other. Then a graph neural network was applied to update the connection between the nodes to get the representative embedding vector for an image and text. Finally, we used the cosine similarity to match images with their relevant texts and vice versa to ensure a low inference time. Our experiments showed that, although HADA contained a tiny number of trainable parameters, it could increase baseline performance by more than 3.6% in terms of evaluation metrics in the Flickr30k dataset. Additionally, the proposed model did not train on any external dataset and did not require many GPUs but only 1 to train due to its small number of parameters. The source code is available at https://github.com/m2man/HADA.



### Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in $\text{Parkinson's Disease}$
- **Arxiv ID**: http://arxiv.org/abs/2301.08141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.08141v1)
- **Published**: 2023-01-11 22:47:12+00:00
- **Updated**: 2023-01-11 22:47:12+00:00
- **Authors**: Fatemeh Haghighi, Soumitra Ghosh, Hai Ngu, Sarah Chu, Han Lin, Mohsen Hejrati, Baris Bingol, Somaye Hashemifar
- **Comment**: None
- **Journal**: None
- **Summary**: $\text{Parkinson's Disease}$ (PD) is the second most common neurodegenerative disease in humans. PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra (a part of the mid-brain). Counting the number of dopaminergic neurons in the Substantia Nigra is one of the most important indexes in evaluating drug efficacy in PD animal models. Currently, analyzing and quantifying dopaminergic neurons is conducted manually by experts through analysis of digital pathology images which is laborious, time-consuming, and highly subjective. As such, a reliable and unbiased automated system is demanded for the quantification of dopaminergic neurons in digital pathology images. We propose an end-to-end deep learning framework for the segmentation and quantification of dopaminergic neurons in PD animal models. To the best of knowledge, this is the first machine learning model that detects the cell body of dopaminergic neurons, counts the number of dopaminergic neurons and provides the phenotypic characteristics of individual dopaminergic neurons as a numerical output. Extensive experiments demonstrate the effectiveness of our model in quantifying neurons with a high precision, which can provide quicker turnaround for drug efficacy studies, better understanding of dopaminergic neuronal health status and unbiased results in PD pre-clinical research.



### Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.04746v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.04746v5)
- **Published**: 2023-01-11 22:55:05+00:00
- **Updated**: 2023-05-16 04:18:00+00:00
- **Authors**: Chi-Hang Suen, Eduardo Alonso
- **Comment**: In Berndt M\"uller (Ed.), Proceedings of AISB Convention 2023 (pp.
  69-75). AISB
- **Journal**: None
- **Summary**: To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to symmetry, as a small step towards artificial general intelligence.



### LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking
- **Arxiv ID**: http://arxiv.org/abs/2301.04748v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04748v2)
- **Published**: 2023-01-11 22:57:31+00:00
- **Updated**: 2023-01-31 12:58:19+00:00
- **Authors**: Zhihua Liu, Bin Yang, Yan Shen, Xuejun Ni, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate tracking of an anatomical landmark over time has been of high interests for disease assessment such as minimally invasive surgery and tumor radiation therapy. Ultrasound imaging is a promising modality benefiting from low-cost and real-time acquisition. However, generating a precise landmark tracklet is very challenging, as attempts can be easily distorted by different interference such as landmark deformation, visual ambiguity and partial observation. In this paper, we propose a long-short diffeomorphic motion network, which is a multi-task framework with a learnable deformation prior to search for the plausible deformation of landmark. Specifically, we design a novel diffeomorphism representation in both long and short temporal domains for delineating motion margins and reducing long-term cumulative tracking errors. To further mitigate local anatomical ambiguity, we propose an expectation maximisation motion alignment module to iteratively optimize both long and short deformation, aligning to the same directional and spatial representation. The proposed multi-task system can be trained in a weakly-supervised manner, which only requires few landmark annotations for tracking and zero annotation for long-short deformation learning. We conduct extensive experiments on two ultrasound landmark tracking datasets. Experimental results show that our proposed method can achieve better or competitive landmark tracking performance compared with other state-of-the-art tracking methods, with a strong generalization capability across different scanner types and different ultrasound modalities.



### Artificial Intelligence Generated Coins for Size Comparison
- **Arxiv ID**: http://arxiv.org/abs/2301.04751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.04751v1)
- **Published**: 2023-01-11 23:10:38+00:00
- **Updated**: 2023-01-11 23:10:38+00:00
- **Authors**: Gerald Artner
- **Comment**: None
- **Journal**: Mitteilungen der \"Osterreichischen Numismatischen Gesellschaft,
  vol. 62, no. 2, pp. 9-16, 2022
- **Summary**: Authors of scientific articles use coins in photographs as a size reference for objects. For this purpose, coins are placed next to objects when taking the photo. In this letter we propose a novel method that uses artificial intelligence (AI) generated images of coins to provide a size reference in photos. The newest generation is able to quickly generate realistic high-quality images from textual descriptions. With the proposed method no physical coin is required while taking photos. Coins can be added to photos that contain none. Furthermore, we show how the coin motif can be matched to the object.



