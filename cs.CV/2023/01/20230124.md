# Arxiv Papers in cs.CV on 2023-01-24
### GAN-Based Object Removal in High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2301.11726v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11726v1)
- **Published**: 2023-01-24 02:23:29+00:00
- **Updated**: 2023-01-24 02:23:29+00:00
- **Authors**: Hadi Mansourifar, Steven J. Simske
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite images often contain a significant level of sensitive data compared to ground-view images. That is why satellite images are more likely to be intentionally manipulated to hide specific objects and structures. GAN-based approaches have been employed to create forged images with two major problems: (i) adding a new object to the scene to hide a specific object or region may create unrealistic merging with surrounding areas; and (ii) using masks on color feature images has proven to be unsuccessful in GAN-based object removal. In this paper, we tackle the problem of object removal in high-resolution satellite images given a limited number of training data. Furthermore, we take advantage of conditional GANs (CGANs) to collect perhaps the first GAN-based forged satellite image data set. All forged instances were manipulated via CGANs trained by Canny Feature Images for object removal. As part of our experiments, we demonstrate that distinguishing the collected forged images from authentic (original) images is highly challenging for fake image detector models.



### LDMIC: Learning-based Distributed Multi-view Image Coding
- **Arxiv ID**: http://arxiv.org/abs/2301.09799v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2301.09799v3)
- **Published**: 2023-01-24 03:47:37+00:00
- **Updated**: 2023-04-12 14:57:13+00:00
- **Authors**: Xinjie Zhang, Jiawei Shao, Jun Zhang
- **Comment**: Accepted by ICLR 2023
- **Journal**: None
- **Summary**: Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relationships between images. Experimental results show that LDMIC significantly outperforms both traditional and learning-based MIC methods while enjoying fast encoding speed. Code will be released at https://github.com/Xinjie-Q/LDMIC.



### RD-NAS: Enhancing One-shot Supernet Ranking Ability via Ranking Distillation from Zero-cost Proxies
- **Arxiv ID**: http://arxiv.org/abs/2301.09850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09850v1)
- **Published**: 2023-01-24 07:49:04+00:00
- **Updated**: 2023-01-24 07:49:04+00:00
- **Authors**: Peijie Dong, Xin Niu, Lujun Li, Zhiliang Tian, Xiaodong Wang, Zimian Wei, Hengyue Pan, Dongsheng Li
- **Comment**: 6 pages, 2 figures, 4 tables, ICASSP 2023
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has made tremendous progress in the automatic design of effective neural network structures but suffers from a heavy computational burden. One-shot NAS significantly alleviates the burden through weight sharing and improves computational efficiency. Zero-shot NAS further reduces the cost by predicting the performance of the network from its initial state, which conducts no training. Both methods aim to distinguish between "good" and "bad" architectures, i.e., ranking consistency of predicted and true performance. In this paper, we propose Ranking Distillation one-shot NAS (RD-NAS) to enhance ranking consistency, which utilizes zero-cost proxies as the cheap teacher and adopts the margin ranking loss to distill the ranking knowledge. Specifically, we propose a margin subnet sampler to distill the ranking knowledge from zero-shot NAS to one-shot NAS by introducing Group distance as margin. Our evaluation of the NAS-Bench-201 and ResNet-based search space demonstrates that RD-NAS achieve 10.7\% and 9.65\% improvements in ranking ability, respectively. Our codes are available at https://github.com/pprp/CVPR2022-NAS-competition-Track1-3th-solution



### PowerQuant: Automorphism Search for Non-Uniform Quantization
- **Arxiv ID**: http://arxiv.org/abs/2301.09858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09858v1)
- **Published**: 2023-01-24 08:30:14+00:00
- **Updated**: 2023-01-24 08:30:14+00:00
- **Authors**: Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and activations. Growing concerns for privacy and security have motivated the development of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing approaches, and propose a data-free non-uniform method. More specifically, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical operations performed by the DNN. This leads to search among the continuous automorphisms of $(\mathbb{R}_+^*,\times)$, which boils down to the power functions defined by their exponent. To find this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modifications in the quantized DNN activation functions. As such, with only negligible overhead, it significantly outperforms existing methods in a variety of configurations.



### Image Super-Resolution using Efficient Striped Window Transformer
- **Arxiv ID**: http://arxiv.org/abs/2301.09869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09869v2)
- **Published**: 2023-01-24 09:09:35+00:00
- **Updated**: 2023-03-14 07:03:54+00:00
- **Authors**: Jinpeng Shi, Hui Li, Tianle Liu, Yulong Liu, Mingjian Zhang, Jinchen Zhu, Ling Zheng, Shizhuang Weng
- **Comment**: SOTA lightweight super-resolution transformer. 8 pages, 9 figures and
  6 tables. The Code is available at
  https://github.com/Fried-Rice-Lab/FriedRiceLab
- **Journal**: None
- **Summary**: Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.



### ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2301.09878v1
- **DOI**: 10.1109/ICPR56361.2022.9956542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09878v1)
- **Published**: 2023-01-24 09:35:43+00:00
- **Updated**: 2023-01-24 09:35:43+00:00
- **Authors**: Mathias Zinnen, Prathmesh Madhu, Ronak Kosti, Peter Bell, Andreas Maier, Vincent Christlein
- **Comment**: 6 pages, 6 figures
- **Journal**: 2022 26th International Conference on Pattern Recognition (ICPR),
  Montreal, QC, Canada, 2022, pp. 4989-4994
- **Summary**: The Odeuropa Challenge on Olfactory Object Recognition aims to foster the development of object detection in the visual arts and to promote an olfactory perspective on digital heritage. Object detection in historical artworks is particularly challenging due to varying styles and artistic periods. Moreover, the task is complicated due to the particularity and historical variance of predefined target objects, which exhibit a large intra-class variance, and the long tail distribution of the dataset labels, with some objects having only very few training examples. These challenges should encourage participants to create innovative approaches using domain adaptation or few-shot learning. We provide a dataset of 2647 artworks annotated with 20 120 tightly fit bounding boxes that are split into a training and validation set (public). A test set containing 1140 artworks and 15 480 annotations is kept private for the challenge evaluation.



### Data Augmentation Alone Can Improve Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2301.09879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.09879v1)
- **Published**: 2023-01-24 09:36:39+00:00
- **Updated**: 2023-01-24 09:36:39+00:00
- **Authors**: Lin Li, Michael Spratling
- **Comment**: published at conference ICLR2023
- **Journal**: None
- **Summary**: Adversarial training suffers from the issue of robust overfitting, which seriously impairs its generalization performance. Data augmentation, which is effective at preventing overfitting in standard training, has been observed by many previous works to be ineffective in mitigating overfitting in adversarial training. This work proves that, contrary to previous findings, data augmentation alone can significantly boost accuracy and robustness in adversarial training. We find that the hardness and the diversity of data augmentation are important factors in combating robust overfitting. In general, diversity can improve both accuracy and robustness, while hardness can boost robustness at the cost of accuracy within a certain limit and degrade them both over that limit. To mitigate robust overfitting, we first propose a new crop transformation, Cropshift, which has improved diversity compared to the conventional one (Padcrop). We then propose a new data augmentation scheme, based on Cropshift, with much improved diversity and well-balanced hardness. Empirically, our augmentation method achieves the state-of-the-art accuracy and robustness for data augmentations in adversarial training. Furthermore, when combined with weight averaging it matches, or even exceeds, the performance of the best contemporary regularization methods for alleviating robust overfitting. Code is available at: https://github.com/TreeLLi/DA-Alone-Improves-AT.



### Deep learning-based method for segmenting epithelial layer of tubules in histopathological images of testicular tissue
- **Arxiv ID**: http://arxiv.org/abs/2301.09887v1
- **DOI**: 10.1117/1.JMI.10.3.037501
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.09887v1)
- **Published**: 2023-01-24 09:46:47+00:00
- **Updated**: 2023-01-24 09:46:47+00:00
- **Authors**: Azadeh Fakhrzadeh, Pouya Karimian, Mahsa Meyari, Cris L. Luengo Hendriks, Lena Holm, Christian Sonne, Rune Dietz, Ellinor Spörndly-Nees
- **Comment**: submitted to Journal of Medical Imaging, 16 pages, 5 figures
- **Journal**: J. Med. Imag. 10(3) 037501 (3 May 2023)
- **Summary**: There is growing concern that male reproduction is affected by environmental chemicals. One way to determine the adverse effect of environmental pollutants is to use wild animals as monitors and evaluate testicular toxicity using histopathology. Automated methods are necessary tools in the quantitative assessment of histopathology to overcome the subjectivity of manual evaluation and accelerate the process. We propose an automated method to process histology images of testicular tissue. Segmenting the epithelial layer of the seminiferous tubule is a prerequisite for developing automated methods to detect abnormalities in tissue. We suggest an encoder-decoder fully connected convolutional neural network (F-CNN) model to segment the epithelial layer of the seminiferous tubules in histological images. Using ResNet-34 modules in the encoder adds a shortcut mechanism to avoid the gradient vanishing and accelerate the network convergence. The squeeze & excitation (SE) attention block is integrated into the encoding module improving the segmentation and localization of epithelium. We applied the proposed method for the 2-class problem where the epithelial layer of the tubule is the target class. The f-score and IoU of the proposed method are 0.85 and 0.92. Although the proposed method is trained on a limited training set, it performs well on an independent dataset and outperforms other state-of-the-art methods. The pretrained ResNet-34 in the encoder and attention block suggested in the decoder result in better segmentation and generalization. The proposed method can be applied to testicular tissue images from any mammalian species and can be used as the first part of a fully automated testicular tissue processing pipeline. The dataset and codes are publicly available on GitHub.



### Transfer Learning for Olfactory Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.09906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09906v1)
- **Published**: 2023-01-24 10:31:43+00:00
- **Updated**: 2023-01-24 10:31:43+00:00
- **Authors**: Mathias Zinnen, Prathmesh Madhu, Peter Bell, Andreas Maier, Vincent Christlein
- **Comment**: 6 pages, 4 figures
- **Journal**: 2022 Digital Humanities Conference, Tokyo, Japan, 2022, pp.409-413
- **Summary**: We investigate the effect of style and category similarity in multiple datasets used for object detection pretraining. We find that including an additional stage of object-detection pretraining can increase the detection performance considerably. While our experiments suggest that style similarities between pre-training and target datasets are less important than matching categories, further experiments are needed to verify this hypothesis.



### Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues
- **Arxiv ID**: http://arxiv.org/abs/2301.09914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09914v1)
- **Published**: 2023-01-24 10:50:45+00:00
- **Updated**: 2023-01-24 10:50:45+00:00
- **Authors**: Verena Jasmin Hallitschke, Tobias Schlumberger, Philipp Kataliakos, Zdravko Marinov, Moon Kim, Lars Heiliger, Constantin Seibold, Jens Kleesiek, Rainer Stiefelhagen
- **Comment**: Accepted at ISBI 2023; 5 pages, 5 figures
- **Journal**: None
- **Summary**: Recently, deep learning enabled the accurate segmentation of various diseases in medical imaging. These performances, however, typically demand large amounts of manual voxel annotations. This tedious process for volumetric data becomes more complex when not all required information is available in a single imaging domain as is the case for PET/CT data. We propose a multimodal interactive segmentation framework that mitigates these issues by combining anatomical and physiological cues from PET/CT data. Our framework utilizes the geodesic distance transform to represent the user annotations and we implement a novel ellipsoid-based user simulation scheme during training. We further propose two annotation interfaces and conduct a user study to estimate their usability. We evaluated our model on the in-domain validation dataset and an unseen PET/CT dataset. We make our code publicly available: https://github.com/verena-hallitschke/pet-ct-annotate.



### Uncertainty-Aware Distillation for Semi-Supervised Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.09964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.09964v1)
- **Published**: 2023-01-24 12:53:06+00:00
- **Updated**: 2023-01-24 12:53:06+00:00
- **Authors**: Yawen Cui, Wanxia Deng, Haoyu Chen, Li Liu
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Given a model well-trained with a large-scale base dataset, Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning novel classes from a few labeled samples by avoiding overfitting, without catastrophically forgetting all encountered classes previously. Currently, semi-supervised learning technique that harnesses freely-available unlabeled data to compensate for limited labeled data can boost the performance in numerous vision tasks, which heuristically can be applied to tackle issues in FSCIL, i.e., the Semi-supervised FSCIL (Semi-FSCIL). So far, very limited work focuses on the Semi-FSCIL task, leaving the adaptability issue of semi-supervised learning to the FSCIL task unresolved. In this paper, we focus on this adaptability issue and present a simple yet efficient Semi-FSCIL framework named Uncertainty-aware Distillation with Class-Equilibrium (UaD-CE), encompassing two modules UaD and CE. Specifically, when incorporating unlabeled data into each incremental session, we introduce the CE module that employs a class-balanced self-training to avoid the gradual dominance of easy-to-classified classes on pseudo-label generation. To distill reliable knowledge from the reference model, we further implement the UaD module that combines uncertainty-guided knowledge refinement with adaptive distillation. Comprehensive experiments on three benchmark datasets demonstrate that our method can boost the adaptability of unlabeled data with the semi-supervised learning technique in FSCIL tasks.



### Few-shot Font Generation by Learning Style Difference and Similarity
- **Arxiv ID**: http://arxiv.org/abs/2301.10008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10008v1)
- **Published**: 2023-01-24 13:57:25+00:00
- **Updated**: 2023-01-24 13:57:25+00:00
- **Authors**: Xiao He, Mingrui Zhu, Nannan Wang, Xinbo Gao, Heng Yang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Few-shot font generation (FFG) aims to preserve the underlying global structure of the original character while generating target fonts by referring to a few samples. It has been applied to font library creation, a personalized signature, and other scenarios. Existing FFG methods explicitly disentangle content and style of reference glyphs universally or component-wisely. However, they ignore the difference between glyphs in different styles and the similarity of glyphs in the same style, which results in artifacts such as local distortions and style inconsistency. To address this issue, we propose a novel font generation approach by learning the Difference between different styles and the Similarity of the same style (DS-Font). We introduce contrastive learning to consider the positive and negative relationship between styles. Specifically, we propose a multi-layer style projector for style encoding and realize a distinctive style representation via our proposed Cluster-level Contrastive Style (CCS) loss. In addition, we design a multi-task patch discriminator, which comprehensively considers different areas of the image and ensures that each style can be distinguished independently. We conduct qualitative and quantitative evaluations comprehensively to demonstrate that our approach achieves significantly better results than state-of-the-art methods.



### Progressive Meta-Pooling Learning for Lightweight Image Classification Model
- **Arxiv ID**: http://arxiv.org/abs/2301.10038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10038v1)
- **Published**: 2023-01-24 14:28:05+00:00
- **Updated**: 2023-01-24 14:28:05+00:00
- **Authors**: Peijie Dong, Xin Niu, Zhiliang Tian, Lujun Li, Xiaodong Wang, Zimian Wei, Hengyue Pan, Dongsheng Li
- **Comment**: 5 pages, 2 figures, ICASSP23
- **Journal**: None
- **Summary**: Practical networks for edge devices adopt shallow depth and small convolutional kernels to save memory and computational cost, which leads to a restricted receptive field. Conventional efficient learning methods focus on lightweight convolution designs, ignoring the role of the receptive field in neural network design. In this paper, we propose the Meta-Pooling framework to make the receptive field learnable for a lightweight network, which consists of parameterized pooling-based operations. Specifically, we introduce a parameterized spatial enhancer, which is composed of pooling operations to provide versatile receptive fields for each layer of a lightweight model. Then, we present a Progressive Meta-Pooling Learning (PMPL) strategy for the parameterized spatial enhancer to acquire a suitable receptive field size. The results on the ImageNet dataset demonstrate that MobileNetV2 using Meta-Pooling achieves top1 accuracy of 74.6\%, which outperforms MobileNetV2 by 2.3\%.



### DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2301.10047v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10047v2)
- **Published**: 2023-01-24 14:44:03+00:00
- **Updated**: 2023-02-02 12:56:21+00:00
- **Authors**: Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: Speech-driven gesture synthesis is a field of growing interest in virtual human creation. However, a critical challenge is the inherent intricate one-to-many mapping between speech and gestures. Previous studies have explored and achieved significant progress with generative models. Notwithstanding, most synthetic gestures are still vastly less natural. This paper presents DiffMotion, a novel speech-driven gesture synthesis architecture based on diffusion models. The model comprises an autoregressive temporal encoder and a denoising diffusion probability Module. The encoder extracts the temporal context of the speech input and historical gestures. The diffusion module learns a parameterized Markov chain to gradually convert a simple distribution into a complex distribution and generates the gestures according to the accompanied speech. Compared with baselines, objective and subjective evaluations confirm that our approach can produce natural and diverse gesticulation and demonstrate the benefits of diffusion-based models on speech-driven gesture synthesis.



### Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2301.10048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10048v1)
- **Published**: 2023-01-24 14:44:44+00:00
- **Updated**: 2023-01-24 14:44:44+00:00
- **Authors**: Kaidong Zhang, Jialun Peng, Jingjing Fu, Dong Liu
- **Comment**: This manuscript is a journal extension of our ECCV 2022 paper
  (arXiv:2208.06768)
- **Journal**: None
- **Summary**: Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.



### Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2301.10051v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10051v3)
- **Published**: 2023-01-24 14:50:40+00:00
- **Updated**: 2023-04-08 13:58:40+00:00
- **Authors**: Zanjia Tong, Yuhang Chen, Zewei Xu, Rong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The loss function for bounding box regression (BBR) is essential to object detection. Its good definition will bring significant performance improvement to the model. Most existing works assume that the examples in the training data are high-quality and focus on strengthening the fitting ability of BBR loss. If we blindly strengthen BBR on low-quality examples, it will jeopardize localization performance. Focal-EIoU v1 was proposed to solve this problem, but due to its static focusing mechanism (FM), the potential of non-monotonic FM was not fully exploited. Based on this idea, we propose an IoU-based loss with a dynamic non-monotonic FM named Wise-IoU (WIoU). The dynamic non-monotonic FM uses the outlier degree instead of IoU to evaluate the quality of anchor boxes and provides a wise gradient gain allocation strategy. This strategy reduces the competitiveness of high-quality anchor boxes while also reducing the harmful gradient generated by low-quality examples. This allows WIoU to focus on ordinary-quality anchor boxes and improve the detector's overall performance. When WIoU is applied to the state-of-the-art real-time detector YOLOv7, the AP-75 on the MS-COCO dataset is improved from 53.03% to 54.50%. Code is available at https://github.com/Instinct323/wiou.



### Event Detection in Football using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2301.10052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10052v1)
- **Published**: 2023-01-24 14:52:54+00:00
- **Updated**: 2023-01-24 14:52:54+00:00
- **Authors**: Aditya Sangram Singh Rana
- **Comment**: None
- **Journal**: None
- **Summary**: The massive growth of data collection in sports has opened numerous avenues for professional teams and media houses to gain insights from this data. The data collected includes per frame player and ball trajectories, and event annotations such as passes, fouls, cards, goals, etc. Graph Convolutional Networks (GCNs) have recently been employed to process this highly unstructured tracking data which can be otherwise difficult to model because of lack of clarity on how to order players in a sequence and how to handle missing objects of interest. In this thesis, we focus on the goal of automatic event detection from football videos. We show how to model the players and the ball in each frame of the video sequence as a graph, and present the results for graph convolutional layers and pooling methods that can be used to model the temporal context present around each action.



### Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses
- **Arxiv ID**: http://arxiv.org/abs/2301.10056v2
- **DOI**: 10.1109/SP46215.2023.00059
- **Categories**: **cs.CR**, cs.CV, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2301.10056v2)
- **Published**: 2023-01-24 15:00:47+00:00
- **Updated**: 2023-01-26 20:11:27+00:00
- **Authors**: Yan Long, Pirouz Naghavi, Blas Kojusner, Kevin Butler, Sara Rampazzi, Kevin Fu
- **Comment**: None
- **Journal**: 2023 IEEE Symposium on Security and Privacy (SP)
- **Summary**: Our research discovers how the rolling shutter and movable lens structures widely found in smartphone cameras modulate structure-borne sounds onto camera images, creating a point-of-view (POV) optical-acoustic side channel for acoustic eavesdropping. The movement of smartphone camera hardware leaks acoustic information because images unwittingly modulate ambient sound as imperceptible distortions. Our experiments find that the side channel is further amplified by intrinsic behaviors of Complementary metal-oxide-semiconductor (CMOS) rolling shutters and movable lenses such as in Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes the limits of acoustic information leakage caused by structure-borne sound that perturbs the POV of smartphone cameras. In contrast with traditional optical-acoustic eavesdropping on vibrating objects, this side channel requires no line of sight and no object within the camera's field of view (images of a ceiling suffice). Our experiments test the limits of this side channel with a novel signal processing pipeline that extracts and recognizes the leaked acoustic information. Our evaluation with 10 smartphones on a spoken digit dataset reports 80.66%, 91.28%, and 99.67% accuracies on recognizing 10 spoken digits, 20 speakers, and 2 genders respectively. We further systematically discuss the possible defense strategies and implementations. By modeling, measuring, and demonstrating the limits of acoustic eavesdropping from smartphone camera image streams, our contributions explain the physics-based causality and possible ways to reduce the threat on current and future devices.



### Planar Object Tracking via Weighted Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2301.10057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10057v1)
- **Published**: 2023-01-24 15:02:44+00:00
- **Updated**: 2023-01-24 15:02:44+00:00
- **Authors**: Jonas Serych, Jiri Matas
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: We propose WOFT -- a novel method for planar object tracking that estimates a full 8 degrees-of-freedom pose, i.e. the homography w.r.t. a reference view. The method uses a novel module that leverages dense optical flow and assigns a weight to each optical flow correspondence, estimating a homography by weighted least squares in a fully differentiable manner. The trained module assigns zero weights to incorrect correspondences (outliers) in most cases, making the method robust and eliminating the need of the typically used non-differentiable robust estimators like RANSAC. The proposed weighted optical flow tracker (WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 and POIC, tracking consistently well across a wide range of scenarios.



### Interpretable Out-Of-Distribution Detection Using Pattern Identification
- **Arxiv ID**: http://arxiv.org/abs/2302.10303v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.10303v1)
- **Published**: 2023-01-24 15:35:54+00:00
- **Updated**: 2023-01-24 15:35:54+00:00
- **Authors**: Romain Xu-Darme, Julien Girard-Satabin, Darryl Hond, Gabriele Incorvaia, Zakaria Chihani
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-distribution (OoD) detection for data-based programs is a goal of paramount importance. Common approaches in the literature tend to train detectors requiring inside-of-distribution (in-distribution, or IoD) and OoD validation samples, and/or implement confidence metrics that are often abstract and therefore difficult to interpret. In this work, we propose to use existing work from the field of explainable AI, namely the PARTICUL pattern identification algorithm, in order to build more interpretable and robust OoD detectors for visual classifiers. Crucially, this approach does not require to retrain the classifier and is tuned directly to the IoD dataset, making it applicable to domains where OoD does not have a clear definition. Moreover, pattern identification allows us to provide images from the IoD dataset as reference points to better explain the confidence scores. We demonstrates that the detection capabilities of this approach are on par with existing methods through an extensive benchmark across four datasets and two definitions of OoD. In particular, we introduce a new benchmark based on perturbations of the IoD dataset which provides a known and quantifiable evaluation of the discrepancy between the IoD and OoD datasets that serves as a reference value for the comparison between various OoD detection methods. Our experiments show that the robustness of all metrics under test does not solely depend on the nature of the IoD dataset or the OoD definition, but also on the architecture of the classifier, which stresses the need for thorough experimentations for future work on OoD detection.



### Model soups to increase inference without increasing compute time
- **Arxiv ID**: http://arxiv.org/abs/2301.10092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.10092v1)
- **Published**: 2023-01-24 15:59:07+00:00
- **Updated**: 2023-01-24 15:59:07+00:00
- **Authors**: Charles Dansereau, Milo Sobral, Maninder Bhogal, Mehdi Zalai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we compare Model Soups performances on three different models (ResNet, ViT and EfficientNet) using three Soup Recipes (Greedy Soup Sorted, Greedy Soup Random and Uniform soup) from arXiv:2203.05482, and reproduce the results of the authors. We then introduce a new Soup Recipe called Pruned Soup. Results from the soups were better than the best individual model for the pre-trained vision transformer, but were much worst for the ResNet and the EfficientNet. Our pruned soup performed better than the uniform and greedy soups presented in the original paper. We also discuss the limitations of weight-averaging that were found during the experiments. The code for our model soup library and the experiments with different models can be found here: https://github.com/milo-sobral/ModelSoup



### Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2301.10100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10100v1)
- **Published**: 2023-01-24 16:10:08+00:00
- **Updated**: 2023-01-24 16:10:08+00:00
- **Authors**: Gilles Puy, Alexandre Boulch, Renaud Marlet
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of point clouds in autonomous driving datasets requires techniques that can process large numbers of points over large field of views. Today, most deep networks designed for this task exploit 3D sparse convolutions to reduce memory and computational loads. The best methods then further exploit specificities of rotating lidar sampling patterns to further improve the performance, e.g., cylindrical voxels, or range images (for feature fusion from multiple point cloud representations). In contrast, we show that one can build a well-performing point-based backbone free of these specialized tools. This backbone, WaffleIron, relies heavily on generic MLPs and dense 2D convolutions, making it easy to implement, and contains just a few parameters easy to tune. Despite its simplicity, our experiments on SemanticKITTI and nuScenes show that WaffleIron competes with the best methods designed specifically for these autonomous driving datasets. Hence, WaffleIron is a strong, easy-to-implement, baseline for semantic segmentation of sparse outdoor point clouds.



### Improving Open-Set Semi-Supervised Learning with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2301.10127v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2301.10127v2)
- **Published**: 2023-01-24 16:46:37+00:00
- **Updated**: 2023-03-08 20:02:28+00:00
- **Authors**: Erik Wallin, Lennart Svensson, Fredrik Kahl, Lars Hammarstrand
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Open-set semi-supervised learning (OSSL) is a realistic setting of semi-supervised learning where the unlabeled training set contains classes that are not present in the labeled set. Many existing OSSL methods assume that these out-of-distribution data are harmful and put effort into excluding data from unknown classes from the training objective. In contrast, we propose an OSSL framework that facilitates learning from all unlabeled data through self-supervision. Additionally, we utilize an energy-based score to accurately recognize data belonging to the known classes, making our method well-suited for handling uncurated data in deployment. We show through extensive experimental evaluations on several datasets that our method shows overall unmatched robustness and performance in terms of closed-set accuracy and open-set recognition compared with state-of-the-art for OSSL. Our code will be released upon publication.



### Bipartite Graph Diffusion Model for Human Interaction Generation
- **Arxiv ID**: http://arxiv.org/abs/2301.10134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2301.10134v1)
- **Published**: 2023-01-24 16:59:46+00:00
- **Updated**: 2023-01-24 16:59:46+00:00
- **Authors**: Baptiste Chopin, Hao Tang, Mohamed Daoudi
- **Comment**: None
- **Journal**: None
- **Summary**: The generation of natural human motion interactions is a hot topic in computer vision and computer animation. It is a challenging task due to the diversity of possible human motion interactions. Diffusion models, which have already shown remarkable generative capabilities in other domains, are a good candidate for this task. In this paper, we introduce a novel bipartite graph diffusion method (BiGraphDiff) to generate human motion interactions between two persons. Specifically, bipartite node sets are constructed to model the inherent geometric constraints between skeleton nodes during interactions. The interaction graph diffusion model is transformer-based, combining some state-of-the-art motion methods. We show that the proposed achieves new state-of-the-art results on leading benchmarks for the human interaction generation task.



### Enhanced Sharp-GAN For Histopathology Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2301.10187v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10187v1)
- **Published**: 2023-01-24 17:54:01+00:00
- **Updated**: 2023-01-24 17:54:01+00:00
- **Authors**: Sujata Butte, Haotian Wang, Aleksandar Vakanski, Min Xian
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology image synthesis aims to address the data shortage issue in training deep learning approaches for accurate cancer detection. However, existing methods struggle to produce realistic images that have accurate nuclei boundaries and less artifacts, which limits the application in downstream tasks. To address the challenges, we propose a novel approach that enhances the quality of synthetic images by using nuclei topology and contour regularization. The proposed approach uses the skeleton map of nuclei to integrate nuclei topology and separate touching nuclei. In the loss function, we propose two new contour regularization terms that enhance the contrast between contour and non-contour pixels and increase the similarity between contour pixels. We evaluate the proposed approach on the two datasets using image quality metrics and a downstream task (nuclei segmentation). The proposed approach outperforms Sharp-GAN in all four image quality metrics on two datasets. By integrating 6k synthetic images from the proposed approach into training, a nuclei segmentation model achieves the state-of-the-art segmentation performance on TNBC dataset and its detection quality (DQ), segmentation quality (SQ), panoptic quality (PQ), and aggregated Jaccard index (AJI) is 0.855, 0.863, 0.691, and 0.683, respectively.



### A Simple Adaptive Unfolding Network for Hyperspectral Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2301.10208v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10208v1)
- **Published**: 2023-01-24 18:28:21+00:00
- **Updated**: 2023-01-24 18:28:21+00:00
- **Authors**: Junyu Wang, Shijie Wang, Wenyu Liu, Zengqiang Zheng, Xinggang Wang
- **Comment**: Project page: https://github.com/hustvl/SAUNet
- **Journal**: None
- **Summary**: We present a simple, efficient, and scalable unfolding network, SAUNet, to simplify the network design with an adaptive alternate optimization framework for hyperspectral image (HSI) reconstruction. SAUNet customizes a Residual Adaptive ADMM Framework (R2ADMM) to connect each stage of the network via a group of learnable parameters to promote the usage of mask prior, which greatly stabilizes training and solves the accuracy degradation issue. Additionally, we introduce a simple convolutional modulation block (CMB), which leads to efficient training, easy scale-up, and less computation. Coupling these two designs, SAUNet can be scaled to non-trivial 13 stages with continuous improvement. Without bells and whistles, SAUNet improves both performance and speed compared with the previous state-of-the-art counterparts, which makes it feasible for practical high-resolution HSI reconstruction scenarios. We set new records on CAVE and KAIST HSI reconstruction benchmarks. Code and models are available at https://github.com/hustvl/SAUNet.



### Detecting and measuring human gastric peristalsis using magnetically controlled capsule endoscope
- **Arxiv ID**: http://arxiv.org/abs/2301.10218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.10218v1)
- **Published**: 2023-01-24 18:46:33+00:00
- **Updated**: 2023-01-24 18:46:33+00:00
- **Authors**: Xueshen Li, Yu Gan, David Duan, Xiao Yang
- **Comment**: 5 pages, 5 figures, accepted by IEEE ISBI 2023
- **Journal**: None
- **Summary**: Magnetically controlled capsule endoscope (MCCE) is an emerging tool for the diagnosis of gastric diseases with the advantages of comfort, safety, and no anesthesia. In this paper, we develop algorithms to detect and measure human gastric peristalsis (contraction wave) using video sequences acquired by MCCE. We develop a spatial-temporal deep learning algorithm to detect gastric contraction waves and measure human gastric peristalsis periods. The quality of MCCE video sequences is prone to camera motion. We design a camera motion detector (CMD) to process the MCCE video sequences, mitigating the camera movement during MCCE examination. To the best of our knowledge, we are the first to propose computer vision-based solutions to detect and measure human gastric peristalsis. Our methods have great potential in assisting the diagnosis of gastric diseases by evaluating gastric motility.



### RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2301.10222v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.10222v2)
- **Published**: 2023-01-24 18:50:48+00:00
- **Updated**: 2023-04-25 13:11:42+00:00
- **Authors**: Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, Renaud Marlet
- **Comment**: CVPR 2023. Code at https://github.com/valeoai/rangevit
- **Journal**: None
- **Summary**: Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate ViTs' lack of inductive bias by substituting a tailored convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https://github.com/valeoai/rangevit.



### K-Planes: Explicit Radiance Fields in Space, Time, and Appearance
- **Arxiv ID**: http://arxiv.org/abs/2301.10241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2301.10241v2)
- **Published**: 2023-01-24 18:59:08+00:00
- **Updated**: 2023-03-24 21:32:50+00:00
- **Authors**: Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg, Benjamin Recht, Angjoo Kanazawa
- **Comment**: Project page https://sarafridov.github.io/K-Planes/
- **Journal**: None
- **Summary**: We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see https://sarafridov.github.io/K-Planes.



### Generating Multidimensional Clusters With Support Lines
- **Arxiv ID**: http://arxiv.org/abs/2301.10327v2
- **DOI**: 10.1016/j.knosys.2023.110836
- **Categories**: **cs.LG**, cs.CV, cs.PL, I.5; I.2.5
- **Links**: [PDF](http://arxiv.org/pdf/2301.10327v2)
- **Published**: 2023-01-24 22:08:24+00:00
- **Updated**: 2023-07-30 23:00:36+00:00
- **Authors**: Nuno Fachada, Diogo de Andrade
- **Comment**: The peer-reviewed version of this paper is published in
  Knowledge-Based Systems at https://doi.org/10.1016/j.knosys.2023.110836. This
  version is typeset by the author and differs only in pagination and
  typographical detail
- **Journal**: Knowledge-Based Systems, 277, 110836, 2023
- **Summary**: Synthetic data is essential for assessing clustering techniques, complementing and extending real data, and allowing for more complete coverage of a given problem's space. In turn, synthetic data generators have the potential of creating vast amounts of data -- a crucial activity when real-world data is at premium -- while providing a well-understood generation procedure and an interpretable instrument for methodically investigating cluster analysis algorithms. Here, we present Clugen, a modular procedure for synthetic data generation, capable of creating multidimensional clusters supported by line segments using arbitrary distributions. Clugen is open source, comprehensively unit tested and documented, and is available for the Python, R, Julia, and MATLAB/Octave ecosystems. We demonstrate that our proposal can produce rich and varied results in various dimensions, is fit for use in the assessment of clustering algorithms, and has the potential to be a widely used framework in diverse clustering-related research tasks.



### Few-Shot Learning Enables Population-Scale Analysis of Leaf Traits in Populus trichocarpa
- **Arxiv ID**: http://arxiv.org/abs/2301.10351v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2301.10351v3)
- **Published**: 2023-01-24 23:40:01+00:00
- **Updated**: 2023-05-18 12:10:54+00:00
- **Authors**: John Lagergren, Mirko Pavicic, Hari B. Chhetri, Larry M. York, P. Doug Hyatt, David Kainer, Erica M. Rutter, Kevin Flores, Jack Bailey-Bale, Marie Klein, Gail Taylor, Daniel Jacobson, Jared Streich
- **Comment**: None
- **Journal**: None
- **Summary**: Plant phenotyping is typically a time-consuming and expensive endeavor, requiring large groups of researchers to meticulously measure biologically relevant plant traits, and is the main bottleneck in understanding plant adaptation and the genetic architecture underlying complex traits at population scale. In this work, we address these challenges by leveraging few-shot learning with convolutional neural networks (CNNs) to segment the leaf body and visible venation of 2,906 P. trichocarpa leaf images obtained in the field. In contrast to previous methods, our approach (i) does not require experimental or image pre-processing, (ii) uses the raw RGB images at full resolution, and (iii) requires very few samples for training (e.g., just eight images for vein segmentation). Traits relating to leaf morphology and vein topology are extracted from the resulting segmentations using traditional open-source image-processing tools, validated using real-world physical measurements, and used to conduct a genome-wide association study to identify genes controlling the traits. In this way, the current work is designed to provide the plant phenotyping community with (i) methods for fast and accurate image-based feature extraction that require minimal training data, and (ii) a new population-scale data set, including 68 different leaf phenotypes, for domain scientists and machine learning researchers. All of the few-shot learning code, data, and results are made publicly available.



