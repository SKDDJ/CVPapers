# Arxiv Papers in cs.CV on 2023-12-23
### PACE: Pose Annotations in Cluttered Environments
- **Arxiv ID**: http://arxiv.org/abs/2312.15130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15130v1)
- **Published**: 2023-12-23 01:38:41+00:00
- **Updated**: 2023-12-23 01:38:41+00:00
- **Authors**: Yang You, Kai Xiong, Zhening Yang, Zhengxiang Huang, Junwei Zhou, Ruoxi Shi, Zhou Fang, Adam W. Harley, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation is a crucial task in computer vision, enabling tracking and manipulating objects in images or videos. While several datasets exist for pose estimation, there is a lack of large-scale datasets specifically focusing on cluttered scenes with occlusions. This limitation is a bottleneck in the development and evaluation of pose estimation methods, particularly toward the goal of real-world application in environments where occlusions are common. Addressing this, we introduce PACE (Pose Annotations in Cluttered Environments), a large-scale benchmark designed to advance the development and evaluation of pose estimation methods in cluttered scenarios. PACE encompasses 54,945 frames with 257,673 annotations across 300 videos, covering 576 objects from 44 categories and featuring a mix of rigid and articulated items in cluttered scenes. To annotate the real-world data efficiently, we developed an innovative annotation system utilizing a calibrated 3-camera setup. We test state-of-the-art algorithms in PACE along two tracks: pose estimation, and object pose tracking, revealing the benchmark's challenges and research opportunities. We plan to release PACE as a public evaluation benchmark, along the annotations tools we developed, to stimulate further advancements in the field. Our code and data is available on https://github.com/qq456cvb/PACE.



### Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2312.15133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15133v1)
- **Published**: 2023-12-23 01:52:14+00:00
- **Updated**: 2023-12-23 01:52:14+00:00
- **Authors**: Shujuan Li, Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Zhizhong Han
- **Comment**: Accepted by AAAI 2024. Project page:
  https://lisj575.github.io/APU-LDI
- **Journal**: None
- **Summary**: Point cloud upsampling aims to generate dense and uniformly distributed point sets from a sparse point cloud, which plays a critical role in 3D computer vision. Previous methods typically split a sparse point cloud into several local patches, upsample patch points, and merge all upsampled patches. However, these methods often produce holes, outliers or nonuniformity due to the splitting and merging process which does not maintain consistency among local patches. To address these issues, we propose a novel approach that learns an unsigned distance field guided by local priors for point cloud upsampling. Specifically, we train a local distance indicator (LDI) that predicts the unsigned distance from a query point to a local implicit surface. Utilizing the learned LDI, we learn an unsigned distance field to represent the sparse point cloud with patch consistency. At inference time, we randomly sample queries around the sparse point cloud, and project these query points onto the zero-level set of the learned implicit field to generate a dense point cloud. We justify that the implicit field is naturally continuous, which inherently enables the application of arbitrary-scale upsampling without necessarily retraining for various scales. We conduct comprehensive experiments on both synthetic data and real scans, and report state-of-the-art results under widely used benchmarks.



### Towards End-to-End Structure Solutions from Information-Compromised Diffraction Data via Generative Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2312.15136v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15136v1)
- **Published**: 2023-12-23 02:17:27+00:00
- **Updated**: 2023-12-23 02:17:27+00:00
- **Authors**: Gabe Guo, Judah Goldfeder, Ling Lan, Aniv Ray, Albert Hanming Yang, Boyuan Chen, Simon JL Billinge, Hod Lipson
- **Comment**: None
- **Journal**: None
- **Summary**: The revolution in materials in the past century was built on a knowledge of the atomic arrangements and the structure-property relationship. The sine qua non for obtaining quantitative structural information is single crystal crystallography. However, increasingly we need to solve structures in cases where the information content in our input signal is significantly degraded, for example, due to orientational averaging of grains, finite size effects due to nanostructure, and mixed signals due to sample heterogeneity. Understanding the structure property relationships in such situations is, if anything, more important and insightful, yet we do not have robust approaches for accomplishing it. In principle, machine learning (ML) and deep learning (DL) are promising approaches since they augment information in the degraded input signal with prior knowledge learned from large databases of already known structures. Here we present a novel ML approach, a variational query-based multi-branch deep neural network that has the promise to be a robust but general tool to address this problem end-to-end. We demonstrate the approach on computed powder x-ray diffraction (PXRD), along with partial chemical composition information, as input. We choose as a structural representation a modified electron density we call the Cartesian mapped electron density (CMED), that straightforwardly allows our ML model to learn material structures across different chemistries, symmetries and crystal systems. When evaluated on theoretically simulated data for the cubic and trigonal crystal systems, the system achieves up to $93.4\%$ average similarity with the ground truth on unseen materials, both with known and partially-known chemical composition information, showing great promise for successful structure solution even from degraded and incomplete input data.



### Automatic Tooth Arrangement with Joint Features of Point and Mesh Representations via Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2312.15139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15139v1)
- **Published**: 2023-12-23 02:27:15+00:00
- **Updated**: 2023-12-23 02:27:15+00:00
- **Authors**: Changsong Lei, Mengfei Xia, Shaofeng Wang, Yaqian Liang, Ran Yi, Yuhui Wen, Yongjin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Tooth arrangement is a crucial step in orthodontics treatment, in which aligning teeth could improve overall well-being, enhance facial aesthetics, and boost self-confidence. To improve the efficiency of tooth arrangement and minimize errors associated with unreasonable designs by inexperienced practitioners, some deep learning-based tooth arrangement methods have been proposed. Currently, most existing approaches employ MLPs to model the nonlinear relationship between tooth features and transformation matrices to achieve tooth arrangement automatically. However, the limited datasets (which to our knowledge, have not been made public) collected from clinical practice constrain the applicability of existing methods, making them inadequate for addressing diverse malocclusion issues. To address this challenge, we propose a general tooth arrangement neural network based on the diffusion probabilistic model. Conditioned on the features extracted from the dental model, the diffusion probabilistic model can learn the distribution of teeth transformation matrices from malocclusion to normal occlusion by gradually denoising from a random variable, thus more adeptly managing real orthodontic data. To take full advantage of effective features, we exploit both mesh and point cloud representations by designing different encoding networks to extract the tooth (local) and jaw (global) features, respectively. In addition to traditional metrics ADD, PA-ADD, CSA, and ME_{rot}, we propose a new evaluation metric based on dental arch curves to judge whether the generated teeth meet the individual normal occlusion. Experimental results demonstrate that our proposed method achieves state-of-the-art tooth alignment results and satisfactory occlusal relationships between dental arches. We will publish the code and dataset.



### Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2312.15144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15144v1)
- **Published**: 2023-12-23 02:54:41+00:00
- **Updated**: 2023-12-23 02:54:41+00:00
- **Authors**: Shaojie Zhang, Jianqin Yin, Yonghao Dang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition is a central task of human-computer interaction. Current methods apply the modeling paradigm of image recognition to it directly. However, the skeleton sequences abstracted from the human body is a sparse representation. The features extracted from the skeleton encoder are spatiotemporal decoupled, which may confuse the semantics. To reduce the coupling and improve the semantics of the global features, we propose a framework (STD-CL) for skeleton-based action recognition. We first decouple the spatial-specific and temporal-specific features from the spatiotemporal features. Then we apply the attentive features to contrastive learning, which pulls together the features from the positive pairs and pushes away the feature embedding from the negative pairs. Moreover, the proposed training strategy STD-CL can be incorporated into current methods. Without additional compute consumption in the testing phase, our STD-CL with four various backbones (HCN, 2S-AGCN, CTR-GCN, and Hyperformer) achieves improvement on NTU60, NTU120, and NW-UCLA benchmarks. We will release our code at: https://github.com/BUPTSJZhang/STD-CL.



### Cycle-Consistency Learning for Captioning and Grounding
- **Arxiv ID**: http://arxiv.org/abs/2312.15162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15162v1)
- **Published**: 2023-12-23 04:35:02+00:00
- **Updated**: 2023-12-23 04:35:02+00:00
- **Authors**: Ning Wang, Jiajun Deng, Mingbo Jia
- **Comment**: To appear in AAAI 2024
- **Journal**: None
- **Summary**: We present that visual grounding and image captioning, which perform as two mutually inverse processes, can be bridged together for collaborative training by careful designs. By consolidating this idea, we introduce CyCo, a cyclic-consistent learning framework to ameliorate the independent training pipelines of visual grounding and image captioning. The proposed framework (1) allows the semi-weakly supervised training of visual grounding; (2) improves the performance of fully supervised visual grounding; (3) yields a general captioning model that can describe arbitrary image regions. Extensive experiments show that our fully supervised grounding model achieves state-of-the-art performance, and the semi-weakly supervised one also exhibits competitive performance compared to the fully supervised counterparts. Our image captioning model has the capability to freely describe image regions and meanwhile shows impressive performance on prevalent captioning benchmarks.



### Pre-trained Trojan Attacks for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2312.15172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15172v1)
- **Published**: 2023-12-23 05:51:40+00:00
- **Updated**: 2023-12-23 05:51:40+00:00
- **Authors**: Aishan Liu, Xinwei Zhang, Yisong Xiao, Yuguang Zhou, Siyuan Liang, Jiakai Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Pre-trained vision models (PVMs) have become a dominant component due to their exceptional performance when fine-tuned for downstream tasks. However, the presence of backdoors within PVMs poses significant threats. Unfortunately, existing studies primarily focus on backdooring PVMs for the classification task, neglecting potential inherited backdoors in downstream tasks such as detection and segmentation. In this paper, we propose the Pre-trained Trojan attack, which embeds backdoors into a PVM, enabling attacks across various downstream vision tasks. We highlight the challenges posed by cross-task activation and shortcut connections in successful backdoor attacks. To achieve effective trigger activation in diverse tasks, we stylize the backdoor trigger patterns with class-specific textures, enhancing the recognition of task-irrelevant low-level features associated with the target class in the trigger pattern. Moreover, we address the issue of shortcut connections by introducing a context-free learning pipeline for poison training. In this approach, triggers without contextual backgrounds are directly utilized as training data, diverging from the conventional use of clean images. Consequently, we establish a direct shortcut from the trigger to the target class, mitigating the shortcut connection issue. We conducted extensive experiments to thoroughly validate the effectiveness of our attacks on downstream detection and segmentation tasks. Additionally, we showcase the potential of our approach in more practical scenarios, including large vision models and 3D object detection in autonomous driving. This paper aims to raise awareness of the potential threats associated with applying PVMs in practical scenarios. Our codes will be available upon paper publication.



### Narrowing the semantic gaps in U-Net with learnable skip connections: The case of medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2312.15182v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2312.15182v1)
- **Published**: 2023-12-23 07:39:42+00:00
- **Updated**: 2023-12-23 07:39:42+00:00
- **Authors**: Haonan Wang, Peng Cao, Xiaoli Liu, Jinzhu Yang, Osmar Zaiane
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art methods for medical image segmentation adopt the encoder-decoder architecture. However, this U-shaped framework still has limitations in capturing the non-local multi-scale information with a simple skip connection. To solve the problem, we firstly explore the potential weakness of skip connections in U-Net on multiple segmentation tasks, and find that i) not all skip connections are useful, each skip connection has different contribution; ii) the optimal combinations of skip connections are different, relying on the specific datasets. Based on our findings, we propose a new segmentation framework, named UDTransNet, to solve three semantic gaps in U-Net. Specifically, we propose a Dual Attention Transformer (DAT) module for capturing the channel- and spatial-wise relationships to better fuse the encoder features, and a Decoder-guided Recalibration Attention (DRA) module for effectively connecting the DAT tokens and the decoder features to eliminate the inconsistency. Hence, both modules establish a learnable connection to solve the semantic gaps between the encoder and the decoder, which leads to a high-performance segmentation model for medical images. Comprehensive experimental results indicate that our UDTransNet produces higher evaluation scores and finer segmentation results with relatively fewer parameters over the state-of-the-art segmentation methods on different public datasets. Code: https://github.com/McGregorWwww/UDTransNet.



### TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation
- **Arxiv ID**: http://arxiv.org/abs/2312.15197v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CL, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2312.15197v1)
- **Published**: 2023-12-23 08:45:57+00:00
- **Updated**: 2023-12-23 08:45:57+00:00
- **Authors**: Xize Cheng, Rongjie Huang, Linjun Li, Tao Jin, Zehan Wang, Aoxiong Yin, Minglei Li, Xinyu Duan, changpeng yang, Zhou Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Direct speech-to-speech translation achieves high-quality results through the introduction of discrete units obtained from self-supervised learning. This approach circumvents delays and cascading errors associated with model cascading. However, talking head translation, converting audio-visual speech (i.e., talking head video) from one language into another, still confronts several challenges compared to audio speech: (1) Existing methods invariably rely on cascading, synthesizing via both audio and text, resulting in delays and cascading errors. (2) Talking head translation has a limited set of reference frames. If the generated translation exceeds the length of the original speech, the video sequence needs to be supplemented by repeating frames, leading to jarring video transitions. In this work, we propose a model for talking head translation, \textbf{TransFace}, which can directly translate audio-visual speech into audio-visual speech in other languages. It consists of a speech-to-unit translation model to convert audio speech into discrete units and a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize synchronized audio-visual speech from discrete units in parallel. Furthermore, we introduce a Bounded Duration Predictor, ensuring isometric talking head translation and preventing duplicate reference frames. Experiments demonstrate that our proposed Unit2Lip model significantly improves synchronization (1.601 and 0.982 on LSE-C for the original and generated audio speech, respectively) and boosts inference speed by a factor of 4.35 on LRS2. Additionally, TransFace achieves impressive BLEU scores of 61.93 and 47.55 for Es-En and Fr-En on LRS3-T and 100% isochronous translations.



### Revealing Shadows: Low-Light Image Enhancement Using Self-Calibrated Illumination
- **Arxiv ID**: http://arxiv.org/abs/2312.15199v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15199v1)
- **Published**: 2023-12-23 08:49:19+00:00
- **Updated**: 2023-12-23 08:49:19+00:00
- **Authors**: Farzaneh Koohestani, Nader Karimi, Shadrokh Samavi
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: In digital imaging, enhancing visual content in poorly lit environments is a significant challenge, as images often suffer from inadequate brightness, hidden details, and an overall reduction in quality. This issue is especially critical in applications like nighttime surveillance, astrophotography, and low-light videography, where clear and detailed visual information is crucial. Our research addresses this problem by enhancing the illumination aspect of dark images. We have advanced past techniques by using varied color spaces to extract the illumination component, enhance it, and then recombine it with the other components of the image. By employing the Self-Calibrated Illumination (SCI) method, a strategy initially developed for RGB images, we effectively intensify and clarify details that are typically lost in low-light conditions. This method of selective illumination enhancement leaves the color information intact, thus preserving the color integrity of the image. Crucially, our method eliminates the need for paired images, making it suitable for situations where they are unavailable. Implementing the modified SCI technique represents a substantial shift from traditional methods, providing a refined and potent solution for low-light image enhancement. Our approach sets the stage for more complex image processing techniques and extends the range of possible real-world applications where accurate color representation and improved visibility are essential.



### Scale Optimization Using Evolutionary Reinforcement Learning for Object Detection on Drone Imagery
- **Arxiv ID**: http://arxiv.org/abs/2312.15219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15219v1)
- **Published**: 2023-12-23 10:49:55+00:00
- **Updated**: 2023-12-23 10:49:55+00:00
- **Authors**: Jialu Zhang, Xiaoying Yang, Wentao He, Jianfeng Ren, Qian Zhang, Titian Zhao, Ruibin Bai, Xiangjian He, Jiang Liu
- **Comment**: Accepted by AAAI 2024
- **Journal**: None
- **Summary**: Object detection in aerial imagery presents a significant challenge due to large scale variations among objects. This paper proposes an evolutionary reinforcement learning agent, integrated within a coarse-to-fine object detection framework, to optimize the scale for more effective detection of objects in such images. Specifically, a set of patches potentially containing objects are first generated. A set of rewards measuring the localization accuracy, the accuracy of predicted labels, and the scale consistency among nearby patches are designed in the agent to guide the scale optimization. The proposed scale-consistency reward ensures similar scales for neighboring objects of the same category. Furthermore, a spatial-semantic attention mechanism is designed to exploit the spatial semantic relations between patches. The agent employs the proximal policy optimization strategy in conjunction with the evolutionary strategy, effectively utilizing both the current patch status and historical experience embedded in the agent. The proposed model is compared with state-of-the-art methods on two benchmark datasets for object detection on drone imagery. It significantly outperforms all the compared methods.



### Regularized PolyKervNets: Optimizing Expressiveness and Efficiency for Private Inference in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2312.15229v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15229v1)
- **Published**: 2023-12-23 11:37:18+00:00
- **Updated**: 2023-12-23 11:37:18+00:00
- **Authors**: Toluwani Aremu
- **Comment**: Technical report on experimental findings published in the IACR
  Cryptology ePrint Archive (https://ia.cr/2023/1917). The report spans 7 pages
  and includes 8 tables
- **Journal**: None
- **Summary**: Private computation of nonlinear functions, such as Rectified Linear Units (ReLUs) and max-pooling operations, in deep neural networks (DNNs) poses significant challenges in terms of storage, bandwidth, and time consumption. To address these challenges, there has been a growing interest in utilizing privacy-preserving techniques that leverage polynomial activation functions and kernelized convolutions as alternatives to traditional ReLUs. However, these alternative approaches often suffer from a trade-off between achieving faster private inference (PI) and sacrificing model accuracy. In particular, when applied to much deeper networks, these methods encounter training instabilities, leading to issues like exploding gradients (resulting in NaNs) or suboptimal approximations. In this study, we focus on PolyKervNets, a technique known for offering improved dynamic approximations in smaller networks but still facing instabilities in larger and more complex networks. Our primary objective is to empirically explore optimization-based training recipes to enhance the performance of PolyKervNets in larger networks. By doing so, we aim to potentially eliminate the need for traditional nonlinear activation functions, thereby advancing the state-of-the-art in privacy-preserving deep neural network architectures. Code can be found on GitHub at: \url{https://github.com/tolusophy/PolyKervNets/}



### Sample selection with noise rate estimation in noise learning of medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/2312.15233v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07, I.4.8.b
- **Links**: [PDF](http://arxiv.org/pdf/2312.15233v1)
- **Published**: 2023-12-23 11:57:21+00:00
- **Updated**: 2023-12-23 11:57:21+00:00
- **Authors**: Maolin Li, Giacomo Tarroni, Vasilis Siomos
- **Comment**: 27 pages, 8 figures
- **Journal**: None
- **Summary**: Deep learning techniques have demonstrated remarkable success in the field of medical image analysis. However, the existence of label noise within data significantly hampers its performance. In this paper, we introduce a novel noise-robust learning method which integrates noise rate estimation into sample selection approaches for handling noisy datasets. We first estimate the noise rate of a dataset with Linear Regression based on the distribution of loss values. Then, potentially noisy samples are excluded based on this estimated noise rate, and sparse regularization is further employed to improve the robustness of our deep learning model. Our proposed method is evaluated on five benchmark medical image classification datasets, including two datasets featuring 3D medical images. Experiments show that our method outperforms other existing noise-robust learning methods, especially when noise rate is very big.



### Classifying Soccer Ball-on-Goal Position Through Kicker Shooting Action
- **Arxiv ID**: http://arxiv.org/abs/2312.15236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15236v1)
- **Published**: 2023-12-23 12:11:38+00:00
- **Updated**: 2023-12-23 12:11:38+00:00
- **Authors**: Javier Torón-Artiles, Daniel Hernández-Sosa, Oliverio J. Santana, Javier Lorenzo-Navarro, David Freire-Obregón
- **Comment**: Accepted at 13th International Conference on Pattern Recognition
  Applications (ICPRAM 2024)
- **Journal**: None
- **Summary**: This research addresses whether the ball's direction after a soccer free-kick can be accurately predicted solely by observing the shooter's kicking technique. To investigate this, we meticulously curated a dataset of soccer players executing free kicks and conducted manual temporal segmentation to identify the moment of the kick precisely. Our approach involves utilizing neural networks to develop a model that integrates Human Action Recognition (HAR) embeddings with contextual information, predicting the ball-on-goal position (BoGP) based on two temporal states: the kicker's run-up and the instant of the kick. The study encompasses a performance evaluation for eleven distinct HAR backbones, shedding light on their effectiveness in BoGP estimation during free-kick situations. An extra tabular metadata input is introduced, leading to an interesting model enhancement without introducing bias. The promising results reveal 69.1% accuracy when considering two primary BoGP classes: right and left. This underscores the model's proficiency in predicting the ball's destination towards the goal with high accuracy, offering promising implications for understanding free-kick dynamics in soccer.



### NoPose-NeuS: Jointly Optimizing Camera Poses with Neural Implicit Surfaces for Multi-view Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2312.15238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2312.15238v1)
- **Published**: 2023-12-23 12:18:22+00:00
- **Updated**: 2023-12-23 12:18:22+00:00
- **Authors**: Mohamed Shawky Sabae, Hoda Anis Baraka, Mayada Mansour Hadhoud
- **Comment**: None
- **Journal**: UniReps: the First Workshop on Unifying Representations in Neural
  Models (2023)
- **Summary**: Learning neural implicit surfaces from volume rendering has become popular for multi-view reconstruction. Neural surface reconstruction approaches can recover complex 3D geometry that are difficult for classical Multi-view Stereo (MVS) approaches, such as non-Lambertian surfaces and thin structures. However, one key assumption for these methods is knowing accurate camera parameters for the input multi-view images, which are not always available. In this paper, we present NoPose-NeuS, a neural implicit surface reconstruction method that extends NeuS to jointly optimize camera poses with the geometry and color networks. We encode the camera poses as a multi-layer perceptron (MLP) and introduce two additional losses, which are multi-view feature consistency and rendered depth losses, to constrain the learned geometry for better estimated camera poses and scene surfaces. Extensive experiments on the DTU dataset show that the proposed method can estimate relatively accurate camera poses, while maintaining a high surface reconstruction quality with 0.89 mean Chamfer distance.



### CaLDiff: Camera Localization in NeRF via Pose Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2312.15242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15242v1)
- **Published**: 2023-12-23 12:36:01+00:00
- **Updated**: 2023-12-23 12:36:01+00:00
- **Authors**: Rashik Shrestha, Bishad Koju, Abhigyan Bhusal, Danda Pani Paudel, François Rameau
- **Comment**: None
- **Journal**: None
- **Summary**: With the widespread use of NeRF-based implicit 3D representation, the need for camera localization in the same representation becomes manifestly apparent. Doing so not only simplifies the localization process -- by avoiding an outside-the-NeRF-based localization -- but also has the potential to offer the benefit of enhanced localization. This paper studies the problem of localizing cameras in NeRF using a diffusion model for camera pose adjustment. More specifically, given a pre-trained NeRF model, we train a diffusion model that iteratively updates randomly initialized camera poses, conditioned upon the image to be localized. At test time, a new camera is localized in two steps: first, coarse localization using the proposed pose diffusion process, followed by local refinement steps of a pose inversion process in NeRF. In fact, the proposed camera localization by pose diffusion (CaLDiff) method also integrates the pose inversion steps within the diffusion process. Such integration offers significantly better localization, thanks to our downstream refinement-aware diffusion process. Our exhaustive experiments on challenging real-world data validate our method by providing significantly better results than the compared methods and the established baselines. Our source code will be made publicly available.



### Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models
- **Arxiv ID**: http://arxiv.org/abs/2312.15247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.15247v1)
- **Published**: 2023-12-23 12:59:22+00:00
- **Updated**: 2023-12-23 12:59:22+00:00
- **Authors**: Gurusha Juneja, Sukrit Kumar
- **Comment**: Accepted at International Workshop on AI for Digital Human in AAAI
  Conference on Articial Intelligence (AAAI, 2024)
- **Journal**: None
- **Summary**: Diffusion models when conditioned on text prompts, generate realistic-looking images with intricate details. But most of these pre-trained models fail to generate accurate images when it comes to human features like hands, teeth, etc. We hypothesize that this inability of diffusion models can be overcome through well-annotated good-quality data. In this paper, we look specifically into improving the hand-object-interaction image generation using diffusion models. We collect a well annotated hand-object interaction synthetic dataset curated using Prompt-Propose-Verify framework and finetune a stable diffusion model on it. We evaluate the image-text dataset on qualitative and quantitative metrics like CLIPScore, ImageReward, Fedility, and alignment and show considerably better performance over the current state-of-the-art benchmarks.



### Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane
- **Arxiv ID**: http://arxiv.org/abs/2312.15253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.15253v1)
- **Published**: 2023-12-23 13:27:50+00:00
- **Updated**: 2023-12-23 13:27:50+00:00
- **Authors**: Chen Yang, Kailing Wang, Yuehao Wang, Qi Dou, Xiaokang Yang, Wei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Intraoperative imaging techniques for reconstructing deformable tissues in vivo are pivotal for advanced surgical systems. Existing methods either compromise on rendering quality or are excessively computationally intensive, often demanding dozens of hours to perform, which significantly hinders their practical application. In this paper, we introduce Fast Orthogonal Plane (Forplane), a novel, efficient framework based on neural radiance fields (NeRF) for the reconstruction of deformable tissues. We conceptualize surgical procedures as 4D volumes, and break them down into static and dynamic fields comprised of orthogonal neural planes. This factorization iscretizes the four-dimensional space, leading to a decreased memory usage and faster optimization. A spatiotemporal importance sampling scheme is introduced to improve performance in regions with tool occlusion as well as large motions and accelerate training. An efficient ray marching method is applied to skip sampling among empty regions, significantly improving inference speed. Forplane accommodates both binocular and monocular endoscopy videos, demonstrating its extensive applicability and flexibility. Our experiments, carried out on two in vivo datasets, the EndoNeRF and Hamlyn datasets, demonstrate the effectiveness of our framework. In all cases, Forplane substantially accelerates both the optimization process (by over 100 times) and the inference process (by over 15 times) while maintaining or even improving the quality across a variety of non-rigid deformations. This significant performance improvement promises to be a valuable asset for future intraoperative surgical applications. The code of our project is now available at https://github.com/Loping151/ForPlane.



### Human101: Training 100+FPS Human Gaussians in 100s from 1 View
- **Arxiv ID**: http://arxiv.org/abs/2312.15258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15258v1)
- **Published**: 2023-12-23 13:41:56+00:00
- **Updated**: 2023-12-23 13:41:56+00:00
- **Authors**: Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang
- **Comment**: Website: https://github.com/longxiang-ai/Human101
- **Journal**: None
- **Summary**: Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101.



### Self-Supervised Depth Completion Guided by 3D Perception and Geometry Consistency
- **Arxiv ID**: http://arxiv.org/abs/2312.15263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15263v1)
- **Published**: 2023-12-23 14:19:56+00:00
- **Updated**: 2023-12-23 14:19:56+00:00
- **Authors**: Yu Cai, Tianyu Shen, Shi-Sheng Huang, Hua Huang
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Depth completion, aiming to predict dense depth maps from sparse depth measurements, plays a crucial role in many computer vision related applications. Deep learning approaches have demonstrated overwhelming success in this task. However, high-precision depth completion without relying on the ground-truth data, which are usually costly, still remains challenging. The reason lies on the ignorance of 3D structural information in most previous unsupervised solutions, causing inaccurate spatial propagation and mixed-depth problems. To alleviate the above challenges, this paper explores the utilization of 3D perceptual features and multi-view geometry consistency to devise a high-precision self-supervised depth completion method. Firstly, a 3D perceptual spatial propagation algorithm is constructed with a point cloud representation and an attention weighting mechanism to capture more reasonable and favorable neighboring features during the iterative depth propagation process. Secondly, the multi-view geometric constraints between adjacent views are explicitly incorporated to guide the optimization of the whole depth completion model in a self-supervised manner. Extensive experiments on benchmark datasets of NYU-Depthv2 and VOID demonstrate that the proposed model achieves the state-of-the-art depth completion performance compared with other unsupervised methods, and competitive performance compared with previous supervised methods.



### MGDepth: Motion-Guided Cost Volume For Self-Supervised Monocular Depth In Dynamic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2312.15268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15268v1)
- **Published**: 2023-12-23 14:36:27+00:00
- **Updated**: 2023-12-23 14:36:27+00:00
- **Authors**: Kaichen Zhou, Jia-Xing Zhong, Jia-Wang Bian, Qian Xie, Jian-Qing Zheng, Niki Trigoni, Andrew Markham
- **Comment**: None
- **Journal**: None
- **Summary**: Despite advancements in self-supervised monocular depth estimation, challenges persist in dynamic scenarios due to the dependence on assumptions about a static world. In this paper, we present MGDepth, a Motion-Guided Cost Volume Depth Net, to achieve precise depth estimation for both dynamic objects and static backgrounds, all while maintaining computational efficiency. To tackle the challenges posed by dynamic content, we incorporate optical flow and coarse monocular depth to create a novel static reference frame. This frame is then utilized to build a motion-guided cost volume in collaboration with the target frame. Additionally, to enhance the accuracy and resilience of the network structure, we introduce an attention-based depth net architecture to effectively integrate information from feature maps with varying resolutions. Compared to methods with similar computational costs, MGDepth achieves a significant reduction of approximately seven percent in root-mean-square error for self-supervised monocular depth estimation on the KITTI-2015 dataset.



### SSFlowNet: Semi-supervised Scene Flow Estimation On Point Clouds With Pseudo Label
- **Arxiv ID**: http://arxiv.org/abs/2312.15271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15271v1)
- **Published**: 2023-12-23 14:43:52+00:00
- **Updated**: 2023-12-23 14:43:52+00:00
- **Authors**: Jingze Chen, Junfeng Yao, Qiqin Lin, Rongzhou Zhou, Lei Li
- **Comment**: None
- **Journal**: None
- **Summary**: In the domain of supervised scene flow estimation, the process of manual labeling is both time-intensive and financially demanding. This paper introduces SSFlowNet, a semi-supervised approach for scene flow estimation, that utilizes a blend of labeled and unlabeled data, optimizing the balance between the cost of labeling and the precision of model training. SSFlowNet stands out through its innovative use of pseudo-labels, mainly reducing the dependency on extensively labeled datasets while maintaining high model accuracy. The core of our model is its emphasis on the intricate geometric structures of point clouds, both locally and globally, coupled with a novel spatial memory feature. This feature is adept at learning the geometric relationships between points over sequential time frames. By identifying similarities between labeled and unlabeled points, SSFlowNet dynamically constructs a correlation matrix to evaluate scene flow dependencies at individual point level. Furthermore, the integration of a flow consistency module within SSFlowNet enhances its capability to consistently estimate flow, an essential aspect for analyzing dynamic scenes. Empirical results demonstrate that SSFlowNet surpasses existing methods in pseudo-label generation and shows adaptability across varying data volumes. Moreover, our semi-supervised training technique yields promising outcomes even with different smaller ratio labeled data, marking a substantial advancement in the field of scene flow estimation.



### Benefit from public unlabeled data: A Frangi filtering-based pretraining network for 3D cerebrovascular segmentation
- **Arxiv ID**: http://arxiv.org/abs/2312.15273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.15273v1)
- **Published**: 2023-12-23 14:47:21+00:00
- **Updated**: 2023-12-23 14:47:21+00:00
- **Authors**: Gen Shi, Hao Lu, Hui Hui, Jie Tian
- **Comment**: Under Review
- **Journal**: None
- **Summary**: The precise cerebrovascular segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) data is crucial for clinically computer-aided diagnosis. However, the sparse distribution of cerebrovascular structures in TOF-MRA results in an exceedingly high cost for manual data labeling. The use of unlabeled TOF-MRA data holds the potential to enhance model performance significantly. In this study, we construct the largest preprocessed unlabeled TOF-MRA datasets (1510 subjects) to date. We also provide three additional labeled datasets totaling 113 subjects. Furthermore, we propose a simple yet effective pertraining strategy based on Frangi filtering, known for enhancing vessel-like structures, to fully leverage the unlabeled data for 3D cerebrovascular segmentation. Specifically, we develop a Frangi filtering-based preprocessing workflow to handle the large-scale unlabeled dataset, and a multi-task pretraining strategy is proposed to effectively utilize the preprocessed data. By employing this approach, we maximize the knowledge gained from the unlabeled data. The pretrained model is evaluated on four cerebrovascular segmentation datasets. The results have demonstrated the superior performance of our model, with an improvement of approximately 3\% compared to state-of-the-art semi- and self-supervised methods. Furthermore, the ablation studies also demonstrate the generalizability and effectiveness of the pretraining method regarding the backbone structures. The code and data have been open source at: \url{https://github.com/shigen-StoneRoot/FFPN}.



### Understanding normalization in contrastive representation learning and out-of-distribution detection
- **Arxiv ID**: http://arxiv.org/abs/2312.15288v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2312.15288v1)
- **Published**: 2023-12-23 16:05:47+00:00
- **Updated**: 2023-12-23 16:05:47+00:00
- **Authors**: Tai Le-Gia, Jaehyun Ahn
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive representation learning has emerged as an outstanding approach for anomaly detection. In this work, we explore the $\ell_2$-norm of contrastive features and its applications in out-of-distribution detection. We propose a simple method based on contrastive learning, which incorporates out-of-distribution data by discriminating against normal samples in the contrastive layer space. Our approach can be applied flexibly as an outlier exposure (OE) approach, where the out-of-distribution data is a huge collective of random images, or as a fully self-supervised learning approach, where the out-of-distribution data is self-generated by applying distribution-shifting transformations. The ability to incorporate additional out-of-distribution samples enables a feasible solution for datasets where AD methods based on contrastive learning generally underperform, such as aerial images or microscopy images. Furthermore, the high-quality features learned through contrastive learning consistently enhance performance in OE scenarios, even when the available out-of-distribution dataset is not diverse enough. Our extensive experiments demonstrate the superiority of our proposed method under various scenarios, including unimodal and multimodal settings, with various image datasets.



### Wavelet Packet Power Spectrum Kullback-Leibler Divergence: A New Metric for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2312.15289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15289v1)
- **Published**: 2023-12-23 16:10:53+00:00
- **Updated**: 2023-12-23 16:10:53+00:00
- **Authors**: Lokesh Veeramacheneni, Moritz Wolter, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Current metrics for generative neural networks are biased towards low frequencies, specific generators, objects from the ImageNet dataset, and value texture more than shape. Many current quality metrics do not measure frequency information directly. In response, we propose a new frequency band-based quality metric, which opens a door into the frequency domain yet, at the same time, preserves spatial aspects of the data. Our metric works well even if the distributions we compare are far from ImageNet or have been produced by differing generator architectures. We verify the quality of our metric by sampling a broad selection of generative networks on a wide variety of data sets. A user study ensures our metric aligns with human perception. Furthermore, we show that frequency band guidance can improve the frequency domain fidelity of a current generative network.



### Make Me a BNN: A Simple Strategy for Estimating Bayesian Uncertainty from Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2312.15297v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2312.15297v1)
- **Published**: 2023-12-23 16:39:24+00:00
- **Updated**: 2023-12-23 16:39:24+00:00
- **Authors**: Gianni Franchi, Olivier Laurent, Maxence Leguéry, Andrei Bursuc, Andrea Pilzer, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are powerful tools for various computer vision tasks, yet they often struggle with reliable uncertainty quantification - a critical requirement for real-world applications. Bayesian Neural Networks (BNN) are equipped for uncertainty estimation but cannot scale to large DNNs that are highly unstable to train. To address this challenge, we introduce the Adaptable Bayesian Neural Network (ABNN), a simple and scalable strategy to seamlessly transform DNNs into BNNs in a post-hoc manner with minimal computational and training overheads. ABNN preserves the main predictive properties of DNNs while enhancing their uncertainty quantification abilities through simple BNN adaptation layers (attached to normalization layers) and a few fine-tuning steps on pre-trained models. We conduct extensive experiments across multiple datasets for image classification and semantic segmentation tasks, and our results demonstrate that ABNN achieves state-of-the-art performance without the computational budget typically associated with ensemble methods.



### Q-Boost: On Visual Quality Assessment Ability of Low-level Multi-Modality Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2312.15300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15300v1)
- **Published**: 2023-12-23 17:02:25+00:00
- **Updated**: 2023-12-23 17:02:25+00:00
- **Authors**: Zicheng Zhang, Haoning Wu, Zhongpeng Ji, Chunyi Li, Erli Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Fengyu Sun, Shangling Jui, Weisi Lin, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in Multi-modality Large Language Models (MLLMs) have demonstrated remarkable capabilities in complex high-level vision tasks. However, the exploration of MLLM potential in visual quality assessment, a vital aspect of low-level vision, remains limited. To address this gap, we introduce Q-Boost, a novel strategy designed to enhance low-level MLLMs in image quality assessment (IQA) and video quality assessment (VQA) tasks, which is structured around two pivotal components: 1) Triadic-Tone Integration: Ordinary prompt design simply oscillates between the binary extremes of $positive$ and $negative$. Q-Boost innovates by incorporating a `middle ground' approach through $neutral$ prompts, allowing for a more balanced and detailed assessment. 2) Multi-Prompt Ensemble: Multiple quality-centric prompts are used to mitigate bias and acquire more accurate evaluation. The experimental results show that the low-level MLLMs exhibit outstanding zeros-shot performance on the IQA/VQA tasks equipped with the Q-Boost strategy.



### Mitigating Algorithmic Bias on Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2312.15307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2312.15307v1)
- **Published**: 2023-12-23 17:41:30+00:00
- **Updated**: 2023-12-23 17:41:30+00:00
- **Authors**: Glauco Amigo, Pablo Rivas Perea, Robert J. Marks
- **Comment**: None
- **Journal**: None
- **Summary**: Biased datasets are ubiquitous and present a challenge for machine learning. For a number of categories on a dataset that are equally important but some are sparse and others are common, the learning algorithms will favor the ones with more presence. The problem of biased datasets is especially sensitive when dealing with minority people groups. How can we, from biased data, generate algorithms that treat every person equally? This work explores one way to mitigate bias using a debiasing variational autoencoder with experiments on facial expression recognition.



### Towards Generalization in Subitizing with Neuro-Symbolic Loss using Holographic Reduced Representations
- **Arxiv ID**: http://arxiv.org/abs/2312.15310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2312.15310v1)
- **Published**: 2023-12-23 17:54:03+00:00
- **Updated**: 2023-12-23 17:54:03+00:00
- **Authors**: Mohammad Mahmudul Alam, Edward Raff, Tim Oates
- **Comment**: Accepted in 38th Annual AAAI Workshop on Neuro-Symbolic Learning and
  Reasoning in the Era of Large Language Models (NuCLeaR), 2024
- **Journal**: None
- **Summary**: While deep learning has enjoyed significant success in computer vision tasks over the past decade, many shortcomings still exist from a Cognitive Science (CogSci) perspective. In particular, the ability to subitize, i.e., quickly and accurately identify the small (less than 6) count of items, is not well learned by current Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) when using a standard cross-entropy (CE) loss. In this paper, we demonstrate that adapting tools used in CogSci research can improve the subitizing generalization of CNNs and ViTs by developing an alternative loss function using Holographic Reduced Representations (HRRs). We investigate how this neuro-symbolic approach to learning affects the subitizing capability of CNNs and ViTs, and so we focus on specially crafted problems that isolate generalization to specific aspects of subitizing. Via saliency maps and out-of-distribution performance, we are able to empirically observe that the proposed HRR loss improves subitizing generalization though it does not completely solve the problem. In addition, we find that ViTs perform considerably worse compared to CNNs in most respects on subitizing, except on one axis where an HRR-based loss provides improvement.



### Pixel-Level Change Detection Pseudo-Label Learning for Remote Sensing Change Captioning
- **Arxiv ID**: http://arxiv.org/abs/2312.15311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15311v1)
- **Published**: 2023-12-23 17:58:48+00:00
- **Updated**: 2023-12-23 17:58:48+00:00
- **Authors**: Chenyang Liu, Keyan Chen, Zipeng Qi, Haotian Zhang, Zhengxia Zou, Zhenwei Shi
- **Comment**: None
- **Journal**: None
- **Summary**: The existing methods for Remote Sensing Image Change Captioning (RSICC) perform well in simple scenes but exhibit poorer performance in complex scenes. This limitation is primarily attributed to the model's constrained visual ability to distinguish and locate changes. Acknowledging the inherent correlation between change detection (CD) and RSICC tasks, we believe pixel-level CD is significant for describing the differences between images through language. Regrettably, the current RSICC dataset lacks readily available pixel-level CD labels. To address this deficiency, we leverage a model trained on existing CD datasets to derive CD pseudo-labels. We propose an innovative network with an auxiliary CD branch, supervised by pseudo-labels. Furthermore, a semantic fusion augment (SFA) module is proposed to fuse the feature information extracted by the CD branch, thereby facilitating the nuanced description of changes. Experiments demonstrate that our method achieves state-of-the-art performance and validate that learning pixel-level CD pseudo-labels significantly contributes to change captioning. Our code will be available at: https://github.com/Chen-Yang-Liu/Pix4Cap



### Multimodal Machine Learning Combining Facial Images and Clinical Texts Improves Diagnosis of Rare Genetic Diseases
- **Arxiv ID**: http://arxiv.org/abs/2312.15320v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, cs.MM, q-bio.GN
- **Links**: [PDF](http://arxiv.org/pdf/2312.15320v1)
- **Published**: 2023-12-23 18:40:25+00:00
- **Updated**: 2023-12-23 18:40:25+00:00
- **Authors**: Da Wu, Jingye Yang, Steven Klein, Cong Liu, Tzung-Chien Hsieh, Peter Krawitz, Chunhua Weng, Gholson J. Lyon, Jennifer M. Kalish, Kai Wang
- **Comment**: Comments are welcome!
- **Journal**: None
- **Summary**: Individuals with suspected rare genetic disorders often undergo multiple clinical evaluations, imaging studies, laboratory tests and genetic tests, to find a possible answer over a prolonged period of multiple years. Addressing this diagnostic odyssey thus have substantial clinical, psychosocial, and economic benefits. Many rare genetic diseases have distinctive facial features, which can be used by artificial intelligence algorithms to facilitate clinical diagnosis, in prioritizing candidate diseases to be further examined by lab tests or genetic assays, or in helping the phenotype-driven reinterpretation of genome/exome sequencing data. However, existing methods using frontal facial photo were built on conventional Convolutional Neural Networks (CNNs), rely exclusively on facial images, and cannot capture non-facial phenotypic traits and demographic information essential for guiding accurate diagnoses. Here we introduce GestaltMML, a multimodal machine learning (MML) approach solely based on the Transformer architecture. It integrates the facial images, demographic information (age, sex, ethnicity), and clinical notes of patients to improve prediction accuracy. Furthermore, we also introduce GestaltGPT, a GPT-based methodology with few-short learning capacities that exclusively harnesses textual inputs using a range of large language models (LLMs) including Llama 2, GPT-J and Falcon. We evaluated these methods on a diverse range of datasets, including 449 diseases from the GestaltMatcher Database, several in-house datasets on Beckwith-Wiedemann syndrome, Sotos syndrome, NAA10-related syndrome (neurodevelopmental syndrome) and others. Our results suggest that GestaltMML/GestaltGPT effectively incorporate multiple modalities of data, greatly narrow down candidate genetic diagnosis of rare diseases, and may facilitate the reinterpretation of genome/exome sequencing data.



### MaDi: Learning to Mask Distractions for Generalization in Visual Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2312.15339v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2312.15339v1)
- **Published**: 2023-12-23 20:11:05+00:00
- **Updated**: 2023-12-23 20:11:05+00:00
- **Authors**: Bram Grooten, Tristan Tomilin, Gautham Vasan, Matthew E. Taylor, A. Rupam Mahmood, Meng Fang, Mykola Pechenizkiy, Decebal Constantin Mocanu
- **Comment**: Accepted as full-paper (oral) at AAMAS 2024. Code is available at
  https://github.com/bramgrooten/mask-distractions and see our 40-second video
  at https://youtu.be/2oImF0h1k48
- **Journal**: None
- **Summary**: The visual world provides an abundance of information, but many input pixels received by agents often contain distracting stimuli. Autonomous agents need the ability to distinguish useful information from task-irrelevant perceptions, enabling them to generalize to unseen environments with new distractions. Existing works approach this problem using data augmentation or large auxiliary networks with additional loss functions. We introduce MaDi, a novel algorithm that learns to mask distractions by the reward signal only. In MaDi, the conventional actor-critic structure of deep reinforcement learning agents is complemented by a small third sibling, the Masker. This lightweight neural network generates a mask to determine what the actor and critic will receive, such that they can focus on learning the task. The masks are created dynamically, depending on the current input. We run experiments on the DeepMind Control Generalization Benchmark, the Distracting Control Suite, and a real UR5 Robotic Arm. Our algorithm improves the agent's focus with useful masks, while its efficient Masker network only adds 0.2% more parameters to the original structure, in contrast to previous work. MaDi consistently achieves generalization results better than or competitive to state-of-the-art methods.



### Scout-Net: Prospective Personalized Estimation of CT Organ Doses from Scout Views
- **Arxiv ID**: http://arxiv.org/abs/2312.15354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.15354v1)
- **Published**: 2023-12-23 21:33:42+00:00
- **Updated**: 2023-12-23 21:33:42+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Sen Wang, Debashish Pal, Sandeep Dutta, Bhavik Patel, Evan Zucker, Adam Wang
- **Comment**: 33 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: Purpose: Estimation of patient-specific organ doses is required for more comprehensive dose metrics, such as effective dose. Currently, available methods are performed retrospectively using the CT images themselves, which can only be done after the scan. To optimize CT acquisitions before scanning, rapid prediction of patient-specific organ dose is needed prospectively, using available scout images. We, therefore, devise an end-to-end, fully-automated deep learning solution to perform real-time, patient-specific, organ-level dosimetric estimation of CT scans.   Approach: We propose the Scout-Net model for CT dose prediction at six different organs as well as for the overall patient body, leveraging the routinely obtained frontal and lateral scout images of patients, before their CT scans. To obtain reference values of the organ doses, we used Monte Carlo simulation and 3D segmentation methods on the corresponding CT images of the patients.   Results: We validate our proposed Scout-Net model against real patient CT data and demonstrate the effectiveness in estimating organ doses in real-time (only 27 ms on average per scan). Additionally, we demonstrate the efficiency (real-time execution), sufficiency (reasonable error rates), and robustness (consistent across varying patient sizes) of the Scout-Net model.   Conclusions: An effective, efficient, and robust Scout-Net model, once incorporated into the CT acquisition plan, could potentially guide the automatic exposure control for balanced image quality and radiation dose.



### LETA: Learning Transferable Attribution for Generic Vision Explainer
- **Arxiv ID**: http://arxiv.org/abs/2312.15359v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15359v1)
- **Published**: 2023-12-23 21:49:23+00:00
- **Updated**: 2023-12-23 21:49:23+00:00
- **Authors**: Guanchu Wang, Yu-Neng Chuang, Fan Yang, Mengnan Du, Chia-Yuan Chang, Shaochen Zhong, Zirui Liu, Zhaozhuo Xu, Kaixiong Zhou, Xuanting Cai, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Explainable machine learning significantly improves the transparency of deep neural networks~(DNN). However, existing work is constrained to explaining the behavior of individual model predictions, and lacks the ability to transfer the explanation across various models and tasks. This limitation results in explaining various tasks being time- and resource-consuming. To address this problem, we develop a pre-trained, DNN-based, generic explainer on large-scale image datasets, and leverage its transferability to explain various vision models for downstream tasks. In particular, the pre-training of generic explainer focuses on LEarning Transferable Attribution (LETA). The transferable attribution takes advantage of the versatile output of the target backbone encoders to comprehensively encode the essential attribution for explaining various downstream tasks. LETA guides the pre-training of the generic explainer towards the transferable attribution, and introduces a rule-based adaptation of the transferable attribution for explaining downstream tasks, without the need for additional training on downstream data. Theoretical analysis demonstrates that the pre-training of LETA enables minimizing the explanation error bound aligned with the conditional $\mathcal{V}$-information on downstream tasks. Empirical studies involve explaining three different architectures of vision models across three diverse downstream datasets. The experiment results indicate LETA is effective in explaining these tasks without the need for additional training on the data of downstream tasks.



### BEV-CV: Birds-Eye-View Transform for Cross-View Geo-Localisation
- **Arxiv ID**: http://arxiv.org/abs/2312.15363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2312.15363v1)
- **Published**: 2023-12-23 22:20:45+00:00
- **Updated**: 2023-12-23 22:20:45+00:00
- **Authors**: Tavis Shore, Simon Hadfield, Oscar Mendez
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Cross-view image matching for geo-localisation is a challenging problem due to the significant visual difference between aerial and ground-level viewpoints. The method provides localisation capabilities from geo-referenced images, eliminating the need for external devices or costly equipment. This enhances the capacity of agents to autonomously determine their position, navigate, and operate effectively in environments where GPS signals are unavailable. Current research employs a variety of techniques to reduce the domain gap such as applying polar transforms to aerial images or synthesising between perspectives. However, these approaches generally rely on having a 360{\deg} field of view, limiting real-world feasibility. We propose BEV-CV, an approach which introduces two key novelties. Firstly we bring ground-level images into a semantic Birds-Eye-View before matching embeddings, allowing for direct comparison with aerial segmentation representations. Secondly, we introduce the use of a Normalised Temperature-scaled Cross Entropy Loss to the sub-field, achieving faster convergence than with the standard triplet loss. BEV-CV achieves state-of-the-art recall accuracies, improving feature extraction Top-1 rates by more than 300%, and Top-1% rates by approximately 150% for 70{\deg} crops, and for orientation-aware application we achieve a 35% Top-1 accuracy increase with 70{\deg} crops.



### WildScenes: A Benchmark for 2D and 3D Semantic Segmentation in Large-scale Natural Environments
- **Arxiv ID**: http://arxiv.org/abs/2312.15364v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.15364v1)
- **Published**: 2023-12-23 22:27:40+00:00
- **Updated**: 2023-12-23 22:27:40+00:00
- **Authors**: Kavisha Vidanapathirana, Joshua Knights, Stephen Hausler, Mark Cox, Milad Ramezani, Jason Jooste, Ethan Griffiths, Shaheer Mohamed, Sridha Sridharan, Clinton Fookes, Peyman Moghadam
- **Comment**: Under review. The first 3 authors contributed equally
- **Journal**: None
- **Summary**: Recent progress in semantic scene understanding has primarily been enabled by the availability of semantically annotated bi-modal (camera and lidar) datasets in urban environments. However, such annotated datasets are also needed for natural, unstructured environments to enable semantic perception for applications, including conservation, search and rescue, environment monitoring, and agricultural automation. Therefore, we introduce WildScenes, a bi-modal benchmark dataset consisting of multiple large-scale traversals in natural environments, including semantic annotations in high-resolution 2D images and dense 3D lidar point clouds, and accurate 6-DoF pose information. The data is (1) trajectory-centric with accurate localization and globally aligned point clouds, (2) calibrated and synchronized to support bi-modal inference, and (3) containing different natural environments over 6 months to support research on domain adaptation. Our 3D semantic labels are obtained via an efficient automated process that transfers the human-annotated 2D labels from multiple views into 3D point clouds, thus circumventing the need for expensive and time-consuming human annotation in 3D. We introduce benchmarks on 2D and 3D semantic segmentation and evaluate a variety of recent deep-learning techniques to demonstrate the challenges in semantic segmentation in natural environments. We propose train-val-test splits for standard benchmarks as well as domain adaptation benchmarks and utilize an automated split generation technique to ensure the balance of class label distributions. The data, evaluation scripts and pretrained models will be released upon acceptance at https://csiro-robotics.github.io/WildScenes.



