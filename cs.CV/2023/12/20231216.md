# Arxiv Papers in cs.CV on 2023-12-16
### Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data
- **Arxiv ID**: http://arxiv.org/abs/2312.10271v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2312.10271v1)
- **Published**: 2023-12-16 00:23:21+00:00
- **Updated**: 2023-12-16 00:23:21+00:00
- **Authors**: Kang Lin, Reinhard Heckel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based methods for image reconstruction are state-of-the-art for a variety of imaging tasks. However, neural networks often perform worse if the training data differs significantly from the data they are applied to. For example, a network trained for accelerated magnetic resonance imaging (MRI) on one scanner performs worse on another scanner. In this work, we investigate the impact of the training data on the model's performance and robustness for accelerated MRI. We find that models trained on the combination of various data distributions, such as those obtained from different MRI scanners and anatomies, exhibit robustness equal or superior to models trained on the best single distribution for a specific target distribution. Thus training on diverse data tends to improve robustness. Furthermore, training on diverse data does not compromise in-distribution performance, i.e., a model trained on diverse data yields in-distribution performance at least as good as models trained on the more narrow individual distributions. Our results suggest that training a model for imaging on a variety of distributions tends to yield a more effective and robust model than maintaining separate models for individual distributions.



### RetailKLIP : Finetuning OpenCLIP backbone using metric learning on a single GPU for Zero-shot retail product image classification
- **Arxiv ID**: http://arxiv.org/abs/2312.10282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10282v1)
- **Published**: 2023-12-16 01:23:42+00:00
- **Updated**: 2023-12-16 01:23:42+00:00
- **Authors**: Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Retail product or packaged grocery goods images need to classified in various computer vision applications like self checkout stores, supply chain automation and retail execution evaluation. Previous works explore ways to finetune deep models for this purpose. But because of the fact that finetuning a large model or even linear layer for a pretrained backbone requires to run at least a few epochs of gradient descent for every new retail product added in classification range, frequent retrainings are needed in a real world scenario. In this work, we propose finetuning the vision encoder of a CLIP model in a way that its embeddings can be easily used for nearest neighbor based classification, while also getting accuracy close to or exceeding full finetuning. A nearest neighbor based classifier needs no incremental training for new products, thus saving resources and wait time.



### Image Restoration Through Generalized Ornstein-Uhlenbeck Bridge
- **Arxiv ID**: http://arxiv.org/abs/2312.10299v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2312.10299v1)
- **Published**: 2023-12-16 03:09:28+00:00
- **Updated**: 2023-12-16 03:09:28+00:00
- **Authors**: Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du, Pengxu Wei, Dongyu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models possess powerful generative capabilities enabling the mapping of noise to data using reverse stochastic differential equations. However, in image restoration tasks, the focus is on the mapping relationship from low-quality images to high-quality images. To address this, we introduced the Generalized Ornstein-Uhlenbeck Bridge (GOUB) model. By leveraging the natural mean-reverting property of the generalized OU process and further adjusting the variance of its steady-state distribution through the Doob's h-transform, we achieve diffusion mappings from point to point with minimal cost. This allows for end-to-end training, enabling the recovery of high-quality images from low-quality ones. Additionally, we uncovered the mathematical essence of some bridge models, all of which are special cases of the GOUB and empirically demonstrated the optimality of our proposed models. Furthermore, benefiting from our distinctive parameterization mechanism, we proposed the Mean-ODE model that is better at capturing pixel-level information and structural perceptions. Experimental results show that both models achieved state-of-the-art results in various tasks, including inpainting, deraining, and super-resolution. Code is available at https://github.com/Hammour-steak/GOUB.



### Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos
- **Arxiv ID**: http://arxiv.org/abs/2312.10300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10300v2)
- **Published**: 2023-12-16 03:17:30+00:00
- **Updated**: 2023-12-19 02:04:18+00:00
- **Authors**: Mingfei Han, Linjie Yang, Xiaojun Chang, Heng Wang
- **Comment**: See https://mingfei.info/shot2story for updates and more information
- **Journal**: None
- **Summary**: A short clip of video may contain progression of multiple events and an interesting story line. A human need to capture both the event in every shot and associate them together to understand the story behind it. In this work, we present a new multi-shot video understanding benchmark Shot2Story20K with detailed shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. We design several distinct tasks including single-shot video and narration captioning, multi-shot video summarization, and video retrieval with shot descriptions. Preliminary experiments show some challenges to generate a long and comprehensive video summary. Nevertheless, the generated imperfect summaries can already significantly boost the performance of existing video understanding tasks such as video question-answering, promoting an under-explored setting of video understanding with detailed summaries.



### Mapping Housing Stock Characteristics from Drone Images for Climate Resilience in the Caribbean
- **Arxiv ID**: http://arxiv.org/abs/2312.10306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10306v1)
- **Published**: 2023-12-16 03:49:40+00:00
- **Updated**: 2023-12-16 03:49:40+00:00
- **Authors**: Isabelle Tingzon, Nuala Margaret Cowan, Pierre Chrzanowski
- **Comment**: Tackling Climate Change with Machine Learning: Workshop at NeurIPS
  2023
- **Journal**: None
- **Summary**: Comprehensive information on housing stock is crucial for climate adaptation initiatives aiming to reduce the adverse impacts of climate-extreme hazards in high-risk regions like the Caribbean. In this study, we propose a workflow for rapidly generating critical baseline housing stock data using very high-resolution drone images and deep learning techniques. Specifically, our work leverages the Segment Anything Model and convolutional neural networks for the automated generation of building footprints and roof classification maps. By strengthening local capacity within government agencies to leverage AI and Earth Observation-based solutions, this work seeks to improve the climate resilience of the housing sector in small island developing states in the Caribbean.



### DeepCalliFont: Few-shot Chinese Calligraphy Font Synthesis by Integrating Dual-modality Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2312.10314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10314v1)
- **Published**: 2023-12-16 04:23:12+00:00
- **Updated**: 2023-12-16 04:23:12+00:00
- **Authors**: Yitian Liu, Zhouhui Lian
- **Comment**: AAAI2024
- **Journal**: None
- **Summary**: Few-shot font generation, especially for Chinese calligraphy fonts, is a challenging and ongoing problem. With the help of prior knowledge that is mainly based on glyph consistency assumptions, some recently proposed methods can synthesize high-quality Chinese glyph images. However, glyphs in calligraphy font styles often do not meet these assumptions. To address this problem, we propose a novel model, DeepCalliFont, for few-shot Chinese calligraphy font synthesis by integrating dual-modality generative models. Specifically, the proposed model consists of image synthesis and sequence generation branches, generating consistent results via a dual-modality representation learning strategy. The two modalities (i.e., glyph images and writing sequences) are properly integrated using a feature recombination module and a rasterization loss function. Furthermore, a new pre-training strategy is adopted to improve the performance by exploiting large amounts of uni-modality data. Both qualitative and quantitative experiments have been conducted to demonstrate the superiority of our method to other state-of-the-art approaches in the task of few-shot Chinese calligraphy font synthesis. The source code can be found at https://github.com/lsflyt-pku/DeepCalliFont.



### Symmetrical Bidirectional Knowledge Alignment for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2312.10320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10320v1)
- **Published**: 2023-12-16 04:50:34+00:00
- **Updated**: 2023-12-16 04:50:34+00:00
- **Authors**: Decheng Liu, Xu Luo, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of zero-shot sketch-based image retrieval (ZS-SBIR), which aims to use sketches from unseen categories as queries to match the images of the same category. Due to the large cross-modality discrepancy, ZS-SBIR is still a challenging task and mimics realistic zero-shot scenarios. The key is to leverage transferable knowledge from the pre-trained model to improve generalizability. Existing researchers often utilize the simple fine-tuning training strategy or knowledge distillation from a teacher model with fixed parameters, lacking efficient bidirectional knowledge alignment between student and teacher models simultaneously for better generalization. In this paper, we propose a novel Symmetrical Bidirectional Knowledge Alignment for zero-shot sketch-based image retrieval (SBKA). The symmetrical bidirectional knowledge alignment learning framework is designed to effectively learn mutual rich discriminative information between teacher and student models to achieve the goal of knowledge alignment. Instead of the former one-to-one cross-modality matching in the testing stage, a one-to-many cluster cross-modality matching method is proposed to leverage the inherent relationship of intra-class images to reduce the adverse effects of the existing modality gap. Experiments on several representative ZS-SBIR datasets (Sketchy Ext dataset, TU-Berlin Ext dataset and QuickDraw Ext dataset) prove the proposed algorithm can achieve superior performance compared with state-of-the-art methods.



### Federated Learning with Instance-Dependent Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2312.10324v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.10324v1)
- **Published**: 2023-12-16 05:08:02+00:00
- **Updated**: 2023-12-16 05:08:02+00:00
- **Authors**: Lei Wang, Jieming Bian, Jie Xu
- **Comment**: Accepted by ICASSP 2024
- **Journal**: None
- **Summary**: Federated learning (FL) with noisy labels poses a significant challenge. Existing methods designed for handling noisy labels in centralized learning tend to lose their effectiveness in the FL setting, mainly due to the small dataset size and the heterogeneity of client data. While some attempts have been made to tackle FL with noisy labels, they primarily focused on scenarios involving class-conditional noise. In this paper, we study the more challenging and practical issue of instance-dependent noise (IDN) in FL. We introduce a novel algorithm called FedBeat (Federated Learning with Bayesian Ensemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global statistically consistent classifier using the IDN transition matrix (IDNTM), which encompasses three synergistic steps: (1) A federated data extraction step that constructs a weak global model and extracts high-confidence data using a Bayesian model ensemble method. (2) A federated transition matrix estimation step in which clients collaboratively train an IDNTM estimation network based on the extracted data. (3) A federated classifier correction step that enhances the global model's performance by training it using a loss function tailored for noisy labels, leveraging the IDNTM. Experiments conducted on CIFAR-10 and SVHN verify that the proposed method significantly outperforms state-of-the-art methods.



### Self-supervised Adaptive Weighting for Cooperative Perception in V2V Communications
- **Arxiv ID**: http://arxiv.org/abs/2312.10342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2312.10342v1)
- **Published**: 2023-12-16 06:21:09+00:00
- **Updated**: 2023-12-16 06:21:09+00:00
- **Authors**: Chenguang Liu, Jianjun Chen, Yunfei Chen, Ryan Payton, Michael Riley, Shuang-Hua Yang
- **Comment**: accepted by IEEE Transactions on Intelligent Vehicles
- **Journal**: None
- **Summary**: Perception of the driving environment is critical for collision avoidance and route planning to ensure driving safety. Cooperative perception has been widely studied as an effective approach to addressing the shortcomings of single-vehicle perception. However, the practical limitations of vehicle-to-vehicle (V2V) communications have not been adequately investigated. In particular, current cooperative fusion models rely on supervised models and do not address dynamic performance degradation caused by arbitrary channel impairments. In this paper, a self-supervised adaptive weighting model is proposed for intermediate fusion to mitigate the adverse effects of channel distortion. The performance of cooperative perception is investigated in different system settings. Rician fading and imperfect channel state information (CSI) are also considered. Numerical results demonstrate that the proposed adaptive weighting algorithm significantly outperforms the benchmarks without weighting. Visualization examples validate that the proposed weighting algorithm can flexibly adapt to various channel conditions. Moreover, the adaptive weighting algorithm demonstrates good generalization to untrained channels and test datasets from different domains.



### MMBaT: A Multi-task Framework for mmWave-based Human Body Reconstruction and Translation Prediction
- **Arxiv ID**: http://arxiv.org/abs/2312.10346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10346v1)
- **Published**: 2023-12-16 06:28:19+00:00
- **Updated**: 2023-12-16 06:28:19+00:00
- **Authors**: Jiarui Yang, Songpengcheng Xia, Yifan Song, Qi Wu, Ling Pei
- **Comment**: 5 pages, 2 figures, accepted by IEEE ICASSP 2024
- **Journal**: None
- **Summary**: Human body reconstruction with Millimeter Wave (mmWave) radar point clouds has gained significant interest due to its ability to work in adverse environments and its capacity to mitigate privacy concerns associated with traditional camera-based solutions. Despite pioneering efforts in this field, two challenges persist. Firstly, raw point clouds contain massive noise points, usually caused by the ambient objects and multi-path effects of Radio Frequency (RF) signals. Recent approaches typically rely on prior knowledge or elaborate preprocessing methods, limiting their applicability. Secondly, even after noise removal, the sparse and inconsistent body-related points pose an obstacle to accurate human body reconstruction. To address these challenges, we introduce mmBaT, a novel multi-task deep learning framework that concurrently estimates the human body and predicts body translations in subsequent frames to extract body-related point clouds. Our method is evaluated on two public datasets that are collected with different radar devices and noise levels. A comprehensive comparison against other state-of-the-art methods demonstrates our method has a superior reconstruction performance and generalization ability from noisy raw data, even when compared to methods provided with body-related point clouds.



### Exploring UMAP in hybrid models of entropy-based and representativeness sampling for active learning in biomedical segmentation
- **Arxiv ID**: http://arxiv.org/abs/2312.10361v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2312.10361v1)
- **Published**: 2023-12-16 07:40:09+00:00
- **Updated**: 2023-12-16 07:40:09+00:00
- **Authors**: H. S. Tan, Kuancheng Wang, Rafe Mcbeth
- **Comment**: 19 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we study various hybrid models of entropy-based and representativeness sampling techniques in the context of active learning in medical segmentation, in particular examining the role of UMAP (Uniform Manifold Approximation and Projection) as a technique for capturing representativeness. Although UMAP has been shown viable as a general purpose dimension reduction method in diverse areas, its role in deep learning-based medical segmentation has yet been extensively explored. Using the cardiac and prostate datasets in the Medical Segmentation Decathlon for validation, we found that a novel hybrid combination of Entropy-UMAP sampling technique achieved a statistically significant Dice score advantage over the random baseline ($3.2 \%$ for cardiac, $4.5 \%$ for prostate), and attained the highest Dice coefficient among the spectrum of 10 distinct active learning methodologies we examined. This provides preliminary evidence that there is an interesting synergy between entropy-based and UMAP methods when the former precedes the latter in a hybrid model of active learning.



### Fusing Conditional Submodular GAN and Programmatic Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2312.10366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10366v1)
- **Published**: 2023-12-16 07:49:13+00:00
- **Updated**: 2023-12-16 07:49:13+00:00
- **Authors**: Kumar Shubham, Pranav Sastry, Prathosh AP
- **Comment**: Published in AAAI 2024
- **Journal**: None
- **Summary**: Programmatic Weak Supervision (PWS) and generative models serve as crucial tools that enable researchers to maximize the utility of existing datasets without resorting to laborious data gathering and manual annotation processes. PWS uses various weak supervision techniques to estimate the underlying class labels of data, while generative models primarily concentrate on sampling from the underlying distribution of the given dataset. Although these methods have the potential to complement each other, they have mostly been studied independently. Recently, WSGAN proposed a mechanism to fuse these two models. Their approach utilizes the discrete latent factors of InfoGAN to train the label model and leverages the class-dependent information of the label model to generate images of specific classes. However, the disentangled latent factors learned by InfoGAN might not necessarily be class-specific and could potentially affect the label model's accuracy. Moreover, prediction made by the label model is often noisy in nature and can have a detrimental impact on the quality of images generated by GAN. In our work, we address these challenges by (i) implementing a noise-aware classifier using the pseudo labels generated by the label model (ii) utilizing the noise-aware classifier's prediction to train the label model and generate class-conditional images. Additionally, we also investigate the effect of training the classifier with a subset of the dataset within a defined uncertainty budget on pseudo labels. We accomplish this by formalizing the subset selection problem as a submodular maximization objective with a knapsack constraint on the entropy of pseudo labels. We conduct experiments on multiple datasets and demonstrate the efficacy of our methods on several tasks vis-a-vis the current state-of-the-art methods.



### SA$^2$VP: Spatially Aligned-and-Adapted Visual Prompt
- **Arxiv ID**: http://arxiv.org/abs/2312.10376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10376v1)
- **Published**: 2023-12-16 08:23:43+00:00
- **Updated**: 2023-12-16 08:23:43+00:00
- **Authors**: Wenjie Pei, Tongqi Xia, Fanglin Chen, Jinsong Li, Jiandong Tian, Guangming Lu
- **Comment**: Accepted by AAAI 2024
- **Journal**: None
- **Summary**: As a prominent parameter-efficient fine-tuning technique in NLP, prompt tuning is being explored its potential in computer vision. Typical methods for visual prompt tuning follow the sequential modeling paradigm stemming from NLP, which represents an input image as a flattened sequence of token embeddings and then learns a set of unordered parameterized tokens prefixed to the sequence representation as the visual prompts for task adaptation of large vision models. While such sequential modeling paradigm of visual prompt has shown great promise, there are two potential limitations. First, the learned visual prompts cannot model the underlying spatial relations in the input image, which is crucial for image encoding. Second, since all prompt tokens play the same role of prompting for all image tokens without distinction, it lacks the fine-grained prompting capability, i.e., individual prompting for different image tokens. In this work, we propose the \mymodel model (\emph{SA$^2$VP}), which learns a two-dimensional prompt token map with equal (or scaled) size to the image token map, thereby being able to spatially align with the image map. Each prompt token is designated to prompt knowledge only for the spatially corresponding image tokens. As a result, our model can conduct individual prompting for different image tokens in a fine-grained manner. Moreover, benefiting from the capability of preserving the spatial structure by the learned prompt token map, our \emph{SA$^2$VP} is able to model the spatial relations in the input image, leading to more effective prompting. Extensive experiments on three challenging benchmarks for image classification demonstrate the superiority of our model over other state-of-the-art methods for visual prompt tuning. Code is available at \emph{https://github.com/tommy-xq/SA2VP}.



### ElasticLaneNet: A Geometry-Flexible Approach for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2312.10389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10389v1)
- **Published**: 2023-12-16 09:04:44+00:00
- **Updated**: 2023-12-16 09:04:44+00:00
- **Authors**: Yaxin Feng, Yuan Lan, Luchan Zhang, Yang Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: The task of lane detection involves identifying the boundaries of driving areas. Recognizing lanes with complex and variable geometric structures remains a challenge. In this paper, we introduce a new lane detection framework named ElasticLaneNet (Elastic-interaction-energy guided Lane detection Network). A novel and flexible way of representing lanes, namely, implicit representation is proposed. The training strategy considers predicted lanes as moving curves that being attracted to the ground truth guided by an elastic interaction energy based loss function (EIE loss). An auxiliary feature refinement (AFR) module is designed to gather information from different layers. The method performs well in complex lane scenarios, including those with large curvature, weak geometric features at intersections, complicated cross lanes, Y-shapes lanes, dense lanes, etc. We apply our approach on three datasets: SDLane, CULane, and TuSimple. The results demonstrate the exceptional performance of our method, with the state-of-the-art results on the structure-diversity dataset SDLane, achieving F1-score of 89.51, Recall rate of 87.50, and Precision of 91.61.



### Not Every Side Is Equal: Localization Uncertainty Estimation for Semi-Supervised 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2312.10390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10390v1)
- **Published**: 2023-12-16 09:08:03+00:00
- **Updated**: 2023-12-16 09:08:03+00:00
- **Authors**: ChuXin Wang, Wenfei Yang, Tianzhu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised 3D object detection from point cloud aims to train a detector with a small number of labeled data and a large number of unlabeled data. The core of existing methods lies in how to select high-quality pseudo-labels using the designed quality evaluation criterion. However, these methods treat each pseudo bounding box as a whole and assign equal importance to each side during training, which is detrimental to model performance due to many sides having poor localization quality. Besides, existing methods filter out a large number of low-quality pseudo-labels, which also contain some correct regression values that can help with model training. To address the above issues, we propose a side-aware framework for semi-supervised 3D object detection consisting of three key designs: a 3D bounding box parameterization method, an uncertainty estimation module, and a pseudo-label selection strategy. These modules work together to explicitly estimate the localization quality of each side and assign different levels of importance during the training phase. Extensive experiment results demonstrate that the proposed method can consistently outperform baseline models under different scenes and evaluation criteria. Moreover, our method achieves state-of-the-art performance on three datasets with different labeled ratios.



### DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content
- **Arxiv ID**: http://arxiv.org/abs/2312.10407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2312.10407v1)
- **Published**: 2023-12-16 10:17:09+00:00
- **Updated**: 2023-12-16 10:17:09+00:00
- **Authors**: Wentao Wang, Xuanyao Huang, Swalpa Kumar Roy
- **Comment**: This is the initial version of this work, and a more comprehensive
  and improved version will be updated later
- **Journal**: None
- **Summary**: This paper explores the image synthesis capabilities of GPT-4, a leading multi-modal large language model. We establish a benchmark for evaluating the fidelity of texture features in images generated by GPT-4, comprising manually painted pictures and their AI-generated counterparts. The contributions of this study are threefold: First, we provide an in-depth analysis of the fidelity of image synthesis features based on GPT-4, marking the first such study on this state-of-the-art model. Second, the quantitative and qualitative experiments fully reveals the limitations of the GPT-4 model in image synthesis. Third, we have compiled a unique benchmark of manual drawings and corresponding GPT-4-generated images, introducing a new task to advance fidelity research in AI-generated content (AIGC). The dataset will be available after being accepted: \url{https://github.com/rickwang28574/DeepArt}. We hope this study will fuel knowledge, scholarship, and innovation, inspiring uses that transform how we discover and understand the world of art and promote the development of AIGC while retaining respect for art.



### Learning Dense Correspondence for NeRF-Based Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2312.10422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10422v2)
- **Published**: 2023-12-16 11:31:34+00:00
- **Updated**: 2023-12-19 03:12:59+00:00
- **Authors**: Songlin Yang, Wei Wang, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong
- **Comment**: Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence, 2024
- **Journal**: None
- **Summary**: Face reenactment is challenging due to the need to establish dense correspondence between various face representations for motion transfer. Recent studies have utilized Neural Radiance Field (NeRF) as fundamental representation, which further enhanced the performance of multi-view face reenactment in photo-realism and 3D consistency. However, establishing dense correspondence between different face NeRFs is non-trivial, because implicit representations lack ground-truth correspondence annotations like mesh-based 3D parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning 3DMM space with NeRF-based face representations can realize motion control, it is sub-optimal for their limited face-only modeling and low identity fidelity. Therefore, we are inspired to ask: Can we learn the dense correspondence between different NeRF-based face representations without a 3D parametric model prior? To address this challenge, we propose a novel framework, which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three components: canonical tri-planes, identity deformations, and motion. In terms of motion control, our key contribution is proposing a Plane Dictionary (PlaneDict) module, which efficiently maps the motion conditions to a linear weighted addition of learnable orthogonal plane bases. To the best of our knowledge, our framework is the first method that achieves one-shot multi-view face reenactment without a 3D parametric model prior. Extensive experiments demonstrate that we produce better results in fine-grained motion control and identity preservation than previous methods.



### Tender Notice Extraction from E-papers Using Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2312.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10437v1)
- **Published**: 2023-12-16 12:54:28+00:00
- **Updated**: 2023-12-16 12:54:28+00:00
- **Authors**: Ashmin Bhattarai, Anuj Sedhai, Devraj Neupane, Manish Khadka, Rama Bastola
- **Comment**: None
- **Journal**: None
- **Summary**: Tender notices are usually sought by most of the companies at regular intervals as a means for obtaining the contracts of various projects. These notices consist of all the required information like description of the work, period of construction, estimated amount of project, etc. In the context of Nepal, tender notices are usually published in national as well as local newspapers. The interested bidders should search all the related tender notices in newspapers. However, it is very tedious for these companies to manually search tender notices in every newspaper and figure out which bid is best suited for them. This project is built with the purpose of solving this tedious task of manually searching the tender notices. Initially, the newspapers are downloaded in PDF format using the selenium library of python. After downloading the newspapers, the e-papers are scanned and tender notices are automatically extracted using a neural network. For extraction purposes, different architectures of CNN namely ResNet, GoogleNet and Xception are used and a model with highest performance has been implemented. Finally, these extracted notices are then published on the website and are accessible to the users. This project is helpful for construction companies as well as contractors assuring quality and efficiency. This project has great application in the field of competitive bidding as well as managing them in a systematic manner.



### Simple Image-level Classification Improves Open-vocabulary Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2312.10439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10439v2)
- **Published**: 2023-12-16 13:06:15+00:00
- **Updated**: 2023-12-19 11:43:07+00:00
- **Authors**: Ruohuan Fang, Guansong Pang, Xiao Bai
- **Comment**: Accepted at AAAI 2024
- **Journal**: None
- **Summary**: Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a given set of base categories on which the detection model is trained. Recent OVOD methods focus on adapting the image-level pre-trained vision-language models (VLMs), such as CLIP, to a region-level object detection task via, eg., region-level knowledge distillation, regional prompt learning, or region-text pre-training, to expand the detection vocabulary. These methods have demonstrated remarkable performance in recognizing regional visual concepts, but they are weak in exploiting the VLMs' powerful global scene understanding ability learned from the billion-scale image-level text descriptions. This limits their capability in detecting hard objects of small, blurred, or occluded appearance from novel/base categories, whose detection heavily relies on contextual information. To address this, we propose a novel approach, namely Simple Image-level Classification for Context-Aware Detection Scoring (SIC-CADS), to leverage the superior global knowledge yielded from CLIP for complementing the current OVOD models from a global perspective. The core of SIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the object co-occurrence-based contextual information from CLIP to recognize all possible object categories in the scene. These image-level MLR scores can then be utilized to refine the instance-level detection scores of the current OVOD models in detecting those hard objects. This is verified by extensive empirical results on two popular benchmarks, OV-LVIS and OV-COCO, which show that SIC-CADS achieves significant and consistent improvement when combined with different types of OVOD models. Further, SIC-CADS also improves the cross-dataset generalization ability on Objects365 and OpenImages. The code is available at https://github.com/mala-lab/SIC-CADS.



### Finger Biometric Recognition With Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2312.10447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10447v2)
- **Published**: 2023-12-16 13:39:52+00:00
- **Updated**: 2023-12-19 14:22:43+00:00
- **Authors**: Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri
- **Comment**: 34 pages. The Biometric Computing: Recognition and Registration, 2019
- **Journal**: None
- **Summary**: Biometrics is indispensable in this modern digital era for secure automated human authentication in various fields of machine learning and pattern recognition. Hand geometry is a promising physiological biometric trait with ample deployed application areas for identity verification. Due to the intricate anatomic foundation of the thumb and substantial inter-finger posture variation, satisfactory performances cannot be achieved while the thumb is included in the contact-free environment. To overcome the hindrances associated with the thumb, four finger-based (excluding the thumb) biometric approaches have been devised. In this chapter, a four-finger based biometric method has been presented. Again, selection of salient features is essential to reduce the feature dimensionality by eliminating the insignificant features. Weights are assigned according to the discriminative efficiency of the features to emphasize on the essential features. Two different strategies namely, the global and local feature selection methods are adopted based on the adaptive forward-selection and backward-elimination (FoBa) algorithm. The identification performances are evaluated using the weighted k-nearest neighbor (wk-NN) and random forest (RF) classifiers. The experiments are conducted using the selected feature subsets over the 300 subjects of the Bosphorus hand database. The best identification accuracy of 98.67%, and equal error rate (EER) of 4.6% have been achieved using the subset of 25 features which are selected by the rank-based local FoBa algorithm.



### Semantic-Aware Autoregressive Image Modeling for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2312.10457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10457v1)
- **Published**: 2023-12-16 14:03:10+00:00
- **Updated**: 2023-12-16 14:03:10+00:00
- **Authors**: Kaiyou Song, Shan Zhang, Tong Wang
- **Comment**: Accepted by AAAI2024
- **Journal**: None
- **Summary**: The development of autoregressive modeling (AM) in computer vision lags behind natural language processing (NLP) in self-supervised pre-training. This is mainly caused by the challenge that images are not sequential signals and lack a natural order when applying autoregressive modeling. In this study, inspired by human beings' way of grasping an image, i.e., focusing on the main object first, we present a semantic-aware autoregressive image modeling (SemAIM) method to tackle this challenge. The key insight of SemAIM is to autoregressive model images from the semantic patches to the less semantic patches. To this end, we first calculate a semantic-aware permutation of patches according to their feature similarities and then perform the autoregression procedure based on the permutation. In addition, considering that the raw pixels of patches are low-level signals and are not ideal prediction targets for learning high-level semantic representation, we also explore utilizing the patch features as the prediction targets. Extensive experiments are conducted on a broad range of downstream tasks, including image classification, object detection, and instance/semantic segmentation, to evaluate the performance of SemAIM. The results demonstrate SemAIM achieves state-of-the-art performance compared with other self-supervised methods. Specifically, with ViT-B, SemAIM achieves 84.1% top-1 accuracy for fine-tuning on ImageNet, 51.3% AP and 45.4% AP for object detection and instance segmentation on COCO, which outperforms the vanilla MAE by 0.5%, 1.0%, and 0.5%, respectively.



### Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2312.10461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10461v1)
- **Published**: 2023-12-16 14:27:06+00:00
- **Updated**: 2023-12-16 14:27:06+00:00
- **Authors**: Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, Yunchao Wei
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Recently, the proliferation of highly realistic synthetic images, facilitated through a variety of GANs and Diffusions, has significantly heightened the susceptibility to misuse. While the primary focus of deepfake detection has traditionally centered on the design of detection algorithms, an investigative inquiry into the generator architectures has remained conspicuously absent in recent years. This paper contributes to this lacuna by rethinking the architectures of CNN-based generators, thereby establishing a generalized representation of synthetic artifacts. Our findings illuminate that the up-sampling operator can, beyond frequency-based artifacts, produce generalized forgery artifacts. In particular, the local interdependence among image pixels caused by upsampling operators is significantly demonstrated in synthetic images generated by GAN or diffusion. Building upon this observation, we introduce the concept of Neighboring Pixel Relationships(NPR) as a means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. A comprehensive analysis is conducted on an open-world dataset, comprising samples generated by \tft{28 distinct generative models}. This analysis culminates in the establishment of a novel state-of-the-art performance, showcasing a remarkable \tft{12.8\%} improvement over existing methods. The code is available at https://github.com/chuangchuangtan/NPR-DeepfakeDetection.



### Fusion of Deep and Shallow Features for Face Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2312.10462v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.10462v1)
- **Published**: 2023-12-16 14:36:43+00:00
- **Updated**: 2023-12-16 14:36:43+00:00
- **Authors**: Belabbaci El Ouanas, Khammari Mohammed, Chouchane Ammar, Mohcene Bessaoudi, Abdelmalik Ouamane, Akram Abderraouf Gharbi
- **Comment**: arXiv admin note: text overlap with arXiv:2312.03562
- **Journal**: None
- **Summary**: Kinship verification from face images is a novel and formidable challenge in the realms of pattern recognition and computer vision. This work makes notable contributions by incorporating a preprocessing technique known as Multiscale Retinex (MSR), which enhances image quality. Our approach harnesses the strength of complementary deep (VGG16) and shallow texture descriptors (BSIF) by combining them at the score level using Logistic Regression (LR) technique. We assess the effectiveness of our approach by conducting comprehensive experiments on three challenging kinship datasets: Cornell Kin Face, UB Kin Face and TS Kin Face



### Unveiling Empirical Pathologies of Laplace Approximation for Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2312.10464v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.10464v1)
- **Published**: 2023-12-16 14:46:24+00:00
- **Updated**: 2023-12-16 14:46:24+00:00
- **Authors**: Maksim Zhdanov, Stanislav Dereka, Sergey Kolesnikov
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we critically evaluate Bayesian methods for uncertainty estimation in deep learning, focusing on the widely applied Laplace approximation and its variants. Our findings reveal that the conventional method of fitting the Hessian matrix negatively impacts out-of-distribution (OOD) detection efficiency. We propose a different point of view, asserting that focusing solely on optimizing prior precision can yield more accurate uncertainty estimates in OOD detection while preserving adequate calibration metrics. Moreover, we demonstrate that this property is not connected to the training stage of a model but rather to its intrinsic properties. Through extensive experimental evaluation, we establish the superiority of our simplified approach over traditional methods in the out-of-distribution domain.



### Enhancing Person Re-Identification through Tensor Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2312.10470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.10470v1)
- **Published**: 2023-12-16 15:04:07+00:00
- **Updated**: 2023-12-16 15:04:07+00:00
- **Authors**: Akram Abderraouf Gharbi, Ammar Chouchane, Mohcene Bessaoudi, Abdelmalik Ouamane, El ouanas Belabbaci
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel person reidentification (PRe-ID) system that based on tensor feature representation and multilinear subspace learning. Our approach utilizes pretrained CNNs for high-level feature extraction, along with Local Maximal Occurrence (LOMO) and Gaussian Of Gaussian (GOG ) descriptors. Additionally, Cross-View Quadratic Discriminant Analysis (TXQDA) algorithm is used for multilinear subspace learning, which models the data in a tensor framework to enhance discriminative capabilities. Similarity measure based on Mahalanobis distance is used for matching between training and test pedestrian images. Experimental evaluations on VIPeR and PRID450s datasets demonstrate the effectiveness of our method.



### A new method color MS-BSIF Features learning for the robust kinship verification
- **Arxiv ID**: http://arxiv.org/abs/2312.10482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10482v1)
- **Published**: 2023-12-16 15:21:51+00:00
- **Updated**: 2023-12-16 15:21:51+00:00
- **Authors**: Rachid Aliradi, Abdealmalik Ouamane, Abdeslam Amrane
- **Comment**: None
- **Journal**: None
- **Summary**: the paper presents a new method color MS-BSIF learning and MS-LBP for the kinship verification is the machine's ability to identify the genetic and blood the relationship and its degree between the facial images of humans. Facial verification of kinship refers to the task of training a machine to recognize the blood relationship between a pair of faces parent and non-parent (verification) based on features extracted from facial images, and determining the exact type or degree of this genetic relationship. We use the LBP and color BSIF learning features for the comparison and the TXQDA method for dimensionality reduction and data classification. We let's test the kinship facial verification application is namely the kinface Cornell database. This system improves the robustness of learning while controlling efficiency. The experimental results obtained and compared to other methods have proven the reliability of our framework and surpass the performance of other state-of-the-art techniques.



### All Attention U-NET for Semantic Segmentation of Intracranial Hemorrhages In Head CT Images
- **Arxiv ID**: http://arxiv.org/abs/2312.10483v1
- **DOI**: 10.1109/BioCAS54905.2022.9948588
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.10483v1)
- **Published**: 2023-12-16 15:23:22+00:00
- **Updated**: 2023-12-16 15:23:22+00:00
- **Authors**: Chia Shuo Chang, Tian Sheuan Chang, Jiun Lin Yan, Li Ko
- **Comment**: 2022 IEEE Biomedical Circuits and Systems Conference (BioCAS)
- **Journal**: None
- **Summary**: Intracranial hemorrhages in head CT scans serve as a first line tool to help specialists diagnose different types. However, their types have diverse shapes in the same type but similar confusing shape, size and location between types. To solve this problem, this paper proposes an all attention U-Net. It uses channel attentions in the U-Net encoder side to enhance class specific feature extraction, and space and channel attentions in the U-Net decoder side for more accurate shape extraction and type classification. The simulation results show up to a 31.8\% improvement compared to baseline, ResNet50 + U-Net, and better performance than in cases with limited attention.



### PETDet: Proposal Enhancement for Two-Stage Fine-Grained Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2312.10515v1
- **DOI**: 10.1109/TGRS.2023.3343453
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10515v1)
- **Published**: 2023-12-16 18:04:56+00:00
- **Updated**: 2023-12-16 18:04:56+00:00
- **Authors**: Wentao Li, Danpei Zhao, Bo Yuan, Yue Gao, Zhenwei Shi
- **Comment**: IEEE TGRS 2023
- **Journal**: None
- **Summary**: Fine-grained object detection (FGOD) extends object detection with the capability of fine-grained recognition. In recent two-stage FGOD methods, the region proposal serves as a crucial link between detection and fine-grained recognition. However, current methods overlook that some proposal-related procedures inherited from general detection are not equally suitable for FGOD, limiting the multi-task learning from generation, representation, to utilization. In this paper, we present PETDet (Proposal Enhancement for Two-stage fine-grained object detection) to better handle the sub-tasks in two-stage FGOD methods. Firstly, an anchor-free Quality Oriented Proposal Network (QOPN) is proposed with dynamic label assignment and attention-based decomposition to generate high-quality oriented proposals. Additionally, we present a Bilinear Channel Fusion Network (BCFN) to extract independent and discriminative features of the proposals. Furthermore, we design a novel Adaptive Recognition Loss (ARL) which offers guidance for the R-CNN head to focus on high-quality proposals. Extensive experiments validate the effectiveness of PETDet. Quantitative analysis reveals that PETDet with ResNet50 reaches state-of-the-art performance on various FGOD datasets, including FAIR1M-v1.0 (42.96 AP), FAIR1M-v2.0 (48.81 AP), MAR20 (85.91 AP) and ShipRSImageNet (74.90 AP). The proposed method also achieves superior compatibility between accuracy and inference speed. Our code and models will be released at https://github.com/canoe-Z/PETDet.



### Transformers in Unsupervised Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2312.10529v1
- **DOI**: 10.1007/978-3-031-45725-8_14
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2312.10529v1)
- **Published**: 2023-12-16 20:00:34+00:00
- **Updated**: 2023-12-16 20:00:34+00:00
- **Authors**: Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz
- **Comment**: International Joint Conference on Computer Vision, Imaging and
  Computer Graphics. Cham: Springer Nature Switzerland, 2022. Published at
  "Communications in Computer and Information Science, vol 1815. Springer
  Nature". arXiv admin note: text overlap with arXiv:2202.03131
- **Journal**: None
- **Summary**: Transformers have revolutionized deep learning based computer vision with improved performance as well as robustness to natural corruptions and adversarial attacks. Transformers are used predominantly for 2D vision tasks, including image classification, semantic segmentation, and object detection. However, robots and advanced driver assistance systems also require 3D scene understanding for decision making by extracting structure-from-motion (SfM). We propose a robust transformer-based monocular SfM method that learns to predict monocular pixel-wise depth, ego vehicle's translation and rotation, as well as camera's focal length and principal point, simultaneously. With experiments on KITTI and DDAD datasets, we demonstrate how to adapt different vision transformers and compare them against contemporary CNN-based methods. Our study shows that transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust against natural corruptions, as well as untargeted and targeted attacks.



### How to Train Neural Field Representations: A Comprehensive Study and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2312.10531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10531v1)
- **Published**: 2023-12-16 20:10:23+00:00
- **Updated**: 2023-12-16 20:10:23+00:00
- **Authors**: Samuele Papa, Riccardo Valperga, David Knigge, Miltiadis Kofinas, Phillip Lippe, Jan-Jakob Sonke, Efstratios Gavves
- **Comment**: None
- **Journal**: None
- **Summary**: Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields.   In this work, we propose $\verb|fit-a-nef|$, a JAX-based library that leverages parallelization to enable fast optimization of large-scale NeF datasets, resulting in a significant speed-up. With this library, we perform a comprehensive study that investigates the effects of different hyperparameters -- including initialization, network architecture, and optimization strategies -- on fitting NeFs for downstream tasks. Our study provides valuable insights on how to train NeFs and offers guidance for optimizing their effectiveness in downstream applications. Finally, based on the proposed library and our analysis, we propose Neural Field Arena, a benchmark consisting of neural field variants of popular vision datasets, including MNIST, CIFAR, variants of ImageNet, and ShapeNetv2. Our library and the Neural Field Arena will be open-sourced to introduce standardized benchmarking and promote further research on neural fields.



### Rethinking Robustness of Model Attributions
- **Arxiv ID**: http://arxiv.org/abs/2312.10534v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2312.10534v1)
- **Published**: 2023-12-16 20:20:38+00:00
- **Updated**: 2023-12-16 20:20:38+00:00
- **Authors**: Sandesh Kamath, Sankalp Mittal, Amit Deshpande, Vineeth N Balasubramanian
- **Comment**: Accepted AAAI 2024
- **Journal**: None
- **Summary**: For machine learning models to be reliable and trustworthy, their decisions must be interpretable. As these models find increasing use in safety-critical applications, it is important that not just the model predictions but also their explanations (as feature attributions) be robust to small human-imperceptible input perturbations. Recent works have shown that many attribution methods are fragile and have proposed improvements in either these methods or the model training. We observe two main causes for fragile attributions: first, the existing metrics of robustness (e.g., top-k intersection) over-penalize even reasonable local shifts in attribution, thereby making random perturbations to appear as a strong attack, and second, the attribution can be concentrated in a small region even when there are multiple important parts in an image. To rectify this, we propose simple ways to strengthen existing metrics and attribution methods that incorporate locality of pixels in robustness metrics and diversity of pixel locations in attributions. Towards the role of model training in attributional robustness, we empirically observe that adversarially trained models have more robust attributions on smaller datasets, however, this advantage disappears in larger datasets. Code is available at https://github.com/ksandeshk/LENS.



### DETER: Detecting Edited Regions for Deterring Generative Manipulations
- **Arxiv ID**: http://arxiv.org/abs/2312.10539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2312.10539v1)
- **Published**: 2023-12-16 20:38:02+00:00
- **Updated**: 2023-12-16 20:38:02+00:00
- **Authors**: Sai Wang, Ye Zhu, Ruoyu Wang, Amaya Dharmasiri, Olga Russakovsky, Yu Wu
- **Comment**: First two authors contribute equally to this work. Project page at
  https://deter2024.github.io/deter/
- **Journal**: None
- **Summary**: Generative AI capabilities have grown substantially in recent years, raising renewed concerns about potential malicious use of generated data, or "deep fakes". However, deep fake datasets have not kept up with generative AI advancements sufficiently to enable the development of deep fake detection technology which can meaningfully alert human users in real-world settings. Existing datasets typically use GAN-based models and introduce spurious correlations by always editing similar face regions. To counteract the shortcomings, we introduce DETER, a large-scale dataset for DETEcting edited image Regions and deterring modern advanced generative manipulations. DETER includes 300,000 images manipulated by four state-of-the-art generators with three editing operations: face swapping (a standard coarse image manipulation), inpainting (a novel manipulation for deep fake datasets), and attribute editing (a subtle fine-grained manipulation). While face swapping and attribute editing are performed on similar face regions such as eyes and nose, the inpainting operation can be performed on random image regions, removing the spurious correlations of previous datasets. Careful image post-processing is performed to ensure deep fakes in DETER look realistic, and human studies confirm that human deep fake detection rate on DETER is 20.4% lower than on other fake datasets. Equipped with the dataset, we conduct extensive experiments and break-down analysis using our rich annotations and improved benchmark protocols, revealing future directions and the next set of challenges in developing reliable regional fake detection models.



### VecFusion: Vector Font Generation with Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2312.10540v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2312.10540v1)
- **Published**: 2023-12-16 20:49:00+00:00
- **Updated**: 2023-12-16 20:49:00+00:00
- **Authors**: Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis
- **Comment**: None
- **Journal**: None
- **Summary**: We present VecFusion, a new neural architecture that can generate vector fonts with varying topological structures and precise control point positions. Our approach is a cascaded diffusion model which consists of a raster diffusion model followed by a vector diffusion model. The raster model generates low-resolution, rasterized fonts with auxiliary control point information, capturing the global style and shape of the font, while the vector model synthesizes vector fonts conditioned on the low-resolution raster fonts from the first stage. To synthesize long and complex curves, our vector diffusion model uses a transformer architecture and a novel vector representation that enables the modeling of diverse vector geometry and the precise prediction of control points. Our experiments show that, in contrast to previous generative models for vector graphics, our new cascaded vector diffusion model generates higher quality vector fonts, with complex structures and diverse styles.



