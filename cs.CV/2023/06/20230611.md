# Arxiv Papers in cs.CV on 2023-06-11
### Hinting Pipeline and Multivariate Regression CNN for Maize Kernel Counting on the Ear
- **Arxiv ID**: http://arxiv.org/abs/2306.06553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06553v1)
- **Published**: 2023-06-11 00:58:38+00:00
- **Updated**: 2023-06-11 00:58:38+00:00
- **Authors**: Felipe Araújo, Igor Gadelha, Rodrigo Tsukahara, Luiz Pita, Filipe Costa, Igor Vaz, Andreza Santos, Guilherme Fôlego
- **Comment**: None
- **Journal**: None
- **Summary**: Maize is a highly nutritional cereal widely used for human and animal consumption and also as raw material by the biofuels industries. This highlights the importance of precisely quantifying the corn grain productivity in season, helping the commercialization process, operationalization, and critical decision-making. Considering the manual labor cost of counting maize kernels, we propose in this work a novel preprocessing pipeline named hinting that guides the attention of the model to the center of the corn kernels and enables a deep learning model to deliver better performance, given a picture of one side of the corn ear. Also, we propose a multivariate CNN regressor that outperforms single regression results. Experiments indicated that the proposed approach excels the current manual estimates, obtaining MAE of 34.4 and R2 of 0.74 against 35.38 and 0.72 for the manual estimate, respectively.



### Semantically-aware Mask CycleGAN for Translating Artistic Portraits to Photo-realistic Visualizations
- **Arxiv ID**: http://arxiv.org/abs/2306.06577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06577v1)
- **Published**: 2023-06-11 03:58:09+00:00
- **Updated**: 2023-06-11 03:58:09+00:00
- **Authors**: Zhuohao Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation (I2I) is defined as a computer vision task where the aim is to transfer images in a source domain to a target domain with minimal loss or alteration of the content representations. Major progress has been made since I2I was proposed with the invention of a variety of revolutionary generative models. Among them, GAN-based models perform exceptionally well as they are mostly tailor-made for specific domains or tasks. However, few works proposed a tailor-made method for the artistic domain. In this project, I propose the Semantic-aware Mask CycleGAN (SMCycleGAN) architecture which can translate artistic portraits to photo-realistic visualizations. This model can generate realistic human portraits by feeding the discriminators semantically masked fake samples, thus enforcing them to make discriminative decisions with partial information so that the generators can be optimized to synthesize more realistic human portraits instead of increasing the similarity of other irrelevant components, such as the background. Experiments have shown that the SMCycleGAN generate images with significantly increased realism and minimal loss of content representations.



### REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge
- **Arxiv ID**: http://arxiv.org/abs/2306.06583v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/2306.06583v1)
- **Published**: 2023-06-11 04:15:56+00:00
- **Updated**: 2023-06-11 04:15:56+00:00
- **Authors**: Siyang Song, Micol Spitale, Cheng Luo, German Barquero, Cristina Palmero, Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, Elisabeth Andre, Hatice Gunes
- **Comment**: None
- **Journal**: None
- **Summary**: The Multi-modal Multiple Appropriate Facial Reaction Generation Challenge (REACT2023) is the first competition event focused on evaluating multimedia processing and machine learning techniques for generating human-appropriate facial reactions in various dyadic interaction scenarios, with all participants competing strictly under the same conditions. The goal of the challenge is to provide the first benchmark test set for multi-modal information processing and to foster collaboration among the audio, visual, and audio-visual affective computing communities, to compare the relative merits of the approaches to automatic appropriate facial reaction generation under different spontaneous dyadic interaction conditions. This paper presents: (i) novelties, contributions and guidelines of the REACT2023 challenge; (ii) the dataset utilized in the challenge; and (iii) the performance of baseline systems on the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation, respectively. The challenge baseline code is publicly available at \url{https://github.com/reactmultimodalchallenge/baseline_react2023}.



### Compositional Prototypical Networks for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2306.06584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06584v1)
- **Published**: 2023-06-11 04:16:12+00:00
- **Updated**: 2023-06-11 04:16:12+00:00
- **Authors**: Qiang Lyu, Weiqiang Wang
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: It is assumed that pre-training provides the feature extractor with strong class transferability and that high novel class generalization can be achieved by simply reusing the transferable feature extractor. In this work, our motivation is to explicitly learn some fine-grained and transferable meta-knowledge so that feature reusability can be further improved. Concretely, inspired by the fact that humans can use learned concepts or components to help them recognize novel classes, we propose Compositional Prototypical Networks (CPN) to learn a transferable prototype for each human-annotated attribute, which we call a component prototype. We empirically demonstrate that the learned component prototypes have good class transferability and can be reused to construct compositional prototypes for novel classes. Then a learnable weight generator is utilized to adaptively fuse the compositional and visual prototypes. Extensive experiments demonstrate that our method can achieve state-of-the-art results on different datasets and settings. The performance gains are especially remarkable in the 5-way 1-shot setting. The code is available at https://github.com/fikry102/CPN.



### Progressive Class-Wise Attention (PCA) Approach for Diagnosing Skin Lesions
- **Arxiv ID**: http://arxiv.org/abs/2306.07300v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.07300v1)
- **Published**: 2023-06-11 04:58:31+00:00
- **Updated**: 2023-06-11 04:58:31+00:00
- **Authors**: Asim Naveed, Syed S. Naqvi, Tariq M. Khan, Imran Razzak
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between different classes. This paper introduces a novel class-wise attention technique that equally regards each class while unearthing more specific details about skin lesions. This attention mechanism is progressively used to amalgamate discriminative feature details from multiple scales. The introduced technique demonstrated impressive performance, surpassing more than 15 cutting-edge methods including the winners of HAM1000 and ISIC 2019 leaderboards. It achieved an impressive accuracy rate of 97.40% on the HAM10000 dataset and 94.9% on the ISIC 2019 dataset.



### Neural Projection Mapping Using Reflectance Fields
- **Arxiv ID**: http://arxiv.org/abs/2306.06595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06595v1)
- **Published**: 2023-06-11 05:33:10+00:00
- **Updated**: 2023-06-11 05:33:10+00:00
- **Authors**: Yotam Erel, Daisuke Iwai, Amit H. Bermano
- **Comment**: Project page: https://yoterel.github.io/nepmap-project-page/
- **Journal**: None
- **Summary**: We introduce a high resolution spatially adaptive light source, or a projector, into a neural reflectance field that allows to both calibrate the projector and photo realistic light editing. The projected texture is fully differentiable with respect to all scene parameters, and can be optimized to yield a desired appearance suitable for applications in augmented reality and projection mapping. Our neural field consists of three neural networks, estimating geometry, material, and transmittance. Using an analytical BRDF model and carefully selected projection patterns, our acquisition process is simple and intuitive, featuring a fixed uncalibrated projected and a handheld camera with a co-located light source. As we demonstrate, the virtual projector incorporated into the pipeline improves scene understanding and enables various projection mapping applications, alleviating the need for time consuming calibration steps performed in a traditional setting per view or projector location. In addition to enabling novel viewpoint synthesis, we demonstrate state-of-the-art performance projector compensation for novel viewpoints, improvement over the baselines in material and scene reconstruction, and three simply implemented scenarios where projection image optimization is performed, including the use of a 2D generative model to consistently dictate scene appearance from multiple viewpoints. We believe that neural projection mapping opens up the door to novel and exciting downstream tasks, through the joint optimization of the scene and projection images.



### Weakly Supervised Visual Question Answer Generation
- **Arxiv ID**: http://arxiv.org/abs/2306.06622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06622v1)
- **Published**: 2023-06-11 08:46:42+00:00
- **Updated**: 2023-06-11 08:46:42+00:00
- **Authors**: Charani Alampalle, Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition, Pages: 5588-5596, 2023
- **Summary**: Growing interest in conversational agents promote twoway human-computer communications involving asking and answering visual questions have become an active area of research in AI. Thus, generation of visual questionanswer pair(s) becomes an important and challenging task. To address this issue, we propose a weakly-supervised visual question answer generation method that generates a relevant question-answer pairs for a given input image and associated caption. Most of the prior works are supervised and depend on the annotated question-answer datasets. In our work, we present a weakly supervised method that synthetically generates question-answer pairs procedurally from visual information and captions. The proposed method initially extracts list of answer words, then does nearest question generation that uses the caption and answer word to generate synthetic question. Next, the relevant question generator converts the nearest question to relevant language question by dependency parsing and in-order tree traversal, finally, fine-tune a ViLBERT model with the question-answer pair(s) generated at end. We perform an exhaustive experimental analysis on VQA dataset and see that our model significantly outperform SOTA methods on BLEU scores. We also show the results wrt baseline models and ablation study.



### Adaptive Multi-Teacher Knowledge Distillation with Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.06634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06634v1)
- **Published**: 2023-06-11 09:38:45+00:00
- **Updated**: 2023-06-11 09:38:45+00:00
- **Authors**: Hailin Zhang, Defang Chen, Can Wang
- **Comment**: ICME 2023
- **Journal**: None
- **Summary**: Multi-Teacher knowledge distillation provides students with additional supervision from multiple pre-trained teachers with diverse information sources. Most existing methods explore different weighting strategies to obtain a powerful ensemble teacher, while ignoring the student with poor learning ability may not benefit from such specialized integrated knowledge. To address this problem, we propose Adaptive Multi-teacher Knowledge Distillation with Meta-Learning (MMKD) to supervise student with appropriate knowledge from a tailored ensemble teacher. With the help of a meta-weight network, the diverse yet compatible teacher knowledge in the output layer and intermediate layers is jointly leveraged to enhance the student performance. Extensive experiments on multiple benchmark datasets validate the effectiveness and flexibility of our methods. Code is available: https://github.com/Rorozhl/MMKD.



### 2-D SSM: A General Spatial Layer for Visual Transformers
- **Arxiv ID**: http://arxiv.org/abs/2306.06635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, F.2.2, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2306.06635v1)
- **Published**: 2023-06-11 09:41:37+00:00
- **Updated**: 2023-06-11 09:41:37+00:00
- **Authors**: Ethan Baron, Itamar Zimerman, Lior Wolf
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: A central objective in computer vision is to design models with appropriate 2-D inductive bias. Desiderata for 2D inductive bias include two-dimensional position awareness, dynamic spatial locality, and translation and permutation invariance. To address these goals, we leverage an expressive variation of the multidimensional State Space Model (SSM). Our approach introduces efficient parameterization, accelerated computation, and a suitable normalization scheme. Empirically, we observe that incorporating our layer at the beginning of each transformer block of Vision Transformers (ViT) significantly enhances performance for multiple ViT backbones and across datasets. The new layer is effective even with a negligible amount of additional parameters and inference time. Ablation studies and visualizations demonstrate that the layer has a strong 2-D inductive bias. For example, vision transformers equipped with our layer exhibit effective performance even without positional encoding



### Face0: Instantaneously Conditioning a Text-to-Image Model on a Face
- **Arxiv ID**: http://arxiv.org/abs/2306.06638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06638v1)
- **Published**: 2023-06-11 09:52:03+00:00
- **Updated**: 2023-06-11 09:52:03+00:00
- **Authors**: Dani Valevski, Danny Wasserman, Yossi Matias, Yaniv Leviathan
- **Comment**: None
- **Journal**: None
- **Summary**: We present Face0, a novel way to instantaneously condition a text-to-image generation model on a face, in sample time, without any optimization procedures such as fine-tuning or inversions. We augment a dataset of annotated images with embeddings of the included faces and train an image generation model, on the augmented dataset. Once trained, our system is practically identical at inference time to the underlying base model, and is therefore able to generate images, given a user-supplied face image and a prompt, in just a couple of seconds. Our method achieves pleasing results, is remarkably simple, extremely fast, and equips the underlying model with new capabilities, like controlling the generated images both via text or via direct manipulation of the input face embeddings. In addition, when using a fixed random vector instead of a face embedding from a user supplied image, our method essentially solves the problem of consistent character generation across images. Finally, while requiring further research, we hope that our method, which decouples the model's textual biases from its biases on faces, might be a step towards some mitigation of biases in future text-to-image models.



### VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.06656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.06656v1)
- **Published**: 2023-06-11 12:00:33+00:00
- **Updated**: 2023-06-11 12:00:33+00:00
- **Authors**: Xu Zhang, Kailun Yang, Jiacheng Lin, Jin Yuan, Zhiyong Li, Shutao Li
- **Comment**: Code will be made publicly available at
  https://github.com/XuZhang1211/VPUFormer
- **Journal**: None
- **Summary**: The integration of diverse visual prompts like clicks, scribbles, and boxes in interactive image segmentation could significantly facilitate user interaction as well as improve interaction efficiency. Most existing studies focus on a single type of visual prompt by simply concatenating prompts and images as input for segmentation prediction, which suffers from low-efficiency prompt representation and weak interaction issues. This paper proposes a simple yet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a concise unified prompt representation with deeper interaction to boost the segmentation performance. Specifically, we design a Prompt-unified Encoder (PuE) by using Gaussian mapping to generate a unified one-dimensional vector for click, box, and scribble prompts, which well captures users' intentions as well as provides a denser representation of user prompts. In addition, we present a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback to gradually refine candidate semantic features, aiming to bring image semantic features closer to the features that are similar to the user prompt, while pushing away those image semantic features that are dissimilar to the user prompt, thereby correcting results that deviate from expectations. On this basis, our approach injects prompt representations as queries into Dual-cross Merging Attention (DMA) blocks to perform a deeper interaction between image and query inputs. A comprehensive variety of experiments on seven challenging datasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL achieves consistent improvements, yielding state-of-the-art segmentation performance. Our code will be made publicly available at https://github.com/XuZhang1211/VPUFormer.



### LF-PGVIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras using Points and Geodesic Segments
- **Arxiv ID**: http://arxiv.org/abs/2306.06663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.06663v1)
- **Published**: 2023-06-11 12:26:41+00:00
- **Updated**: 2023-06-11 12:26:41+00:00
- **Authors**: Ze Wang, Kailun Yang, Hao Shi, Yufan Zhang, Fei Gao, Kaiwei Wang
- **Comment**: Code will be open-sourced at https://github.com/flysoaryun/LF-PGVIO
- **Journal**: None
- **Summary**: In this paper, we propose LF-PGVIO, a Visual-Inertial-Odometry (VIO) framework for large Field-of-View (FoV) cameras with a negative plane using points and geodesic segments. Notoriously, when the FoV of a panoramic camera reaches the negative half-plane, the image cannot be unfolded into a single pinhole image. Moreover, if a traditional straight-line detection method is directly applied to the original panoramic image, it cannot be normally used due to the large distortions in the panoramas and remains under-explored in the literature. To address these challenges, we put forward LF-PGVIO, which can provide line constraints for cameras with large FoV, even for cameras with negative-plane FoV, and directly extract omnidirectional curve segments from the raw omnidirectional image. We propose an Omnidirectional Curve Segment Detection (OCSD) method combined with a camera model which is applicable to images with large distortions, such as panoramic annular images, fisheye images, and various panoramic images. Each point on the image is projected onto the sphere, and the detected omnidirectional curve segments in the image named geodesic segments must satisfy the criterion of being a geodesic segment on the unit sphere. The detected geodesic segment is sliced into multiple straight-line segments according to the radian of the geodesic, and descriptors are extracted separately and recombined to obtain new descriptors. Based on descriptor matching, we obtain the constraint relationship of the 3D line segments between multiple frames. In our VIO system, we use sliding window optimization using point feature residuals, line feature residuals, and IMU residuals. Our evaluation of the proposed system on public datasets demonstrates that LF-PGVIO outperforms state-of-the-art methods in terms of accuracy and robustness. Code will be open-sourced at https://github.com/flysoaryun/LF-PGVIO.



### TransMRSR: Transformer-based Self-Distilled Generative Prior for Brain MRI Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2306.06669v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06669v1)
- **Published**: 2023-06-11 12:41:23+00:00
- **Updated**: 2023-06-11 12:41:23+00:00
- **Authors**: Shan Huang, Xiaohong Liu, Tao Tan, Menghan Hu, Xiaoer Wei, Tingli Chen, Bin Sheng
- **Comment**: 2023 CGI
- **Journal**: None
- **Summary**: Magnetic resonance images (MRI) acquired with low through-plane resolution compromise time and cost. The poor resolution in one orientation is insufficient to meet the requirement of high resolution for early diagnosis of brain disease and morphometric study. The common Single image super-resolution (SISR) solutions face two main challenges: (1) local detailed and global anatomical structural information combination; and (2) large-scale restoration when applied for reconstructing thick-slice MRI into high-resolution (HR) iso-tropic data. To address these problems, we propose a novel two-stage network for brain MRI SR named TransMRSR based on the convolutional blocks to extract local information and transformer blocks to capture long-range dependencies. TransMRSR consists of three modules: the shallow local feature extraction, the deep non-local feature capture, and the HR image reconstruction. We perform a generative task to encapsulate diverse priors into a generative network (GAN), which is the decoder sub-module of the deep non-local feature capture part, in the first stage. The pre-trained GAN is used for the second stage of SR task. We further eliminate the potential latent space shift caused by the two-stage training strategy through the self-distilled truncation trick. The extensive experiments show that our method achieves superior performance to other SSIR methods on both public and private datasets. Code is released at https://github.com/goddesshs/TransMRSR.git .



### Happy People -- Image Synthesis as Black-Box Optimization Problem in the Discrete Latent Space of Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2306.06684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06684v1)
- **Published**: 2023-06-11 13:58:36+00:00
- **Updated**: 2023-06-11 13:58:36+00:00
- **Authors**: Steffen Jung, Jan Christian Schwedhelm, Claudia Schillings, Margret Keuper
- **Comment**: CVPR 2023 workshop: Generative Models for Computer Vision
- **Journal**: None
- **Summary**: In recent years, optimization in the learned latent space of deep generative models has been successfully applied to black-box optimization problems such as drug design, image generation or neural architecture search. Existing models thereby leverage the ability of neural models to learn the data distribution from a limited amount of samples such that new samples from the distribution can be drawn. In this work, we propose a novel image generative approach that optimizes the generated sample with respect to a continuously quantifiable property. While we anticipate absolutely no practically meaningful application for the proposed framework, it is theoretically principled and allows to quickly propose samples at the mere boundary of the training data distribution. Specifically, we propose to use tree-based ensemble models as mathematical programs over the discrete latent space of vector quantized VAEs, which can be globally solved. Subsequent weighted retraining on these queries allows to induce a distribution shift. In lack of a practically relevant problem, we consider a visually appealing application: the generation of happily smiling faces (where the training distribution only contains less happy people) - and show the principled behavior of our approach in terms of improved FID and higher smile degree over baseline approaches.



### LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2306.06687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06687v2)
- **Published**: 2023-06-11 14:01:17+00:00
- **Updated**: 2023-06-18 13:15:47+00:00
- **Authors**: Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, Wanli Ouyang
- **Comment**: 37 pages, 33 figures. Code available at
  https://github.com/OpenLAMM/LAMM ; Project page: https://openlamm.github.io/
- **Journal**: None
- **Summary**: Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at https://github.com/OpenLAMM/LAMM.



### Self-Enhancement Improves Text-Image Retrieval in Foundation Visual-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2306.06691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06691v1)
- **Published**: 2023-06-11 14:25:38+00:00
- **Updated**: 2023-06-11 14:25:38+00:00
- **Authors**: Yuguang Yang, Yiming Wang, Shupeng Geng, Runqi Wang, Yimi Wang, Sheng Wu, Baochang Zhang
- **Comment**: Accepted by CVPR 2023 Workshop
- **Journal**: None
- **Summary**: The emergence of cross-modal foundation models has introduced numerous approaches grounded in text-image retrieval. However, on some domain-specific retrieval tasks, these models fail to focus on the key attributes required. To address this issue, we propose a self-enhancement framework, A^{3}R, based on the CLIP-ViT/G-14, one of the largest cross-modal models. First, we perform an Attribute Augmentation strategy to enrich the textual description for fine-grained representation before model learning. Then, we propose an Adaption Re-ranking method to unify the representation space of textual query and candidate images and re-rank candidate images relying on the adapted query after model learning. The proposed framework is validated to achieve a salient improvement over the baseline and other teams' solutions in the cross-modal image retrieval track of the 1st foundation model challenge without introducing any additional samples. The code is available at \url{https://github.com/CapricornGuang/A3R}.



### Toward Fair Facial Expression Recognition with Improved Distribution Alignment
- **Arxiv ID**: http://arxiv.org/abs/2306.06696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06696v1)
- **Published**: 2023-06-11 14:59:20+00:00
- **Updated**: 2023-06-11 14:59:20+00:00
- **Authors**: Mojtaba Kolahdouzi, Ali Etemad
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to mitigate bias in facial expression recognition (FER) models. Our method aims to reduce sensitive attribute information such as gender, age, or race, in the embeddings produced by FER models. We employ a kernel mean shrinkage estimator to estimate the kernel mean of the distributions of the embeddings associated with different sensitive attribute groups, such as young and old, in the Hilbert space. Using this estimation, we calculate the maximum mean discrepancy (MMD) distance between the distributions and incorporate it in the classifier loss along with an adversarial loss, which is then minimized through the learning process to improve the distribution alignment. Our method makes sensitive attributes less recognizable for the model, which in turn promotes fairness. Additionally, for the first time, we analyze the notion of attractiveness as an important sensitive attribute in FER models and demonstrate that FER models can indeed exhibit biases towards more attractive faces. To prove the efficacy of our model in reducing bias regarding different sensitive attributes (including the newly proposed attractiveness attribute), we perform several experiments on two widely used datasets, CelebA and RAF-DB. The results in terms of both accuracy and fairness measures outperform the state-of-the-art in most cases, demonstrating the effectiveness of the proposed method.



### Neural Architecture Design and Robustness: A Dataset
- **Arxiv ID**: http://arxiv.org/abs/2306.06712v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.06712v1)
- **Published**: 2023-06-11 16:02:14+00:00
- **Updated**: 2023-06-11 16:02:14+00:00
- **Authors**: Steffen Jung, Jovita Lukasik, Margret Keuper
- **Comment**: ICLR 2023; project page: http://robustness.vision/
- **Journal**: None
- **Summary**: Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves a tedious trial-and-error process with one additional challenge: the evaluation of a network's robustness is significantly more expensive than its evaluation for clean accuracy. Thus, the aim of this paper is to facilitate better streamlined research on architectural design choices with respect to their impact on robustness as well as, for example, the evaluation of surrogate measures for robustness. We therefore borrow one of the most commonly considered search spaces for neural architecture search for image classification, NAS-Bench-201, which contains a manageable size of 6466 non-isomorphic network designs. We evaluate all these networks on a range of common adversarial attacks and corruption types and introduce a database on neural architecture design and robustness evaluations. We further present three exemplary use cases of this dataset, in which we (i) benchmark robustness measurements based on Jacobian and Hessian matrices for their robustness predictability, (ii) perform neural architecture search on robust accuracies, and (iii) provide an initial analysis of how architectural design choices affect robustness. We find that carefully crafting the topology of a network can have substantial impact on its robustness, where networks with the same parameter count range in mean adversarial robust accuracy from 20%-41%. Code and data is available at http://robustness.vision/.



### PWR-Align: Leveraging Part-Whole Relationships for Part-wise Rigid Point Cloud Registration in Mixed Reality Applications
- **Arxiv ID**: http://arxiv.org/abs/2306.06717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2306.06717v1)
- **Published**: 2023-06-11 16:36:31+00:00
- **Updated**: 2023-06-11 16:36:31+00:00
- **Authors**: Manorama Jha, Bhaskar Banerjee
- **Comment**: Accepted for presentation at WiCV @ CVPR 2023
- **Journal**: None
- **Summary**: We present an efficient and robust point cloud registration (PCR) workflow for part-wise rigid point cloud alignment using the Microsoft HoloLens 2. Point Cloud Registration (PCR) is an important problem in Augmented and Mixed Reality use cases, and we present a study for a special class of non-rigid transformations. Many commonly encountered objects are composed of rigid parts that move relative to one another about joints resulting in non-rigid deformation of the whole object such as robots with manipulators, and machines with hinges. The workflow presented allows us to register the point cloud with various configurations of the point cloud.



### $E(2)$-Equivariant Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2306.06722v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.06722v3)
- **Published**: 2023-06-11 16:48:03+00:00
- **Updated**: 2023-07-07 06:59:26+00:00
- **Authors**: Renjun Xu, Kaifan Yang, Ke Liu, Fengxiang He
- **Comment**: Accept to UAI2023
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has achieved remarkable performance in computer vision. However, positional encoding in ViT makes it substantially difficult to learn the intrinsic equivariance in data. Initial attempts have been made on designing equivariant ViT but are proved defective in some cases in this paper. To address this issue, we design a Group Equivariant Vision Transformer (GE-ViT) via a novel, effective positional encoding operator. We prove that GE-ViT meets all the theoretical requirements of an equivariant neural network. Comprehensive experiments are conducted on standard benchmark datasets, demonstrating that GE-ViT significantly outperforms non-equivariant self-attention networks. The code is available at https://github.com/ZJUCDSYangKaifan/GEVit.



### Precise and Generalized Robustness Certification for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2306.06747v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06747v1)
- **Published**: 2023-06-11 19:00:41+00:00
- **Updated**: 2023-06-11 19:00:41+00:00
- **Authors**: Yuanyuan Yuan, Shuai Wang, Zhendong Su
- **Comment**: The extended version of a paper to appear in the Proceedings of the
  32nd USENIX Security Symposium, 2023, (USENIX Security '23), 19 pages
- **Journal**: None
- **Summary**: The objective of neural network (NN) robustness certification is to determine if a NN changes its predictions when mutations are made to its inputs. While most certification research studies pixel-level or a few geometrical-level and blurring operations over images, this paper proposes a novel framework, GCERT, which certifies NN robustness under a precise and unified form of diverse semantic-level image mutations. We formulate a comprehensive set of semantic-level image mutations uniformly as certain directions in the latent space of generative models. We identify two key properties, independence and continuity, that convert the latent space into a precise and analysis-friendly input space representation for certification. GCERT can be smoothly integrated with de facto complete, incomplete, or quantitative certification frameworks. With its precise input space representation, GCERT enables for the first time complete NN robustness certification with moderate cost under diverse semantic-level input mutations, such as weather-filter, style transfer, and perceptual changes (e.g., opening/closing eyes). We show that GCERT enables certifying NN robustness under various common and security-sensitive scenarios like autonomous driving.



### 3rd Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.06753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.06753v1)
- **Published**: 2023-06-11 19:44:40+00:00
- **Updated**: 2023-06-11 19:44:40+00:00
- **Authors**: Jinming Su, Wangwang Yang, Junfeng Luo, Xiaolin Wei
- **Comment**: 3rd Place Solution for PVUW Challenge 2023: Video Panoptic
  Segmentation
- **Journal**: None
- **Summary**: In order to deal with the task of video panoptic segmentation in the wild, we propose a robust integrated video panoptic segmentation solution. In our solution, we regard the video panoptic segmentation task as a segmentation target querying task, represent both semantic and instance targets as a set of queries, and then combine these queries with video features extracted by neural networks to predict segmentation masks. In order to improve the learning accuracy and convergence speed of the solution, we add additional tasks of video semantic segmentation and video instance segmentation for joint training. In addition, we also add an additional image semantic segmentation model to further improve the performance of semantic classes. In addition, we also add some additional operations to improve the robustness of the model. Extensive experiments on the VIPSeg dataset show that the proposed solution achieves state-of-the-art performance with 50.04\% VPQ on the VIPSeg test set, which is 3rd place on the video panoptic segmentation track of the PVUW Challenge 2023.



### Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework
- **Arxiv ID**: http://arxiv.org/abs/2306.07992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.07992v1)
- **Published**: 2023-06-11 19:59:35+00:00
- **Updated**: 2023-06-11 19:59:35+00:00
- **Authors**: Minglei Yin, Bin Liu, Neil Zhenqiang Gong, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision transformers; and (2) accurately detect adversarial examples using a novel contrastive learning approach. Meanwhile, our framework is designed to be used as both a filter and a detector so that they can be jointly trained to improve the flexibility of our defense strategy to a variety of attacks and VARS models. We have conducted extensive experimental studies with two popular attack methods (FGSM and PGD). Our experimental results on two real-world datasets show that our defense strategy against visual attacks is effective and outperforms existing methods on different attacks. Moreover, our method can detect adversarial examples with high accuracy.



### The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases
- **Arxiv ID**: http://arxiv.org/abs/2306.06767v2
- **DOI**: 10.1016/j.metrad.2023.100007
- **Categories**: **eess.IV**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06767v2)
- **Published**: 2023-06-11 20:39:13+00:00
- **Updated**: 2023-07-06 14:54:22+00:00
- **Authors**: Jiancheng Yang, Hongwei Bran Li, Donglai Wei
- **Comment**: Paper invited for the first issue of Meta-Radiology
- **Journal**: None
- **Summary**: This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.



### Multimodal Pathology Image Search Between H&E Slides and Multiplexed Immunofluorescent Images
- **Arxiv ID**: http://arxiv.org/abs/2306.06780v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2306.06780v1)
- **Published**: 2023-06-11 21:30:20+00:00
- **Updated**: 2023-06-11 21:30:20+00:00
- **Authors**: Amir Hajighasemi, MD Jillur Rahman Saurav, Mohammad S Nasr, Jai Prakash Veerla, Aarti Darji, Parisa Boodaghi Malidarreh, Michael Robben, Helen H Shang, Jacob M Luber
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for multimodal pathology image search, using dynamic time warping (DTW) on Variational Autoencoder (VAE) latent space that is fed into a ranked choice voting scheme to retrieve multiplexed immunofluorescent imaging (mIF) that is most similar to a query H&E slide. Through training the VAE and applying DTW, we align and compare mIF and H&E slides. Our method improves differential diagnosis and therapeutic decisions by integrating morphological H&E data with immunophenotyping from mIF, providing clinicians a rich perspective of disease states. This facilitates an understanding of the spatial relationships in tissue samples and could revolutionize the diagnostic process, enhancing precision and enabling personalized therapy selection. Our technique demonstrates feasibility using colorectal cancer and healthy tonsil samples. An exhaustive ablation study was conducted on a search engine designed to explore the correlation between multiplexed Immunofluorescence (mIF) and Hematoxylin and Eosin (H&E) staining, in order to validate its ability to map these distinct modalities into a unified vector space. Despite extreme class imbalance, the system demonstrated robustness and utility by returning similar results across various data features, which suggests potential for future use in multimodal histopathology data analysis.



### VBSF-TLD: Validation-Based Approach for Soft Computing-Inspired Transfer Learning in Drone Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.06797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.06797v1)
- **Published**: 2023-06-11 22:30:23+00:00
- **Updated**: 2023-06-11 22:30:23+00:00
- **Authors**: Jaskaran Singh
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing utilization of Internet of Things (IoT) enabled drones in diverse applications like photography, delivery, and surveillance, concerns regarding privacy and security have become more prominent. Drones have the ability to capture sensitive information, compromise privacy, and pose security risks. As a result, the demand for advanced technology to automate drone detection has become crucial. This paper presents a project on a transfer-based drone detection scheme, which forms an integral part of a computer vision-based module and leverages transfer learning to enhance performance. By harnessing the knowledge of pre-trained models from a related domain, transfer learning enables improved results even with limited training data. To evaluate the scheme's performance, we conducted tests on benchmark datasets, including the Drone-vs-Bird Dataset and the UAVDT dataset. Notably, the scheme's effectiveness is highlighted by its IOU-based validation results, demonstrating the potential of deep learning-based technology in automating drone detection in critical areas such as airports, military bases, and other high-security zones.



### Stable Remaster: Bridging the Gap Between Old Content and New Displays
- **Arxiv ID**: http://arxiv.org/abs/2306.06803v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.06803v1)
- **Published**: 2023-06-11 23:11:29+00:00
- **Updated**: 2023-06-11 23:11:29+00:00
- **Authors**: Nathan Paull, Shuvam Keshari, Yian Wong
- **Comment**: None
- **Journal**: None
- **Summary**: The invention of modern displays has enhanced the viewer experience for any kind of content: ranging from sports to movies in 8K high-definition resolution. However, older content developed for CRT or early Plasma screen TVs has become outdated quickly and no longer meets current aspect ratio and resolution standards. In this paper, we explore whether we can solve this problem with the use of diffusion models to adapt old content to meet contemporary expectations. We explore the ability to combine multiple independent computer vision tasks to attempt to solve the problem of expanding aspect ratios of old animated content such that the new content would be indistinguishable from the source material to a brand-new viewer. These existing capabilities include Stable Diffusion, Content-Aware Scene Detection, Object Detection, and Key Point Matching. We were able to successfully chain these tasks together in a way that generated reasonable outputs, however, future work needs to be done to improve and expand the application to non-animated content as well.



### Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization
- **Arxiv ID**: http://arxiv.org/abs/2306.06805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.06805v1)
- **Published**: 2023-06-11 23:33:59+00:00
- **Updated**: 2023-06-11 23:33:59+00:00
- **Authors**: Thomas Fel, Thibaut Boissin, Victor Boutin, Agustin Picard, Paul Novello, Julien Colin, Drew Linsley, Tom Rousseau, Rémi Cadène, Laurent Gardes, Thomas Serre
- **Comment**: None
- **Journal**: None
- **Summary**: Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spectrum while keeping the magnitude constant to ensure that generated explanations lie in the space of natural images. Our approach yields significantly better results (both qualitatively and quantitatively) and unlocks efficient and interpretable feature visualizations for large state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing us to augment feature visualizations with spatial importance. We validate our method on a novel benchmark for comparing feature visualization methods, and release its visualizations for all classes of the ImageNet dataset on https://serre-lab.github.io/Lens/.   Overall, our approach unlocks, for the first time, feature visualizations for large, state-of-the-art deep neural networks without resorting to any parametric prior image model.



