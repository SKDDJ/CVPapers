# Arxiv Papers in cs.CV on 2023-06-03
### Temporal-spatial Correlation Attention Network for Clinical Data Analysis in Intensive Care Unit
- **Arxiv ID**: http://arxiv.org/abs/2306.01970v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2306.01970v1)
- **Published**: 2023-06-03 00:38:40+00:00
- **Updated**: 2023-06-03 00:38:40+00:00
- **Authors**: Weizhi Nie, Yuhe Yu, Chen Zhang, Dan Song, Lina Zhao, Yunpeng Bai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, medical information technology has made it possible for electronic health record (EHR) to store fairly complete clinical data. This has brought health care into the era of "big data". However, medical data are often sparse and strongly correlated, which means that medical problems cannot be solved effectively. With the rapid development of deep learning in recent years, it has provided opportunities for the use of big data in healthcare. In this paper, we propose a temporal-saptial correlation attention network (TSCAN) to handle some clinical characteristic prediction problems, such as predicting death, predicting length of stay, detecting physiologic decline, and classifying phenotypes. Based on the design of the attention mechanism model, our approach can effectively remove irrelevant items in clinical data and irrelevant nodes in time according to different tasks, so as to obtain more accurate prediction results. Our method can also find key clinical indicators of important outcomes that can be used to improve treatment options. Our experiments use information from the Medical Information Mart for Intensive Care (MIMIC-IV) database, which is open to the public. Finally, we have achieved significant performance benefits of 2.0\% (metric) compared to other SOTA prediction methods. We achieved a staggering 90.7\% on mortality rate, 45.1\% on length of stay. The source code can be find: \url{https://github.com/yuyuheintju/TSCAN}.



### Mitigating Backdoor Attack Via Prerequisite Transformation
- **Arxiv ID**: http://arxiv.org/abs/2306.01983v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.01983v1)
- **Published**: 2023-06-03 02:33:38+00:00
- **Updated**: 2023-06-03 02:33:38+00:00
- **Authors**: Han Gao
- **Comment**: 7 pages,7 figures,2 tables
- **Journal**: None
- **Summary**: In recent years, with the successful application of DNN in fields such as NLP and CV, its security has also received widespread attention. (Author) proposed the method of backdoor attack in Badnet. Switch implanted backdoor into the model by poisoning the training samples. The model with backdoor did not exhibit any abnormalities on the normal validation sample set, but in the input with trigger, they were mistakenly classified as the attacker's designated category or randomly classified as a different category from the ground truth, This attack method seriously threatens the normal application of DNN in real life, such as autonomous driving, object detection, etc.This article proposes a new method to combat backdoor attacks. We refer to the features in the area covered by the trigger as trigger features, and the remaining areas as normal features. By introducing prerequisite calculation conditions during the training process, these conditions have little impact on normal features and trigger features, and can complete the training of a standard backdoor model. The model trained under these prerequisite calculation conditions can, In the verification set D'val with the same premise calculation conditions, the performance is consistent with that of the ordinary backdoor model. However, in the verification set Dval without the premise calculation conditions, the verification accuracy decreases very little (7%~12%), while the attack success rate (ASR) decreases from 90% to about 8%.Author call this method Prerequisite Transformation(PT).



### Lightweight Structure-aware Transformer Network for VHR Remote Sensing Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.01988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.01988v1)
- **Published**: 2023-06-03 03:21:18+00:00
- **Updated**: 2023-06-03 03:21:18+00:00
- **Authors**: Tao Lei, Yetong Xu, Hailong Ning, Zhiyong Lv, Chongdan Min, Yaochu Jin, Asoke K. Nandi
- **Comment**: None
- **Journal**: None
- **Summary**: Popular Transformer networks have been successfully applied to remote sensing (RS) image change detection (CD) identifications and achieve better results than most convolutional neural networks (CNNs), but they still suffer from two main problems. First, the computational complexity of the Transformer grows quadratically with the increase of image spatial resolution, which is unfavorable to very high-resolution (VHR) RS images. Second, these popular Transformer networks tend to ignore the importance of fine-grained features, which results in poor edge integrity and internal tightness for largely changed objects and leads to the loss of small changed objects. To address the above issues, this Letter proposes a Lightweight Structure-aware Transformer (LSAT) network for RS image CD. The proposed LSAT has two advantages. First, a Cross-dimension Interactive Self-attention (CISA) module with linear complexity is designed to replace the vanilla self-attention in visual Transformer, which effectively reduces the computational complexity while improving the feature representation ability of the proposed LSAT. Second, a Structure-aware Enhancement Module (SAEM) is designed to enhance difference features and edge detail information, which can achieve double enhancement by difference refinement and detail aggregation so as to obtain fine-grained features of bi-temporal RS images. Experimental results show that the proposed LSAT achieves significant improvement in detection accuracy and offers a better tradeoff between accuracy and computational costs than most state-of-the-art CD methods for VHR RS images.



### Context-TAP: Tracking Any Point Demands Spatial Context Features
- **Arxiv ID**: http://arxiv.org/abs/2306.02000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02000v1)
- **Published**: 2023-06-03 04:47:05+00:00
- **Updated**: 2023-06-03 04:47:05+00:00
- **Authors**: Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, Hongsheng Li
- **Comment**: Project Page: this
  $\href{https://wkbian.github.io/Projects/Context-TAP/}{webpage}$
- **Journal**: None
- **Summary**: We tackle the problem of Tracking Any Point (TAP) in videos, which specifically aims at estimating persistent long-term trajectories of query points in videos. Previous methods attempted to estimate these trajectories independently to incorporate longer image sequences, therefore, ignoring the potential benefits of incorporating spatial context features. We argue that independent video point tracking also demands spatial context features. To this end, we propose a novel framework Context-TAP, which effectively improves point trajectory accuracy by aggregating spatial context features in videos. Context-TAP contains two main modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature Aggregation (TAFA) module. Context-TAP significantly improves PIPs all-sided, reducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD and increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on TAP-Vid-Kinectics. Demos are available at this $\href{https://wkbian.github.io/Projects/Context-TAP/}{webpage}$.



### Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2306.02014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.02014v1)
- **Published**: 2023-06-03 06:10:20+00:00
- **Updated**: 2023-06-03 06:10:20+00:00
- **Authors**: Pritam Sarkar, Ahmad Beirami, Ali Etemad
- **Comment**: 57 pages
- **Journal**: None
- **Summary**: Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MOCO, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of $17$ in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally struggle with context shifts, v-MAE and supervised learning exhibit more robustness. Moreover, our study shows that v-MAE is a strong temporal learner, whereas contrastive methods, v-SimCLR and v-MOCO, exhibit strong performances against viewpoint shifts. When studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set recognition performance, particularly if the pretrained VSSL encoders are used without finetuning. We hope that our work will contribute to the development of robust video representation learning frameworks for various real-world scenarios.



### VideoComposer: Compositional Video Synthesis with Motion Controllability
- **Arxiv ID**: http://arxiv.org/abs/2306.02018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02018v2)
- **Published**: 2023-06-03 06:29:02+00:00
- **Updated**: 2023-06-06 03:54:10+00:00
- **Authors**: Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, Jingren Zhou
- **Comment**: The first four authors contributed equally. Project page:
  https://videocomposer.github.io
- **Journal**: None
- **Summary**: The pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis. However, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency. Based on the paradigm of compositional generation, this work presents VideoComposer that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions. Specifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics. In addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. Extensive experimental results suggest that VideoComposer is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions. The code and models will be publicly available at https://videocomposer.github.io.



### Towards Black-box Adversarial Example Detection: A Data Reconstruction-based Method
- **Arxiv ID**: http://arxiv.org/abs/2306.02021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02021v1)
- **Published**: 2023-06-03 06:34:17+00:00
- **Updated**: 2023-06-03 06:34:17+00:00
- **Authors**: Yifei Gao, Zhiyu Lin, Yunfan Yang, Jitao Sang
- **Comment**: 14 pages, 8 figures, 13 tables
- **Journal**: None
- **Summary**: Adversarial example detection is known to be an effective adversarial defense method. Black-box attack, which is a more realistic threat and has led to various black-box adversarial training-based defense methods, however, does not attract considerable attention in adversarial example detection. In this paper, we fill this gap by positioning the problem of black-box adversarial example detection (BAD). Data analysis under the introduced BAD settings demonstrates (1) the incapability of existing detectors in addressing the black-box scenario and (2) the potential of exploring BAD solutions from a data perspective. To tackle the BAD problem, we propose a data reconstruction-based adversarial example detection method. Specifically, we use variational auto-encoder (VAE) to capture both pixel and frequency representations of normal examples. Then we use reconstruction error to detect adversarial examples. Compared with existing detection methods, the proposed method achieves substantially better detection performance in BAD, which helps promote the deployment of adversarial example detection-based defense solutions in real-world models.



### Efficient Multi-Grained Knowledge Reuse for Class Incremental Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.02027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02027v1)
- **Published**: 2023-06-03 07:03:15+00:00
- **Updated**: 2023-06-03 07:03:15+00:00
- **Authors**: Zhihe Lu, Shuicheng Yan, Xinchao Wang
- **Comment**: Technical Report. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Class Incremental Semantic Segmentation (CISS) has been a trend recently due to its great significance in real-world applications. Although the existing CISS methods demonstrate remarkable performance, they either leverage the high-level knowledge (feature) only while neglecting the rich and diverse knowledge in the low-level features, leading to poor old knowledge preservation and weak new knowledge exploration; or use multi-level features for knowledge distillation by retraining a heavy backbone, which is computationally intensive. In this paper, we for the first time propose to efficiently reuse the multi-grained knowledge for CISS by fusing multi-level features with the frozen backbone and show a simple aggregation of varying-level features, i.e., naive feature pyramid, can boost the performance significantly. We further introduce a novel densely-interactive feature pyramid (DEFY) module that enhances the fusion of high- and low-level features by enabling their dense interaction. Specifically, DEFY establishes a per-pixel relationship between pairs of feature maps, allowing for multi-pair outputs to be aggregated. This results in improved semantic segmentation by leveraging the complementary information from multi-level features. We show that DEFY can be effortlessly integrated into three representative methods for performance enhancement. Our method yields a new state-of-the-art performance when combined with the current SOTA by notably averaged mIoU gains on two widely used benchmarks, i.e., 2.5% on PASCAL VOC 2012 and 2.3% on ADE20K.



### Provable Dynamic Fusion for Low-Quality Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/2306.02050v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.02050v2)
- **Published**: 2023-06-03 08:32:35+00:00
- **Updated**: 2023-06-06 13:46:22+00:00
- **Authors**: Qingyang Zhang, Haitao Wu, Changqing Zhang, Qinghua Hu, Huazhu Fu, Joey Tianyi Zhou, Xi Peng
- **Comment**: Accepted by ICML 2023
- **Journal**: None
- **Summary**: The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can support our findings.



### Balancing Logit Variation for Long-tailed Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.02061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02061v1)
- **Published**: 2023-06-03 09:19:24+00:00
- **Updated**: 2023-06-03 09:19:24+00:00
- **Authors**: Yuchao Wang, Jingjing Fei, Haochen Wang, Wei Li, Tianpeng Bao, Liwei Wu, Rui Zhao, Yujun Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation usually suffers from a long-tail data distribution. Due to the imbalanced number of samples across categories, the features of those tail classes may get squeezed into a narrow area in the feature space. Towards a balanced feature distribution, we introduce category-wise variation into the network predictions in the training phase such that an instance is no longer projected to a feature point, but a small region instead. Such a perturbation is highly dependent on the category scale, which appears as assigning smaller variation to head classes and larger variation to tail classes. In this way, we manage to close the gap between the feature areas of different categories, resulting in a more balanced representation. It is noteworthy that the introduced variation is discarded at the inference stage to facilitate a confident prediction. Although with an embarrassingly simple implementation, our method manifests itself in strong generalizability to various datasets and task settings. Extensive experiments suggest that our plug-in design lends itself well to a range of state-of-the-art approaches and boosts the performance on top of them.



### Flew Over Learning Trap: Learn Unlearnable Samples by Progressive Staged Training
- **Arxiv ID**: http://arxiv.org/abs/2306.02064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02064v1)
- **Published**: 2023-06-03 09:36:16+00:00
- **Updated**: 2023-06-03 09:36:16+00:00
- **Authors**: Pucheng Dang, Xing Hu, Kaidi Xu, Jinhao Duan, Di Huang, Husheng Han, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Unlearning techniques are proposed to prevent third parties from exploiting unauthorized data, which generate unlearnable samples by adding imperceptible perturbations to data for public publishing. These unlearnable samples effectively misguide model training to learn perturbation features but ignore image semantic features. We make the in-depth analysis and observe that models can learn both image features and perturbation features of unlearnable samples at an early stage, but rapidly go to the overfitting stage since the shallow layers tend to overfit on perturbation features and make models fall into overfitting quickly. Based on the observations, we propose Progressive Staged Training to effectively prevent models from overfitting in learning perturbation features. We evaluated our method on multiple model architectures over diverse datasets, e.g., CIFAR-10, CIFAR-100, and ImageNet-mini. Our method circumvents the unlearnability of all state-of-the-art methods in the literature and provides a reliable baseline for further evaluation of unlearnable techniques.



### A Conditional Generative Chatbot using Transformer Model
- **Arxiv ID**: http://arxiv.org/abs/2306.02074v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.02074v1)
- **Published**: 2023-06-03 10:35:04+00:00
- **Updated**: 2023-06-03 10:35:04+00:00
- **Authors**: Nura Esfandiari, Kourosh Kiani, Razieh Rastgoo
- **Comment**: None
- **Journal**: None
- **Summary**: A Chatbot serves as a communication tool between a human user and a machine to achieve an appropriate answer based on the human input. In more recent approaches, a combination of Natural Language Processing and sequential models are used to build a generative Chatbot. The main challenge of these models is their sequential nature, which leads to less accurate results. To tackle this challenge, in this paper, a novel end-to-end architecture is proposed using conditional Wasserstein Generative Adversarial Networks and a transformer model for answer generation in Chatbots. While the generator of the proposed model consists of a full transformer model to generate an answer, the discriminator includes only the encoder part of a transformer model followed by a classifier. To the best of our knowledge, this is the first time that a generative Chatbot is proposed using the embedded transformer in both generator and discriminator models. Relying on the parallel computing of the transformer model, the results of the proposed model on the Cornell Movie-Dialog corpus and the Chit-Chat datasets confirm the superiority of the proposed model compared to state-of-the-art alternatives using different evaluation metrics.



### Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2306.02080v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.02080v2)
- **Published**: 2023-06-03 11:05:04+00:00
- **Updated**: 2023-06-11 13:00:15+00:00
- **Authors**: Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, Volker Tresp
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead it results in even lower robustness. We hope this study could benefit future research in the development of robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io .



### Unsupervised Low Light Image Enhancement Using SNR-Aware Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2306.02082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02082v2)
- **Published**: 2023-06-03 11:07:56+00:00
- **Updated**: 2023-07-27 13:59:10+00:00
- **Authors**: Zhijian Luo, Jiahui Tang, Yueen Hou, Zihan Huang, Yanzeng Gao
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Image captured under low-light conditions presents unpleasing artifacts, which debilitate the performance of feature extraction for many upstream visual tasks. Low-light image enhancement aims at improving brightness and contrast, and further reducing noise that corrupts the visual quality. Recently, many image restoration methods based on Swin Transformer have been proposed and achieve impressive performance. However, on one hand, trivially employing Swin Transformer for low-light image enhancement would expose some artifacts, including over-exposure, brightness imbalance and noise corruption, etc. On the other hand, it is impractical to capture image pairs of low-light images and corresponding ground-truth, i.e. well-exposed image in same visual scene. In this paper, we propose a dual-branch network based on Swin Transformer, guided by a signal-to-noise ratio prior map which provides the spatial-varying information for low-light image enhancement. Moreover, we leverage unsupervised learning to construct the optimization objective based on Retinex model, to guide the training of proposed network. Experimental results demonstrate that the proposed model is competitive with the baseline models.



### Efficient Text-Guided 3D-Aware Portrait Generation with Score Distillation Sampling on Distribution
- **Arxiv ID**: http://arxiv.org/abs/2306.02083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02083v1)
- **Published**: 2023-06-03 11:08:38+00:00
- **Updated**: 2023-06-03 11:08:38+00:00
- **Authors**: Yiji Cheng, Fei Yin, Xiaoke Huang, Xintong Yu, Jiaxiang Liu, Shikun Feng, Yujiu Yang, Yansong Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-3D is an emerging task that allows users to create 3D content with infinite possibilities. Existing works tackle the problem by optimizing a 3D representation with guidance from pre-trained diffusion models. An apparent drawback is that they need to optimize from scratch for each prompt, which is computationally expensive and often yields poor visual fidelity. In this paper, we propose DreamPortrait, which aims to generate text-guided 3D-aware portraits in a single-forward pass for efficiency. To achieve this, we extend Score Distillation Sampling from datapoint to distribution formulation, which injects semantic prior into a 3D distribution. However, the direct extension will lead to the mode collapse problem since the objective only pursues semantic alignment. Hence, we propose to optimize a distribution with hierarchical condition adapters and GAN loss regularization. For better 3D modeling, we further design a 3D-aware gated cross-attention mechanism to explicitly let the model perceive the correspondence between the text and the 3D-aware space. These elaborated designs enable our model to generate portraits with robust multi-view semantic consistency, eliminating the need for optimization-based methods. Extensive experiments demonstrate our model's highly competitive performance and significant speed boost against existing methods.



### Relieving Triplet Ambiguity: Consensus Network for Language-Guided Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2306.02092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02092v1)
- **Published**: 2023-06-03 11:50:44+00:00
- **Updated**: 2023-06-03 11:50:44+00:00
- **Authors**: Xu Zhang, Zhedong Zheng, Xiaohan Wang, Yi Yang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Language-guided image retrieval enables users to search for images and interact with the retrieval system more naturally and expressively by using a reference image and a relative caption as a query. Most existing studies mainly focus on designing image-text composition architecture to extract discriminative visual-linguistic relations. Despite great success, we identify an inherent problem that obstructs the extraction of discriminative features and considerably compromises model training: \textbf{triplet ambiguity}. This problem stems from the annotation process wherein annotators view only one triplet at a time. As a result, they often describe simple attributes, such as color, while neglecting fine-grained details like location and style. This leads to multiple false-negative candidates matching the same modification text. We propose a novel Consensus Network (Css-Net) that self-adaptively learns from noisy triplets to minimize the negative effects of triplet ambiguity. Inspired by the psychological finding that groups perform better than individuals, Css-Net comprises 1) a consensus module featuring four distinct compositors that generate diverse fused image-text embeddings and 2) a Kullback-Leibler divergence loss, which fosters learning among the compositors, enabling them to reduce biases learned from noisy triplets and reach a consensus. The decisions from four compositors are weighted during evaluation to further achieve consensus. Comprehensive experiments on three datasets demonstrate that Css-Net can alleviate triplet ambiguity, achieving competitive performance on benchmarks, such as $+2.77\%$ R@10 and $+6.67\%$ R@50 on FashionIQ.



### Segment Anything Meets Semantic Communication
- **Arxiv ID**: http://arxiv.org/abs/2306.02094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02094v1)
- **Published**: 2023-06-03 11:54:56+00:00
- **Updated**: 2023-06-03 11:54:56+00:00
- **Authors**: Shehbaz Tariq, Brian Estadimas Arfeto, Chaoning Zhang, Hyundong Shin
- **Comment**: Submitted to MILCOM 23
- **Journal**: None
- **Summary**: In light of the diminishing returns of traditional methods for enhancing transmission rates, the domain of semantic communication presents promising new frontiers. Focusing on image transmission, this paper explores the application of foundation models, particularly the Segment Anything Model (SAM) developed by Meta AI Research, to improve semantic communication. SAM is a promptable image segmentation model that has gained attention for its ability to perform zero-shot segmentation tasks without explicit training or domain-specific knowledge. By employing SAM's segmentation capability and lightweight neural network architecture for semantic coding, we propose a practical approach to semantic communication. We demonstrate that this approach retains critical semantic features, achieving higher image reconstruction quality and reducing communication overhead. This practical solution eliminates the resource-intensive stage of training a segmentation model and can be applied to any semantic coding architecture, paving the way for real-world applications.



### Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2306.02095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02095v1)
- **Published**: 2023-06-03 12:05:07+00:00
- **Updated**: 2023-06-03 12:05:07+00:00
- **Authors**: Chenyang Lu, Daan de Geus, Gijs Dubbelman
- **Comment**: CVPR 2023. Project page and code: https://tue-mps.github.io/CTS/
- **Journal**: None
- **Summary**: This paper introduces Content-aware Token Sharing (CTS), a token reduction approach that improves the computational efficiency of semantic segmentation networks that use Vision Transformers (ViTs). Existing works have proposed token reduction approaches to improve the efficiency of ViT-based image classification networks, but these methods are not directly applicable to semantic segmentation, which we address in this work. We observe that, for semantic segmentation, multiple image patches can share a token if they contain the same semantic class, as they contain redundant information. Our approach leverages this by employing an efficient, class-agnostic policy network that predicts if image patches contain the same semantic class, and lets them share a token if they do. With experiments, we explore the critical design choices of CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes datasets, various ViT backbones, and different segmentation decoders. With Content-aware Token Sharing, we are able to reduce the number of processed tokens by up to 44%, without diminishing the segmentation quality.



### Towards Complex Real-World Safety Factory Inspection: A High-Quality Dataset for Safety Clothing and Helmet Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.02098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02098v1)
- **Published**: 2023-06-03 12:15:20+00:00
- **Updated**: 2023-06-03 12:15:20+00:00
- **Authors**: Fusheng Yu, Xiaoping Wang, Jiang Li, Shaojin Wu, Junjie Zhang, Zhigang Zeng
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Safety clothing and helmets play a crucial role in ensuring worker safety at construction sites. Recently, deep learning methods have garnered significant attention in the field of computer vision for their potential to enhance safety and efficiency in various industries. However, limited availability of high-quality datasets has hindered the development of deep learning methods for safety clothing and helmet detection. In this work, we present a large, comprehensive, and realistic high-quality dataset for safety clothing and helmet detection, which was collected from a real-world chemical plant and annotated by professional security inspectors. Our dataset has been compared with several existing open-source datasets, and its effectiveness has been verified applying some classic object detection methods. The results demonstrate that our dataset is more complete and performs better in real-world settings. Furthermore, we have released our deployment code to the public to encourage the adoption of our dataset and improve worker safety. We hope that our efforts will promote the convergence of academic research and industry, ultimately contribute to the betterment of society.



### An Improved Model for Diabetic Retinopathy Detection by using Transfer Learning and Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2308.05178v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2308.05178v1)
- **Published**: 2023-06-03 12:19:21+00:00
- **Updated**: 2023-06-03 12:19:21+00:00
- **Authors**: Md. Simul Hasan Talukder, Ajay Kirshno Sarkar, Sharmin Akter, Md. Nuhi-Alamin
- **Comment**: 22 pages, 7 Tables and 7 Figures
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is an ocular condition caused by a sustained high level of sugar in the blood, which causes the retinal capillaries to block and bleed, causing retinal tissue damage. It usually results in blindness. Early detection can help in lowering the risk of DR and its severity. The robust and accurate prediction and detection of diabetic retinopathy is a challenging task. This paper develops a machine learning model for detecting Diabetic Retinopathy that is entirely accurate. Pre-trained models such as ResNet50, InceptionV3, Xception, DenseNet121, VGG19, NASNetMobile, MobileNetV2, DensNet169, and DenseNet201 with pooling layer, dense layer, and appropriate dropout layer at the bottom of them were carried out in transfer learning (TL) approach. Data augmentation and regularization was performed to reduce overfitting. Transfer Learning model of DenseNet121, Average and weighted ensemble of DenseNet169 and DenseNet201 TL architectures contribute individually the highest accuracy of 100%, the highest precision, recall, F-1 score of 100%, 100%, and 100%, respectively.



### Weight-Aware Implicit Geometry Reconstruction with Curvature-Guided Sampling
- **Arxiv ID**: http://arxiv.org/abs/2306.02099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02099v1)
- **Published**: 2023-06-03 12:23:17+00:00
- **Updated**: 2023-06-03 12:23:17+00:00
- **Authors**: Lu Sang, Abhishek Saroha, Maolin Gao, Daniel Cremers
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Neural surface implicit representations offer numerous advantages, including the ability to easily modify topology and surface resolution. However, reconstructing implicit geometry representation with only limited known data is challenging. In this paper, we present an approach that effectively interpolates and extrapolates within training points, generating additional training data to reconstruct a surface with superior qualitative and quantitative results. We also introduce a technique that efficiently calculates differentiable geometric properties, i.e., mean and Gaussian curvatures, to enhance the sampling process during training. Additionally, we propose a weight-aware implicit neural representation that not only streamlines surface extraction but also extend to non-closed surfaces by depicting non-closed areas as locally degenerated patches, thereby mitigating the drawbacks of the previous assumption in implicit neural representations.



### Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models
- **Arxiv ID**: http://arxiv.org/abs/2306.02115v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.02115v2)
- **Published**: 2023-06-03 14:01:54+00:00
- **Updated**: 2023-07-26 02:20:30+00:00
- **Authors**: Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe
- **Comment**: Accepted at ACL 2023
- **Journal**: None
- **Summary**: In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks.



### Graph Mover's Distance: An Efficiently Computable Distance Measure for Geometric Graphs
- **Arxiv ID**: http://arxiv.org/abs/2306.02133v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.02133v1)
- **Published**: 2023-06-03 15:06:12+00:00
- **Updated**: 2023-06-03 15:06:12+00:00
- **Authors**: Sushovan Majhi
- **Comment**: arXiv admin note: text overlap with arXiv:2209.12869
- **Journal**: None
- **Summary**: Many applications in pattern recognition represent patterns as a geometric graph. The geometric graph distance (GGD) has recently been studied as a meaningful measure of similarity between two geometric graphs. Since computing the GGD is known to be $\mathcal{NP}$-hard, the distance measure proves an impractical choice for applications. As a computationally tractable alternative, we propose in this paper the Graph Mover's Distance (GMD), which has been formulated as an instance of the earth mover's distance. The computation of the GMD between two geometric graphs with at most $n$ vertices takes only $O(n^3)$-time. Alongside studying the metric properties of the GMD, we investigate the stability of the GGD and GMD. The GMD also demonstrates extremely promising empirical evidence at recognizing letter drawings from the {\tt LETTER} dataset \cite{da_vitoria_lobo_iam_2008}.



### TransDocAnalyser: A Framework for Offline Semi-structured Handwritten Document Analysis in the Legal Domain
- **Arxiv ID**: http://arxiv.org/abs/2306.02142v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2306.02142v1)
- **Published**: 2023-06-03 15:56:30+00:00
- **Updated**: 2023-06-03 15:56:30+00:00
- **Authors**: Sagar Chakraborty, Gaurav Harit, Saptarshi Ghosh
- **Comment**: This paper has been accepted in 17th International Conference on
  Document Analysis and Recognition(ICDAR) as an Oral presentation
- **Journal**: None
- **Summary**: State-of-the-art offline Optical Character Recognition (OCR) frameworks perform poorly on semi-structured handwritten domain-specific documents due to their inability to localize and label form fields with domain-specific semantics. Existing techniques for semi-structured document analysis have primarily used datasets comprising invoices, purchase orders, receipts, and identity-card documents for benchmarking. In this work, we build the first semi-structured document analysis dataset in the legal domain by collecting a large number of First Information Report (FIR) documents from several police stations in India. This dataset, which we call the FIR dataset, is more challenging than most existing document analysis datasets, since it combines a wide variety of handwritten text with printed text. We also propose an end-to-end framework for offline processing of handwritten semi-structured documents, and benchmark it on our novel FIR dataset. Our framework used Encoder-Decoder architecture for localizing and labelling the form fields and for recognizing the handwritten content. The encoder consists of Faster-RCNN and Vision Transformers. Further the Transformer-based decoder architecture is trained with a domain-specific tokenizer. We also propose a post-correction method to handle recognition errors pertaining to the domain-specific terms. Our proposed framework achieves state-of-the-art results on the FIR dataset outperforming several existing models



### Hierarchical Multiresolution Feature- and Prior-based Graphs for Classification
- **Arxiv ID**: http://arxiv.org/abs/2306.02143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.02143v1)
- **Published**: 2023-06-03 15:58:38+00:00
- **Updated**: 2023-06-03 15:58:38+00:00
- **Authors**: Faezeh Fallah
- **Comment**: None
- **Journal**: None
- **Summary**: To incorporate spatial (neighborhood) and bidirectional hierarchical relationships as well as features and priors of the samples into their classification, we formulated the classification problem on three variants of multiresolution neighborhood graphs and the graph of a hierarchical conditional random field. Each of these graphs was weighted and undirected and could thus incorporate the spatial or hierarchical relationships in all directions. In addition, each variant of the proposed neighborhood graphs was composed of a spatial feature-based subgraph and an aspatial prior-based subgraph. It expanded on a random walker graph by using novel mechanisms to derive the edge weights of its spatial feature-based subgraph. These mechanisms included implicit and explicit edge detection to enhance detection of weak boundaries between different classes in spatial domain. The implicit edge detection relied on the outlier detection capability of the Tukey's function and the classification reliabilities of the samples estimated by a hierarchical random forest classifier. Similar mechanism was used to derive the edge weights and thus the energy function of the hierarchical conditional random field. This way, the classification problem boiled down to a system of linear equations and a minimization of the energy function which could be done via fast and efficient techniques.



### A two-way translation system of Chinese sign language based on computer vision
- **Arxiv ID**: http://arxiv.org/abs/2306.02144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.02144v2)
- **Published**: 2023-06-03 16:00:57+00:00
- **Updated**: 2023-06-17 18:04:07+00:00
- **Authors**: Shengzhuo Wei, Yan Lan
- **Comment**: None
- **Journal**: None
- **Summary**: As the main means of communication for deaf people, sign language has a special grammatical order, so it is meaningful and valuable to develop a real-time translation system for sign language. In the research process, we added a TSM module to the lightweight neural network model for the large Chinese continuous sign language dataset . It effectively improves the network performance with high accuracy and fast recognition speed. At the same time, we improve the Bert-Base-Chinese model to divide Chinese sentences into words and mapping the natural word order to the statute sign language order, and finally use the corresponding word videos in the isolated sign language dataset to generate the sentence video, so as to achieve the function of text-to-sign language translation. In the last of our research we built a system with sign language recognition and translation functions, and conducted performance tests on the complete dataset. The sign language video recognition accuracy reached about 99.3% with a time of about 0.05 seconds, and the sign language generation video time was about 1.3 seconds. The sign language system has good performance performance and is feasible.



### TransRUPNet for Improved Out-of-Distribution Generalization in Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.02176v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.02176v2)
- **Published**: 2023-06-03 19:06:06+00:00
- **Updated**: 2023-07-05 15:29:55+00:00
- **Authors**: Debesh Jha, Nikhil Kumar Tomar, Debayan Bhattacharya, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) generalization is a critical challenge in deep learning. It is specifically important when the test samples are drawn from a different distribution than the training data. We develop a novel real-time deep learning based architecture, TransRUPNet that is based on a Transformer and residual upsampling network for colorectal polyp segmentation to improve OOD generalization. The proposed architecture, TransRUPNet, is an encoder-decoder network that consists of three encoder blocks, three decoder blocks, and some additional upsampling blocks at the end of the network. With the image size of $256\times256$, the proposed method achieves an excellent real-time operation speed of \textbf{47.07} frames per second with an average mean dice coefficient score of 0.7786 and mean Intersection over Union of 0.7210 on the out-of-distribution polyp datasets. The results on the publicly available PolypGen dataset (OOD dataset in our case) suggest that TransRUPNet can give real-time feedback while retaining high accuracy for in-distribution dataset. Furthermore, we demonstrate the generalizability of the proposed method by showing that it significantly improves performance on OOD datasets compared to the existing methods.



### Word-Level Explanations for Analyzing Bias in Text-to-Image Models
- **Arxiv ID**: http://arxiv.org/abs/2306.05500v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.05500v1)
- **Published**: 2023-06-03 21:39:07+00:00
- **Updated**: 2023-06-03 21:39:07+00:00
- **Authors**: Alexander Lin, Lucas Monteiro Paes, Sree Harsha Tanneru, Suraj Srinivas, Himabindu Lakkaraju
- **Comment**: 5 main pages, 3 pages in appendix, and 3 figures
- **Journal**: None
- **Summary**: Text-to-image models take a sentence (i.e., prompt) and generate images associated with this input prompt. These models have created award wining-art, videos, and even synthetic datasets. However, text-to-image (T2I) models can generate images that underrepresent minorities based on race and sex. This paper investigates which word in the input prompt is responsible for bias in generated images. We introduce a method for computing scores for each word in the prompt; these scores represent its influence on biases in the model's output. Our method follows the principle of \emph{explaining by removing}, leveraging masked language models to calculate the influence scores. We perform experiments on Stable Diffusion to demonstrate that our method identifies the replication of societal stereotypes in generated images.



### Cycle Consistency Driven Object Discovery
- **Arxiv ID**: http://arxiv.org/abs/2306.02204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.02204v1)
- **Published**: 2023-06-03 21:49:06+00:00
- **Updated**: 2023-06-03 21:49:06+00:00
- **Authors**: Aniket Didolkar, Anirudh Goyal, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these consistency objectives to various existing slot-based object-centric methods, we demonstrate significant enhancements in object-discovery performance. These improvements are consistent across both synthetic and real-world scenes, highlighting the effectiveness and generalizability of the proposed approach. Furthermore, our experiments show that the learned slots from the proposed method exhibit superior suitability for downstream reinforcement learning (RL) tasks.



### Forgettable Federated Linear Learning with Certified Data Removal
- **Arxiv ID**: http://arxiv.org/abs/2306.02216v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.02216v1)
- **Published**: 2023-06-03 23:53:57+00:00
- **Updated**: 2023-06-03 23:53:57+00:00
- **Authors**: Ruinan Jin, Minghui Chen, Qiong Zhang, Xiaoxiao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) is a trending distributed learning framework that enables collaborative model training without data sharing. Machine learning models trained on datasets can potentially expose the private information of the training data, revealing details about individual data records. In this study, we focus on the FL paradigm that grants clients the ``right to be forgotten''. The forgettable FL framework should bleach its global model weights as it has never seen that client and hence does not reveal any information about the client. To this end, we propose the Forgettable Federated Linear Learning (2F2L) framework featured with novel training and data removal strategies. The training pipeline, named Federated linear training, employs linear approximation on the model parameter space to enable our 2F2L framework work for deep neural networks while achieving comparable results with canonical neural network training. We also introduce FedRemoval, an efficient and effective removal strategy that tackles the computational challenges in FL by approximating the Hessian matrix using public server data from the pretrained model. Unlike the previous uncertified and heuristic machine unlearning methods in FL, we provide theoretical guarantees by bounding the differences of model weights by our FedRemoval and that from retraining from scratch. Experimental results on MNIST and Fashion-MNIST datasets demonstrate the effectiveness of our method in achieving a balance between model accuracy and information removal, outperforming baseline strategies and approaching retraining from scratch.



