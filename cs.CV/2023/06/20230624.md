# Arxiv Papers in cs.CV on 2023-06-24
### Is Pre-training Truly Better Than Meta-Learning?
- **Arxiv ID**: http://arxiv.org/abs/2306.13841v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13841v1)
- **Published**: 2023-06-24 02:26:45+00:00
- **Updated**: 2023-06-24 02:26:45+00:00
- **Authors**: Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo
- **Comment**: None
- **Journal**: Proceedings of the 40th International Conference on Machine
  Learning 2023 DMLR Workshop
- **Summary**: In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is high, MAML beats PT on average. The caveat is that the magnitude of the average difference between a PT vs. MAML using the effect size is low (according to classical statistical thresholds) -- less than 0.2. Nevertheless, this observation is contrary to the currently held belief that a pre-trained model is always better than a meta-learning model. Our extensive experiments consider 21 few-shot learning benchmarks, including the large-scale few-shot learning dataset Meta-Data set. We also show no significant difference between a MAML model vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a pre-trained model does not always beat a meta-learned model and that the formal diversity of a dataset is a driving factor.



### Score-based Generative Models for Photoacoustic Image Reconstruction with Rotation Consistency Constraints
- **Arxiv ID**: http://arxiv.org/abs/2306.13843v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13843v1)
- **Published**: 2023-06-24 02:47:03+00:00
- **Updated**: 2023-06-24 02:47:03+00:00
- **Authors**: Shangqing Tong, Hengrong Lan, Liming Nie, Jianwen Luo, Fei Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Photoacoustic tomography (PAT) is a newly emerged imaging modality which enables both high optical contrast and acoustic depth of penetration. Reconstructing images of photoacoustic tomography from limited amount of senser data is among one of the major challenges in photoacoustic imaging. Previous works based on deep learning were trained in supervised fashion, which directly map the input partially known sensor data to the ground truth reconstructed from full field of view. Recently, score-based generative models played an increasingly significant role in generative modeling. Leveraging this probabilistic model, we proposed Rotation Consistency Constrained Score-based Generative Model (RCC-SGM), which recovers the PAT images by iterative sampling between Langevin dynamics and a constraint term utilizing the rotation consistency between the images and the measurements. Our proposed method can generalize to different measurement processes (32.29 PSNR with 16 measurements under random sampling, whereas 28.50 for supervised counterpart), while supervised methods need to train on specific inverse mappings.



### Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification
- **Arxiv ID**: http://arxiv.org/abs/2306.13856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.13856v1)
- **Published**: 2023-06-24 04:11:31+00:00
- **Updated**: 2023-06-24 04:11:31+00:00
- **Authors**: Rui Wang, Peipei Li, Huaibo Huang, Chunshui Cao, Ran He, Zhaofeng He
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordinal pairwise loss to refine the CLIP feature space, where texts and images maintain both semantic alignment and ordering alignment. Extensive experiments on three ordinal classification tasks, including facial age estimation, historical color image (HCI) classification, and aesthetic assessment demonstrate its promising performance.



### Real-World Video for Zoom Enhancement based on Spatio-Temporal Coupling
- **Arxiv ID**: http://arxiv.org/abs/2306.13875v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13875v1)
- **Published**: 2023-06-24 06:19:00+00:00
- **Updated**: 2023-06-24 06:19:00+00:00
- **Authors**: Zhiling Guo, Yinqiang Zheng, Haoran Zhang, Xiaodan Shi, Zekun Cai, Ryosuke Shibasaki, Jinyue Yan
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In recent years, single-frame image super-resolution (SR) has become more realistic by considering the zooming effect and using real-world short- and long-focus image pairs. In this paper, we further investigate the feasibility of applying realistic multi-frame clips to enhance zoom quality via spatio-temporal information coupling. Specifically, we first built a real-world video benchmark, VideoRAW, by a synchronized co-axis optical system. The dataset contains paired short-focus raw and long-focus sRGB videos of different dynamic scenes. Based on VideoRAW, we then presented a Spatio-Temporal Coupling Loss, termed as STCL. The proposed STCL is intended for better utilization of information from paired and adjacent frames to align and fuse features both temporally and spatially at the feature level. The outperformed experimental results obtained in different zoom scenarios demonstrate the superiority of integrating real-world video dataset and STCL into existing SR models for zoom quality enhancement, and reveal that the proposed method can serve as an advanced and viable tool for video zoom.



### Radio Generation Using Generative Adversarial Networks with An Unrolled Design
- **Arxiv ID**: http://arxiv.org/abs/2306.13893v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13893v1)
- **Published**: 2023-06-24 07:47:22+00:00
- **Updated**: 2023-06-24 07:47:22+00:00
- **Authors**: Weidong Wang, Jiancheng An, Hongshu Liao, Lu Gan, Chau Yuen
- **Comment**: Submitted to IEEE Transactions on Cognitive Communications and
  Networking on 20-Dec-2022
- **Journal**: None
- **Summary**: As a revolutionary generative paradigm of deep learning, generative adversarial networks (GANs) have been widely applied in various fields to synthesize realistic data. However, it is challenging for conventional GANs to synthesize raw signal data, especially in some complex cases. In this paper, we develop a novel GAN framework for radio generation called "Radio GAN". Compared to conventional methods, it benefits from three key improvements. The first is learning based on sampling points, which aims to model an underlying sampling distribution of radio signals. The second is an unrolled generator design, combined with an estimated pure signal distribution as a prior, which can greatly reduce learning difficulty and effectively improve learning precision. Finally, we present an energy-constrained optimization algorithm to achieve better training stability and convergence. Experimental results with extensive simulations demonstrate that our proposed GAN framework can effectively learn transmitter characteristics and various channel effects, thus accurately modeling for an underlying sampling distribution to synthesize radio signals of high quality.



### Open-Set RF Fingerprinting via Improved Prototype Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.13895v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13895v1)
- **Published**: 2023-06-24 08:04:06+00:00
- **Updated**: 2023-06-24 08:04:06+00:00
- **Authors**: Weidong Wang, Hongshu Liao, Lu Gan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been widely used in radio frequency (RF) fingerprinting. Despite its excellent performance, most existing methods only consider a closed-set assumption, which cannot effectively tackle signals emitted from those unknown devices that have never been seen during training. In this letter, we exploit prototype learning for open-set RF fingerprinting and propose two improvements, including consistency-based regularization and online label smoothing, which aim to learn a more robust feature space. Experimental results on a real-world RF dataset demonstrate that our proposed measures can significantly improve prototype learning to achieve promising open-set recognition performance for RF fingerprinting.



### Person Recognition using Facial Micro-Expressions with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.13907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13907v1)
- **Published**: 2023-06-24 08:57:15+00:00
- **Updated**: 2023-06-24 08:57:15+00:00
- **Authors**: Tuval Kay, Yuval Ringel, Khen Cohen, Mor-Avi Azulay, David Mendlovic
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: This study investigates the efficacy of facial micro-expressions as a soft biometric for enhancing person recognition, aiming to broaden the understanding of the subject and its potential applications. We propose a deep learning approach designed to capture spatial semantics and motion at a fine temporal resolution. Experiments on three widely-used micro-expression databases demonstrate a notable increase in identification accuracy compared to existing benchmarks, highlighting the potential of integrating facial micro-expressions for improved person recognition across various fields.



### ClothFit: Cloth-Human-Attribute Guided Virtual Try-On Network Using 3D Simulated Dataset
- **Arxiv ID**: http://arxiv.org/abs/2306.13908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13908v1)
- **Published**: 2023-06-24 08:57:36+00:00
- **Updated**: 2023-06-24 08:57:36+00:00
- **Authors**: Yunmin Cho, Lala Shakti Swarup Ray, Kundan Sai Prabhu Thota, Sungho Suh, Paul Lukowicz
- **Comment**: Accepted at IEEE International Conference on Image Processing 2023
  (ICIP 2023)
- **Journal**: None
- **Summary**: Online clothing shopping has become increasingly popular, but the high rate of returns due to size and fit issues has remained a major challenge. To address this problem, virtual try-on systems have been developed to provide customers with a more realistic and personalized way to try on clothing. In this paper, we propose a novel virtual try-on method called ClothFit, which can predict the draping shape of a garment on a target body based on the actual size of the garment and human attributes. Unlike existing try-on models, ClothFit considers the actual body proportions of the person and available cloth sizes for clothing virtualization, making it more appropriate for current online apparel outlets. The proposed method utilizes a U-Net-based network architecture that incorporates cloth and human attributes to guide the realistic virtual try-on synthesis. Specifically, we extract features from a cloth image using an auto-encoder and combine them with features from the user's height, weight, and cloth size. The features are concatenated with the features from the U-Net encoder, and the U-Net decoder synthesizes the final virtual try-on image. Our experimental results demonstrate that ClothFit can significantly improve the existing state-of-the-art methods in terms of photo-realistic virtual try-on results.



### Deep learning-based deconvolution for interferometric radio transient reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2306.13909v1
- **DOI**: 10.1051/0004-6361/202245013
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13909v1)
- **Published**: 2023-06-24 08:58:52+00:00
- **Updated**: 2023-06-24 08:58:52+00:00
- **Authors**: Benjamin Naoto Chiche, Julien N. Girard, Joana Frontera-Pons, Arnaud Woiselle, Jean-Luc Starck
- **Comment**: 15 pages, 19 figures. Published in A&A, Section Astronomical
  instrumentation
- **Journal**: A&A 675, A116 (2023)
- **Summary**: Radio astronomy is currently thriving with new large ground-based radio telescopes coming online in preparation for the upcoming Square Kilometre Array (SKA). Facilities like LOFAR, MeerKAT/SKA, ASKAP/SKA, and the future SKA-LOW bring tremendous sensitivity in time and frequency, improved angular resolution, and also high-rate data streams that need to be processed. They enable advanced studies of radio transients, volatile by nature, that can be detected or missed in the data. These transients are markers of high-energy accelerations of electrons and manifest in a wide range of temporal scales. Usually studied with dynamic spectroscopy of time series analysis, there is a motivation to search for such sources in large interferometric datasets. This requires efficient and robust signal reconstruction algorithms. To correctly account for the temporal dependency of the data, we improve the classical image deconvolution inverse problem by adding the temporal dependency in the reconstruction problem. Then, we introduce two novel neural network architectures that can do both spatial and temporal modeling of the data and the instrumental response. Then, we simulate representative time-dependent image cubes of point source distributions and realistic telescope pointings of MeerKAT to generate toy models to build the training, validation, and test datasets. Finally, based on the test data, we evaluate the source profile reconstruction performance of the proposed methods and classical image deconvolution algorithm CLEAN applied frame-by-frame. In the presence of increasing noise level in data frame, the proposed methods display a high level of robustness compared to frame-by-frame imaging with CLEAN. The deconvolved image cubes bring a factor of 3 improvement in fidelity of the recovered temporal profiles and a factor of 2 improvement in background denoising.



### Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.13924v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13924v1)
- **Published**: 2023-06-24 10:07:52+00:00
- **Updated**: 2023-06-24 10:07:52+00:00
- **Authors**: Sharut Gupta, Joshua Robinson, Derek Lim, Soledad Villar, Stefanie Jegelka
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE.



### Boost Video Frame Interpolation via Motion Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2306.13933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13933v1)
- **Published**: 2023-06-24 10:44:02+00:00
- **Updated**: 2023-06-24 10:44:02+00:00
- **Authors**: Haoning Wu, Xiaoyun Zhang, Weidi Xie, Ya Zhang, Yanfeng Wang
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Video frame interpolation (VFI) is a challenging task that aims to generate intermediate frames between two consecutive frames in a video. Existing learning-based VFI methods have achieved great success, but they still suffer from limited generalization ability due to the limited motion distribution of training datasets. In this paper, we propose a novel optimization-based VFI method that can adapt to unseen motions at test time. Our method is based on a cycle-consistency adaptation strategy that leverages the motion characteristics among video frames. We also introduce a lightweight adapter that can be inserted into the motion estimation module of existing pre-trained VFI models to improve the efficiency of adaptation. Extensive experiments on various benchmarks demonstrate that our method can boost the performance of two-frame VFI models, outperforming the existing state-of-the-art methods, even those that use extra input.



### Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2306.13960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13960v2)
- **Published**: 2023-06-24 13:29:54+00:00
- **Updated**: 2023-07-20 10:26:56+00:00
- **Authors**: Thijs P. Kuipers, Erik J. Bekkers
- **Comment**: 10 pages, 1 figure, 2 tables, accepted at MICCAI 2023. Updated
  version to camera ready version 1
- **Journal**: None
- **Summary**: Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in accuracy over regular CNNs.



### Mobile-Cloud Inference for Collaborative Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2306.13982v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13982v1)
- **Published**: 2023-06-24 14:22:53+00:00
- **Updated**: 2023-06-24 14:22:53+00:00
- **Authors**: Mateen Ulhaq
- **Comment**: 56 pages, 20 figures, Bachelor's Thesis, defended in 2020
- **Journal**: None
- **Summary**: As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches.   There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference latency, energy consumption, and network bandwidth usage, as well as provide privacy protection, because the original signal never leaves the mobile. Further performance gain can be achieved by compressing the feature tensor before its transmission.



### SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference
- **Arxiv ID**: http://arxiv.org/abs/2306.13988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13988v1)
- **Published**: 2023-06-24 14:46:49+00:00
- **Updated**: 2023-06-24 14:46:49+00:00
- **Authors**: Xiaoyu Bai, Yong Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Medical images like CT and MRI provide detailed information about the internal structure of the body, and identifying key anatomical structures from these images plays a crucial role in clinical workflows. Current methods treat it as a registration or key-point regression task, which has limitations in accurate matching and can only handle predefined landmarks. Recently, some methods have been introduced to address these limitations. One such method, called SAM, proposes using a dense self-supervised approach to learn a distinct embedding for each point on the CT image and achieving promising results. Nonetheless, SAM may still face difficulties when dealing with structures that have similar appearances but different semantic meanings or similar semantic meanings but different appearances. To overcome these limitations, we propose SAM++, a framework that simultaneously learns appearance and semantic embeddings with a novel fixed-points matching mechanism. We tested the SAM++ framework on two challenging tasks, demonstrating a significant improvement over the performance of SAM and outperforming other existing methods.



### Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation
- **Arxiv ID**: http://arxiv.org/abs/2306.13990v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13990v1)
- **Published**: 2023-06-24 14:50:20+00:00
- **Updated**: 2023-06-24 14:50:20+00:00
- **Authors**: Jianan Chen, Anne Martel
- **Comment**: None
- **Journal**: None
- **Summary**: Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we show that removing ReCoV-identified noisy samples in two medical imaging outcome prediction datasets significantly improves model performance on test sets. As a statistical approach that does not rely on hyperparameters, noise distributions, or model structures, ReCoV is compatible with any machine learning analysis.



### Thinking Like an Annotator: Generation of Dataset Labeling Instructions
- **Arxiv ID**: http://arxiv.org/abs/2306.14035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.14035v1)
- **Published**: 2023-06-24 18:32:48+00:00
- **Updated**: 2023-06-24 18:32:48+00:00
- **Authors**: Nadine Chang, Francesco Ferroni, Michael J. Tarr, Martial Hebert, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale datasets are essential to modern day deep learning. Advocates argue that understanding these methods requires dataset transparency (e.g. "dataset curation, motivation, composition, collection process, etc..."). However, almost no one has suggested the release of the detailed definitions and visual category examples provided to annotators - information critical to understanding the structure of the annotations present in each dataset. These labels are at the heart of public datasets, yet few datasets include the instructions that were used to generate them. We introduce a new task, Labeling Instruction Generation, to address missing publicly available labeling instructions. In Labeling Instruction Generation, we take a reasonably annotated dataset and: 1) generate a set of examples that are visually representative of each category in the dataset; 2) provide a text label that corresponds to each of the examples. We introduce a framework that requires no model training to solve this task and includes a newly created rapid retrieval system that leverages a large, pre-trained vision and language model. This framework acts as a proxy to human annotators that can help to both generate a final labeling instruction set and evaluate its quality. Our framework generates multiple diverse visual and text representations of dataset categories. The optimized instruction set outperforms our strongest baseline across 5 folds by 7.06 mAP for NuImages and 12.9 mAP for COCO.



### Semantic Segmentation of Porosity in 4D Spatio-Temporal X-ray μCT of Titanium Coated Ni wires using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.14039v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.14039v1)
- **Published**: 2023-06-24 19:08:57+00:00
- **Updated**: 2023-06-24 19:08:57+00:00
- **Authors**: Pradyumna Elavarthi, Arun Bhattacharjee, Ashley Paz y Puente, Anca Ralescu
- **Comment**: None
- **Journal**: None
- **Summary**: A fully convolutional neural network was used to measure the evolution of the volume fraction of two different Kirkendall pores during the homogenization of Ti coated Ni wires. Traditional methods like Otsus thresholding and the largest connected component analysis were used to obtain the masks for training the segmentation model. Once trained, the model was used to semantically segment the two types of pores at different stages in their evolution. Masks of the pores predicted by the network were then used to measure the volume fraction of porosity at 0 mins, 240 mins, and 480 mins of homogenization. The model predicted an increase in porosity for one type of pore and a decrease in porosity for another type of pore due to pore sintering, and it achieved an F1 Score of 0.95.



### Utilizing Segment Anything Model For Assessing Localization of GRAD-CAM in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2306.15692v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.15692v1)
- **Published**: 2023-06-24 19:54:50+00:00
- **Updated**: 2023-06-24 19:54:50+00:00
- **Authors**: Evan Kellener, Ihina Nath, An Ngo, Thomas Nguyen, Joshua Schuman, Coen Adler, Arnav Kartikeya
- **Comment**: 11 pages, 14 figures, 1 table
- **Journal**: None
- **Summary**: The introduction of saliency map algorithms as an approach for assessing the interoperability of images has allowed for a deeper understanding of current black-box models with Artificial Intelligence. Their rise in popularity has led to these algorithms being applied in multiple fields, including medical imaging. With a classification task as important as those in the medical domain, a need for rigorous testing of their capabilities arises. Current works examine capabilities through assessing the localization of saliency maps upon medical abnormalities within an image, through comparisons with human annotations. We propose utilizing Segment Anything Model (SAM) to both further the accuracy of such existing metrics, while also generalizing beyond the need for human annotations. Our results show both high degrees of similarity to existing metrics while also highlighting the capabilities of this methodology to beyond human-annotation. Furthermore, we explore the applications (and challenges) of SAM within the medical domain, including image pre-processing before segmenting, natural language proposals to SAM in the form of CLIP-SAM, and SAM accuracy across multiple medical imaging datasets.



### Stable Yaw Estimation of Boats from the Viewpoint of UAVs and USVs
- **Arxiv ID**: http://arxiv.org/abs/2306.14056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.14056v1)
- **Published**: 2023-06-24 20:47:37+00:00
- **Updated**: 2023-06-24 20:47:37+00:00
- **Authors**: Benjamin Kiefer, Timon Höfer, Andreas Zell
- **Comment**: Accepted at ECMR 2023
- **Journal**: None
- **Summary**: Yaw estimation of boats from the viewpoint of unmanned aerial vehicles (UAVs) and unmanned surface vehicles (USVs) or boats is a crucial task in various applications such as 3D scene rendering, trajectory prediction, and navigation. However, the lack of literature on yaw estimation of objects from the viewpoint of UAVs has motivated us to address this domain. In this paper, we propose a method based on HyperPosePDF for predicting the orientation of boats in the 6D space. For that, we use existing datasets, such as PASCAL3D+ and our own datasets, SeaDronesSee-3D and BOArienT, which we annotated manually. We extend HyperPosePDF to work in video-based scenarios, such that it yields robust orientation predictions across time. Naively applying HyperPosePDF on video data yields single-point predictions, resulting in far-off predictions and often incorrect symmetric orientations due to unseen or visually different data. To alleviate this issue, we propose aggregating the probability distributions of pose predictions, resulting in significantly improved performance, as shown in our experimental evaluation. Our proposed method could significantly benefit downstream tasks in marine robotics.



### Creating Realistic Anterior Segment Optical Coherence Tomography Images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2306.14058v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2306.14058v1)
- **Published**: 2023-06-24 20:48:00+00:00
- **Updated**: 2023-06-24 20:48:00+00:00
- **Authors**: Jad F. Assaf, Anthony Abou Mrad, Dan Z. Reinstein, Guillermo Amescua, Cyril Zakka, Timothy Archer, Jeffrey Yammine, Elsa Lamah, Michèle Haykal, Shady T. Awwad
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the development and validation of a Generative Adversarial Network (GAN) purposed to create high-resolution, realistic Anterior Segment Optical Coherence Tomography (AS-OCT) images. We trained the Style and WAvelet based GAN (SWAGAN) on 142,628 AS-OCT B-scans. Three experienced refractive surgeons performed a blinded assessment to evaluate the realism of the generated images; their results were not significantly better than chance in distinguishing between real and synthetic images, thus demonstrating a high degree of image realism. To gauge their suitability for machine learning tasks, a convolutional neural network (CNN) classifier was trained with a dataset containing both real and GAN-generated images. The CNN demonstrated an accuracy rate of 78% trained on real images alone, but this accuracy rose to 100% when training included the generated images. This underscores the utility of synthetic images for machine learning applications. We further improved the resolution of the generated images by up-sampling them twice (2x) using an Enhanced Super Resolution GAN (ESRGAN), which outperformed traditional up-sampling techniques. In conclusion, GANs can effectively generate high-definition, realistic AS-OCT images, proving highly beneficial for machine learning and image analysis tasks.



### DesCo: Learning Object Recognition with Rich Language Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2306.14060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.14060v1)
- **Published**: 2023-06-24 21:05:02+00:00
- **Updated**: 2023-06-24 21:05:02+00:00
- **Authors**: Liunian Harold Li, Zi-Yi Dou, Nanyun Peng, Kai-Wei Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition models with rich language descriptions consisting of two major innovations: 1) we employ a large language model as a commonsense knowledge engine to generate rich language descriptions of objects based on object names and the raw image-text caption; 2) we design context-sensitive queries to improve the model's ability in deciphering intricate nuances embedded within descriptions and enforce the model to focus on context rather than object names alone. On two novel object detection benchmarks, LVIS and OminiLabel, under the zero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and 29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models, GLIP and FIBER, by a large margin.



### SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.14070v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2306.14070v1)
- **Published**: 2023-06-24 22:39:33+00:00
- **Updated**: 2023-06-24 22:39:33+00:00
- **Authors**: Pu Ren, N. Benjamin Erichson, Shashank Subramanian, Omer San, Zarija Lukic, Michael W. Mahoney
- **Comment**: None
- **Journal**: None
- **Summary**: Super-Resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets (up to $2048\times2048$ dimensions), including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will significantly advance SR methods for scientific tasks.



### Efficient Annotation of Medieval Charters
- **Arxiv ID**: http://arxiv.org/abs/2306.14071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.14071v1)
- **Published**: 2023-06-24 22:55:55+00:00
- **Updated**: 2023-06-24 22:55:55+00:00
- **Authors**: Anguelos Nicolaou, Daniel Luger, Franziska Decker, Nicolas Renet, Vincent Christlein, Georg Vogeler
- **Comment**: None
- **Journal**: None
- **Summary**: Diplomatics, the analysis of medieval charters, is a major field of research in which paleography is applied. Annotating data, if performed by laymen, needs validation and correction by experts. In this paper, we propose an effective and efficient annotation approach for charter segmentation, essentially reducing it to object detection. This approach allows for a much more efficient use of the paleographer's time and produces results that can compete and even outperform pixel-level segmentation in some use cases. Further experiments shed light on how to design a class ontology in order to make the best use of annotators' time and effort. Exploiting the presence of calibration cards in the image, we further annotate the data with the physical length in pixels and train regression neural networks to predict it from image patches.



