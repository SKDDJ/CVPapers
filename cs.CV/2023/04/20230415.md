# Arxiv Papers in cs.CV on 2023-04-15
### Temporally Consistent Online Depth Estimation Using Point-Based Fusion
- **Arxiv ID**: http://arxiv.org/abs/2304.07435v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07435v3)
- **Published**: 2023-04-15 00:04:18+00:00
- **Updated**: 2023-08-05 00:01:32+00:00
- **Authors**: Numair Khan, Eric Penner, Douglas Lanman, Lei Xiao
- **Comment**: Source code:
  https://github.com/facebookresearch/TemporallyConsistentDepth
- **Journal**: CVPR 2023
- **Summary**: Depth estimation is an important step in many computer vision problems such as 3D reconstruction, novel view synthesis, and computational photography. Most existing work focuses on depth estimation from single frames. When applied to videos, the result lacks temporal consistency, showing flickering and swimming artifacts. In this paper we aim to estimate temporally consistent depth maps of video streams in an online setting. This is a difficult problem as future frames are not available and the method must choose between enforcing consistency and correcting errors from previous estimations. The presence of dynamic objects further complicates the problem. We propose to address these challenges by using a global point cloud that is dynamically updated each frame, along with a learned fusion approach in image space. Our approach encourages consistency while simultaneously allowing updates to handle errors and dynamic objects. Qualitative and quantitative results show that our method achieves state-of-the-art quality for consistent video depth estimation.



### Few-shot Camouflaged Animal Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.07444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07444v1)
- **Published**: 2023-04-15 01:33:14+00:00
- **Updated**: 2023-04-15 01:33:14+00:00
- **Authors**: Thanh-Danh Nguyen, Anh-Khoa Nguyen Vu, Nhat-Duy Nguyen, Vinh-Tiep Nguyen, Thanh Duc Ngo, Thanh-Toan Do, Minh-Triet Tran, Tam V. Nguyen
- **Comment**: Under-review Journal
- **Journal**: None
- **Summary**: Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data of camouflaged objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. We then propose a novel method to efficiently detect and segment the camouflaged objects in the images. In particular, we introduce the instance triplet loss and the instance memory storage. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset.



### Instance-level Few-shot Learning with Class Hierarchy Mining
- **Arxiv ID**: http://arxiv.org/abs/2304.07459v1
- **DOI**: 10.1109/TIP.2023.3267621
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07459v1)
- **Published**: 2023-04-15 02:55:08+00:00
- **Updated**: 2023-04-15 02:55:08+00:00
- **Authors**: Anh-Khoa Nguyen Vu, Thanh-Toan Do, Nhat-Duy Nguyen, Vinh-Tiep Nguyen, Thanh Duc Ngo, Tam V. Nguyen
- **Comment**: accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Few-shot learning is proposed to tackle the problem of scarce training data in novel classes. However, prior works in instance-level few-shot learning have paid less attention to effectively utilizing the relationship between categories. In this paper, we exploit the hierarchical information to leverage discriminative and relevant features of base classes to effectively classify novel objects. These features are extracted from abundant data of base classes, which could be utilized to reasonably describe classes with scarce data. Specifically, we propose a novel superclass approach that automatically creates a hierarchy considering base and novel classes as fine-grained classes for few-shot instance segmentation (FSIS). Based on the hierarchical information, we design a novel framework called Soft Multiple Superclass (SMS) to extract relevant features or characteristics of classes in the same superclass. A new class assigned to the superclass is easier to classify by leveraging these relevant features. Besides, in order to effectively train the hierarchy-based-detector in FSIS, we apply the label refinement to further describe the associations between fine-grained classes. The extensive experiments demonstrate the effectiveness of our method on FSIS benchmarks. Code is available online.



### Beta-Rank: A Robust Convolutional Filter Pruning Method For Imbalanced Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2304.07461v2
- **DOI**: None
- **Categories**: **cs.CV**, 03D15, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2304.07461v2)
- **Published**: 2023-04-15 03:05:47+00:00
- **Updated**: 2023-06-26 03:35:51+00:00
- **Authors**: Morteza Homayounfar, Mohamad Koohi-Moghadam, Reza Rawassizadeh, Varut Vardhanabhuti
- **Comment**: 13 pages, 3 figures, and 3 tables
- **Journal**: None
- **Summary**: As deep neural networks include a high number of parameters and operations, it can be a challenge to implement these models on devices with limited computational resources. Despite the development of novel pruning methods toward resource-efficient models, it has become evident that these models are not capable of handling "imbalanced" and "limited number of data points". We proposed a novel filter pruning method by considering the input and output of filters along with the values of the filters that deal with imbalanced datasets better than others. Our pruning method considers the fact that all information about the importance of a filter may not be reflected in the value of the filter. Instead, it is reflected in the changes made to the data after the filter is applied to it. In this work, three methods are compared with the same training conditions except for the ranking values of each method, and 14 methods are compared from other papers. We demonstrated that our model performed significantly better than other methods for imbalanced medical datasets. For example, when we removed up to 58% of FLOPs for the IDRID dataset and up to 45% for the ISIC dataset, our model was able to yield an equivalent (or even superior) result to the baseline model. To evaluate FLOP and parameter reduction using our model in real-world settings, we built a smartphone app, where we demonstrated a reduction of up to 79% in memory usage and 72% in prediction time. All codes and parameters for training different models are available at https://github.com/mohofar/Beta-Rank



### Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2304.08498v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.08498v1)
- **Published**: 2023-04-15 03:38:09+00:00
- **Updated**: 2023-04-15 03:38:09+00:00
- **Authors**: Pooria Mazaheri, Azam Asilian Bidgoli, Shahryar Rahnamayan, H. R. Tizhoosh
- **Comment**: Under Review for publication
- **Journal**: None
- **Summary**: Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision, significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality. The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking of matched outputs, the representation learning is customized toward image search instead of learning a class label. Second, we introduce the concept of sequestering learning to enhance the generalization of feature extraction. By excluding the images of the input hospital from the matched outputs, i.e., sequestering the input domain, the institutional bias is reduced. The proposed ideas are implemented and validated through the largest public dataset of whole slide images. The experiments demonstrate superior results compare to the-state-of-art.



### MvCo-DoT:Multi-View Contrastive Domain Transfer Network for Medical Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2304.07465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07465v1)
- **Published**: 2023-04-15 03:42:26+00:00
- **Updated**: 2023-04-15 03:42:26+00:00
- **Authors**: Ruizhi Wang, Xiangtao Wang, Zhenghua Xu, Wenting Xu, Junyang Chen, Thomas Lukasiewicz
- **Comment**: Received by the ICASSP2023
- **Journal**: None
- **Summary**: In clinical scenarios, multiple medical images with different views are usually generated at the same time, and they have high semantic consistency. However, the existing medical report generation methods cannot exploit the rich multi-view mutual information of medical images. Therefore, in this work, we propose the first multi-view medical report generation model, called MvCo-DoT. Specifically, MvCo-DoT first propose a multi-view contrastive learning (MvCo) strategy to help the deep reinforcement learning based model utilize the consistency of multi-view inputs for better model learning. Then, to close the performance gaps of using multi-view and single-view inputs, a domain transfer network is further proposed to ensure MvCo-DoT achieve almost the same performance as multi-view inputs using only single-view inputs.Extensive experiments on the IU X-Ray public dataset show that MvCo-DoT outperforms the SOTA medical report generation baselines in all metrics.



### Generating an interactive online map of future sea level rise along the North Shore of Vancouver: methods and insights on enabling geovisualisation for coastal communities
- **Arxiv ID**: http://arxiv.org/abs/2304.07469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2304.07469v1)
- **Published**: 2023-04-15 04:12:55+00:00
- **Updated**: 2023-04-15 04:12:55+00:00
- **Authors**: Forrest DiPaola, Anshuman Bhardwaj, Lydia Sam
- **Comment**: 29 pages, 10 figures, 8 tables
- **Journal**: None
- **Summary**: Contemporary sea level rise (SLR) research seldom considers enabling effective geovisualisation for the communities. This lack of knowledge transfer impedes raising awareness on climate change and its impacts. The goal of this study is to produce an online SLR map accessible to the public that allows them to interact with evolving high-resolution geospatial data and techniques. The study area was the North Shore of Vancouver, British Columbia, Canada. While typically coarser resolution (10m+/pixel) Digital Elevation Models have been used by previous studies, we explored an open access airborne 1 metre LiDAR which has a higher resolution and vertical accuracy and can penetrate tree cover at a higher degree than most satellite imagery. A bathtub method model with hydrologic connectivity was used to delineate the inundation zones for various SLR scenarios which allows for a not overly complex model and process using standard tools such as ArcGIS and QGIS with similar levels of accuracy as more complex models, especially with the high-resolution data. Deep Learning and 3D visualizations were used to create past, present, and modelled future Land Use/Land Cover and 3D flyovers. Analysis of the possible impacts of 1m, 2m, 3m, and 4m SLR over the unique coastline, terrain and land use was detailed. The generated interactive online map helps local communities visualise and understand the future of their coastlines. We have provided a detailed methodology and the methods and results are easily reproducible for other regions. Such initiatives can help popularise community-focused geovisualisation to raise awareness about SLR.



### Hierarchical Interactive Reconstruction Network For Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2304.07473v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07473v1)
- **Published**: 2023-04-15 04:57:57+00:00
- **Updated**: 2023-04-15 04:57:57+00:00
- **Authors**: Tong Zhang, Wenxue Cui, Chen Hui, Feng Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep network-based image and video Compressive Sensing(CS) has attracted increasing attentions in recent years. However, in the existing deep network-based CS methods, a simple stacked convolutional network is usually adopted, which not only weakens the perception of rich contextual prior knowledge, but also limits the exploration of the correlations between temporal video frames. In this paper, we propose a novel Hierarchical InTeractive Video CS Reconstruction Network(HIT-VCSNet), which can cooperatively exploit the deep priors in both spatial and temporal domains to improve the reconstruction quality. Specifically, in the spatial domain, a novel hierarchical structure is designed, which can hierarchically extract deep features from keyframes and non-keyframes. In the temporal domain, a novel hierarchical interaction mechanism is proposed, which can cooperatively learn the correlations among different frames in the multiscale space. Extensive experiments manifest that the proposed HIT-VCSNet outperforms the existing state-of-the-art video and image CS methods in a large margin.



### Video Generation Beyond a Single Clip
- **Arxiv ID**: http://arxiv.org/abs/2304.07483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07483v1)
- **Published**: 2023-04-15 06:17:30+00:00
- **Updated**: 2023-04-15 06:17:30+00:00
- **Authors**: Hsin-Ping Huang, Yu-Chuan Su, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the long video generation problem, i.e.~generating videos beyond the output length of video generation models. Due to the computation resource constraints, video generation models can only generate video clips that are relatively short compared with the length of real videos. Existing works apply a sliding window approach to generate long videos at inference time, which is often limited to generating recurrent events or homogeneous content. To generate long videos covering diverse content and multiple events, we propose to use additional guidance to control the video generation process. We further present a two-stage approach to the problem, which allows us to utilize existing video generation models to generate high-quality videos within a small time window while modeling the video holistically based on the input guidance. The proposed approach is complementary to existing efforts on video generation, which focus on generating realistic video within a fixed time window. Extensive experiments on challenging real-world videos validate the benefit of the proposed method, which improves over state-of-the-art by up to 9.5% in objective metrics and is preferred by users more than 80% of time.



### Region-Enhanced Feature Learning for Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.07486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07486v2)
- **Published**: 2023-04-15 06:35:06+00:00
- **Updated**: 2023-04-18 01:50:18+00:00
- **Authors**: Xin Kang, Chaoqun Wang, Xuejin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation in complex scenes not only relies on local object appearance but also on object locations and the surrounding environment. Nonetheless, it is difficult to model long-range context in the format of pairwise point correlations due to its huge computational cost for large-scale point clouds. In this paper, we propose to use regions as the intermediate representation of point clouds instead of fine-grained points or voxels to reduce the computational burden. We introduce a novel Region-Enhanced Feature Learning network (REFL-Net) that leverages region correlations to enhance the features of ambiguous points. We design a Region-based Feature Enhancement module (RFE) which consists of a Semantic-Spatial Region Extraction (SSRE) stage and a Region Dependency Modeling (RDM) stage. In the SSRE stage, we group the input points into a set of regions according to the point distances in both semantic and spatial space. In the RDM part, we explore region-wise semantic and spatial relationships via a self-attention block on region features and fuse point features with the region features to obtain more discriminative representations. Our proposed RFE module is a plug-and-play module that can be integrated with common semantic segmentation backbones. We conduct extensive experiments on ScanNetv2 and S3DIS datasets, and evaluate our RFE module with different segmentation backbones. Our REFL-Net achieves 1.8% mIoU gain on ScanNetv2 and 1.0% mIoU gain on S3DIS respectively with negligible computational cost compared to the backbone networks. Both quantitative and qualitative results show the powerful long-range context modeling ability and strong generalization ability of our REFL-Net.



### The 7th AI City Challenge
- **Arxiv ID**: http://arxiv.org/abs/2304.07500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07500v1)
- **Published**: 2023-04-15 08:02:16+00:00
- **Updated**: 2023-04-15 08:02:16+00:00
- **Authors**: Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Yue Yao, Liang Zheng, Mohammed Shaiqur Rahman, Meenakshi S. Arya, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff, Pranamesh Chakraborty, Sanjita Prajapati, Alice Li, Shangru Li, Krishna Kunadharaju, Shenxin Jiang, Rama Chellappa
- **Comment**: Summary of the 7th AI City Challenge Workshop in conjunction with
  CVPR 2023
- **Journal**: None
- **Summary**: The AI City Challenge's seventh edition emphasizes two domains at the intersection of computer vision and artificial intelligence - retail business and Intelligent Traffic Systems (ITS) - that have considerable untapped potential. The 2023 challenge had five tracks, which drew a record-breaking number of participation requests from 508 teams across 46 countries. Track 1 was a brand new track that focused on multi-target multi-camera (MTMC) people tracking, where teams trained and evaluated using both real and highly realistic synthetic data. Track 2 centered around natural-language-based vehicle track retrieval. Track 3 required teams to classify driver actions in naturalistic driving analysis. Track 4 aimed to develop an automated checkout system for retail stores using a single view camera. Track 5, another new addition, tasked teams with detecting violations of the helmet rule for motorcyclists. Two leader boards were released for submissions based on different methods: a public leader board for the contest where external private data wasn't allowed and a general leader board for all results submitted. The participating teams' top performances established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks.



### S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2304.07515v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.07515v2)
- **Published**: 2023-04-15 09:39:52+00:00
- **Updated**: 2023-07-24 08:10:52+00:00
- **Authors**: Lennart Bastian, Alexander Baumann, Emily Hoppe, Vincent Bürgin, Ha Young Kim, Mahdi Saleh, Benjamin Busam, Nassir Navab
- **Comment**: Accepted at MICCAI 2023. 13 pages, 6 figures
- **Journal**: None
- **Summary**: Statistical shape models (SSMs) are an established way to represent the anatomy of a population with various clinically relevant applications. However, they typically require domain expertise, and labor-intensive landmark annotations to construct. We address these shortcomings by proposing an unsupervised method that leverages deep geometric features and functional correspondences to simultaneously learn local and global shape structures across population anatomies. Our pipeline significantly improves unsupervised correspondence estimation for SSMs compared to baseline methods, even on highly irregular surface topologies. We demonstrate this for two different anatomical structures: the thyroid and a multi-chamber heart dataset. Furthermore, our method is robust enough to learn from noisy neural network predictions, potentially enabling scaling SSMs to larger patient populations without manual segmentation annotation.



### Compete to Win: Enhancing Pseudo Labels for Barely-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.07519v2
- **DOI**: 10.1109/TMI.2023.3279110
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07519v2)
- **Published**: 2023-04-15 10:04:14+00:00
- **Updated**: 2023-07-30 14:15:55+00:00
- **Authors**: Huimin Wu, Xiaomeng Li, Yiqun Lin, Kwang-Ting Cheng
- **Comment**: Accepted by TMI (in IEEE Transactions on Neural Networks and Learning
  Systems). Code available at https://github.com/Huiimin5/comwin
- **Journal**: None
- **Summary**: This study investigates barely-supervised medical image segmentation where only few labeled data, i.e., single-digit cases are available. We observe the key limitation of the existing state-of-the-art semi-supervised solution cross pseudo supervision is the unsatisfactory precision of foreground classes, leading to a degenerated result under barely-supervised learning. In this paper, we propose a novel Compete-to-Win method (ComWin) to enhance the pseudo label quality. In contrast to directly using one model's predictions as pseudo labels, our key idea is that high-quality pseudo labels should be generated by comparing multiple confidence maps produced by different networks to select the most confident one (a compete-to-win strategy). To further refine pseudo labels at near-boundary areas, an enhanced version of ComWin, namely, ComWin+, is proposed by integrating a boundary-aware enhancement module. Experiments show that our method can achieve the best performance on three public medical image datasets for cardiac structure segmentation, pancreas segmentation and colon tumor segmentation, respectively. The source code is now available at https://github.com/Huiimin5/comwin.



### ID2image: Leakage of non-ID information into face descriptors and inversion from descriptors to images
- **Arxiv ID**: http://arxiv.org/abs/2304.07522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07522v1)
- **Published**: 2023-04-15 10:11:22+00:00
- **Updated**: 2023-04-15 10:11:22+00:00
- **Authors**: Mingrui Li, William A. P. Smith, Patrik Huber
- **Comment**: SCIA 2023
- **Journal**: None
- **Summary**: Embedding a face image to a descriptor vector using a deep CNN is a widely used technique in face recognition. Via several possible training strategies, such embeddings are supposed to capture only identity information. Information about the environment (such as background and lighting) or changeable aspects of the face (such as pose, expression, presence of glasses, hat etc.) should be discarded since they are not useful for recognition. In this paper, we present a surprising result that this is not the case. We show that non-ID attributes, as well as landmark positions and the image histogram can be recovered from the ID embedding of state-of-the-art face embedding networks (VGGFace2 and ArcFace). In fact, these non-ID attributes can be predicted from ID embeddings with similar accuracy to a prediction from the original image. Going further, we present an optimisation strategy that uses a generative model (specifically StyleGAN2 for faces) to recover images from an ID embedding. We show photorealistic inversion from ID embedding to face image in which not only is the ID realistically reconstructed but the pose, lighting and background/apparel to some extent as well.



### Align-DETR: Improving DETR with Simple IoU-aware BCE loss
- **Arxiv ID**: http://arxiv.org/abs/2304.07527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07527v1)
- **Published**: 2023-04-15 10:24:51+00:00
- **Updated**: 2023-04-15 10:24:51+00:00
- **Authors**: Zhi Cai, Songtao Liu, Guodong Wang, Zheng Ge, Xiangyu Zhang, Di Huang
- **Comment**: None
- **Journal**: None
- **Summary**: DETR has set up a simple end-to-end pipeline for object detection by formulating this task as a set prediction problem, showing promising potential. However, despite the significant progress in improving DETR, this paper identifies a problem of misalignment in the output distribution, which prevents the best-regressed samples from being assigned with high confidence, hindering the model's accuracy. We propose a metric, recall of best-regressed samples, to quantitively evaluate the misalignment problem. Observing its importance, we propose a novel Align-DETR that incorporates a localization precision-aware classification loss in optimization. The proposed loss, IA-BCE, guides the training of DETR to build a strong correlation between classification score and localization precision. We also adopt the mixed-matching strategy, to facilitate DETR-based detectors with faster training convergence while keeping an end-to-end scheme. Moreover, to overcome the dramatic decrease in sample quality induced by the sparsity of queries, we introduce a prime sample weighting mechanism to suppress the interference of unimportant samples. Extensive experiments are conducted with very competitive results reported. In particular, it delivers a 46 (+3.8)% AP on the DAB-DETR baseline with the ResNet-50 backbone and reaches a new SOTA performance of 50.2% AP in the 1x setting on the COCO validation set when employing the strong baseline DINO. Our code is available at https://github.com/FelixCaae/AlignDETR.



### ALiSNet: Accurate and Lightweight Human Segmentation Network for Fashion E-Commerce
- **Arxiv ID**: http://arxiv.org/abs/2304.07533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07533v1)
- **Published**: 2023-04-15 11:06:32+00:00
- **Updated**: 2023-04-15 11:06:32+00:00
- **Authors**: Amrollah Seifoddini, Koen Vernooij, Timon Künzle, Alessandro Canopoli, Malte Alf, Anna Volokitin, Reza Shirvany
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately estimating human body shape from photos can enable innovative applications in fashion, from mass customization, to size and fit recommendations and virtual try-on. Body silhouettes calculated from user pictures are effective representations of the body shape for downstream tasks. Smartphones provide a convenient way for users to capture images of their body, and on-device image processing allows predicting body segmentation while protecting users privacy. Existing off-the-shelf methods for human segmentation are closed source and cannot be specialized for our application of body shape and measurement estimation. Therefore, we create a new segmentation model by simplifying Semantic FPN with PointRend, an existing accurate model. We finetune this model on a high-quality dataset of humans in a restricted set of poses relevant for our application. We obtain our final model, ALiSNet, with a size of 4MB and 97.6$\pm$1.0$\%$ mIoU, compared to Apple Person Segmentation, which has an accuracy of 94.4$\pm$5.7$\%$ mIoU on our dataset.



### Within-Camera Multilayer Perceptron DVS Denoising
- **Arxiv ID**: http://arxiv.org/abs/2304.07543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07543v1)
- **Published**: 2023-04-15 12:30:04+00:00
- **Updated**: 2023-04-15 12:30:04+00:00
- **Authors**: A. Rios-Navarro, S. Guo, G Abarajithan, K. Vijayakumar, A. Linares-Barranco, T. Aarrestad, R. Kastner, T. Delbruck
- **Comment**: Accepted to 2023 CVPRW Workshop on Event-Based Vision
- **Journal**: None
- **Summary**: In-camera event denoising reduces the data rate of event cameras by filtering out noise at the source. A lightweight multilayer perceptron denoising filter (MLPF) provides state-of-the-art low-cost denoising accuracy. It processes a small neighborhood of pixels from the timestamp image around each event to discriminate signal and noise events. This paper proposes two digital logic implementations of the MLPF denoiser and quantifies their resource cost, power, and latency. The hardware MLPF quantizes the weights and hidden unit activations to 4 bits and has about 1k weights with about 40% sparsity. The Area-Under-Curve Receiver Operating Characteristic accuracy is nearly indistinguishable from that of the floating point network. The FPGA MLPF processes each event in 10 clock cycles. In FPGA, it uses 3.5k flip flops and 11.5k LUTs. Our ASIC implementation in 65nm digital technology for a 346x260 pixel camera occupies an area of 4.3mm^2 and consumes 4nJ of energy per event at event rates up to 25MHz. The MLPF can be easily integrated into an event camera using an FPGA or as an ASIC directly on the camera chip or in the same package. This denoising could dramatically reduce the energy consumed by the communication and host processor and open new areas of always-on event camera application under scavenged and battery power. Code: https://github.com/SensorsINI/dnd_hls



### TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.07547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07547v1)
- **Published**: 2023-04-15 12:52:23+00:00
- **Updated**: 2023-04-15 12:52:23+00:00
- **Authors**: Jingyao Li, Pengguang Chen, Shengju Qian, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown great promise in pixel-level open-vocabulary learning tasks. A general paradigm utilizes CLIP's text and patch embeddings to generate semantic masks. However, existing models easily misidentify input pixels from unseen classes, thus confusing novel classes with semantically-similar ones. In our work, we disentangle the ill-posed optimization problem into two parallel processes: one performs semantic matching individually, and the other judges reliability for improving discrimination ability. Motivated by special tokens in language modeling that represents sentence-level embeddings, we design a trusty token that decouples the known and novel category prediction tendency. With almost no extra overhead, we upgrade the pixel-level generalization capacity of existing models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.



### MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2304.07549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07549v1)
- **Published**: 2023-04-15 13:03:44+00:00
- **Updated**: 2023-04-15 13:03:44+00:00
- **Authors**: Ajian Liu, Yanyan Liang
- **Comment**: 7 pages, 4 figures, conference
- **Journal**: None
- **Summary**: The existing multi-modal face anti-spoofing (FAS) frameworks are designed based on two strategies: halfway and late fusion. However, the former requires test modalities consistent with the training input, which seriously limits its deployment scenarios. And the latter is built on multiple branches to process different modalities independently, which limits their use in applications with low memory or fast execution requirements. In this work, we present a single branch based Transformer framework, namely Modality-Agnostic Vision Transformer (MA-ViT), which aims to improve the performance of arbitrary modal attacks with the help of multi-modal data. Specifically, MA-ViT adopts the early fusion to aggregate all the available training modalities data and enables flexible testing of any given modal samples. Further, we develop the Modality-Agnostic Transformer Block (MATB) in MA-ViT, which consists of two stacked attentions named Modal-Disentangle Attention (MDA) and Cross-Modal Attention (CMA), to eliminate modality-related information for each modal sequences and supplement modality-agnostic liveness features from another modal sequences, respectively. Experiments demonstrate that the single model trained based on MA-ViT can not only flexibly evaluate different modal samples, but also outperforms existing single-modal frameworks by a large margin, and approaches the multi-modal frameworks introduced with smaller FLOPs and model parameters.



### Continual Domain Adaptation through Pruning-aided Domain-specific Weight Modulation
- **Arxiv ID**: http://arxiv.org/abs/2304.07560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07560v1)
- **Published**: 2023-04-15 13:44:58+00:00
- **Updated**: 2023-04-15 13:44:58+00:00
- **Authors**: Prasanna B, Sunandini Sanyal, R. Venkatesh Babu
- **Comment**: CVPR CLVision Workshop 2023, For code see
  https://github.com/PrasannaB29/PACDA
- **Journal**: None
- **Summary**: In this paper, we propose to develop a method to address unsupervised domain adaptation (UDA) in a practical setting of continual learning (CL). The goal is to update the model on continually changing domains while preserving domain-specific knowledge to prevent catastrophic forgetting of past-seen domains. To this end, we build a framework for preserving domain-specific features utilizing the inherent model capacity via pruning. We also perform effective inference using a novel batch-norm based metric to predict the final model parameters to be used accurately. Our approach achieves not only state-of-the-art performance but also prevents catastrophic forgetting of past domains significantly. Our code is made publicly available.



### CoVLR: Coordinating Cross-Modal Consistency and Intra-Modal Structure for Vision-Language Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2304.07567v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2304.07567v2)
- **Published**: 2023-04-15 14:08:47+00:00
- **Updated**: 2023-08-18 02:32:17+00:00
- **Authors**: Yang Yang, Zhongtian Fu, Xiangyu Wu, Wenjie Li
- **Comment**: I apologize for my operational mistake, which has resulted in the
  absence of a revised version of the manuscript. Furthermore, I am concerned
  that the submission process of this paper may potentially lead to conflicts.
  Therefore, I kindly request the withdrawal of the manuscript
- **Journal**: None
- **Summary**: Current vision-language retrieval aims to perform cross-modal instance search, in which the core idea is to learn the consistent visionlanguage representations. Although the performance of cross-modal retrieval has greatly improved with the development of deep models, we unfortunately find that traditional hard consistency may destroy the original relationships among single-modal instances, leading the performance degradation for single-modal retrieval. To address this challenge, in this paper, we experimentally observe that the vision-language divergence may cause the existence of strong and weak modalities, and the hard cross-modal consistency cannot guarantee that strong modal instances' relationships are not affected by weak modality, resulting in the strong modal instances' relationships perturbed despite learned consistent representations.To this end, we propose a novel and directly Coordinated VisionLanguage Retrieval method (dubbed CoVLR), which aims to study and alleviate the desynchrony problem between the cross-modal alignment and single-modal cluster-preserving tasks. CoVLR addresses this challenge by developing an effective meta-optimization based strategy, in which the cross-modal consistency objective and the intra-modal relation preserving objective are acted as the meta-train and meta-test tasks, thereby CoVLR encourages both tasks to be optimized in a coordinated way. Consequently, we can simultaneously insure cross-modal consistency and intra-modal structure. Experiments on different datasets validate CoVLR can improve single-modal retrieval accuracy whilst preserving crossmodal retrieval capacity compared with the baselines.



### Exploring Incompatible Knowledge Transfer in Few-shot Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2304.07574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07574v1)
- **Published**: 2023-04-15 14:57:15+00:00
- **Updated**: 2023-04-15 14:57:15+00:00
- **Authors**: Yunqing Zhao, Chao Du, Milad Abdollahzadeh, Tianyu Pang, Min Lin, Shuicheng Yan, Ngai-Man Cheung
- **Comment**: 25 pages, 16 figures, 10 tables. The IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR) 2023
- **Journal**: None
- **Summary**: Few-shot image generation (FSIG) learns to generate diverse and high-fidelity images from a target domain using a few (e.g., 10) reference samples. Existing FSIG methods select, preserve and transfer prior knowledge from a source generator (pretrained on a related domain) to learn the target generator. In this work, we investigate an underexplored issue in FSIG, dubbed as incompatible knowledge transfer, which would significantly degrade the realisticness of synthetic samples. Empirical observations show that the issue stems from the least significant filters from the source generator. To this end, we propose knowledge truncation to mitigate this issue in FSIG, which is a complementary operation to knowledge preservation and is implemented by a lightweight pruning-based method. Extensive experiments show that knowledge truncation is simple and effective, consistently achieving state-of-the-art performance, including challenging setups where the source and target domains are more distant. Project Page: yunqing-me.github.io/RICK.



### Surveillance Face Presentation Attack Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/2304.07580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07580v1)
- **Published**: 2023-04-15 15:23:19+00:00
- **Updated**: 2023-04-15 15:23:19+00:00
- **Authors**: Hao Fang, Ajian Liu, Jun Wan, Sergio Escalera, Hugo Jair Escalante, Zhen Lei
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Face Anti-spoofing (FAS) is essential to secure face recognition systems from various physical attacks. However, most of the studies lacked consideration of long-distance scenarios. Specifically, compared with FAS in traditional scenes such as phone unlocking, face payment, and self-service security inspection, FAS in long-distance such as station squares, parks, and self-service supermarkets are equally important, but it has not been sufficiently explored yet. In order to fill this gap in the FAS community, we collect a large-scale Surveillance High-Fidelity Mask (SuHiFiMask). SuHiFiMask contains $10,195$ videos from $101$ subjects of different age groups, which are collected by $7$ mainstream surveillance cameras. Based on this dataset and protocol-$3$ for evaluating the robustness of the algorithm under quality changes, we organized a face presentation attack detection challenge in surveillance scenarios. It attracted 180 teams for the development phase with a total of 37 teams qualifying for the final round. The organization team re-verified and re-ran the submitted code and used the results as the final ranking. In this paper, we present an overview of the challenge, including an introduction to the dataset used, the definition of the protocol, the evaluation metrics, and the announcement of the competition results. Finally, we present the top-ranked algorithms and the research ideas provided by the competition for attack detection in long-range surveillance scenarios.



### Can SAM Segment Polyps?
- **Arxiv ID**: http://arxiv.org/abs/2304.07583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07583v1)
- **Published**: 2023-04-15 15:41:10+00:00
- **Updated**: 2023-04-15 15:41:10+00:00
- **Authors**: Tao Zhou, Yizhe Zhang, Yi Zhou, Ye Wu, Chen Gong
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Recently, Meta AI Research releases a general Segment Anything Model (SAM), which has demonstrated promising performance in several segmentation tasks. As we know, polyp segmentation is a fundamental task in the medical imaging field, which plays a critical role in the diagnosis and cure of colorectal cancer. In particular, applying SAM to the polyp segmentation task is interesting. In this report, we evaluate the performance of SAM in segmenting polyps, in which SAM is under unprompted settings. We hope this report will provide insights to advance this polyp segmentation field and promote more interesting works in the future. This project is publicly at https://github.com/taozh2017/SAMPolyp.



### FSDNet-An efficient fire detection network for complex scenarios based on YOLOv3 and DenseNet
- **Arxiv ID**: http://arxiv.org/abs/2304.07584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.07584v1)
- **Published**: 2023-04-15 15:46:08+00:00
- **Updated**: 2023-04-15 15:46:08+00:00
- **Authors**: Li Zhu, Jiahui Xiong, Wenxian Wu, Hongyu Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Fire is one of the common disasters in daily life. To achieve fast and accurate detection of fires, this paper proposes a detection network called FSDNet (Fire Smoke Detection Network), which consists of a feature extraction module, a fire classification module, and a fire detection module. Firstly, a dense connection structure is introduced in the basic feature extraction module to enhance the feature extraction ability of the backbone network and alleviate the gradient disappearance problem. Secondly, a spatial pyramid pooling structure is introduced in the fire detection module, and the Mosaic data augmentation method and CIoU loss function are used in the training process to comprehensively improve the flame feature extraction ability. Finally, in view of the shortcomings of public fire datasets, a fire dataset called MS-FS (Multi-scene Fire And Smoke) containing 11938 fire images was created through data collection, screening, and object annotation. To prove the effectiveness of the proposed method, the accuracy of the method was evaluated on two benchmark fire datasets and MS-FS. The experimental results show that the accuracy of FSDNet on the two benchmark datasets is 99.82% and 91.15%, respectively, and the average precision on MS-FS is 86.80%, which is better than the mainstream fire detection methods.



### Teacher Network Calibration Improves Cross-Quality Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2304.07593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.07593v1)
- **Published**: 2023-04-15 16:55:43+00:00
- **Updated**: 2023-04-15 16:55:43+00:00
- **Authors**: Pia Čuk, Robin Senge, Mikko Lauri, Simone Frintrop
- **Comment**: The implementation is available at:
  https://github.com/PiaCuk/distillistic
- **Journal**: None
- **Summary**: We investigate cross-quality knowledge distillation (CQKD), a knowledge distillation method where knowledge from a teacher network trained with full-resolution images is transferred to a student network that takes as input low-resolution images. As image size is a deciding factor for the computational load of computer vision applications, CQKD notably reduces the requirements by only using the student network at inference time. Our experimental results show that CQKD outperforms supervised learning in large-scale image classification problems. We also highlight the importance of calibrating neural networks: we show that with higher temperature smoothing of the teacher's output distribution, the student distribution exhibits a higher entropy, which leads to both, a lower calibration error and a higher network accuracy.



### An Instance Segmentation Dataset of Yeast Cells in Microstructures
- **Arxiv ID**: http://arxiv.org/abs/2304.07597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07597v3)
- **Published**: 2023-04-15 17:05:24+00:00
- **Updated**: 2023-08-22 16:14:53+00:00
- **Authors**: Christoph Reich, Tim Prangemeier, André O. Françani, Heinz Koeppl
- **Comment**: IEEE EMBC 2023 (in press), Christoph Reich and Tim Prangemeier - both
  authors contributed equally
- **Journal**: None
- **Summary**: Extracting single-cell information from microscopy data requires accurate instance-wise segmentations. Obtaining pixel-wise segmentations from microscopy imagery remains a challenging task, especially with the added complexity of microstructured environments. This paper presents a novel dataset for segmenting yeast cells in microstructures. We offer pixel-wise instance segmentation labels for both cells and trap microstructures. In total, we release 493 densely annotated microscopy images. To facilitate a unified comparison between novel segmentation algorithms, we propose a standardized evaluation strategy for our dataset. The aim of the dataset and evaluation strategy is to facilitate the development of new cell segmentation approaches. The dataset is publicly available at https://christophreich1996.github.io/yeast_in_microstructures_dataset/ .



### ODSmoothGrad: Generating Saliency Maps for Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2304.07609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07609v1)
- **Published**: 2023-04-15 18:21:56+00:00
- **Updated**: 2023-04-15 18:21:56+00:00
- **Authors**: Chul Gwon, Steven C. Howell
- **Comment**: To be published in XAI4CV Workshop Proceedings at CVPR 2023
- **Journal**: None
- **Summary**: Techniques for generating saliency maps continue to be used for explainability of deep learning models, with efforts primarily applied to the image classification task. Such techniques, however, can also be applied to object detectors, not only with the classification scores, but also for the bounding box parameters, which are regressed values for which the relevant pixels contributing to these parameters can be identified. In this paper, we present ODSmoothGrad, a tool for generating saliency maps for the classification and the bounding box parameters in object detectors. Given the noisiness of saliency maps, we also apply the SmoothGrad algorithm to visually enhance the pixels of interest. We demonstrate these capabilities on one-stage and two-stage object detectors, with comparisons using classifier-based techniques.



### TransDocs: Optical Character Recognition with word to word translation
- **Arxiv ID**: http://arxiv.org/abs/2304.07637v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.07637v1)
- **Published**: 2023-04-15 21:40:14+00:00
- **Updated**: 2023-04-15 21:40:14+00:00
- **Authors**: Abhishek Bamotra, Phani Krishna Uppala
- **Comment**: None
- **Journal**: None
- **Summary**: While OCR has been used in various applications, its output is not always accurate, leading to misfit words. This research work focuses on improving the optical character recognition (OCR) with ML techniques with integration of OCR with long short-term memory (LSTM) based sequence to sequence deep learning models to perform document translation. This work is based on ANKI dataset for English to Spanish translation. In this work, I have shown comparative study for pre-trained OCR while using deep learning model using LSTM-based seq2seq architecture with attention for machine translation. End-to-end performance of the model has been expressed in BLEU-4 score. This research paper is aimed at researchers and practitioners interested in OCR and its applications in document translation.



### LASER: Neuro-Symbolic Learning of Semantic Video Representations
- **Arxiv ID**: http://arxiv.org/abs/2304.07647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/2304.07647v1)
- **Published**: 2023-04-15 22:24:05+00:00
- **Updated**: 2023-04-15 22:24:05+00:00
- **Authors**: Jiani Huang, Ziyang Li, David Jacobs, Mayur Naik, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich spatial and temporal specifications: 20BN-Something-Something and MUGEN. We demonstrate that our method not only learns fine-grained video semantics but also outperforms existing baselines on downstream tasks such as video retrieval.



