# Arxiv Papers in cs.CV on 2023-04-23
### An Order-Complexity Model for Aesthetic Quality Assessment of Homophony Music Performance
- **Arxiv ID**: http://arxiv.org/abs/2304.11521v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2304.11521v1)
- **Published**: 2023-04-23 03:02:24+00:00
- **Updated**: 2023-04-23 03:02:24+00:00
- **Authors**: Xin Jin, Wu Zhou, Jinyu Wang, Duo Xu, Yiqing Rong, Jialin Sun
- **Comment**: None
- **Journal**: AIART 2023 ICME Workshop
- **Summary**: Although computational aesthetics evaluation has made certain achievements in many fields, its research of music performance remains to be explored. At present, subjective evaluation is still a ultimate method of music aesthetics research, but it will consume a lot of human and material resources. In addition, the music performance generated by AI is still mechanical, monotonous and lacking in beauty. In order to guide the generation task of AI music performance, and to improve the performance effect of human performers, this paper uses Birkhoff's aesthetic measure to propose a method of objective measurement of beauty. The main contributions of this paper are as follows: Firstly, we put forward an objective aesthetic evaluation method to measure the music performance aesthetic; Secondly, we propose 10 basic music features and 4 aesthetic music features. Experiments show that our method performs well on performance assessment.



### TransFlow: Transformer as Flow Learner
- **Arxiv ID**: http://arxiv.org/abs/2304.11523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11523v1)
- **Published**: 2023-04-23 03:11:23+00:00
- **Updated**: 2023-04-23 03:11:23+00:00
- **Authors**: Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen, Dongfang Liu
- **Comment**: 11 pages. Accepted by CVPR2023
- **Journal**: None
- **Summary**: Optical flow is an indispensable building block for various important computer vision tasks, including motion estimation, object tracking, and disparity measurement. In this work, we propose TransFlow, a pure transformer architecture for optical flow estimation. Compared to dominant CNN-based methods, TransFlow demonstrates three advantages. First, it provides more accurate correlation and trustworthy matching in flow estimation by utilizing spatial self-attention and cross-attention mechanisms between adjacent frames to effectively capture global dependencies; Second, it recovers more compromised information (e.g., occlusion and motion blur) in flow estimation through long-range temporal association in dynamic scenes; Third, it enables a concise self-learning paradigm and effectively eliminate the complex and laborious multi-stage pre-training procedures. We achieve the state-of-the-art results on the Sintel, KITTI-15, as well as several downstream tasks, including video object detection, interpolation and stabilization. For its efficacy, we hope TransFlow could serve as a flexible baseline for optical flow estimation.



### Vision Transformer for Efficient Chest X-ray and Gastrointestinal Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2304.11529v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11529v1)
- **Published**: 2023-04-23 04:07:03+00:00
- **Updated**: 2023-04-23 04:07:03+00:00
- **Authors**: Smriti Regmi, Aliza Subedi, Ulas Bagci, Debesh Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image analysis is a hot research topic because of its usefulness in different clinical applications, such as early disease diagnosis and treatment. Convolutional neural networks (CNNs) have become the de-facto standard in medical image analysis tasks because of their ability to learn complex features from the available datasets, which makes them surpass humans in many image-understanding tasks. In addition to CNNs, transformer architectures also have gained popularity for medical image analysis tasks. However, despite progress in the field, there are still potential areas for improvement. This study uses different CNNs and transformer-based methods with a wide range of data augmentation techniques. We evaluated their performance on three medical image datasets from different modalities. We evaluated and compared the performance of the vision transformer model with other state-of-the-art (SOTA) pre-trained CNN networks. For Chest X-ray, our vision transformer model achieved the highest F1 score of 0.9532, recall of 0.9533, Matthews correlation coefficient (MCC) of 0.9259, and ROC-AUC score of 0.97. Similarly, for the Kvasir dataset, we achieved an F1 score of 0.9436, recall of 0.9437, MCC of 0.9360, and ROC-AUC score of 0.97. For the Kvasir-Capsule (a large-scale VCE dataset), our ViT model achieved a weighted F1-score of 0.7156, recall of 0.7182, MCC of 0.3705, and ROC-AUC score of 0.57. We found that our transformer-based models were better or more effective than various CNN models for classifying different anatomical structures, findings, and abnormalities. Our model showed improvement over the CNN-based approaches and suggests that it could be used as a new benchmarking algorithm for algorithm development.



### Semi-Supervised Semantic Segmentation With Region Relevance
- **Arxiv ID**: http://arxiv.org/abs/2304.11539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11539v1)
- **Published**: 2023-04-23 04:51:27+00:00
- **Updated**: 2023-04-23 04:51:27+00:00
- **Authors**: Rui Chen, Tao Chen, Qiong Wang, Yazhou Yao
- **Comment**: accepted by IEEE International Conference on Multimedia and Expo 2023
- **Journal**: None
- **Summary**: Semi-supervised semantic segmentation aims to learn from a small amount of labeled data and plenty of unlabeled ones for the segmentation task. The most common approach is to generate pseudo-labels for unlabeled images to augment the training data. However, the noisy pseudo-labels will lead to cumulative classification errors and aggravate the local inconsistency in prediction. This paper proposes a Region Relevance Network (RRN) to alleviate the problem mentioned above. Specifically, we first introduce a local pseudo-label filtering module that leverages discriminator networks to assess the accuracy of the pseudo-label at the region level. A local selection loss is proposed to mitigate the negative impact of wrong pseudo-labels in consistency regularization training. In addition, we propose a dynamic region-loss correction module, which takes the merit of network diversity to further rate the reliability of pseudo-labels and correct the convergence direction of the segmentation network with a dynamic region loss. Extensive experiments are conducted on PASCAL VOC 2012 and Cityscapes datasets with varying amounts of labeled data, demonstrating that our proposed approach achieves state-of-the-art performance compared to current counterparts.



### KBody: Towards general, robust, and aligned monocular whole-body estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.11542v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11542v2)
- **Published**: 2023-04-23 05:05:02+00:00
- **Updated**: 2023-04-25 19:49:02+00:00
- **Authors**: Nikolaos Zioulis, James F. O'Brien
- **Comment**: 11 pages, 6 figures, 58 supplemental figures, project page
  https://klothed.github.io/KBody , also posted at with high-res images
  http://graphics.berkeley.edu/papers/Zioulis-KBT-2023-06
- **Journal**: In Proceedings of 1st Workshop on Reconstruction of Human-Object
  Interactions (RHOBIN) at CVPR 2023, pages 11, June 2023
- **Summary**: KBody is a method for fitting a low-dimensional body model to an image. It follows a predict-and-optimize approach, relying on data-driven model estimates for the constraints that will be used to solve for the body's parameters. Acknowledging the importance of high quality correspondences, it leverages ``virtual joints" to improve fitting performance, disentangles the optimization between the pose and shape parameters, and integrates asymmetric distance fields to strike a balance in terms of pose and shape capturing capacity, as well as pixel alignment. We also show that generative model inversion offers a strong appearance prior that can be used to complete partial human images and used as a building block for generalized and robust monocular body fitting. Project page: https://klothed.github.io/KBody.



### CANet: Curved Guide Line Network with Adaptive Decoder for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.11546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11546v1)
- **Published**: 2023-04-23 05:54:44+00:00
- **Updated**: 2023-04-23 05:54:44+00:00
- **Authors**: Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing, Runbo Hu, Pengfei Xu, Hua Chai, Ruini Xue
- **Comment**: 5 pages, IEEE ICASSP 2023
- **Journal**: None
- **Summary**: Lane detection is challenging due to the complicated on road scenarios and line deformation from different camera perspectives. Lots of solutions were proposed, but can not deal with corner lanes well. To address this problem, this paper proposes a new top-down deep learning lane detection approach, CANET. A lane instance is first responded by the heat-map on the U-shaped curved guide line at global semantic level, thus the corresponding features of each lane are aggregated at the response point. Then CANET obtains the heat-map response of the entire lane through conditional convolution, and finally decodes the point set to describe lanes via adaptive decoder. The experimental results show that CANET reaches SOTA in different metrics. Our code will be released soon.



### Spectral Sensitivity Estimation Without a Camera
- **Arxiv ID**: http://arxiv.org/abs/2304.11549v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11549v2)
- **Published**: 2023-04-23 06:18:07+00:00
- **Updated**: 2023-07-11 13:34:50+00:00
- **Authors**: Grigory Solomatov, Derya Akkaynak
- **Comment**: 11 pages, 4 figures, conference: ICCP 2023
- **Journal**: None
- **Summary**: A number of problems in computer vision and related fields would be mitigated if camera spectral sensitivities were known. As consumer cameras are not designed for high-precision visual tasks, manufacturers do not disclose spectral sensitivities. Their estimation requires a costly optical setup, which triggered researchers to come up with numerous indirect methods that aim to lower cost and complexity by using color targets. However, the use of color targets gives rise to new complications that make the estimation more difficult, and consequently, there currently exists no simple, low-cost, robust go-to method for spectral sensitivity estimation. Furthermore, even if not limited by hardware or cost, researchers frequently work with imagery from multiple cameras that they do not have in their possession. To provide a practical solution to this problem, we propose a framework for spectral sensitivity estimation that not only does not require any hardware, but also does not require physical access to the camera itself. Similar to other work, we formulate an optimization problem that minimizes a two-term objective function: a camera-specific term from a system of equations, and a universal term that bounds the solution space. Different than other work, we use publicly available high-quality calibration data to construct both terms. We use the colorimetric mapping matrices provided by the Adobe DNG Converter to formulate the camera-specific system of equations, and constrain the solutions using an autoencoder trained on a database of ground-truth curves. On average, we achieve reconstruction errors as low as those that can arise due to manufacturing imperfections between two copies of the same camera. We provide our code and predicted sensitivities for 1,000+ cameras, and discuss which tasks can become trivial when camera responses are available.



### FAN-Net: Fourier-Based Adaptive Normalization For Cross-Domain Stroke Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.11557v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11557v1)
- **Published**: 2023-04-23 06:58:21+00:00
- **Updated**: 2023-04-23 06:58:21+00:00
- **Authors**: Weiyi Yu, Yiming Lei, Hongming Shan
- **Comment**: Accepted by IEEE ICASSP 2023
- **Journal**: None
- **Summary**: Since stroke is the main cause of various cerebrovascular diseases, deep learning-based stroke lesion segmentation on magnetic resonance (MR) images has attracted considerable attention. However, the existing methods often neglect the domain shift among MR images collected from different sites, which has limited performance improvement. To address this problem, we intend to change style information without affecting high-level semantics via adaptively changing the low-frequency amplitude components of the Fourier transform so as to enhance model robustness to varying domains. Thus, we propose a novel FAN-Net, a U-Net--based segmentation network incorporated with a Fourier-based adaptive normalization (FAN) and a domain classifier with a gradient reversal layer. The FAN module is tailored for learning adaptive affine parameters for the amplitude components of different domains, which can dynamically normalize the style information of source images. Then, the domain classifier provides domain-agnostic knowledge to endow FAN with strong domain generalizability. The experimental results on the ATLAS dataset, which consists of MR images from 9 sites, show the superior performance of the proposed FAN-Net compared with baseline methods.



### Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data
- **Arxiv ID**: http://arxiv.org/abs/2304.11560v1
- **DOI**: None
- **Categories**: **cs.LG**, astro-ph.IM, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2304.11560v1)
- **Published**: 2023-04-23 07:17:45+00:00
- **Updated**: 2023-04-23 07:17:45+00:00
- **Authors**: Chakka Sai Pradeep, Neelam Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we report an autoencoder-based 2D representation to classify a time-series as stochastic or non-stochastic, to understand the underlying physical process. Content-aware conversion of 1D time-series to 2D representation, that simultaneously utilizes time- and frequency-domain characteristics, is proposed. An autoencoder is trained with a loss function to learn latent space (using both time- and frequency domains) representation, that is designed to be, time-invariant. Every element of the time-series is represented as a tuple with two components, one each, from latent space representation in time- and frequency-domains, forming a binary image. In this binary image, those tuples that represent the points in the time-series, together form the ``Latent Space Signature" (LSS) of the input time-series. The obtained binary LSS images are fed to a classification network. The EfficientNetv2-S classifier is trained using 421 synthetic time-series, with fair representation from both categories. The proposed methodology is evaluated on publicly available astronomical data which are 12 distinct temporal classes of time-series pertaining to the black hole GRS 1915 + 105, obtained from RXTE satellite. Results obtained using the proposed methodology are compared with existing techniques. Concurrence in labels obtained across the classes, illustrates the efficacy of the proposed 2D representation using the latent space co-ordinates. The proposed methodology also outputs the confidence in the classification label.



### PathRTM: Real-time prediction of KI-67 and tumor-infiltrated lymphocytes
- **Arxiv ID**: http://arxiv.org/abs/2305.00223v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.00223v1)
- **Published**: 2023-04-23 08:17:26+00:00
- **Updated**: 2023-04-23 08:17:26+00:00
- **Authors**: Steven Zvi Lapp, Eli David, Nathan S. Netanyahu
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: In this paper, we introduce PathRTM, a novel deep neural network detector based on RTMDet, for automated KI-67 proliferation and tumor-infiltrated lymphocyte estimation. KI-67 proliferation and tumor-infiltrated lymphocyte estimation play a crucial role in cancer diagnosis and treatment. PathRTM is an extension of the PathoNet work, which uses single pixel keypoints for within each cell. We demonstrate that PathRTM, with higher-level supervision in the form of bounding box labels generated automatically from the keypoints using NuClick, can significantly improve KI-67 proliferation and tumorinfiltrated lymphocyte estimation. Experiments on our custom dataset show that PathRTM achieves state-of-the-art performance in KI-67 immunopositive, immunonegative, and lymphocyte detection, with an average precision (AP) of 41.3%. Our results suggest that PathRTM is a promising approach for accurate KI-67 proliferation and tumor-infiltrated lymphocyte estimation, offering annotation efficiency, accurate predictive capabilities, and improved runtime. The method also enables estimation of cell sizes of interest, which was previously unavailable, through the bounding box predictions.



### StyLess: Boosting the Transferability of Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2304.11579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11579v1)
- **Published**: 2023-04-23 08:23:48+00:00
- **Updated**: 2023-04-23 08:23:48+00:00
- **Authors**: Kaisheng Liang, Bin Xiao
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Adversarial attacks can mislead deep neural networks (DNNs) by adding imperceptible perturbations to benign examples. The attack transferability enables adversarial examples to attack black-box DNNs with unknown architectures or parameters, which poses threats to many real-world applications. We find that existing transferable attacks do not distinguish between style and content features during optimization, limiting their attack transferability. To improve attack transferability, we propose a novel attack method called style-less perturbation (StyLess). Specifically, instead of using a vanilla network as the surrogate model, we advocate using stylized networks, which encode different style features by perturbing an adaptive instance normalization. Our method can prevent adversarial examples from using non-robust style features and help generate transferable perturbations. Comprehensive experiments show that our method can significantly improve the transferability of adversarial examples. Furthermore, our approach is generic and can outperform state-of-the-art transferable attacks when combined with other attack techniques.



### A Framework for Benchmarking Real-Time Embedded Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.11580v1
- **DOI**: 10.1007/978-3-031-16788-1_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11580v1)
- **Published**: 2023-04-23 08:34:05+00:00
- **Updated**: 2023-04-23 08:34:05+00:00
- **Authors**: Michael Schlosser, Daniel König, Michael Teutsch
- **Comment**: Proceedings of the DAGM German Conference on Pattern Recognition
  (GCPR) 2022
- **Journal**: In: Andres, B., Bernard, F., Cremers, D., Frintrop, S.,
  Goldl\"ucke, B., Ihrke, I. (eds) Pattern Recognition. DAGM GCPR 2022. Lecture
  Notes in Computer Science, vol 13485. Springer, Cham
- **Summary**: Object detection is one of the key tasks in many applications of computer vision. Deep Neural Networks (DNNs) are undoubtedly a well-suited approach for object detection. However, such DNNs need highly adapted hardware together with hardware-specific optimization to guarantee high efficiency during inference. This is especially the case when aiming for efficient object detection in video streaming applications on limited hardware such as edge devices. Comparing vendor-specific hardware and related optimization software pipelines in a fair experimental setup is a challenge. In this paper, we propose a framework that uses a host computer with a host software application together with a light-weight interface based on the Message Queuing Telemetry Transport (MQTT) protocol. Various different target devices with target apps can be connected via MQTT with this host computer. With well-defined and standardized MQTT messages, object detection results can be reported to the host computer, where the results are evaluated without harming or influencing the processing on the device. With this quite generic framework, we can measure the object detection performance, the runtime, and the energy efficiency at the same time. The effectiveness of this framework is demonstrated in multiple experiments that offer deep insights into the optimization of DNNs.



### UHRNet: A Deep Learning-Based Method for Accurate 3D Reconstruction from a Single Fringe-Pattern
- **Arxiv ID**: http://arxiv.org/abs/2304.14503v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.ins-det, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2304.14503v1)
- **Published**: 2023-04-23 08:39:05+00:00
- **Updated**: 2023-04-23 08:39:05+00:00
- **Authors**: Yixiao Wang, Canlin Zhou, Xingyang Qi, Hui Li
- **Comment**: None
- **Journal**: None
- **Summary**: The quick and accurate retrieval of an object height from a single fringe pattern in Fringe Projection Profilometry has been a topic of ongoing research. While a single shot fringe to depth CNN based method can restore height map directly from a single pattern, its accuracy is currently inferior to the traditional phase shifting technique. To improve this method's accuracy, we propose using a U shaped High resolution Network (UHRNet). The network uses UNet encoding and decoding structure as backbone, with Multi-Level convolution Block and High resolution Fusion Block applied to extract local features and global features. We also designed a compound loss function by combining Structural Similarity Index Measure Loss (SSIMLoss) function and chunked L2 loss function to improve 3D reconstruction details.We conducted several experiments to demonstrate the validity and robustness of our proposed method. A few experiments have been conducted to demonstrate the validity and robustness of the proposed method, The average RMSE of 3D reconstruction by our method is only 0.443(mm). which is 41.13% of the UNet method and 33.31% of Wang et al hNet method. Our experimental results show that our proposed method can increase the accuracy of 3D reconstruction from a single fringe pattern.



### OSP2B: One-Stage Point-to-Box Network for 3D Siamese Tracking
- **Arxiv ID**: http://arxiv.org/abs/2304.11584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11584v2)
- **Published**: 2023-04-23 08:52:36+00:00
- **Updated**: 2023-05-09 02:27:49+00:00
- **Authors**: Jiahao Nie, Zhiwei He, Yuxiang Yang, Zhengyi Bao, Mingyu Gao, Jing Zhang
- **Comment**: Accepted to IJCAI'23. Code will be available at
  https://github.com/haooozi/OSP2B
- **Journal**: None
- **Summary**: Two-stage point-to-box network acts as a critical role in the recent popular 3D Siamese tracking paradigm, which first generates proposals and then predicts corresponding proposal-wise scores. However, such a network suffers from tedious hyper-parameter tuning and task misalignment, limiting the tracking performance. Towards these concerns, we propose a simple yet effective one-stage point-to-box network for point cloud-based 3D single object tracking. It synchronizes 3D proposal generation and center-ness score prediction by a parallel predictor without tedious hyper-parameters. To guide a task-aligned score ranking of proposals, a center-aware focal loss is proposed to supervise the training of the center-ness branch, which enhances the network's discriminative ability to distinguish proposals of different quality. Besides, we design a binary target classifier to identify target-relevant points. By integrating the derived classification scores with the center-ness scores, the resulting network can effectively suppress interference proposals and further mitigate task misalignment. Finally, we present a novel one-stage Siamese tracker OSP2B equipped with the designed network. Extensive experiments on challenging benchmarks including KITTI and Waymo SOT Dataset show that our OSP2B achieves leading performance with a considerable real-time speed.Code will be available at https://github.com/haooozi/OSP2B.



### Broken Rail Detection With Texture Image Processing Using Two-Dimensional Gray Level Co-occurrence Matrix
- **Arxiv ID**: http://arxiv.org/abs/2304.11592v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2304.11592v1)
- **Published**: 2023-04-23 09:44:32+00:00
- **Updated**: 2023-04-23 09:44:32+00:00
- **Authors**: Mohsen Ebrahimi
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Application of electronic railway systems as well as the implication of Automatic Train Control (ATC) System has increased the safety of rail transportation. However, one of the most important causes of accidents on the railway is rail damage and breakage. In this paper, we have proposed a method that the rail region is first recognized from the observation area, then by investigating the image texture processing data, the types of rail defects including cracks, wear, peeling, disintegration, and breakage are detected. In order to reduce the computational cost, the image is changed from the RGB color spectrum to the gray spectrum. Image texture processing data is obtained by the two-dimensional Gray Levels Co-occurrence Matrix (GLCM) at different angles; this data demonstrates second-order features of the images. Large data of features has a negative effect on the overall accuracy of the classifiers. To tackle this issue and acquire faster response, Principal Component Analysis (PCA) algorithm is used, before entering the band into the classifier. Then the features extracted from the images are compared by three different classifiers including Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbor (KNN) classification. The results obtained from this method indicate that the Random Forest classifier has better performance (accuracy 97%, precision 96%, and recall 96%) than other classifiers.



### Segment Anything in Non-Euclidean Domains: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/2304.11595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.11595v1)
- **Published**: 2023-04-23 10:01:34+00:00
- **Updated**: 2023-04-23 10:01:34+00:00
- **Authors**: Yongcheng Jing, Xinchao Wang, Dacheng Tao
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: The recent work known as Segment Anything (SA) has made significant strides in pushing the boundaries of semantic segmentation into the era of foundation models. The impact of SA has sparked extremely active discussions and ushered in an encouraging new wave of developing foundation models for the diverse tasks in the Euclidean domain, such as object detection and image inpainting. Despite the promising advances led by SA, the concept has yet to be extended to the non-Euclidean graph domain. In this paper, we explore a novel Segment Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models that can handle the diverse range of graph data within the non-Euclidean domain, seeking to expand the scope of SA and lay the groundwork for future research in this direction. To achieve this goal, we begin by discussing the recent achievements in foundation models associated with SA. We then shed light on the unique challenges that arise when applying the SA concept to graph analysis, which involves understanding the differences between the Euclidean and non-Euclidean domains from both the data and task perspectives. Motivated by these observations, we present several preliminary solutions to tackle the challenges of SNA and detail their corresponding limitations, along with several potential directions to pave the way for future SNA research. Experiments on five Open Graph Benchmark (OGB) datasets across various tasks, including graph property classification and regression, as well as multi-label prediction, demonstrate that the performance of the naive SNA solutions has considerable room for improvement, pointing towards a promising avenue for future exploration of Graph General Intelligence.



### Learning Partial Correlation based Deep Visual Representation for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2304.11597v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.11597v2)
- **Published**: 2023-04-23 10:09:01+00:00
- **Updated**: 2023-04-26 07:00:11+00:00
- **Authors**: Saimunur Rahman, Piotr Koniusz, Lei Wang, Luping Zhou, Peyman Moghadam, Changming Sun
- **Comment**: This paper is published at CVPR 2023
- **Journal**: None
- **Summary**: Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representation and mitigates the small sample problem often encountered by covariance matrix estimation in CNN. Computationally, our model can be effectively trained with GPU and works well with a large number of channels of advanced CNNs. Experiments show the efficacy and superior classification performance of our deep visual representation compared to covariance matrix based counterparts.



### Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement
- **Arxiv ID**: http://arxiv.org/abs/2304.11598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.11598v1)
- **Published**: 2023-04-23 10:09:26+00:00
- **Updated**: 2023-04-23 10:09:26+00:00
- **Authors**: Hao Zhu, Piotr Koniusz
- **Comment**: This paper is published at CVPR 2023
- **Journal**: None
- **Summary**: Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two existing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes. We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the proposed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some unlabeled data accompanies the novel few-shot task.



### LaMD: Latent Motion Diffusion for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2304.11603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11603v1)
- **Published**: 2023-04-23 10:32:32+00:00
- **Updated**: 2023-04-23 10:32:32+00:00
- **Authors**: Yaosi Hu, Zhenzhong Chen, Chong Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Generating coherent and natural movement is the key challenge in video generation. This research proposes to condense video generation into a problem of motion generation, to improve the expressiveness of motion and make video generation more manageable. This can be achieved by breaking down the video generation process into latent motion generation and video reconstruction. We present a latent motion diffusion (LaMD) framework, which consists of a motion-decomposed video autoencoder and a diffusion-based motion generator, to implement this idea. Through careful design, the motion-decomposed video autoencoder can compress patterns in movement into a concise latent motion representation. Meanwhile, the diffusion-based motion generator is able to efficiently generate realistic motion on a continuous latent space under multi-modal conditions, at a cost that is similar to that of image diffusion models. Results show that LaMD generates high-quality videos with a wide range of motions, from stochastic dynamics to highly controllable movements. It achieves new state-of-the-art performance on benchmark datasets, including BAIR, Landscape and CATER-GENs, for Image-to-Video (I2V) and Text-Image-to-Video (TI2V) generation. The source code of LaMD will be made available soon.



### PiClick: Picking the desired mask in click-based interactive segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.11609v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11609v3)
- **Published**: 2023-04-23 10:46:16+00:00
- **Updated**: 2023-08-28 13:26:52+00:00
- **Authors**: Cilin Yan, Haochen Wang, Jie Liu, Xiaolong Jiang, Yao Hu, Xu Tang, Guoliang Kang, Efstratios Gavves
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Click-based interactive segmentation aims to generate target masks via human clicking, which facilitates efficient pixel-level annotation and image editing. In such a task, target ambiguity remains a problem hindering the accuracy and efficiency of segmentation. That is, in scenes with rich context, one click may correspond to multiple potential targets, while most previous interactive segmentors only generate a single mask and fail to deal with target ambiguity. In this paper, we propose a novel interactive segmentation network named PiClick, to yield all potentially reasonable masks and suggest the most plausible one for the user. Specifically, PiClick utilizes a Transformer-based architecture to generate all potential target masks by mutually interactive mask queries. Moreover, a Target Reasoning module is designed in PiClick to automatically suggest the user-desired mask from all candidates, relieving target ambiguity and extra-human efforts. Extensive experiments on 9 interactive segmentation datasets demonstrate PiClick performs favorably against previous state-of-the-arts considering the segmentation results. Moreover, we show that PiClick effectively reduces human efforts in annotating and picking the desired masks. To ease the usage and inspire future research, we release the source code of PiClick together with a plug-and-play annotation tool at https://github.com/cilinyan/PiClick.



### SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2304.11619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.11619v1)
- **Published**: 2023-04-23 11:23:05+00:00
- **Updated**: 2023-04-23 11:23:05+00:00
- **Authors**: Jonathan Roberts, Kai Han, Samuel Albanie
- **Comment**: None
- **Journal**: None
- **Summary**: Interpreting remote sensing imagery enables numerous downstream applications ranging from land-use planning to deforestation monitoring. Robustly classifying this data is challenging due to the Earth's geographic diversity. While many distinct satellite and aerial image classification datasets exist, there is yet to be a benchmark curated that suitably covers this diversity. In this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluate the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN. We find SATIN to be a challenging benchmark-the strongest method we evaluate achieves a classification accuracy of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public leaderboard}}$ to guide and track the progress of VL models in this important domain.



### TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential
- **Arxiv ID**: http://arxiv.org/abs/2304.11631v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.11631v1)
- **Published**: 2023-04-23 12:10:36+00:00
- **Updated**: 2023-04-23 12:10:36+00:00
- **Authors**: Dongjingdin Liu, Pengpeng Chen, Miao Yao, Yijing Lu, Zijie Cai, Yuxin Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition has achieved remarkable results in human action recognition with the development of graph convolutional networks (GCNs). However, the recent works tend to construct complex learning mechanisms with redundant training and exist a bottleneck for long time-series. To solve these problems, we propose the Temporal-Spatio Graph ConvNeXt (TSGCNeXt) to explore efficient learning mechanism of long temporal skeleton sequences. Firstly, a new graph learning mechanism with simple structure, Dynamic-Static Separate Multi-graph Convolution (DS-SMG) is proposed to aggregate features of multiple independent topological graphs and avoid the node information being ignored during dynamic convolution. Next, we construct a graph convolution training acceleration mechanism to optimize the back-propagation computing of dynamic graph learning with 55.08\% speed-up. Finally, the TSGCNeXt restructure the overall structure of GCN with three Spatio-temporal learning modules,efficiently modeling long temporal features. In comparison with existing previous methods on large-scale datasets NTU RGB+D 60 and 120, TSGCNeXt outperforms on single-stream networks. In addition, with the ema model introduced into the multi-stream fusion, TSGCNeXt achieves SOTA levels. On the cross-subject and cross-set of the NTU 120, accuracies reach 90.22% and 91.74%.



### An Efficient Ensemble Explainable AI (XAI) Approach for Morphed Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.14509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.14509v1)
- **Published**: 2023-04-23 13:43:06+00:00
- **Updated**: 2023-04-23 13:43:06+00:00
- **Authors**: Rudresh Dwivedi, Ritesh Kumar, Deepak Chopra, Pranay Kothari, Manjot Singh
- **Comment**: None
- **Journal**: None
- **Summary**: The extensive utilization of biometric authentication systems have emanated attackers / imposters to forge user identity based on morphed images. In this attack, a synthetic image is produced and merged with genuine. Next, the resultant image is user for authentication. Numerous deep neural convolutional architectures have been proposed in literature for face Morphing Attack Detection (MADs) to prevent such attacks and lessen the risks associated with them. Although, deep learning models achieved optimal results in terms of performance, it is difficult to understand and analyse these networks since they are black box/opaque in nature. As a consequence, incorrect judgments may be made. There is, however, a dearth of literature that explains decision-making methods of black box deep learning models for biometric Presentation Attack Detection (PADs) or MADs that can aid the biometric community to have trust in deep learning-based biometric systems for identification and authentication in various security applications such as border control, criminal database establishment etc. In this work, we present a novel visual explanation approach named Ensemble XAI integrating Saliency maps, Class Activation Maps (CAM) and Gradient-CAM (Grad-CAM) to provide a more comprehensive visual explanation for a deep learning prognostic model (EfficientNet-B1) that we have employed to predict whether the input presented to a biometric authentication system is morphed or genuine. The experimentations have been performed on three publicly available datasets namely Face Research Lab London Set, Wide Multi-Channel Presentation Attack (WMCA), and Makeup Induced Face Spoofing (MIFS). The experimental evaluations affirms that the resultant visual explanations highlight more fine-grained details of image features/areas focused by EfficientNet-B1 to reach decisions along with appropriate reasoning.



### AirBirds: A Large-scale Challenging Dataset for Bird Strike Prevention in Real-world Airports
- **Arxiv ID**: http://arxiv.org/abs/2304.11662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11662v1)
- **Published**: 2023-04-23 14:19:28+00:00
- **Updated**: 2023-04-23 14:19:28+00:00
- **Authors**: Hongyu Sun, Yongcai Wang, Xudong Cai, Peng Wang, Zhe Huang, Deying Li, Yu Shao, Shuo Wang
- **Comment**: 17 pages, 9 figures, 3 tables; accepted by ACCV 2022
- **Journal**: None
- **Summary**: One fundamental limitation to the research of bird strike prevention is the lack of a large-scale dataset taken directly from real-world airports. Existing relevant datasets are either small in size or not dedicated for this purpose. To advance the research and practical solutions for bird strike prevention, in this paper, we present a large-scale challenging dataset AirBirds that consists of 118,312 time-series images, where a total of 409,967 bounding boxes of flying birds are manually, carefully annotated. The average size of all annotated instances is smaller than 10 pixels in 1920x1080 images. Images in the dataset are captured over 4 seasons of a whole year by a network of cameras deployed at a real-world airport, covering diverse bird species, lighting conditions and 13 meteorological scenarios. To the best of our knowledge, it is the first large-scale image dataset that directly collects flying birds in real-world airports for bird strike prevention. This dataset is publicly available at https://airbirdsdata.github.io/.



### CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2304.11668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.11668v1)
- **Published**: 2023-04-23 14:33:24+00:00
- **Updated**: 2023-04-23 14:33:24+00:00
- **Authors**: Youzhe Song, Feng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The discriminability of feature representation is the key to open-set face recognition. Previous methods rely on the learnable weights of the classification layer that represent the identities. However, the evaluation process learns no identity representation and drops the classifier from training. This inconsistency could confuse the feature encoder in understanding the evaluation goal and hinder the effect of identity-based methods. To alleviate the above problem, we propose a novel approach namely Contrastive Regularization for Face recognition (CoReFace) to apply image-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to regularize the training with the image-image relationship directly, which is consistent with the evaluation process. To integrate contrastive learning into face recognition, we augment embeddings instead of images to avoid the image quality degradation. Then, we propose a novel contrastive loss for the representation distribution by incorporating an adaptive margin and a supervised contrastive mask to generate steady loss values and avoid the collision with the classification supervision signal. Finally, we discover and solve the semantically repetitive signal problem in contrastive learning by exploring new pair coupling protocols. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace which is highly competitive with the state-of-the-art approaches.



### Evading DeepFake Detectors via Adversarial Statistical Consistency
- **Arxiv ID**: http://arxiv.org/abs/2304.11670v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11670v1)
- **Published**: 2023-04-23 14:40:42+00:00
- **Updated**: 2023-04-23 14:40:42+00:00
- **Authors**: Yang Hou, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma, Jianjun Zhao
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: In recent years, as various realistic face forgery techniques known as DeepFake improves by leaps and bounds,more and more DeepFake detection techniques have been proposed. These methods typically rely on detecting statistical differences between natural (i.e., real) and DeepFakegenerated images in both spatial and frequency domains. In this work, we propose to explicitly minimize the statistical differences to evade state-of-the-art DeepFake detectors. To this end, we propose a statistical consistency attack (StatAttack) against DeepFake detectors, which contains two main parts. First, we select several statistical-sensitive natural degradations (i.e., exposure, blur, and noise) and add them to the fake images in an adversarial way. Second, we find that the statistical differences between natural and DeepFake images are positively associated with the distribution shifting between the two kinds of images, and we propose to use a distribution-aware loss to guide the optimization of different degradations. As a result, the feature distributions of generated adversarial examples is close to the natural images.Furthermore, we extend the StatAttack to a more powerful version, MStatAttack, where we extend the single-layer degradation to multi-layer degradations sequentially and use the loss to tune the combination weights jointly. Comprehensive experimental results on four spatial-based detectors and two frequency-based detectors with four datasets demonstrate the effectiveness of our proposed attack method in both white-box and black-box settings.



### A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2304.11674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2304.11674v1)
- **Published**: 2023-04-23 14:54:15+00:00
- **Updated**: 2023-04-23 14:54:15+00:00
- **Authors**: Yu Zhou, Yu Chen, Xiao Zhang, Pan Lai, Lei Huang, Jianmin Jiang
- **Comment**: has been accepted to IEEE TETCI
- **Journal**: None
- **Summary**: Recently, deep learning-based compressed sensing (CS) has achieved great success in reducing the sampling and computational cost of sensing systems and improving the reconstruction quality. These approaches, however, largely overlook the issue of the computational cost; they rely on complex structures and task-specific operator designs, resulting in extensive storage and high energy consumption in CS imaging systems. In this paper, we propose a lightweight but effective deep neural network based on recurrent learning to achieve a sustainable CS system; it requires a smaller number of parameters but obtains high-quality reconstructions. Specifically, our proposed network consists of an initial reconstruction sub-network and a residual reconstruction sub-network. While the initial reconstruction sub-network has a hierarchical structure to progressively recover the image, reducing the number of parameters, the residual reconstruction sub-network facilitates recurrent residual feature extraction via recurrent learning to perform both feature fusion and deep reconstructions across different scales. In addition, we also demonstrate that, after the initial reconstruction, feature maps with reduced sizes are sufficient to recover the residual information, and thus we achieved a significant reduction in the amount of memory required. Extensive experiments illustrate that our proposed model can achieve a better reconstruction quality than existing state-of-the-art CS algorithms, and it also has a smaller number of network parameters than these algorithms. Our source codes are available at: https://github.com/C66YU/CSRN.



### Indiscernible Object Counting in Underwater Scenes
- **Arxiv ID**: http://arxiv.org/abs/2304.11677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11677v1)
- **Published**: 2023-04-23 15:09:02+00:00
- **Updated**: 2023-04-23 15:09:02+00:00
- **Authors**: Guolei Sun, Zhaochong An, Yun Liu, Ce Liu, Christos Sakaridis, Deng-Ping Fan, Luc Van Gool
- **Comment**: To appear in CVPR 2023. The resources are available at
  https://github.com/GuoleiSun/Indiscernible-Object-Counting
- **Journal**: None
- **Summary**: Recently, indiscernible scene understanding has attracted a lot of attention in the vision community. We further advance the frontier of this field by systematically studying a new challenge named indiscernible object counting (IOC), the goal of which is to count objects that are blended with respect to their surroundings. Due to a lack of appropriate IOC datasets, we present a large-scale dataset IOCfish5K which contains a total of 5,637 high-resolution images and 659,024 annotated center points. Our dataset consists of a large number of indiscernible objects (mainly fish) in underwater scenes, making the annotation process all the more challenging. IOCfish5K is superior to existing datasets with indiscernible scenes because of its larger scale, higher image resolutions, more annotations, and denser scenes. All these aspects make it the most challenging dataset for IOC so far, supporting progress in this area. For benchmarking purposes, we select 14 mainstream methods for object counting and carefully evaluate them on IOCfish5K. Furthermore, we propose IOCFormer, a new strong baseline that combines density and regression branches in a unified framework and can effectively tackle object counting under concealed scenes. Experiments show that IOCFormer achieves state-of-the-art scores on IOCfish5K.



### Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2304.11685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11685v1)
- **Published**: 2023-04-23 15:29:26+00:00
- **Updated**: 2023-04-23 15:29:26+00:00
- **Authors**: Magnus Falkenberg, Anders Bensen Ottsen, Mathias Ibsen, Christian Rathgeb
- **Comment**: None
- **Journal**: None
- **Summary**: We address the need for a large-scale database of children's faces by using generative adversarial networks (GANs) and face age progression (FAP) models to synthesize a realistic dataset referred to as HDA-SynChildFaces. To this end, we proposed a processing pipeline that initially utilizes StyleGAN3 to sample adult subjects, which are subsequently progressed to children of varying ages using InterFaceGAN. Intra-subject variations, such as facial expression and pose, are created by further manipulating the subjects in their latent space. Additionally, the presented pipeline allows to evenly distribute the races of subjects, allowing to generate a balanced and fair dataset with respect to race distribution. The created HDA-SynChildFaces consists of 1,652 subjects and a total of 188,832 images, each subject being present at various ages and with many different intra-subject variations. Subsequently, we evaluates the performance of various facial recognition systems on the generated database and compare the results of adults and children at different ages. The study reveals that children consistently perform worse than adults, on all tested systems, and the degradation in performance is proportional to age. Additionally, our study uncovers some biases in the recognition systems, with Asian and Black subjects and females performing worse than White and Latino Hispanic subjects and males.



### Informative Data Selection with Uncertainty for Multi-modal Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.11697v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11697v1)
- **Published**: 2023-04-23 16:36:13+00:00
- **Updated**: 2023-04-23 16:36:13+00:00
- **Authors**: Xinyu Zhang, Zhiwei Li, Zhenhong Zou, Xin Gao, Yijin Xiong, Dafeng Jin, Jun Li, Huaping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Noise has always been nonnegligible trouble in object detection by creating confusion in model reasoning, thereby reducing the informativeness of the data. It can lead to inaccurate recognition due to the shift in the observed pattern, that requires a robust generalization of the models. To implement a general vision model, we need to develop deep learning models that can adaptively select valid information from multi-modal data. This is mainly based on two reasons. Multi-modal learning can break through the inherent defects of single-modal data, and adaptive information selection can reduce chaos in multi-modal data. To tackle this problem, we propose a universal uncertainty-aware multi-modal fusion model. It adopts a multi-pipeline loosely coupled architecture to combine the features and results from point clouds and images. To quantify the correlation in multi-modal information, we model the uncertainty, as the inverse of data information, in different modalities and embed it in the bounding box generation. In this way, our model reduces the randomness in fusion and generates reliable output. Moreover, we conducted a completed investigation on the KITTI 2D object detection dataset and its derived dirty data. Our fusion model is proven to resist severe noise interference like Gaussian, motion blur, and frost, with only slight degradation. The experiment results demonstrate the benefits of our adaptive fusion. Our analysis on the robustness of multi-modal fusion will provide further insights for future research.



### HKNAS: Classification of Hyperspectral Imagery Based on Hyper Kernel Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2304.11701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11701v1)
- **Published**: 2023-04-23 17:27:40+00:00
- **Updated**: 2023-04-23 17:27:40+00:00
- **Authors**: Di Wang, Bo Du, Liangpei Zhang, Dacheng Tao
- **Comment**: Accepted by IEEE TNNLS. The code will be released at
  https://github.com/DotWang/HKNAS
- **Journal**: None
- **Summary**: Recent neural architecture search (NAS) based approaches have made great progress in hyperspectral image (HSI) classification tasks. However, the architectures are usually optimized independently of the network weights, increasing searching time and restricting model performances. To tackle these issues, in this paper, different from previous methods that extra define structural parameters, we propose to directly generate structural parameters by utilizing the specifically designed hyper kernels, ingeniously converting the original complex dual optimization problem into easily implemented one-tier optimizations, and greatly shrinking searching costs. Then, we develop a hierarchical multi-module search space whose candidate operations only contain convolutions, and these operations can be integrated into unified kernels. Using the above searching strategy and searching space, we obtain three kinds of networks to separately conduct pixel-level or image-level classifications with 1-D or 3-D convolutions. In addition, by combining the proposed hyper kernel searching scheme with the 3-D convolution decomposition mechanism, we obtain diverse architectures to simulate 3-D convolutions, greatly improving network flexibilities. A series of quantitative and qualitative experiments on six public datasets demonstrate that the proposed methods achieve state-of-the-art results compared with other advanced NAS-based HSI classification approaches.



### Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.11705v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.11705v2)
- **Published**: 2023-04-23 17:43:29+00:00
- **Updated**: 2023-08-29 10:08:24+00:00
- **Authors**: Cristiano Saltori, Aljoša Ošep, Elisa Ricci, Laura Leal-Taixé
- **Comment**: Accepted at ICCV 2023
- **Journal**: None
- **Summary**: The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional decoder that learns to classify a birds-eye view of the point cloud. This simple auxiliary task encourages the 3D network to learn features that are robust to sensor placement shifts and resolution, and are transferable across domains. With this work, we aim to inspire the community to develop and evaluate future models in such cross-domain conditions.



### Deep Convolutional Tables: Deep Learning without Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2304.11706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11706v1)
- **Published**: 2023-04-23 17:49:21+00:00
- **Updated**: 2023-04-23 17:49:21+00:00
- **Authors**: Shay Dekel, Yosi Keller, Aharon Bar-Hillel
- **Comment**: Accepted for publication. IEEE Transactions on Neural Networks and
  Learning Systems
- **Journal**: None
- **Summary**: We propose a novel formulation of deep networks that do not use dot-product neurons and rely on a hierarchy of voting tables instead, denoted as Convolutional Tables (CT), to enable accelerated CPU-based inference. Convolutional layers are the most time-consuming bottleneck in contemporary deep learning techniques, severely limiting their use in Internet of Things and CPU-based devices. The proposed CT performs a fern operation at each image location: it encodes the location environment into a binary index and uses the index to retrieve the desired local output from a table. The results of multiple tables are combined to derive the final output. The computational complexity of a CT transformation is independent of the patch (filter) size and grows gracefully with the number of channels, outperforming comparable convolutional layers. It is shown to have a better capacity:compute ratio than dot-product neurons, and that deep CT networks exhibit a universal approximation property similar to neural networks. As the transformation involves computing discrete indices, we derive a soft relaxation and gradient-based approach for training the CT hierarchy. Deep CT networks have been experimentally shown to have accuracy comparable to that of CNNs of similar architectures. In the low compute regime, they enable an error:speed trade-off superior to alternative efficient CNN architectures.



### Automatized marine vessel monitoring from sentinel-1 data using convolution neural network
- **Arxiv ID**: http://arxiv.org/abs/2304.11717v1
- **DOI**: 10.1109/IGARSS47720.2021.9555149
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11717v1)
- **Published**: 2023-04-23 18:09:44+00:00
- **Updated**: 2023-04-23 18:09:44+00:00
- **Authors**: Surya Prakash Tiwari, Sudhir Kumar Chaturvedi, Subhrangshu Adhikary, Saikat Banerjee, Sourav Basu
- **Comment**: 2021 IEEE International Geoscience and Remote Sensing Symposium
  IGARSS
- **Journal**: 2021 IEEE International Geoscience and Remote Sensing Symposium
  IGARSS, Brussels, Belgium, 2021, pp. 1311-1314
- **Summary**: The advancement of multi-channel synthetic aperture radar (SAR) system is considered as an upgraded technology for surveillance activities. SAR sensors onboard provide data for coastal ocean surveillance and a view of the oceanic surface features. Vessel monitoring has earlier been performed using Constant False Alarm Rate (CFAR) algorithm which is not a smart technique as it lacks decision-making capabilities, therefore we introduce wavelet transformation-based Convolution Neural Network approach to recognize objects from SAR images during the heavy naval traffic, which corresponds to the numerous object detection. The utilized information comprises Sentinel-1 SAR-C dual-polarization data acquisitions over the western coastal zones of India and with help of the proposed technique we have obtained 95.46% detection accuracy. Utilizing this model can automatize the monitoring of naval objects and recognition of foreign maritime intruders.



### No Free Lunch in Self Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.11718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.5.1, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2304.11718v1)
- **Published**: 2023-04-23 18:14:19+00:00
- **Updated**: 2023-04-23 18:14:19+00:00
- **Authors**: Ihab Bendidi, Adrien Bardes, Ethan Cohen, Alexis Lamiable, Guillaume Bollot, Auguste Genovesio
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised representation learning in computer vision relies heavily on hand-crafted image transformations to learn meaningful and invariant features. However few extensive explorations of the impact of transformation design have been conducted in the literature. In particular, the dependence of downstream performances to transformation design has been established, but not studied in depth. In this work, we explore this relationship, its impact on a domain other than natural images, and show that designing the transformations can be viewed as a form of supervision. First, we demonstrate that not only do transformations have an effect on downstream performance and relevance of clustering, but also that each category in a supervised dataset can be impacted in a different way. Following this, we explore the impact of transformation design on microscopy images, a domain where the difference between classes is more subtle and fuzzy than in natural images. In this case, we observe a greater impact on downstream tasks performances. Finally, we demonstrate that transformation design can be leveraged as a form of supervision, as careful selection of these by a domain expert can lead to a drastic increase in performance on a given downstream task.



### Urban GeoBIM construction by integrating semantic LiDAR point clouds with as-designed BIM models
- **Arxiv ID**: http://arxiv.org/abs/2304.11719v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11719v2)
- **Published**: 2023-04-23 18:16:14+00:00
- **Updated**: 2023-05-22 05:21:06+00:00
- **Authors**: Jie Shao, Wei Yao, Puzuo Wang, Zhiyi He, Lei Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Developments in three-dimensional real worlds promote the integration of geoinformation and building information models (BIM) known as GeoBIM in urban construction. Light detection and ranging (LiDAR) integrated with global navigation satellite systems can provide geo-referenced spatial information. However, constructing detailed urban GeoBIM poses challenges in terms of LiDAR data quality. BIM models designed from software are rich in geometrical information but often lack accurate geo-referenced locations. In this paper, we propose a complementary strategy that integrates LiDAR point clouds with as-designed BIM models for reconstructing urban scenes. A state-of-the-art deep learning framework and graph theory are first combined for LiDAR point cloud segmentation. A coarse-to-fine matching program is then developed to integrate object point clouds with corresponding BIM models. Results show the overall segmentation accuracy of LiDAR datasets reaches up to 90%, and average positioning accuracies of BIM models are 0.023 m for pole-like objects and 0.156 m for buildings, demonstrating the effectiveness of the method in segmentation and matching processes. This work offers a practical solution for rapid and accurate urban GeoBIM construction.



### Analysis of Recent Trends in Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2304.11725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11725v1)
- **Published**: 2023-04-23 18:55:45+00:00
- **Updated**: 2023-04-23 18:55:45+00:00
- **Authors**: Krishnendu K. S
- **Comment**: None
- **Journal**: None
- **Summary**: With the tremendous advancements in face recognition technology, face modality has been widely recognized as a significant biometric identifier in establishing a person's identity rather than any other biometric trait like fingerprints that require contact sensors. However, due to inter-class similarities and intra-class variations, face recognition systems generate false match and false non-match errors respectively. Recent research focuses on improving the robustness of extracted features and the pre-processing algorithms to enhance recognition accuracy. Since face recognition has been extensively used for several applications ranging from law enforcement to surveillance systems, the accuracy and performance of face recognition must be the finest. In this paper various face recognition systems are discussed and analysed like RPRV, LWKPCA, SVM Model, LTrP based SPM and a deep learning framework for recognising images from CCTV. All these face recognition methods, their implementations and performance evaluations are compared to derive the best outcome for future developmental works.



### Class-Specific Variational Auto-Encoder for Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2304.11734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11734v1)
- **Published**: 2023-04-23 19:51:25+00:00
- **Updated**: 2023-04-23 19:51:25+00:00
- **Authors**: Mehdi Rafiei, Alexandros Iosifidis
- **Comment**: 8 pages, 7 figures, 6 tables, accepted at IJCNN conference
- **Journal**: None
- **Summary**: Using a discriminative representation obtained by supervised deep learning methods showed promising results on diverse Content-Based Image Retrieval (CBIR) problems. However, existing methods exploiting labels during training try to discriminate all available classes, which is not ideal in cases where the retrieval problem focuses on a class of interest. In this paper, we propose a regularized loss for Variational Auto-Encoders (VAEs) forcing the model to focus on a given class of interest. As a result, the model learns to discriminate the data belonging to the class of interest from any other possibility, making the learnt latent space of the VAE suitable for class-specific retrieval tasks. The proposed Class-Specific Variational Auto-Encoder (CS-VAE) is evaluated on three public and one custom datasets, and its performance is compared with that of three related VAE-based methods. Experimental results show that the proposed method outperforms its competition in both in-domain and out-of-domain retrieval problems.



### GamutMLP: A Lightweight MLP for Color Loss Recovery
- **Arxiv ID**: http://arxiv.org/abs/2304.11743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11743v1)
- **Published**: 2023-04-23 20:26:11+00:00
- **Updated**: 2023-04-23 20:26:11+00:00
- **Authors**: Hoang M. Le, Brian Price, Scott Cohen, Michael S. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Cameras and image-editing software often process images in the wide-gamut ProPhoto color space, encompassing 90% of all visible colors. However, when images are encoded for sharing, this color-rich representation is transformed and clipped to fit within the small-gamut standard RGB (sRGB) color space, representing only 30% of visible colors. Recovering the lost color information is challenging due to the clipping procedure. Inspired by neural implicit representations for 2D images, we propose a method that optimizes a lightweight multi-layer-perceptron (MLP) model during the gamut reduction step to predict the clipped values. GamutMLP takes approximately 2 seconds to optimize and requires only 23 KB of storage. The small memory footprint allows our GamutMLP model to be saved as metadata in the sRGB image -- the model can be extracted when needed to restore wide-gamut color values. We demonstrate the effectiveness of our approach for color recovery and compare it with alternative strategies, including pre-trained DNN-based gamut expansion networks and other implicit neural representation methods. As part of this effort, we introduce a new color gamut dataset of 2200 wide-gamut/small-gamut images for training and testing. Our code and dataset can be found on the project website: https://gamut-mlp.github.io.



### SketchXAI: A First Look at Explainability for Human Sketches
- **Arxiv ID**: http://arxiv.org/abs/2304.11744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11744v1)
- **Published**: 2023-04-23 20:28:38+00:00
- **Updated**: 2023-04-23 20:28:38+00:00
- **Authors**: Zhiyu Qu, Yulia Gryaditskaya, Ke Li, Kaiyue Pang, Tao Xiang, Yi-Zhe Song
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial Intelligence). We argue that sketch as a ``human-centred'' data form, represents a natural interface to study explainability. We focus on cultivating sketch-specific explainability designs. This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we design a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, location, and order. We then move on to define the first ever XAI task for sketch, that of stroke location inversion SLI. Just as we have heat maps for photos, and correlation matrices for text, SLI offers an explainability angle to sketch in terms of asking a network how well it can recover stroke locations of an unseen sketch. We offer qualitative results for readers to interpret as snapshots of the SLI process in the paper, and as GIFs on the project page. A minor but interesting note is that thanks to its sketch-specific design, our sketch encoder also yields the best sketch recognition accuracy to date while having the smallest number of parameters. The code is available at \url{https://sketchxai.github.io}.



### IDLL: Inverse Depth Line based Visual Localization in Challenging Environments
- **Arxiv ID**: http://arxiv.org/abs/2304.11748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2304.11748v1)
- **Published**: 2023-04-23 20:53:05+00:00
- **Updated**: 2023-04-23 20:53:05+00:00
- **Authors**: Wanting Li, Yu Shao, Yongcai Wang, Shuo Wang, Xuewei Bai, Deying Li
- **Comment**: None
- **Journal**: None
- **Summary**: Precise and real-time localization of unmanned aerial vehicles (UAVs) or robots in GNSS denied indoor environments are critically important for various logistics and surveillance applications. Vision-based simultaneously locating and mapping (VSLAM) are key solutions but suffer location drifts in texture-less, man-made indoor environments. Line features are rich in man-made environments which can be exploited to improve the localization robustness, but existing point-line based VSLAM methods still lack accuracy and efficiency for the representation of lines introducing unnecessary degrees of freedoms. In this paper, we propose Inverse Depth Line Localization(IDLL), which models each extracted line feature using two inverse depth variables exploiting the fact that the projected pixel coordinates on the image plane are rather accurate, which partially restrict the lines. This freedom-reduced representation of lines enables easier line determination and faster convergence of bundle adjustment in each step, therefore achieves more accurate and more efficient frame-to-frame registration and frame-to-map registration using both point and line visual features. We redesign the whole front-end and back-end modules of VSLAM using this line model. IDLL is extensively evaluated in multiple perceptually-challenging datasets. The results show it is more accurate, robust, and needs lower computational overhead than the current state-of-the-art of feature-based VSLAM methods.



### Score-Based Diffusion Models as Principled Priors for Inverse Imaging
- **Arxiv ID**: http://arxiv.org/abs/2304.11751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11751v2)
- **Published**: 2023-04-23 21:05:59+00:00
- **Updated**: 2023-08-28 22:38:31+00:00
- **Authors**: Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, William T. Freeman
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Priors are essential for reconstructing images from noisy and/or incomplete measurements. The choice of the prior determines both the quality and uncertainty of recovered images. We propose turning score-based diffusion models into principled image priors ("score-based priors") for analyzing a posterior of images given measurements. Previously, probabilistic priors were limited to handcrafted regularizers and simple distributions. In this work, we empirically validate the theoretically-proven probability function of a score-based diffusion model. We show how to sample from resulting posteriors by using this probability function for variational inference. Our results, including experiments on denoising, deblurring, and interferometric imaging, suggest that score-based priors enable principled inference with a sophisticated, data-driven image prior.



### Improving Classification Neural Networks by using Absolute activation function (MNIST/LeNET-5 example)
- **Arxiv ID**: http://arxiv.org/abs/2304.11758v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.11758v1)
- **Published**: 2023-04-23 22:17:58+00:00
- **Updated**: 2023-04-23 22:17:58+00:00
- **Authors**: Oleg I. Berngardt
- **Comment**: 19 pages, 5 figures, 3 tables, submitted to Pattern Recognition
- **Journal**: None
- **Summary**: The paper discusses the use of the Absolute activation function in classification neural networks. An examples are shown of using this activation function in simple and more complex problems. Using as a baseline LeNet-5 network for solving the MNIST problem, the efficiency of Absolute activation function is shown in comparison with the use of Tanh, ReLU and SeLU activations. It is shown that in deep networks Absolute activation does not cause vanishing and exploding gradients, and therefore Absolute activation can be used in both simple and deep neural networks. Due to high volatility of training networks with Absolute activation, a special modification of ADAM training algorithm is used, that estimates lower bound of accuracy at any test dataset using validation dataset analysis at each training epoch, and uses this value to stop/decrease learning rate, and re-initializes ADAM algorithm between these steps. It is shown that solving the MNIST problem with the LeNet-like architectures based on Absolute activation allows to significantly reduce the number of trained parameters in the neural network with improving the prediction accuracy.



### You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.11762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.11762v1)
- **Published**: 2023-04-23 22:38:25+00:00
- **Updated**: 2023-04-23 22:38:25+00:00
- **Authors**: Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We propose SeedAL, a method to seed active learning for efficient annotation of 3D point clouds for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a 'seed') to be already annotated to estimate the benefit of annotating other data fractions. We first show that the choice of the seed can significantly affect the performance of many AL methods. We then propose a method for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets. Code is available at \url{https://github.com/nerminsamet/seedal}.



