# Arxiv Papers in cs.CV on 2023-04-16
### Multimodal Representation Learning of Cardiovascular Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2304.07675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07675v1)
- **Published**: 2023-04-16 02:35:27+00:00
- **Updated**: 2023-04-16 02:35:27+00:00
- **Authors**: Jielin Qiu, Peide Huang, Makiya Nakashima, Jaehyun Lee, Jiacheng Zhu, Wilson Tang, Pohao Chen, Christopher Nguyen, Byung-Hak Kim, Debbie Kwon, Douglas Weber, Ding Zhao, David Chen
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Self-supervised learning is crucial for clinical imaging applications, given the lack of explicit labels in healthcare. However, conventional approaches that rely on precise vision-language alignment are not always feasible in complex clinical imaging modalities, such as cardiac magnetic resonance (CMR). CMR provides a comprehensive visualization of cardiac anatomy, physiology, and microstructure, making it challenging to interpret. Additionally, CMR reports require synthesizing information from sequences of images and different views, resulting in potentially weak alignment between the study and diagnosis report pair. To overcome these challenges, we propose \textbf{CMRformer}, a multimodal learning framework to jointly learn sequences of CMR images and associated cardiologist's reports. Moreover, one of the major obstacles to improving CMR study is the lack of large, publicly available datasets. To bridge this gap, we collected a large \textbf{CMR dataset}, which consists of 13,787 studies from clinical cases. By utilizing our proposed CMRformer and our collected dataset, we achieved remarkable performance in real-world clinical tasks, such as CMR image retrieval and diagnosis report retrieval. Furthermore, the learned representations are evaluated to be practically helpful for downstream applications, such as disease classification. Our work could potentially expedite progress in the CMR study and lead to more accurate and effective diagnosis and treatment.



### Enhancing Electrical Impedance Tomography reconstruction using Learned Half-Quadratic Splitting Networks with Anderson Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2304.14491v1
- **DOI**: None
- **Categories**: **cs.CV**, 78A46, 68U10, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2304.14491v1)
- **Published**: 2023-04-16 03:06:26+00:00
- **Updated**: 2023-04-16 03:06:26+00:00
- **Authors**: Guixian Xu, Huihui Wang, Qingping Zhou
- **Comment**: 27 pages, 8figures, 6 algorithms
- **Journal**: None
- **Summary**: Electrical Impedance Tomography (EIT) is widely applied in medical diagnosis, industrial inspection, and environmental monitoring. Combining the physical principles of the imaging system with the advantages of data-driven deep learning networks, physics-embedded deep unrolling networks have recently emerged as a promising solution in computational imaging. However, the inherent nonlinear and ill-posed properties of EIT image reconstruction still present challenges to existing methods in terms of accuracy and stability. To tackle this challenge, we propose the learned half-quadratic splitting (HQSNet) algorithm for incorporating physics into learning-based EIT imaging. We then apply Anderson acceleration (AA) to the HQSNet algorithm, denoted as AA-HQSNet, which can be interpreted as AA applied to the Gauss-Newton step and the learned proximal gradient descent step of the HQSNet, respectively. AA is a widely-used technique for accelerating the convergence of fixed-point iterative algorithms and has gained significant interest in numerical optimization and machine learning. However, the technique has received little attention in the inverse problems community thus far. Employing AA enhances the convergence rate compared to the standard HQSNet while simultaneously avoiding artifacts in the reconstructions. Lastly, we conduct rigorous numerical and visual experiments to show that the AA module strengthens the HQSNet, leading to robust, accurate, and considerably superior reconstructions compared to state-of-the-art methods. Our Anderson acceleration scheme to enhance HQSNet is generic and can be applied to improve the performance of various physics-embedded deep learning methods.



### Autoencoders with Intrinsic Dimension Constraints for Learning Low Dimensional Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2304.07686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07686v1)
- **Published**: 2023-04-16 03:43:08+00:00
- **Updated**: 2023-04-16 03:43:08+00:00
- **Authors**: Jianzhang Zheng, Hao Shen, Jian Yang, Xuan Tang, Mingsong Chen, Hui Yu, Jielong Guo, Xian Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Autoencoders have achieved great success in various computer vision applications. The autoencoder learns appropriate low dimensional image representations through the self-supervised paradigm, i.e., reconstruction. Existing studies mainly focus on the minimizing the reconstruction error on pixel level of image, while ignoring the preservation of Intrinsic Dimension (ID), which is a fundamental geometric property of data representations in Deep Neural Networks (DNNs). Motivated by the important role of ID, in this paper, we propose a novel deep representation learning approach with autoencoder, which incorporates regularization of the global and local ID constraints into the reconstruction of data representations. This approach not only preserves the global manifold structure of the whole dataset, but also maintains the local manifold structure of the feature maps of each point, which makes the learned low-dimensional features more discriminant and improves the performance of the downstream algorithms. To our best knowledge, existing works are rare and limited on exploiting both global and local ID invariant properties on the regularization of autoencoders. Numerical experimental results on benchmark datasets (Extended Yale B, Caltech101 and ImageNet) show that the resulting regularized learning models achieve better discriminative representations for downstream tasks including image classification and clustering.



### Learning Empirical Bregman Divergence for Uncertain Distance Representation
- **Arxiv ID**: http://arxiv.org/abs/2304.07689v3
- **DOI**: 10.23919/FUSION52260.2023.10224080
- **Categories**: **cs.CV**, cs.AI, cs.IT, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2304.07689v3)
- **Published**: 2023-04-16 04:16:28+00:00
- **Updated**: 2023-05-15 16:38:23+00:00
- **Authors**: Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu
- **Comment**: Accepted by IEEE FUSION 2023
- **Journal**: None
- **Summary**: Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognition problems.



### Long-term Visual Localization with Mobile Sensors
- **Arxiv ID**: http://arxiv.org/abs/2304.07691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07691v1)
- **Published**: 2023-04-16 04:35:10+00:00
- **Updated**: 2023-04-16 04:35:10+00:00
- **Authors**: Shen Yan, Yu Liu, Long Wang, Zehong Shen, Zhen Peng, Haomin Liu, Maojun Zhang, Guofeng Zhang, Xiaowei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the remarkable advances in image matching and pose estimation, image-based localization of a camera in a temporally-varying outdoor environment is still a challenging problem due to huge appearance disparity between query and reference images caused by illumination, seasonal and structural changes. In this work, we propose to leverage additional sensors on a mobile phone, mainly GPS, compass, and gravity sensor, to solve this challenging problem. We show that these mobile sensors provide decent initial poses and effective constraints to reduce the searching space in image matching and final pose estimation. With the initial pose, we are also able to devise a direct 2D-3D matching network to efficiently establish 2D-3D correspondences instead of tedious 2D-2D matching in existing systems. As no public dataset exists for the studied problem, we collect a new dataset that provides a variety of mobile sensor data and significant scene appearance variations, and develop a system to acquire ground-truth poses for query images. We benchmark our method as well as several state-of-the-art baselines and demonstrate the effectiveness of the proposed approach. The code and dataset will be released publicly.



### Translating Simulation Images to X-ray Images via Multi-Scale Semantic Matching
- **Arxiv ID**: http://arxiv.org/abs/2304.07693v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07693v1)
- **Published**: 2023-04-16 04:49:46+00:00
- **Updated**: 2023-04-16 04:49:46+00:00
- **Authors**: Jingxuan Kang, Tudor Jianu, Baoru Huang, Binod Bhattarai, Ngan Le, Frans Coenen, Anh Nguyen
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Endovascular intervention training is increasingly being conducted in virtual simulators. However, transferring the experience from endovascular simulators to the real world remains an open problem. The key challenge is the virtual environments are usually not realistically simulated, especially the simulation images. In this paper, we propose a new method to translate simulation images from an endovascular simulator to X-ray images. Previous image-to-image translation methods often focus on visual effects and neglect structure information, which is critical for medical images. To address this gap, we propose a new method that utilizes multi-scale semantic matching. We apply self-domain semantic matching to ensure that the input image and the generated image have the same positional semantic relationships. We further apply cross-domain matching to eliminate the effects of different styles. The intensive experiment shows that our method generates realistic X-ray images and outperforms other state-of-the-art approaches by a large margin. We also collect a new large-scale dataset to serve as the new benchmark for this task. Our source code and dataset will be made publicly available.



### Handling Heavy Occlusion in Dense Crowd Tracking by Focusing on the Heads
- **Arxiv ID**: http://arxiv.org/abs/2304.07705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07705v2)
- **Published**: 2023-04-16 06:00:35+00:00
- **Updated**: 2023-04-27 05:08:33+00:00
- **Authors**: Yu Zhang, Huaming Chen, Wei Bao, Zhongzheng Lai, Zao Zhang, Dong Yuan
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: With the rapid development of deep learning, object detection and tracking play a vital role in today's society. Being able to identify and track all the pedestrians in the dense crowd scene with computer vision approaches is a typical challenge in this field, also known as the Multiple Object Tracking (MOT) challenge. Modern trackers are required to operate on more and more complicated scenes. According to the MOT20 challenge result, the pedestrian is 4 times denser than the MOT17 challenge. Hence, improving the ability to detect and track in extremely crowded scenes is the aim of this work. In light of the occlusion issue with the human body, the heads are usually easier to identify. In this work, we have designed a joint head and body detector in an anchor-free style to boost the detection recall and precision performance of pedestrians in both small and medium sizes. Innovatively, our model does not require information on the statistical head-body ratio for common pedestrians detection for training. Instead, the proposed model learns the ratio dynamically. To verify the effectiveness of the proposed model, we evaluate the model with extensive experiments on different datasets, including MOT20, Crowdhuman, and HT21 datasets. As a result, our proposed method significantly improves both the recall and precision rate on small & medium sized pedestrians and achieves state-of-the-art results in these challenging datasets.



### Non-exemplar Class-incremental Learning by Random Auxiliary Classes Augmentation and Mixed Features
- **Arxiv ID**: http://arxiv.org/abs/2304.07707v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07707v2)
- **Published**: 2023-04-16 06:33:43+00:00
- **Updated**: 2023-08-25 15:17:41+00:00
- **Authors**: Ke Song, Quan Xia, Guoqiang Liang, Zhaojie Chen, Yanning Zhang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Non-exemplar class-incremental learning refers to classifying new and old classes without storing samples of old classes. Since only new class samples are available for optimization, it often occurs catastrophic forgetting of old knowledge. To alleviate this problem, many new methods are proposed such as model distillation, class augmentation. In this paper, we propose an effective non-exemplar method called RAMF consisting of Random Auxiliary classes augmentation and Mixed Feature. On the one hand, we design a novel random auxiliary classes augmentation method, where one augmentation is randomly selected from three augmentations and applied on the input to generate augmented samples and extra class labels. By extending data and label space, it allows the model to learn more diverse representations, which can prevent the model from being biased towards learning task-specific features. When learning new tasks, it will reduce the change of feature space and improve model generalization. On the other hand, we employ mixed feature to replace the new features since only using new feature to optimize the model will affect the representation that was previously embedded in the feature space. Instead, by mixing new and old features, old knowledge can be retained without increasing the computational complexity. Extensive experiments on three benchmarks demonstrate the superiority of our approach, which outperforms the state-of-the-art non-exemplar methods and is comparable to high-performance replay-based methods.



### Obstacle-Transformer: A Trajectory Prediction Network Based on Surrounding Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2304.07711v1
- **DOI**: 10.1049/csy2.12066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07711v1)
- **Published**: 2023-04-16 07:26:59+00:00
- **Updated**: 2023-04-16 07:26:59+00:00
- **Authors**: Wendong Zhang, Qingjie Chai, Quanqi Zhang, Chengwei Wu
- **Comment**: 8 pages, 4 figures
- **Journal**: IET Cyber-Systems and Robotics, 2023, 5(1), e12066
- **Summary**: Recurrent Neural Network, Long Short-Term Memory, and Transformer have made great progress in predicting the trajectories of moving objects. Although the trajectory element with the surrounding scene features has been merged to improve performance, there still exist some problems to be solved. One is that the time series processing models will increase the inference time with the increase of the number of prediction sequences. Another lies in which the features can not be extracted from the scene's image and point cloud in some situations. Therefore, this paper proposes an Obstacle-Transformer to predict trajectory in a constant inference time. An ``obstacle'' is designed by the surrounding trajectory rather than images or point clouds, making Obstacle-Transformer more applicable in a wider range of scenarios. Experiments are conducted on ETH and UCY data sets to verify the performance of our model.



### A Novel end-to-end Framework for Occluded Pixel Reconstruction with Spatio-temporal Features for Improved Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2304.07721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07721v1)
- **Published**: 2023-04-16 08:14:29+00:00
- **Updated**: 2023-04-16 08:14:29+00:00
- **Authors**: Prathistith Raj Medi, Ghanta Sai Krishna, Praneeth Nemani, Satyanarayana Vollala, Santosh Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is vital for monitoring and tracking crowd movement to enhance public security. However, re-identification in the presence of occlusion substantially reduces the performance of existing systems and is a challenging area. In this work, we propose a plausible solution to this problem by developing effective occlusion detection and reconstruction framework for RGB images/videos consisting of Deep Neural Networks. Specifically, a CNN-based occlusion detection model classifies individual input frames, followed by a Conv-LSTM and Autoencoder to reconstruct the occluded pixels corresponding to the occluded frames for sequential (video) and non-sequential (image) data, respectively. The quality of the reconstructed RGB frames is further refined and fine-tuned using a Conditional Generative Adversarial Network (cGAN). Our method is evaluated on four well-known public data sets of the domain, and the qualitative reconstruction results are indeed appealing. Quantitative evaluation in terms of re-identification accuracy of the Siamese network showed an exceptional Rank-1 accuracy after occluded pixel reconstruction on various datasets. A comparative analysis with state-of-the-art approaches also demonstrates the robustness of our work for use in real-life surveillance systems.



### MS-LSTM: Exploring Spatiotemporal Multiscale Representations in Video Prediction Domain
- **Arxiv ID**: http://arxiv.org/abs/2304.07724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07724v2)
- **Published**: 2023-04-16 08:25:02+00:00
- **Updated**: 2023-04-22 05:38:45+00:00
- **Authors**: Zhifeng Ma, Hao Zhang, Jie Liu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2206.03010
- **Journal**: None
- **Summary**: The drastic variation of motion in spatial and temporal dimensions makes the video prediction task extremely challenging. Existing RNN models obtain higher performance by deepening or widening the model. They obtain the multi-scale features of the video only by stacking layers, which is inefficient and brings unbearable training costs (such as memory, FLOPs, and training time). Different from them, this paper proposes a spatiotemporal multi-scale model called MS-LSTM wholly from a multi-scale perspective. On the basis of stacked layers, MS-LSTM incorporates two additional efficient multi-scale designs to fully capture spatiotemporal context information. Concretely, we employ LSTMs with mirrored pyramid structures to construct spatial multi-scale representations and LSTMs with different convolution kernels to construct temporal multi-scale representations. Detailed comparison experiments with eight baseline models on four video datasets show that MS-LSTM has better performance but lower training costs.



### TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.07728v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07728v2)
- **Published**: 2023-04-16 08:54:36+00:00
- **Updated**: 2023-04-26 00:44:25+00:00
- **Authors**: Leyuan Sun, Guanqun Ding, Yue Qiu, Yusuke Yoshiyasu, Fumio Kanehiro
- **Comment**: Submitted to IEEE Sensors Journal with some modifications. This work
  has been submitted to the IEEE for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalities to address the overfitting problem that can arise from blindly increasing the complexity of the model. Additionally, to interpret the learning process of the Transformer-based multi-modal interactions, a general visualization approach is introduced to illustrate the interactions between modalities. Moreover, exhaustive ablation studies evaluate different multi-modal fusion strategies to verify the performance of the proposed fusion strategy. A synthetic multi-modal dataset is made public to validate the generalization ability of the proposed fusion strategy, which also works for other combinations of different modalities. The quantitative and qualitative odometry evaluations on the KITTI dataset verify the proposed TransFusionOdom could achieve superior performance compared with other related works.



### SeaThru-NeRF: Neural Radiance Fields in Scattering Media
- **Arxiv ID**: http://arxiv.org/abs/2304.07743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07743v1)
- **Published**: 2023-04-16 10:17:26+00:00
- **Updated**: 2023-04-16 10:17:26+00:00
- **Authors**: Deborah Levy, Amit Peleg, Naama Pearl, Dan Rosenbaum, Derya Akkaynak, Simon Korman, Tali Treibitz
- **Comment**: None
- **Journal**: None
- **Summary**: Research on neural radiance fields (NeRFs) for novel view generation is exploding with new models and extensions. However, a question that remains unanswered is what happens in underwater or foggy scenes where the medium strongly influences the appearance of objects. Thus far, NeRF and its variants have ignored these cases. However, since the NeRF framework is based on volumetric rendering, it has inherent capability to account for the medium's effects, once modeled appropriately. We develop a new rendering model for NeRFs in scattering media, which is based on the SeaThru image formation model, and suggest a suitable architecture for learning both scene information and medium parameters. We demonstrate the strength of our method using simulated and real-world scenes, correctly rendering novel photorealistic views underwater. Even more excitingly, we can render clear views of these scenes, removing the medium between the camera and the scene and reconstructing the appearance and depth of far objects, which are severely occluded by the medium. Our code and unique datasets are available on the project's website.



### JoB-VS: Joint Brain-Vessel Segmentation in TOF-MRA Images
- **Arxiv ID**: http://arxiv.org/abs/2304.07744v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07744v1)
- **Published**: 2023-04-16 10:18:24+00:00
- **Updated**: 2023-04-16 10:18:24+00:00
- **Authors**: Natalia Valderrama, Ioannis Pitsiorlas, Luisa Vargas, Pablo Arbeláez, Maria A. Zuluaga
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first joint-task learning framework for brain and vessel segmentation (JoB-VS) from Time-of-Flight Magnetic Resonance images. Unlike state-of-the-art vessel segmentation methods, our approach avoids the pre-processing step of implementing a model to extract the brain from the volumetric input data. Skipping this additional step makes our method an end-to-end vessel segmentation framework. JoB-VS uses a lattice architecture that favors the segmentation of structures of different scales (e.g., the brain and vessels). Its segmentation head allows the simultaneous prediction of the brain and vessel mask. Moreover, we generate data augmentation with adversarial examples, which our results demonstrate to enhance the performance. JoB-VS achieves 70.03% mean AP and 69.09% F1-score in the OASIS-3 dataset and is capable of generalizing the segmentation in the IXI dataset. These results show the adequacy of JoB-VS for the challenging task of vessel segmentation in complete TOF-MRA images.



### Framework for Quality Evaluation of Smart Roadside Infrastructure Sensors for Automated Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2304.07745v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07745v1)
- **Published**: 2023-04-16 10:21:07+00:00
- **Updated**: 2023-04-16 10:21:07+00:00
- **Authors**: Laurent Kloeker, Chenghua Liu, Chao Wei, Lutz Eckstein
- **Comment**: Accepted to be published as part of the 34th IEEE Intelligent
  Vehicles Symposium (IV), Anchorage, Alaska, USA, June 4-7, 2023
- **Journal**: None
- **Summary**: The use of smart roadside infrastructure sensors is highly relevant for future applications of connected and automated vehicles. External sensor technology in the form of intelligent transportation system stations (ITS-Ss) can provide safety-critical real-time information about road users in the form of a digital twin. The choice of sensor setups has a major influence on the downstream function as well as the data quality. To date, there is insufficient research on which sensor setups result in which levels of ITS-S data quality. We present a novel approach to perform detailed quality assessment for smart roadside infrastructure sensors. Our framework is multimodal across different sensor types and is evaluated on the DAIR-V2X dataset. We analyze the composition of different lidar and camera sensors and assess them in terms of accuracy, latency, and reliability. The evaluations show that the framework can be used reliably for several future ITS-S applications.



### Language Guided Local Infiltration for Interactive Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2304.07747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07747v1)
- **Published**: 2023-04-16 10:33:08+00:00
- **Updated**: 2023-04-16 10:33:08+00:00
- **Authors**: Fuxiang Huang, Lei Zhang
- **Comment**: 10 pages, 9 figures, 4 tables, IEEE/CVF Conference on Computer Vision
  and Pattern Recognition
- **Journal**: None
- **Summary**: Interactive Image Retrieval (IIR) aims to retrieve images that are generally similar to the reference image but under the requested text modification. The existing methods usually concatenate or sum the features of image and text simply and roughly, which, however, is difficult to precisely change the local semantics of the image that the text intends to modify. To solve this problem, we propose a Language Guided Local Infiltration (LGLI) system, which fully utilizes the text information and penetrates text features into image features as much as possible. Specifically, we first propose a Language Prompt Visual Localization (LPVL) module to generate a localization mask which explicitly locates the region (semantics) intended to be modified. Then we introduce a Text Infiltration with Local Awareness (TILA) module, which is deployed in the network to precisely modify the reference image and generate image-text infiltrated representation. Extensive experiments on various benchmark databases validate that our method outperforms most state-of-the-art IIR approaches.



### GeoMultiTaskNet: remote sensing unsupervised domain adaptation using geographical coordinates
- **Arxiv ID**: http://arxiv.org/abs/2304.07750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07750v1)
- **Published**: 2023-04-16 11:00:43+00:00
- **Updated**: 2023-04-16 11:00:43+00:00
- **Authors**: Valerio Marsocci, Nicolas Gonthier, Anatol Garioud, Simone Scardapane, Clément Mallet
- **Comment**: None
- **Journal**: None
- **Summary**: Land cover maps are a pivotal element in a wide range of Earth Observation (EO) applications. However, annotating large datasets to develop supervised systems for remote sensing (RS) semantic segmentation is costly and time-consuming. Unsupervised Domain Adaption (UDA) could tackle these issues by adapting a model trained on a source domain, where labels are available, to a target domain, without annotations. UDA, while gaining importance in computer vision, is still under-investigated in RS. Thus, we propose a new lightweight model, GeoMultiTaskNet, based on two contributions: a GeoMultiTask module (GeoMT), which utilizes geographical coordinates to align the source and target domains, and a Dynamic Class Sampling (DCS) strategy, to adapt the semantic segmentation loss to the frequency of classes. This approach is the first to use geographical metadata for UDA in semantic segmentation. It reaches state-of-the-art performances (47,22% mIoU), reducing at the same time the number of parameters (33M), on a subset of the FLAIR dataset, a recently proposed dataset properly shaped for RS UDA, used for the first time ever for research scopes here.



### Fairness in AI and Its Long-Term Implications on Society
- **Arxiv ID**: http://arxiv.org/abs/2304.09826v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.09826v2)
- **Published**: 2023-04-16 11:22:59+00:00
- **Updated**: 2023-07-19 19:30:52+00:00
- **Authors**: Ondrej Bohdal, Timothy Hospedales, Philip H. S. Torr, Fazl Barez
- **Comment**: Stanford Existential Risks Conference 2023
- **Journal**: None
- **Summary**: Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. AI fairness focuses on mitigating such biases to ensure AI decision making is not discriminatory towards certain groups. We take a closer look at AI fairness and analyze how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. More specifically, we discuss how biased models can lead to more negative real-world outcomes for certain groups, which may then become more prevalent by deploying new AI models trained on increasingly biased data, resulting in a feedback loop. If the issues persist, they could be reinforced by interactions with other risks and have severe implications on society in the form of social unrest. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without causing society's collapse.



### Arbitrary Reduction of MRI Inter-slice Spacing Using Hierarchical Feature Conditional Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2304.07756v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07756v2)
- **Published**: 2023-04-16 12:05:46+00:00
- **Updated**: 2023-04-18 09:51:10+00:00
- **Authors**: Xin Wang, Zhenrong Shen, Zhiyun Song, Sheng Wang, Mengjun Liu, Lichi Zhang, Kai Xuan, Qian Wang
- **Comment**: not the time
- **Journal**: None
- **Summary**: Magnetic resonance (MR) images collected in 2D scanning protocols typically have large inter-slice spacing, resulting in high in-plane resolution but reduced through-plane resolution. Super-resolution techniques can reduce the inter-slice spacing of 2D scanned MR images, facilitating the downstream visual experience and computer-aided diagnosis. However, most existing super-resolution methods are trained at a fixed scaling ratio, which is inconvenient in clinical settings where MR scanning may have varying inter-slice spacings. To solve this issue, we propose Hierarchical Feature Conditional Diffusion (HiFi-Diff)} for arbitrary reduction of MR inter-slice spacing. Given two adjacent MR slices and the relative positional offset, HiFi-Diff can iteratively convert a Gaussian noise map into any desired in-between MR slice. Furthermore, to enable fine-grained conditioning, the Hierarchical Feature Extraction (HiFE) module is proposed to hierarchically extract conditional features and conduct element-wise modulation. Our experimental results on the publicly available HCP-1200 dataset demonstrate the high-fidelity super-resolution capability of HiFi-Diff and its efficacy in enhancing downstream segmentation performance.



### Deep learning universal crater detection using Segment Anything Model (SAM)
- **Arxiv ID**: http://arxiv.org/abs/2304.07764v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 86, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2304.07764v1)
- **Published**: 2023-04-16 12:36:37+00:00
- **Updated**: 2023-04-16 12:36:37+00:00
- **Authors**: Iraklis Giannakis, Anshuman Bhardwaj, Lydia Sam, Georgios Leontidis
- **Comment**: 11 pages, 7 Figures, preprint of a submitted paper in Icarus (under
  review)
- **Journal**: None
- **Summary**: Craters are amongst the most important morphological features in planetary exploration. To that extent, detecting, mapping and counting craters is a mainstream process in planetary science, done primarily manually, which is a very laborious and time-consuming process. Recently, machine learning (ML) and computer vision have been successfully applied for both detecting craters and estimating their size. Existing ML approaches for automated crater detection have been trained in specific types of data e.g. digital elevation model (DEM), images and associated metadata for orbiters such as the Lunar Reconnaissance Orbiter Camera (LROC) etc.. Due to that, each of the resulting ML schemes is applicable and reliable only to the type of data used during the training process. Data from different sources, angles and setups can compromise the reliability of these ML schemes. In this paper we present a universal crater detection scheme that is based on the recently proposed Segment Anything Model (SAM) from META AI. SAM is a prompt-able segmentation system with zero-shot generalization to unfamiliar objects and images without the need for additional training. Using SAM we can successfully identify crater-looking objects in any type of data (e,g, raw satellite images Level-1 and 2 products, DEMs etc.) for different setups (e.g. Lunar, Mars) and different capturing angles. Moreover, using shape indexes, we only keep the segmentation masks of crater-like features. These masks are subsequently fitted with an ellipse, recovering both the location and the size/geometry of the detected craters.



### PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction
- **Arxiv ID**: http://arxiv.org/abs/2304.07773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2304.07773v1)
- **Published**: 2023-04-16 13:12:33+00:00
- **Updated**: 2023-04-16 13:12:33+00:00
- **Authors**: Zhen Luo, Junyi Ma, Zijie Zhou, Guangming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to predict future structure features of environments based on past perception information is extremely needed by autonomous vehicles, which helps to make the following decision-making and path planning more reasonable. Recently, point cloud prediction (PCP) is utilized to predict and describe future environmental structures by the point cloud form. In this letter, we propose a novel efficient Transformer-based network to predict the future LiDAR point clouds exploiting the past point cloud sequences. We also design a semantic auxiliary training strategy to make the predicted LiDAR point cloud sequence semantically similar to the ground truth and thus improves the significance of the deployment for more tasks in real-vehicle applications. Our approach is completely self-supervised, which means it does not require any manual labeling and has a solid generalization ability toward different environments. The experimental results show that our method outperforms the state-of-the-art PCP methods on the prediction results and semantic similarity, and has a good real-time performance. Our open-source code and pre-trained models are available at https://github.com/Blurryface0814/PCPNet.



### Robust Cross-Modal Knowledge Distillation for Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/2304.07775v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2304.07775v2)
- **Published**: 2023-04-16 13:16:58+00:00
- **Updated**: 2023-04-27 04:08:08+00:00
- **Authors**: Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal distillation has been widely used to transfer knowledge across different modalities, enriching the representation of the target unimodal one. Recent studies highly relate the temporal synchronization between vision and sound to the semantic consistency for cross-modal distillation. However, such semantic consistency from the synchronization is hard to guarantee in unconstrained videos, due to the irrelevant modality noise and differentiated semantic correlation. To this end, we first propose a \textit{Modality Noise Filter} (MNF) module to erase the irrelevant noise in teacher modality with cross-modal context. After this purification, we then design a \textit{Contrastive Semantic Calibration} (CSC) module to adaptively distill useful knowledge for target modality, by referring to the differentiated sample-wise semantic correlation in a contrastive fashion. Extensive experiments show that our method could bring a performance boost compared with other distillation methods in both visual action recognition and video retrieval task. We also extend to the audio tagging task to prove the generalization of our method. The source code is available at \href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.



### EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.07803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07803v1)
- **Published**: 2023-04-16 15:14:17+00:00
- **Updated**: 2023-04-16 15:14:17+00:00
- **Authors**: Ilwi Yun, Chanyong Shin, Hyunku Lee, Hyuk-Jae Lee, Chae Eun Rhee
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Estimating the depths of equirectangular (360) images (EIs) is challenging given the distorted 180 x 360 field-of-view, which is hard to be addressed via convolutional neural network (CNN). Although a transformer with global attention achieves significant improvements over CNN for EI depth estimation task, it is computationally inefficient, which raises the need for transformer with local attention. However, to apply local attention successfully for EIs, a specific strategy, which addresses distorted equirectangular geometry and limited receptive field simultaneously, is required. Prior works have only cared either of them, resulting in unsatisfactory depths occasionally. In this paper, we propose an equirectangular geometry-biased transformer termed EGformer, which enables local attention extraction in a global manner considering the equirectangular geometry. To achieve this, we actively utilize the equirectangular geometry as the bias for the local attention instead of struggling to reduce the distortion of EIs. As compared to the most recent transformer based EI depth estimation studies, the proposed approach yields the best depth outcomes overall with the lowest computational cost and the fewest parameters, demonstrating the effectiveness of the proposed methods.



### A Random-patch based Defense Strategy Against Physical Attacks for Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2304.07822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.07822v1)
- **Published**: 2023-04-16 16:11:56+00:00
- **Updated**: 2023-04-16 16:11:56+00:00
- **Authors**: JiaHao Xie, Ye Luo, Jianwei Lu
- **Comment**: None
- **Journal**: None
- **Summary**: The physical attack has been regarded as a kind of threat against real-world computer vision systems. Still, many existing defense methods are only useful for small perturbations attacks and can't detect physical attacks effectively. In this paper, we propose a random-patch based defense strategy to robustly detect physical attacks for Face Recognition System (FRS). Different from mainstream defense methods which focus on building complex deep neural networks (DNN) to achieve high recognition rate on attacks, we introduce a patch based defense strategy to a standard DNN aiming to obtain robust detection models. Extensive experimental results on the employed datasets show the superiority of the proposed defense method on detecting white-box attacks and adaptive attacks which attack both FRS and the defense method. Additionally, due to the simpleness yet robustness of our method, it can be easily applied to the real world face recognition system and extended to other defense methods to boost the detection performance.



### Vehicle Safety Management System
- **Arxiv ID**: http://arxiv.org/abs/2304.14497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.14497v1)
- **Published**: 2023-04-16 16:15:25+00:00
- **Updated**: 2023-04-16 16:15:25+00:00
- **Authors**: Chanthini Bhaskar, Bharath Manoj Nair, Dev Mehta
- **Comment**: None
- **Journal**: None
- **Summary**: Overtaking is a critical maneuver in driving that requires accurate information about the location and distance of other vehicles on the road. This study suggests a real-time overtaking assistance system that uses a combination of the You Only Look Once (YOLO) object detection algorithm and stereo vision techniques to accurately identify and locate vehicles in front of the driver, and estimate their distance. The system then signals the vehicles behind the driver using colored lights to inform them of the safe overtaking distance. The proposed system has been implemented using Stereo vision for distance analysis and You Only Look Once (YOLO) for object identification. The results demonstrate its effectiveness in providing vehicle type and the distance between the camera module and the vehicle accurately with an approximate error of 4.107%. Our system has the potential to reduce the risk of accidents and improve the safety of overtaking maneuvers, especially on busy highways and roads.



### AI driven shadow model detection in agropv farms
- **Arxiv ID**: http://arxiv.org/abs/2304.07853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07853v1)
- **Published**: 2023-04-16 18:33:20+00:00
- **Updated**: 2023-04-16 18:33:20+00:00
- **Authors**: Sai Paavan Kumar Dornadula, Pascal Brunet, Dr. Susan Elias
- **Comment**: None
- **Journal**: None
- **Summary**: Agro-photovoltaic (APV) is a growing farming practice that combines agriculture and solar photovoltaic projects within the same area. This emerging market is expected to experience significant growth in the next few years, with a projected investment of $9 billion in 2030. Identifying shadows is crucial to understanding the APV environment, as they impact plant growth, microclimate, and evapotranspiration. In this study, we use state-of-the-art CNN and GAN-based neural networks to detect shadows in agro-PV farms, demonstrating their effectiveness. However, challenges remain, including partial shadowing from moving objects and real-time monitoring. Future research should focus on developing more sophisticated neural network-based shadow detection algorithms and integrating them with control systems for APV farms. Overall, shadow detection is crucial to increase productivity and profitability while supporting the environment, soil, and farmers.



### A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2304.07874v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.07874v2)
- **Published**: 2023-04-16 19:44:25+00:00
- **Updated**: 2023-04-18 17:54:43+00:00
- **Authors**: Yangyi Liu, Huan Liu, Liangyan Li, Zijun Wu, Jun Chen
- **Comment**: Accepted by CVPRW 2023
- **Journal**: None
- **Summary**: Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target dataset and the augmented one. This finding indeed aligns with the essence of data-centric AI. With a novel network architecture and a principled data-preprocessing approach that systematically enhances data quality, we present an innovative dehazing method. Specifically, we apply RGB-channel-wise transformations on the augmented datasets, and incorporate the state-of-the-art transformers as the backbone in the two-branch framework. We conduct extensive experiments and ablation study to demonstrate the effectiveness of our proposed method.



### The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning
- **Arxiv ID**: http://arxiv.org/abs/2304.07875v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.07875v1)
- **Published**: 2023-04-16 19:46:46+00:00
- **Updated**: 2023-04-16 19:46:46+00:00
- **Authors**: Florian Putz, Johanna Grigo, Thomas Weissmann, Philipp Schubert, Daniel Hoefler, Ahmed Gomaa, Hassen Ben Tkhayat, Amr Hagag, Sebastian Lettmaier, Benjamin Frey, Udo S. Gaipl, Luitpold V. Distel, Sabine Semrau, Christoph Bert, Rainer Fietkau, Yixing Huang
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Background: Tumor segmentation in MRI is crucial in radiotherapy (RT) treatment planning for brain tumor patients. Segment anything (SA), a novel promptable foundation model for autosegmentation, has shown high accuracy for multiple segmentation tasks but was not evaluated on medical datasets yet. Methods: SA was evaluated in a point-to-mask task for glioma brain tumor autosegmentation on 16744 transversal slices from 369 MRI datasets (BraTS 2020). Up to 9 point prompts were placed per slice. Tumor core (enhancing tumor + necrotic core) was segmented on contrast-enhanced T1w sequences. Out of the 3 masks predicted by SA, accuracy was evaluated for the mask with the highest calculated IoU (oracle mask) and with highest model predicted IoU (suggested mask). In addition to assessing SA on whole MRI slices, SA was also evaluated on images cropped to the tumor (max. 3D extent + 2 cm). Results: Mean best IoU (mbIoU) using oracle mask on full MRI slices was 0.762 (IQR 0.713-0.917). Best 2D mask was achieved after a mean of 6.6 point prompts (IQR 5-9). Segmentation accuracy was significantly better for high- compared to low-grade glioma cases (mbIoU 0.789 vs. 0.668). Accuracy was worse using MRI slices cropped to the tumor (mbIoU 0.759) and was much worse using suggested mask (full slices 0.572). For all experiments, accuracy was low on peripheral slices with few tumor voxels (mbIoU, <300: 0.537 vs. >=300: 0.841). Stacking best oracle segmentations from full axial MRI slices, mean 3D DSC for tumor core was 0.872, which was improved to 0.919 by combining axial, sagittal and coronal masks. Conclusions: The Segment Anything foundation model, while trained on photos, can achieve high zero-shot accuracy for glioma brain tumor segmentation on MRI slices. The results suggest that Segment Anything can accelerate and facilitate RT treatment planning, when properly integrated in a clinical application.



### Federated Learning of Shareable Bases for Personalization-Friendly Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2304.07882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07882v1)
- **Published**: 2023-04-16 20:19:18+00:00
- **Updated**: 2023-04-16 20:19:18+00:00
- **Authors**: Hong-You Chen, Jike Zhong, Mingda Zhang, Xuhui Jia, Hang Qi, Boqing Gong, Wei-Lun Chao, Li Zhang
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Personalized federated learning (PFL) aims to harness the collective wisdom of clients' data to build customized models tailored to individual clients' data distributions. Existing works offer personalization primarily to clients who participate in the FL process, making it hard to encompass new clients who were absent or newly show up. In this paper, we propose FedBasis, a novel PFL framework to tackle such a deficiency. FedBasis learns a set of few, shareable ``basis'' models, which can be linearly combined to form personalized models for clients. Specifically for a new client, only a small set of combination coefficients, not the models, needs to be learned. This notion makes FedBasis more parameter-efficient, robust, and accurate compared to other competitive PFL baselines, especially in the low data regime, without increasing the inference cost. To demonstrate its applicability, we also present a more practical PFL testbed for image classification, featuring larger data discrepancies across clients in both the image and label spaces as well as more faithful training and test splits.



### Bent & Broken Bicycles: Leveraging synthetic data for damaged object re-identification
- **Arxiv ID**: http://arxiv.org/abs/2304.07883v1
- **DOI**: 10.1109/WACV56688.2023.00486
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.07883v1)
- **Published**: 2023-04-16 20:23:58+00:00
- **Updated**: 2023-04-16 20:23:58+00:00
- **Authors**: Luca Piano, Filippo Gabriele Pratticò, Alessandro Sebastian Russo, Lorenzo Lanari, Lia Morra, Fabrizio Lamberti
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2023, pp. 4881-4891
- **Summary**: Instance-level object re-identification is a fundamental computer vision task, with applications from image retrieval to intelligent monitoring and fraud detection. In this work, we propose the novel task of damaged object re-identification, which aims at distinguishing changes in visual appearance due to deformations or missing parts from subtle intra-class variations. To explore this task, we leverage the power of computer-generated imagery to create, in a semi-automatic fashion, high-quality synthetic images of the same bike before and after a damage occurs. The resulting dataset, Bent & Broken Bicycles (BBBicycles), contains 39,200 images and 2,800 unique bike instances spanning 20 different bike models. As a baseline for this task, we propose TransReI3D, a multi-task, transformer-based deep network unifying damage detection (framed as a multi-label classification task) with object re-identification. The BBBicycles dataset is available at https://huggingface.co/datasets/GrainsPolito/BBBicycles



### Brain Tumor classification and Segmentation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.07901v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.07901v1)
- **Published**: 2023-04-16 21:42:21+00:00
- **Updated**: 2023-04-16 21:42:21+00:00
- **Authors**: Belal Badawy, Romario Sameh Samir, Youssef Tarek, Mohammed Ahmed, Rana Ibrahim, Manar Ahmed, Mohamed Hassan
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumors are a complex and potentially life-threatening medical condition that requires accurate diagnosis and timely treatment. In this paper, we present a machine learning-based system designed to assist healthcare professionals in the classification and diagnosis of brain tumors using MRI images. Our system provides a secure login, where doctors can upload or take a photo of MRI and our app can classify the model and segment the tumor, providing the doctor with a folder of each patient's history, name, and results. Our system can also add results or MRI to this folder, draw on the MRI to send it to another doctor, and save important results in a saved page in the app. Furthermore, our system can classify in less than 1 second and allow doctors to chat with a community of brain tumor doctors.   To achieve these objectives, our system uses a state-of-the-art machine learning algorithm that has been trained on a large dataset of MRI images. The algorithm can accurately classify different types of brain tumors and provide doctors with detailed information on the size, location, and severity of the tumor. Additionally, our system has several features to ensure its security and privacy, including secure login and data encryption.   We evaluated our system using a dataset of real-world MRI images and compared its performance to other existing systems. Our results demonstrate that our system is highly accurate, efficient, and easy to use. We believe that our system has the potential to revolutionize the field of brain tumor diagnosis and treatment and provide healthcare professionals with a powerful tool for improving patient outcomes.



### CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling
- **Arxiv ID**: http://arxiv.org/abs/2304.07915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07915v1)
- **Published**: 2023-04-16 23:24:40+00:00
- **Updated**: 2023-04-16 23:24:40+00:00
- **Authors**: Haidong Zhu, Zhaoheng Zheng, Wanrong Zheng, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of human rendering in the video with temporal appearance constancy. Reconstructing dynamic body shapes with volumetric neural rendering methods, such as NeRF, requires finding the correspondence of the points in the canonical and observation space, which demands understanding human body shape and motion. Some methods use rigid transformation, such as SE(3), which cannot precisely model each frame's unique motion and muscle movements. Others generate the transformation for each frame with a trainable network, such as neural blend weight field or translation vector field, which does not consider the appearance constancy of general body shape. In this paper, we propose CAT-NeRF for self-awareness of appearance constancy with Tx$^2$Former, a novel way to combine two Transformer layers, to separate appearance constancy and uniqueness. Appearance constancy models the general shape across the video, and uniqueness models the unique patterns for each frame. We further introduce a novel Covariance Loss to limit the correlation between each pair of appearance uniquenesses to ensure the frame-unique pattern is maximally captured in appearance uniqueness. We assess our method on H36M and ZJU-MoCap and show state-of-the-art performance.



### GaitRef: Gait Recognition with Refined Sequential Skeletons
- **Arxiv ID**: http://arxiv.org/abs/2304.07916v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.07916v3)
- **Published**: 2023-04-16 23:37:24+00:00
- **Updated**: 2023-08-08 16:06:11+00:00
- **Authors**: Haidong Zhu, Wanrong Zheng, Zhaoheng Zheng, Ram Nevatia
- **Comment**: IJCB 2023 oral. Code is available at
  https://github.com/haidongz-usc/GaitRef
- **Journal**: None
- **Summary**: Identifying humans with their walking sequences, known as gait recognition, is a useful biometric understanding task as it can be observed from a long distance and does not require cooperation from the subject. Two common modalities used for representing the walking sequence of a person are silhouettes and joint skeletons. Silhouette sequences, which record the boundary of the walking person in each frame, may suffer from the variant appearances from carried-on objects and clothes of the person. Framewise joint detections are noisy and introduce some jitters that are not consistent with sequential detections. In this paper, we combine the silhouettes and skeletons and refine the framewise joint predictions for gait recognition. With temporal information from the silhouette sequences, we show that the refined skeletons can improve gait recognition performance without extra annotations. We compare our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show state-of-the-art performance.



### Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2304.07918v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2304.07918v1)
- **Published**: 2023-04-16 23:44:41+00:00
- **Updated**: 2023-04-16 23:44:41+00:00
- **Authors**: Yaxuan Zhu, Jianwen Xie, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image generative model that incorporates 3D representation via Neural Radiance Fields (NeRF) and 2D imaging process via differentiable volume rendering. The model represents an image as a rendering process from 3D object to 2D image and is conditioned on some latent variables that account for object characteristics and are assumed to follow informative trainable energy-based prior models. We propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i) maximum likelihood estimation with Markov chain Monte Carlo-based inference and (ii) variational inference with the reparameterization trick. We study our models in the scenarios with both known and unknown camera poses. Experiments on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D object structures from 2D images, generate 2D images with novel views and objects, learn from incomplete 2D images, and learn from 2D images with known or unknown camera poses.



### Chain of Thought Prompt Tuning in Vision Language Models
- **Arxiv ID**: http://arxiv.org/abs/2304.07919v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.07919v2)
- **Published**: 2023-04-16 23:59:25+00:00
- **Updated**: 2023-06-17 06:40:27+00:00
- **Authors**: Jiaxin Ge, Hongyin Luo, Siyuan Qian, Yulu Gan, Jie Fu, Shanghang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes



