# Arxiv Papers in cs.CV on 2023-05-14
### On enhancing the robustness of Vision Transformers: Defensive Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2305.08031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.08031v1)
- **Published**: 2023-05-14 00:17:33+00:00
- **Updated**: 2023-05-14 00:17:33+00:00
- **Authors**: Raza Imam, Muhammad Huzaifa, Mohammed El-Amine Azz
- **Comment**: Our code is publicly available at
  https://github.com/Muhammad-Huzaifaa/Defensive_Diffusion
- **Journal**: None
- **Summary**: Privacy and confidentiality of medical data are of utmost importance in healthcare settings. ViTs, the SOTA vision model, rely on large amounts of patient data for training, which raises concerns about data security and the potential for unauthorized access. Adversaries may exploit vulnerabilities in ViTs to extract sensitive patient information and compromising patient privacy. This work address these vulnerabilities to ensure the trustworthiness and reliability of ViTs in medical applications. In this work, we introduced a defensive diffusion technique as an adversarial purifier to eliminate adversarial noise introduced by attackers in the original image. By utilizing the denoising capabilities of the diffusion model, we employ a reverse diffusion process to effectively eliminate the adversarial noise from the attack sample, resulting in a cleaner image that is then fed into the ViT blocks. Our findings demonstrate the effectiveness of the diffusion model in eliminating attack-agnostic adversarial noise from images. Additionally, we propose combining knowledge distillation with our framework to obtain a lightweight student model that is both computationally efficient and robust against gray box attacks. Comparison of our method with a SOTA baseline method, SEViT, shows that our work is able to outperform the baseline. Extensive experiments conducted on a publicly available Tuberculosis X-ray dataset validate the computational efficiency and improved robustness achieved by our proposed architecture.



### CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data
- **Arxiv ID**: http://arxiv.org/abs/2305.08042v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08042v1)
- **Published**: 2023-05-14 01:43:10+00:00
- **Updated**: 2023-05-14 01:43:10+00:00
- **Authors**: Sheng Zhong, Nima Fazeli, Dmitry Berenson
- **Comment**: 10 pages with 1 page appendix, camera-ready version for RSS 2023
  (accepted)
- **Journal**: None
- **Summary**: This paper proposes a novel method for estimating the set of plausible poses of a rigid object from a set of points with volumetric information, such as whether each point is in free space or on the surface of the object. In particular, we study how pose can be estimated from force and tactile data arising from contact. Using data derived from contact is challenging because it is inherently less information-dense than visual data, and thus the pose estimation problem is severely under-constrained when there are few contacts. Rather than attempting to estimate the true pose of the object, which is not tractable without a large number of contacts, we seek to estimate a plausible set of poses which obey the constraints imposed by the sensor data. Existing methods struggle to estimate this set because they are either designed for single pose estimates or require informative priors to be effective. Our approach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL), has three key attributes: 1) It considers volumetric information, which allows us to account for known free space; 2) It uses a novel differentiable volumetric cost function to take advantage of powerful gradient-based optimization tools; and 3) It uses methods from the Quality Diversity (QD) optimization literature to produce a diverse set of high-quality poses. To our knowledge, QD methods have not been used previously for pose registration. We also show how to update our plausible pose estimates online as more data is gathered by the robot. Our experiments suggest that CHSEL shows large performance improvements over several baseline methods for both simulated and real-world data.



### SCRNet: a Retinex Structure-based Low-light Enhancement Model Guided by Spatial Consistency
- **Arxiv ID**: http://arxiv.org/abs/2305.08053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08053v1)
- **Published**: 2023-05-14 03:32:19+00:00
- **Updated**: 2023-05-14 03:32:19+00:00
- **Authors**: Miao Zhang, Yiqing Shen, Shenghui Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Images captured under low-light conditions are often plagued by several challenges, including diminished contrast, increased noise, loss of fine details, and unnatural color reproduction. These factors can significantly hinder the performance of computer vision tasks such as object detection and image segmentation. As a result, improving the quality of low-light images is of paramount importance for practical applications in the computer vision domain.To effectively address these challenges, we present a novel low-light image enhancement model, termed Spatial Consistency Retinex Network (SCRNet), which leverages the Retinex-based structure and is guided by the principle of spatial consistency.Specifically, our proposed model incorporates three levels of consistency: channel level, semantic level, and texture level, inspired by the principle of spatial consistency.These levels of consistency enable our model to adaptively enhance image features, ensuring more accurate and visually pleasing results.Extensive experimental evaluations on various low-light image datasets demonstrate that our proposed SCRNet outshines existing state-of-the-art methods, highlighting the potential of SCRNet as an effective solution for enhancing low-light images.



### Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2305.08059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.08059v1)
- **Published**: 2023-05-14 03:57:11+00:00
- **Updated**: 2023-05-14 03:57:11+00:00
- **Authors**: Chenyang Lyu, Tianbo Ji, Yvette Graham, Jennifer Foster
- **Comment**: None
- **Journal**: None
- **Summary**: Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code will be made publicly available for research use.



### Helping Visually Impaired People Take Better Quality Pictures
- **Arxiv ID**: http://arxiv.org/abs/2305.08066v1
- **DOI**: 10.1109/TIP.2023.3282067
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08066v1)
- **Published**: 2023-05-14 04:37:53+00:00
- **Updated**: 2023-05-14 04:37:53+00:00
- **Authors**: Maniratnam Mandal, Deepti Ghadiyaram, Danna Gurari, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Perception-based image analysis technologies can be used to help visually impaired people take better quality pictures by providing automated guidance, thereby empowering them to interact more confidently on social media. The photographs taken by visually impaired users often suffer from one or both of two kinds of quality issues: technical quality (distortions), and semantic quality, such as framing and aesthetic composition. Here we develop tools to help them minimize occurrences of common technical distortions, such as blur, poor exposure, and noise. We do not address the complementary problems of semantic quality, leaving that aspect for future work. The problem of assessing and providing actionable feedback on the technical quality of pictures captured by visually impaired users is hard enough, owing to the severe, commingled distortions that often occur. To advance progress on the problem of analyzing and measuring the technical quality of visually impaired user-generated content (VI-UGC), we built a very large and unique subjective image quality and distortion dataset. This new perceptual resource, which we call the LIVE-Meta VI-UGC Database, contains $40$K real-world distorted VI-UGC images and $40$K patches, on which we recorded $2.7$M human perceptual quality judgments and $2.7$M distortion labels. Using this psychometric resource we also created an automatic blind picture quality and distortion predictor that learns local-to-global spatial quality relationships, achieving state-of-the-art prediction performance on VI-UGC pictures, significantly outperforming existing picture quality models on this unique class of distorted picture data. We also created a prototype feedback system that helps to guide users to mitigate quality issues and take better quality pictures, by creating a multi-task learning framework.



### Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.08069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08069v1)
- **Published**: 2023-05-14 04:53:05+00:00
- **Updated**: 2023-05-14 04:53:05+00:00
- **Authors**: Burhaneddin Yaman, Tanvir Mahmud, Chun-Hao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an embarrassingly simple method -- instance-aware repeat factor sampling (IRFS) to address the problem of imbalanced data in long-tailed object detection. Imbalanced datasets in real-world object detection often suffer from a large disparity in the number of instances for each class. To improve the generalization performance of object detection models on rare classes, various data sampling techniques have been proposed. Repeat factor sampling (RFS) has shown promise due to its simplicity and effectiveness. Despite its efficiency, RFS completely neglects the instance counts and solely relies on the image count during re-sampling process. However, instance count may immensely vary for different classes with similar image counts. Such variation highlights the importance of both image and instance for addressing the long-tail distributions. Thus, we propose IRFS which unifies instance and image counts for the re-sampling process to be aware of different perspectives of the imbalance in long-tailed datasets. Our method shows promising results on the challenging LVIS v1.0 benchmark dataset over various architectures and backbones, demonstrating their effectiveness in improving the performance of object detection models on rare classes with a relative $+50\%$ average precision (AP) improvement over counterpart RFS. IRFS can serve as a strong baseline and be easily incorporated into existing long-tailed frameworks.



### Analyzing Compression Techniques for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2305.08075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08075v1)
- **Published**: 2023-05-14 05:17:32+00:00
- **Updated**: 2023-05-14 05:17:32+00:00
- **Authors**: Maniratnam Mandal, Imran Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Compressing deep networks is highly desirable for practical use-cases in computer vision applications. Several techniques have been explored in the literature, and research has been done in finding efficient strategies for combining them. For this project, we aimed to explore three different basic compression techniques - knowledge distillation, pruning, and quantization for small-scale recognition tasks. Along with the basic methods, we also test the efficacy of combining them in a sequential manner. We analyze them using MNIST and CIFAR-10 datasets and present the results along with few observations inferred from them.



### Improving Defensive Distillation using Teacher Assistant
- **Arxiv ID**: http://arxiv.org/abs/2305.08076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.08076v1)
- **Published**: 2023-05-14 05:27:17+00:00
- **Updated**: 2023-05-14 05:27:17+00:00
- **Authors**: Maniratnam Mandal, Suna Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks pose a significant threat to the security and safety of deep neural networks being applied to modern applications. More specifically, in computer vision-based tasks, experts can use the knowledge of model architecture to create adversarial samples imperceptible to the human eye. These attacks can lead to security problems in popular applications such as self-driving cars, face recognition, etc. Hence, building networks which are robust to such attacks is highly desirable and essential. Among the various methods present in literature, defensive distillation has shown promise in recent years. Using knowledge distillation, researchers have been able to create models robust against some of those attacks. However, more attacks have been developed exposing weakness in defensive distillation. In this project, we derive inspiration from teacher assistant knowledge distillation and propose that introducing an assistant network can improve the robustness of the distilled model. Through a series of experiments, we evaluate the distilled models for different distillation temperatures in terms of accuracy, sensitivity, and robustness. Our experiments demonstrate that the proposed hypothesis can improve robustness in most cases. Additionally, we show that multi-step distillation can further improve robustness with very little impact on model accuracy.



### Cross-domain Collaborative Learning for Recognizing Multiple Retinal Diseases from Wide-Field Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2305.08078v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08078v1)
- **Published**: 2023-05-14 05:57:11+00:00
- **Updated**: 2023-05-14 05:57:11+00:00
- **Authors**: Qijie Wei, Jingyuan Yang, Bo Wang, Jinrui Wang, Jianchun Zhao, Xinyu Zhao, Sheng Yang, Niranchana Manivannan, Youxin Chen, Dayong Ding, Xirong Li
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: This paper addresses the emerging task of recognizing multiple retinal diseases from wide-field (WF) and ultra-wide-field (UWF) fundus images. For an effective reuse of existing labeled color fundus photo (CFP) data, we propose Cross-domain Collaborative Learning (CdCL). Inspired by the success of fixed-ratio based mixup in unsupervised domain adaptation, we re-purpose this strategy for the current task. Due to the intrinsic disparity between the field-of-view of CFP and WF/UWF images, a scale bias naturally exists in a mixup sample that the anatomic structure from a CFP image will be considerably larger than its WF/UWF counterpart. The CdCL method resolves the issue by Scale-bias Correction, which employs Transformers for producing scale-invariant features. As demonstrated by extensive experiments on multiple datasets covering both WF and UWF images, the proposed method compares favorably against a number of competitive baselines.



### Meta-DM: Applications of Diffusion Models on Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.08092v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08092v1)
- **Published**: 2023-05-14 08:05:30+00:00
- **Updated**: 2023-05-14 08:05:30+00:00
- **Authors**: Wentao Hu, Xiurong Jiang, Jiarun Liu, Yuqi Yang, Hui Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of few-shot learning (FSL), extensive research has focused on improving network structures and training strategies. However, the role of data processing modules has not been fully explored. Therefore, in this paper, we propose Meta-DM, a generalized data processing module for FSL problems based on diffusion models. Meta-DM is a simple yet effective module that can be easily integrated with existing FSL methods, leading to significant performance improvements in both supervised and unsupervised settings. We provide a theoretical analysis of Meta-DM and evaluate its performance on several algorithms. Our experiments show that combining Meta-DM with certain methods achieves state-of-the-art results.



### Tao General Differential and Difference: Theory and Application
- **Arxiv ID**: http://arxiv.org/abs/2305.08098v1
- **DOI**: None
- **Categories**: **cs.DM**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2305.08098v1)
- **Published**: 2023-05-14 08:24:59+00:00
- **Updated**: 2023-05-14 08:24:59+00:00
- **Authors**: Linmi Tao, Ruiyang Liu, Donglai Tao, Wu Xia, Feilong Ma, Jingmao Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Modern numerical analysis is executed on discrete data, of which numerical difference computation is one of the cores and is indispensable. Nevertheless, difference algorithms have a critical weakness in their sensitivity to noise, which has long posed a challenge in various fields including signal processing. Difference is an extension or generalization of differential in the discrete domain. However, due to the finite interval in discrete calculation, there is a failure in meeting the most fundamental definition of differential, where dy and dx are both infinitesimal (Leibniz) or the limit of dx is 0 (Cauchy). In this regard, the generalization of differential to difference does not hold. To address this issue, we depart from the original derivative approach, construct a finite interval-based differential, and further generalize it to obtain the difference by convolution. Based on this theory, we present a variety of difference operators suitable for practical signal processing. Experimental results demonstrate that these difference operators possess exceptional signal processing capabilities, including high noise immunity.



### MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/2305.08117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08117v1)
- **Published**: 2023-05-14 10:17:09+00:00
- **Updated**: 2023-05-14 10:17:09+00:00
- **Authors**: Yunshan Zhong, Mingbao Lin, Yuyao Zhou, Mengzhao Chen, Yuxin Zhang, Fei Chao, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Arbitrary bit-width network quantization has received significant attention due to its high adaptability to various bit-width requirements during runtime. However, in this paper, we investigate existing methods and observe a significant accumulation of quantization errors caused by frequent bit-width switching of weights and activations, leading to limited performance. To address this issue, we propose MultiQuant, a novel method that utilizes a multi-branch topology for arbitrary bit-width quantization. MultiQuant duplicates the network body into multiple independent branches and quantizes the weights of each branch to a fixed 2-bit while retaining the input activations in the expected bit-width. This approach maintains the computational cost as the same while avoiding the switching of weight bit-widths, thereby substantially reducing errors in weight quantization. Additionally, we introduce an amortization branch selection strategy to distribute quantization errors caused by activation bit-width switching among branches to enhance performance. Finally, we design an in-place distillation strategy that facilitates guidance between branches to further enhance MultiQuant's performance. Extensive experiments demonstrate that MultiQuant achieves significant performance gains compared to existing arbitrary bit-width quantization methods. Code is at \url{https://github.com/zysxmu/MultiQuant}.



### Altered Topological Properties of Functional Brain Network Associated with Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2305.08159v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08159v2)
- **Published**: 2023-05-14 13:39:12+00:00
- **Updated**: 2023-05-16 03:34:03+00:00
- **Authors**: Yongcheng Yao
- **Comment**: 32 pages,17 figures, 5 tables,
- **Journal**: None
- **Summary**: Functional Magnetic Resonance Imaging (fMRI) is commonly utilized to study human brain activity, including abnormal functional properties related to neurodegenerative diseases. This study aims to investigate the differences in the topological properties of functional brain networks between individuals with Alzheimer's Disease (AD) and normal controls. A total of 590 subjects, consisting of 175 with AD dementia and 415 age-, gender-, and handedness-matched controls, were included. The topological properties of the brain network were quantified using graph-theory-based analyses. The results indicate abnormal network integration and segregation in the AD group. These findings enhance our understanding of AD pathophysiology from a functional brain network structure perspective and may aid in identifying AD biomarkers. Supplementary data to aid in the validation of this research are available at https://github.com/YongchengYAO/AD-FunctionalBrainNetwork.



### TSGN: Temporal Scene Graph Neural Networks with Projected Vectorized Representation for Multi-Agent Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2305.08190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08190v1)
- **Published**: 2023-05-14 15:58:55+00:00
- **Updated**: 2023-05-14 15:58:55+00:00
- **Authors**: Yunong Wu, Thomas Gilles, Bogdan Stanciulescu, Fabien Moutarde
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Predicting future motions of nearby agents is essential for an autonomous vehicle to take safe and effective actions. In this paper, we propose TSGN, a framework using Temporal Scene Graph Neural Networks with projected vectorized representations for multi-agent trajectory prediction. Projected vectorized representation models the traffic scene as a graph which is constructed by a set of vectors. These vectors represent agents, road network, and their spatial relative relationships. All relative features under this representation are both translationand rotation-invariant. Based on this representation, TSGN captures the spatial-temporal features across agents, road network, interactions among them, and temporal dependencies of temporal traffic scenes. TSGN can predict multimodal future trajectories for all agents simultaneously, plausibly, and accurately. Meanwhile, we propose a Hierarchical Lane Transformer for capturing interactions between agents and road network, which filters the surrounding road network and only keeps the most probable lane segments which could have an impact on the future behavior of the target agent. Without sacrificing the prediction performance, this greatly reduces the computational burden. Experiments show TSGN achieves state-of-the-art performance on the Argoverse motion forecasting benchmar.



### Is end-to-end learning enough for fitness activity recognition?
- **Arxiv ID**: http://arxiv.org/abs/2305.08191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.08191v1)
- **Published**: 2023-05-14 16:00:03+00:00
- **Updated**: 2023-05-14 16:00:03+00:00
- **Authors**: Antoine Mercier, Guillaume Berger, Sunny Panchal, Florian Letsch, Cornelius Boehm, Nahua Kang, Ingo Bax, Roland Memisevic
- **Comment**: 9 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: End-to-end learning has taken hold of many computer vision tasks, in particular, related to still images, with task-specific optimization yielding very strong performance. Nevertheless, human-centric action recognition is still largely dominated by hand-crafted pipelines, and only individual components are replaced by neural networks that typically operate on individual frames. As a testbed to study the relevance of such pipelines, we present a new fully annotated video dataset of fitness activities. Any recognition capabilities in this domain are almost exclusively a function of human poses and their temporal dynamics, so pose-based solutions should perform well. We show that, with this labelled data, end-to-end learning on raw pixels can compete with state-of-the-art action recognition pipelines based on pose estimation. We also show that end-to-end learning can support temporally fine-grained tasks such as real-time repetition counting.



### Diffusion Models for Imperceptible and Transferable Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2305.08192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08192v1)
- **Published**: 2023-05-14 16:02:36+00:00
- **Updated**: 2023-05-14 16:02:36+00:00
- **Authors**: Jianqi Chen, Hao Chen, Keyan Chen, Yilan Zhang, Zhengxia Zou, Zhenwei Shi
- **Comment**: Code Page: https://github.com/WindVChen/DiffAttack
- **Journal**: None
- **Summary**: Many existing adversarial attacks generate $L_p$-norm perturbations on image RGB space. Despite some achievements in transferability and attack success rate, the crafted adversarial examples are easily perceived by human eyes. Towards visual imperceptibility, some recent works explore unrestricted attacks without $L_p$-norm constraints, yet lacking transferability of attacking black-box models. In this work, we propose a novel imperceptible and transferable attack by leveraging both the generative and discriminative power of diffusion models. Specifically, instead of direct manipulation in pixel space, we craft perturbations in latent space of diffusion models. Combined with well-designed content-preserving structures, we can generate human-insensitive perturbations embedded with semantic clues. For better transferability, we further "deceive" the diffusion model which can be viewed as an additional recognition surrogate, by distracting its attention away from the target regions. To our knowledge, our proposed method, DiffAttack, is the first that introduces diffusion models into adversarial attack field. Extensive experiments on various model structures (including CNNs, Transformers, MLPs) and defense methods have demonstrated our superiority over other attack methods.



### A Comprehensive Survey on Segment Anything Model for Vision and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2305.08196v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.08196v2)
- **Published**: 2023-05-14 16:23:22+00:00
- **Updated**: 2023-05-19 16:33:03+00:00
- **Authors**: Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu
- **Comment**: 28 pages, Homepage:
  https://github.com/liliu-avril/Awesome-Segment-Anything
- **Journal**: None
- **Summary**: Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing its historical development, recent progress, and profound impact on broad applications. We first introduce the background and terminology for foundation models including SAM, as well as state-of-the-art methods contemporaneous with SAM that are significant for segmenting anything task. Then, we analyze and summarize the advantages and limitations of SAM across various image processing applications, including software scenes, real-world scenes, and complex scenes. Importantly, many insights are drawn to guide future research to develop more versatile foundation models and improve the architecture of SAM. We also summarize massive other amazing applications of SAM in vision and beyond. Finally, we maintain a continuously updated paper list and an open-source project summary for foundation model SAM at \href{https://github.com/liliu-avril/Awesome-Segment-Anything}{\color{magenta}{here}}.



### Learning Structure Aware Deep Spectral Embedding
- **Arxiv ID**: http://arxiv.org/abs/2305.08215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08215v1)
- **Published**: 2023-05-14 18:18:05+00:00
- **Updated**: 2023-05-14 18:18:05+00:00
- **Authors**: Hira Yaseen, Arif Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral Embedding (SE) has often been used to map data points from non-linear manifolds to linear subspaces for the purpose of classification and clustering. Despite significant advantages, the subspace structure of data in the original space is not preserved in the embedding space. To address this issue subspace clustering has been proposed by replacing the SE graph affinity with a self-expression matrix. It works well if the data lies in a union of linear subspaces however, the performance may degrade in real-world applications where data often spans non-linear manifolds. To address this problem we propose a novel structure-aware deep spectral embedding by combining a spectral embedding loss and a structure preservation loss. To this end, a deep neural network architecture is proposed that simultaneously encodes both types of information and aims to generate structure-aware spectral embedding. The subspace structure of the input data is encoded by using attention-based self-expression learning. The proposed algorithm is evaluated on six publicly available real-world datasets. The results demonstrate the excellent clustering performance of the proposed algorithm compared to the existing state-of-the-art methods. The proposed algorithm has also exhibited better generalization to unseen data points and it is scalable to larger datasets without requiring significant computational resources.



### Skeleton Graph-based Ultrasound-CT Non-rigid Registration
- **Arxiv ID**: http://arxiv.org/abs/2305.08228v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.08228v1)
- **Published**: 2023-05-14 19:21:43+00:00
- **Updated**: 2023-05-14 19:21:43+00:00
- **Authors**: Zhongliang Jiang, Xuesong Li, Chenyu Zhang, Yuan Bi, Walter Stechele, Nassir Navab
- **Comment**: online video: https://www.youtube.com/watch?v=LkSHL7FJ8eU
- **Journal**: None
- **Summary**: Autonomous ultrasound (US) scanning has attracted increased attention, and it has been seen as a potential solution to overcome the limitations of conventional US examinations, such as inter-operator variations. However, it is still challenging to autonomously and accurately transfer a planned scan trajectory on a generic atlas to the current setup for different patients, particularly for thorax applications with limited acoustic windows. To address this challenge, we proposed a skeleton graph-based non-rigid registration to adapt patient-specific properties using subcutaneous bone surface features rather than the skin surface. To this end, the self-organization mapping is successively used twice to unify the input point cloud and extract the key points, respectively. Afterward, the minimal spanning tree is employed to generate a tree graph to connect all extracted key points. To appropriately characterize the rib cartilage outline to match the source and target point cloud, the path extracted from the tree graph is optimized by maximally maintaining continuity throughout each rib. To validate the proposed approach, we manually extract the US cartilage point cloud from one volunteer and seven CT cartilage point clouds from different patients. The results demonstrate that the proposed graph-based registration is more effective and robust in adapting to the inter-patient variations than the ICP (distance error mean/SD: 5.0/1.9 mm vs 8.6/6.7 mm on seven CTs).



### A Hybrid 3D Eddy Detection Technique Based on Sea Surface Height and Velocity Field
- **Arxiv ID**: http://arxiv.org/abs/2305.08229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08229v2)
- **Published**: 2023-05-14 19:22:51+00:00
- **Updated**: 2023-06-26 18:48:59+00:00
- **Authors**: Weiping Hua, Karen Bemis, Dujuan Kang, Sedat Ozer, Deborah Silver
- **Comment**: 8 pages, 14 figures. Accepted by EnvirVis 2023. Project Link:
  https://github.com/VizlabRutgers/Feature_Tracking
- **Journal**: None
- **Summary**: Eddy detection is a critical task for ocean scientists to understand and analyze ocean circulation. In this paper, we introduce a hybrid eddy detection approach that combines sea surface height (SSH) and velocity fields with geometric criteria defining eddy behavior. Our approach searches for SSH minima and maxima, which oceanographers expect to find at the center of eddies. Geometric criteria are used to verify expected velocity field properties, such as net rotation and symmetry, by tracing velocity components along a circular path surrounding each eddy center. Progressive searches outward and into deeper layers yield each eddy's 3D region of influence. Isolation of each eddy structure from the dataset, using it's cylindrical footprint, facilitates visualization of internal eddy structures using horizontal velocity, vertical velocity, temperature and salinity. A quantitative comparison of Okubo-Weiss vorticity (OW) thresholding, the standard winding angle, and this new SSH-velocity hybrid methods of eddy detection as applied to the Red Sea dataset suggests that detection results are highly dependent on the choices of method, thresholds, and criteria. Our new SSH-velocity hybrid detection approach has the advantages of providing eddy structures with verified rotation properties, 3D visualization of the internal structure of physical properties, and rapid efficient estimations of eddy footprints without calculating streamlines. Our approach combines visualization of internal structure and tracking overall movement to support the study of the transport mechanisms key to understanding the interaction of nutrient distribution and ocean circulation. Our method is applied to three different datasets to showcase the generality of its application.



### Combining geolocation and height estimation of objects from street level imagery
- **Arxiv ID**: http://arxiv.org/abs/2305.08232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08232v1)
- **Published**: 2023-05-14 19:40:02+00:00
- **Updated**: 2023-05-14 19:40:02+00:00
- **Authors**: Matej Ulicny, Vladimir A. Krylov, Julie Connelly, Rozenn Dahyot
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a pipeline for combined multi-class object geolocation and height estimation from street level RGB imagery, which is considered as a single available input data modality. Our solution is formulated via Markov Random Field optimization with deterministic output. The proposed technique uses image metadata along with coordinates of objects detected in the image plane as found by a custom-trained Convolutional Neural Network. Computing the object height using our methodology, in addition to object geolocation, has negligible effect on the overall computational cost. Accuracy is demonstrated experimentally for water drains and road signs on which we achieve average elevation estimation error lower than 20cm.



### Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity
- **Arxiv ID**: http://arxiv.org/abs/2305.08252v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.08252v3)
- **Published**: 2023-05-14 21:18:18+00:00
- **Updated**: 2023-05-24 12:28:21+00:00
- **Authors**: Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales
- **Comment**: None
- **Journal**: None
- **Summary**: We present a comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) techniques for diverse medical image analysis tasks. PEFT is increasingly exploited as a valuable approach for knowledge transfer from pre-trained models in natural language processing, vision, speech, and cross-modal tasks, such as vision-language and text-to-image generation. However, its application in medical image analysis remains relatively unexplored. As foundation models are increasingly exploited in the medical domain, it is crucial to investigate and comparatively assess various strategies for knowledge transfer that can bolster a range of downstream tasks. Our study, the first of its kind (to the best of our knowledge), evaluates 16 distinct PEFT methodologies proposed for convolutional and transformer-based networks, focusing on image classification and text-to-image generation tasks across six medical datasets ranging in size, modality, and complexity. Through a battery of more than 600 controlled experiments, we demonstrate performance gains of up to 22% under certain scenarios and demonstrate the efficacy of PEFT for medical text-to-image generation. Further, we reveal the instances where PEFT methods particularly dominate over conventional fine-tuning approaches by studying their relationship with downstream data volume.



### Vehicle Detection and Classification without Residual Calculation: Accelerating HEVC Image Decoding with Random Perturbation Injection
- **Arxiv ID**: http://arxiv.org/abs/2305.08265v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T20, E.4; I.4.5; H.3.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2305.08265v3)
- **Published**: 2023-05-14 22:04:00+00:00
- **Updated**: 2023-08-05 12:53:09+00:00
- **Authors**: Muhammet Sebul Beratoğlu, Behçet Uğur Töreyin
- **Comment**: 10 pages 4 figures
- **Journal**: None
- **Summary**: In the field of video analytics, particularly traffic surveillance, there is a growing need for efficient and effective methods for processing and understanding video data. Traditional full video decoding techniques can be computationally intensive and time-consuming, leading researchers to explore alternative approaches in the compressed domain. This study introduces a novel random perturbation-based compressed domain method for reconstructing images from High Efficiency Video Coding (HEVC) bitstreams, specifically designed for traffic surveillance applications. To the best of our knowledge, our method is the first to propose substituting random perturbations for residual values, creating a condensed representation of the original image while retaining information relevant to video understanding tasks, particularly focusing on vehicle detection and classification as key use cases.   By not using residual data, our proposed method significantly reduces the data needed in the image reconstruction process, allowing for more efficient storage and transmission of information. This is particularly important when considering the vast amount of video data involved in surveillance applications. Applied to the public BIT-Vehicle dataset, we demonstrate a significant increase in the reconstruction speed compared to the traditional full decoding approach, with our proposed method being approximately 56% faster than the pixel domain method. Additionally, we achieve a detection accuracy of 99.9%, on par with the pixel domain method, and a classification accuracy of 96.84%, only 0.98% lower than the pixel domain method. Furthermore, we showcase the significant reduction in data size, leading to more efficient storage and transmission. Our research establishes the potential of compressed domain methods in traffic surveillance applications, where speed and data size are critical factors.



### ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding
- **Arxiv ID**: http://arxiv.org/abs/2305.08275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08275v2)
- **Published**: 2023-05-14 23:14:09+00:00
- **Updated**: 2023-05-18 19:43:03+00:00
- **Authors**: Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in multimodal pre-training methods have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing multimodal pre-training frameworks to gather multimodal data for 3D applications lack scalability and comprehensiveness, potentially constraining the full potential of multimodal learning. The main bottleneck lies in the language modality's scalability and comprehensiveness. To address this, we introduce ULIP-2, a tri-modal pre-training framework that leverages state-of-the-art large multimodal models to automatically generate holistic language counterparts for 3D objects. It does not require any 3D annotations, and is therefore scalable to large datasets. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. ULIP-2 achieves significant improvements on downstream zero-shot classification on ModelNet40 (74.0% in top-1 accuracy); on the real-world ScanObjectNN benchmark, it obtains 91.5% in overall accuracy with only 1.4 million parameters, signifying a breakthrough in scalable multimodal 3D representation learning without human 3D annotations. The code, along with the generated tri-modal datasets, can be found at https://github.com/salesforce/ULIP.



