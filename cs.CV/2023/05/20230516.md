# Arxiv Papers in cs.CV on 2023-05-16
### Consensus and Subjectivity of Skin Tone Annotation for ML Fairness
- **Arxiv ID**: http://arxiv.org/abs/2305.09073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2305.09073v1)
- **Published**: 2023-05-16 00:03:09+00:00
- **Updated**: 2023-05-16 00:03:09+00:00
- **Authors**: Candice Schumann, Gbolahan O. Olanubi, Auriel Wright, Ellis Monk Jr., Courtney Heldreth, Susanna Ricco
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in computer vision fairness have relied on datasets augmented with perceived attribute signals (e.g. gender presentation, skin tone, and age) and benchmarks enabled by these datasets. Typically labels for these tasks come from human annotators. However, annotating attribute signals, especially skin tone, is a difficult and subjective task. Perceived skin tone is affected by technical factors, like lighting conditions, and social factors that shape an annotator's lived experience. This paper examines the subjectivity of skin tone annotation through a series of annotation experiments using the Monk Skin Tone (MST) scale, a small pool of professional photographers, and a much larger pool of trained crowdsourced annotators. Our study shows that annotators can reliably annotate skin tone in a way that aligns with an expert in the MST scale, even under challenging environmental conditions. We also find evidence that annotators from different geographic regions rely on different mental models of MST categories resulting in annotations that systematically vary across regions. Given this, we advise practitioners to use a diverse set of annotators and a higher replication count for each image when annotating skin tone for fairness research.



### PanelNet: Understanding 360 Indoor Environment via Panel Representation
- **Arxiv ID**: http://arxiv.org/abs/2305.09078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09078v1)
- **Published**: 2023-05-16 00:37:58+00:00
- **Updated**: 2023-05-16 00:37:58+00:00
- **Authors**: Haozheng Yu, Lu He, Bing Jian, Weiwei Feng, Shan Liu
- **Comment**: To appear in CVPR 2023
- **Journal**: None
- **Summary**: Indoor 360 panoramas have two essential properties. (1) The panoramas are continuous and seamless in the horizontal direction. (2) Gravity plays an important role in indoor environment design. By leveraging these properties, we present PanelNet, a framework that understands indoor environments using a novel panel representation of 360 images. We represent an equirectangular projection (ERP) as consecutive vertical panels with corresponding 3D panel geometry. To reduce the negative impact of panoramic distortion, we incorporate a panel geometry embedding network that encodes both the local and global geometric features of a panel. To capture the geometric context in room design, we introduce Local2Global Transformer, which aggregates local information within a panel and panel-wise global context. It greatly improves the model performance with low training overhead. Our method outperforms existing methods on indoor 360 depth estimation and shows competitive results against state-of-the-art approaches on the task of indoor layout estimation and semantic segmentation.



### ProtoVAE: Prototypical Networks for Unsupervised Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2305.09092v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09092v1)
- **Published**: 2023-05-16 01:29:26+00:00
- **Updated**: 2023-05-16 01:29:26+00:00
- **Authors**: Vaishnavi Patil, Matthew Evanusa, Joseph JaJa
- **Comment**: None
- **Journal**: None
- **Summary**: Generative modeling and self-supervised learning have in recent years made great strides towards learning from data in a completely unsupervised way. There is still however an open area of investigation into guiding a neural network to encode the data into representations that are interpretable or explainable. The problem of unsupervised disentanglement is of particular importance as it proposes to discover the different latent factors of variation or semantic concepts from the data alone, without labeled examples, and encode them into structurally disjoint latent representations. Without additional constraints or inductive biases placed in the network, a generative model may learn the data distribution and encode the factors, but not necessarily in a disentangled way. Here, we introduce a novel deep generative VAE-based model, ProtoVAE, that leverages a deep metric learning Prototypical network trained using self-supervision to impose these constraints. The prototypical network constrains the mapping of the representation space to data space to ensure that controlled changes in the representation space are mapped to changes in the factors of variations in the data space. Our model is completely unsupervised and requires no a priori knowledge of the dataset, including the number of factors. We evaluate our proposed model on the benchmark dSprites, 3DShapes, and MPI3D disentanglement datasets, showing state of the art results against previous methods via qualitative traversals in the latent space, as well as quantitative disentanglement metrics. We further qualitatively demonstrate the effectiveness of our model on the real-world CelebA dataset.



### Multi-view MERA Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2305.09095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09095v1)
- **Published**: 2023-05-16 01:41:10+00:00
- **Updated**: 2023-05-16 01:41:10+00:00
- **Authors**: Zhen Long, Ce Zhu, Jie Chen, Zihan Li, Yazhou Ren, Yipeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor-based multi-view subspace clustering (MSC) can capture high-order correlation in the self-representation tensor. Current tensor decompositions for MSC suffer from highly unbalanced unfolding matrices or rotation sensitivity, failing to fully explore inter/intra-view information. Using the advanced tensor network, namely, multi-scale entanglement renormalization ansatz (MERA), we propose a low-rank MERA based MSC (MERA-MSC) algorithm, where MERA factorizes a tensor into contractions of one top core factor and the rest orthogonal/semi-orthogonal factors. Benefiting from multiple interactions among orthogonal/semi-orthogonal (low-rank) factors, the low-rank MERA has a strong representation power to capture the complex inter/intra-view information in the self-representation tensor. The alternating direction method of multipliers is adopted to solve the optimization model. Experimental results on five multi-view datasets demonstrate MERA-MSC has superiority against the compared algorithms on six evaluation metrics. Furthermore, we extend MERA-MSC by incorporating anchor learning to develop a scalable low-rank MERA based multi-view clustering method (sMREA-MVC). The effectiveness and efficiency of sMERA-MVC have been validated on three large-scale multi-view datasets. To our knowledge, this is the first work to introduce MERA to the multi-view clustering topic. The codes of MERA-MSC and sMERA-MVC are publicly available at https://github.com/longzhen520/MERA-MSC.



### Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2305.09107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.09107v1)
- **Published**: 2023-05-16 02:12:57+00:00
- **Updated**: 2023-05-16 02:12:57+00:00
- **Authors**: Chenyang Lyu, Tianbo Ji, Yvette Graham, Jennifer Foster
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and inference. We believe the proposed approach can facilitate VideoQA-related research by reducing the computational requirements for those who have limited access to budgets and resources. Our code will be made publicly available for research use.



### A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2305.09121v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09121v2)
- **Published**: 2023-05-16 03:00:04+00:00
- **Updated**: 2023-08-29 06:11:58+00:00
- **Authors**: Ruoqi Wang, Zhuoyang Chen, Qiong Luo, Feng Wang
- **Comment**: Accepted by ECAI 2023
- **Journal**: None
- **Summary**: In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources, preserving detailed structures, and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to use both the original visibility data in the spectral domain and dirty images in the spatial domain to guide the image generation process with DDPM. This way, we can leverage DDPM to generate fine details and eliminate noise, while utilizing visibility data to separate signals from noise and retaining spatial information in dirty images. We have conducted experiments in comparison with both traditional methods and recent deep learning based approaches. Our results show that our method significantly improves the resulting images by reducing artifacts, preserving fine details, and recovering dim sources. This advancement further facilitates radio astronomical data analysis tasks on celestial phenomena.



### DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2305.09132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09132v1)
- **Published**: 2023-05-16 03:25:38+00:00
- **Updated**: 2023-05-16 03:25:38+00:00
- **Authors**: Pengcheng Shi, Haozhe Cheng, Xu Han, Yiyang Zhou, Jihua Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud completion estimates complete shapes from incomplete point clouds to obtain higher-quality point cloud data. Most existing methods only consider global object features, ignoring spatial and semantic information of adjacent points. They cannot distinguish structural information well between different object parts, and the robustness of models is poor. To tackle these challenges, we propose an information interaction-based generative network for point cloud completion ($\mathbf{DualGenerator}$). It contains an adversarial generation path and a variational generation path, which interact with each other and share weights. DualGenerator introduces a local refinement module in generation paths, which captures general structures from partial inputs, and then refines shape details of the point cloud. It promotes completion in the unknown region and makes a distinction between different parts more obvious. Moreover, we design DGStyleGAN to improve the generation quality further. It promotes the robustness of this network combined with fusion analysis of dual-path completion results. Qualitative and quantitative evaluations demonstrate that our method is superior on MVP and Completion3D datasets. The performance will not degrade significantly after adding noise interference or sparse sampling.



### Deep Ensembling for Perceptual Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2305.09141v1
- **DOI**: 10.1007/s00500-021-06662-9
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09141v1)
- **Published**: 2023-05-16 03:45:02+00:00
- **Updated**: 2023-05-16 03:45:02+00:00
- **Authors**: Nisar Ahmed, H. M. Shahzad Asif, Abdul Rauf Bhatti, Atif Khan
- **Comment**: None
- **Journal**: Soft Comput 26, 7601 to 7622 (2022)
- **Summary**: Blind image quality assessment is a challenging task particularly due to the unavailability of reference information. Training a deep neural network requires a large amount of training data which is not readily available for image quality. Transfer learning is usually opted to overcome this limitation and different deep architectures are used for this purpose as they learn features differently. After extensive experiments, we have designed a deep architecture containing two CNN architectures as its sub-units. Moreover, a self-collected image database BIQ2021 is proposed with 12,000 images having natural distortions. The self-collected database is subjectively scored and is used for model training and validation. It is demonstrated that synthetic distortion databases cannot provide generalization beyond the distortion types used in the database and they are not ideal candidates for general-purpose image quality assessment. Moreover, a large-scale database of 18.75 million images with synthetic distortions is used to pretrain the model and then retrain it on benchmark databases for evaluation. Experiments are conducted on six benchmark databases three of which are synthetic distortion databases (LIVE, CSIQ and TID2013) and three are natural distortion databases (LIVE Challenge Database, CID2013 and KonIQ-10 k). The proposed approach has provided a Pearson correlation coefficient of 0.8992, 0.8472 and 0.9452 subsequently and Spearman correlation coefficient of 0.8863, 0.8408 and 0.9421. Moreover, the performance is demonstrated using perceptually weighted rank correlation to indicate the perceptual superiority of the proposed approach. Multiple experiments are conducted to validate the generalization performance of the proposed model by training on different subsets of the databases and validating on the test subset of BIQ2021 database.



### Deep ReLU Networks Have Surprisingly Simple Polytopes
- **Arxiv ID**: http://arxiv.org/abs/2305.09145v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.09145v1)
- **Published**: 2023-05-16 03:51:34+00:00
- **Updated**: 2023-05-16 03:51:34+00:00
- **Authors**: Feng-Lei Fan, Wei Huang, Xiangru Zhong, Lecheng Ruan, Tieyong Zeng, Huan Xiong, Fei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the average number of faces of polytopes with a function of the dimensionality. Our results concretely reveal what kind of simple functions a network learns and its space partition property. Also, by characterizing the shape of polytopes, the number of simplices be a leverage for other problems, \textit{e.g.}, serving as a generic functional complexity measure to explain the power of popular shortcut networks such as ResNet and analyzing the impact of different regularization strategies on a network's space partition.



### Self-Aware Trajectory Prediction for Safe Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2305.09147v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09147v1)
- **Published**: 2023-05-16 03:53:23+00:00
- **Updated**: 2023-05-16 03:53:23+00:00
- **Authors**: Wenbo Shao, Jun Li, Hong Wang
- **Comment**: Accepted by IEEE Intelligent Vehicles Symposium 2023 (IV 2023)
- **Journal**: None
- **Summary**: Trajectory prediction is one of the key components of the autonomous driving software stack. Accurate prediction for the future movement of surrounding traffic participants is an important prerequisite for ensuring the driving efficiency and safety of intelligent vehicles. Trajectory prediction algorithms based on artificial intelligence have been widely studied and applied in recent years and have achieved remarkable results. However, complex artificial intelligence models are uncertain and difficult to explain, so they may face unintended failures when applied in the real world. In this paper, a self-aware trajectory prediction method is proposed. By introducing a self-awareness module and a two-stage training process, the original trajectory prediction module's performance is estimated online, to facilitate the system to deal with the possible scenario of insufficient prediction function in time, and create conditions for the realization of safe and reliable autonomous driving. Comprehensive experiments and analysis are performed, and the proposed method performed well in terms of self-awareness, memory footprint, and real-time performance, showing that it may serve as a promising paradigm for safe autonomous driving.



### SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.09160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.09160v2)
- **Published**: 2023-05-16 04:36:04+00:00
- **Updated**: 2023-07-27 04:36:15+00:00
- **Authors**: Siyuan Huang, Bo Zhang, Botian Shi, Peng Gao, Yikang Li, Hongsheng Li
- **Comment**: Accepted by ACM MM-2023, and our code is available at
  https://github.com/SiyuanHuang95/SUG
- **Journal**: None
- **Summary**: Although Domain Generalization (DG) problem has been fast-growing in the 2D image tasks, its exploration on 3D point cloud data is still insufficient and challenged by more complex and uncertain cross-domain variances with uneven inter-class modality distribution. In this paper, different from previous 2D DG works, we focus on the 3D DG problem and propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA) method, which can constrain the learned representations to be domain-agnostic and discriminative, by performing a multi-grained feature alignment process between the splitted sub-domains from the single source dataset. Then, a Sample-level Domain-aware Attention (SDA) strategy is presented, which can selectively enhance easy-to-adapt samples from different sub-domains according to the sample-level inter-domain distance to avoid the negative transfer. Experiments demonstrate that our SUG can boost the generalization ability for unseen target domains, even outperforming the existing unsupervised domain adaptation methods that have to access extensive target domain data. Our code is available at https://github.com/SiyuanHuang95/SUG.



### Lightweight Self-Knowledge Distillation with Multi-source Information Fusion
- **Arxiv ID**: http://arxiv.org/abs/2305.09183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09183v1)
- **Published**: 2023-05-16 05:46:31+00:00
- **Updated**: 2023-05-16 05:46:31+00:00
- **Authors**: Xucong Wang, Pengchao Han, Lei Guo
- **Comment**: Submitted to IEEE TNNLS
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) is a powerful technique for transferring knowledge between neural network models, where a pre-trained teacher model is used to facilitate the training of the target student model. However, the availability of a suitable teacher model is not always guaranteed. To address this challenge, Self-Knowledge Distillation (SKD) attempts to construct a teacher model from itself. Existing SKD methods add Auxiliary Classifiers (AC) to intermediate layers of the model or use the history models and models with different input data within the same class. However, these methods are computationally expensive and only capture time-wise and class-wise features of data. In this paper, we propose a lightweight SKD framework that utilizes multi-source information to construct a more informative teacher. Specifically, we introduce a Distillation with Reverse Guidance (DRG) method that considers different levels of information extracted by the model, including edge, shape, and detail of the input data, to construct a more informative teacher. Additionally, we design a Distillation with Shape-wise Regularization (DSR) method that ensures a consistent shape of ranked model output for all data. We validate the performance of the proposed DRG, DSR, and their combination through comprehensive experiments on various datasets and models. Our results demonstrate the superiority of the proposed methods over baselines (up to 2.87%) and state-of-the-art SKD methods (up to 1.15%), while being computationally efficient and robust. The code is available at https://github.com/xucong-parsifal/LightSKD.



### Abnormal Functional Brain Network Connectivity Associated with Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2305.09186v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09186v1)
- **Published**: 2023-05-16 05:50:47+00:00
- **Updated**: 2023-05-16 05:50:47+00:00
- **Authors**: Yongcheng Yao
- **Comment**: 23 pages, 19 figures, 1 table
- **Journal**: None
- **Summary**: The study's objective is to explore the distinctions in the functional brain network connectivity between Alzheimer's Disease (AD) patients and normal controls using Functional Magnetic Resonance Imaging (fMRI). The study included 590 individuals, with 175 having AD dementia and 415 age-, gender-, and handedness-matched normal controls. The connectivity of functional brain networks was measured using ROI-to-ROI and ROI-to-Voxel connectivity analyses. The findings reveal a general decrease in functional connectivity among the AD group in comparison to the normal control group. These results advance our comprehension of AD pathophysiology and could assist in identifying AD biomarkers.



### Correlation Pyramid Network for 3D Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2305.09195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09195v1)
- **Published**: 2023-05-16 06:07:20+00:00
- **Updated**: 2023-05-16 06:07:20+00:00
- **Authors**: Mengmeng Wang, Teli Ma, Xingxing Zuo, Jiajun Lv, Yong Liu
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition 2023,
  workshop
- **Summary**: 3D LiDAR-based single object tracking (SOT) has gained increasing attention as it plays a crucial role in 3D applications such as autonomous driving. The central problem is how to learn a target-aware representation from the sparse and incomplete point clouds. In this paper, we propose a novel Correlation Pyramid Network (CorpNet) with a unified encoder and a motion-factorized decoder. Specifically, the encoder introduces multi-level self attentions and cross attentions in its main branch to enrich the template and search region features and realize their fusion and interaction, respectively. Additionally, considering the sparsity characteristics of the point clouds, we design a lateral correlation pyramid structure for the encoder to keep as many points as possible by integrating hierarchical correlated features. The output features of the search region from the encoder can be directly fed into the decoder for predicting target locations without any extra matcher. Moreover, in the decoder of CorpNet, we design a motion-factorized head to explicitly learn the different movement patterns of the up axis and the x-y plane together. Extensive experiments on two commonly-used datasets show our CorpNet achieves state-of-the-art results while running in real-time.



### Iterative Adversarial Attack on Image-guided Story Ending Generation
- **Arxiv ID**: http://arxiv.org/abs/2305.13208v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.13208v1)
- **Published**: 2023-05-16 06:19:03+00:00
- **Updated**: 2023-05-16 06:19:03+00:00
- **Authors**: Youze Wang, Wenbo Hu, Richang Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal learning involves developing models that can integrate information from various sources like images and texts. In this field, multimodal text generation is a crucial aspect that involves processing data from multiple modalities and outputting text. The image-guided story ending generation (IgSEG) is a particularly significant task, targeting on an understanding of complex relationships between text and image data with a complete story text ending. Unfortunately, deep neural networks, which are the backbone of recent IgSEG models, are vulnerable to adversarial samples. Current adversarial attack methods mainly focus on single-modality data and do not analyze adversarial attacks for multimodal text generation tasks that use cross-modal information. To this end, we propose an iterative adversarial attack method (Iterative-attack) that fuses image and text modality attacks, allowing for an attack search for adversarial text and image in an more effective iterative way. Experimental results demonstrate that the proposed method outperforms existing single-modal and non-iterative multimodal attack methods, indicating the potential for improving the adversarial robustness of multimodal text generation models, such as multimodal machine translation, multimodal question answering, etc.



### CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images
- **Arxiv ID**: http://arxiv.org/abs/2305.09211v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09211v3)
- **Published**: 2023-05-16 06:40:04+00:00
- **Updated**: 2023-07-19 10:52:30+00:00
- **Authors**: Momina Liaqat Ali, Zunaira Rauf, Asifullah Khan, Anabia Sohail, Rafi Ullah, Jeonghwan Gwak
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learning to extract diverse channels from different auxiliary learners. In the CB HVT, these boosted channels are first concatenated and ranked using an attention mechanism in the channel exploitation module. A fusion block is then utilized in the channel merging module for a gradual and systematic merging of the diverse boosted channels to improve the network's learning representations. The CB HVT also employs a proposal network in its region aware module and a head to effectively identify objects, even in overlapping regions and with artifacts. We evaluated the proposed CB HVT on two publicly available datasets for lymphocyte assessment in histopathological images. The results show that CB HVT outperformed other state of the art detection models, and has good generalization ability, demonstrating its value as a tool for pathologists.



### Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.09212v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2305.09212v1)
- **Published**: 2023-05-16 06:41:25+00:00
- **Updated**: 2023-05-16 06:41:25+00:00
- **Authors**: Yuchen Hu, Ruizhe Li, Chen Chen, Heqing Zou, Qiushi Zhu, Eng Siong Chng
- **Comment**: 12 pages, 5 figures, Accepted by IJCAI 2023
- **Journal**: None
- **Summary**: Audio-visual speech recognition (AVSR) research has gained a great success recently by improving the noise-robustness of audio-only automatic speech recognition (ASR) with noise-invariant visual information. However, most existing AVSR approaches simply fuse the audio and visual features by concatenation, without explicit interactions to capture the deep correlations between them, which results in sub-optimal multimodal representations for downstream speech recognition task. In this paper, we propose a cross-modal global interaction and local alignment (GILA) approach for AVSR, which captures the deep audio-visual (A-V) correlations from both global and local perspectives. Specifically, we design a global interaction model to capture the A-V complementary relationship on modality level, as well as a local alignment approach to model the A-V temporal consistency on frame level. Such a holistic view of cross-modal correlations enable better multimodal representations for AVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA outperforms the supervised learning state-of-the-art.



### PIQI: Perceptual Image Quality Index based on Ensemble of Gaussian Process Regression
- **Arxiv ID**: http://arxiv.org/abs/2305.09214v1
- **DOI**: 10.1007/s11042-020-10286-w
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09214v1)
- **Published**: 2023-05-16 06:44:17+00:00
- **Updated**: 2023-05-16 06:44:17+00:00
- **Authors**: Nisar Ahmed, Hafiz Muhammad Shahzad Asif, Hassan Khalid
- **Comment**: None
- **Journal**: AMultimed Tools Appl 80, 15677 to 15700 (2021)
- **Summary**: Digital images contain a lot of redundancies, therefore, compression techniques are applied to reduce the image size without loss of reasonable image quality. Same become more prominent in the case of videos which contains image sequences and higher compression ratios are achieved in low throughput networks. Assessment of quality of images in such scenarios has become of particular interest. Subjective evaluation in most of the scenarios is infeasible so objective evaluation is preferred. Among the three objective quality measures, full-reference and reduced-reference methods require an original image in some form to calculate the image quality which is unfeasible in scenarios such as broadcasting, acquisition or enhancement. Therefore, a no-reference Perceptual Image Quality Index (PIQI) is proposed in this paper to assess the quality of digital images which calculates luminance and gradient statistics along with mean subtracted contrast normalized products in multiple scales and color spaces. These extracted features are provided to a stacked ensemble of Gaussian Process Regression (GPR) to perform the perceptual quality evaluation. The performance of the PIQI is checked on six benchmark databases and compared with twelve state-of-the-art methods and competitive results are achieved. The comparison is made based on RMSE, Pearson and Spearman correlation coefficients between ground truth and predicted quality scores. The scores of 0.0552, 0.9802 and 0.9776 are achieved respectively for these metrics on CSIQ database. Two cross-dataset evaluation experiments are performed to check the generalization of PIQI.



### Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors
- **Arxiv ID**: http://arxiv.org/abs/2305.09222v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.09222v2)
- **Published**: 2023-05-16 06:58:11+00:00
- **Updated**: 2023-05-17 06:35:55+00:00
- **Authors**: Samuel Zühlke, Andreas Stöckl, David C. Schedl
- **Comment**: 8 pages, 3 figures, submitted to IHSED 2023
- **Journal**: None
- **Summary**: This study presents a novel approach for touch sensing using semi-elastic textile surfaces that does not require the placement of additional sensors in the sensing area, instead relying on sensors located on the border of the textile. The proposed approach is demonstrated through experiments involving an elastic Jersey fabric and a variety of machine-learning models. The performance of one particular border-based sensor design is evaluated in depth. By using visual markers, the best-performing visual sensor arrangement predicts a single touch point with a mean squared error of 1.36 mm on an area of 125mm by 125mm. We built a textile only prototype that is able to classify touch at three indent levels (0, 15, and 20 mm) with an accuracy of 82.85%. Our results suggest that this approach has potential applications in wearable technology and smart textiles, making it a promising avenue for further exploration in these fields.



### Mobile User Interface Element Detection Via Adaptively Prompt Tuning
- **Arxiv ID**: http://arxiv.org/abs/2305.09699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09699v1)
- **Published**: 2023-05-16 07:16:36+00:00
- **Updated**: 2023-05-16 07:16:36+00:00
- **Authors**: Zhangxuan Gu, Zhuoer Xu, Haoxing Chen, Jun Lan, Changhua Meng, Weiqiang Wang
- **Comment**: Accepted by CVPR23
- **Journal**: None
- **Summary**: Recent object detection approaches rely on pretrained vision-language models for image-text alignment. However, they fail to detect the Mobile User Interface (MUI) element since it contains additional OCR information, which describes its content and function but is often ignored. In this paper, we develop a new MUI element detection dataset named MUI-zh and propose an Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR information. APT is a lightweight and effective module to jointly optimize category prompts across different modalities. For every element, APT uniformly encodes its visual features and OCR descriptions to dynamically adjust the representation of frozen category prompts. We evaluate the effectiveness of our plug-and-play APT upon several existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments show that our method achieves considerable improvements on two datasets. The datasets is available at \url{github.com/antmachineintelligence/MUI-zh}.



### One-shot neural band selection for spectral recovery
- **Arxiv ID**: http://arxiv.org/abs/2305.09236v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09236v1)
- **Published**: 2023-05-16 07:34:03+00:00
- **Updated**: 2023-05-16 07:34:03+00:00
- **Authors**: Hai-Miao Hu, Zhenbo Xu, Wenshuai Xu, You Song, YiTao Zhang, Liu Liu, Zhilin Han, Ajin Meng
- **Comment**: Accepted by ICASSP 2023, any questions contact
  xuzhenbo@mail.ustc.edu.cn
- **Journal**: None
- **Summary**: Band selection has a great impact on the spectral recovery quality. To solve this ill-posed inverse problem, most band selection methods adopt hand-crafted priors or exploit clustering or sparse regularization constraints to find most prominent bands. These methods are either very slow due to the computational cost of repeatedly training with respect to different selection frequencies or different band combinations. Many traditional methods rely on the scene prior and thus are not applicable to other scenarios. In this paper, we present a novel one-shot Neural Band Selection (NBS) framework for spectral recovery. Unlike conventional searching approaches with a discrete search space and a non-differentiable search strategy, our NBS is based on the continuous relaxation of the band selection process, thus allowing efficient band search using gradient descent. To enable the compatibility for se- lecting any number of bands in one-shot, we further exploit the band-wise correlation matrices to progressively suppress similar adjacent bands. Extensive evaluations on the NTIRE 2022 Spectral Reconstruction Challenge demonstrate that our NBS achieves consistent performance gains over competitive baselines when examined with four different spectral recov- ery methods. Our code will be publicly available.



### Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples
- **Arxiv ID**: http://arxiv.org/abs/2305.09241v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09241v3)
- **Published**: 2023-05-16 07:40:05+00:00
- **Updated**: 2023-08-14 12:35:57+00:00
- **Authors**: Wan Jiang, Yunfeng Diao, He Wang, Jianxin Sun, Meng Wang, Richang Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs conditioned on the pixel and perceptual similarity between UEs and LEs. Extensive experiments demonstrate that LE delivers state-of-the-art countering performance against both supervised UEs and unsupervised UEs in various scenarios, which is the first generalizable countermeasure to UEs across supervised learning and unsupervised learning. Our code is available at \url{https://github.com/jiangw-0/LE_JCDP}.



### Online Continual Learning Without the Storage Constraint
- **Arxiv ID**: http://arxiv.org/abs/2305.09253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09253v1)
- **Published**: 2023-05-16 08:03:07+00:00
- **Updated**: 2023-05-16 08:03:07+00:00
- **Authors**: Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, Ozan Sener
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Online continual learning (OCL) research has primarily focused on mitigating catastrophic forgetting with fixed and limited storage allocation throughout the agent's lifetime. However, the growing affordability of data storage highlights a broad range of applications that do not adhere to these assumptions. In these cases, the primary concern lies in managing computational expenditures rather than storage. In this paper, we target such settings, investigating the online continual learning problem by relaxing storage constraints and emphasizing fixed, limited economical budget. We provide a simple algorithm that can compactly store and utilize the entirety of the incoming data stream under tiny computational budgets using a kNN classifier and universal pre-trained feature extractors. Our algorithm provides a consistency property attractive to continual learning: It will never forget past seen data. We set a new state of the art on two large-scale OCL datasets: Continual LOCalization (CLOC), which has 39M images over 712 classes, and Continual Google Landmarks V2 (CGLM), which has 580K images over 10,788 classes -- beating methods under far higher computational budgets than ours in terms of both reducing catastrophic forgetting of past data and quickly adapting to rapidly changing data streams. We provide code to reproduce our results at \url{https://github.com/drimpossible/ACM}.



### Accurate Gigapixel Crowd Counting by Iterative Zooming and Refinement
- **Arxiv ID**: http://arxiv.org/abs/2305.09271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09271v1)
- **Published**: 2023-05-16 08:25:27+00:00
- **Updated**: 2023-05-16 08:25:27+00:00
- **Authors**: Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing prevalence of gigapixel resolutions has presented new challenges for crowd counting. Such resolutions are far beyond the memory and computation limits of current GPUs, and available deep neural network architectures and training procedures are not designed for such massive inputs. Although several methods have been proposed to address these challenges, they are either limited to downsampling the input image to a small size, or borrowing from other gigapixel tasks, which are not tailored for crowd counting. In this paper, we propose a novel method called GigaZoom, which iteratively zooms into the densest areas of the image and refines coarser density maps with finer details. Through experiments, we show that GigaZoom obtains the state-of-the-art for gigapixel crowd counting and improves the accuracy of the next best method by 42%.



### Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?
- **Arxiv ID**: http://arxiv.org/abs/2305.09275v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09275v1)
- **Published**: 2023-05-16 08:29:33+00:00
- **Updated**: 2023-05-16 08:29:33+00:00
- **Authors**: Hasan Abed Al Kader Hammoud, Ameya Prabhu, Ser-Nam Lim, Philip H. S. Torr, Adel Bibi, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be achieved by retaining and reusing past seen information. We believe that our proposed metric can aid in the development of truly adaptive OCL methods. We provide code to reproduce our results at https://github.com/drimpossible/EvalOCL.



### Noise robust neural network architecture
- **Arxiv ID**: http://arxiv.org/abs/2305.09276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09276v1)
- **Published**: 2023-05-16 08:30:45+00:00
- **Updated**: 2023-05-16 08:30:45+00:00
- **Authors**: Xiong Yunuo, Xiong Hongwei
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.



### Latent Distribution Adjusting for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2305.09285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09285v1)
- **Published**: 2023-05-16 08:43:14+00:00
- **Updated**: 2023-05-16 08:43:14+00:00
- **Authors**: Qinghong Sun, Zhenfei Yin, Yichao Wu, Yuanhan Zhang, Jing Shao
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of deep learning, the field of face anti-spoofing (FAS) has witnessed great progress. FAS is usually considered a classification problem, where each class is assumed to contain a single cluster optimized by softmax loss. In practical deployment, one class can contain several local clusters, and a single-center is insufficient to capture the inherent structure of the FAS data. However, few approaches consider large distribution discrepancies in the field of FAS. In this work, we propose a unified framework called Latent Distribution Adjusting (LDA) with properties of latent, discriminative, adaptive, generic to improve the robustness of the FAS model by adjusting complex data distribution with multiple prototypes. 1) Latent. LDA attempts to model the data of each class as a Gaussian mixture distribution, and acquire a flexible number of centers for each class in the last fully connected layer implicitly. 2) Discriminative. To enhance the intra-class compactness and inter-class discrepancy, we propose a margin-based loss for providing distribution constrains for prototype learning. 3) Adaptive. To make LDA more efficient and decrease redundant parameters, we propose Adaptive Prototype Selection (APS) by selecting the appropriate number of centers adaptively according to different distributions. 4) Generic. Furthermore, LDA can adapt to unseen distribution by utilizing very few training data without re-training. Extensive experiments demonstrate that our framework can 1) make the final representation space both intra-class compact and inter-class separable, 2) outperform the state-of-the-art methods on multiple standard FAS benchmarks.



### Out-of-Distribution Detection for Adaptive Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2305.09293v1
- **DOI**: 10.1007/978-3-031-31438-4_21
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09293v1)
- **Published**: 2023-05-16 09:01:42+00:00
- **Updated**: 2023-05-16 09:01:42+00:00
- **Authors**: Simon Kristoffersson Lind, Rudolph Triebel, Luigi Nardi, Volker Krueger
- **Comment**: Published in Springer Lecture Notes for Computer Science Vol. 13886
  as part of the conference proceedings for Scandinavian Conference on Image
  Analysis 2023
- **Journal**: In: Gade, R., Felsberg, M., K\"am\"ar\"ainen, JK. (eds) Image
  Analysis. SCIA 2023. Lecture Notes in Computer Science, vol 13886. Springer,
  Cham
- **Summary**: It is well known that computer vision can be unreliable when faced with previously unseen imaging conditions. This paper proposes a method to adapt camera parameters according to a normalizing flow-based out-of-distibution detector. A small-scale study is conducted which shows that adapting camera parameters according to this out-of-distibution detector leads to an average increase of 3 to 4 percentage points in mAP, mAR and F1 performance metrics of a YOLOv4 object detector. As a secondary result, this paper also shows that it is possible to train a normalizing flow model for out-of-distribution detection on the COCO dataset, which is larger and more diverse than most benchmarks for out-of-distibution detectors.



### UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.09299v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.09299v1)
- **Published**: 2023-05-16 09:18:38+00:00
- **Updated**: 2023-05-16 09:18:38+00:00
- **Authors**: Heqing Zou, Meng Shen, Chen Chen, Yuchen Hu, Deepu Rajan, Eng Siong Chng
- **Comment**: ACL 2023 Findings
- **Journal**: None
- **Summary**: Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method outperforms current state-of-the-art multimodal methods. The detailed ablation study and analysis further demonstrate the advantage of our proposed method.



### Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions
- **Arxiv ID**: http://arxiv.org/abs/2305.09302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2305.09302v1)
- **Published**: 2023-05-16 09:21:56+00:00
- **Updated**: 2023-05-16 09:21:56+00:00
- **Authors**: Di Xu, Yang Zhao, Xiang Hao, Xin Meng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel dataset consisting of images depicting pink eggs that have been identified as Pomacea canaliculata eggs, accompanied by corresponding bounding box annotations. The purpose of this dataset is to aid researchers in the analysis of the spread of Pomacea canaliculata species by utilizing deep learning techniques, as well as supporting other investigative pursuits that require visual data pertaining to the eggs of Pomacea canaliculata. It is worth noting, however, that the identity of the eggs in question is not definitively established, as other species within the same taxonomic family have been observed to lay similar-looking eggs in regions of the Americas. Therefore, a crucial prerequisite to any decision regarding the elimination of these eggs would be to establish with certainty whether they are exclusively attributable to invasive Pomacea canaliculata or if other species are also involved. The dataset is available at https://www.kaggle.com/datasets/deeshenzhen/pinkeggs



### Releasing Inequality Phenomena in $L_{\infty}$-Adversarial Training via Input Gradient Distillation
- **Arxiv ID**: http://arxiv.org/abs/2305.09305v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09305v2)
- **Published**: 2023-05-16 09:23:42+00:00
- **Updated**: 2023-05-17 15:03:17+00:00
- **Authors**: Junxi Chen, Junhao Dong, Xiaohua Xie
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Since adversarial examples appeared and showed the catastrophic degradation they brought to DNN, many adversarial defense methods have been devised, among which adversarial training is considered the most effective. However, a recent work showed the inequality phenomena in $l_{\infty}$-adversarial training and revealed that the $l_{\infty}$-adversarially trained model is vulnerable when a few important pixels are perturbed by i.i.d. noise or occluded. In this paper, we propose a simple yet effective method called Input Gradient Distillation (IGD) to release the inequality phenomena in $l_{\infty}$-adversarial training. Experiments show that while preserving the model's adversarial robustness, compared to PGDAT, IGD decreases the $l_{\infty}$-adversarially trained model's error rate to inductive noise and inductive occlusion by up to 60\% and 16.53\%, and to noisy images in Imagenet-C by up to 21.11\%. Moreover, we formally explain why the equality of the model's saliency map can improve such robustness.



### Improved Type III solar radio burst detection using congruent deep learning models
- **Arxiv ID**: http://arxiv.org/abs/2305.09327v1
- **DOI**: 10.1051/0004-6361/202346404
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09327v1)
- **Published**: 2023-05-16 10:04:30+00:00
- **Updated**: 2023-05-16 10:04:30+00:00
- **Authors**: Jeremiah Scully, Ronan Flynn, Peter Gallagher, Eoin Carley, Mark Daly
- **Comment**: None
- **Journal**: A&A 674, A218 (2023)
- **Summary**: Solar flares are energetic events in the solar atmosphere that are often linked with solar radio bursts (SRBs). SRBs are observed at metric to decametric wavelengths and are classified into five spectral classes (Type I--V) based on their signature in dynamic spectra. The automatic detection and classification of SRBs is a challenge due to their heterogeneous form. Near-realtime detection and classification of SRBs has become a necessity in recent years due to large data rates generated by advanced radio telescopes such as the LOw Frequency ARray (LOFAR). In this study, we implement congruent deep learning models to automatically detect and classify Type III SRBs. We generated simulated Type III SRBs, which were comparable to Type IIIs seen in real observations, using a deep learning method known as Generative Adversarial Network (GAN). This simulated data was combined with observations from LOFAR to produce a training set that was used to train an object detection model known as YOLOv2 (You Only Look Once). Using this congruent deep learning model system, we can accurately detect Type III SRBs at a mean Average Precision (mAP) value of 77.71%.



### Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image
- **Arxiv ID**: http://arxiv.org/abs/2305.09333v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.09333v1)
- **Published**: 2023-05-16 10:15:44+00:00
- **Updated**: 2023-05-16 10:15:44+00:00
- **Authors**: Yuzhou Peng
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Multi-modal visual understanding of images with prompts involves using various visual and textual cues to enhance the semantic understanding of images. This approach combines both vision and language processing to generate more accurate predictions and recognition of images. By utilizing prompt-based techniques, models can learn to focus on certain features of an image to extract useful information for downstream tasks. Additionally, multi-modal understanding can improve upon single modality models by providing more robust representations of images. Overall, the combination of visual and textual information is a promising area of research for advancing image recognition and understanding. In this paper we will try an amount of prompt design methods and propose a new method for better extraction of semantic information



### Blind Image Quality Assessment via Transformer Predicted Error Map and Perceptual Quality Token
- **Arxiv ID**: http://arxiv.org/abs/2305.09353v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09353v1)
- **Published**: 2023-05-16 11:17:54+00:00
- **Updated**: 2023-05-16 11:17:54+00:00
- **Authors**: Jinsong Shi, Pan Gao, Aljosa Smolic
- **Comment**: Submitted to TMM
- **Journal**: None
- **Summary**: Image quality assessment is a fundamental problem in the field of image processing, and due to the lack of reference images in most practical scenarios, no-reference image quality assessment (NR-IQA), has gained increasing attention recently. With the development of deep learning technology, many deep neural network-based NR-IQA methods have been developed, which try to learn the image quality based on the understanding of database information. Currently, Transformer has achieved remarkable progress in various vision tasks. Since the characteristics of the attention mechanism in Transformer fit the global perceptual impact of artifacts perceived by a human, Transformer is thus well suited for image quality assessment tasks. In this paper, we propose a Transformer based NR-IQA model using a predicted objective error map and perceptual quality token. Specifically, we firstly generate the predicted error map by pre-training one model consisting of a Transformer encoder and decoder, in which the objective difference between the distorted and the reference images is used as supervision. Then, we freeze the parameters of the pre-trained model and design another branch using the vision Transformer to extract the perceptual quality token for feature fusion with the predicted error map. Finally, the fused features are regressed to the final image quality score. Extensive experiments have shown that our proposed method outperforms the current state-of-the-art in both authentic and synthetic image databases. Moreover, the attentional map extracted by the perceptual quality token also does conform to the characteristics of the human visual system.



### Multi-task convolutional neural network for image aesthetic assessment
- **Arxiv ID**: http://arxiv.org/abs/2305.09373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09373v1)
- **Published**: 2023-05-16 11:56:02+00:00
- **Updated**: 2023-05-16 11:56:02+00:00
- **Authors**: Derya Soydaner, Johan Wagemans
- **Comment**: None
- **Journal**: None
- **Summary**: As people's aesthetic preferences for images are far from understood, image aesthetic assessment is a challenging artificial intelligence task. The range of factors underlying this task is almost unlimited, but we know that some aesthetic attributes affect those preferences. In this study, we present a multi-task convolutional neural network that takes into account these attributes. The proposed neural network jointly learns the attributes along with the overall aesthetic scores of images. This multi-task learning framework allows for effective generalization through the utilization of shared representations. Our experiments demonstrate that the proposed method outperforms the state-of-the-art approaches in predicting overall aesthetic scores for images in one benchmark of image aesthetics. We achieve near-human performance in terms of overall aesthetic scores when considering the Spearman's rank correlations. Moreover, our model pioneers the application of multi-tasking in another benchmark, serving as a new baseline for future research. Notably, our approach achieves this performance while using fewer parameters compared to existing multi-task neural networks in the literature, and consequently makes our method more efficient in terms of computational complexity.



### EXPRESSNET: An Explainable Residual Slim Network for Fingerprint Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.09397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09397v2)
- **Published**: 2023-05-16 12:29:50+00:00
- **Updated**: 2023-06-06 18:24:21+00:00
- **Authors**: Anuj Rai, Somnath Dey
- **Comment**: arXiv admin note: text overlap with arXiv:2303.01465
- **Journal**: None
- **Summary**: Presentation attack is a challenging issue that persists in the security of automatic fingerprint recognition systems. This paper proposes a novel explainable residual slim network that detects the presentation attack by representing the visual features in the input fingerprint sample. The encoder-decoder of this network along with the channel attention block converts the input sample into its heatmap representation while the modified residual convolutional neural network classifier discriminates between live and spoof fingerprints. The entire architecture of the heatmap generator block and modified ResNet classifier works together in an end-to-end manner. The performance of the proposed model is validated on benchmark liveness detection competition databases i.e. Livdet 2011, 2013, 2015, 2017, and 2019 and the classification accuracy of 96.86\%, 99.84\%, 96.45\%, 96.07\%, 96.27\% are achieved on them, respectively. The performance of the proposed model is compared with the state-of-the-art techniques, and the proposed method outperforms state-of-the-art methods in benchmark protocols of presentation attack detection in terms of classification accuracy.



### Diffusion Dataset Generation: Towards Closing the Sim2Real Gap for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.09401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.09401v1)
- **Published**: 2023-05-16 12:33:51+00:00
- **Updated**: 2023-05-16 12:33:51+00:00
- **Authors**: Andrew Farley, Mohsen Zand, Michael Greenspan
- **Comment**: 8 pages, 4 figures, Accepted to CRV2023 conference
- **Journal**: None
- **Summary**: We propose a method that augments a simulated dataset using diffusion models to improve the performance of pedestrian detection in real-world data. The high cost of collecting and annotating data in the real-world has motivated the use of simulation platforms to create training datasets. While simulated data is inexpensive to collect and annotate, it unfortunately does not always closely match the distribution of real-world data, which is known as the sim2real gap. In this paper we propose a novel method of synthetic data creation meant to close the sim2real gap for the challenging pedestrian detection task. Our method uses a diffusion-based architecture to learn a real-world distribution which, once trained, is used to generate datasets. We mix this generated data with simulated data as a form of augmentation and show that training on a combination of generated and simulated data increases average precision by as much as 27.3% for pedestrian detection models in real-world data, compared against training on purely simulated data.



### A Novel Strategy for Improving Robustness in Computer Vision Manufacturing Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.09407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09407v1)
- **Published**: 2023-05-16 12:51:51+00:00
- **Updated**: 2023-05-16 12:51:51+00:00
- **Authors**: Ahmad Mohamad Mezher, Andrew E. Marble
- **Comment**: None
- **Journal**: None
- **Summary**: Visual quality inspection in high performance manufacturing can benefit from automation, due to cost savings and improved rigor. Deep learning techniques are the current state of the art for generic computer vision tasks like classification and object detection. Manufacturing data can pose a challenge for deep learning because data is highly repetitive and there are few images of defects or deviations to learn from. Deep learning models trained with such data can be fragile and sensitive to context, and can under-detect new defects not found in the training data. In this work, we explore training defect detection models to learn specific defects out of context, so that they are more likely to be detected in new situations. We demonstrate how models trained on diverse images containing a common defect type can pick defects out in new circumstances. Such generic models could be more robust to new defects not found data collected for training, and can reduce data collection impediments to implementing visual inspection on production lines. Additionally, we demonstrate that object detection models trained to predict a label and bounding box outperform classifiers that predict a label only on held out test data typical of manufacturing inspection tasks. Finally, we studied the factors that affect generalization in order to train models that work under a wider range of conditions.



### Generating coherent comic with rich story using ChatGPT and Stable Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2305.11067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.11067v2)
- **Published**: 2023-05-16 13:11:45+00:00
- **Updated**: 2023-05-19 02:04:56+00:00
- **Authors**: Ze Jin, Zorina Song
- **Comment**: None
- **Journal**: None
- **Summary**: Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.



### Leaf Only SAM: A Segment Anything Pipeline for Zero-Shot Automated Leaf Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.09418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09418v2)
- **Published**: 2023-05-16 13:16:33+00:00
- **Updated**: 2023-05-22 09:53:21+00:00
- **Authors**: Dominic Williams, Fraser Macfarlane, Avril Britten
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Segment Anything Model (SAM) is a new foundation model that can be used as a zero-shot object segmentation method with the use of either guide prompts such as bounding boxes, polygons, or points. Alternatively, additional post processing steps can be used to identify objects of interest after segmenting everything in an image. Here we present a method using segment anything together with a series of post processing steps to segment potato leaves, called Leaf Only SAM. The advantage of this proposed method is that it does not require any training data to produce its results so has many applications across the field of plant phenotyping where there is limited high quality annotated data available. We compare the performance of Leaf Only SAM to a Mask R-CNN model which has been fine-tuned on our small novel potato leaf dataset. On the evaluation dataset, Leaf Only SAM finds an average recall of 63.2 and an average precision of 60.3, compared to recall of 78.7 and precision of 74.7 for Mask R-CNN. Leaf Only SAM does not perform better than the fine-tuned Mask R-CNN model on our data, but the SAM based model does not require any extra training or annotation of our new dataset. This shows there is potential to use SAM as a zero-shot classifier with the addition of post processing steps.



### Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2305.09447v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2305.09447v2)
- **Published**: 2023-05-16 14:08:24+00:00
- **Updated**: 2023-05-17 13:35:27+00:00
- **Authors**: Fenghe Tang, Jianrui Ding, Lingtao Wang, Min Xian, Chunping Ning
- **Comment**: 10 pages, 8 figures, Released code for
  https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency
- **Journal**: None
- **Summary**: Medical image segmentation is a critical step in computer-aided diagnosis, and convolutional neural networks are popular segmentation networks nowadays. However, the inherent local operation characteristics make it difficult to focus on the global contextual information of lesions with different positions, shapes, and sizes. Semi-supervised learning can be used to learn from both labeled and unlabeled samples, alleviating the burden of manual labeling. However, obtaining a large number of unlabeled images in medical scenarios remains challenging. To address these issues, we propose a Multi-level Global Context Cross-consistency (MGCC) framework that uses images generated by a Latent Diffusion Model (LDM) as unlabeled images for semi-supervised learning. The framework involves of two stages. In the first stage, a LDM is used to generate synthetic medical images, which reduces the workload of data annotation and addresses privacy concerns associated with collecting medical data. In the second stage, varying levels of global context noise perturbation are added to the input of the auxiliary decoder, and output consistency is maintained between decoders to improve the representation ability. Experiments conducted on open-source breast ultrasound and private thyroid ultrasound datasets demonstrate the effectiveness of our framework in bridging the probability distribution and the semantic representation of the medical image. Our approach enables the effective transfer of probability distribution knowledge to the segmentation network, resulting in improved segmentation accuracy. The code is available at https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency.



### Content-Adaptive Downsampling in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.09504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09504v1)
- **Published**: 2023-05-16 14:58:30+00:00
- **Updated**: 2023-05-16 14:58:30+00:00
- **Authors**: Robin Hesse, Simone Schaub-Meyer, Stefan Roth
- **Comment**: Accepted at CVPR 2023 Workshop on Efficient Deep Learning for
  Computer Vision (ECV). Code: https://github.com/visinf/cad
- **Journal**: None
- **Summary**: Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by allowing to process informative regions at a higher resolution than less informative ones. In a variety of experiments, we demonstrate the versatility of our adaptive downsampling strategy and empirically show that it improves the cost-accuracy trade-off of various established CNNs.



### Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction
- **Arxiv ID**: http://arxiv.org/abs/2305.09510v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, I.4.5; I.4.8; I.4.10; I.2.9; I.2.10; I.6.3
- **Links**: [PDF](http://arxiv.org/pdf/2305.09510v1)
- **Published**: 2023-05-16 15:03:20+00:00
- **Updated**: 2023-05-16 15:03:20+00:00
- **Authors**: Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.



### Light-VQA: A Multi-Dimensional Quality Assessment Model for Low-Light Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2305.09512v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09512v2)
- **Published**: 2023-05-16 15:04:01+00:00
- **Updated**: 2023-08-06 13:36:24+00:00
- **Authors**: Yunlong Dong, Xiaohong Liu, Yixuan Gao, Xunchu Zhou, Tao Tan, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Users Generated Content (UGC) videos becomes ubiquitous in our daily lives. However, due to the limitations of photographic equipments and techniques, UGC videos often contain various degradations, in which one of the most visually unfavorable effects is the underexposure. Therefore, corresponding video enhancement algorithms such as Low-Light Video Enhancement (LLVE) have been proposed to deal with the specific degradation. However, different from video enhancement algorithms, almost all existing Video Quality Assessment (VQA) models are built generally rather than specifically, which measure the quality of a video from a comprehensive perspective. To the best of our knowledge, there is no VQA model specially designed for videos enhanced by LLVE algorithms. To this end, we first construct a Low-Light Video Enhancement Quality Assessment (LLVE-QA) dataset in which 254 original low-light videos are collected and then enhanced by leveraging 8 LLVE algorithms to obtain 2,060 videos in total. Moreover, we propose a quality assessment model specialized in LLVE, named Light-VQA. More concretely, since the brightness and noise have the most impact on low-light enhanced VQA, we handcraft corresponding features and integrate them with deep-learning-based semantic features as the overall spatial information. As for temporal information, in addition to deep-learning-based motion features, we also investigate the handcrafted brightness consistency among video frames, and the overall temporal information is their concatenation. Subsequently, spatial and temporal information is fused to obtain the quality-aware representation of a video. Extensive experimental results show that our Light-VQA achieves the best performance against the current State-Of-The-Art (SOTA) on LLVE-QA and public dataset. Dataset and Codes can be found at https://github.com/wenzhouyidu/Light-VQA.



### SCTracker: Multi-object tracking with shape and confidence constraints
- **Arxiv ID**: http://arxiv.org/abs/2305.09523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09523v1)
- **Published**: 2023-05-16 15:18:42+00:00
- **Updated**: 2023-05-16 15:18:42+00:00
- **Authors**: Huan Mao, Yulin Chen, Zongtan Li, Feng Chen, Pingping Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Detection-based tracking is one of the main methods of multi-object tracking. It can obtain good tracking results when using excellent detectors but it may associate wrong targets when facing overlapping and low-confidence detections. To address this issue, this paper proposes a multi-object tracker based on shape constraint and confidence named SCTracker. In the data association stage, an Intersection of Union distance with shape constraints is applied to calculate the cost matrix between tracks and detections, which can effectively avoid the track tracking to the wrong target with the similar position but inconsistent shape, so as to improve the accuracy of data association. Additionally, the Kalman Filter based on the detection confidence is used to update the motion state to improve the tracking performance when the detection has low confidence. Experimental results on MOT 17 dataset show that the proposed method can effectively improve the tracking performance of multi-object tracking.



### Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares
- **Arxiv ID**: http://arxiv.org/abs/2305.09527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09527v2)
- **Published**: 2023-05-16 15:21:09+00:00
- **Updated**: 2023-05-18 18:35:23+00:00
- **Authors**: Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a differentiable nonlinear least squares framework to account for uncertainty in relative pose estimation from feature correspondences. Specifically, we introduce a symmetric version of the probabilistic normal epipolar constraint, and an approach to estimate the covariance of feature positions by differentiating through the camera pose estimation procedure. We evaluate our approach on synthetic, as well as the KITTI and EuRoC real-world datasets. On the synthetic dataset, we confirm that our learned covariances accurately approximate the true noise distribution. In real world experiments, we find that our approach consistently outperforms state-of-the-art non-probabilistic and probabilistic approaches, regardless of the feature extraction algorithm of choice.



### NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer
- **Arxiv ID**: http://arxiv.org/abs/2305.09533v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09533v3)
- **Published**: 2023-05-16 15:26:09+00:00
- **Updated**: 2023-08-13 15:31:58+00:00
- **Authors**: Yun Liu, Zhongsheng Yan, Sixiang Chen, Tian Ye, Wenqi Ren, Erkang Chen
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Nighttime image dehazing is a challenging task due to the presence of multiple types of adverse degrading effects including glow, haze, blurry, noise, color distortion, and so on. However, most previous studies mainly focus on daytime image dehazing or partial degradations presented in nighttime hazy scenes, which may lead to unsatisfactory restoration results. In this paper, we propose an end-to-end transformer-based framework for nighttime haze removal, called NightHazeFormer. Our proposed approach consists of two stages: supervised pre-training and semi-supervised fine-tuning. During the pre-training stage, we introduce two powerful priors into the transformer decoder to generate the non-learnable prior queries, which guide the model to extract specific degradations. For the fine-tuning, we combine the generated pseudo ground truths with input real-world nighttime hazy images as paired images and feed into the synthetic domain to fine-tune the pre-trained model. This semi-supervised fine-tuning paradigm helps improve the generalization to real domain. In addition, we also propose a large-scale synthetic dataset called UNREAL-NH, to simulate the real-world nighttime haze scenarios comprehensively. Extensive experiments on several synthetic and real-world datasets demonstrate the superiority of our NightHazeFormer over state-of-the-art nighttime haze removal methods in terms of both visually and quantitatively.



### Learning Higher-order Object Interactions for Keypoint-based Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2305.09539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09539v1)
- **Published**: 2023-05-16 15:30:33+00:00
- **Updated**: 2023-05-16 15:30:33+00:00
- **Authors**: Yi Huang, Asim Kadav, Farley Lai, Deep Patel, Hans Peter Graf
- **Comment**: SRVU - ICCV' 2021 workshop
- **Journal**: None
- **Summary**: Action recognition is an important problem that requires identifying actions in video by learning complex interactions across scene actors and objects. However, modern deep-learning based networks often require significant computation, and may capture scene context using various modalities that further increases compute costs. Efficient methods such as those used for AR/VR often only use human-keypoint information but suffer from a loss of scene context that hurts accuracy. In this paper, we describe an action-localization method, KeyNet, that uses only the keypoint data for tracking and action recognition. Specifically, KeyNet introduces the use of object based keypoint information to capture context in the scene. Our method illustrates how to build a structured intermediate representation that allows modeling higher-order interactions in the scene from object and human keypoints without using any RGB information. We find that KeyNet is able to track and classify human actions at just 5 FPS. More importantly, we demonstrate that object keypoints can be modeled to recover any loss in context from using keypoint information over AVA action and Kinetics datasets.



### Increasing Melanoma Diagnostic Confidence: Forcing the Convolutional Network to Learn from the Lesion
- **Arxiv ID**: http://arxiv.org/abs/2305.09542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09542v1)
- **Published**: 2023-05-16 15:34:12+00:00
- **Updated**: 2023-05-16 15:34:12+00:00
- **Authors**: Norsang Lama, R. Joe Stanley, Anand Nambisan, Akanksha Maurya, Jason Hagerty, William V. Stoecker
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Deep learning implemented with convolutional network architectures can exceed specialists' diagnostic accuracy. However, whole-image deep learning trained on a given dataset may not generalize to other datasets. The problem arises because extra-lesional features - ruler marks, ink marks, and other melanoma correlates - may serve as information leaks. These extra-lesional features, discoverable by heat maps, degrade melanoma diagnostic performance and cause techniques learned on one data set to fail to generalize. We propose a novel technique to improve melanoma recognition by an EfficientNet model. The model trains the network to detect the lesion and learn features from the detected lesion. A generalizable elliptical segmentation model for lesions was developed, with an ellipse enclosing a lesion and the ellipse enclosed by an extended rectangle (bounding box). The minimal bounding box was extended by 20% to allow some background around the lesion. The publicly available International Skin Imaging Collaboration (ISIC) 2020 skin lesion image dataset was used to evaluate the effectiveness of the proposed method. Our test results show that the proposed method improved diagnostic accuracy by increasing the mean area under receiver operating characteristic curve (mean AUC) score from 0.9 to 0.922. Additionally, correctly diagnosed scores are also improved, providing better separation of scores, thereby increasing melanoma diagnostic confidence. The proposed lesion-focused convolutional technique warrants further study.



### Image Reconstruction using Superpixel Clustering and Tensor Completion
- **Arxiv ID**: http://arxiv.org/abs/2305.09564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2305.09564v1)
- **Published**: 2023-05-16 16:00:48+00:00
- **Updated**: 2023-05-16 16:00:48+00:00
- **Authors**: Maame G. Asante-Mensah, Anh Huy Phan, Salman Ahmadi-Asl, Zaher Al Aghbari, Andrzej Cichocki
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a pixel selection method for compact image representation based on superpixel segmentation and tensor completion. Our method divides the image into several regions that capture important textures or semantics and selects a representative pixel from each region to store. We experiment with different criteria for choosing the representative pixel and find that the centroid pixel performs the best. We also propose two smooth tensor completion algorithms that can effectively reconstruct different types of images from the selected pixels. Our experiments show that our superpixel-based method achieves better results than uniform sampling for various missing ratios.



### Ray-Patch: An Efficient Querying for Light Field Transformers
- **Arxiv ID**: http://arxiv.org/abs/2305.09566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09566v2)
- **Published**: 2023-05-16 16:03:27+00:00
- **Updated**: 2023-08-17 09:39:05+00:00
- **Authors**: T. Berriel Martins, Javier Civera
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose the Ray-Patch querying, a novel model to efficiently query transformers to decode implicit representations into target views. Our Ray-Patch decoding reduces the computational footprint and increases inference speed up to one order of magnitude compared to previous models, without losing global attention, and hence maintaining specific task metrics. The key idea of our novel querying is to split the target image into a set of patches, then querying the transformer for each patch to extract a set of feature vectors, which are finally decoded into the target image using convolutional layers. Our experimental results, implementing Ray-Patch in 3 different architectures and evaluating it in 2 different tasks and datasets, demonstrate and quantify the effectiveness of our method, specifically a notable boost in rendering speed for the same task metrics.



### Inductive Graph Neural Networks for Moving Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.09585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09585v1)
- **Published**: 2023-05-16 16:32:08+00:00
- **Updated**: 2023-05-16 16:32:08+00:00
- **Authors**: Wieke Prummel, Jhony H. Giraldo, Anastasia Zakharova, Thierry Bouwmans
- **Comment**: Submitted to ICIP 2023
- **Journal**: None
- **Summary**: Moving Object Segmentation (MOS) is a challenging problem in computer vision, particularly in scenarios with dynamic backgrounds, abrupt lighting changes, shadows, camouflage, and moving cameras. While graph-based methods have shown promising results in MOS, they have mainly relied on transductive learning which assumes access to the entire training and testing data for evaluation. However, this assumption is not realistic in real-world applications where the system needs to handle new data during deployment. In this paper, we propose a novel Graph Inductive Moving Object Segmentation (GraphIMOS) algorithm based on a Graph Neural Network (GNN) architecture. Our approach builds a generic model capable of performing prediction on newly added data frames using the already trained model. GraphIMOS outperforms previous inductive learning methods and is more generic than previous transductive techniques. Our proposed algorithm enables the deployment of graph-based MOS models in real-world applications.



### Urban-StyleGAN: Learning to Generate and Manipulate Images of Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/2305.09602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09602v1)
- **Published**: 2023-05-16 16:54:48+00:00
- **Updated**: 2023-05-16 16:54:48+00:00
- **Authors**: George Eskandar, Youssef Farag, Tarun Yenamandra, Daniel Cremers, Karim Guirguis, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: A promise of Generative Adversarial Networks (GANs) is to provide cheap photorealistic data for training and validating AI models in autonomous driving. Despite their huge success, their performance on complex images featuring multiple objects is understudied. While some frameworks produce high-quality street scenes with little to no control over the image content, others offer more control at the expense of high-quality generation. A common limitation of both approaches is the use of global latent codes for the whole image, which hinders the learning of independent object distributions. Motivated by SemanticStyleGAN (SSG), a recent work on latent space disentanglement in human face generation, we propose a novel framework, Urban-StyleGAN, for urban scene generation and manipulation. We find that a straightforward application of SSG leads to poor results because urban scenes are more complex than human faces. To provide a more compact yet disentangled latent representation, we develop a class grouping strategy wherein individual classes are grouped into super-classes. Moreover, we employ an unsupervised latent exploration algorithm in the $\mathcal{S}$-space of the generator and show that it is more efficient than the conventional $\mathcal{W}^{+}$-space in controlling the image content. Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.



### Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2305.09610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09610v1)
- **Published**: 2023-05-16 17:02:57+00:00
- **Updated**: 2023-05-16 17:02:57+00:00
- **Authors**: Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata
- **Comment**: Accepted to UAI 2023. Preprint
- **Journal**: None
- **Summary**: Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and SegFormer semantic segmentation models.



### Generative Adversarial Networks for Brain Images Synthesis: A Review
- **Arxiv ID**: http://arxiv.org/abs/2305.15421v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2305.15421v1)
- **Published**: 2023-05-16 17:28:06+00:00
- **Updated**: 2023-05-16 17:28:06+00:00
- **Authors**: Firoozeh Shomal Zadeh, Sevda Molani, Maysam Orouskhani, Marziyeh Rezaei, Mehrzad Shafiei, Hossein Abbasi
- **Comment**: 9 pages, 3 tabels, 4 figures
- **Journal**: None
- **Summary**: In medical imaging, image synthesis is the estimation process of one image (sequence, modality) from another image (sequence, modality). Since images with different modalities provide diverse biomarkers and capture various features, multi-modality imaging is crucial in medicine. While multi-screening is expensive, costly, and time-consuming to report by radiologists, image synthesis methods are capable of artificially generating missing modalities. Deep learning models can automatically capture and extract the high dimensional features. Especially, generative adversarial network (GAN) as one of the most popular generative-based deep learning methods, uses convolutional networks as generators, and estimated images are discriminated as true or false based on a discriminator network. This review provides brain image synthesis via GANs. We summarized the recent developments of GANs for cross-modality brain image synthesis including CT to PET, CT to MRI, MRI to PET, and vice versa.



### FitMe: Deep Photorealistic 3D Morphable Model Avatars
- **Arxiv ID**: http://arxiv.org/abs/2305.09641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, I.2.10; I.3.7; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2305.09641v1)
- **Published**: 2023-05-16 17:42:45+00:00
- **Updated**: 2023-05-16 17:42:45+00:00
- **Authors**: Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, Stefanos Zafeiriou
- **Comment**: Accepted at CVPR 2023, project page at https://lattas.github.io/fitme
  , 17 pages including supplementary material
- **Journal**: None
- **Summary**: In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.



### torchosr -- a PyTorch extension package for Open Set Recognition models evaluation in Python
- **Arxiv ID**: http://arxiv.org/abs/2305.09646v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09646v1)
- **Published**: 2023-05-16 17:45:32+00:00
- **Updated**: 2023-05-16 17:45:32+00:00
- **Authors**: Joanna Komorniczak, Pawel Ksieniewicz
- **Comment**: None
- **Journal**: None
- **Summary**: The article presents the torchosr package - a Python package compatible with PyTorch library - offering tools and methods dedicated to Open Set Recognition in Deep Neural Networks. The package offers two state-of-the-art methods in the field, a set of functions for handling base sets and generation of derived sets for the Open Set Recognition task (where some classes are considered unknown and used only in the testing process) and additional tools to handle datasets and methods. The main goal of the package proposal is to simplify and promote the correct experimental evaluation, where experiments are carried out on a large number of derivative sets with various Openness and class-to-category assignments. The authors hope that state-of-the-art methods available in the package will become a source of a correct and open-source implementation of the relevant solutions in the domain.



### Wavelet-based Unsupervised Label-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2305.09647v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09647v1)
- **Published**: 2023-05-16 17:48:44+00:00
- **Updated**: 2023-05-16 17:48:44+00:00
- **Authors**: George Eskandar, Mohamed Abdelsamad, Karim Armanious, Shuai Zhang, Bin Yang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2109.14715
- **Journal**: None
- **Summary**: Semantic Image Synthesis (SIS) is a subclass of image-to-image translation where a semantic layout is used to generate a photorealistic image. State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge amount of paired data to accomplish this task while generic unpaired image-to-image translation frameworks underperform in comparison, because they color-code semantic layouts and learn correspondences in appearance instead of semantic content. Starting from the assumption that a high quality generated image should be segmented back to its semantic layout, we propose a new Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised segmentation loss and whole image wavelet based discrimination. Furthermore, in order to match the high-frequency distribution of real images, a novel generator architecture in the wavelet domain is proposed. We test our methodology on 3 challenging datasets and demonstrate its ability to bridge the performance gap between paired and unpaired models.



### Osteosarcoma Tumor Detection using Transfer Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2305.09660v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09660v1)
- **Published**: 2023-05-16 17:58:29+00:00
- **Updated**: 2023-05-16 17:58:29+00:00
- **Authors**: Raisa Fairooz Meem, Khandaker Tabin Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: The field of clinical image analysis has been applying transfer learning models increasingly due to their less computational complexity, better accuracy etc. These are pre-trained models that don't require to be trained from scratch which eliminates the necessity of large datasets. Transfer learning models are mostly used for the analysis of brain, breast, or lung images but other sectors such as bone marrow cell detection or bone cancer detection can also benefit from using transfer learning models, especially considering the lack of available large datasets for these tasks. This paper studies the performance of several transfer learning models for osteosarcoma tumour detection. Osteosarcoma is a type of bone cancer mostly found in the cells of the long bones of the body. The dataset consists of H&E stained images divided into 4 categories- Viable Tumor, Non-viable Tumor, Non-Tumor and Viable Non-viable. Both datasets were randomly divided into train and test sets following an 80-20 ratio. 80% was used for training and 20\% for test. 4 models are considered for comparison- EfficientNetB7, InceptionResNetV2, NasNetLarge and ResNet50. All these models are pre-trained on ImageNet. According to the result, InceptionResNetV2 achieved the highest accuracy (93.29%), followed by NasNetLarge (90.91%), ResNet50 (89.83%) and EfficientNetB7 (62.77%). It also had the highest precision (0.8658) and recall (0.8658) values among the 4 models.



### Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation
- **Arxiv ID**: http://arxiv.org/abs/2305.09662v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.09662v1)
- **Published**: 2023-05-16 17:58:43+00:00
- **Updated**: 2023-05-16 17:58:43+00:00
- **Authors**: Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta
- **Comment**: arXiv admin note: text overlap with arXiv:2304.07410
- **Journal**: None
- **Summary**: Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation.



### Understanding 3D Object Interaction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2305.09664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09664v2)
- **Published**: 2023-05-16 17:59:26+00:00
- **Updated**: 2023-08-04 20:29:58+00:00
- **Authors**: Shengyi Qian, David F. Fouhey
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Humans can easily understand a single image as depicting multiple potential objects permitting interaction. We use this skill to plan our interactions with the world and accelerate understanding new objects without engaging in interaction. In this paper, we would like to endow machines with the similar ability, so that intelligent agents can better explore the 3D scene or manipulate objects. Our approach is a transformer-based model that predicts the 3D location, physical properties and affordance of objects. To power this model, we collect a dataset with Internet videos, egocentric videos and indoor images to train and validate our approach. Our model yields strong performance on our data, and generalizes well to robotics data. Project site: https://jasonqsy.github.io/3DOI/



### Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks
- **Arxiv ID**: http://arxiv.org/abs/2305.09666v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09666v1)
- **Published**: 2023-05-16 17:59:59+00:00
- **Updated**: 2023-05-16 17:59:59+00:00
- **Authors**: Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, Zongwei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintaining a similar or even better annotation quality. This achievement is attributed to three unique properties of our method: (1) label bias reduction using multiple pre-trained segmentation models, (2) effective error detection in the model predictions, and (3) attention guidance for annotators to make corrections on the most salient errors. Furthermore, we summarize the taxonomy of common errors made by AI algorithms and annotators. This allows for continuous refinement of both AI and annotations and significantly reduces the annotation costs required to create large-scale datasets for a wider variety of medical imaging tasks.



### Towards Pragmatic Semantic Image Synthesis for Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/2305.09726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09726v1)
- **Published**: 2023-05-16 18:01:12+00:00
- **Updated**: 2023-05-16 18:01:12+00:00
- **Authors**: George Eskandar, Diandian Guo, Karim Guirguis, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The need for large amounts of training and validation data is a huge concern in scaling AI algorithms for autonomous driving. Semantic Image Synthesis (SIS), or label-to-image translation, promises to address this issue by translating semantic layouts to images, providing a controllable generation of photorealistic data. However, they require a large amount of paired data, incurring extra costs. In this work, we present a new task: given a dataset with synthetic images and labels and a dataset with unlabeled real images, our goal is to learn a model that can generate images with the content of the input mask and the appearance of real images. This new task reframes the well-known unsupervised SIS task in a more practical setting, where we leverage cheaply available synthetic data from a driving simulator to learn how to generate photorealistic images of urban scenes. This stands in contrast to previous works, which assume that labels and images come from the same domain but are unpaired during training. We find that previous unsupervised works underperform on this task, as they do not handle distribution shifts between two different domains. To bypass these problems, we propose a novel framework with two main contributions. First, we leverage the synthetic image as a guide to the content of the generated image by penalizing the difference between their high-level features on a patch level. Second, in contrast to previous works which employ one discriminator that overfits the target domain semantic distribution, we employ a discriminator for the whole image and multiscale discriminators on the image patches. Extensive comparisons on the benchmarks GTA-V $\rightarrow$ Cityscapes and GTA-V $\rightarrow$ Mapillary show the superior performance of the proposed model against state-of-the-art on this task.



### ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2305.09736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09736v1)
- **Published**: 2023-05-16 18:08:24+00:00
- **Updated**: 2023-05-16 18:08:24+00:00
- **Authors**: Sanyam Jain
- **Comment**: None
- **Journal**: None
- **Summary**: For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.



### A Range-Null Space Decomposition Approach for Fast and Flexible Spectral Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2305.09746v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09746v1)
- **Published**: 2023-05-16 18:37:58+00:00
- **Updated**: 2023-05-16 18:37:58+00:00
- **Authors**: Junyu Wang, Shijie Wang, Ruijie Zhang, Zengqiang Zheng, Wenyu Liu, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present RND-SCI, a novel framework for compressive hyperspectral image (HSI) reconstruction. Our framework decomposes the reconstructed object into range-space and null-space components, where the range-space part ensures the solution conforms to the compression process, and the null-space term introduces a deep HSI prior to constraining the output to have satisfactory properties. RND-SCI is not only simple in design with strong interpretability but also can be easily adapted to various HSI reconstruction networks, improving the quality of HSIs with minimal computational overhead. RND-SCI significantly boosts the performance of HSI reconstruction networks in retraining, fine-tuning or plugging into a pre-trained off-the-shelf model. Based on the framework and SAUNet, we design an extremely fast HSI reconstruction network, RND-SAUNet, which achieves an astounding 91 frames per second while maintaining superior reconstruction accuracy compared to other less time-consuming methods. Code and models are available at https://github.com/hustvl/RND-SCI.



### ICDAR 2023 Competition on Hierarchical Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.09750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09750v1)
- **Published**: 2023-05-16 18:56:12+00:00
- **Updated**: 2023-05-16 18:56:12+00:00
- **Authors**: Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, Michalis Raptis
- **Comment**: ICDAR 2023 competition report by organizers (accepted and to be
  published officially later)
- **Journal**: None
- **Summary**: We organize a competition on hierarchical text detection and recognition. The competition is aimed to promote research into deep learning models and systems that can jointly perform text detection and recognition and geometric layout analysis. We present details of the proposed competition organization, including tasks, datasets, evaluations, and schedule. During the competition period (from January 2nd 2023 to April 1st 2023), at least 50 submissions from more than 20 teams were made in the 2 proposed tasks. Considering the number of teams and submissions, we conclude that the HierText competition has been successfully held. In this report, we will also present the competition results and insights from them.



### A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot
- **Arxiv ID**: http://arxiv.org/abs/2305.09758v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.09758v2)
- **Published**: 2023-05-16 19:13:11+00:00
- **Updated**: 2023-05-23 03:58:13+00:00
- **Authors**: Aanisha Bhattacharya, Yaman K Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, Changyou Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. While previous research in multimedia understanding has focused mainly on videos with specific actions like cooking, there is a dearth of large annotated training datasets, hindering the development of supervised learning models with satisfactory performance for real-world applications. However, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To bridge this performance gap in multimedia understanding, we propose verbalizing story videos to generate their descriptions in natural language and then performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on five video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Further, alleviating a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science, persuasion strategy identification.



### Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.10450v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CL, cs.CV, cs.LG, cs.NE, eess.SP, 68, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2305.10450v1)
- **Published**: 2023-05-16 19:52:40+00:00
- **Updated**: 2023-05-16 19:52:40+00:00
- **Authors**: Bekir Yavuz Koc, Taner Arsan, Onder Pekcan
- **Comment**: 18 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: Cardiac diseases are one of the leading mortality factors in modern, industrialized societies, which cause high expenses in public health systems. Due to high costs, developing analytical methods to improve cardiac diagnostics is essential. The heart's electric activity was first modeled using a set of nonlinear differential equations. Following this, variations of cardiac spectra originating from deterministic dynamics are investigated. Analyzing a normal human heart's power spectra offers His-Purkinje network, which possesses a fractal-like structure. Phase space trajectories are extracted from the time series electrocardiogram (ECG) graph with third-order derivate Taylor Series. Here in this study, phase space analysis and Convolutional Neural Networks (CNNs) method are applied to 44 records via the MIT-BIH database recorded with MLII. In order to increase accuracy, a straight line is drawn between the highest Q-R distance in the phase space images of the records. Binary CNN classification is used to determine healthy or unhealthy hearts. With a 90.90% accuracy rate, this model could classify records according to their heart status.



### Semi-Supervised Object Detection for Sorghum Panicles in UAV Imagery
- **Arxiv ID**: http://arxiv.org/abs/2305.09810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.09810v1)
- **Published**: 2023-05-16 21:24:26+00:00
- **Updated**: 2023-05-16 21:24:26+00:00
- **Authors**: Enyu Cai, Jiaqi Guo, Changye Yang, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: The sorghum panicle is an important trait related to grain yield and plant development. Detecting and counting sorghum panicles can provide significant information for plant phenotyping. Current deep-learning-based object detection methods for panicles require a large amount of training data. The data labeling is time-consuming and not feasible for real application. In this paper, we present an approach to reduce the amount of training data for sorghum panicle detection via semi-supervised learning. Results show we can achieve similar performance as supervised methods for sorghum panicle detection by only using 10\% of original training data.



### A Method for Training-free Person Image Picture Generation
- **Arxiv ID**: http://arxiv.org/abs/2305.09817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.09817v1)
- **Published**: 2023-05-16 21:46:28+00:00
- **Updated**: 2023-05-16 21:46:28+00:00
- **Authors**: Tianyu Chen
- **Comment**: 8 pages, 5 figures, 2023 International Conference on Artificial
  Intelligence,Database and Machine Learning
- **Journal**: None
- **Summary**: The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Character Image Feature Encoder extracts only the relevant image features, rather than information about the model's composition or movements. In addition, the Character Image Feature Encoder can be adapted to different models after training. The proposed model can be conveniently incorporated into the Stable Diffusion generation process without modifying the model's ontology or used in combination with Stable Diffusion as a joint model.



### Mimetic Initialization of Self-Attention Layers
- **Arxiv ID**: http://arxiv.org/abs/2305.09828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2305.09828v1)
- **Published**: 2023-05-16 22:12:25+00:00
- **Updated**: 2023-05-16 22:12:25+00:00
- **Authors**: Asher Trockman, J. Zico Kolter
- **Comment**: None
- **Journal**: None
- **Summary**: It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".



### Segmentation of Aortic Vessel Tree in CT Scans with Deep Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.09833v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09833v1)
- **Published**: 2023-05-16 22:24:01+00:00
- **Updated**: 2023-05-16 22:24:01+00:00
- **Authors**: Shaofeng Yuan, Feng Yang
- **Comment**: 7 pages, 1 figure, 1 table
- **Journal**: None
- **Summary**: Automatic and accurate segmentation of aortic vessel tree (AVT) in computed tomography (CT) scans is crucial for early detection, diagnosis and prognosis of aortic diseases, such as aneurysms, dissections and stenosis. However, this task remains challenges, due to the complexity of aortic vessel tree and amount of CT angiography data. In this technical report, we use two-stage fully convolutional networks (FCNs) to automatically segment AVT in CTA scans from multiple centers. Specifically, we firstly adopt a 3D FCN with U-shape network architecture to segment AVT in order to produce topology attention and accelerate medical image analysis pipeline. And then another one 3D FCN is trained to segment branches of AVT along the pseudo-centerline of AVT. In the 2023 MICCAI Segmentation of the Aorta (SEG.A.) Challenge , the reported method was evaluated on the public dataset of 56 cases. The resulting Dice Similarity Coefficient (DSC) is 0.920, Jaccard Similarity Coefficient (JSC) is 0.861, Recall is 0.922, and Precision is 0.926 on a 5-fold random split of training and validation set.



### Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?
- **Arxiv ID**: http://arxiv.org/abs/2305.09847v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09847v1)
- **Published**: 2023-05-16 23:30:01+00:00
- **Updated**: 2023-05-16 23:30:01+00:00
- **Authors**: Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: This study examines the impact of optimizing the Stable Diffusion (SD) guided inference pipeline. We propose optimizing certain denoising steps by limiting the noise computation to conditional noise and eliminating unconditional noise computation, thereby reducing the complexity of the target iterations by 50%. Additionally, we demonstrate that later iterations of the SD are less sensitive to optimization, making them ideal candidates for applying the suggested optimization. Our experiments show that optimizing the last 20% of the denoising loop iterations results in an 8.2% reduction in inference time with almost no perceivable changes to the human eye. Furthermore, we found that by extending the optimization to 50% of the last iterations, we can reduce inference time by approximately 20.3%, while still generating visually pleasing images.



