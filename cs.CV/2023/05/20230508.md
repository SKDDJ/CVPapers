# Arxiv Papers in cs.CV on 2023-05-08
### SegGPT Meets Co-Saliency Scene
- **Arxiv ID**: http://arxiv.org/abs/2305.04396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04396v1)
- **Published**: 2023-05-08 00:19:05+00:00
- **Updated**: 2023-05-08 00:19:05+00:00
- **Authors**: Yi Liu, Shoukun Xu, Dingwen Zhang, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: Co-salient object detection targets at detecting co-existed salient objects among a group of images. Recently, a generalist model for segmenting everything in context, called SegGPT, is gaining public attention. In view of its breakthrough for segmentation, we can hardly wait to probe into its contribution to the task of co-salient object detection. In this report, we first design a framework to enable SegGPT for the problem of co-salient object detection. Proceed to the next step, we evaluate the performance of SegGPT on the problem of co-salient object detection on three available datasets. We achieve a finding that co-saliency scenes challenges SegGPT due to context discrepancy within a group of co-saliency images.



### Few Shot Learning for Medical Imaging: A Comparative Analysis of Methodologies and Formal Mathematical Framework
- **Arxiv ID**: http://arxiv.org/abs/2305.04401v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04401v2)
- **Published**: 2023-05-08 01:05:22+00:00
- **Updated**: 2023-05-31 16:35:08+00:00
- **Authors**: Jannatul Nayem, Sayed Sahriar Hasan, Noshin Amina, Bristy Das, Md Shahin Ali, Md Manjurul Ahsan, Shivakumar Raman
- **Comment**: Accepted for a Springer book chapter for a book title "Data-driven
  approaches to Medical Imaging"
- **Journal**: None
- **Summary**: Deep learning becomes an elevated context regarding disposing of many machine learning tasks and has shown a breakthrough upliftment to extract features from unstructured data. Though this flourishing context is developing in the medical image processing sector, scarcity of problem-dependent training data has become a larger issue in the way of easy application of deep learning in the medical sector. To unravel the confined data source, researchers have developed a model that can solve machine learning problems with fewer data called ``Few shot learning". Few hot learning algorithms determine to solve the data limitation problems by extracting the characteristics from a small dataset through classification and segmentation methods. In the medical sector, there is frequently a shortage of available datasets in respect of some confidential diseases. Therefore, Few shot learning gets the limelight in this data scarcity sector. In this chapter, the background and basic overview of a few shots of learning is represented. Henceforth, the classification of few-shot learning is described also. Even the paper shows a comparison of methodological approaches that are applied in medical image analysis over time. The current advancement in the implementation of few-shot learning concerning medical imaging is illustrated. The future scope of this domain in the medical imaging sector is further described.



### TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear Unit to Enhance Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.04402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04402v2)
- **Published**: 2023-05-08 01:13:59+00:00
- **Updated**: 2023-05-19 15:28:10+00:00
- **Authors**: Md. Mehedi Hasan, Md. Ali Hossain, Azmain Yakin Srizon, Abu Sayeed
- **Comment**: None
- **Journal**: None
- **Summary**: The application of the deep learning model in classification plays an important role in the accurate detection of the target objects. However, the accuracy is affected by the activation function in the hidden and output layer. In this paper, an activation function called TaLU, which is a combination of Tanh and Rectified Linear Units (ReLU), is used to improve the prediction. ReLU activation function is used by many deep learning researchers for its computational efficiency, ease of implementation, intuitive nature, etc. However, it suffers from a dying gradient problem. For instance, when the input is negative, its output is always zero because its gradient is zero. A number of researchers used different approaches to solve this issue. Some of the most notable are LeakyReLU, Softplus, Softsign, ELU, ThresholdedReLU, etc. This research developed TaLU, a modified activation function combining Tanh and ReLU, which mitigates the dying gradient problem of ReLU. The deep learning model with the proposed activation function was tested on MNIST and CIFAR-10, and it outperforms ReLU and some other studied activation functions in terms of accuracy(upto 6% in most cases, when used with Batch Normalization and a reasonable learning rate).



### Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models
- **Arxiv ID**: http://arxiv.org/abs/2305.04422v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04422v2)
- **Published**: 2023-05-08 02:28:45+00:00
- **Updated**: 2023-07-17 20:18:38+00:00
- **Authors**: Linglin Zhang, Beatrice Brown-Mulry, Vineela Nalla, InChan Hwang, Judy Wawira Gichoya, Aimilia Gastounioti, Imon Banerjee, Laleh Seyyed-Kalantari, MinJae Woo, Hari Trivedi
- **Comment**: 21 pages, 4 tables, 5 figures, 2 supplemental table and 1
  supplemental figure
- **Journal**: None
- **Summary**: Even though deep learning models for abnormality classification can perform well in screening mammography, the demographic and imaging characteristics associated with increased risk of failure for abnormality classification in screening mammograms remain unclear. This retrospective study used data from the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931 patients imaged at Emory University Healthcare between 2013 to 2020. Clinical and imaging data includes Breast Imaging Reporting and Data System (BI-RADS) assessment, region of interest coordinates for abnormalities, imaging features, pathologic outcomes, and patient demographics. Deep learning models including InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish between patches of abnormal tissue and randomly selected patches of normal tissue from the screening mammograms. The distributions of the training, validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients, 9,910 (18.9%) patches of 3,609 (18.3%) patients, and 13,390 (25.5%) patches of 5,404 (27.5%) patients. We assessed model performance overall and within subgroups defined by age, race, pathologic outcome, and imaging characteristics to evaluate reasons for misclassifications. On the test set, a ResNet152V2 model trained to classify normal versus abnormal tissue patches achieved an accuracy of 92.6% (95%CI=92.0-93.2%), and area under the receiver operative characteristics curve 0.975 (95%CI=0.972-0.978). Imaging characteristics associated with higher misclassifications of images include higher tissue densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C and RR=2.026; p=.003, BI-RADS density D), and presence of architectural distortion (RR=1.026; p<.001). Small but statistically significant differences in performance were observed by age, race, pathologic outcome, and other imaging features (p<.001).



### Improving 2D face recognition via fine-level facial depth generation and RGB-D complementary feature learning
- **Arxiv ID**: http://arxiv.org/abs/2305.04426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04426v1)
- **Published**: 2023-05-08 02:33:59+00:00
- **Updated**: 2023-05-08 02:33:59+00:00
- **Authors**: Wenhao Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition in complex scenes suffers severe challenges coming from perturbations such as pose deformation, ill illumination, partial occlusion. Some methods utilize depth estimation to obtain depth corresponding to RGB to improve the accuracy of face recognition. However, the depth generated by them suffer from image blur, which introduces noise in subsequent RGB-D face recognition tasks. In addition, existing RGB-D face recognition methods are unable to fully extract complementary features. In this paper, we propose a fine-grained facial depth generation network and an improved multimodal complementary feature learning network. Extensive experiments on the Lock3DFace dataset and the IIIT-D dataset show that the proposed FFDGNet and I MCFLNet can improve the accuracy of RGB-D face recognition while achieving the state-of-the-art performance.



### Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method based on Fast Fourier Convolution and ConvNeXt
- **Arxiv ID**: http://arxiv.org/abs/2305.04430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04430v1)
- **Published**: 2023-05-08 02:59:02+00:00
- **Updated**: 2023-05-08 02:59:02+00:00
- **Authors**: Han Zhou, Wei Dong, Yangyi Liu, Jun Chen
- **Comment**: Accepted by CVPRW 2023
- **Journal**: None
- **Summary**: Haze usually leads to deteriorated images with low contrast, color shift and structural distortion. We observe that many deep learning based models exhibit exceptional performance on removing homogeneous haze, but they usually fail to address the challenge of non-homogeneous dehazing. Two main factors account for this situation. Firstly, due to the intricate and non uniform distribution of dense haze, the recovery of structural and chromatic features with high fidelity is challenging, particularly in regions with heavy haze. Secondly, the existing small scale datasets for non-homogeneous dehazing are inadequate to support reliable learning of feature mappings between hazy images and their corresponding haze-free counterparts by convolutional neural network (CNN)-based models. To tackle these two challenges, we propose a novel two branch network that leverages 2D discrete wavelete transform (DWT), fast Fourier convolution (FFC) residual block and a pretrained ConvNeXt model. Specifically, in the DWT-FFC frequency branch, our model exploits DWT to capture more high-frequency features. Moreover, by taking advantage of the large receptive field provided by FFC residual blocks, our model is able to effectively explore global contextual information and produce images with better perceptual quality. In the prior knowledge branch, an ImageNet pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to learn more supplementary information and acquire a stronger generalization ability. The feasibility and effectiveness of the proposed method is demonstrated via extensive experiments and ablation studies. The code is available at https://github.com/zhouh115/DWT-FFC.



### Adversarial Examples Detection with Enhanced Image Difference Features based on Local Histogram Equalization
- **Arxiv ID**: http://arxiv.org/abs/2305.04436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04436v1)
- **Published**: 2023-05-08 03:14:01+00:00
- **Updated**: 2023-05-08 03:14:01+00:00
- **Authors**: Zhaoxia Yin, Shaowei Zhu, Hang Su, Jianteng Peng, Wanli Lyu, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have recently made significant progress in many fields. However, studies have shown that DNNs are vulnerable to adversarial examples, where imperceptible perturbations can greatly mislead DNNs even if the full underlying model parameters are not accessible. Various defense methods have been proposed, such as feature compression and gradient masking. However, numerous studies have proven that previous methods create detection or defense against certain attacks, which renders the method ineffective in the face of the latest unknown attack methods. The invisibility of adversarial perturbations is one of the evaluation indicators for adversarial example attacks, which also means that the difference in the local correlation of high-frequency information in adversarial examples and normal examples can be used as an effective feature to distinguish the two. Therefore, we propose an adversarial example detection framework based on a high-frequency information enhancement strategy, which can effectively extract and amplify the feature differences between adversarial examples and normal examples. Experimental results show that the feature augmentation module can be combined with existing detection models in a modular way under this framework. Improve the detector's performance and reduce the deployment cost without modifying the existing detection model.



### Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting
- **Arxiv ID**: http://arxiv.org/abs/2305.04440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04440v1)
- **Published**: 2023-05-08 03:25:17+00:00
- **Updated**: 2023-05-08 03:25:17+00:00
- **Authors**: Zhicheng Wang, Liwen Xiao, Zhiguo Cao, Hao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. This task is typically addressed by extracting the features of query image and exemplars respectively with (un)shared feature extractors and by matching their feature similarity, leading to an extract-\textit{then}-match paradigm. In this work, we show that CAC can be simplified in an extract-\textit{and}-match manner, particularly using a pretrained and plain vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. We reveal the rationale of such simplification from a decoupled view of the self-attention and point out that the simplification is only made possible if the query and exemplar tokens are concatenated as input. The resulting model, termed CACViT, simplifies the CAC pipeline and unifies the feature spaces between the query image and exemplars. In addition, we find CACViT naturally encodes background information within self-attention, which helps reduce background disturbance. Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in ViT, we present two effective strategies for scale and magnitude embedding. Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available.



### Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2305.04441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04441v1)
- **Published**: 2023-05-08 03:34:33+00:00
- **Updated**: 2023-05-08 03:34:33+00:00
- **Authors**: Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han
- **Comment**: None
- **Journal**: None
- **Summary**: Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.



### Towards Accurate Human Motion Prediction via Iterative Refinement
- **Arxiv ID**: http://arxiv.org/abs/2305.04443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04443v1)
- **Published**: 2023-05-08 03:43:51+00:00
- **Updated**: 2023-05-08 03:43:51+00:00
- **Authors**: Jiarui Sun, Girish Chowdhary
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction aims to forecast an upcoming pose sequence given a past human motion trajectory. To address the problem, in this work we propose FreqMRN, a human motion prediction framework that takes into account both the kinematic structure of the human body and the temporal smoothness nature of motion. Specifically, FreqMRN first generates a fixed-size motion history summary using a motion attention module, which helps avoid inaccurate motion predictions due to excessively long motion inputs. Then, supervised by the proposed spatial-temporal-aware, velocity-aware and global-smoothness-aware losses, FreqMRN iteratively refines the predicted motion though the proposed motion refinement module, which converts motion representations back and forth between pose space and frequency space. We evaluate FreqMRN on several standard benchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental results demonstrate that FreqMRN outperforms previous methods by large margins for both short-term and long-term predictions, while demonstrating superior robustness.



### FashionTex: Controllable Virtual Try-on with Text and Texture
- **Arxiv ID**: http://arxiv.org/abs/2305.04451v1
- **DOI**: 10.1145/3588432.3591568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04451v1)
- **Published**: 2023-05-08 04:10:36+00:00
- **Updated**: 2023-05-08 04:10:36+00:00
- **Authors**: Anran Lin, Nanxuan Zhao, Shuliang Ning, Yuda Qiu, Baoyuan Wang, Xiaoguang Han
- **Comment**: Accepted to SIGGRAPH 2023 (Conference Proceedings)
- **Journal**: None
- **Summary**: Virtual try-on attracts increasing research attention as a promising way for enhancing the user experience for online cloth shopping. Though existing methods can generate impressive results, users need to provide a well-designed reference image containing the target fashion clothes that often do not exist. To support user-friendly fashion customization in full-body portraits, we propose a multi-modal interactive setting by combining the advantages of both text and texture for multi-level fashion manipulation. With the carefully designed fashion editing module and loss functions, FashionTex framework can semantically control cloth types and local texture patterns without annotated pairwise training data. We further introduce an ID recovery module to maintain the identity of input portrait. Extensive experiments have demonstrated the effectiveness of our proposed pipeline.



### Real-World Denoising via Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2305.04457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04457v1)
- **Published**: 2023-05-08 04:48:03+00:00
- **Updated**: 2023-05-08 04:48:03+00:00
- **Authors**: Cheng Yang, Lijing Liang, Zhixun Su
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world image denoising is an extremely important image processing problem, which aims to recover clean images from noisy images captured in natural environments. In recent years, diffusion models have achieved very promising results in the field of image generation, outperforming previous generation models. However, it has not been widely used in the field of image denoising because it is difficult to control the appropriate position of the added noise. Inspired by diffusion models, this paper proposes a novel general denoising diffusion model that can be used for real-world image denoising. We introduce a diffusion process with linear interpolation, and the intermediate noisy image is interpolated from the original clean image and the corresponding real-world noisy image, so that this diffusion model can handle the level of added noise. In particular, we also introduce two sampling algorithms for this diffusion model. The first one is a simple sampling procedure defined according to the diffusion process, and the second one targets the problem of the first one and makes a number of improvements. Our experimental results show that our proposed method with a simple CNNs Unet achieves comparable results compared to the Transformer architecture. Both quantitative and qualitative evaluations on real-world denoising benchmarks show that the proposed general diffusion model performs almost as well as against the state-of-the-art methods.



### Locally Attentional SDF Diffusion for Controllable 3D Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2305.04461v2
- **DOI**: 10.1145/3592103
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2305.04461v2)
- **Published**: 2023-05-08 05:07:23+00:00
- **Updated**: 2023-05-09 01:36:00+00:00
- **Authors**: Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum
- **Comment**: Accepted to SIGGRAPH 2023 (Journal version)
- **Journal**: ACM Transactions on Graphics (SIGGRAPH), 42, 4 (August 2023), 13
  pages
- **Summary**: Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework -- locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named occupancy-diffusion, aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named SDF-diffusion, synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel view-aware local attention mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work. Our code and trained models are available at https://zhengxinyang.github.io/projects/LAS-Diffusion.html



### Generalized Universal Domain Adaptation with Generative Flow Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.04466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04466v2)
- **Published**: 2023-05-08 05:34:15+00:00
- **Updated**: 2023-08-30 03:10:19+00:00
- **Authors**: Didi Zhu, Yinchuan Li, Yunfeng Shao, Jianye Hao, Fei Wu, Kun Kuang, Jun Xiao, Chao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new problem in unsupervised domain adaptation, termed as Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise prediction of all target labels including unknown categories. GUDA bridges the gap between label distribution shift-based and label space mismatch-based variants, essentially categorizing them as a unified problem, guiding to a comprehensive framework for thoroughly solving all the variants. The key challenge of GUDA is developing and identifying novel target categories while estimating the target label distribution. To address this problem, we take advantage of the powerful exploration capability of generative flow networks and propose an active domain adaptation algorithm named GFlowDA, which selects diverse samples with probabilities proportional to a reward function. To enhance the exploration capability and effectively perceive the target label distribution, we tailor the states and rewards, and introduce an efficient solution for parent exploration and state transition. We also propose a training paradigm for GUDA called Generalized Universal Adversarial Network (GUAN), which involves collaborative optimization between GUAN and GFlowNet. Theoretical analysis highlights the importance of exploration, and extensive experiments on benchmark datasets demonstrate the superiority of GFlowDA.



### Video Object Segmentation in Panoptic Wild Scenes
- **Arxiv ID**: http://arxiv.org/abs/2305.04470v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04470v2)
- **Published**: 2023-05-08 05:46:59+00:00
- **Updated**: 2023-06-25 06:39:30+00:00
- **Authors**: Yuanyou Xu, Zongxin Yang, Yi Yang
- **Comment**: Accepted to IJCAI2023
- **Journal**: None
- **Summary**: In this paper, we introduce semi-supervised video object segmentation (VOS) to panoptic wild scenes and present a large-scale benchmark as well as a baseline method for it. Previous benchmarks for VOS with sparse annotations are not sufficient to train or evaluate a model that needs to process all possible objects in real-world scenarios. Our new benchmark (VIPOSeg) contains exhaustive object annotations and covers various real-world object categories which are carefully divided into subsets of thing/stuff and seen/unseen classes for comprehensive evaluation. Considering the challenges in panoptic VOS, we propose a strong baseline method named panoptic object association with transformers (PAOT), which uses panoptic identification to associate objects with a pyramid architecture on multiple scales. Experimental results show that VIPOSeg can not only boost the performance of VOS models by panoptic training but also evaluate them comprehensively in panoptic scenes. Previous methods for classic VOS still need to improve in performance and efficiency when dealing with panoptic scenes, while our PAOT achieves SOTA performance with good efficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in the VOT2022 challenge. Our dataset is available at https://github.com/yoxu515/VIPOSeg-Benchmark.



### Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation
- **Arxiv ID**: http://arxiv.org/abs/2305.04474v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.04474v3)
- **Published**: 2023-05-08 05:53:30+00:00
- **Updated**: 2023-06-22 06:44:57+00:00
- **Authors**: Chaoya Jiang, Wei Ye, Haiyang Xu, Miang yan, Shikun Zhang, Jie Zhang, Fei Huang
- **Comment**: Accepted by ACL2023
- **Journal**: None
- **Summary**: Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.



### IIITD-20K: Dense captioning for Text-Image ReID
- **Arxiv ID**: http://arxiv.org/abs/2305.04497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.04497v1)
- **Published**: 2023-05-08 06:46:56+00:00
- **Updated**: 2023-05-08 06:46:56+00:00
- **Authors**: A V Subramanyam, Niranjan Sundararajan, Vibhu Dubey, Brejesh Lall
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-Image (T2I) ReID has attracted a lot of attention in the recent past. CUHK-PEDES, RSTPReid and ICFG-PEDES are the three available benchmarks to evaluate T2I ReID methods. RSTPReid and ICFG-PEDES comprise of identities from MSMT17 but due to limited number of unique persons, the diversity is limited. On the other hand, CUHK-PEDES comprises of 13,003 identities but has relatively shorter text description on average. Further, these datasets are captured in a restricted environment with limited number of cameras. In order to further diversify the identities and provide dense captions, we propose a novel dataset called IIITD-20K. IIITD-20K comprises of 20,000 unique identities captured in the wild and provides a rich dataset for text-to-image ReID. With a minimum of 26 words for a description, each image is densely captioned. We further synthetically generate images and fine-grained captions using Stable-diffusion and BLIP models trained on our dataset. We perform elaborate experiments using state-of-art text-to-image ReID models and vision-language pre-trained models and present a comprehensive analysis of the dataset. Our experiments also reveal that synthetically generated data leads to a substantial performance improvement in both same dataset as well as cross dataset settings. Our dataset is available at https://bit.ly/3pkA3Rj.



### Building Footprint Extraction with Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2305.04499v1
- **DOI**: 10.1109/IGARSS.2019.8898764
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04499v1)
- **Published**: 2023-05-08 06:50:05+00:00
- **Updated**: 2023-05-08 06:50:05+00:00
- **Authors**: Yilei Shi, Qinyu Li, Xiaoxiang Zhu
- **Comment**: 4 pages. arXiv admin note: text overlap with arXiv:1911.03165
- **Journal**: None
- **Summary**: Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. Recent developments in deep convolutional neural networks (DCNNs) have enabled accurate pixel-level labeling tasks. One central issue remains, which is the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to progressive downsampling. In this work, we have proposed a end-to-end framework to overcome this issue, which uses the graph convolutional network (GCN) for building footprint extraction task. Our proposed framework outperforms state-of-the-art methods.



### Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and Real-World Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2305.04506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.04506v1)
- **Published**: 2023-05-08 07:03:26+00:00
- **Updated**: 2023-05-08 07:03:26+00:00
- **Authors**: Ross Greer, Samveed Desai, Lulua Rakla, Akshay Gopalkrishnan, Afnan Alofi, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: It is critical for vehicles to prevent any collisions with pedestrians. Current methods for pedestrian collision prevention focus on integrating visual pedestrian detectors with Automatic Emergency Braking (AEB) systems which can trigger warnings and apply brakes as a pedestrian enters a vehicle's path. Unfortunately, pedestrian-detection-based systems can be hindered in certain situations such as night-time or when pedestrians are occluded. Our system addresses such issues using an online, map-based pedestrian detection aggregation system where common pedestrian locations are learned after repeated passes of locations. Using a carefully collected and annotated dataset in La Jolla, CA, we demonstrate the system's ability to learn pedestrian zones and generate advisory notices when a vehicle is approaching a pedestrian despite challenges like dark lighting or pedestrian occlusion. Using the number of correct advisories, false advisories, and missed advisories to define precision and recall performance metrics, we evaluate our system and discuss future positive effects with further data collection. We have made our code available at https://github.com/s7desai/ped-mapping, and a video demonstration of the CHAMP system at https://youtu.be/dxeCrS_Gpkw.



### Robust Traffic Light Detection Using Salience-Sensitive Loss: Computational Framework and Evaluations
- **Arxiv ID**: http://arxiv.org/abs/2305.04516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04516v1)
- **Published**: 2023-05-08 07:22:15+00:00
- **Updated**: 2023-05-08 07:22:15+00:00
- **Authors**: Ross Greer, Akshay Gopalkrishnan, Jacob Landgren, Lulua Rakla, Anish Gopalan, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most important tasks for ensuring safe autonomous driving systems is accurately detecting road traffic lights and accurately determining how they impact the driver's actions. In various real-world driving situations, a scene may have numerous traffic lights with varying levels of relevance to the driver, and thus, distinguishing and detecting the lights that are relevant to the driver and influence the driver's actions is a critical safety task. This paper proposes a traffic light detection model which focuses on this task by first defining salient lights as the lights that affect the driver's future decisions. We then use this salience property to construct the LAVA Salient Lights Dataset, the first US traffic light dataset with an annotated salience property. Subsequently, we train a Deformable DETR object detection transformer model using Salience-Sensitive Focal Loss to emphasize stronger performance on salient traffic lights, showing that a model trained with this loss function has stronger recall than one trained without.



### DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/2305.04517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04517v2)
- **Published**: 2023-05-08 07:22:37+00:00
- **Updated**: 2023-08-08 15:50:11+00:00
- **Authors**: Xinmin Qiu, Congying Han, Zicheng Zhang, Bonan Li, Tiande Guo, Xuecheng Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Blind face restoration (BFR) is important while challenging. Prior works prefer to exploit GAN-based frameworks to tackle this task due to the balance of quality and efficiency. However, these methods suffer from poor stability and adaptability to long-tail distribution, failing to simultaneously retain source identity and restore detail. We propose DiffBFR to introduce Diffusion Probabilistic Model (DPM) for BFR to tackle the above problem, given its superiority over GAN in aspects of avoiding training collapse and generating long-tail distribution. DiffBFR utilizes a two-step design, that first restores identity information from low-quality images and then enhances texture details according to the distribution of real faces. This design is implemented with two key components: 1) Identity Restoration Module (IRM) for preserving the face details in results. Instead of denoising from pure Gaussian random distribution with LQ images as the condition during the reverse process, we propose a novel truncated sampling method which starts from LQ images with part noise added. We theoretically prove that this change shrinks the evidence lower bound of DPM and then restores more original details. With theoretical proof, two cascade conditional DPMs with different input sizes are introduced to strengthen this sampling effect and reduce training difficulty in the high-resolution image generated directly. 2) Texture Enhancement Module (TEM) for polishing the texture of the image. Here an unconditional DPM, a LQ-free model, is introduced to further force the restorations to appear realistic. We theoretically proved that this unconditional DPM trained on pure HQ images contributes to justifying the correct distribution of inference images output from IRM in pixel-level space. Truncated sampling with fractional time step is utilized to polish pixel-level textures while preserving identity information.



### Scene Text Recognition with Image-Text Matching-guided Dictionary
- **Arxiv ID**: http://arxiv.org/abs/2305.04524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04524v1)
- **Published**: 2023-05-08 07:47:49+00:00
- **Updated**: 2023-05-08 07:47:49+00:00
- **Authors**: Jiajun Wei, Hongjian Zhan, Xiao Tu, Yue Lu, Umapada Pal
- **Comment**: Accepted at ICDAR2023
- **Journal**: None
- **Summary**: Employing a dictionary can efficiently rectify the deviation between the visual prediction and the ground truth in scene text recognition methods. However, the independence of the dictionary on the visual features may lead to incorrect rectification of accurate visual predictions. In this paper, we propose a new dictionary language model leveraging the Scene Image-Text Matching(SITM) network, which avoids the drawbacks of the explicit dictionary language model: 1) the independence of the visual features; 2) noisy choice in candidates etc. The SITM network accomplishes this by using Image-Text Contrastive (ITC) Learning to match an image with its corresponding text among candidates in the inference stage. ITC is widely used in vision-language learning to pull the positive image-text pair closer in feature space. Inspired by ITC, the SITM network combines the visual features and the text features of all candidates to identify the candidate with the minimum distance in the feature space. Our lexicon method achieves better results(93.8\% accuracy) than the ordinary method results(92.1\% accuracy) on six mainstream benchmarks. Additionally, we integrate our method with ABINet and establish new state-of-the-art results on several benchmarks.



### CrAFT: Compression-Aware Fine-Tuning for Efficient Visual Task Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2305.04526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04526v2)
- **Published**: 2023-05-08 07:51:40+00:00
- **Updated**: 2023-07-09 00:08:11+00:00
- **Authors**: Jung Hwan Heo, Seyedarmin Azizi, Arash Fayyazi, Massoud Pedram
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Transfer learning has become a popular task adaptation method in the era of foundation models. However, many foundation models require large storage and computing resources, which makes off-the-shelf deployment impractical. Post-training compression techniques such as pruning and quantization can help lower deployment costs. Unfortunately, the resulting performance degradation limits the usability and benefits of such techniques. To close this performance gap, we propose CrAFT, a simple fine-tuning framework that enables effective post-training network compression. In CrAFT, users simply employ the default fine-tuning schedule along with sharpness minimization objective, simultaneously facilitating task adaptation and compression-friendliness. Contrary to the conventional sharpness minimization techniques, which are applied during pretraining, the CrAFT approach adds negligible training overhead as fine-tuning is done in under a couple of minutes or hours with a single GPU. The effectiveness of CrAFT, which is a general-purpose tool that can significantly boost one-shot pruning and post-training quantization, is demonstrated on both convolution-based and attention-based vision foundation models on a variety of target tasks. The code will be made publicly available.



### Latest Trends in Artificial Intelligence Technology: A Scoping Review
- **Arxiv ID**: http://arxiv.org/abs/2305.04532v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04532v2)
- **Published**: 2023-05-08 08:06:16+00:00
- **Updated**: 2023-05-23 13:32:54+00:00
- **Authors**: Teemu Niskanen, Tuomo Sipola, Olli Väänänen
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must show improvements against comparable solutions. One of the most important parts of the technology development appeared to be how to process and exploit the data gathered from multiple sources. The data can be highly unstructured and the technological solution should be able to utilize the data with minimum manual work from humans. The results of this review indicate that creating labeled datasets is very laborious, and solutions exploiting unsupervised or semi-supervised learning technologies are more and more researched. The learning algorithms should be able to be updated efficiently, and predictions should be interpretable. Using artificial intelligence technologies in real-world applications, safety and explainable predictions are mandatory to consider before mass adoption can occur.



### Smart Home Device Detection Algorithm Based on FSA-YOLOv5
- **Arxiv ID**: http://arxiv.org/abs/2305.04534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04534v1)
- **Published**: 2023-05-08 08:10:24+00:00
- **Updated**: 2023-05-08 08:10:24+00:00
- **Authors**: Jiafeng Zhang, Xuejing Pu
- **Comment**: None
- **Journal**: None
- **Summary**: Smart home device detection is a critical aspect of human-computer interaction. However, detecting targets in indoor environments can be challenging due to interference from ambient light and background noise. In this paper, we present a new model called FSA-YOLOv5, which addresses the limitations of traditional convolutional neural networks by introducing the Transformer to learn long-range dependencies. Additionally, we propose a new attention module, the full-separation attention module, which integrates spatial and channel dimensional information to learn contextual information. To improve tiny device detection, we include a prediction head for the indoor smart home device detection task. We also release the Southeast University Indoor Smart Speaker Dataset (SUSSD) to supplement existing data samples. Through a series of experiments on SUSSD, we demonstrate that our method outperforms other methods, highlighting the effectiveness of FSA-YOLOv5.



### LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed Multi-Label Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.04536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04536v1)
- **Published**: 2023-05-08 08:14:46+00:00
- **Updated**: 2023-05-08 08:14:46+00:00
- **Authors**: Peng Xia, Di Xu, Lie Ju, Ming Hu, Jun Chen, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: Long-tailed multi-label visual recognition (LTML) task is a highly challenging task due to the label co-occurrence and imbalanced data distribution. In this work, we propose a unified framework for LTML, namely prompt tuning with class-specific embedding loss (LMPT), capturing the semantic feature interactions between categories by combining text and image modality data and improving the performance synchronously on both head and tail classes. Specifically, LMPT introduces the embedding loss function with class-aware soft margin and re-weighting to learn class-specific contexts with the benefit of textual descriptions (captions), which could help establish semantic relationships between classes, especially between the head and tail classes. Furthermore, taking into account the class imbalance, the distribution-balanced loss is adopted as the classification loss function to further improve the performance on the tail classes without compromising head classes. Extensive experiments are conducted on VOC-LT and COCO-LT datasets, which demonstrates that the proposed method significantly surpasses the previous state-of-the-art methods and zero-shot CLIP in LTML. Our codes are fully available at \url{https://github.com/richard-peng-xia/LMPT}.



### High Quality Large-Scale 3-D Urban Mapping with Multi-Master TomoSAR
- **Arxiv ID**: http://arxiv.org/abs/2305.04541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04541v1)
- **Published**: 2023-05-08 08:29:21+00:00
- **Updated**: 2023-05-08 08:29:21+00:00
- **Authors**: Yilei Shi, Richard Bamler, Yuanyuan Wang, Xiao Xiang Zhu
- **Comment**: 7 pages. arXiv admin note: substantial text overlap with
  arXiv:2003.07803
- **Journal**: None
- **Summary**: Multi-baseline interferometric synthetic aperture radar (InSAR) techniques are effective approaches for retrieving the 3-D information of urban areas. In order to obtain a plausible reconstruction, it is necessary to use large-stack interferograms. Hence, these methods are commonly not appropriate for large-scale 3-D urban mapping using TanDEM-X data where only a few acquisitions are available in average for each city. This work proposes a new SAR tomographic processing framework to work with those extremely small stacks, which integrates the non-local filtering into SAR tomography inversion. The applicability of the algorithm is demonstrated using a TanDEM-X multi-baseline stack with 5 bistatic interferograms over the whole city of Munich, Germany. Systematic comparison of our result with airborne LiDAR data shows that the relative height accuracy of two third buildings is within two meters, which outperforms the TanDEM-X raw DEM. The promising performance of the proposed algorithm paved the first step towards high quality large-scale 3-D urban mapping.



### Multi-Temporal Lip-Audio Memory for Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.04542v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2305.04542v1)
- **Published**: 2023-05-08 08:30:52+00:00
- **Updated**: 2023-05-08 08:30:52+00:00
- **Authors**: Jeong Hun Yeo, Minsu Kim, Yong Man Ro
- **Comment**: Presented at ICASSP 2023
- **Journal**: None
- **Summary**: Visual Speech Recognition (VSR) is a task to predict a sentence or word from lip movements. Some works have been recently presented which use audio signals to supplement visual information. However, existing methods utilize only limited information such as phoneme-level features and soft labels of Automatic Speech Recognition (ASR) networks. In this paper, we present a Multi-Temporal Lip-Audio Memory (MTLAM) that makes the best use of audio signals to complement insufficient information of lip movements. The proposed method is mainly composed of two parts: 1) MTLAM saves multi-temporal audio features produced from short- and long-term audio signals, and the MTLAM memorizes a visual-to-audio mapping to load stored multi-temporal audio features from visual features at the inference phase. 2) We design an audio temporal model to produce multi-temporal audio features capturing the context of neighboring words. In addition, to construct effective visual-to-audio mapping, the audio temporal models can generate audio features time-aligned with visual features. Through extensive experiments, we validate the effectiveness of the MTLAM achieving state-of-the-art performances on two public VSR datasets.



### Distribution-aware Fairness Test Generation
- **Arxiv ID**: http://arxiv.org/abs/2305.13935v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2305.13935v3)
- **Published**: 2023-05-08 08:38:22+00:00
- **Updated**: 2023-06-27 08:45:06+00:00
- **Authors**: Sai Sathiesh Rajan, Ezekiel Soremekun, Yves Le Traon, Sudipta Chattopadhyay
- **Comment**: Paper submitted for review to TSE; 15 pages, 4 figures, LaTex;
  Results and methodology have been updated
- **Journal**: None
- **Summary**: This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair is up to 2.3x more effective than two main baselines, i.e., (a) an approach which focuses on generating images only within the distribution (ID) and (b) fairness analysis using only the original image dataset. We further observed that DistroFair is efficient, it generates 460 images per hour, on average. Finally, we evaluate the semantic validity of our approach via a user study with 81 participants, using 30 real images and 30 corresponding mutated images generated by DistroFair. We found that images generated by DistroFair are 80% as realistic as real-world images.



### Privacy-preserving Adversarial Facial Features
- **Arxiv ID**: http://arxiv.org/abs/2305.05391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.05391v1)
- **Published**: 2023-05-08 08:52:08+00:00
- **Updated**: 2023-05-08 08:52:08+00:00
- **Authors**: Zhibo Wang, He Wang, Shuaifan Jin, Wenwen Zhang, Jiahui Hu, Yan Wang, Peng Sun, Wei Yuan, Kaixin Liu, Kui Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition service providers protect face privacy by extracting compact and discriminative facial features (representations) from images, and storing the facial features for real-time recognition. However, such features can still be exploited to recover the appearance of the original face by building a reconstruction network. Although several privacy-preserving methods have been proposed, the enhancement of face privacy protection is at the expense of accuracy degradation. In this paper, we propose an adversarial features-based face privacy protection (AdvFace) approach to generate privacy-preserving adversarial features, which can disrupt the mapping from adversarial features to facial images to defend against reconstruction attacks. To this end, we design a shadow model which simulates the attackers' behavior to capture the mapping function from facial features to images and generate adversarial latent noise to disrupt the mapping. The adversarial features rather than the original features are stored in the server's database to prevent leaked features from exposing facial information. Moreover, the AdvFace requires no changes to the face recognition network and can be implemented as a privacy-enhancing plugin in deployed face recognition systems. Extensive experimental results demonstrate that AdvFace outperforms the state-of-the-art face privacy-preserving methods in defending against reconstruction attacks while maintaining face recognition accuracy.



### Privacy-Preserving Representations are not Enough -- Recovering Scene Content from Camera Poses
- **Arxiv ID**: http://arxiv.org/abs/2305.04603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04603v1)
- **Published**: 2023-05-08 10:25:09+00:00
- **Updated**: 2023-05-08 10:25:09+00:00
- **Authors**: Kunal Chelani, Torsten Sattler, Fredrik Kahl, Zuzana Kukelova
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization is the task of estimating the camera pose from which a given image was taken and is central to several 3D computer vision applications. With the rapid growth in the popularity of AR/VR/MR devices and cloud-based applications, privacy issues are becoming a very important aspect of the localization process. Existing work on privacy-preserving localization aims to defend against an attacker who has access to a cloud-based service. In this paper, we show that an attacker can learn about details of a scene without any access by simply querying a localization service. The attack is based on the observation that modern visual localization algorithms are robust to variations in appearance and geometry. While this is in general a desired property, it also leads to algorithms localizing objects that are similar enough to those present in a scene. An attacker can thus query a server with a large enough set of images of objects, \eg, obtained from the Internet, and some of them will be localized. The attacker can thus learn about object placements from the camera poses returned by the service (which is the minimal information returned by such a service). In this paper, we develop a proof-of-concept version of this attack and demonstrate its practical feasibility. The attack does not place any requirements on the localization algorithm used, and thus also applies to privacy-preserving representations. Current work on privacy-preserving representations alone is thus insufficient.



### Development of a Vision System to Enhance the Reliability of the Pick-and-Place Robot for Autonomous Testing of Camera Module used in Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2305.04605v1
- **DOI**: 10.1109/ICEET53442.2021.9659578
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2305.04605v1)
- **Published**: 2023-05-08 10:26:46+00:00
- **Updated**: 2023-05-08 10:26:46+00:00
- **Authors**: Hoang-Anh Phan, Duy Nam Bui, Tuan Nguyen Dinh, Bao-Anh Hoang, An Nguyen Ngoc, Dong Tran Huu Quoc, Ha Tran Thi Thuy, Tung Thanh Bui, Van Nguyen Thi Thanh
- **Comment**: Published to 2021 International Conference on Engineering and
  Emerging Technologies (ICEET 2021). 6 pages
- **Journal**: None
- **Summary**: Pick-and-place robots are commonly used in modern industrial manufacturing. For complex devices/parts like camera modules used in smartphones, which contain optical parts, electrical components and interfacing connectors, the placement operation may not absolutely accurate, which may cause damage in the device under test during the mechanical movement to make good contact for electrical functions inspection. In this paper, we proposed an effective vision system including hardware and algorithm to enhance the reliability of the pick-and-place robot for autonomous testing memory of camera modules. With limited hardware based on camera and raspberry PI and using simplify image processing algorithm based on histogram information, the vision system can confirm the presence of the camera modules in feeding tray and the placement accuracy of the camera module in test socket. Through that, the system can work with more flexibility and avoid damaging the device under test. The system was experimentally quantified through testing approximately 2000 camera modules in a stable light condition. Experimental results demonstrate that the system achieves accuracy of more than 99.92%. With its simplicity and effectiveness, the proposed vision system can be considered as a useful solution for using in pick-and-place systems in industry.



### SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.04609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04609v1)
- **Published**: 2023-05-08 10:38:14+00:00
- **Updated**: 2023-05-08 10:38:14+00:00
- **Authors**: Ayan Banerjee, Sanket Biswas, Josep Lladós, Umapada Pal
- **Comment**: Accepted to ICDAR 2023 (San Jose, California)
- **Journal**: None
- **Summary**: Instance-level segmentation of documents consists in assigning a class-aware and instance-aware label to each pixel of the image. It is a key step in document parsing for their understanding. In this paper, we present a unified transformer encoder-decoder architecture for en-to-end instance segmentation of complex layouts in document images. The method adapts a contrastive training with a mixed query selection for anchor initialization in the decoder. Later on, it performs a dot product between the obtained query embeddings and the pixel embedding map (coming from the encoder) for semantic reasoning. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone achieves better segmentation performance than the existing state-of-the-art approaches with the average precision of \textbf{93.72}, \textbf{54.39}, \textbf{84.65} and \textbf{98.04} respectively under one billion parameters. The code is made publicly available at: \href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}



### Target-driven One-Shot Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2305.04628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04628v2)
- **Published**: 2023-05-08 11:10:25+00:00
- **Updated**: 2023-07-17 10:35:00+00:00
- **Authors**: Julio Ivan Davila Carrazco, Suvarna Kishorkumar Kadam, Pietro Morerio, Alessio Del Bue, Vittorio Murino
- **Comment**: Accepted to 22nd International Conference on IMAGE ANALYSIS AND
  PROCESSING (ICIAP) 2023
- **Journal**: 22nd International Conference on IMAGE ANALYSIS AND PROCESSING
  (ICIAP) 2023
- **Summary**: In this paper, we introduce a novel framework for the challenging problem of One-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a target domain with only a single unlabeled target sample. Unlike existing approaches that rely on large labeled source and unlabeled target data, our Target-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation strategy guided by the target sample's style to align the source distribution with the target distribution. Our method consists of three modules: an augmentation module, a style alignment module, and a classifier. Unlike existing methods, our augmentation module allows for strong transformations of the source samples, and the style of the single target sample available is exploited to guide the augmentation by ensuring perceptual similarity. Furthermore, our approach integrates augmentation with style alignment, eliminating the need for separate pre-training on additional datasets. Our method outperforms or performs comparably to existing OS-UDA methods on the Digits and DomainNet benchmarks.



### ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2305.04651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04651v1)
- **Published**: 2023-05-08 12:08:12+00:00
- **Updated**: 2023-05-08 12:08:12+00:00
- **Authors**: Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi
- **Comment**: https://yupeilin2388.github.io/publication/ReDiffuser
- **Journal**: None
- **Summary**: Large-scale text-to-image models have demonstrated amazing ability to synthesize diverse and high-fidelity images. However, these models are often violated by several limitations. Firstly, they require the user to provide precise and contextually relevant descriptions for the desired image modifications. Secondly, current models can impose significant changes to the original image content during the editing process. In this paper, we explore ReGeneration learning in an image-to-image Diffusion model (ReDiffuser), that preserves the content of the original image without human prompting and the requisite editing direction is automatically discovered within the text embedding space. To ensure consistent preservation of the shape during image editing, we propose cross-attention guidance based on regeneration learning. This novel approach allows for enhanced expression of the target domain features while preserving the original shape of the image. In addition, we introduce a cooperative update strategy, which allows for efficient preservation of the original shape of an image, thereby improving the quality and consistency of shape preservation throughout the editing process. Our proposed method leverages an existing pre-trained text-image diffusion model without any additional training. Extensive experiments show that the proposed method outperforms existing work in both real and synthetic image editing.



### Riesz networks: scale invariant neural networks in a single forward pass
- **Arxiv ID**: http://arxiv.org/abs/2305.04665v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04665v1)
- **Published**: 2023-05-08 12:39:49+00:00
- **Updated**: 2023-05-08 12:39:49+00:00
- **Authors**: Tin Barisin, Katja Schladitz, Claudia Redenbach
- **Comment**: None
- **Journal**: None
- **Summary**: Scale invariance of an algorithm refers to its ability to treat objects equally independently of their size. For neural networks, scale invariance is typically achieved by data augmentation. However, when presented with a scale far outside the range covered by the training set, neural networks may fail to generalize.   Here, we introduce the Riesz network, a novel scale invariant neural network. Instead of standard 2d or 3d convolutions for combining spatial information, the Riesz network is based on the Riesz transform which is a scale equivariant operation. As a consequence, this network naturally generalizes to unseen or even arbitrary scales in a single forward pass. As an application example, we consider detecting and segmenting cracks in tomographic images of concrete. In this context, 'scale' refers to the crack thickness which may vary strongly even within the same sample. To prove its scale invariance, the Riesz network is trained on one fixed crack width. We then validate its performance in segmenting simulated and real tomographic images featuring a wide range of crack widths. An additional experiment is carried out on the MNIST Large Scale data set.



### Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2305.04691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04691v1)
- **Published**: 2023-05-08 13:20:55+00:00
- **Updated**: 2023-05-08 13:20:55+00:00
- **Authors**: Ben Fei, Weidong Yang, Liwen Liu, Tianyue Luo, Rui Zhang, Yixuan Li, Ying He
- **Comment**: 27 pages, 12 figures, 14 tables
- **Journal**: None
- **Summary**: Point cloud data has been extensively studied due to its compact form and flexibility in representing complex 3D structures. The ability of point cloud data to accurately capture and represent intricate 3D geometry makes it an ideal choice for a wide range of applications, including computer vision, robotics, and autonomous driving, all of which require an understanding of the underlying spatial structures. Given the challenges associated with annotating large-scale point clouds, self-supervised point cloud representation learning has attracted increasing attention in recent years. This approach aims to learn generic and useful point cloud representations from unlabeled data, circumventing the need for extensive manual annotations. In this paper, we present a comprehensive survey of self-supervised point cloud representation learning using DNNs. We begin by presenting the motivation and general trends in recent research. We then briefly introduce the commonly used datasets and evaluation metrics. Following that, we delve into an extensive exploration of self-supervised point cloud representation learning methods based on these techniques. Finally, we share our thoughts on some of the challenges and potential issues that future research in self-supervised learning for pre-training 3D point clouds may encounter.



### ElasticHash: Semantic Image Similarity Search by Deep Hashing with Elasticsearch
- **Arxiv ID**: http://arxiv.org/abs/2305.04710v1
- **DOI**: 10.1007/978-3-030-89131-2_2
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.04710v1)
- **Published**: 2023-05-08 13:50:47+00:00
- **Updated**: 2023-05-08 13:50:47+00:00
- **Authors**: Nikolaus Korfhage, Markus Mühling, Bernd Freisleben
- **Comment**: None
- **Journal**: The 19th International Conference on Computer Analysis of Images
  and Patterns (CAIP), 2021. Lecture Notes in Computer Science, vol 13053.
  Springer, Cham
- **Summary**: We present ElasticHash, a novel approach for high-quality, efficient, and large-scale semantic image similarity search. It is based on a deep hashing model to learn hash codes for fine-grained image similarity search in natural images and a two-stage method for efficiently searching binary hash codes using Elasticsearch (ES). In the first stage, a coarse search based on short hash codes is performed using multi-index hashing and ES terms lookup of neighboring hash codes. In the second stage, the list of results is re-ranked by computing the Hamming distance on long hash codes. We evaluate the retrieval performance of \textit{ElasticHash} for more than 120,000 query images on about 6.9 million database images of the OpenImages data set. The results show that our approach achieves high-quality retrieval results and low search latencies.



### The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2305.04718v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04718v2)
- **Published**: 2023-05-08 14:05:38+00:00
- **Updated**: 2023-08-05 12:09:19+00:00
- **Authors**: Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Wolfram Burgard, Joschka Boedecker, Abhinav Valada
- **Comment**: Currently under review for publication
- **Journal**: None
- **Summary**: In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning techniques. Furthermore, we show outstanding robustness towards disturbances such as clutter, occlusions, and noisy depth measurements, as well as generalization to unseen objects both in simulation and real-world robotic experiments.



### Learning to Generate Poetic Chinese Landscape Painting with Calligraphy
- **Arxiv ID**: http://arxiv.org/abs/2305.04719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04719v1)
- **Published**: 2023-05-08 14:10:10+00:00
- **Updated**: 2023-05-08 14:10:10+00:00
- **Authors**: Shaozu Yuan, Aijun Dai, Zhiling Yan, Ruixue Liu, Meng Chen, Baoyang Chen, Zhijie Qiu, Xiaodong He
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: In this paper, we present a novel system (denoted as Polaca) to generate poetic Chinese landscape painting with calligraphy. Unlike previous single image-to-image painting generation, Polaca takes the classic poetry as input and outputs the artistic landscape painting image with the corresponding calligraphy. It is equipped with three different modules to complete the whole piece of landscape painting artwork: the first one is a text-to-image module to generate landscape painting image, the second one is an image-to-image module to generate stylistic calligraphy image, and the third one is an image fusion module to fuse the two images into a whole piece of aesthetic artwork.



### Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields
- **Arxiv ID**: http://arxiv.org/abs/2305.04722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04722v1)
- **Published**: 2023-05-08 14:12:25+00:00
- **Updated**: 2023-05-08 14:12:25+00:00
- **Authors**: Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Sang Woo Kim
- **Comment**: 11 pages, 7 Figures
- **Journal**: None
- **Summary**: Vision transformers (ViTs) that model an image as a sequence of partitioned patches have shown notable performance in diverse vision tasks. Because partitioning patches eliminates the image structure, to reflect the order of patches, ViTs utilize an explicit component called positional embedding. However, we claim that the use of positional embedding does not simply guarantee the order-awareness of ViT. To support this claim, we analyze the actual behavior of ViTs using an effective receptive field. We demonstrate that during training, ViT acquires an understanding of patch order from the positional embedding that is trained to be a specific pattern. Based on this observation, we propose explicitly adding a Gaussian attention bias that guides the positional embedding to have the corresponding pattern from the beginning of training. We evaluated the influence of Gaussian attention bias on the performance of ViTs in several image classification, object detection, and semantic segmentation experiments. The results showed that proposed method not only facilitates ViTs to understand images but also boosts their performance on various datasets, including ImageNet, COCO 2017, and ADE20K.



### Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced Feature Extraction Processing
- **Arxiv ID**: http://arxiv.org/abs/2305.04724v1
- **DOI**: 10.32604/cmc.2023.038696
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.04724v1)
- **Published**: 2023-05-08 14:17:33+00:00
- **Updated**: 2023-05-08 14:17:33+00:00
- **Authors**: V. Banupriya, S. Anusuya
- **Comment**: None
- **Journal**: None
- **Summary**: In the modern world, one of the most severe eye infections brought on by diabetes is known as diabetic retinopathy, which will result in retinal damage, and, thus, lead to blindness. Diabetic retinopathy can be well treated with early diagnosis. Retinal fundus images of humans are used to screen for lesions in the retina. However, detecting DR in the early stages is challenging due to the minimal symptoms. Furthermore, the occurrence of diseases linked to vascular anomalies brought on by DR aids in diagnosing the condition. Nevertheless, the resources required for manually identifying the lesions are high. Similarly, training for Convolutional Neural Networks is more time-consuming. This proposed research aims to improve diabetic retinopathy diagnosis by developing an enhanced deep learning model for timely DR identification that is potentially more accurate than existing CNN-based models. The proposed model will detect various lesions from retinal images in the early stages. First, characteristics are retrieved from the retinal fundus picture and put into the EDLM for classification. For dimensionality reduction, EDLM is used. Additionally, the classification and feature extraction processes are optimized using the stochastic gradient descent optimizer. The EDLM effectiveness is assessed on the KAG GLE dataset with 3459 retinal images, and results are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.



### Controllable Light Diffusion for Portraits
- **Arxiv ID**: http://arxiv.org/abs/2305.04745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2305.04745v1)
- **Published**: 2023-05-08 14:46:28+00:00
- **Updated**: 2023-05-08 14:46:28+00:00
- **Authors**: David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sýkora, Rohit Pandey
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: We introduce light diffusion, a novel method to improve lighting in portraits, softening harsh shadows and specular highlights while preserving overall scene illumination. Inspired by professional photographers' diffusers and scrims, our method softens lighting given only a single portrait photo. Previous portrait relighting approaches focus on changing the entire lighting environment, removing shadows (ignoring strong specular highlights), or removing shading entirely. In contrast, we propose a learning based method that allows us to control the amount of light diffusion and apply it on in-the-wild portraits. Additionally, we design a method to synthetically generate plausible external shadows with sub-surface scattering effects while conforming to the shape of the subject's face. Finally, we show how our approach can increase the robustness of higher level vision applications, such as albedo estimation, geometry estimation and semantic segmentation.



### Toeplitz Neural Network for Sequence Modeling
- **Arxiv ID**: http://arxiv.org/abs/2305.04749v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04749v1)
- **Published**: 2023-05-08 14:49:01+00:00
- **Updated**: 2023-05-08 14:49:01+00:00
- **Authors**: Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, Yiran Zhong
- **Comment**: Accepted to ICLR 2023 Spotlight. Yiran Zhong is the corresponding
  author. 15B pretrained LLM with TNN will be released at
  https://github.com/OpenNLPLab/Tnn soon
- **Journal**: None
- **Summary**: Sequence modeling has important applications in natural language processing and computer vision. Recently, the transformer-based models have shown strong performance on various sequence modeling tasks, which rely on attention to capture pairwise token relations, and position embedding to inject positional information. While showing good performance, the transformer models are inefficient to scale to long input sequences, mainly due to the quadratic space-time complexity of attention. To overcome this inefficiency, we propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths. In addition, despite being trained on 512-token sequences, our model can extrapolate input sequence length up to 14K tokens in inference with consistent performance. Extensive experiments on autoregressive and bidirectional language modeling, image modeling, and the challenging Long-Range Arena benchmark show that our method achieves better performance than its competitors in most downstream tasks while being significantly faster. The code is available at https://github.com/OpenNLPLab/Tnn.



### Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/2305.04763v1
- **DOI**: 10.1109/TGRS.2023.3274091
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04763v1)
- **Published**: 2023-05-08 15:11:28+00:00
- **Updated**: 2023-05-08 15:11:28+00:00
- **Authors**: Xiao ling, Rongjun Qin
- **Comment**: 13 Figures
- **Journal**: None
- **Summary**: Texture mapping as a fundamental task in 3D modeling has been well established for well-acquired aerial assets under consistent illumination, yet it remains a challenge when it is scaled to large datasets with images under varying views and illuminations. A well-performed texture mapping algorithm must be able to efficiently select views, fuse and map textures from these views to mesh models, at the same time, achieve consistent radiometry over the entire model. Existing approaches achieve efficiency either by limiting the number of images to one view per face, or simplifying global inferences to only achieve local color consistency. In this paper, we break this tie by proposing a novel and efficient texture mapping framework that allows the use of multiple views of texture per face, at the same time to achieve global color consistency. The proposed method leverages a loopy belief propagation algorithm to perform an efficient and global-level probabilistic inferences to rank candidate views per face, which enables face-level multi-view texture fusion and blending. The texture fusion algorithm, being non-parametric, brings another advantage over typical parametric post color correction methods, due to its improved robustness to non-linear illumination differences. The experiments on three different types of datasets (i.e. satellite dataset, unmanned-aerial vehicle dataset and close-range dataset) show that the proposed method has produced visually pleasant and texturally consistent results in all scenarios, with an added advantage of consuming less running time as compared to the state of the art methods, especially for large-scale dataset such as satellite-derived models.



### OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation of Multichannel Images
- **Arxiv ID**: http://arxiv.org/abs/2305.04766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04766v1)
- **Published**: 2023-05-08 15:15:37+00:00
- **Updated**: 2023-05-08 15:15:37+00:00
- **Authors**: Yuanzhi Cai, Jagannath Aryal, Yuan Fang, Hong Huang, Lei Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of multichannel images is a fundamental task for many applications. Selecting an appropriate channel combination from the original multichannel image can improve the accuracy of semantic segmentation and reduce the cost of data storage, processing and future acquisition. Existing channel selection methods typically use a reasonable selection procedure to determine a desirable channel combination, and then train a semantic segmentation network using that combination. In this study, the concept of pruning from a supernet is used for the first time to integrate the selection of channel combination and the training of a semantic segmentation network. Based on this concept, a One-Shot Task-Adaptive (OSTA) channel selection method is proposed for the semantic segmentation of multichannel images. OSTA has three stages, namely the supernet training stage, the pruning stage and the fine-tuning stage. The outcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C, L8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiency of OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49% (mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86% (mIoU), respectively). It even exceeded the highest accuracies of exhaustive tests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA) and 70.27% (mIoU), respectively), where all possible channel combinations were tested. All of this can be accomplished within a predictable and relatively efficient timeframe, ranging from 101.71% to 298.1% times the time required to train the segmentation network alone. In addition, there were interesting findings that were deemed valuable for several fields.



### BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.04769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2305.04769v1)
- **Published**: 2023-05-08 15:19:39+00:00
- **Updated**: 2023-05-08 15:19:39+00:00
- **Authors**: Kishaan Jeeveswaran, Prashant Bhat, Bahram Zonooz, Elahe Arani
- **Comment**: Accepted at 40th International Conference on Machine Learning (ICML
  2023)
- **Journal**: None
- **Summary**: The ability of deep neural networks to continually learn and adapt to a sequence of tasks has remained challenging due to catastrophic forgetting of previously learned tasks. Humans, on the other hand, have a remarkable ability to acquire, assimilate, and transfer knowledge across tasks throughout their lifetime without catastrophic forgetting. The versatility of the brain can be attributed to the rehearsal of abstract experiences through a complementary learning system. However, representation rehearsal in vision transformers lacks diversity, resulting in overfitting and consequently, performance drops significantly compared to raw image rehearsal. Therefore, we propose BiRT, a novel representation rehearsal-based continual learning approach using vision transformers. Specifically, we introduce constructive noises at various stages of the vision transformer and enforce consistency in predictions with respect to an exponential moving average of the working model. Our method provides consistent performance gain over raw image and vanilla representation rehearsal on several challenging CL benchmarks, while being memory efficient and robust to natural and adversarial corruptions.



### AvatarReX: Real-time Expressive Full-body Avatars
- **Arxiv ID**: http://arxiv.org/abs/2305.04789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2305.04789v1)
- **Published**: 2023-05-08 15:43:00+00:00
- **Updated**: 2023-05-08 15:43:00+00:00
- **Authors**: Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu
- **Comment**: To appear in SIGGRAPH 2023 Journal Track. Project page at
  https://liuyebin.com/AvatarRex/
- **Journal**: None
- **Summary**: We present AvatarReX, a new method for learning NeRF-based full-body avatars from video data. The learnt avatar not only provides expressive control of the body, hands and the face together, but also supports real-time animation and rendering. To this end, we propose a compositional avatar representation, where the body, hands and the face are separately modeled in a way that the structural prior from parametric mesh templates is properly utilized without compromising representation flexibility. Furthermore, we disentangle the geometry and appearance for each part. With these technical designs, we propose a dedicated deferred rendering pipeline, which can be executed in real-time framerate to synthesize high-quality free-view images. The disentanglement of geometry and appearance also allows us to design a two-pass training strategy that combines volume rendering and surface rendering for network training. In this way, patch-level supervision can be applied to force the network to learn sharp appearance details on the basis of geometry estimation. Overall, our method enables automatic construction of expressive full-body avatars with real-time rendering capability, and can generate photo-realistic images with dynamic details for novel body motions and facial expressions.



### MultiModal-GPT: A Vision and Language Model for Dialogue with Humans
- **Arxiv ID**: http://arxiv.org/abs/2305.04790v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.04790v3)
- **Published**: 2023-05-08 15:45:42+00:00
- **Updated**: 2023-06-13 13:31:12+00:00
- **Authors**: Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint training of language-only and visual-language instructions with the \emph{same} instruction template effectively improves dialogue performance. Various demos show the ability of continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are at https://github.com/open-mmlab/Multimodal-GPT



### Compressed Video Quality Assessment for Super-Resolution: a Benchmark and a Quality Metric
- **Arxiv ID**: http://arxiv.org/abs/2305.04844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.04844v1)
- **Published**: 2023-05-08 16:42:55+00:00
- **Updated**: 2023-05-08 16:42:55+00:00
- **Authors**: Evgeney Bogatyrev, Ivan Molodetskikh, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: We developed a super-resolution (SR) benchmark to analyze SR's capacity to upscale compressed videos. Our dataset employed video codecs based on five compression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17 state-ofthe-art SR models using our benchmark and evaluated their ability to preserve scene context and their susceptibility to compression artifacts. To get an accurate perceptual ranking of SR models, we conducted a crowd-sourced side-by-side comparison of their outputs. The benchmark is publicly available at https://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html. We also analyzed benchmark results and developed an objective-quality-assessment metric based on the current bestperforming objective metrics. Our metric outperforms others, according to Spearman correlation with subjective scores for compressed video upscaling. It is publicly available at https://github.com/EvgeneyBogatyrev/super-resolution-metric.



### SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2305.04868v1
- **DOI**: 10.1109/TPAMI.2023.3269220
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04868v1)
- **Published**: 2023-05-08 17:16:38+00:00
- **Updated**: 2023-05-08 17:16:38+00:00
- **Authors**: Hezhen Hu, Weichao Zhao, Wengang Zhou, Houqiang Li
- **Comment**: Accepted to TPAMI. Project Page: https://signbert-zoo.github.io/
- **Journal**: None
- **Summary**: Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.



### Learning to Evaluate the Artness of AI-generated Images
- **Arxiv ID**: http://arxiv.org/abs/2305.04923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.04923v1)
- **Published**: 2023-05-08 17:58:27+00:00
- **Updated**: 2023-05-08 17:58:27+00:00
- **Authors**: Junyu Chen, Jie An, Hanjia Lyu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Assessing the artness of AI-generated images continues to be a challenge within the realm of image generation. Most existing metrics cannot be used to perform instance-level and reference-free artness evaluation. This paper presents ArtScore, a metric designed to evaluate the degree to which an image resembles authentic artworks by artists (or conversely photographs), thereby offering a novel approach to artness assessment. We first blend pre-trained models for photo and artwork generation, resulting in a series of mixed models. Subsequently, we utilize these mixed models to generate images exhibiting varying degrees of artness with pseudo-annotations. Each photorealistic image has a corresponding artistic counterpart and a series of interpolated images that range from realistic to artistic. This dataset is then employed to train a neural network that learns to estimate quantized artness levels of arbitrary images. Extensive experiments reveal that the artness levels predicted by ArtScore align more closely with human artistic evaluation than existing evaluation metrics, such as Gram loss and ArtFID.



### PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2305.04925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04925v1)
- **Published**: 2023-05-08 17:59:14+00:00
- **Updated**: 2023-05-08 17:59:14+00:00
- **Authors**: Jinyu Li, Chenxu Luo, Xiaodong Yang
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: In order to deal with the sparse and unstructured raw point clouds, LiDAR based 3D object detection research mostly focuses on designing dedicated local point aggregators for fine-grained geometrical modeling. In this paper, we revisit the local point aggregators from the perspective of allocating computational resources. We find that the simplest pillar based models perform surprisingly well considering both accuracy and latency. Additionally, we show that minimal adaptions from the success of 2D object detection, such as enlarging receptive field, significantly boost the performance. Extensive experiments reveal that our pillar based networks with modernized designs in terms of architecture and training render the state-of-the-art performance on the two popular benchmarks: Waymo Open Dataset and nuScenes. Our results challenge the common intuition that the detailed geometry modeling is essential to achieve high performance for 3D object detection.



### RelPose++: Recovering 6D Poses from Sparse-view Observations
- **Arxiv ID**: http://arxiv.org/abs/2305.04926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04926v1)
- **Published**: 2023-05-08 17:59:58+00:00
- **Updated**: 2023-05-08 17:59:58+00:00
- **Authors**: Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani
- **Comment**: Project webpage: https://amyxlase.github.io/relpose-plus-plus
- **Journal**: None
- **Summary**: We address the task of estimating 6D camera poses from sparse-view image sets (2-8 images). This task is a vital pre-processing stage for nearly all contemporary (neural) reconstruction algorithms but remains challenging given sparse views, especially for objects with visual symmetries and texture-less surfaces. We build on the recent RelPose framework which learns a network that infers distributions over relative rotations over image pairs. We extend this approach in two key ways; first, we use attentional transformer layers to process multiple images jointly, since additional views of an object may resolve ambiguous symmetries in any given image pair (such as the handle of a mug that becomes visible in a third view). Second, we augment this network to also report camera translations by defining an appropriate coordinate system that decouples the ambiguity in rotation estimation from translation prediction. Our final system results in large improvements in 6D pose prediction over prior art on both seen and unseen object categories and also enables pose estimation and 3D reconstruction for in-the-wild objects.



### Joint Moment Retrieval and Highlight Detection Via Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/2305.04961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.04961v1)
- **Published**: 2023-05-08 18:00:33+00:00
- **Updated**: 2023-05-08 18:00:33+00:00
- **Authors**: Richard Luo, Austin Peng, Heidi Yap, Koby Beard
- **Comment**: None
- **Journal**: None
- **Summary**: Video summarization has become an increasingly important task in the field of computer vision due to the vast amount of video content available on the internet. In this project, we propose a new method for natural language query based joint video summarization and highlight detection using multi-modal transformers. This approach will use both visual and audio cues to match a user's natural language query to retrieve the most relevant and interesting moments from a video. Our approach employs multiple recent techniques used in Vision Transformers (ViTs) to create a transformer-like encoder-decoder model. We evaluated our approach on multiple datasets such as YouTube Highlights and TVSum to demonstrate the flexibility of our proposed method.



### NerfAcc: Efficient Sampling Accelerates NeRFs
- **Arxiv ID**: http://arxiv.org/abs/2305.04966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04966v1)
- **Published**: 2023-05-08 18:02:11+00:00
- **Updated**: 2023-05-08 18:02:11+00:00
- **Authors**: Ruilong Li, Hang Gao, Matthew Tancik, Angjoo Kanazawa
- **Comment**: Website: https://www.nerfacc.com
- **Journal**: None
- **Summary**: Optimizing and rendering Neural Radiance Fields is computationally expensive due to the vast number of samples required by volume rendering. Recent works have included alternative sampling approaches to help accelerate their methods, however, they are often not the focus of the work. In this paper, we investigate and compare multiple sampling approaches and demonstrate that improved sampling is generally applicable across NeRF variants under an unified concept of transmittance estimator. To facilitate future experiments, we develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating advanced sampling methods into NeRF related methods. We demonstrate its flexibility by showing that it can reduce the training time of several recent NeRF methods by 1.5x to 20x with minimal modifications to the existing codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be implemented in native PyTorch using NerfAcc.



### Crop identification using deep learning on LUCAS crop cover photos
- **Arxiv ID**: http://arxiv.org/abs/2305.04994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.04994v1)
- **Published**: 2023-05-08 19:03:21+00:00
- **Updated**: 2023-05-08 19:03:21+00:00
- **Authors**: Momchil Yordanov, Raphael d'Andrimont, Laura Martinez-Sanchez, Guido Lemoine, Dominique Fasbender, Marijn van der Velde
- **Comment**: None
- **Journal**: None
- **Summary**: Crop classification via deep learning on ground imagery can deliver timely and accurate crop-specific information to various stakeholders. Dedicated ground-based image acquisition exercises can help to collect data in data scarce regions, improve control on timing of collection, or when study areas are to small to monitor via satellite. Automatic labelling is essential when collecting large volumes of data. One such data collection is the EU's Land Use Cover Area frame Survey (LUCAS), and in particular, the recently published LUCAS Cover photos database. The aim of this paper is to select and publish a subset of LUCAS Cover photos for 12 mature major crops across the EU, to deploy, benchmark, and identify the best configuration of Mobile-net for the classification task, to showcase the possibility of using entropy-based metrics for post-processing of results, and finally to show the applications and limitations of the model in a practical and policy relevant context. In particular, the usefulness of automatically identifying crops on geo-tagged photos is illustrated in the context of the EU's Common Agricultural Policy. The work has produced a dataset of 169,460 images of mature crops for the 12 classes, out of which 15,876 were manually selected as representing a clean sample without any foreign objects or unfavorable conditions. The best performing model achieved a Macro F1 (M-F1) of 0.75 on an imbalanced test dataset of 8,642 photos. Using metrics from information theory, namely - the Equivalence Reference Probability, resulted in achieving an increase of 6%. The most unfavorable conditions for taking such images, across all crop classes, were found to be too early or late in the season. The proposed methodology shows the possibility for using minimal auxiliary data, outside the images themselves, in order to achieve a M-F1 of 0.817 for labelling between 12 major European crops.



### Synthesis of Annotated Colorectal Cancer Tissue Images from Gland Layout
- **Arxiv ID**: http://arxiv.org/abs/2305.05006v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.05006v1)
- **Published**: 2023-05-08 19:25:50+00:00
- **Updated**: 2023-05-08 19:25:50+00:00
- **Authors**: Srijay Deshpande, Fayyaz Minhas, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Generating annotated pairs of realistic tissue images along with their annotations is a challenging task in computational histopathology. Such synthetic images and their annotations can be useful in training and evaluation of algorithms in the domain of computational pathology. To address this, we present an interactive framework to generate pairs of realistic colorectal cancer histology images with corresponding tissue component masks from the input gland layout. The framework shows the ability to generate realistic qualitative tissue images preserving morphological characteristics including stroma, goblet cells and glandular lumen. We show the appearance of glands can be controlled by user inputs such as number of glands, their locations and sizes. We also validate the quality of generated annotated pair with help of the gland segmentation algorithm.



### Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning
- **Arxiv ID**: http://arxiv.org/abs/2305.05023v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.05023v2)
- **Published**: 2023-05-08 19:58:49+00:00
- **Updated**: 2023-05-11 03:15:45+00:00
- **Authors**: Mohamed Abid, Arman Afrasiyabi, Ihsen Hedhli, Jean-François Lalonde, Christian Gagné
- **Comment**: 19 pages, 23 figures. arXiv admin note: substantial text overlap with
  arXiv:2107.11262. Under consideration in Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style). Conditioned on a target image, such methods extract the target style and combine it with the source image content, keeping coherence between the domains. In our proposal, we depart from this traditional view and instead consider the scenario where the target domain is represented by a very low-resolution (LR) image, proposing a domain-agnostic i2i method for fine-grained problems, where the domains are related. More specifically, our domain-agnostic approach aims at generating an image that combines visual features from the source image with low-frequency information (e.g. pose, color) of the LR target image. To do so, we present a novel approach that relies on training the generative model to produce images that both share distinctive information of the associated source image and correctly match the LR target image when downscaled. We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality. Qualitative and quantitative results show that when dealing with intra-domain image translation, our method generates realistic samples compared to state-of-the-art methods such as StarGAN v2. Ablation studies also reveal that our method is robust to changes in color, it can be applied to out-of-distribution images, and it allows for manual control over the final results.



### Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2305.05026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.05026v1)
- **Published**: 2023-05-08 20:09:19+00:00
- **Updated**: 2023-05-08 20:09:19+00:00
- **Authors**: Li Jiang, Zetong Yang, Shaoshuai Shi, Vladislav Golyanik, Dengxin Dai, Bernt Schiele
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Masked signal modeling has greatly advanced self-supervised pre-training for language and 2D images. However, it is still not fully explored in 3D scene understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new framework to conduct masked signal modeling in 3D scenes. MSP uses the essential 3D semantic cue, i.e., geometric shape, as the prediction target for masked points. The context-enhanced shape target consisting of explicit shape context and implicit deep shape feature is proposed to facilitate exploiting contextual cues in shape prediction. Meanwhile, the pre-training architecture in MSP is carefully designed to alleviate the masked shape leakage from point coordinates. Experiments on multiple 3D understanding tasks on both indoor and outdoor datasets demonstrate the effectiveness of MSP in learning good feature representations to consistently boost downstream performance.



### Crack Detection of Asphalt Concrete Using Combined Fracture Mechanics and Digital Image Correlation
- **Arxiv ID**: http://arxiv.org/abs/2305.05057v1
- **DOI**: 10.1061/JPEODX.PVENG-1249
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.05057v1)
- **Published**: 2023-05-08 21:28:40+00:00
- **Updated**: 2023-05-08 21:28:40+00:00
- **Authors**: Zehui Zhu, Imad L. Al-Qadi
- **Comment**: None
- **Journal**: Journal of Transportation Engineering, Part B: Pavements, 149(3),
  04023012 (2023)
- **Summary**: Cracking is a common failure mode in asphalt concrete (AC) pavements. Many tests have been developed to characterize the fracture behavior of AC. Accurate crack detection during testing is crucial to describe AC fracture behavior. This paper proposed a framework to detect surface cracks in AC specimens using two-dimensional digital image correlation (DIC). Two significant drawbacks in previous research in this field were addressed. First, a multi-seed incremental reliability-guided DIC was proposed to solve the decorrelation issue due to large deformation and discontinuities. The method was validated using synthetic deformed images. A correctly implemented analysis could accurately measure strains up to 450\%, even with significant discontinuities (cracks) present in the deformed image. Second, a robust method was developed to detect cracks based on displacement fields. The proposed method uses critical crack tip opening displacement ($\delta_c$) to define the onset of cleavage fracture. The proposed method relies on well-developed fracture mechanics theory. The proposed threshold $\delta_c$ has a physical meaning and can be easily determined from DIC measurement. The method was validated using an extended finite element model. The framework was implemented to measure the crack propagation rate while conducting the Illinois-flexibility index test on two AC mixes. The calculated rates could distinguish mixes based on their cracking potential. The proposed framework could be applied to characterize AC cracking phenomenon, evaluate its fracture properties, assess asphalt mixture testing protocols, and develop theoretical models.



### Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2305.05062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.05062v1)
- **Published**: 2023-05-08 21:38:42+00:00
- **Updated**: 2023-05-08 21:38:42+00:00
- **Authors**: Hyeokhyen Kwon, Chaitra Hedge, Yashar Kiarashi, Venkata Siva Krishna Madala, Ratan Singh, ArjunSinh Nakum, Robert Tweedy, Leandro Miletto Tonetto, Craig M. Zimring, Gari D. Clifford
- **Comment**: None
- **Journal**: None
- **Summary**: Localization of individuals in a built environment is a growing research topic. Estimating the positions, face orientation (or gaze direction) and trajectories of people through space has many uses, such as in crowd management, security, and healthcare. In this work, we present an open-source, low-cost, scalable and privacy-preserving edge computing framework for multi-person localization, i.e. estimating the positions, orientations, and trajectories of multiple people in an indoor space. Our computing framework consists of 38 Tensor Processing Unit (TPU)-enabled edge computing camera systems placed in the ceiling of the indoor therapeutic space. The edge compute systems are connected to an on-premise fog server through a secure and private network. A multi-person detection algorithm and a pose estimation model run on the edge TPU in real-time to collect features which are used, instead of raw images, for downstream computations. This ensures the privacy of individuals in the space, reduces data transmission/storage and improves scalability. We implemented a Kalman filter-based multi-person tracking method and a state-of-the-art body orientation estimation method to determine the positions and facing orientations of multiple people simultaneously in the indoor space. For our study site with size of 18,000 square feet, our system demonstrated an average localization error of 1.41 meters, a multiple-object tracking accuracy score of 62%, and a mean absolute body orientation error of 29{\deg}, which is sufficient for understanding group activity behaviors in indoor environments. Additionally, our study provides practical guidance for deploying the proposed system by analyzing various elements of the camera installation with respect to tracking accuracy.



### Atmospheric Turbulence Correction via Variational Deep Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2305.05077v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.05077v2)
- **Published**: 2023-05-08 22:35:07+00:00
- **Updated**: 2023-07-26 23:57:23+00:00
- **Authors**: Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos
- **Comment**: This work has been accepted to the 2023 IEEE 6th International
  Conference on Multimedia Information Processing and Retrieval (MIPR)
- **Journal**: None
- **Summary**: Atmospheric Turbulence (AT) correction is a challenging restoration task as it consists of two distortions: geometric distortion and spatially variant blur. Diffusion models have shown impressive accomplishments in photo-realistic image synthesis and beyond. In this paper, we propose a novel deep conditional diffusion model under a variational inference framework to solve the AT correction problem. We use this framework to improve performance by learning latent prior information from the input and degradation processes. We use the learned information to further condition the diffusion model. Experiments are conducted in a comprehensive synthetic AT dataset. We show that the proposed framework achieves good quantitative and qualitative results.



### Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2305.05095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.05095v1)
- **Published**: 2023-05-08 23:47:07+00:00
- **Updated**: 2023-05-08 23:47:07+00:00
- **Authors**: Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang, Xianzhi Du, Wencong Zhang, Zhiyun Lu, Yantao Zheng
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we build a new dataset named ImageNet with Adversarial Text Regions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1 accuracy of 68.78\%, outperforming previous models whose accuracy was all below 50\%.



