# Arxiv Papers in cs.CV on 2023-05-13
### Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy
- **Arxiv ID**: http://arxiv.org/abs/2305.07805v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.07805v2)
- **Published**: 2023-05-13 00:03:59+00:00
- **Updated**: 2023-07-30 06:10:16+00:00
- **Authors**: Krithika Iyer, Shireen Elhabian
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reducing any bias due to template selection. The proposed method operates directly on meshes and is computationally efficient, making it an attractive alternative to traditional and deep learning-based SSM approaches.



### Lightweight Delivery Detection on Doorbell Cameras
- **Arxiv ID**: http://arxiv.org/abs/2305.07812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07812v1)
- **Published**: 2023-05-13 01:28:28+00:00
- **Updated**: 2023-05-13 01:28:28+00:00
- **Authors**: Pirazh Khorramshahi, Zhe Wu, Tianchen Wang, Luke Deluccia, Hongcheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances in video-based action recognition and robust spatio-temporal modeling, most of the proposed approaches rely on the abundance of computational resources to afford running huge and computation-intensive convolutional or transformer-based neural networks to obtain satisfactory results. This limits the deployment of such models on edge devices with limited power and computing resources. In this work we investigate an important smart home application, video based delivery detection, and present a simple and lightweight pipeline for this task that can run on resource-constrained doorbell cameras. Our proposed pipeline relies on motion cues to generate a set of coarse activity proposals followed by their classification with a mobile-friendly 3DCNN network. For training we design a novel semi-supervised attention module that helps the network to learn robust spatio-temporal features and adopt an evidence-based optimization objective that allows for quantifying the uncertainty of predictions made by the network. Experimental results on our curated delivery dataset shows the significant effectiveness of our pipeline compared to alternatives and highlights the benefits of our training phase novelties to achieve free and considerable inference-time performance gains.



### Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance
- **Arxiv ID**: http://arxiv.org/abs/2305.07814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.07814v1)
- **Published**: 2023-05-13 01:57:39+00:00
- **Updated**: 2023-05-13 01:57:39+00:00
- **Authors**: Yiming Cui, Lecheng Ruan, Hang-Cheng Dong, Qiang Li, Zhongming Wu, Tieyong Zeng, Feng-Lei Fan
- **Comment**: None
- **Journal**: None
- **Summary**: The networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \underline{Cloud} models with \underline{R}eflection\underline{A}l \underline{IN}variance. We prove a theorem to explain why Cloud-RAIN can enjoy reflection symmetry. Furthermore, extensive experiments also corroborate the reflection property of the proposed Cloud-RAIN and show that Cloud-RAIN is superior to data augmentation. Our code is available at https://github.com/YimingCuiCuiCui/Cloud-RAIN.



### MetaMorphosis: Task-oriented Privacy Cognizant Feature Generation for Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.07815v1
- **DOI**: 10.1145/3576842.3582372
- **Categories**: **cs.CV**, cs.CR, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2305.07815v1)
- **Published**: 2023-05-13 01:59:07+00:00
- **Updated**: 2023-05-13 01:59:07+00:00
- **Authors**: Md Adnan Arefeen, Zhouyu Li, Md Yusuf Sarwar Uddin, Anupam Das
- **Comment**: Preprint version, 22 pages. Keywords: Multi-task learning, neural
  networks, collaborative intelligence, differential privacy, task privacy
- **Journal**: None
- **Summary**: With the growth of computer vision applications, deep learning, and edge computing contribute to ensuring practical collaborative intelligence (CI) by distributing the workload among edge devices and the cloud. However, running separate single-task models on edge devices is inefficient regarding the required computational resource and time. In this context, multi-task learning allows leveraging a single deep learning model for performing multiple tasks, such as semantic segmentation and depth estimation on incoming video frames. This single processing pipeline generates common deep features that are shared among multi-task modules. However, in a collaborative intelligence scenario, generating common deep features has two major issues. First, the deep features may inadvertently contain input information exposed to the downstream modules (violating input privacy). Second, the generated universal features expose a piece of collective information than what is intended for a certain task, in which features for one task can be utilized to perform another task (violating task privacy). This paper proposes a novel deep learning-based privacy-cognizant feature generation process called MetaMorphosis that limits inference capability to specific tasks at hand. To achieve this, we propose a channel squeeze-excitation based feature metamorphosis module, Cross-SEC, to achieve distinct attention of all tasks and a de-correlation loss function with differential-privacy to train a deep learning model that produces distinct privacy-aware features as an output for the respective tasks. With extensive experimentation on four datasets consisting of diverse images related to scene understanding and facial attributes, we show that MetaMorphosis outperforms recent adversarial learning and universal feature generation methods by guaranteeing privacy requirements in an efficient way for image and video analytics.



### PALM: Open Fundus Photograph Dataset with Pathologic Myopia Recognition and Anatomical Structure Annotation
- **Arxiv ID**: http://arxiv.org/abs/2305.07816v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.07816v1)
- **Published**: 2023-05-13 02:00:06+00:00
- **Updated**: 2023-05-13 02:00:06+00:00
- **Authors**: Huihui Fang, Fei Li, Junde Wu, Huazhu Fu, Xu Sun, José Ignacio Orlando, Hrvoje Bogunović, Xiulan Zhang, Yanwu Xu
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Pathologic myopia (PM) is a common blinding retinal degeneration suffered by highly myopic population. Early screening of this condition can reduce the damage caused by the associated fundus lesions and therefore prevent vision loss. Automated diagnostic tools based on artificial intelligence methods can benefit this process by aiding clinicians to identify disease signs or to screen mass populations using color fundus photographs as inputs. This paper provides insights about PALM, our open fundus imaging dataset for pathological myopia recognition and anatomical structure annotation. Our databases comprises 1200 images with associated labels for the pathologic myopia category and manual annotations of the optic disc, the position of the fovea and delineations of lesions such as patchy retinal atrophy (including peripapillary atrophy) and retinal detachment. In addition, this paper elaborates on other details such as the labeling process used to construct the database, the quality and characteristics of the samples and provides other relevant usage notes.



### Deep Learning-based Prediction of Electrical Arrhythmia Circuits from Cardiac Motion: An In-Silico Study
- **Arxiv ID**: http://arxiv.org/abs/2305.07822v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/2305.07822v1)
- **Published**: 2023-05-13 02:16:40+00:00
- **Updated**: 2023-05-13 02:16:40+00:00
- **Authors**: Jan Lebert, Daniel Deng, Lei Fan, Lik Chuan Lee, Jan Christoph
- **Comment**: None
- **Journal**: None
- **Summary**: The heart's contraction is caused by electrical excitation which propagates through the heart muscle. It was recently shown that the electrical excitation can be computed from the contractile motion of a simulated piece of heart muscle tissue using deep learning. In cardiac electrophysiology, a primary diagnostic goal is to identify electrical triggers or drivers of heart rhythm disorders. However, using electrical mapping techniques, it is currently impossible to map the three-dimensional morphology of the electrical waves throughout the entire heart muscle, especially during ventricular arrhythmias. Therefore, the approach to calculate or predict electrical excitation from the hearts motion could be a promising alternative diagnostic approach. Here, we demonstrate in computer simulations that it is possible to predict three-dimensional electrical wave dynamics from ventricular deformation mechanics using deep learning. We performed thousands of simulations of electromechanical activation dynamics in ventricular geometries and used the data to train a neural network which subsequently predicts the three-dimensional electrical wave pattern that caused the deformation. We demonstrate that, next to focal wave patterns, even complicated three-dimensional electrical wave patterns can be reconstructed, even if the network has never seen the particular arrhythmia. We show that the deep learning model has the ability to generalize by training it on data generated with the smoothed particle hydrodynamics (SPH) method and subsequently applying it to data generated with the finite element method (FEM). Predictions can be performed in the presence of scars and with significant heterogeneity. Our results suggest that, deep neural networks could be used to calculate intramural action potential wave patterns from imaging data of the motion of the heart muscle.



### M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2305.08877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08877v1)
- **Published**: 2023-05-13 02:38:15+00:00
- **Updated**: 2023-05-13 02:38:15+00:00
- **Authors**: Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang
- **Comment**: Accepted in the 2022 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops (CVPRW)
- **Journal**: None
- **Summary**: Ensuring traffic safety and preventing accidents is a critical goal in daily driving, where the advancement of computer vision technologies can be leveraged to achieve this goal. In this paper, we present a multi-view, multi-scale framework for naturalistic driving action recognition and localization in untrimmed videos, namely M$^2$DAR, with a particular focus on detecting distracted driving behaviors. Our system features a weight-sharing, multi-scale Transformer-based action recognition network that learns robust hierarchical representations. Furthermore, we propose a new election algorithm consisting of aggregation, filtering, merging, and selection processes to refine the preliminary results from the action recognition module across multiple views. Extensive experiments conducted on the 7th AI City Challenge Track 3 dataset demonstrate the effectiveness of our approach, where we achieved an overlap score of 0.5921 on the A2 test set. Our source code is available at \url{https://github.com/PurdueDigitalTwin/M2DAR}.



### Student Classroom Behavior Detection based on YOLOv7-BRA and Multi-Model Fusion
- **Arxiv ID**: http://arxiv.org/abs/2305.07825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07825v1)
- **Published**: 2023-05-13 02:46:41+00:00
- **Updated**: 2023-05-13 02:46:41+00:00
- **Authors**: Fan Yang, Tao Wang, Xiaofei Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2304.02488
- **Journal**: None
- **Summary**: Accurately detecting student behavior in classroom videos can aid in analyzing their classroom performance and improving teaching effectiveness. However, the current accuracy rate in behavior detection is low. To address this challenge, we propose the Student Classroom Behavior Detection system based on based on YOLOv7-BRA (YOLOv7 with Bi-level Routing Attention ). We identified eight different behavior patterns, including standing, sitting, speaking, listening, walking, raising hands, reading, and writing. We constructed a dataset, which contained 11,248 labels and 4,001 images, with an emphasis on the common behavior of raising hands in a classroom setting (Student Classroom Behavior dataset, SCB-Dataset). To improve detection accuracy, we added the biformer attention module to the YOLOv7 network. Finally, we fused the results from YOLOv7 CrowdHuman, SlowFast, and DeepSort models to obtain student classroom behavior data. We conducted experiments on the SCB-Dataset, and YOLOv7-BRA achieved an mAP@0.5 of 87.1%, resulting in a 2.2% improvement over previous results. Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-datase



### No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/2305.07829v2
- **DOI**: 10.18293/SEKE2023-185
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.07829v2)
- **Published**: 2023-05-13 03:20:33+00:00
- **Updated**: 2023-06-09 09:27:33+00:00
- **Authors**: Jun Cheng, Honglei Su, Jari Korhonen
- **Comment**: 6 pages, 5 figures, Accepted by International Conference on Software
  Engineering and Knowledge Engineering(SEKE2023)
- **Journal**: None
- **Summary**: With the rapid development of 3D vision applications based on point clouds, point cloud quality assessment(PCQA) is becoming an important research topic. However, the prior PCQA methods ignore the effect of local quality variance across different areas of the point cloud. To take an advantage of the quality distribution imbalance, we propose a no-reference point cloud quality assessment (NR-PCQA) method with local area correlation analysis capability, denoted as COPP-Net. More specifically, we split a point cloud into patches, generate texture and structure features for each patch, and fuse them into patch features to predict patch quality. Then, we gather the features of all the patches of a point cloud for correlation analysis, to obtain the correlation weights. Finally, the predicted qualities and correlation weights for all the patches are used to derive the final quality score. Experimental results show that our method outperforms the state-of-the-art benchmark NR-PCQA methods. The source code for the proposed COPP-Net can be found at https://github.com/philox12358/COPP-Net.



### Learning to Learn Unlearned Feature for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.08878v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.08878v1)
- **Published**: 2023-05-13 05:26:25+00:00
- **Updated**: 2023-05-13 05:26:25+00:00
- **Authors**: Seungyub Han, Yeongmo Kim, Seokhyeon Ha, Jungwoo Lee, Seunghong Choi
- **Comment**: Medical Imaging Meets NeurIPS 2018
- **Journal**: None
- **Summary**: We propose a fine-tuning algorithm for brain tumor segmentation that needs only a few data samples and helps networks not to forget the original tasks. Our approach is based on active learning and meta-learning. One of the difficulties in medical image segmentation is the lack of datasets with proper annotations, because it requires doctors to tag reliable annotation and there are many variants of a disease, such as glioma and brain metastasis, which are the different types of brain tumor and have different structural features in MR images. Therefore, it is impossible to produce the large-scale medical image datasets for all types of diseases. In this paper, we show a transfer learning method from high grade glioma to brain metastasis, and demonstrate that the proposed algorithm achieves balanced parameters for both glioma and brain metastasis domains within a few steps.



### CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2305.07840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07840v1)
- **Published**: 2023-05-13 05:27:36+00:00
- **Updated**: 2023-05-13 05:27:36+00:00
- **Authors**: Yunsheng Ma, Wenqian Ye, Xu Cao, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Ziran Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Driver intention prediction seeks to anticipate drivers' actions by analyzing their behaviors with respect to surrounding traffic environments. Existing approaches primarily focus on late-fusion techniques, and neglect the importance of maintaining consistency between predictions and prevailing driving contexts. In this paper, we introduce a new framework called Cross-View Episodic Memory Transformer (CEMFormer), which employs spatio-temporal transformers to learn unified memory representations for an improved driver intention prediction. Specifically, we develop a spatial-temporal encoder to integrate information from both in-cabin and external camera views, along with episodic memory representations to continuously fuse historical data. Furthermore, we propose a novel context-consistency loss that incorporates driving context as an auxiliary supervision signal to improve prediction performance. Comprehensive experiments on the Brain4Cars dataset demonstrate that CEMFormer consistently outperforms existing state-of-the-art methods in driver intention prediction.



### Meta-Polyp: a baseline for efficient Polyp segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.07848v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.07848v3)
- **Published**: 2023-05-13 06:27:33+00:00
- **Updated**: 2023-07-18 13:13:36+00:00
- **Authors**: Quoc-Huy Trinh
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, with local information and boundary information, which is crucial for the decision of the medical segmentation. Our proposed approach achieved competitive performance and obtained the top result in the State of the Art on the CVC-300 dataset, Kvasir, and CVC-ColonDB dataset. Apart from Kvasir-SEG, others are out-of-distribution datasets. The implementation can be found at: https://github.com/huyquoctrinh/MetaPolyp-CBMS2023.



### Squeeze Excitation Embedded Attention UNet for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.07850v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.07850v1)
- **Published**: 2023-05-13 06:46:07+00:00
- **Updated**: 2023-05-13 06:46:07+00:00
- **Authors**: Gaurav Prasanna, John Rohit Ernest, Lalitha G, Sathiya Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning based techniques have gained significance over the past few years in the field of medicine. They are used in various applications such as classifying medical images, segmentation and identification. The existing architectures such as UNet, Attention UNet and Attention Residual UNet are already currently existing methods for the same application of brain tumor segmentation, but none of them address the issue of how to extract the features in channel level. In this paper, we propose a new architecture called Squeeze Excitation Embedded Attention UNet (SEEA-UNet), this architecture has both Attention UNet and Squeeze Excitation Network for better results and predictions, this is used mainly because to get information at both Spatial and channel levels. The proposed model was compared with the existing architectures based on the comparison it was found out that for lesser number of epochs trained, the proposed model performed better. Binary focal loss and Jaccard Coefficient were used to monitor the model's performance.



### EV-MGRFlowNet: Motion-Guided Recurrent Network for Unsupervised Event-based Optical Flow with Hybrid Motion-Compensation Loss
- **Arxiv ID**: http://arxiv.org/abs/2305.07853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07853v1)
- **Published**: 2023-05-13 07:08:48+00:00
- **Updated**: 2023-05-13 07:08:48+00:00
- **Authors**: Hao Zhuang, Xinjie Huang, Kuanxu Hou, Delei Kong, Chenming Hu, Zheng Fang
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Event cameras offer promising properties, such as high temporal resolution and high dynamic range. These benefits have been utilized into many machine vision tasks, especially optical flow estimation. Currently, most existing event-based works use deep learning to estimate optical flow. However, their networks have not fully exploited prior hidden states and motion flows. Additionally, their supervision strategy has not fully leveraged the geometric constraints of event data to unlock the potential of networks. In this paper, we propose EV-MGRFlowNet, an unsupervised event-based optical flow estimation pipeline with motion-guided recurrent networks using a hybrid motion-compensation loss. First, we propose a feature-enhanced recurrent encoder network (FERE-Net) which fully utilizes prior hidden states to obtain multi-level motion features. Then, we propose a flow-guided decoder network (FGD-Net) to integrate prior motion flows. Finally, we design a hybrid motion-compensation loss (HMC-Loss) to strengthen geometric constraints for the more accurate alignment of events. Experimental results show that our method outperforms the current state-of-the-art (SOTA) method on the MVSEC dataset, with an average reduction of approximately 22.71% in average endpoint error (AEE). To our knowledge, our method ranks first among unsupervised learning-based methods.



### AURA : Automatic Mask Generator using Randomized Input Sampling for Object Removal
- **Arxiv ID**: http://arxiv.org/abs/2305.07857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07857v1)
- **Published**: 2023-05-13 07:51:35+00:00
- **Updated**: 2023-05-13 07:51:35+00:00
- **Authors**: Changsuk Oh, Dongseok Shim, H. Jin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of the image inpainting task is to fill missing regions of an image in a visually plausible way. Recently, deep-learning-based image inpainting networks have generated outstanding results, and some utilize their models as object removers by masking unwanted objects in an image. However, while trying to better remove objects using their networks, the previous works pay less attention to the importance of the input mask. In this paper, we focus on generating the input mask to better remove objects using the off-the-shelf image inpainting network. We propose an automatic mask generator inspired by the explainable AI (XAI) method, whose output can better remove objects than a semantic segmentation mask. The proposed method generates an importance map using randomly sampled input masks and quantitatively estimated scores of the completed images obtained from the random masks. The output mask is selected by a judge module among the candidate masks which are generated from the importance map. We design the judge module to quantitatively estimate the quality of the object removal results. In addition, we empirically find that the evaluation methods used in the previous works reporting object removal results are not appropriate for estimating the performance of an object remover. Therefore, we propose new evaluation metrics (FID$^*$ and U-IDS$^*$) to properly evaluate the quality of object removers. Experiments confirm that our method shows better performance in removing target class objects than the masks generated from the semantic segmentation maps, and the two proposed metrics make judgments consistent with humans.



### Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2305.07881v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07881v3)
- **Published**: 2023-05-13 10:00:24+00:00
- **Updated**: 2023-08-23 14:53:59+00:00
- **Authors**: Shuai Wang, Daoan Zhang, Zipei Yan, Shitong Shao, Rui Li
- **Comment**: The short version is accepted by IJCAI 1st International Workshop on
  Generalizing from Limited Resources in the Open World. (This version is long
  version)
- **Journal**: None
- **Summary**: Source-free domain adaptation aims to adapt deep neural networks using only pre-trained source models and target data. However, accessing the source model still has a potential concern about leaking the source data, which reveals the patient's privacy. In this paper, we study the challenging but practical problem: black-box source-free domain adaptation where only the outputs of the source model and target data are available. We propose a simple but effective two-stage knowledge distillation method. In Stage \uppercase\expandafter{\romannumeral1}, we train the target model from scratch with soft pseudo-labels generated by the source model in a knowledge distillation manner. In Stage \uppercase\expandafter{\romannumeral2}, we initialize another model as the new student model to avoid the error accumulation caused by noisy pseudo-labels. We feed the images with weak augmentation to the teacher model to guide the learning of the student model. Our method is simple and flexible, and achieves surprising results on three cross-domain segmentation tasks.



### Towards Generalizable Medical Image Segmentation with Pixel-wise Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2305.07883v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.07883v3)
- **Published**: 2023-05-13 10:09:40+00:00
- **Updated**: 2023-06-24 06:50:14+00:00
- **Authors**: Shuai Wang, Zipei Yan, Daoan Zhang, Zhongsen Li, Sirui Wu, Wenxuan Chen, Rui Li
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) achieve promising performance in visual recognition under the independent and identically distributed (IID) hypothesis. In contrast, the IID hypothesis is not universally guaranteed in numerous real-world applications, especially in medical image analysis. Medical image segmentation is typically formulated as a pixel-wise classification task in which each pixel is classified into a category. However, this formulation ignores the hard-to-classified pixels, e.g., some pixels near the boundary area, as they usually confuse DNNs. In this paper, we first explore that hard-to-classified pixels are associated with high uncertainty. Based on this, we propose a novel framework that utilizes uncertainty estimation to highlight hard-to-classified pixels for DNNs, thereby improving its generalization. We evaluate our method on two popular benchmarks: prostate and fundus datasets. The results of the experiment demonstrate that our method outperforms state-of-the-art methods.



### DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.07892v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.07892v1)
- **Published**: 2023-05-13 11:01:47+00:00
- **Updated**: 2023-05-13 11:01:47+00:00
- **Authors**: Jun Shu, Xiang Yuan, Deyu Meng, Zongben Xu
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmentation consistency to encode invariance as meta-knowledge for instantiating MR objective, denoted by DAC-MR. The proposed DAC-MR is hopeful to learn well-performing meta-models from training tasks with noisy, sparse or unavailable meta-data. We theoretically demonstrate that DAC-MR can be treated as a proxy meta-objective used to evaluate meta-model without high-quality meta-data. Besides, meta-data-driven meta-loss objective combined with DAC-MR is capable of achieving better meta-level generalization. 10 meta-learning tasks with different network architectures and benchmarks substantiate the capability of our DAC-MR on aiding meta-model learning. Fine performance of DAC-MR are obtained across all settings, and are well-aligned with our theoretical insights. This implies that our DAC-MR is problem-agnostic, and hopeful to be readily applied to extensive meta-learning problems and tasks.



### Voxel-wise classification for porosity investigation of additive manufactured parts with 3D unsupervised and (deeply) supervised neural networks
- **Arxiv ID**: http://arxiv.org/abs/2305.07894v2
- **DOI**: None
- **Categories**: **cs.CE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.07894v2)
- **Published**: 2023-05-13 11:23:00+00:00
- **Updated**: 2023-06-09 06:28:52+00:00
- **Authors**: Domenico Iuso, Soumick Chatterjee, Sven Cornelissen, Dries Verhees, Jan De Beenhouwer, Jan Sijbers
- **Comment**: None
- **Journal**: None
- **Summary**: Additive Manufacturing (AM) has emerged as a manufacturing process that allows the direct production of samples from digital models. To ensure that quality standards are met in all manufactured samples of a batch, X-ray computed tomography (X-CT) is often used combined with automated anomaly detection. For the latter, deep learning (DL) anomaly detection techniques are increasingly, as they can be trained to be robust to the material being analysed and resilient towards poor image quality. Unfortunately, most recent and popular DL models have been developed for 2D image processing, thereby disregarding valuable volumetric information.   This study revisits recent supervised (UNet, UNet++, UNet 3+, MSS-UNet) and unsupervised (VAE, ceVAE, gmVAE, vqVAE) DL models for porosity analysis of AM samples from X-CT images and extends them to accept 3D input data with a 3D-patch pipeline for lower computational requirements, improved efficiency and generalisability. The supervised models were trained using the Focal Tversky loss to address class imbalance that arises from the low porosity in the training datasets. The output of the unsupervised models is post-processed to reduce misclassifications caused by their inability to adequately represent the object surface. The findings were cross-validated in a 5-fold fashion and include: a performance benchmark of the DL models, an evaluation of the post-processing algorithm, an evaluation of the effect of training supervised models with the output of unsupervised models. In a final performance benchmark on a test set with poor image quality, the best performing supervised model was UNet++ with an average precision of 0.751 $\pm$ 0.030, while the best unsupervised model was the post-processed ceVAE with 0.830 $\pm$ 0.003. The VAE/ceVAE models demonstrated superior capabilities, particularly when leveraging post-processing techniques.



### On the Hidden Mystery of OCR in Large Multimodal Models
- **Arxiv ID**: http://arxiv.org/abs/2305.07895v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.07895v4)
- **Published**: 2023-05-13 11:28:37+00:00
- **Updated**: 2023-06-19 03:36:08+00:00
- **Authors**: Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Yang Liu, Biao Yang, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition (document text, artistic text, handwritten text, scene text), text-based visual question answering (document text, scene text, and bilingual text), key information extraction (receipts, documents, and nutrition facts) and handwritten mathematical expression recognition. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting finegrained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. Evaluation pipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.



### Temporal Consistent Automatic Video Colorization via Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2305.07904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07904v1)
- **Published**: 2023-05-13 12:06:09+00:00
- **Updated**: 2023-05-13 12:06:09+00:00
- **Authors**: Yu Zhang, Siqi Chen, Mingdao Wang, Xianlin Zhang, Chuang Zhu, Yue Zhang, Xueming Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video colorization task has recently attracted wide attention. Recent methods mainly work on the temporal consistency in adjacent frames or frames with small interval. However, it still faces severe challenge of the inconsistency between frames with large interval.To address this issue, we propose a novel video colorization framework, which combines semantic correspondence into automatic video colorization to keep long-range consistency. Firstly, a reference colorization network is designed to automatically colorize the first frame of each video, obtaining a reference image to supervise the following whole colorization process. Such automatically colorized reference image can not only avoid labor-intensive and time-consuming manual selection, but also enhance the similarity between reference and grayscale images. Afterwards, a semantic correspondence network and an image colorization network are introduced to colorize a series of the remaining frames with the help of the reference. Each frame is supervised by both the reference image and the immediately colorized preceding frame to improve both short-range and long-range temporal consistency. Extensive experiments demonstrate that our method outperforms other methods in maintaining temporal consistency both qualitatively and quantitatively. In the NTIRE 2023 Video Colorization Challenge, our method ranks at the 3rd place in Color Distribution Consistency (CDC) Optimization track.



### Mask to reconstruct: Cooperative Semantics Completion for Video-text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2305.07910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07910v1)
- **Published**: 2023-05-13 12:31:37+00:00
- **Updated**: 2023-05-13 12:31:37+00:00
- **Authors**: Han Fang, Zhifei Yang, Xianghao Zang, Chao Ban, Hao Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, masked video modeling has been widely explored and significantly improved the model's understanding ability of visual regions at a local level. However, existing methods usually adopt random masking and follow the same reconstruction paradigm to complete the masked regions, which do not leverage the correlations between cross-modal content. In this paper, we present Mask for Semantics Completion (MASCOT) based on semantic-based masked modeling. Specifically, after applying attention-based video masking to generate high-informed and low-informed masks, we propose Informed Semantics Completion to recover masked semantics information. The recovery mechanism is achieved by aligning the masked content with the unmasked visual regions and corresponding textual context, which makes the model capture more text-related details at a patch level. Additionally, we shift the emphasis of reconstruction from irrelevant backgrounds to discriminative parts to ignore regions with low-informed masks. Furthermore, we design dual-mask co-learning to incorporate video cues under different masks and learn more aligned video representation. Our MASCOT performs state-of-the-art performance on four major text-video retrieval benchmarks, including MSR-VTT, LSMDC, ActivityNet, and DiDeMo. Extensive ablation studies demonstrate the effectiveness of the proposed schemes.



### Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2305.07920v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.07920v2)
- **Published**: 2023-05-13 13:53:48+00:00
- **Updated**: 2023-05-31 06:18:08+00:00
- **Authors**: Ke Zhang, Yan Yang, Jun Yu, Hanliang Jiang, Jianping Fan, Qingming Huang, Weidong Han
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the growing demand for medical imaging diagnosis has placed a significant burden on radiologists. As a solution, Medical Vision-Language Pre-training (Med-VLP) methods have been proposed to learn universal representations from medical images and reports, benefiting downstream tasks without requiring fine-grained annotations. However, existing methods have overlooked the importance of cross-modal alignment in joint image-text reconstruction, resulting in insufficient cross-modal interaction. To address this limitation, we propose a unified Med-VLP framework based on Multi-task Paired Masking with Alignment (MPMA) to integrate the cross-modal alignment task into the joint image-text reconstruction framework to achieve more comprehensive cross-modal interaction, while a Global and Local Alignment (GLA) module is designed to assist self-supervised paradigm in obtaining semantic representations with rich domain knowledge. Furthermore, we introduce a Memory-Augmented Cross-Modal Fusion (MA-CMF) module to fully integrate visual information to assist report reconstruction and fuse the multi-modal representations adequately. Experimental results demonstrate that the proposed unified approach outperforms previous methods in all downstream tasks, including uni-modal, cross-modal, and multi-modal tasks.



### GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples
- **Arxiv ID**: http://arxiv.org/abs/2305.07931v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07931v3)
- **Published**: 2023-05-13 14:48:09+00:00
- **Updated**: 2023-05-19 02:45:29+00:00
- **Authors**: Tian Gao, Cheng-Zhong Xu, Le Zhang, Hui Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Affected by the massive amount of parameters, ViT usually suffers from serious overfitting problems with a relatively limited number of training samples. In addition, ViT generally demands heavy computing resources, which limit its deployment on resource-constrained devices. As a type of model-compression method,model binarization is potentially a good choice to solve the above problems. Compared with the full-precision one, the model with the binarization method replaces complex tensor multiplication with simple bit-wise binary operations and represents full-precision model parameters and activations with only 1-bit ones, which potentially solves the problem of model size and computational complexity, respectively. In this paper, we find that the decline of the accuracy of the binary ViT model is mainly due to the information loss of the Attention module and the Value vector. Therefore, we propose a novel model binarization technique, called Group Superposition Binarization (GSB), to deal with these issues. Furthermore, in order to further improve the performance of the binarization model, we have investigated the gradient calculation procedure in the binarization process and derived more proper gradient calculation equations for GSB to reduce the influence of gradient mismatch. Then, the knowledge distillation technique is introduced to alleviate the performance degradation caused by model binarization. Experiments on three datasets with limited numbers of training samples demonstrate that the proposed GSB model achieves state-of-the-art performance among the binary quantization schemes and exceeds its full-precision counterpart on some indicators.



### Illumination-insensitive Binary Descriptor for Visual Measurement Based on Local Inter-patch Invariance
- **Arxiv ID**: http://arxiv.org/abs/2305.07943v1
- **DOI**: 10.1109/TIM.2023.3273211
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07943v1)
- **Published**: 2023-05-13 15:15:18+00:00
- **Updated**: 2023-05-13 15:15:18+00:00
- **Authors**: Xinyu Lin, Yingjie Zhou, Xun Zhang, Yipeng Liu, Ce Zhu
- **Comment**: Accepted by IEEE Transactions on Instrumentation and Measurement
- **Journal**: IEEE Transactions on Instrumentation and Measurement 2023
- **Summary**: Binary feature descriptors have been widely used in various visual measurement tasks, particularly those with limited computing resources and storage capacities. Existing binary descriptors may not perform well for long-term visual measurement tasks due to their sensitivity to illumination variations. It can be observed that when image illumination changes dramatically, the relative relationship among local patches mostly remains intact. Based on the observation, consequently, this study presents an illumination-insensitive binary (IIB) descriptor by leveraging the local inter-patch invariance exhibited in multiple spatial granularities to deal with unfavorable illumination variations. By taking advantage of integral images for local patch feature computation, a highly efficient IIB descriptor is achieved. It can encode scalable features in multiple spatial granularities, thus facilitating a computationally efficient hierarchical matching from coarse to fine. Moreover, the IIB descriptor can also apply to other types of image data, such as depth maps and semantic segmentation results, when available in some applications. Numerical experiments on both natural and synthetic datasets reveal that the proposed IIB descriptor outperforms state-of-the-art binary descriptors and some testing float descriptors. The proposed IIB descriptor has also been successfully employed in a demo system for long-term visual localization. The code of the IIB descriptor will be publicly available.



### Vanishing Activations: A Symptom of Deep Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.11178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.11178v1)
- **Published**: 2023-05-13 15:42:26+00:00
- **Updated**: 2023-05-13 15:42:26+00:00
- **Authors**: Miles Everett, Mingjun Zhong, Georgios Leontidis
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Capsule Networks, an extension to Neural Networks utilizing vector or matrix representations instead of scalars, were initially developed to create a dynamic parse tree where visual concepts evolve from parts to complete objects. Early implementations of Capsule Networks achieved and maintain state-of-the-art results on various datasets. However, recent studies have revealed shortcomings in the original Capsule Network architecture, notably its failure to construct a parse tree and its susceptibility to vanishing gradients when deployed in deeper networks. This paper extends the investigation to a range of leading Capsule Network architectures, demonstrating that these issues are not confined to the original design. We argue that the majority of Capsule Network research has produced architectures that, while modestly divergent from the original Capsule Network, still retain a fundamentally similar structure. We posit that this inherent design similarity might be impeding the scalability of Capsule Networks. Our study contributes to the broader discussion on improving the robustness and scalability of Capsule Networks.



### Image Segmentation via Probabilistic Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/2305.07954v1
- **DOI**: 10.1109/TIP.2016.2590832
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07954v1)
- **Published**: 2023-05-13 15:56:54+00:00
- **Updated**: 2023-05-13 15:56:54+00:00
- **Authors**: Ayelet Heimowitz, Yosi Keller
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 25, no. 10, pp.
  4743-4752, Oct. 2016
- **Summary**: This work presents an unsupervised and semi-automatic image segmentation approach where we formulate the segmentation as a inference problem based on unary and pairwise assignment probabilities computed using low-level image cues. The inference is solved via a probabilistic graph matching scheme, which allows rigorous incorporation of low level image cues and automatic tuning of parameters. The proposed scheme is experimentally shown to compare favorably with contemporary semi-supervised and unsupervised image segmentation schemes, when applied to contemporary state-of-the-art image sets.



### Nonnegative Low-Rank Tensor Completion via Dual Formulation with Applications to Image and Video Completion
- **Arxiv ID**: http://arxiv.org/abs/2305.07976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.07976v1)
- **Published**: 2023-05-13 17:51:00+00:00
- **Updated**: 2023-05-13 17:51:00+00:00
- **Authors**: Tanmay Kumar Sinha, Jayadev Naram, Pawan Kumar
- **Comment**: accepted in WACV 2022
- **Journal**: None
- **Summary**: Recent approaches to the tensor completion problem have often overlooked the nonnegative structure of the data. We consider the problem of learning a nonnegative low-rank tensor, and using duality theory, we propose a novel factorization of such tensors. The factorization decouples the nonnegative constraints from the low-rank constraints. The resulting problem is an optimization problem on manifolds, and we propose a variant of Riemannian conjugate gradients to solve it. We test the proposed algorithm across various tasks such as colour image inpainting, video completion, and hyperspectral image completion. Experimental results show that the proposed method outperforms many state-of-the-art tensor completion algorithms.



### A Two-Stage Real Image Deraining Method for GT-RAIN Challenge CVPR 2023 Workshop UG$^{\textbf{2}}$+ Track 3
- **Arxiv ID**: http://arxiv.org/abs/2305.07979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.07979v1)
- **Published**: 2023-05-13 18:30:27+00:00
- **Updated**: 2023-05-13 18:30:27+00:00
- **Authors**: Yun Guo, Xueyao Xiao, Xiaoxiong Wang, Yi Li, Yi Chang, Luxin Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we briefly introduce the solution of our team HUST\li VIE for GT-Rain Challenge in CVPR 2023 UG$^{2}$+ Track 3. In this task, we propose an efficient two-stage framework to reconstruct a clear image from rainy frames. Firstly, a low-rank based video deraining method is utilized to generate pseudo GT, which fully takes the advantage of multi and aligned rainy frames. Secondly, a transformer-based single image deraining network Uformer is implemented to pre-train on large real rain dataset and then fine-tuned on pseudo GT to further improve image restoration. Moreover, in terms of visual pleasing effect, a comprehensive image processor module is utilized at the end of pipeline. Our overall framework is elaborately designed and able to handle both heavy rainy and foggy sequences provided in the final testing phase. Finally, we rank 1st on the average structural similarity (SSIM) and rank 2nd on the average peak signal-to-noise ratio (PSNR). Our code is available at https://github.com/yunguo224/UG2_Deraining.



### CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning
- **Arxiv ID**: http://arxiv.org/abs/2305.10442v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.10442v1)
- **Published**: 2023-05-13 20:06:53+00:00
- **Updated**: 2023-05-13 20:06:53+00:00
- **Authors**: Abhinav Sagar, Sai Teja Gilukara
- **Comment**: 11 pages, 6 figures, 8 tables
- **Journal**: None
- **Summary**: Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score, FID score, and path planning metrics like time cost and the number of nodes. We conduct detailed experiments and ablation studies to illustrate the feasibility of our study and show that our model performs well not only on the training dataset but also on the unseen test dataset. The advantage of our approach is that we can avoid the complicated preprocessing in the state space, our model can be generalized to complicated environments like those containing turns and narrow passages without loss of accuracy, and our model can be easily integrated with other sampling-based path planning algorithms.



### DNN-Compressed Domain Visual Recognition with Feature Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2305.08000v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.08000v2)
- **Published**: 2023-05-13 20:45:17+00:00
- **Updated**: 2023-07-26 09:43:15+00:00
- **Authors**: Yingpeng Deng, Lina J. Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based image compression was shown to achieve a competitive performance with state-of-the-art transform-based codecs. This motivated the development of new learning-based visual compression standards such as JPEG-AI. Of particular interest to these emerging standards is the development of learning-based image compression systems targeting both humans and machines. This paper is concerned with learning-based compression schemes whose compressed-domain representations can be utilized to perform visual processing and computer vision tasks directly in the compressed domain. In our work, we adopt a learning-based compressed-domain classification framework for performing visual recognition using the compressed-domain latent representation at varying bit-rates. We propose a novel feature adaptation module integrating a lightweight attention model to adaptively emphasize and enhance the key features within the extracted channel-wise information. Also, we design an adaptation training strategy to utilize the pretrained pixel-domain weights. For comparison, in addition to the performance results that are obtained using our proposed latent-based compressed-domain method, we also present performance results using compressed but fully decoded images in the pixel domain as well as original uncompressed images. The obtained performance results show that our proposed compressed-domain classification model can distinctly outperform the existing compressed-domain classification models, and that it can also yield similar accuracy results with a much higher computational efficiency as compared to the pixel-domain models that are trained using fully decoded images.



### Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.08014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2305.08014v2)
- **Published**: 2023-05-13 21:47:55+00:00
- **Updated**: 2023-06-18 18:56:00+00:00
- **Authors**: Md. Rabiul Islam, Daniel Massicotte, Philippe Y. Massicotte, Wei-Ping Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recognition performance. The All-ConvNet+TL model consists solely of convolutional layers, a simple yet efficient framework for learning invariant and discriminative representations to address the distribution shifts caused by inter-session and inter-subject data variability. Experiments on four datasets demonstrate that our proposed methods outperform the most complex existing approaches by a large margin and achieve state-of-the-art results on inter-session and inter-subject scenarios and perform on par or competitively on intra-session gesture recognition. These performance gaps increase even more when a tiny amount (e.g., a single trial) of data is available on the target domain for adaptation. These outstanding experimental results provide evidence that the current state-of-the-art models may be overparameterized for sEMG-based inter-session and inter-subject gesture recognition tasks.



### How to Train Your CheXDragon: Training Chest X-Ray Models for Transfer to Novel Tasks and Healthcare Systems
- **Arxiv ID**: http://arxiv.org/abs/2305.08017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.08017v1)
- **Published**: 2023-05-13 22:33:09+00:00
- **Updated**: 2023-05-13 22:33:09+00:00
- **Authors**: Cara Van Uden, Jeremy Irvin, Mars Huang, Nathan Dean, Jason Carr, Andrew Ng, Curtis Langlotz
- **Comment**: 13 pages, 12 figures
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) enables label efficient training for machine learning models. This is essential for domains such as medical imaging, where labels are costly and time-consuming to curate. However, the most effective supervised or SSL strategy for transferring models to different healthcare systems or novel tasks is not well understood. In this work, we systematically experiment with a variety of supervised and self-supervised pretraining strategies using multimodal datasets of medical images (chest X-rays) and text (radiology reports). We then evaluate their performance on data from two external institutions with diverse sets of tasks. In addition, we experiment with different transfer learning strategies to effectively adapt these pretrained models to new tasks and healthcare systems. Our empirical results suggest that multimodal SSL gives substantial gains over unimodal SSL in performance across new healthcare systems and tasks, comparable to models pretrained with full supervision. We demonstrate additional performance gains with models further adapted to the new dataset and task, using multimodal domain-adaptive pretraining (DAPT), linear probing then finetuning (LP-FT), and both methods combined. We offer suggestions for alternative models to use in scenarios where not all of these additions are feasible. Our results provide guidance for improving the generalization of medical image interpretation models to new healthcare systems and novel tasks.



