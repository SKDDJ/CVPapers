# Arxiv Papers in cs.CV on 2023-03-05
### Securing Biomedical Images from Unauthorized Training with Anti-Learning Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2303.02559v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02559v1)
- **Published**: 2023-03-05 03:09:03+00:00
- **Updated**: 2023-03-05 03:09:03+00:00
- **Authors**: Yixin Liu, Haohui Ye, Kai Zhang, Lichao Sun
- **Comment**: This paper is accepted as a poster for NDSS 2023
- **Journal**: None
- **Summary**: The volume of open-source biomedical data has been essential to the development of various spheres of the healthcare community since more `free' data can provide individual researchers more chances to contribute. However, institutions often hesitate to share their data with the public due to the risk of data exploitation by unauthorized third parties for another commercial usage (e.g., training AI models). This phenomenon might hinder the development of the whole healthcare research community. To address this concern, we propose a novel approach termed `unlearnable biomedical image' for protecting biomedical data by injecting imperceptible but delusive noises into the data, making them unexploitable for AI models. We formulate the problem as a bi-level optimization and propose three kinds of anti-learning perturbation generation approaches to solve the problem. Our method is an important step toward encouraging more institutions to contribute their data for the long-term development of the research community.



### MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.02575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02575v1)
- **Published**: 2023-03-05 04:05:17+00:00
- **Updated**: 2023-03-05 04:05:17+00:00
- **Authors**: Ruiqi Xian, Xijun Wang, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al., 2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16% improvement on NEC Drones(Choi et al., 2020). We will release the code at the time of publication



### How to Construct Energy for Images? Denoising Autoencoder Can Be Energy Based Model
- **Arxiv ID**: http://arxiv.org/abs/2303.03887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.03887v1)
- **Published**: 2023-03-05 05:35:55+00:00
- **Updated**: 2023-03-05 05:35:55+00:00
- **Authors**: Weili Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Energy-based models parameterize the unnormalized log-probability of data samples, but there is a lack of guidance on how to construct the "energy". In this paper, we propose a Denoising-EBM which decomposes the image energy into "semantic energy" and "texture energy". We define the "semantic energy" in the latent space of DAE to model the high-level representations, and define the pixel-level reconstruction error for denoising as "texture energy". Inspired by score-based model, our model utilizes multi-scale noisy samples for maximum-likelihood training and it outputs a vector instead of a scalar for exploring a larger set of functions during optimization. After training, the semantics are first synthesized by fast MCMC through "semantic energy", and then the pixel-level refinement of semantic image will be performed to generate perfect samples based on "texture energy". Ultimately, our model can outperform most EBMs in image generation. And we also demonstrate that Denoising-EBM has top performance among EBMs for out-of-distribution detection.



### Super-Resolution Neural Operator
- **Arxiv ID**: http://arxiv.org/abs/2303.02584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02584v1)
- **Published**: 2023-03-05 06:17:43+00:00
- **Updated**: 2023-03-05 06:17:43+00:00
- **Authors**: Min Wei, Xuesong Zhang
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: We propose Super-resolution Neural Operator (SRNO), a deep operator learning framework that can resolve high-resolution (HR) images at arbitrary scales from the low-resolution (LR) counterparts. Treating the LR-HR image pairs as continuous functions approximated with different grid sizes, SRNO learns the mapping between the corresponding function spaces. From the perspective of approximation theory, SRNO first embeds the LR input into a higher-dimensional latent representation space, trying to capture sufficient basis functions, and then iteratively approximates the implicit image function with a kernel integral mechanism, followed by a final dimensionality reduction step to generate the RGB representation at the target coordinates. The key characteristics distinguishing SRNO from prior continuous SR works are: 1) the kernel integral in each layer is efficiently implemented via the Galerkin-type attention, which possesses non-local properties in the spatial domain and therefore benefits the grid-free continuum; and 2) the multilayer attention architecture allows for the dynamic latent basis update, which is crucial for SR problems to "hallucinate" high-frequency information from the LR image. Experiments show that SRNO outperforms existing continuous SR methods in terms of both accuracy and running time. Our code is at https://github.com/2y7c3/Super-Resolution-Neural-Operator



### PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2303.02595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.02595v1)
- **Published**: 2023-03-05 07:37:28+00:00
- **Updated**: 2023-03-05 07:37:28+00:00
- **Authors**: Jiarui Lei, Xiaobo Hu, Yue Wang, Dong Liu
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: During industrial processing, unforeseen defects may arise in products due to uncontrollable factors. Although unsupervised methods have been successful in defect localization, the usual use of pre-trained models results in low-resolution outputs, which damages visual performance. To address this issue, we propose PyramidFlow, the first fully normalizing flow method without pre-trained models that enables high-resolution defect localization. Specifically, we propose a latent template-based defect contrastive localization paradigm to reduce intra-class variance, as the pre-trained models do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for multi-scale fusing and volume normalization to help generalization. Our comprehensive studies on MVTecAD demonstrate the proposed method outperforms the comparable algorithms that do not use external priors, even achieving state-of-the-art performance in more challenging BTAD scenarios.



### DPA-P2PNet: Deformable Proposal-aware P2PNet for Accurate Point-based Cell Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.02602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02602v2)
- **Published**: 2023-03-05 08:02:42+00:00
- **Updated**: 2023-08-26 07:24:35+00:00
- **Authors**: Zhongyi Shui, Sunyi Zheng, Chenglu Zhu, Shichuan Zhang, Xiaoxuan Yu, Honglin Li, Jingxiong Li, Pingyi Chen, Lin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Point-based cell detection (PCD), which pursues high-performance cell sensing under low-cost data annotation, has garnered increased attention in computational pathology community. Unlike mainstream PCD methods that rely on intermediate density map representations, the Point-to-Point network (P2PNet) has recently emerged as an end-to-end solution for PCD, demonstrating impressive cell detection accuracy and efficiency. Nevertheless, P2PNet is limited to decoding from a single-level feature map due to the scale-agnostic property of point proposals, which is insufficient to leverage multi-scale information. Moreover, the spatial distribution of pre-set point proposals is biased from that of cells, leading to inaccurate cell localization. To lift these limitations, we present DPA-P2PNet in this work. The proposed method directly extracts multi-scale features for decoding according to the coordinates of point proposals on hierarchical feature maps. On this basis, we further devise deformable point proposals to mitigate the positional bias between proposals and potential cells to promote cell localization. Inspired by practical pathological diagnosis that usually combines high-level tissue structure and low-level cell morphology for accurate cell classification, we propose a multi-field-of-view (mFoV) variant of DPA-P2PNet to accommodate additional large FoV images with tissue information as model input. Finally, we execute the first self-supervised pre-training on immunohistochemistry histopathology image data and evaluate the suitability of four representative self-supervised methods on the PCD task. Experimental results on three benchmarks and a large-scale and real-world interval dataset demonstrate the superiority of our proposed models over the state-of-the-art counterparts. Codes and pre-trained weights will be available.



### Event-based Camera Simulation using Monte Carlo Path Tracing with Adaptive Denoising
- **Arxiv ID**: http://arxiv.org/abs/2303.02608v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.02608v2)
- **Published**: 2023-03-05 08:44:01+00:00
- **Updated**: 2023-08-22 06:19:48+00:00
- **Authors**: Yuta Tsuji, Tatsuya Yatagawa, Hiroyuki Kubo, Shigeo Morishima
- **Comment**: 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: This paper presents an algorithm to obtain an event-based video from noisy frames given by physics-based Monte Carlo path tracing over a synthetic 3D scene. Given the nature of dynamic vision sensor (DVS), rendering event-based video can be viewed as a process of detecting the changes from noisy brightness values. We extend a denoising method based on a weighted local regression (WLR) to detect the brightness changes rather than applying denoising to every pixel. Specifically, we derive a threshold to determine the likelihood of event occurrence and reduce the number of times to perform the regression. Our method is robust to noisy video frames obtained from a few path-traced samples. Despite its efficiency, our method performs comparably to or even better than an approach that exhaustively denoises every frame.



### HyperPose: Camera Pose Localization using Attention Hypernetworks
- **Arxiv ID**: http://arxiv.org/abs/2303.02610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02610v1)
- **Published**: 2023-03-05 08:45:50+00:00
- **Updated**: 2023-03-05 08:45:50+00:00
- **Authors**: Ron Ferens, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose the use of attention hypernetworks in camera pose localization. The dynamic nature of natural scenes, including changes in environment, perspective, and lighting, creates an inherent domain gap between the training and test sets that limits the accuracy of contemporary localization networks. To overcome this issue, we suggest a camera pose regressor that integrates a hypernetwork. During inference, the hypernetwork generates adaptive weights for the localization regression heads based on the input image, effectively reducing the domain gap. We also suggest the use of a Transformer-Encoder as the hypernetwork, instead of the common multilayer perceptron, to derive an attention hypernetwork. The proposed approach achieves superior results compared to state-of-the-art methods on contemporary datasets. To the best of our knowledge, this is the first instance of using hypernetworks in camera pose regression, as well as using Transformer-Encoders as hypernetworks. We make our code publicly available.



### Estimating Extreme 3D Image Rotation with Transformer Cross-Attention
- **Arxiv ID**: http://arxiv.org/abs/2303.02615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02615v1)
- **Published**: 2023-03-05 09:07:26+00:00
- **Updated**: 2023-03-05 09:07:26+00:00
- **Authors**: Shay Dekel, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: The estimation of large and extreme image rotation plays a key role in multiple computer vision domains, where the rotated images are related by a limited or a non-overlapping field of view. Contemporary approaches apply convolutional neural networks to compute a 4D correlation volume to estimate the relative rotation between image pairs. In this work, we propose a cross-attention-based approach that utilizes CNN feature maps and a Transformer-Encoder, to compute the cross-attention between the activation maps of the image pairs, which is shown to be an improved equivalent of the 4D correlation volume, used in previous works. In the suggested approach, higher attention scores are associated with image regions that encode visual cues of rotation. Our approach is end-to-end trainable and optimizes a simple regression loss. It is experimentally shown to outperform contemporary state-of-the-art schemes when applied to commonly used image rotation datasets and benchmarks, and establishes a new state-of-the-art accuracy on these datasets. We make our code publicly available.



### Deep-Learning-based Counting Methods, Datasets, and Applications in Agriculture -- A Review
- **Arxiv ID**: http://arxiv.org/abs/2303.02632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02632v2)
- **Published**: 2023-03-05 10:17:10+00:00
- **Updated**: 2023-05-09 08:29:16+00:00
- **Authors**: Guy Farjon, Liu Huijun, Yael Edan
- **Comment**: None
- **Journal**: None
- **Summary**: The number of objects is considered an important factor in a variety of tasks in the agricultural domain. Automated counting can improve farmers decisions regarding yield estimation, stress detection, disease prevention, and more. In recent years, deep learning has been increasingly applied to many agriculture-related applications, complementing conventional computer-vision algorithms for counting agricultural objects. This article reviews progress in the past decade and the state of the art for counting methods in agriculture, focusing on deep-learning methods. It presents an overview of counting algorithms, metrics, platforms, and sensors, a list of all publicly available datasets, and an in-depth discussion of various deep-learning methods used for counting. Finally, it discusses open challenges in object counting using deep learning and gives a glimpse into new directions and future perspectives for counting research. The review reveals a major leap forward in object counting in agriculture in the past decade, led by the penetration of deep learning methods into counting platforms.



### VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2303.02635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.02635v1)
- **Published**: 2023-03-05 10:32:26+00:00
- **Updated**: 2023-03-05 10:32:26+00:00
- **Authors**: Kang Chen, Xiangqian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The ideal form of Visual Question Answering requires understanding, grounding and reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most existing VQA benchmarks are limited to just picking the answer from a pre-defined set of options and lack attention to text. We present a new challenge with a dataset that contains 23,781 questions based on 10124 image-text pairs. Specifically, the task requires the model to align multimedia representations of the same entity to implement multi-hop reasoning between image and text and finally use natural language to answer the question. The aim of this challenge is to develop and benchmark models that are capable of multimedia entity alignment, multi-step reasoning and open-ended answer generation.



### CueCAn: Cue Driven Contextual Attention For Identifying Missing Traffic Signs on Unconstrained Roads
- **Arxiv ID**: http://arxiv.org/abs/2303.02641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.02641v1)
- **Published**: 2023-03-05 11:06:20+00:00
- **Updated**: 2023-03-05 11:06:20+00:00
- **Authors**: Varun Gupta, Anbumani Subramanian, C. V. Jawahar, Rohit Saluja
- **Comment**: International Conference on Robotics and Automation (ICRA'23)
- **Journal**: None
- **Summary**: Unconstrained Asian roads often involve poor infrastructure, affecting overall road safety. Missing traffic signs are a regular part of such roads. Missing or non-existing object detection has been studied for locating missing curbs and estimating reasonable regions for pedestrians on road scene images. Such methods involve analyzing task-specific single object cues. In this paper, we present the first and most challenging video dataset for missing objects, with multiple types of traffic signs for which the cues are visible without the signs in the scenes. We refer to it as the Missing Traffic Signs Video Dataset (MTSVD). MTSVD is challenging compared to the previous works in two aspects i) The traffic signs are generally not present in the vicinity of their cues, ii) The traffic signs cues are diverse and unique. Also, MTSVD is the first publicly available missing object dataset. To train the models for identifying missing signs, we complement our dataset with 10K traffic sign tracks, with 40 percent of the traffic signs having cues visible in the scenes. For identifying missing signs, we propose the Cue-driven Contextual Attention units (CueCAn), which we incorporate in our model encoder. We first train the encoder to classify the presence of traffic sign cues and then train the entire segmentation model end-to-end to localize missing traffic signs. Quantitative and qualitative analysis shows that CueCAn significantly improves the performance of base models.



### Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2303.02648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02648v1)
- **Published**: 2023-03-05 11:45:53+00:00
- **Updated**: 2023-03-05 11:45:53+00:00
- **Authors**: Pranav Dandwate, Chaitanya Shahane, Vandana Jagtap, Shridevi C. Karande
- **Comment**: 13 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: In a globalized world at the present epoch of generative intelligence, most of the manual labour tasks are automated with increased efficiency. This can support businesses to save time and money. A crucial component of generative intelligence is the integration of vision and language. Consequently, image captioning become an intriguing area of research. There have been multiple attempts by the researchers to solve this problem with different deep learning architectures, although the accuracy has increased, but the results are still not up to standard. This study buckles down to the comparison of Transformer and LSTM with attention block model on MS-COCO dataset, which is a standard dataset for image captioning. For both the models we have used pretrained Inception-V3 CNN encoder for feature extraction of the images. The Bilingual Evaluation Understudy score (BLEU) is used to checked the accuracy of caption generated by both models. Along with the transformer and LSTM with attention block models,CLIP-diffusion model, M2-Transformer model and the X-Linear Attention model have been discussed with state of the art accuracy.



### Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2303.03966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.03966v1)
- **Published**: 2023-03-05 11:50:34+00:00
- **Updated**: 2023-03-05 11:50:34+00:00
- **Authors**: Jaewon Lee, Injae Kim, Hwan Heo, Hyunwoo J. Kim
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: We present a learning framework for reconstructing neural scene representations from a small number of unconstrained tourist photos. Since each image contains transient occluders, decomposing the static and transient components is necessary to construct radiance fields with such in-the-wild photographs where existing methods require a lot of training data. We introduce SF-NeRF, aiming to disentangle those two components with only a few images given, which exploits semantic information without any supervision. The proposed method contains an occlusion filtering module that predicts the transient color and its opacity for each pixel, which enables the NeRF model to solely learn the static scene representation. This filtering module learns the transient phenomena guided by pixel-wise semantic features obtained by a trainable image encoder that can be trained across multiple scenes to learn the prior of transient objects. Furthermore, we present two techniques to prevent ambiguous decomposition and noisy results of the filtering module. We demonstrate that our method outperforms state-of-the-art novel view synthesis methods on Phototourism dataset in a few-shot setting.



### On Modifying a Neural Network's Perception
- **Arxiv ID**: http://arxiv.org/abs/2303.02655v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2303.02655v1)
- **Published**: 2023-03-05 12:09:37+00:00
- **Updated**: 2023-03-05 12:09:37+00:00
- **Authors**: Manuel de Sousa Ribeiro, João Leite
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks have proven to be extremely useful models that have allowed for multiple recent breakthroughs in the field of Artificial Intelligence and many others. However, they are typically regarded as black boxes, given how difficult it is for humans to interpret how these models reach their results. In this work, we propose a method which allows one to modify what an artificial neural network is perceiving regarding specific human-defined concepts, enabling the generation of hypothetical scenarios that could help understand and even debug the neural network model. Through empirical evaluation, in a synthetic dataset and in the ImageNet dataset, we test the proposed method on different models, assessing whether the performed manipulations are well interpreted by the models, and analyzing how they react to them.



### SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-friendly Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2303.02660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02660v2)
- **Published**: 2023-03-05 12:35:58+00:00
- **Updated**: 2023-04-11 09:38:16+00:00
- **Authors**: Meiling Fang, Marco Huber, Naser Damer
- **Comment**: Accepted at CVPR workshop 2023
- **Journal**: None
- **Summary**: Recently, significant progress has been made in face presentation attack detection (PAD), which aims to secure face recognition systems against presentation attacks, owing to the availability of several face PAD datasets. However, all available datasets are based on privacy and legally-sensitive authentic biometric data with a limited number of subjects. To target these legal and technical challenges, this work presents the first synthetic-based face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset. The bona fide samples in SynthASpoof are synthetically generated and the attack samples are collected by presenting such synthetic data to capture systems in a real attack scenario. The experimental results demonstrate the feasibility of using SynthASpoof for the development of face PAD. Moreover, we boost the performance of such a solution by incorporating the domain generalization tool MixStyle into the PAD solutions. Additionally, we showed the viability of using synthetic data as a supplement to enrich the diversity of limited authentic training data and consistently enhance PAD performances. The SynthASpoof dataset, containing 25,000 bona fide and 78,800 attack samples, the implementation, and the pre-trained weights are made publicly available.



### Learned Lossless Compression for JPEG via Frequency-Domain Prediction
- **Arxiv ID**: http://arxiv.org/abs/2303.02666v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02666v1)
- **Published**: 2023-03-05 13:15:28+00:00
- **Updated**: 2023-03-05 13:15:28+00:00
- **Authors**: Jixiang Luo, Shaohui Li, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: JPEG images can be further compressed to enhance the storage and transmission of large-scale image datasets. Existing learned lossless compressors for RGB images cannot be well transferred to JPEG images due to the distinguishing distribution of DCT coefficients and raw pixels. In this paper, we propose a novel framework for learned lossless compression of JPEG images that achieves end-to-end optimized prediction of the distribution of decoded DCT coefficients. To enable learning in the frequency domain, DCT coefficients are partitioned into groups to utilize implicit local redundancy. An autoencoder-like architecture is designed based on the weight-shared blocks to realize entropy modeling of grouped DCT coefficients and independently compress the priors. We attempt to realize learned lossless compression of JPEG images in the frequency domain. Experimental results demonstrate that the proposed framework achieves superior or comparable performance in comparison to most recent lossless compressors with handcrafted context modeling for JPEG images.



### Continuous-Time Gaussian Process Motion-Compensation for Event-vision Pattern Tracking with Distance Fields
- **Arxiv ID**: http://arxiv.org/abs/2303.02672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02672v1)
- **Published**: 2023-03-05 13:48:20+00:00
- **Updated**: 2023-03-05 13:48:20+00:00
- **Authors**: Cedric Le Gentil, Ignacio Alzugaray, Teresa Vidal-Calleja
- **Comment**: Accepted for presentation at the 2023 IEEE International Conference
  on Robotics and Automation
- **Journal**: None
- **Summary**: This work addresses the issue of motion compensation and pattern tracking in event camera data. An event camera generates asynchronous streams of events triggered independently by each of the pixels upon changes in the observed intensity. Providing great advantages in low-light and rapid-motion scenarios, such unconventional data present significant research challenges as traditional vision algorithms are not directly applicable to this sensing modality. The proposed method decomposes the tracking problem into a local SE(2) motion-compensation step followed by a homography registration of small motion-compensated event batches. The first component relies on Gaussian Process (GP) theory to model the continuous occupancy field of the events in the image plane and embed the camera trajectory in the covariance kernel function. In doing so, estimating the trajectory is done similarly to GP hyperparameter learning by maximising the log marginal likelihood of the data. The continuous occupancy fields are turned into distance fields and used as templates for homography-based registration. By benchmarking the proposed method against other state-of-the-art techniques, we show that our open-source implementation performs high-accuracy motion compensation and produces high-quality tracks in real-world scenarios.



### Text2Face: A Multi-Modal 3D Face Model
- **Arxiv ID**: http://arxiv.org/abs/2303.02688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02688v2)
- **Published**: 2023-03-05 15:06:54+00:00
- **Updated**: 2023-03-08 11:28:21+00:00
- **Authors**: Will Rowan, Patrik Huber, Nick Pears, Andrew Keeling
- **Comment**: Fixed formatting and a typo
- **Journal**: None
- **Summary**: We present the first 3D morphable modelling approach, whereby 3D face shape can be directly and completely defined using a textual prompt. Building on work in multi-modal learning, we extend the FLAME head model to a common image-and-text latent space. This allows for direct 3D Morphable Model (3DMM) parameter generation and therefore shape manipulation from textual descriptions. Our method, Text2Face, has many applications; for example: generating police photofits where the input is already in natural language. It further enables multi-modal 3DMM image fitting to sketches and sculptures, as well as images.



### Maximizing Spatio-Temporal Entropy of Deep 3D CNNs for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.02693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02693v1)
- **Published**: 2023-03-05 15:11:53+00:00
- **Updated**: 2023-03-05 15:11:53+00:00
- **Authors**: Junyan Wang, Zhenhong Sun, Yichen Qian, Dong Gong, Xiuyu Sun, Ming Lin, Maurice Pagnucco, Yang Song
- **Comment**: This manuscript has been accepted at ICLR 2023
- **Journal**: None
- **Summary**: 3D convolution neural networks (CNNs) have been the prevailing option for video recognition. To capture the temporal information, 3D convolutions are computed along the sequences, leading to cubically growing and expensive computations. To reduce the computational cost, previous methods resort to manually designed 3D/2D CNN structures with approximations or automatic search, which sacrifice the modeling ability or make training time-consuming. In this work, we propose to automatically design efficient 3D CNN architectures via a novel training-free neural architecture search approach tailored for 3D CNNs considering the model complexity. To measure the expressiveness of 3D CNNs efficiently, we formulate a 3D CNN as an information system and derive an analytic entropy score, based on the Maximum Entropy Principle. Specifically, we propose a spatio-temporal entropy score (STEntr-Score) with a refinement factor to handle the discrepancy of visual information in spatial and temporal dimensions, through dynamically leveraging the correlation between the feature map size and kernel size depth-wisely. Highly efficient and expressive 3D CNN architectures, \ie entropy-based 3D CNNs (E3D family), can then be efficiently searched by maximizing the STEntr-Score under a given computational budget, via an evolutionary algorithm without training the network parameters. Extensive experiments on Something-Something V1\&V2 and Kinetics400 demonstrate that the E3D family achieves state-of-the-art performance with higher computational efficiency. Code is available at https://github.com/alibaba/lightweight-neural-architecture-search.



### Robust affine point matching via quadratic assignment on Grassmannians
- **Arxiv ID**: http://arxiv.org/abs/2303.02698v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02698v3)
- **Published**: 2023-03-05 15:27:24+00:00
- **Updated**: 2023-08-29 20:57:07+00:00
- **Authors**: Alexander Kolpakov, Michael Werman
- **Comment**: 8 pages, 23 figures; GitHub repository at
  (https://github.com/sashakolpakov/rag)
- **Journal**: None
- **Summary**: Robust Affine matching with Grassmannians (RAG) is a new algorithm to perform affine registration of point clouds. The algorithm is based on minimizing the Frobenius distance between two elements of the Grassmannian. For this purpose, an indefinite relaxation of the Quadratic Assignment Problem (QAP) is used, and several approaches to affine feature matching are studied and compared. Experiments demonstrate that RAG is more robust to noise and point discrepancy than previous methods.



### HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling
- **Arxiv ID**: http://arxiv.org/abs/2303.02700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02700v2)
- **Published**: 2023-03-05 15:28:13+00:00
- **Updated**: 2023-03-24 03:34:25+00:00
- **Authors**: Yujian Zheng, Zirong Jin, Moran Li, Haibin Huang, Chongyang Ma, Shuguang Cui, Xiaoguang Han
- **Comment**: CVPR 2023 Highlight, project page:
  https://paulyzheng.github.io/research/hairstep/
- **Journal**: None
- **Summary**: In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typically use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate representation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncertain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermediate representation, termed as HairStep, which consists of a strand map and a depth map. It is found that HairStep not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real images. Specifically, we collect a dataset of 1,250 portrait images with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling. Our experiments show that HairStep narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction.



### Deep Learning in the Field of Biometric Template Protection: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2303.02715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02715v1)
- **Published**: 2023-03-05 17:06:40+00:00
- **Updated**: 2023-03-05 17:06:40+00:00
- **Authors**: Christian Rathgeb, Jascha Kolberg, Andreas Uhl, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protection. Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometrics. In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes. This work provides an overview of how advances in deep learning take influence on the field of biometric template protection. The interrelation between improved biometric performance rates and security in biometric template protection is elaborated. Further, the use of deep learning for obtaining feature representations that are suitable for biometric template protection is discussed. Novel methods that apply deep learning to achieve various goals of biometric template protection are surveyed along with deep learning-based attacks.



### Learning to Localize in Unseen Scenes with Relative Pose Regressors
- **Arxiv ID**: http://arxiv.org/abs/2303.02717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02717v1)
- **Published**: 2023-03-05 17:12:50+00:00
- **Updated**: 2023-03-05 17:12:50+00:00
- **Authors**: Ofer Idan, Yoli Shavit, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Relative pose regressors (RPRs) localize a camera by estimating its relative translation and rotation to a pose-labelled reference. Unlike scene coordinate regression and absolute pose regression methods, which learn absolute scene parameters, RPRs can (theoretically) localize in unseen environments, since they only learn the residual pose between camera pairs. In practice, however, the performance of RPRs is significantly degraded in unseen scenes. In this work, we propose to aggregate paired feature maps into latent codes, instead of operating on global image descriptors, in order to improve the generalization of RPRs. We implement aggregation with concatenation, projection, and attention operations (Transformer Encoders) and learn to regress the relative pose parameters from the resulting latent codes. We further make use of a recently proposed continuous representation of rotation matrices, which alleviates the limitations of the commonly used quaternions. Compared to state-of-the-art RPRs, our model is shown to localize significantly better in unseen environments, across both indoor and outdoor benchmarks, while maintaining competitive performance in seen scenes. We validate our findings and architecture design through multiple ablations. Our code and pretrained models is publicly available.



### Vision based Virtual Guidance for Navigation
- **Arxiv ID**: http://arxiv.org/abs/2303.02731v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02731v1)
- **Published**: 2023-03-05 17:55:15+00:00
- **Updated**: 2023-03-05 17:55:15+00:00
- **Authors**: Hsuan-Kung Yang, Yu-Ying Chen, Tsung-Chih Chiang, Chia-Chuan Hsu, Chun-Chia Huang, Chun-Wei Huang, Jou-Min Liu, Ting-Ru Liu, Tsu-Ching Hsiao, Chun-Yi Lee
- **Comment**: Yu-Ying Chen, Tsung-Chih Chiang, Chia-Chuan Hsu, Chun-Chia Huang,
  Chun-Wei Huang, Jou-Min Liu, and Ting-Ru Liu contributed equally to this
  work, names listed in alphabetical order; This work has been submitted to the
  IEEE for possible publication
- **Journal**: None
- **Summary**: This paper explores the impact of virtual guidance on mid-level representation-based navigation, where an agent performs navigation tasks based solely on visual observations. Instead of providing distance measures or numerical directions to guide the agent, which may be difficult for it to interpret visually, the paper investigates the potential of different forms of virtual guidance schemes on navigation performance. Three schemes of virtual guidance signals are explored: virtual navigation path, virtual waypoints, and a combination of both. The experiments were conducted using a virtual city built with the Unity engine to train the agents while avoiding obstacles. The results show that virtual guidance provides the agent with more meaningful navigation information and achieves better performance in terms of path completion rates and navigation efficiency. In addition, a set of analyses were provided to investigate the failure cases and the navigated trajectories, and a pilot study was conducted for the real-world scenarios.



### Reparameterization through Spatial Gradient Scaling
- **Arxiv ID**: http://arxiv.org/abs/2303.02733v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02733v2)
- **Published**: 2023-03-05 17:57:33+00:00
- **Updated**: 2023-03-07 02:07:01+00:00
- **Authors**: Alexander Detkov, Mohammad Salameh, Muhammad Fetrat Qharabagh, Jialin Zhang, Wei Lui, Shangling Jui, Di Niu
- **Comment**: Published at ICLR 2023. Code available at
  https://github.com/Ascend-Research/Reparameterization
- **Journal**: None
- **Summary**: Reparameterization aims to improve the generalization of deep neural networks by transforming convolutional layers into equivalent multi-branched structures during training. However, there exists a gap in understanding how reparameterization may change and benefit the learning process of neural networks. In this paper, we present a novel spatial gradient scaling method to redistribute learning focus among weights in convolutional networks. We prove that spatial gradient scaling achieves the same learning dynamics as a branched reparameterization yet without introducing structural changes into the network. We further propose an analytical approach that dynamically learns scalings for each convolutional layer based on the spatial characteristics of its input feature map gauged by mutual information. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that without searching for reparameterized structures, our proposed scaling method outperforms the state-of-the-art reparameterization strategies at a lower computational cost.



### Scalable Object Detection on Embedded Devices Using Weight Pruning and Singular Value Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2303.02735v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.02735v2)
- **Published**: 2023-03-05 18:02:54+00:00
- **Updated**: 2023-03-17 15:49:09+00:00
- **Authors**: Dohyun Ham, Jaeyeop Jeong, June-Kyoo Park, Raehyeon Jeong, Seungmin Jeon, Hyeongjun Jeon, Yewon Lim
- **Comment**: 8 pages, 3 figures. A report of the project done as part of the
  Yonsei-Roboin project for the 2nd semester, 2022
- **Journal**: None
- **Summary**: This paper presents a method for optimizing object detection models by combining weight pruning and singular value decomposition (SVD). The proposed method was evaluated on a custom dataset of street work images obtained from https://universe.roboflow.com/roboflow-100/street-work. The dataset consists of 611 training images, 175 validation images, and 87 test images with 7 classes. We compared the performance of the optimized models with the original unoptimized model in terms of frame rate, mean average precision (mAP@50), and weight size. The results show that the weight pruning + SVD model achieved a 0.724 mAP@50 with a frame rate of 1.48 FPS and a weight size of 12.1 MB, outperforming the original model (0.717 mAP@50, 1.50 FPS, and 12.3 MB). Precision-recall curves were also plotted for all models. Our work demonstrates that the proposed method can effectively optimize object detection models while balancing accuracy, speed, and model size.



### SePaint: Semantic Map Inpainting via Multinomial Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2303.02737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02737v1)
- **Published**: 2023-03-05 18:04:28+00:00
- **Updated**: 2023-03-05 18:04:28+00:00
- **Authors**: Zheng Chen, Deepak Duggirala, David Crandall, Lei Jiang, Lantao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Prediction beyond partial observations is crucial for robots to navigate in unknown environments because it can provide extra information regarding the surroundings beyond the current sensing range or resolution. In this work, we consider the inpainting of semantic Bird's-Eye-View maps. We propose SePaint, an inpainting model for semantic data based on generative multinomial diffusion. To maintain semantic consistency, we need to condition the prediction for the missing regions on the known regions. We propose a novel and efficient condition strategy, Look-Back Condition (LB-Con), which performs one-step look-back operations during the reverse diffusion process. By doing so, we are able to strengthen the harmonization between unknown and known parts, leading to better completion performance. We have conducted extensive experiments on different datasets, showing our proposed model outperforms commonly used interpolation methods in various robotic applications.



### IDA: Informed Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.02741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02741v1)
- **Published**: 2023-03-05 18:16:34+00:00
- **Updated**: 2023-03-05 18:16:34+00:00
- **Authors**: Zheng Chen, Zhengming Ding, Jason M. Gregory, Lantao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Mixup-based data augmentation has been validated to be a critical stage in the self-training framework for unsupervised domain adaptive semantic segmentation (UDA-SS), which aims to transfer knowledge from a well-annotated (source) domain to an unlabeled (target) domain. Existing self-training methods usually adopt the popular region-based mixup techniques with a random sampling strategy, which unfortunately ignores the dynamic evolution of different semantics across various domains as training proceeds. To improve the UDA-SS performance, we propose an Informed Domain Adaptation (IDA) model, a self-training framework that mixes the data based on class-level segmentation performance, which aims to emphasize small-region semantics during mixup. In our IDA model, the class-level performance is tracked by an expected confidence score (ECS). We then use a dynamic schedule to determine the mixing ratio for data in different domains. Extensive experimental results reveal that our proposed method is able to outperform the state-of-the-art UDA-SS method by a margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in the adaptation of SYNTHIA to Cityscapes.



### Frequency-domain Blind Quality Assessment of Blurred and Blocking-artefact Images using Gaussian Process Regression model
- **Arxiv ID**: http://arxiv.org/abs/2303.02753v1
- **DOI**: 10.1016/j.image.2022.116651
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2303.02753v1)
- **Published**: 2023-03-05 19:20:55+00:00
- **Updated**: 2023-03-05 19:20:55+00:00
- **Authors**: Maryam Viqar, Athar A. Moinuddin, Ekram Khan, M. Ghanbari
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the standard image and video codecs are block-based and depending upon the compression ratio the compressed images/videos suffer from different distortions. At low ratios, blurriness is observed and as compression increases blocking artifacts occur. Generally, in order to reduce blockiness, images are low-pass filtered which leads to more blurriness. Also, in bokeh mode images they are commonly seen: blurriness as a result of intentional blurred background while blocking artifact and global blurriness arising due to compression. Therefore, such visual media suffer from both blockiness and blurriness distortions. Along with this, noise is also commonly encountered distortion. Most of the existing works on quality assessment quantify these distortions individually. This paper proposes a methodology to blindly measure overall quality of an image suffering from these distortions, individually as well as jointly. This is achieved by considering the sum of absolute values of low and high-frequency Discrete Frequency Transform (DFT) coefficients defined as sum magnitudes. The number of blocks lying in specific ranges of sum magnitudes including zero-valued AC coefficients and mean of 100 maximum and 100 minimum values of these sum magnitudes are used as feature vectors. These features are then fed to the Machine Learning (ML) based Gaussian Process Regression (GPR) model, which quantifies the image quality. The simulation results show that the proposed method can estimate the quality of images distorted with the blockiness, blurriness, noise and their combinations. It is relatively fast compared to many state-of-art methods, and therefore is suitable for real-time quality monitoring applications.



### Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes
- **Arxiv ID**: http://arxiv.org/abs/2303.02760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02760v2)
- **Published**: 2023-03-05 20:05:21+00:00
- **Updated**: 2023-04-05 07:36:48+00:00
- **Authors**: Xuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, Lei Zhang
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: Humans have long been recorded in a variety of forms since antiquity. For example, sculptures and paintings were the primary media for depicting human beings before the invention of cameras. However, most current human-centric computer vision tasks like human pose estimation and human image generation focus exclusively on natural images in the real world. Artificial humans, such as those in sculptures, paintings, and cartoons, are commonly neglected, making existing models fail in these scenarios. As an abstraction of life, art incorporates humans in both natural and artificial scenes. We take advantage of it and introduce the Human-Art dataset to bridge related tasks in natural and artificial scenarios. Specifically, Human-Art contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D. It is, therefore, comprehensive and versatile for various downstream tasks. We also provide a rich set of baseline results and detailed analyses for related tasks, including human detection, 2D and 3D human pose estimation, image generation, and motion transfer. As a challenging dataset, we hope Human-Art can provide insights for relevant research and open up new research questions.



### A Study of Augmentation Methods for Handwritten Stenography Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.02761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02761v1)
- **Published**: 2023-03-05 20:06:19+00:00
- **Updated**: 2023-03-05 20:06:19+00:00
- **Authors**: Raphaela Heil, Eva Breznik
- **Comment**: None
- **Journal**: None
- **Summary**: One of the factors limiting the performance of handwritten text recognition (HTR) for stenography is the small amount of annotated training data. To alleviate the problem of data scarcity, modern HTR methods often employ data augmentation. However, due to specifics of the stenographic script, such settings may not be directly applicable for stenography recognition. In this work, we study 22 classical augmentation techniques, most of which are commonly used for HTR of other scripts, such as Latin handwriting. Through extensive experiments, we identify a group of augmentations, including for example contained ranges of random rotation, shifts and scaling, that are beneficial to the use case of stenography recognition. Furthermore, a number of augmentation approaches, leading to a decrease in recognition performance, are identified. Our results are supported by statistical hypothesis testing. Links to the publicly available dataset and codebase are provided.



### A Low-Cost Portable Apparatus to Analyze Oral Fluid Droplets and Quantify the Efficacy of Masks
- **Arxiv ID**: http://arxiv.org/abs/2303.02776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02776v1)
- **Published**: 2023-03-05 21:30:49+00:00
- **Updated**: 2023-03-05 21:30:49+00:00
- **Authors**: Ava Tan Bhowmik
- **Comment**: 13 pages, 15 figures. arXiv admin note: substantial text overlap with
  arXiv:2201.03993
- **Journal**: None
- **Summary**: Every year, about 4 million people die from upper respiratory infections. Mask-wearing is crucial in preventing the spread of pathogen-containing droplets, which is the primary cause of these illnesses. However, most techniques for mask efficacy evaluation are expensive to set up and complex to operate. In this work, a novel, low-cost, and quantitative metrology to visualize, track, and analyze orally-generated fluid droplets is developed. The project has four stages: setup optimization, data collection, data analysis, and application development. The metrology was initially developed in a dark closet as a proof of concept using common household materials and was subsequently implemented into a portable apparatus. Tonic water and UV darklight tube lights are selected to visualize fluorescent droplet and aerosol propagation with automated analysis developed using open-source software. The dependencies of oral fluid droplet generation and propagation on various factors are studied in detail and established using this metrology. Additionally, the smallest detectable droplet size was mathematically correlated to height and airborne time. The efficacy of different types of masks is evaluated and associated with fabric microstructures. It is found that masks with smaller-sized pores and thicker material are more effective. This technique can easily be constructed at home using materials that total to a cost of below \$60, thereby enabling a low-cost and accurate metrology.



### DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2303.02165v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2303.02165v3)
- **Published**: 2023-03-05 21:31:49+00:00
- **Updated**: 2023-05-29 19:27:38+00:00
- **Authors**: Xuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, Yanzhi Wang
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: The rapid advances in Vision Transformer (ViT) refresh the state-of-the-art performances in various vision tasks, overshadowing the conventional CNN-based models. This ignites a few recent striking-back research in the CNN world showing that pure CNN models can achieve as good performance as ViT models when carefully tuned. While encouraging, designing such high-performance CNN models is challenging, requiring non-trivial prior knowledge of network design. To this end, a novel framework termed Mathematical Architecture Design for Deep CNN (DeepMAD) is proposed to design high-performance CNN models in a principled way. In DeepMAD, a CNN network is modeled as an information processing system whose expressiveness and effectiveness can be analytically formulated by their structural parameters. Then a constrained mathematical programming (MP) problem is proposed to optimize these structural parameters. The MP problem can be easily solved by off-the-shelf MP solvers on CPUs with a small memory footprint. In addition, DeepMAD is a pure mathematical framework: no GPU or training data is required during network design. The superiority of DeepMAD is validated on multiple large-scale computer vision benchmark datasets. Notably on ImageNet-1k, only using conventional convolutional layers, DeepMAD achieves 0.7% and 1.5% higher top-1 accuracy than ConvNeXt and Swin on Tiny level, and 0.8% and 0.9% higher on Small level.



