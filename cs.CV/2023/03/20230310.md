# Arxiv Papers in cs.CV on 2023-03-10
### Fusarium head blight detection, spikelet estimation, and severity assessment in wheat using 3D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2303.05634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05634v1)
- **Published**: 2023-03-10 00:46:32+00:00
- **Updated**: 2023-03-10 00:46:32+00:00
- **Authors**: Oumaima Hamila, Christopher J. Henry, Oscar I. Molina, Christopher P. Bidinosti, Maria Antonia Henriquez
- **Comment**: None
- **Journal**: None
- **Summary**: Fusarium head blight (FHB) is one of the most significant diseases affecting wheat and other small grain cereals worldwide. The development of resistant varieties requires the laborious task of field and greenhouse phenotyping. The applications considered in this work are the automated detection of FHB disease symptoms expressed on a wheat plant, the automated estimation of the total number of spikelets and the total number of infected spikelets on a wheat head, and the automated assessment of the FHB severity in infected wheat. The data used to generate the results are 3-dimensional (3D) multispectral point clouds (PC), which are 3D collections of points - each associated with a red, green, blue (RGB), and near-infrared (NIR) measurement. Over 300 wheat plant images were collected using a multispectral 3D scanner, and the labelled UW-MRDC 3D wheat dataset was created. The data was used to develop novel and efficient 3D convolutional neural network (CNN) models for FHB detection, which achieved 100% accuracy. The influence of the multispectral information on performance was evaluated, and our results showed the dominance of the RGB channels over both the NIR and the NIR plus RGB channels combined. Furthermore, novel and efficient 3D CNNs were created to estimate the total number of spikelets and the total number of infected spikelets on a wheat head, and our best models achieved mean absolute errors (MAE) of 1.13 and 1.56, respectively. Moreover, 3D CNN models for FHB severity estimation were created, and our best model achieved 8.6 MAE. A linear regression analysis between the visual FHB severity assessment and the FHB severity predicted by our 3D CNN was performed, and the results showed a significant correlation between the two variables with a 0.0001 P-value and 0.94 R-squared.



### Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images
- **Arxiv ID**: http://arxiv.org/abs/2303.05639v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05639v2)
- **Published**: 2023-03-10 01:04:27+00:00
- **Updated**: 2023-03-17 18:50:19+00:00
- **Authors**: Ankit Manerikar, Avinash C. Kak
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework for the automatic one-shot segmentation of synthetic images generated by a StyleGAN. Our framework is based on the observation that the multi-scale hidden features in the GAN generator hold useful semantic information that can be utilized for automatic on-the-fly segmentation of the generated images. Using these features, our framework learns to segment synthetic images using a self-supervised contrastive clustering algorithm that projects the hidden features into a compact space for per-pixel classification. This novel contrastive learner is based on using a pixel-wise swapped prediction loss for image segmentation that leads to faster learning of the feature vectors for one-shot segmentation. We have tested our implementation on a number of standard benchmarks to yield a segmentation performance that not only outperforms the semi-supervised baseline methods by an average wIoU margin of 1.02% but also improves the inference speeds by a factor of 4.5. Finally, we also show the results of using the proposed one-shot learner in implementing BagGAN, a framework for producing annotated synthetic baggage X-ray scans for threat detection. This framework was trained and tested on the PIDRay baggage benchmark to yield a performance comparable to its baseline segmenter based on manual annotations.



### Longitudinal Performance of Iris Recognition in Children: Time Intervals up to Six years
- **Arxiv ID**: http://arxiv.org/abs/2303.12720v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.12720v1)
- **Published**: 2023-03-10 01:39:09+00:00
- **Updated**: 2023-03-10 01:39:09+00:00
- **Authors**: Priyanka Das, Naveen G Venkataswamy, Laura Holsopple, Masudul H Imtiaz, Michael Schuckers, Stephanie Schuckers
- **Comment**: Accepted for presentation at International Workshop on Biometrics and
  Forensics 2023 (IWBF)
- **Journal**: None
- **Summary**: The temporal stability of iris recognition performance is core to its success as a biometric modality. With the expanding horizon of applications for children, gaps in the knowledge base on the temporal stability of iris recognition performance in children have impacted decision-making during applications at the global scale. This report presents the most extensive analysis of longitudinal iris recognition performance in children with data from the same 230 children over 6.5 years between enrollment and query for ages 4 to 17 years. Assessment of match scores, statistical modelling of variability factors impacting match scores and in-depth assessment of the root causes of the false rejections concludes no impact on iris recognition performance due to aging.



### Iterative Few-shot Semantic Segmentation from Image Label Text
- **Arxiv ID**: http://arxiv.org/abs/2303.05646v1
- **DOI**: 10.24963/ijcai.2022/193
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05646v1)
- **Published**: 2023-03-10 01:48:14+00:00
- **Updated**: 2023-03-10 01:48:14+00:00
- **Authors**: Haohan Wang, Liang Liu, Wuhao Zhang, Jiangning Zhang, Zhenye Gan, Yabiao Wang, Chengjie Wang, Haoqian Wang
- **Comment**: ijcai 2022
- **Journal**: None
- **Summary**: Few-shot semantic segmentation aims to learn to segment unseen class objects with the guidance of only a few support images. Most previous methods rely on the pixel-level label of support images. In this paper, we focus on a more challenging setting, in which only the image-level labels are available. We propose a general framework to firstly generate coarse masks with the help of the powerful vision-language model CLIP, and then iteratively and mutually refine the mask predictions of support and query images. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our method not only outperforms the state-of-the-art weakly supervised approaches by a significant margin, but also achieves comparable or better results to recent supervised methods. Moreover, our method owns an excellent generalization ability for the images in the wild and uncommon classes. Code will be available at https://github.com/Whileherham/IMR-HSNet.



### GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose
- **Arxiv ID**: http://arxiv.org/abs/2303.05652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05652v1)
- **Published**: 2023-03-10 02:08:01+00:00
- **Updated**: 2023-03-10 02:08:01+00:00
- **Authors**: Yingxuan You, Hong Liu, Xia Li, Wenhao Li, Ti Wang, Runwei Ding
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: 3D human mesh recovery from a 2D pose plays an important role in various applications. However, it is hard for existing methods to simultaneously capture the multiple relations during the evolution from skeleton to mesh, including joint-joint, joint-vertex and vertex-vertex relations, which often leads to implausible results. To address this issue, we propose a novel solution, called GATOR, that contains an encoder of Graph-Aware Transformer (GAT) and a decoder with Motion-Disentangled Regression (MDR) to explore these multiple relations. Specifically, GAT combines a GCN and a graph-aware self-attention in parallel to capture physical and hidden joint-joint relations. Furthermore, MDR models joint-vertex and vertex-vertex interactions to explore joint and vertex relations. Based on the clustering characteristics of vertex offset fields, MDR regresses the vertices by composing the predicted base motions. Extensive experiments show that GATOR achieves state-of-the-art performance on two challenging benchmarks.



### Direct Robot Configuration Space Construction using Convolutional Encoder-Decoders
- **Arxiv ID**: http://arxiv.org/abs/2303.05653v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05653v1)
- **Published**: 2023-03-10 02:08:56+00:00
- **Updated**: 2023-03-10 02:08:56+00:00
- **Authors**: Christopher Benka, Carl Gross, Riya Gupta, Hod Lipson
- **Comment**: 6 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Intelligent robots must be able to perform safe and efficient motion planning in their environments. Central to modern motion planning is the configuration space. Configuration spaces define the set of configurations of a robot that result in collisions with obstacles in the workspace, C-clsn, and the set of configurations that do not, C-free. Modern approaches to motion planning first compute the configuration space and then perform motion planning using the calculated configuration space. Real-time motion planning requires accurate and efficient construction of configuration spaces.   We are the first to apply a convolutional encoder-decoder framework for calculating highly accurate approximations to configuration spaces. Our model achieves an average 97.5% F1-score for predicting C-free and C-clsn for 2-D robotic workspaces with a dual-arm robot. Our method limits undetected collisions to less than 2.5% on robotic workspaces that involve translation, rotation, and removal of obstacles. Our model learns highly transferable features between robotic workspaces, requiring little to no fine-tuning to adapt to new transformations of obstacles in the workspace.



### EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2303.05656v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05656v1)
- **Published**: 2023-03-10 02:15:58+00:00
- **Updated**: 2023-03-10 02:15:58+00:00
- **Authors**: Hongyi Yuan, Songchi Zhou, Sheng Yu
- **Comment**: Working in progress
- **Journal**: None
- **Summary**: Electronic health records (EHR) contain vast biomedical knowledge and are rich resources for developing precise medicine systems. However, due to privacy concerns, there are limited high-quality EHR data accessible to researchers hence hindering the advancement of methodologies. Recent research has explored using generative modelling methods to synthesize realistic EHR data, and most proposed methods are based on the generative adversarial network (GAN) and its variants for EHR synthesis. Although GAN-style methods achieved state-of-the-art performance in generating high-quality EHR data, such methods are hard to train and prone to mode collapse. Diffusion models are recently proposed generative modelling methods and set cutting-edge performance in image generation. The performance of diffusion models in realistic EHR synthesis is rarely explored. In this work, we explore whether the superior performance of diffusion models can translate to the domain of EHR synthesis and propose a novel EHR synthesis method named EHRDiff. Through comprehensive experiments, EHRDiff achieves new state-of-the-art performance for the quality of synthetic EHR data and can better protect private information in real training EHRs in the meanwhile.



### Tag2Text: Guiding Vision-Language Model via Image Tagging
- **Arxiv ID**: http://arxiv.org/abs/2303.05657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05657v1)
- **Published**: 2023-03-10 02:16:35+00:00
- **Updated**: 2023-03-10 02:16:35+00:00
- **Authors**: Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with a limited detector, our approach utilizes tags parsed from its paired text to learn an image tagger and meanwhile provides guidance to vision-language models. Given that, Tag2Text can utilize large-scale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. As a result, Tag2Text achieves a superior image tag recognition ability by exploiting fine-grained text information. Moreover, by leveraging tagging guidance, Tag2Text effectively enhances the performance of vision-language models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art or competitive results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance.



### HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2303.05675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05675v1)
- **Published**: 2023-03-10 02:57:07+00:00
- **Updated**: 2023-03-10 02:57:07+00:00
- **Authors**: Shixiang Tang, Cheng Chen, Qingsong Xie, Meilin Chen, Yizhou Wang, Yuanzheng Ci, Lei Bai, Feng Zhu, Haiyang Yang, Li Yi, Rui Zhao, Wanli Ouyang
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: Human-centric perceptions include a variety of vision tasks, which have widespread industrial applications, including surveillance, autonomous driving, and the metaverse. It is desirable to have a general pretrain model for versatile human-centric downstream tasks. This paper forges ahead along this path from the aspects of both benchmark and pretraining methods. Specifically, we propose a \textbf{HumanBench} based on existing datasets to comprehensively evaluate on the common ground the generalization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a \textbf{P}rojector \textbf{A}ssis\textbf{T}ed \textbf{H}ierarchical pretraining method (\textbf{PATH}) to learn diverse knowledge at different granularity levels. Comprehensive evaluations on HumanBench show that our PATH achieves new state-of-the-art results on 17 downstream datasets and on-par results on the other 2 datasets. The code will be publicly at \href{https://github.com/OpenGVLab/HumanBench}{https://github.com/OpenGVLab/HumanBench}.



### An Adaptive GViT for Gas Mixture Identification and Concentration Estimation
- **Arxiv ID**: http://arxiv.org/abs/2303.05685v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05685v1)
- **Published**: 2023-03-10 03:37:05+00:00
- **Updated**: 2023-03-10 03:37:05+00:00
- **Authors**: Ding Wang, Wenwen Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the composition and concentration of ambient gases is crucial for industrial gas safety. Even though other researchers have proposed some gas identification and con-centration estimation algorithms, these algorithms still suffer from severe flaws, particularly in fulfilling industry demands. One example is that the lengths of data collected in an industrial setting tend to vary. The conventional algorithm, yet, cannot be used to analyze the variant-length data effectively. Trimming the data will preserve only steady-state values, inevitably leading to the loss of vital information. The gas identification and concentration estimation model called GCN-ViT(GViT) is proposed in this paper; we view the sensor data to be a one-way chain that has only been downscaled to retain the majority of the original in-formation. The GViT model can directly utilize sensor ar-rays' variable-length real-time signal data as input. We validated the above model on a dataset of 12-hour uninterrupted monitoring of two randomly varying gas mixtures, CO-ethylene and methane-ethylene. The accuracy of gas identification can reach 97.61%, R2 of the pure gas concentration estimation is above 99.5% on average, and R2 of the mixed gas concentration estimation is above 95% on average.



### Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers
- **Arxiv ID**: http://arxiv.org/abs/2303.05686v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05686v1)
- **Published**: 2023-03-10 03:39:23+00:00
- **Updated**: 2023-03-10 03:39:23+00:00
- **Authors**: Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion MRI is a non-invasive, in-vivo medical imaging method able to map tissue microstructure and structural connectivity of the human brain, as well as detect changes, such as brain development and injury, not visible by other clinical neuroimaging techniques. However, acquiring high signal-to-noise ratio (SNR) datasets with high angular and spatial sampling requires prohibitively long scan times, limiting usage in many important clinical settings, especially children, the elderly, and emergency patients with acute neurological disorders who might not be able to cooperate with the MRI scan without conscious sedation or general anesthesia. Here, we propose to use a Swin UNEt TRansformers (Swin UNETR) model, trained on augmented Human Connectome Project (HCP) data and conditioned on registered T1 scans, to perform generalized denoising and super-resolution of diffusion MRI invariant to acquisition parameters, patient populations, scanners, and sites. We qualitatively demonstrate super-resolution with artificially downsampled HCP data in normal adult volunteers. Our experiments on two other unrelated datasets, one of children with neurodevelopmental disorders and one of traumatic brain injury patients, show that our method demonstrates superior denoising despite wide data distribution shifts. Further improvement can be achieved via finetuning with just one additional subject. We apply our model to diffusion tensor (2nd order spherical harmonic) and higher-order spherical harmonic coefficient estimation and show results superior to current state-of-the-art methods. Our method can be used out-of-the-box or minimally finetuned to denoise and super-resolve a wide variety of diffusion MRI datasets. The code and model are publicly available at https://github.com/ucsfncl/dmri-swin.



### Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity
- **Arxiv ID**: http://arxiv.org/abs/2303.05689v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.05689v2)
- **Published**: 2023-03-10 03:44:01+00:00
- **Updated**: 2023-08-09 17:31:20+00:00
- **Authors**: Tong Liang, Jim Davis
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of heights ranging from 3 to 12. Code: https://github.com/ltong1130ztr/HAFrame



### Human Pose Estimation from Ambiguous Pressure Recordings with Spatio-temporal Masked Transformers
- **Arxiv ID**: http://arxiv.org/abs/2303.05691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05691v1)
- **Published**: 2023-03-10 03:49:50+00:00
- **Updated**: 2023-03-10 03:49:50+00:00
- **Authors**: Vandad Davoodnia, Ali Etemad
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive performance of vision-based pose estimators, they generally fail to perform well under adverse vision conditions and often don't satisfy the privacy demands of customers. As a result, researchers have begun to study tactile sensing systems as an alternative. However, these systems suffer from noisy and ambiguous recordings. To tackle this problem, we propose a novel solution for pose estimation from ambiguous pressure data. Our method comprises a spatio-temporal vision transformer with an encoder-decoder architecture. Detailed experiments on two popular public datasets reveal that our model outperforms existing solutions in the area. Moreover, we observe that increasing the number of temporal crops in the early stages of the network positively impacts the performance while pre-training the network in a self-supervised setting using a masked auto-encoder approach also further improves the results.



### Semantic-Preserving Augmentation for Robust Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2303.05692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05692v1)
- **Published**: 2023-03-10 03:50:44+00:00
- **Updated**: 2023-03-10 03:50:44+00:00
- **Authors**: Sunwoo Kim, Kyuhong Shim, Luong Trung Nguyen, Byonghyo Shim
- **Comment**: Accepted to ICASSP 2023
- **Journal**: None
- **Summary**: Image text retrieval is a task to search for the proper textual descriptions of the visual world and vice versa. One challenge of this task is the vulnerability to input image and text corruptions. Such corruptions are often unobserved during the training, and degrade the retrieval model decision quality substantially. In this paper, we propose a novel image text retrieval technique, referred to as robust visual semantic embedding (RVSE), which consists of novel image-based and text-based augmentation techniques called semantic preserving augmentation for image (SPAugI) and text (SPAugT). Since SPAugI and SPAugT change the original data in a way that its semantic information is preserved, we enforce the feature extractors to generate semantic aware embedding vectors regardless of the corruption, improving the model robustness significantly. From extensive experiments using benchmark datasets, we show that RVSE outperforms conventional retrieval schemes in terms of image-text retrieval performance.



### Mode-locking Theory for Long-Range Interaction in Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2303.05695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05695v1)
- **Published**: 2023-03-10 04:23:01+00:00
- **Updated**: 2023-03-10 04:23:01+00:00
- **Authors**: Xiuxiu Bai, Shuaishuai Zhao, Yao Gao, Zhe Liu
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Visual long-range interaction refers to modeling dependencies between distant feature points or blocks within an image, which can significantly enhance the model's robustness. Both CNN and Transformer can establish long-range interactions through layering and patch calculations. However, the underlying mechanism of long-range interaction in visual space remains unclear. We propose the mode-locking theory as the underlying mechanism, which constrains the phase and wavelength relationship between waves to achieve mode-locked interference waveform. We verify this theory through simulation experiments and demonstrate the mode-locking pattern in real-world scene models. Our proposed theory of long-range interaction provides a comprehensive understanding of the mechanism behind this phenomenon in artificial neural networks. This theory can inspire the integration of the mode-locking pattern into models to enhance their robustness.



### Explainable Semantic Medical Image Segmentation with Style
- **Arxiv ID**: http://arxiv.org/abs/2303.05696v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05696v1)
- **Published**: 2023-03-10 04:34:51+00:00
- **Updated**: 2023-03-10 04:34:51+00:00
- **Authors**: Wei Dai, Siyu Liu, Craig B. Engstrom, Shekhar S. Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic medical image segmentation using deep learning has recently achieved high accuracy, making it appealing to clinical problems such as radiation therapy. However, the lack of high-quality semantically labelled data remains a challenge leading to model brittleness to small shifts to input data. Most works require extra data for semi-supervised learning and lack the interpretability of the boundaries of the training data distribution during training, which is essential for model deployment in clinical practice. We propose a fully supervised generative framework that can achieve generalisable segmentation with only limited labelled data by simultaneously constructing an explorable manifold during training. The proposed approach creates medical image style paired with a segmentation task driven discriminator incorporating end-to-end adversarial training. The discriminator is generalised to small domain shifts as much as permissible by the training data, and the generator automatically diversifies the training samples using a manifold of input features learnt during segmentation. All the while, the discriminator guides the manifold learning by supervising the semantic content and fine-grained features separately during the image diversification. After training, visualisation of the learnt manifold from the generator is available to interpret the model limits. Experiments on a fully semantic, publicly available pelvis dataset demonstrated that our method is more generalisable to shifts than other state-of-the-art methods while being more explainable using an explorable manifold.



### Feature Unlearning for Pre-trained GANs and VAEs
- **Arxiv ID**: http://arxiv.org/abs/2303.05699v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05699v3)
- **Published**: 2023-03-10 04:49:01+00:00
- **Updated**: 2023-08-25 06:34:14+00:00
- **Authors**: Saemi Moon, Seunghyuk Cho, Dongwoo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.



### MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field
- **Arxiv ID**: http://arxiv.org/abs/2303.05703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.05703v2)
- **Published**: 2023-03-10 05:06:30+00:00
- **Updated**: 2023-04-07 06:57:06+00:00
- **Authors**: Kaizhi Yang, Xiaoshuai Zhang, Zhiao Huang, Xuejin Chen, Zexiang Xu, Hao Su
- **Comment**: Project Page: https://silenkzyoung.github.io/MovingParts-WebPage/
- **Journal**: None
- **Summary**: We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.



### MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling
- **Arxiv ID**: http://arxiv.org/abs/2303.05707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.05707v1)
- **Published**: 2023-03-10 05:22:39+00:00
- **Updated**: 2023-03-10 05:22:39+00:00
- **Authors**: Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume large amounts of GPU memory. Especially, they have difficulty dealing with dense video frames or long text that are prevalent in industrial applications. In this paper, we propose MuLTI, a highly accurate and memory-efficient video-and-language understanding model that achieves efficient and effective feature fusion through feature sampling and attention modules. Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model's performance with low GPU memory consumption. Finally, to further improve the model's performance, we introduce a new pretraining task named Multiple Choice Modeling to bridge the task gap between pretraining and downstream tasks and enhance the model's ability to align the video and the text. Benefiting from the efficient feature fusion module, the attention-based adapter and the new pretraining task, MuLTI achieves state-of-the-art performance on multiple datasets. Implementation and pretrained models will be released.



### Self-supervised Facial Action Unit Detection with Region and Relation Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.05708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05708v1)
- **Published**: 2023-03-10 05:22:45+00:00
- **Updated**: 2023-03-10 05:22:45+00:00
- **Authors**: Juan Song, Zhilei Liu
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: Facial action unit (AU) detection is a challenging task due to the scarcity of manual annotations. Recent works on AU detection with self-supervised learning have emerged to address this problem, aiming to learn meaningful AU representations from numerous unlabeled data. However, most existing AU detection works with self-supervised learning utilize global facial features only, while AU-related properties such as locality and relevance are not fully explored. In this paper, we propose a novel self-supervised framework for AU detection with the region and relation learning. In particular, AU related attention map is utilized to guide the model to focus more on AU-specific regions to enhance the integrity of AU local features. Meanwhile, an improved Optimal Transport (OT) algorithm is introduced to exploit the correlation characteristics among AUs. In addition, Swin Transformer is exploited to model the long-distance dependencies within each AU region during feature learning. The evaluation results on BP4D and DISFA demonstrate that our proposed method is comparable or even superior to the state-of-the-art self-supervised learning methods and supervised AU detection methods.



### Context-Based Trit-Plane Coding for Progressive Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2303.05715v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05715v2)
- **Published**: 2023-03-10 05:46:25+00:00
- **Updated**: 2023-03-13 07:09:03+00:00
- **Authors**: Seungmin Jeon, Kwang Pyo Choi, Youngo Park, Chang-Su Kim
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Trit-plane coding enables deep progressive image compression, but it cannot use autoregressive context models. In this paper, we propose the context-based trit-plane coding (CTC) algorithm to achieve progressive compression more compactly. First, we develop the context-based rate reduction module to estimate trit probabilities of latent elements accurately and thus encode the trit-planes compactly. Second, we develop the context-based distortion reduction module to refine partial latent tensors from the trit-planes and improve the reconstructed image quality. Third, we propose a retraining scheme for the decoder to attain better rate-distortion tradeoffs. Extensive experiments show that CTC outperforms the baseline trit-plane codec significantly in BD-rate on the Kodak lossless dataset, while increasing the time complexity only marginally. Our codes are available at https://github.com/seungminjeon-github/CTC.



### Boosting Adversarial Attacks by Leveraging Decision Boundary Information
- **Arxiv ID**: http://arxiv.org/abs/2303.05719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05719v1)
- **Published**: 2023-03-10 05:54:11+00:00
- **Updated**: 2023-03-10 05:54:11+00:00
- **Authors**: Boheng Zeng, LianLi Gao, QiLong Zhang, ChaoQun Li, JingKuan Song, ShuaiQi Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the gap between a substitute model and a victim model, the gradient-based noise generated from a substitute model may have low transferability for a victim model since their gradients are different. Inspired by the fact that the decision boundaries of different models do not differ much, we conduct experiments and discover that the gradients of different models are more similar on the decision boundary than in the original position. Moreover, since the decision boundary in the vicinity of an input image is flat along most directions, we conjecture that the boundary gradients can help find an effective direction to cross the decision boundary of the victim models. Based on it, we propose a Boundary Fitting Attack to improve transferability. Specifically, we introduce a method to obtain a set of boundary points and leverage the gradient information of these points to update the adversarial examples. Notably, our method can be combined with existing gradient-based methods. Extensive experiments prove the effectiveness of our method, i.e., improving the success rate by 5.6% against normally trained CNNs and 14.9% against defense CNNs on average compared to state-of-the-art transfer-based attacks. Further we compare transformers with CNNs, the results indicate that transformers are more robust than CNNs. However, our method still outperforms existing methods when attacking transformers. Specifically, when using CNNs as substitute models, our method obtains an average attack success rate of 58.2%, which is 10.8% higher than other state-of-the-art transfer-based attacks.



### 3D Cinemagraphy from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2303.05724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.05724v1)
- **Published**: 2023-03-10 06:08:23+00:00
- **Updated**: 2023-03-10 06:08:23+00:00
- **Authors**: Xingyi Li, Zhiguo Cao, Huiqiang Sun, Jianming Zhang, Ke Xian, Guosheng Lin
- **Comment**: Accepted by CVPR 2023. Project page:
  https://xingyi-li.github.io/3d-cinemagraphy/
- **Journal**: None
- **Summary**: We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.



### CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment
- **Arxiv ID**: http://arxiv.org/abs/2303.05725v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.05725v4)
- **Published**: 2023-03-10 06:12:36+00:00
- **Updated**: 2023-04-12 10:07:11+00:00
- **Authors**: Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li
- **Comment**: Accepted to CVPR 2023 (Highlight paper, 2.5% acceptance rate); Open
  source
- **Journal**: None
- **Summary**: Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign datasets becomes the main bottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variational autoencoder (VAE) for pretrained contextual knowledge while introducing the complete pretrained language module. The VAE implicitly aligns visual and textual modalities while benefiting from pretrained contextual knowledge as the traditional contextual module. Meanwhile, a contrastive cross-modal alignment algorithm is designed to explicitly enhance the consistency constraints. Extensive experiments on public datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR consistently outperforms existing single-cue methods and even outperforms SOTA multi-cue methods.



### IC classifier: a classifier for 3D industrial components based on geometric prior using GNN
- **Arxiv ID**: http://arxiv.org/abs/2303.05730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05730v1)
- **Published**: 2023-03-10 06:23:16+00:00
- **Updated**: 2023-03-10 06:23:16+00:00
- **Authors**: Zipeng Lin, Zhenguo Nie
- **Comment**: 15 pages including citations, 3 pages of figures
- **Journal**: None
- **Summary**: In this paper, we propose an approach to address the problem of classifying 3D industrial components by introducing a novel framework named IC-classifier (Industrial Component classifier). Our framework is designed to focus on the object's local and global structures, emphasizing the former by incorporating specific local features for embedding the model. By utilizing graphical neural networks and embedding derived from geometric properties, IC-classifier facilitates the exploration of the local structures of the object while using geometric attention for the analysis of global structures. Furthermore, the framework uses point clouds to circumvent the heavy computation workload. The proposed framework's performance is benchmarked against state-of-the-art models, demonstrating its potential to compete in the field.



### Generative Model Based Noise Robust Training for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2303.05734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05734v1)
- **Published**: 2023-03-10 06:43:55+00:00
- **Updated**: 2023-03-10 06:43:55+00:00
- **Authors**: Zhongying Deng, Da Li, Junjun He, Yi-Zhe Song, Tao Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Target domain pseudo-labelling has shown effectiveness in unsupervised domain adaptation (UDA). However, pseudo-labels of unlabeled target domain data are inevitably noisy due to the distribution shift between source and target domains. This paper proposes a Generative model-based Noise-Robust Training method (GeNRT), which eliminates domain shift while mitigating label noise. GeNRT incorporates a Distribution-based Class-wise Feature Augmentation (D-CFA) and a Generative-Discriminative classifier Consistency (GDC), both based on the class-wise target distributions modelled by generative models. D-CFA minimizes the domain gap by augmenting the source data with distribution-sampled target features, and trains a noise-robust discriminative classifier by using target domain knowledge from the generative models. GDC regards all the class-wise generative models as generative classifiers and enforces a consistency regularization between the generative and discriminative classifiers. It exploits an ensemble of target knowledge from all the generative models to train a noise-robust discriminative classifier and eventually gets theoretically linked to the Ben-David domain adaptation theorem for reducing the domain gap. Extensive experiments on Office-Home, PACS, and Digit-Five show that our GeNRT achieves comparable performance to state-of-the-art methods under single-source and multi-source UDA settings.



### Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher
- **Arxiv ID**: http://arxiv.org/abs/2303.05739v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05739v2)
- **Published**: 2023-03-10 06:49:31+00:00
- **Updated**: 2023-05-21 18:56:31+00:00
- **Authors**: Phi Vu Tran
- **Comment**: Technical Report. Project page at
  https://github.com/lexisnexis-risk-open-source/ledetection
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) is an emerging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD assume abundant base labels to adapt to novel objects. This paper studies the task of semi-supervised FSOD by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with representation learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Extensive experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without experiencing catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between semi-supervised and few-shot detection suggesting that a stronger semi-supervised detector leads to a more effective few-shot detector. The code and models are available at https://github.com/lexisnexis-risk-open-source/ledetection



### Multi-site, Multi-domain Airway Tree Modeling (ATM'22): A Public Benchmark for Pulmonary Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.05745v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05745v3)
- **Published**: 2023-03-10 07:08:25+00:00
- **Updated**: 2023-06-27 06:36:42+00:00
- **Authors**: Minghui Zhang, Yangqian Wu, Hanxiao Zhang, Yulei Qin, Hao Zheng, Wen Tang, Corey Arnold, Chenhao Pei, Pengxin Yu, Yang Nan, Guang Yang, Simon Walsh, Dominic C. Marshall, Matthieu Komorowski, Puyang Wang, Dazhou Guo, Dakai Jin, Ya'nan Wu, Shuiqing Zhao, Runsheng Chang, Boyu Zhang, Xing Lv, Abdul Qayyum, Moona Mazher, Qi Su, Yonghuang Wu, Ying'ao Liu, Yufei Zhu, Jiancheng Yang, Ashkan Pakzad, Bojidar Rangelov, Raul San Jose Estepar, Carlos Cano Espinosa, Jiayuan Sun, Guang-Zhong Yang, Yun Gu
- **Comment**: 32 pages, 16 figures. Homepage: https://atm22.grand-challenge.org/.
  Submitted
- **Journal**: None
- **Summary**: Open international challenges are becoming the de facto standard for assessing computer vision and image analysis algorithms. In recent years, new methods have extended the reach of pulmonary airway segmentation that is closer to the limit of image resolution. Since EXACT'09 pulmonary airway segmentation, limited effort has been directed to quantitative comparison of newly emerged algorithms driven by the maturity of deep learning based approaches and clinical drive for resolving finer details of distal airways for early intervention of pulmonary diseases. Thus far, public annotated datasets are extremely limited, hindering the development of data-driven methods and detailed performance evaluation of new algorithms. To provide a benchmark for the medical imaging community, we organized the Multi-site, Multi-domain Airway Tree Modeling (ATM'22), which was held as an official challenge event during the MICCAI 2022 conference. ATM'22 provides large-scale CT scans with detailed pulmonary airway annotation, including 500 CT scans (300 for training, 50 for validation, and 150 for testing). The dataset was collected from different sites and it further included a portion of noisy COVID-19 CTs with ground-glass opacity and consolidation. Twenty-three teams participated in the entire phase of the challenge and the algorithms for the top ten teams are reviewed in this paper. Quantitative and qualitative results revealed that deep learning models embedded with the topological continuity enhancement achieved superior performance in general. ATM'22 challenge holds as an open-call design, the training data and the gold standard evaluation are available upon successful registration via its homepage.



### Deep Learning for Predicting Metastasis on Melanoma WSIs
- **Arxiv ID**: http://arxiv.org/abs/2303.05752v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05752v1)
- **Published**: 2023-03-10 07:40:09+00:00
- **Updated**: 2023-03-10 07:40:09+00:00
- **Authors**: Christopher Andreassen, Saul Fuster, Helga Hardardottir, Emiel A. M. Janssen, Kjersti Engan
- **Comment**: None
- **Journal**: None
- **Summary**: Northern Europe has the second highest mortality rate of melanoma globally. In 2020, the mortality rate of melanoma rose to 1.9 per 100 000 habitants. Melanoma prognosis is based on a pathologist's subjective visual analysis of the patient's tumor. This methodology is heavily time-consuming, and the prognosis variability among experts is notable, drastically jeopardizing its reproducibility. Thus, the need for faster and more reproducible methods arises. Machine learning has paved its way into digital pathology, but so far, most contributions are on localization, segmentation, and diagnostics, with little emphasis on prognostics. This paper presents a convolutional neural network (CNN) method based on VGG16 to predict melanoma prognosis as the presence of metastasis within five years. Patches are extracted from regions of interest from Whole Slide Images (WSIs) at different magnification levels used in model training and validation. Results infer that utilizing WSI patches at 20x magnification level has the best performance, with an F1 score of 0.7667 and an AUC of 0.81.



### Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2303.05754v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2303.05754v1)
- **Published**: 2023-03-10 07:42:49+00:00
- **Updated**: 2023-03-10 07:42:49+00:00
- **Authors**: Hyungjin Chung, Suhyeon Lee, Jong Chul Ye
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: Diffusion models have shown exceptional performance in solving inverse problems. However, one major limitation is the slow inference time. While faster diffusion samplers have been developed for unconditional sampling, there has been limited research on conditional sampling in the context of inverse problems. In this study, we propose a novel and efficient diffusion sampling strategy that employs the geometric decomposition of diffusion sampling. Specifically, we discover that the samples generated from diffusion models can be decomposed into two orthogonal components: a ``denoised" component obtained by projecting the sample onto the clean data manifold, and a ``noise" component that induces a transition to the next lower-level noisy manifold with the addition of stochastic noise. Furthermore, we prove that, under some conditions on the clean data manifold, the conjugate gradient update for imposing conditioning from the denoised signal belongs to the clean manifold, resulting in a much faster and more accurate diffusion sampling. Our method is applicable regardless of the parameterization and setting (i.e., VE, VP). Notably, we achieve state-of-the-art reconstruction quality on challenging real-world medical inverse imaging problems, including multi-coil MRI reconstruction and 3D CT reconstruction. Moreover, our proposed method achieves more than 80 times faster inference time than the previous state-of-the-art method.



### TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets
- **Arxiv ID**: http://arxiv.org/abs/2303.05762v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05762v1)
- **Published**: 2023-03-10 08:01:23+00:00
- **Updated**: 2023-03-10 08:01:23+00:00
- **Authors**: Weixin Chen, Dawn Song, Bo Li
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under potential training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffusion models? What are the adversarial targets that such Trojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion models, TrojDiff, which optimizes the Trojan diffusion and generative processes during training. In particular, we design novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian distribution and propose a new parameterization of the Trojan generative process that leads to an effective training objective for the attack. In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific instance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM diffusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets using different types of triggers, while the performance in benign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff.



### Automatic Detection and Rectification of Paper Receipts on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2303.05763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2303.05763v1)
- **Published**: 2023-03-10 08:04:16+00:00
- **Updated**: 2023-03-10 08:04:16+00:00
- **Authors**: Edward Whittaker, Masashi Tanaka, Ikuo Kitagishi
- **Comment**: None
- **Journal**: None
- **Summary**: We describe the development of a real-time smartphone app that allows the user to digitize paper receipts in a novel way by "waving" their phone over the receipts and letting the app automatically detect and rectify the receipts for subsequent text recognition.   We show that traditional computer vision algorithms for edge and corner detection do not robustly detect the non-linear and discontinuous edges and corners of a typical paper receipt in real-world settings. This is particularly the case when the colors of the receipt and background are similar, or where other interfering rectangular objects are present. Inaccurate detection of a receipt's corner positions then results in distorted images when using an affine projective transformation to rectify the perspective.   We propose an innovative solution to receipt corner detection by treating each of the four corners as a unique "object", and training a Single Shot Detection MobileNet object detection model. We use a small amount of real data and a large amount of automatically generated synthetic data that is designed to be similar to real-world imaging scenarios.   We show that our proposed method robustly detects the four corners of a receipt, giving a receipt detection accuracy of 85.3% on real-world data, compared to only 36.9% with a traditional edge detection-based approach. Our method works even when the color of the receipt is virtually indistinguishable from the background.   Moreover, our method is trained to detect only the corners of the central target receipt and implicitly learns to ignore other receipts, and other rectangular objects. Including synthetic data allows us to train an even better model. These factors are a major advantage over traditional edge detection-based approaches, allowing us to deliver a much better experience to the user.



### Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.05768v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05768v2)
- **Published**: 2023-03-10 08:09:40+00:00
- **Updated**: 2023-03-29 01:13:00+00:00
- **Authors**: Haiming Yao, Wenyong Yu, Wei Luo, Zhenfeng Qiang, Donghao Luo, Xiaotian Zhang
- **Comment**: Submission to IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO
  TECHNOLOGY
- **Journal**: None
- **Summary**: This paper presents a novel framework, named Global-Local Correspondence Framework (GLCF), for visual anomaly detection with logical constraints. Visual anomaly detection has become an active research area in various real-world applications, such as industrial anomaly detection and medical disease diagnosis. However, most existing methods focus on identifying local structural degeneration anomalies and often fail to detect high-level functional anomalies that involve logical constraints. To address this issue, we propose a two-branch approach that consists of a local branch for detecting structural anomalies and a global branch for detecting logical anomalies. To facilitate local-global feature correspondence, we introduce a novel semantic bottleneck enabled by the visual Transformer. Moreover, we develop feature estimation networks for each branch separately to detect anomalies. Our proposed framework is validated using various benchmarks, including industrial datasets, Mvtec AD, Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show that our method outperforms existing methods, particularly in detecting logical anomalies.



### Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2303.05775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.05775v1)
- **Published**: 2023-03-10 08:22:36+00:00
- **Updated**: 2023-03-10 08:22:36+00:00
- **Authors**: Jiayang Bai, Letian Huang, Wen Gong, Jie Guo, Yanwen Guo
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: Recently, Neural Radiance Fields (NeRF) have emerged as a potent method for synthesizing novel views from a dense set of images. Despite its impressive performance, NeRF is plagued by its necessity for numerous calibrated views and its accuracy diminishes significantly in a few-shot setting. To address this challenge, we propose Self-NeRF, a self-evolved NeRF that iteratively refines the radiance fields with very few number of input views, without incorporating additional priors. Basically, we train our model under the supervision of reference and unseen views simultaneously in an iterative procedure. In each iteration, we label unseen views with the predicted colors or warped pixels generated by the model from the preceding iteration. However, these expanded pseudo-views are afflicted by imprecision in color and warping artifacts, which degrades the performance of NeRF. To alleviate this issue, we construct an uncertainty-aware NeRF with specialized embeddings. Some techniques such as cone entropy regularization are further utilized to leverage the pseudo-views in the most efficient manner. Through experiments under various settings, we verified that our Self-NeRF is robust to input with uncertainty and surpasses existing methods when trained on limited training data.



### Self-Supervised CSF Inpainting with Synthetic Atrophy for Improved Accuracy Validation of Cortical Surface Analyses
- **Arxiv ID**: http://arxiv.org/abs/2303.05777v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05777v1)
- **Published**: 2023-03-10 08:27:14+00:00
- **Updated**: 2023-03-10 08:27:14+00:00
- **Authors**: Jiacheng Wang, Kathleen E. Larson, Ipek Oguz
- **Comment**: Accepted at Medical Imaging with Deep Learning (MIDL) 2023
- **Journal**: None
- **Summary**: Accuracy validation of cortical thickness measurement is a difficult problem due to the lack of ground truth data. To address this need, many methods have been developed to synthetically induce gray matter (GM) atrophy in an MRI via deformable registration, creating a set of images with known changes in cortical thickness. However, these methods often cause blurring in atrophied regions, and cannot simulate realistic atrophy within deep sulci where cerebrospinal fluid (CSF) is obscured or absent. In this paper, we present a solution using a self-supervised inpainting model to generate CSF in these regions and create images with more plausible GM/CSF boundaries. Specifically, we introduce a novel, 3D GAN model that incorporates patch-based dropout training, edge map priors, and sinusoidal positional encoding, all of which are established methods previously limited to 2D domains. We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unseen data with fine-tuning. We also demonstrate that our resulting dataset can be employed for accuracy validation of cortical segmentation and thickness measurement.



### Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.05780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.05780v1)
- **Published**: 2023-03-10 08:29:35+00:00
- **Updated**: 2023-03-10 08:29:35+00:00
- **Authors**: Conghao Xiong, Yi Lin, Hao Chen, Joseph Sung, Irwin King
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring prior knowledge from a source domain to the same or similar target domain can greatly enhance the performance of models on the target domain. However, it is challenging to directly leverage the knowledge from the source domain due to task discrepancy and domain shift. To bridge the gaps between different tasks and domains, we propose a Multi-Head Feature Adaptation module, which projects features in the source feature space to a new space that is more similar to the target space. Knowledge transfer is particularly important in Whole Slide Image (WSI) classification since the number of WSIs in one dataset might be too small to achieve satisfactory performance. Therefore, WSI classification is an ideal testbed for our method, and we adapt multiple knowledge transfer methods for WSI classification. The experimental results show that models with knowledge transfer outperform models that are trained from scratch by a large margin regardless of the number of WSIs in the datasets, and our method achieves state-of-the-art performances among other knowledge transfer methods on multiple datasets, including TCGA-RCC, TCGA-NSCLC, and Camelyon16 datasets.



### Scaling Up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.05785v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05785v2)
- **Published**: 2023-03-10 08:38:34+00:00
- **Updated**: 2023-06-06 03:05:07+00:00
- **Authors**: Ho Hin Lee, Quan Liu, Shunxing Bao, Qi Yang, Xin Yu, Leon Y. Cai, Thomas Li, Yuankai Huo, Xenofon Koutsoukos, Bennett A. Landman
- **Comment**: Accepted to MICCAI 2023 (top 13.6%), both codes and pretrained models
  are available at: https://github.com/MASILab/RepUX-Net
- **Journal**: None
- **Summary**: With the inspiration of vision transformers, the concept of depth-wise convolution revisits to provide a large Effective Receptive Field (ERF) using Large Kernel (LK) sizes for medical image segmentation. However, the segmentation performance might be saturated and even degraded as the kernel sizes scaled up (e.g., $21\times 21\times 21$) in a Convolutional Neural Network (CNN). We hypothesize that convolution with LK sizes is limited to maintain an optimal convergence for locality learning. While Structural Re-parameterization (SR) enhances the local convergence with small kernels in parallel, optimal small kernel branches may hinder the computational efficiency for training. In this work, we propose RepUX-Net, a pure CNN architecture with a simple large kernel block design, which competes favorably with current network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6 challenging public datasets. We derive an equivalency between kernel re-parameterization and the branch-wise variation in kernel convergence. Inspired by the spatial frequency in the human visual system, we extend to vary the kernel convergence into element-wise setting and model the spatial frequency as a Bayesian prior to re-parameterize convolutional weights during training. Specifically, a reciprocal function is leveraged to estimate a frequency-weighted value, which rescales the corresponding kernel element for stochastic gradient descent. From the experimental results, RepUX-Net consistently outperforms 3D SOTA benchmarks with internal validation (FLARE: 0.929 to 0.944), external validation (MSD: 0.901 to 0.932, KiTS: 0.815 to 0.847, LiTS: 0.933 to 0.949, TCIA: 0.736 to 0.779) and transfer learning (AMOS: 0.880 to 0.911) scenarios in Dice Score.



### AnoMalNet: Outlier Detection based Malaria Cell Image Classification Method Leveraging Deep Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2303.05789v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05789v1)
- **Published**: 2023-03-10 08:49:31+00:00
- **Updated**: 2023-03-10 08:49:31+00:00
- **Authors**: Aminul Huq, Md Tanzim Reza, Shahriar Hossain, Shakib Mahmud Dipto
- **Comment**: None
- **Journal**: None
- **Summary**: Class imbalance is a pervasive issue in the field of disease classification from medical images. It is necessary to balance out the class distribution while training a model for decent results. However, in the case of rare medical diseases, images from affected patients are much harder to come by compared to images from non-affected patients, resulting in unwanted class imbalance. Various processes of tackling class imbalance issues have been explored so far, each having its fair share of drawbacks. In this research, we propose an outlier detection based binary medical image classification technique which can handle even the most extreme case of class imbalance. We have utilized a dataset of malaria parasitized and uninfected cells. An autoencoder model titled AnoMalNet is trained with only the uninfected cell images at the beginning and then used to classify both the affected and non-affected cell images by thresholding a loss value. We have achieved an accuracy, precision, recall, and F1 score of 98.49%, 97.07%, 100%, and 98.52% respectively, performing better than large deep learning models and other published works. As our proposed approach can provide competitive results without needing the disease-positive samples during training, it should prove to be useful in binary disease classification on imbalanced datasets.



### Enhancing the accuracies by performing pooling decisions adjacent to the output layer
- **Arxiv ID**: http://arxiv.org/abs/2303.05800v2
- **DOI**: 10.1038/S41598-023-40566-Y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05800v2)
- **Published**: 2023-03-10 09:09:37+00:00
- **Updated**: 2023-08-31 10:09:06+00:00
- **Authors**: Yuval Meir, Yarden Tzach, Ronit D. Gross, Ofek Tevet, Roni Vardi, Ido Kanter
- **Comment**: 29 pages, 3 figures, 1 table, and Supplementary Information
- **Journal**: Sci Rep 13, 13385 (2023)
- **Summary**: Learning classification tasks of (2^nx2^n) inputs typically consist of \le n (2x2) max-pooling (MP) operators along the entire feedforward deep architecture. Here we show, using the CIFAR-10 database, that pooling decisions adjacent to the last convolutional layer significantly enhance accuracies. In particular, average accuracies of the advanced-VGG with m layers (A-VGGm) architectures are 0.936, 0.940, 0.954, 0.955, and 0.955 for m=6, 8, 14, 13, and 16, respectively. The results indicate A-VGG8s' accuracy is superior to VGG16s', and that the accuracies of A-VGG13 and A-VGG16 are equal, and comparable to that of Wide-ResNet16. In addition, replacing the three fully connected (FC) layers with one FC layer, A-VGG6 and A-VGG14, or with several linear activation FC layers, yielded similar accuracies. These significantly enhanced accuracies stem from training the most influential input-output routes, in comparison to the inferior routes selected following multiple MP decisions along the deep architecture. In addition, accuracies are sensitive to the order of the non-commutative MP and average pooling operators adjacent to the output layer, varying the number and location of training routes. The results call for the reexamination of previously proposed deep architectures and their accuracies by utilizing the proposed pooling strategy adjacent to the output layer.



### Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields
- **Arxiv ID**: http://arxiv.org/abs/2303.05807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05807v1)
- **Published**: 2023-03-10 09:28:09+00:00
- **Updated**: 2023-03-10 09:28:09+00:00
- **Authors**: Ziteng Cui, Lin Gu, Xiao Sun, Yu Qiao, Tatsuya Harada
- **Comment**: website page: https://cuiziteng.github.io/Aleth_NeRF_web/
- **Journal**: None
- **Summary**: Common capture low-light scenes are challenging for most computer vision techniques, including Neural Radiance Fields (NeRF). Vanilla NeRF is viewer-centred that simplifies the rendering process only as light emission from 3D locations in the viewing direction, thus failing to model the low-illumination induced darkness. Inspired by emission theory of ancient Greek that visual perception is accomplished by rays casting from eyes, we make slight modifications on vanilla NeRF to train on multiple views of low-light scene, we can thus render out the well-lit scene in an unsupervised manner. We introduce a surrogate concept, Concealing Fields, that reduce the transport of light during the volume rendering stage. Specifically, our proposed method, Aleth-NeRF, directly learns from the dark image to understand volumetric object representation and concealing field under priors. By simply eliminating Concealing Fields, we can render a single or multi-view well-lit image(s) and gain superior performance over other 2D low light enhancement methods. Additionally, we collect the first paired LOw-light and normal-light Multi-view (LOM) datasets for future research.



### Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors
- **Arxiv ID**: http://arxiv.org/abs/2303.05828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05828v1)
- **Published**: 2023-03-10 10:02:18+00:00
- **Updated**: 2023-03-10 10:02:18+00:00
- **Authors**: Felix Michels, Nikolas Adaloglou, Tim Kaiser, Markus Kollmann
- **Comment**: None
- **Journal**: None
- **Summary**: We present a comprehensive experimental study on pretrained feature extractors for visual out-of-distribution (OOD) detection. We examine several setups, based on the availability of labels or image captions and using different combinations of in- and out-distributions. Intriguingly, we find that (i) contrastive language-image pretrained models achieve state-of-the-art unsupervised out-of-distribution performance using nearest neighbors feature similarity as the OOD detection score, (ii) supervised state-of-the-art OOD detection performance can be obtained without in-distribution fine-tuning, (iii) even top-performing billion-scale vision transformers trained with natural language supervision fail at detecting adversarially manipulated OOD images. Finally, we argue whether new benchmarks for visual anomaly detection are needed based on our experiments. Using the largest publicly available vision transformer, we achieve state-of-the-art performance across all $18$ reported OOD benchmarks, including an AUROC of 87.6\% (9.2\% gain, unsupervised) and 97.4\% (1.2\% gain, supervised) for the challenging task of CIFAR100 $\rightarrow$ CIFAR10 OOD detection. The code will be open-sourced.



### You Only Train Once: Multi-Identity Free-Viewpoint Neural Human Rendering from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2303.05835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05835v1)
- **Published**: 2023-03-10 10:23:17+00:00
- **Updated**: 2023-03-10 10:23:17+00:00
- **Authors**: Jaehyeok Kim, Dongyoon Wee, Dan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce You Only Train Once (YOTO), a dynamic human generation framework, which performs free-viewpoint rendering of different human identities with distinct motions, via only one-time training from monocular videos. Most prior works for the task require individualized optimization for each input video that contains a distinct human identity, leading to a significant amount of time and resources for the deployment, thereby impeding the scalability and the overall application potential of the system. In this paper, we tackle this problem by proposing a set of learnable identity codes to expand the capability of the framework for multi-identity free-viewpoint rendering, and an effective pose-conditioned code query mechanism to finely model the pose-dependent non-rigid motions. YOTO optimizes neural radiance fields (NeRF) by utilizing designed identity codes to condition the model for learning various canonical T-pose appearances in a single shared volumetric representation. Besides, our joint learning of multiple identities within a unified model incidentally enables flexible motion transfer in high-quality photo-realistic renderings for all learned appearances. This capability expands its potential use in important applications, including Virtual Reality. We present extensive experimental results on ZJU-MoCap and PeopleSnapshot to clearly demonstrate the effectiveness of our proposed model. YOTO shows state-of-the-art performance on all evaluation metrics while showing significant benefits in training and inference efficiency as well as rendering quality. The code and model will be made publicly available soon.



### Accurate Real-time Polyp Detection in Videos from Concatenation of Latent Features Extracted from Consecutive Frames
- **Arxiv ID**: http://arxiv.org/abs/2303.05871v1
- **DOI**: 10.1109/BIBM55620.2022.9995323
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05871v1)
- **Published**: 2023-03-10 11:51:22+00:00
- **Updated**: 2023-03-10 11:51:22+00:00
- **Authors**: Hemin Ali Qadir, Younghak Shin, Jacob Bergsland, Ilangko Balasingham
- **Comment**: None
- **Journal**: 2022 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM) (pp. 2461-2466). IEEE
- **Summary**: An efficient deep learning model that can be implemented in real-time for polyp detection is crucial to reducing polyp miss-rate during screening procedures. Convolutional neural networks (CNNs) are vulnerable to small changes in the input image. A CNN-based model may miss the same polyp appearing in a series of consecutive frames and produce unsubtle detection output due to changes in camera pose, lighting condition, light reflection, etc. In this study, we attempt to tackle this problem by integrating temporal information among neighboring frames. We propose an efficient feature concatenation method for a CNN-based encoder-decoder model without adding complexity to the model. The proposed method incorporates extracted feature maps of previous frames to detect polyps in the current frame. The experimental results demonstrate that the proposed method of feature concatenation improves the overall performance of automatic polyp detection in videos. The following results are obtained on a public video dataset: sensitivity 90.94\%, precision 90.53\%, and specificity 92.46%



### Handheld Burst Super-Resolution Meets Multi-Exposure Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2303.05879v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05879v1)
- **Published**: 2023-03-10 12:13:31+00:00
- **Updated**: 2023-03-10 12:13:31+00:00
- **Authors**: Jamy Lafenetre, Ngoc Long Nguyen, Gabriele Facciolo, Thomas Eboli
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Image resolution is an important criterion for many applications based on satellite imagery. In this work, we adapt a state-of-the-art kernel regression technique for smartphone camera burst super-resolution to satellites. This technique leverages the local structure of the image to optimally steer the fusion kernels, limiting blur in the final high-resolution prediction, denoising the image, and recovering details up to a zoom factor of 2. We extend this approach to the multi-exposure case to predict from a sequence of multi-exposure low-resolution frames a high-resolution and noise-free one. Experiments on both single and multi-exposure scenarios show the merits of the approach. Since the fusion is learning-free, the proposed method is ensured to not hallucinate details, which is crucial for many remote sensing applications.



### Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.05886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05886v1)
- **Published**: 2023-03-10 12:38:37+00:00
- **Updated**: 2023-03-10 12:38:37+00:00
- **Authors**: Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao
- **Comment**: Accepted by CVPR2023; Code is available at
  https://github.com/PJLabADG/3DTrans
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) technique has been explored in 3D cross-domain tasks recently. Though preliminary progress has been made, the performance gap between the UDA-based 3D model and the supervised one trained with fully annotated target domain is still large. This motivates us to consider selecting partial-yet-important target data and labeling them at a minimum cost, to achieve a good trade-off between high performance and low annotation cost. To this end, we propose a Bi-domain active learning approach, namely Bi3D, to solve the cross-domain 3D object detection task. The Bi3D first develops a domainness-aware source sampling strategy, which identifies target-domain-like samples from the source domain to avoid the model being interfered by irrelevant source data. Then a diversity-based target sampling strategy is developed, which selects the most informative subset of target domain to improve the model adaptability to the target domain using as little annotation budget as possible. Experiments are conducted on typical cross-domain adaptation scenarios including cross-LiDAR-beam, cross-country, and cross-sensor, where Bi3D achieves a promising target-domain detection accuracy (89.63% on KITTI) compared with UDAbased work (84.29%), even surpassing the detector trained on the full set of the labeled target domain (88.98%). Our code is available at: https://github.com/PJLabADG/3DTrans.



### Inter-sphere consistency-based method for camera-projector pair calibration
- **Arxiv ID**: http://arxiv.org/abs/2304.14485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.14485v1)
- **Published**: 2023-03-10 12:58:16+00:00
- **Updated**: 2023-03-10 12:58:16+00:00
- **Authors**: Zhaoshuai Qi, Jingqi Pang, Yifeng Hao, Yanning Zhang
- **Comment**: 3 pages,1 figure
- **Journal**: None
- **Summary**: We construct constraints from consistency between estimated parameters from different spheres, termed inter-sphere consistency. It facilitates more flexible calibration using only two spheres, which has been considered a challenging and not well addressed ill-posed problem.



### Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.05892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05892v1)
- **Published**: 2023-03-10 12:58:34+00:00
- **Updated**: 2023-03-10 12:58:34+00:00
- **Authors**: Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, Si Liu
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Open-vocabulary object detection aims to provide object detectors trained on a fixed set of object categories with the generalizability to detect objects described by arbitrary text queries. Previous methods adopt knowledge distillation to extract knowledge from Pretrained Vision-and-Language Models (PVLMs) and transfer it to detectors. However, due to the non-adaptive proposal cropping and single-level feature mimicking processes, they suffer from information destruction during knowledge extraction and inefficient knowledge transfer. To remedy these limitations, we propose an Object-Aware Distillation Pyramid (OADP) framework, including an Object-Aware Knowledge Extraction (OAKE) module and a Distillation Pyramid (DP) mechanism. When extracting object knowledge from PVLMs, the former adaptively transforms object proposals and adopts object-aware mask attention to obtain precise and complete knowledge of objects. The latter introduces global and block distillation for more comprehensive knowledge transfer to compensate for the missing relation information in object distillation. Extensive experiments show that our method achieves significant improvement compared to current methods. Especially on the MS-COCO dataset, our OADP framework reaches $35.6$ mAP$^{\text{N}}_{50}$, surpassing the current state-of-the-art method by $3.3$ mAP$^{\text{N}}_{50}$. Code is released at https://github.com/LutingWang/OADP.



### DACov: A Deeper Analysis of Data Augmentation on the Computed Tomography Segmentation Problem
- **Arxiv ID**: http://arxiv.org/abs/2303.05912v1
- **DOI**: 10.1080/21681163.2023.2183807
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05912v1)
- **Published**: 2023-03-10 13:41:20+00:00
- **Updated**: 2023-03-10 13:41:20+00:00
- **Authors**: Bruno A. Krinski, Daniel V. Ruiz, Rayson Laroca, Eduardo Todt
- **Comment**: None
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization, 2023
- **Summary**: Due to the COVID-19 global pandemic, computer-assisted diagnoses of medical images have gained much attention, and robust methods of semantic segmentation of Computed Tomography (CT) images have become highly desirable. In this work, we present a deeper analysis of how data augmentation techniques improve segmentation performance on this problem. We evaluate 20 traditional augmentation techniques on five public datasets. Six different probabilities of applying each augmentation technique on an image were evaluated. We also assess a different training methodology where the training subsets are combined into a single larger set. All networks were evaluated through a 5-fold cross-validation strategy, resulting in over 4,600 experiments. We also propose a novel data augmentation technique based on Generative Adversarial Networks (GANs) to create new healthy and unhealthy lung CT images, evaluating four variations of our approach with the same six probabilities of the traditional methods. Our findings show that GAN-based techniques and spatial-level transformations are the most promising for improving the learning of deep models on this problem, with the StarGANv2 + F with a probability of 0.3 achieving the highest F-score value on the Ricord1a dataset in the unified training strategy. Our code is publicly available at https://github.com/VRI-UFPR/DACov2022



### GECCO: Geometrically-Conditioned Point Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2303.05916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05916v1)
- **Published**: 2023-03-10 13:45:44+00:00
- **Updated**: 2023-03-10 13:45:44+00:00
- **Authors**: Michał J. Tyszkiewicz, Pascal Fua, Eduard Trulls
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models generating images conditionally on text, such as Dall-E 2 and Stable Diffusion, have recently made a splash far beyond the computer vision community. Here, we tackle the related problem of generating point clouds, both unconditionally, and conditionally with images. For the latter, we introduce a novel geometrically-motivated conditioning scheme based on projecting sparse image features into the point cloud and attaching them to each individual point, at every step in the denoising process. This approach improves geometric consistency and yields greater fidelity than current methods relying on unstructured, global latent codes. Additionally, we show how to apply recent continuous-time diffusion schemes. Our method performs on par or above the state of art on conditional and unconditional experiments on synthetic data, while being faster, lighter, and delivering tractable likelihoods. We show it can also scale to diverse indoors scenes.



### Estimating friction coefficient using generative modelling
- **Arxiv ID**: http://arxiv.org/abs/2303.05927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.05927v1)
- **Published**: 2023-03-10 13:58:28+00:00
- **Updated**: 2023-03-10 13:58:28+00:00
- **Authors**: Mohammad Otoofi, William J. B. Midgley, Leo Laine, Henderson Leon, Laura Justham, James Fleming
- **Comment**: To be published in ICM2023
- **Journal**: None
- **Summary**: It is common to utilise dynamic models to measure the tyre-road friction in real-time. Alternatively, predictive approaches estimate the tyre-road friction by identifying the environmental factors affecting it. This work aims to formulate the problem of friction estimation as a visual perceptual learning task. The problem is broken down into detecting surface characteristics by applying semantic segmentation and using the extracted features to predict the frictional force. This work for the first time formulates the friction estimation problem as a regression from the latent space of a semantic segmentation model. The preliminary results indicate that this approach can estimate frictional force.



### Marginalia and machine learning: Handwritten text recognition for Marginalia Collections
- **Arxiv ID**: http://arxiv.org/abs/2303.05929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05929v1)
- **Published**: 2023-03-10 14:00:53+00:00
- **Updated**: 2023-03-10 14:00:53+00:00
- **Authors**: Adam Axelsson, Liang Cheng, Jonas Frankemölle, Ekta Vats
- **Comment**: Work under progress
- **Journal**: None
- **Summary**: The pressing need for digitization of historical document collections has led to a strong interest in designing computerised image processing methods for automatic handwritten text recognition (HTR). Handwritten text possesses high variability due to different writing styles, languages and scripts. Training an accurate and robust HTR system calls for data-efficient approaches due to the unavailability of sufficient amounts of annotated multi-writer text. A case study on an ongoing project ``Marginalia and Machine Learning" is presented here that focuses on automatic detection and recognition of handwritten marginalia texts i.e., text written in margins or handwritten notes. Faster R-CNN network is used for detection of marginalia and AttentionHTR is used for word recognition. The data comes from early book collections (printed) found in the Uppsala University Library, with handwritten marginalia texts. Source code and pretrained models are available at https://github.com/ektavats/Project-Marginalia.



### Self-Paced Learning for Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2303.05933v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05933v3)
- **Published**: 2023-03-10 14:11:09+00:00
- **Updated**: 2023-03-21 11:52:47+00:00
- **Authors**: Xinghong Liu, Yi Zhou, Tao Zhou, Jie Qin, Shengcai Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation tackles the challenge of generalizing knowledge acquired from a source domain to a target domain with different data distributions. Traditional domain adaptation methods presume that the classes in the source and target domains are identical, which is not always the case in real-world scenarios. Open-set domain adaptation (OSDA) addresses this limitation by allowing previously unseen classes in the target domain. Open-set domain adaptation aims to not only recognize target samples belonging to common classes shared by source and target domains but also perceive unknown class samples. We propose a novel framework based on self-paced learning to distinguish common and unknown class samples precisely, referred to as SPLOS (self-paced learning for open-set). To utilize unlabeled target samples for self-paced learning, we generate pseudo labels and design a cross-domain mixup method tailored for OSDA scenarios. This strategy minimizes the noise from pseudo labels and ensures our model progressively learns common class features of the target domain, beginning with simpler examples and advancing to more complex ones. Furthermore, unlike existing OSDA methods that require manual hyperparameter $threshold$ tuning to separate common and unknown classes, our approach self-tunes a suitable threshold, eliminating the need for empirical tuning during testing. Comprehensive experiments illustrate that our method consistently achieves superior performance on different benchmarks compared with various state-of-the-art methods.



### Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2303.05937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05937v1)
- **Published**: 2023-03-10 14:18:40+00:00
- **Updated**: 2023-03-10 14:18:40+00:00
- **Authors**: Mingfang Zhang, Jinglu Wang, Xiao Li, Yifei Huang, Yoichi Sato, Yan Lu
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: The Multiplane Image (MPI), containing a set of fronto-parallel RGBA layers, is an effective and efficient representation for view synthesis from sparse inputs. Yet, its fixed structure limits the performance, especially for surfaces imaged at oblique angles. We introduce the Structural MPI (S-MPI), where the plane structure approximates 3D scenes concisely. Conveying RGBA contexts with geometrically-faithful structures, the S-MPI directly bridges view synthesis and 3D reconstruction. It can not only overcome the critical limitations of MPI, i.e., discretization artifacts from sloped surfaces and abuse of redundant layers, and can also acquire planar 3D reconstruction. Despite the intuition and demand of applying S-MPI, great challenges are introduced, e.g., high-fidelity approximation for both RGBA layers and plane poses, multi-view consistency, non-planar regions modeling, and efficient rendering with intersected planes. Accordingly, we propose a transformer-based network based on a segmentation model. It predicts compact and expressive S-MPI layers with their corresponding masks, poses, and RGBA contexts. Non-planar regions are inclusively handled as a special case in our unified framework. Multi-view consistency is ensured by sharing global proxy embeddings, which encode plane-level features covering the complete 3D scenes with aligned coordinates. Intensive experiments show that our method outperforms both previous state-of-the-art MPI-based view synthesis methods and planar reconstruction methods.



### ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2303.05938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05938v1)
- **Published**: 2023-03-10 14:19:02+00:00
- **Updated**: 2023-03-10 14:19:02+00:00
- **Authors**: Zhengdi Yu, Shaoli Huang, Chen Fang, Toby P. Breckon, Jue Wang
- **Comment**: Accepted by CVPR 2023; Code at
  https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction
- **Journal**: None
- **Summary**: Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction.



### Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.05952v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.05952v1)
- **Published**: 2023-03-10 14:38:49+00:00
- **Updated**: 2023-03-10 14:38:49+00:00
- **Authors**: Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, Trishul Chilimbi
- **Comment**: 14 pages, 8 figure, CVPR 2023 accepted
- **Journal**: None
- **Summary**: Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular multi-modal representation learning frameworks: the CLIP-based two-tower model and the ALBEF-based fusion model. We test our model on a variety of tasks including zero/few-shot image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our method achieves consistent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization.



### Neuron Structure Modeling for Generalizable Remote Physiological Measurement
- **Arxiv ID**: http://arxiv.org/abs/2303.05955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05955v1)
- **Published**: 2023-03-10 14:44:11+00:00
- **Updated**: 2023-03-10 14:44:11+00:00
- **Authors**: Hao Lu, Zitong Yu, Xuesong Niu, Yingcong Chen
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG) technology has drawn increasing attention in recent years. It can extract Blood Volume Pulse (BVP) from facial videos, making many applications like health monitoring and emotional analysis more accessible. However, as the BVP signal is easily affected by environmental changes, existing methods struggle to generalize well for unseen domains. In this paper, we systematically address the domain shift problem in the rPPG measurement task. We show that most domain generalization methods do not work well in this problem, as domain labels are ambiguous in complicated environmental changes. In light of this, we propose a domain-label-free approach called NEuron STructure modeling (NEST). NEST improves the generalization capacity by maximizing the coverage of feature space during training, which reduces the chance for under-optimized feature activation during inference. Besides, NEST can also enrich and enhance domain invariant features across multi-domain. We create and benchmark a large-scale domain generalization protocol for the rPPG measurement task. Extensive experiments show that our approach outperforms the state-of-the-art methods on both cross-dataset and intra-dataset settings.



### Joint ANN-SNN Co-training for Object Localization and Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.12738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.12738v1)
- **Published**: 2023-03-10 14:45:02+00:00
- **Updated**: 2023-03-10 14:45:02+00:00
- **Authors**: Marc Baltes, Nidal Abujahar, Ye Yue, Charles D. Smith, Jundong Liu
- **Comment**: Accepted to ICASSP 2023
- **Journal**: None
- **Summary**: The field of machine learning has been greatly transformed with the advancement of deep artificial neural networks (ANNs) and the increased availability of annotated data. Spiking neural networks (SNNs) have recently emerged as a low-power alternative to ANNs due to their sparsity nature. In this work, we propose a novel hybrid ANN-SNN co-training framework to improve the performance of converted SNNs. Our approach is a fine-tuning scheme, conducted through an alternating, forward-backward training procedure. We apply our framework to object detection and image segmentation tasks. Experiments demonstrate the effectiveness of our approach in achieving the design goals.



### Automated crack propagation measurement on asphalt concrete specimens using an optical flow-based deep neural network
- **Arxiv ID**: http://arxiv.org/abs/2303.05957v1
- **DOI**: 10.1080/10298436.2023.2186407
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05957v1)
- **Published**: 2023-03-10 14:45:37+00:00
- **Updated**: 2023-03-10 14:45:37+00:00
- **Authors**: Zehui Zhu, Imad L. Al-Qadi
- **Comment**: None
- **Journal**: International Journal of Pavement Engineering, 24:1 (2023)
- **Summary**: This article proposes a deep neural network, namely CrackPropNet, to measure crack propagation on asphalt concrete (AC) specimens. It offers an accurate, flexible, efficient, and low-cost solution for crack propagation measurement using images collected during cracking tests. CrackPropNet significantly differs from traditional deep learning networks, as it involves learning to locate displacement field discontinuities by matching features at various locations in the reference and deformed images. An image library representing the diversified cracking behavior of AC was developed for supervised training. CrackPropNet achieved an optimal dataset scale F-1 of 0.755 and optimal image scale F-1 of 0.781 on the testing dataset at a running speed of 26 frame-per-second. Experiments demonstrated that low to medium-level Gaussian noises had a limited impact on the measurement accuracy of CrackPropNet. Moreover, the model showed promising generalization on fundamentally different images. As a crack measurement technique, the CrackPropNet can detect complex crack patterns accurately and efficiently in AC cracking tests. It can be applied to characterize the cracking phenomenon, evaluate AC cracking potential, validate test protocols, and verify theoretical models.



### Score-Based Generative Models for Medical Image Segmentation using Signed Distance Functions
- **Arxiv ID**: http://arxiv.org/abs/2303.05966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05966v2)
- **Published**: 2023-03-10 14:55:35+00:00
- **Updated**: 2023-07-21 11:21:30+00:00
- **Authors**: Lea Bogensperger, Dominik Narnhofer, Filip Ilic, Thomas Pock
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is a crucial task that relies on the ability to accurately identify and isolate regions of interest in medical images. Thereby, generative approaches allow to capture the statistical properties of segmentation masks that are dependent on the respective structures. In this work we propose a conditional score-based generative modeling framework to represent the signed distance function (SDF) leading to an implicit distribution of segmentation masks. The advantage of leveraging the SDF is a more natural distortion when compared to that of binary masks. By learning the score function of the conditional distribution of SDFs we can accurately sample from the distribution of segmentation masks, allowing for the evaluation of statistical quantities. Thus, this probabilistic representation allows for the generation of uncertainty maps represented by the variance, which can aid in further analysis and enhance the predictive robustness. We qualitatively and quantitatively illustrate competitive performance of the proposed method on a public nuclei and gland segmentation data set, highlighting its potential utility in medical image segmentation applications.



### Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception
- **Arxiv ID**: http://arxiv.org/abs/2303.05970v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05970v2)
- **Published**: 2023-03-10 15:01:51+00:00
- **Updated**: 2023-03-13 04:41:36+00:00
- **Authors**: Chunrui Han, Jianjian Sun, Zheng Ge, Jinrong Yang, Runpei Dong, Hongyu Zhou, Weixin Mao, Yuang Peng, Xiangyu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Long-term temporal fusion is a crucial but often overlooked technique in camera-based Bird's-Eye-View (BEV) 3D perception. Existing methods are mostly in a parallel manner. While parallel fusion can benefit from long-term information, it suffers from increasing computational and memory overheads as the fusion window size grows. Alternatively, BEVFormer adopts a recurrent fusion pipeline so that history information can be efficiently integrated, yet it fails to benefit from longer temporal frames. In this paper, we explore an embarrassingly simple long-term recurrent fusion strategy built upon the LSS-based methods and find it already able to enjoy the merits from both sides, i.e., rich long-term information and efficient fusion pipeline. A temporal embedding module is further proposed to improve the model's robustness against occasionally missed frames in practical scenarios. We name this simple but effective fusing pipeline VideoBEV. Experimental results on the nuScenes benchmark show that VideoBEV obtains leading performance on various camera-based 3D perception tasks, including object detection (55.4% mAP and 62.9% NDS), segmentation (48.6% vehicle mIoU), tracking (54.8% AMOTA), and motion prediction (0.80m minADE and 0.463 EPA). Code will be available.



### Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models
- **Arxiv ID**: http://arxiv.org/abs/2303.05977v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2303.05977v2)
- **Published**: 2023-03-10 15:17:22+00:00
- **Updated**: 2023-07-21 22:33:49+00:00
- **Authors**: Tom van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring
- **Comment**: None
- **Journal**: None
- **Summary**: Medical Visual Question Answering (VQA) is an important challenge, as it would lead to faster and more accurate diagnoses and treatment decisions. Most existing methods approach it as a multi-class classification problem, which restricts the outcome to a predefined closed-set of curated answers. We focus on open-ended VQA and motivated by the recent advances in language models consider it as a generative task. Leveraging pre-trained language models, we introduce a novel method particularly suited for small, domain-specific, medical datasets. To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens. Then, alongside the question, these learnable tokens directly prompt the language model. We explore recent parameter-efficient fine-tuning strategies for language models, which allow for resource- and data-efficient fine-tuning. We evaluate our approach on the prime medical VQA benchmarks, namely, Slake, OVQA and PathVQA. The results demonstrate that our approach outperforms existing methods across various training settings while also being computationally efficient.



### Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation
- **Arxiv ID**: http://arxiv.org/abs/2303.05983v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.05983v2)
- **Published**: 2023-03-10 15:35:11+00:00
- **Updated**: 2023-06-14 16:03:55+00:00
- **Authors**: Zhiwei Zhang, Yuliang Liu
- **Comment**: 35 pages
- **Journal**: None
- **Summary**: The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch. The first state involves a discrete variational autoencoder (dVAE) to compress each image into short tokens, which are then concatenated with text tokens as a single data stream to be fed into the decoder-based transformer for generating visual re-creation and textual feedback in the second state. We provide comprehensive analyses of experimental results in terms of re-created image quality, answer accuracy, and the model behavior when faced with uncertainty and imperfect user queries. We hope our explorations and findings contribute valuable insights regarding the accountability of textual-visual generative models.



### Combining visibility analysis and deep learning for refinement of semantic 3D building models by conflict classification
- **Arxiv ID**: http://arxiv.org/abs/2303.05998v1
- **DOI**: 10.5194/isprs-annals-X-4-W2-2022-289-2022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.05998v1)
- **Published**: 2023-03-10 16:01:30+00:00
- **Updated**: 2023-03-10 16:01:30+00:00
- **Authors**: Olaf Wysocki, Eleonora Grilli, Ludwig Hoegner, Uwe Stilla
- **Comment**: ISPRS Annals, 3DGeoInfo 2022, Australia, Sydney
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., X-4/W2-2022
- **Summary**: Semantic 3D building models are widely available and used in numerous applications. Such 3D building models display rich semantics but no fa\c{c}ade openings, chiefly owing to their aerial acquisition techniques. Hence, refining models' fa\c{c}ades using dense, street-level, terrestrial point clouds seems a promising strategy. In this paper, we propose a method of combining visibility analysis and neural networks for enriching 3D models with window and door features. In the method, occupancy voxels are fused with classified point clouds, which provides semantics to voxels. Voxels are also used to identify conflicts between laser observations and 3D models. The semantic voxels and conflicts are combined in a Bayesian network to classify and delineate fa\c{c}ade openings, which are reconstructed using a 3D model library. Unaffected building semantics is preserved while the updated one is added, thereby upgrading the building model to LoD3. Moreover, Bayesian network results are back-projected onto point clouds to improve points' classification accuracy. We tested our method on a municipal CityGML LoD2 repository and the open point cloud datasets: TUM-MLS-2016 and TUM-FA\c{C}ADE. Validation results revealed that the method improves the accuracy of point cloud semantic segmentation and upgrades buildings with fa\c{c}ade elements. The method can be applied to enhance the accuracy of urban simulations and facilitate the development of semantic segmentation algorithms.



### Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.06015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06015v2)
- **Published**: 2023-03-10 16:19:34+00:00
- **Updated**: 2023-03-13 00:32:29+00:00
- **Authors**: Mathieu Pagé-Fortin, Brahim Chaib-draa
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the success of deep learning methods on instance segmentation, these models still suffer from catastrophic forgetting in continual learning scenarios. In this paper, our contributions for continual instance segmentation are threefold. First, we propose the Y-knowledge distillation (Y-KD), a knowledge distillation strategy that shares a common feature extractor between the teacher and student networks. As the teacher is also updated with new data in Y-KD, the increased plasticity results in new modules that are specialized on new classes. Second, our Y-KD approach is supported by a dynamic architecture method that grows new modules for each task and uses all of them for inference with a unique instance segmentation head, which significantly reduces forgetting. Third, we complete our approach by leveraging checkpoint averaging as a simple method to manually balance the trade-off between the performance on the various sets of classes, thus increasing the control over the model's behavior without any additional cost. These contributions are united in our model that we name the Dynamic Y-KD network.   We perform extensive experiments on several single-step and multi-steps scenarios on Pascal-VOC, and we show that our approach outperforms previous methods both on past and new classes. For instance, compared to recent work, our method obtains +2.1% mAP on old classes in 15-1, +7.6% mAP on new classes in 19-1 and reaches 91.5% of the mAP obtained by joint-training on all classes in 15-5.



### Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.06040v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06040v3)
- **Published**: 2023-03-10 16:30:09+00:00
- **Updated**: 2023-08-18 12:31:45+00:00
- **Authors**: Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu
- **Comment**: Accepted at Deep Generative Models workshop at MICCAI 2023
- **Journal**: None
- **Summary**: Recently, denoising diffusion probabilistic models (DDPM) have been applied to image segmentation by generating segmentation masks conditioned on images, while the applications were mainly limited to 2D networks without exploiting potential benefits from the 3D formulation. In this work, we studied the DDPM-based segmentation model for 3D multiclass segmentation on two large multiclass data sets (prostate MR and abdominal CT). We observed that the difference between training and test methods led to inferior performance for existing DDPM methods. To mitigate the inconsistency, we proposed a recycling method which generated corrupted masks based on the model's prediction at a previous time step instead of using ground truth. The proposed method achieved statistically significantly improved performance compared to existing DDPMs, independent of a number of other techniques for reducing train-test discrepancy, including performing mask prediction, using Dice loss, and reducing the number of diffusion time steps during training. The performance of diffusion models was also competitive and visually similar to non-diffusion-based U-net, within the same compute budget. The JAX-based diffusion framework has been released at https://github.com/mathpluscode/ImgX-DiffSeg.



### MVImgNet: A Large-scale Dataset of Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/2303.06042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06042v1)
- **Published**: 2023-03-10 16:31:31+00:00
- **Updated**: 2023-03-10 16:31:31+00:00
- **Authors**: Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, Guanying Chen, Shuguang Cui, Xiaoguang Han
- **Comment**: To be appear in CVPR2023. Project page:
  https://gaplab.cuhk.edu.cn/projects/MVImgNet/
- **Journal**: None
- **Summary**: Being data-driven is one of the most iconic properties of deep learning algorithms. The birth of ImageNet drives a remarkable trend of "learning from large-scale data" in computer vision. Pretraining on ImageNet to obtain rich universal representations has been manifested to benefit various 2D visual tasks, and becomes a standard in 2D vision. However, due to the laborious collection of real-world 3D data, there is yet no generic dataset serving as a counterpart of ImageNet in 3D vision, thus how such a dataset can impact the 3D community is unraveled. To remedy this defect, we introduce MVImgNet, a large-scale dataset of multi-view images, which is highly convenient to gain by shooting videos of real-world objects in human daily life. It contains 6.5 million frames from 219,188 videos crossing objects from 238 classes, with rich annotations of object masks, camera parameters, and point clouds. The multi-view attribute endows our dataset with 3D-aware signals, making it a soft bridge between 2D and 3D vision.   We conduct pilot studies for probing the potential of MVImgNet on a variety of 3D and 2D visual tasks, including radiance field reconstruction, multi-view stereo, and view-consistent image understanding, where MVImgNet demonstrates promising performance, remaining lots of possibilities for future explorations.   Besides, via dense reconstruction on MVImgNet, a 3D object point cloud dataset is derived, called MVPNet, covering 87,200 samples from 150 categories, with the class label on each point cloud. Experiments show that MVPNet can benefit the real-world 3D object classification while posing new challenges to point cloud understanding.   MVImgNet and MVPNet will be publicly available, hoping to inspire the broader vision community.



### Long-tailed Classification from a Bayesian-decision-theory Perspective
- **Arxiv ID**: http://arxiv.org/abs/2303.06075v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2303.06075v2)
- **Published**: 2023-03-10 16:53:51+00:00
- **Updated**: 2023-03-21 00:36:17+00:00
- **Authors**: Bolian Li, Ruqi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Long-tailed classification poses a challenge due to its heavy imbalance in class probabilities and tail-sensitivity risks with asymmetric misprediction costs. Recent attempts have used re-balancing loss and ensemble methods, but they are largely heuristic and depend heavily on empirical results, lacking theoretical explanation. Furthermore, existing methods overlook the decision loss, which characterizes different costs associated with tailed classes. This paper presents a general and principled framework from a Bayesian-decision-theory perspective, which unifies existing techniques including re-balancing and ensemble methods, and provides theoretical justifications for their effectiveness. From this perspective, we derive a novel objective based on the integrated risk and a Bayesian deep-ensemble approach to improve the accuracy of all classes, especially the "tail". Besides, our framework allows for task-adaptive decision loss which provides provably optimal decisions in varying task scenarios, along with the capability to quantify uncertainty. Finally, We conduct comprehensive experiments, including standard classification, tail-sensitive classification with a new False Head Rate metric, calibration, and ablation studies. Our framework significantly improves the current SOTA even on large-scale real-world datasets like ImageNet.



### Communication-Critical Planning via Multi-Agent Trajectory Exchange
- **Arxiv ID**: http://arxiv.org/abs/2303.06080v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06080v1)
- **Published**: 2023-03-10 16:59:24+00:00
- **Updated**: 2023-03-10 16:59:24+00:00
- **Authors**: Nathaniel Moore Glaser, Zsolt Kira
- **Comment**: Accepted to ICRA 2023
- **Journal**: None
- **Summary**: This paper addresses the task of joint multi-agent perception and planning, especially as it relates to the real-world challenge of collision-free navigation for connected self-driving vehicles. For this task, several communication-enabled vehicles must navigate through a busy intersection while avoiding collisions with each other and with obstacles. To this end, this paper proposes a learnable costmap-based planning mechanism, given raw perceptual data, that is (1) distributed, (2) uncertainty-aware, and (3) bandwidth-efficient. Our method produces a costmap and uncertainty-aware entropy map to sort and fuse candidate trajectories as evaluated across multiple-agents. The proposed method demonstrates several favorable performance trends on a suite of open-source overhead datasets as well as within a novel communication-critical simulator. It produces accurate semantic occupancy forecasts as an intermediate perception output, attaining a 72.5% average pixel-wise classification accuracy. By selecting the top trajectory, the multi-agent method scales well with the number of agents, reducing the hard collision rate by up to 57% with eight agents compared to the single-agent version.



### Improving Domain-Invariance in Self-Supervised Learning via Batch Styles Standardization
- **Arxiv ID**: http://arxiv.org/abs/2303.06088v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06088v3)
- **Published**: 2023-03-10 17:09:04+00:00
- **Updated**: 2023-04-24 10:04:08+00:00
- **Authors**: Marin Scalbert, Maria Vakalopoulou, Florent Couzinié-Devy
- **Comment**: None
- **Journal**: None
- **Summary**: The recent rise of Self-Supervised Learning (SSL) as one of the preferred strategies for learning with limited labeled data, and abundant unlabeled data has led to the widespread use of these models. They are usually pretrained, finetuned, and evaluated on the same data distribution, i.e., within an in-distribution setting. However, they tend to perform poorly in out-of-distribution evaluation scenarios, a challenge that Unsupervised Domain Generalization (UDG) seeks to address.   This paper introduces a novel method to standardize the styles of images in a batch. Batch styles standardization, relying on Fourier-based augmentations, promotes domain invariance in SSL by preventing spurious correlations from leaking into the features. The combination of batch styles standardization with the well-known contrastive-based method SimCLR leads to a novel UDG method named CLaSSy ($\textbf{C}$ontrastive $\textbf{L}$e$\textbf{a}$rning with $\textbf{S}$tandardized $\textbf{S}$t$\textbf{y}$les). CLaSSy offers serious advantages over prior methods, as it does not rely on domain labels and is scalable to handle a large number of domains. Experimental results on various UDG datasets demonstrate the superior performance of CLaSSy compared to existing UDG methods. Finally, the versatility of the proposed batch styles standardization is demonstrated by extending respectively the contrastive-based and non-contrastive-based SSL methods, SWaV and MSN, while considering different backbone architectures (convolutional-based, transformers-based).



### Self-supervised Training Sample Difficulty Balancing for Local Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.06124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.06124v1)
- **Published**: 2023-03-10 18:37:43+00:00
- **Updated**: 2023-03-10 18:37:43+00:00
- **Authors**: Jiahan Zhang, Dayong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In the case of an imbalance between positive and negative samples, hard negative mining strategies have been shown to help models learn more subtle differences between positive and negative samples, thus improving recognition performance. However, if too strict mining strategies are promoted in the dataset, there may be a risk of introducing false negative samples. Meanwhile, the implementation of the mining strategy disrupts the difficulty distribution of samples in the real dataset, which may cause the model to over-fit these difficult samples. Therefore, in this paper, we investigate how to trade off the difficulty of the mined samples in order to obtain and exploit high-quality negative samples, and try to solve the problem in terms of both the loss function and the training strategy. The proposed balance loss provides an effective discriminant for the quality of negative samples by combining a self-supervised approach to the loss function, and uses a dynamic gradient modulation strategy to achieve finer gradient adjustment for samples of different difficulties. The proposed annealing training strategy then constrains the difficulty of the samples drawn from negative sample mining to provide data sources with different difficulty distributions for the loss function, and uses samples of decreasing difficulty to train the model. Extensive experiments show that our new descriptors outperform previous state-of-the-art descriptors for patch validation, matching, and retrieval tasks.



### Single-branch Network for Multimodal Training
- **Arxiv ID**: http://arxiv.org/abs/2303.06129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06129v1)
- **Published**: 2023-03-10 18:48:40+00:00
- **Updated**: 2023-03-10 18:48:40+00:00
- **Authors**: Muhammad Saad Saeed, Shah Nawaz, Muhammad Haris Khan, Muhammad Zaigham Zaheer, Karthik Nandakumar, Muhammad Haroon Yousaf, Arif Mahmood
- **Comment**: Accepted at ICASSP 2023
- **Journal**: None
- **Summary**: With the rapid growth of social media platforms, users are sharing billions of multimedia posts containing audio, images, and text. Researchers have focused on building autonomous systems capable of processing such multimedia data to solve challenging multimodal tasks including cross-modal retrieval, matching, and verification. Existing works use separate networks to extract embeddings of each modality to bridge the gap between them. The modular structure of their branched networks is fundamental in creating numerous multimodal applications and has become a defacto standard to handle multiple modalities. In contrast, we propose a novel single-branch network capable of learning discriminative representation of unimodal as well as multimodal tasks without changing the network. An important feature of our single-branch network is that it can be trained either using single or multiple modalities without sacrificing performance. We evaluated our proposed single-branch network on the challenging multimodal problem (face-voice association) for cross-modal verification and matching tasks with various loss formulations. Experimental results demonstrate the superiority of our proposed single-branch network over the existing methods in a wide range of experiments. Code: https://github.com/msaadsaeed/SBNet



### Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition
- **Arxiv ID**: http://arxiv.org/abs/2303.06138v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.06138v3)
- **Published**: 2023-03-10 18:55:46+00:00
- **Updated**: 2023-06-08 06:19:20+00:00
- **Authors**: Hong-Xing Yu, Michelle Guo, Alireza Fathi, Yen-Yu Chang, Eric Ryan Chan, Ruohan Gao, Thomas Funkhouser, Jiajun Wu
- **Comment**: Journal extension of arXiv:2012.08503 (TMLR 2023). The first two
  authors contributed equally to this work. Project page:
  https://kovenyu.com/osf/
- **Journal**: Transactions on Machine Learning Research (TMLR), 2023
- **Summary**: Photorealistic object appearance modeling from 2D images is a constant topic in vision and graphics. While neural implicit methods (such as Neural Radiance Fields) have shown high-fidelity view synthesis results, they cannot relight the captured objects. More recent neural inverse rendering approaches have enabled object relighting, but they represent surface properties as simple BRDFs, and therefore cannot handle translucent objects. We propose Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct object appearance from only images. OSFs not only support free-viewpoint object relighting, but also can model both opaque and translucent objects. While accurately modeling subsurface light transport for translucent objects can be highly complex and even intractable for neural methods, OSFs learn to approximate the radiance transfer from a distant light to an outgoing direction at any spatial location. This approximation avoids explicitly modeling complex subsurface scattering, making learning a neural implicit model tractable. Experiments on real and synthetic data show that OSFs accurately reconstruct appearances for both opaque and translucent objects, allowing faithful free-viewpoint relighting as well as scene composition.



### Learning to Select Camera Views: Efficient Multiview Understanding at Few Glances
- **Arxiv ID**: http://arxiv.org/abs/2303.06145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06145v1)
- **Published**: 2023-03-10 18:59:10+00:00
- **Updated**: 2023-03-10 18:59:10+00:00
- **Authors**: Yunzhong Hou, Stephen Gould, Liang Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Multiview camera setups have proven useful in many computer vision applications for reducing ambiguities, mitigating occlusions, and increasing field-of-view coverage. However, the high computational cost associated with multiple views poses a significant challenge for end devices with limited computational resources. To address this issue, we propose a view selection approach that analyzes the target object or scenario from given views and selects the next best view for processing. Our approach features a reinforcement learning based camera selection module, MVSelect, that not only selects views but also facilitates joint training with the task network. Experimental results on multiview classification and detection tasks show that our approach achieves promising performance while using only 2 or 3 out of N available views, significantly reducing computational costs. Furthermore, analysis on the selected views reveals that certain cameras can be shut off with minimal performance impact, shedding light on future camera layout optimization for multiview systems. Code is available at https://github.com/hou-yz/MVSelect.



### StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces
- **Arxiv ID**: http://arxiv.org/abs/2303.06146v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.06146v2)
- **Published**: 2023-03-10 18:59:33+00:00
- **Updated**: 2023-07-21 06:34:54+00:00
- **Authors**: Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
- **Comment**: ICCV 2023. Code: https://github.com/williamyang1991/StyleGANEX
  Project page: https://www.mmlab-ntu.com/project/styleganex/
- **Journal**: None
- **Summary**: Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification.



### Category-Level Multi-Part Multi-Joint 3D Shape Assembly
- **Arxiv ID**: http://arxiv.org/abs/2303.06163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06163v1)
- **Published**: 2023-03-10 19:02:26+00:00
- **Updated**: 2023-03-10 19:02:26+00:00
- **Authors**: Yichen Li, Kaichun Mo, Yueqi Duan, He Wang, Jiequan Zhang, Lin Shao, Wojciech Matusik, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Shape assembly composes complex shapes geometries by arranging simple part geometries and has wide applications in autonomous robotic assembly and CAD modeling. Existing works focus on geometry reasoning and neglect the actual physical assembly process of matching and fitting joints, which are the contact surfaces connecting different parts. In this paper, we consider contacting joints for the task of multi-part assembly. A successful joint-optimized assembly needs to satisfy the bilateral objectives of shape structure and joint alignment. We propose a hierarchical graph learning approach composed of two levels of graph representation learning. The part graph takes part geometries as input to build the desired shape structure. The joint-level graph uses part joints information and focuses on matching and aligning joints. The two kinds of information are combined to achieve the bilateral objectives. Extensive experiments demonstrate that our method outperforms previous methods, achieving better shape structure and higher joint alignment accuracy.



### Overwriting Pretrained Bias with Finetuning Data
- **Arxiv ID**: http://arxiv.org/abs/2303.06167v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.06167v2)
- **Published**: 2023-03-10 19:10:58+00:00
- **Updated**: 2023-08-17 02:01:07+00:00
- **Authors**: Angelina Wang, Olga Russakovsky
- **Comment**: ICCV 2023 Oral
- **Journal**: None
- **Summary**: Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.



### Spatially-varying Regularization with Conditional Transformer for Unsupervised Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2303.06168v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06168v1)
- **Published**: 2023-03-10 19:11:16+00:00
- **Updated**: 2023-03-10 19:11:16+00:00
- **Authors**: Junyu Chen, Yihao Liu, Yufan He, Yong Du
- **Comment**: None
- **Journal**: None
- **Summary**: In the past, optimization-based registration models have used spatially-varying regularization to account for deformation variations in different image regions. However, deep learning-based registration models have mostly relied on spatially-invariant regularization. Here, we introduce an end-to-end framework that uses neural networks to learn a spatially-varying deformation regularizer directly from data. The hyperparameter of the proposed regularizer is conditioned into the network, enabling easy tuning of the regularization strength. The proposed method is built upon a Transformer-based model, but it can be readily adapted to any network architecture. We thoroughly evaluated the proposed approach using publicly available datasets and observed a significant performance improvement while maintaining smooth deformation. The source code of this work will be made available after publication.



### Deformable Cross-Attention Transformer for Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2303.06179v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06179v1)
- **Published**: 2023-03-10 19:22:01+00:00
- **Updated**: 2023-03-10 19:22:01+00:00
- **Authors**: Junyu Chen, Yihao Liu, Yufan He, Yong Du
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have recently shown promise for medical image applications, leading to an increasing interest in developing such models for medical image registration. Recent advancements in designing registration Transformers have focused on using cross-attention (CA) to enable a more precise understanding of spatial correspondences between moving and fixed images. Here, we propose a novel CA mechanism that computes windowed attention using deformable windows. In contrast to existing CA mechanisms that require intensive computational complexity by either computing CA globally or locally with a fixed and expanded search window, the proposed deformable CA can selectively sample a diverse set of features over a large search window while maintaining low computational complexity. The proposed model was extensively evaluated on multi-modal, mono-modal, and atlas-to-patient registration tasks, demonstrating promising performance against state-of-the-art methods and indicating its effectiveness for medical image registration. The source code for this work will be available after publication.



### Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels
- **Arxiv ID**: http://arxiv.org/abs/2303.06180v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06180v1)
- **Published**: 2023-03-10 19:23:33+00:00
- **Updated**: 2023-03-10 19:23:33+00:00
- **Authors**: Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
- **Comment**: 10 pages, 1 algorithm, 4 tables
- **Journal**: None
- **Summary**: Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data with partial labels.



### Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/2303.06193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06193v1)
- **Published**: 2023-03-10 19:56:34+00:00
- **Updated**: 2023-03-10 19:56:34+00:00
- **Authors**: Fangda Li, Zhiqiang Hu, Wen Chen, Avinash Kak
- **Comment**: None
- **Journal**: None
- **Summary**: Immunohistochemical (IHC) staining highlights the molecular information critical to diagnostics in tissue samples. However, compared to H&E staining, IHC staining can be much more expensive in terms of both labor and the laboratory equipment required. This motivates recent research that demonstrates that the correlations between the morphological information present in the H&E-stained slides and the molecular information in the IHC-stained slides can be used for H&E-to-IHC stain translation. However, due to a lack of pixel-perfect H&E-IHC groundtruth pairs, most existing methods have resorted to relying on expert annotations. To remedy this situation, we present a new loss function, Adaptive Supervised PatchNCE (ASP), to directly deal with the input to target inconsistencies in a proposed H&E-to-IHC image-to-image translation framework. The ASP loss is built upon a patch-based contrastive learning criterion, named Supervised PatchNCE (SP), and augments it further with weight scheduling to mitigate the negative impact of noisy supervision. Lastly, we introduce the Multi-IHC Stain Translation (MIST) dataset, which contains aligned H&E-IHC patches for 4 different IHC stains critical to breast cancer diagnosis. In our experiment, we demonstrate that our proposed method outperforms existing image-to-image translation methods for stain translation to multiple IHC stains. All of our code and datasets are available at https://github.com/lifangda01/AdaptiveSupervisedPatchNCE.



### A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture
- **Arxiv ID**: http://arxiv.org/abs/2303.06202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.06202v1)
- **Published**: 2023-03-10 20:38:40+00:00
- **Updated**: 2023-03-10 20:38:40+00:00
- **Authors**: Vinit Katariya, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle Trajectory datasets that provide multiple point-of-views (POVs) can be valuable for various traffic safety and management applications. Despite the abundance of trajectory datasets, few offer a comprehensive and diverse range of driving scenes, capturing multiple viewpoints of various highway layouts, merging lanes, and configurations. This limits their ability to capture the nuanced interactions between drivers, vehicles, and the roadway infrastructure. We introduce the \emph{Carolinas Highway Dataset (CHD\footnote{\emph{CHD} available at: \url{https://github.com/TeCSAR-UNCC/Carolinas\_Dataset}})}, a vehicle trajectory, detection, and tracking dataset. \emph{CHD} is a collection of 1.6 million frames captured in highway-based videos from eye-level and high-angle POVs at eight locations across Carolinas with 338,000 vehicle trajectories. The locations, timing of recordings, and camera angles were carefully selected to capture various road geometries, traffic patterns, lighting conditions, and driving behaviors.   We also present \emph{PishguVe}\footnote{\emph{PishguVe} code available at: \url{https://github.com/TeCSAR-UNCC/PishguVe}}, a novel vehicle trajectory prediction architecture that uses attention-based graph isomorphism and convolutional neural networks. The results demonstrate that \emph{PishguVe} outperforms existing algorithms to become the new state-of-the-art (SotA) in bird's-eye, eye-level, and high-angle POV trajectory datasets. Specifically, it achieves a 12.50\% and 10.20\% improvement in ADE and FDE, respectively, over the current SotA on NGSIM dataset. Compared to best-performing models on CHD, \emph{PishguVe} achieves lower ADE and FDE on eye-level data by 14.58\% and 27.38\%, respectively, and improves ADE and FDE on high-angle data by 8.3\% and 6.9\%, respectively.



### A New Super-Resolution Measurement of Perceptual Quality and Fidelity
- **Arxiv ID**: http://arxiv.org/abs/2303.06207v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.06207v1)
- **Published**: 2023-03-10 21:08:24+00:00
- **Updated**: 2023-03-10 21:08:24+00:00
- **Authors**: Sheng Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Super-resolution results are usually measured by full-reference image quality metrics or human rating scores. However, these evaluation methods are general image quality measurement, and do not account for the nature of the super-resolution problem. In this work, we analyze the evaluation problem based on the one-to-many mapping nature of super-resolution, and propose a novel distribution-based metric for super-resolution. Starting from the distribution distance, we derive the proposed metric to make it accessible and easy to compute. Through a human subject study on super-resolution, we show that the proposed metric is highly correlated with the human perceptual quality, and better than most existing metrics. Moreover, the proposed metric has a higher correlation with the fidelity measure compared to the perception-based metrics. To understand the properties of the proposed metric, we conduct extensive evaluation in terms of its design choices, and show that the metric is robust to its design choices. Finally, we show that the metric can be used to train super-resolution networks for better perceptual quality.



### SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2303.06209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.06209v2)
- **Published**: 2023-03-10 21:17:14+00:00
- **Updated**: 2023-08-08 08:06:48+00:00
- **Authors**: Shuai Yuan, Shuzhi Yu, Hannah Kim, Carlo Tomasi
- **Comment**: Accepted by ICCV-2023; Code is available at
  https://github.com/duke-vision/semantic-unsup-flow-release
- **Journal**: None
- **Summary**: Unsupervised optical flow estimation is especially hard near occlusions and motion boundaries and in low-texture regions. We show that additional information such as semantics and domain knowledge can help better constrain this problem. We introduce SemARFlow, an unsupervised optical flow network designed for autonomous driving data that takes estimated semantic segmentation masks as additional inputs. This additional information is injected into the encoder and into a learned upsampler that refines the flow output. In addition, a simple yet effective semantic augmentation module provides self-supervision when learning flow and its boundaries for vehicles, poles, and sky. Together, these injections of semantic information improve the KITTI-2015 optical flow test error rate from 11.80% to 8.38%. We also show visible improvements around object boundaries as well as a greater ability to generalize across datasets. Code is available at https://github.com/duke-vision/semantic-unsup-flow-release.



### NeRFlame: FLAME-based conditioning of NeRF for 3D face rendering
- **Arxiv ID**: http://arxiv.org/abs/2303.06226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.06226v1)
- **Published**: 2023-03-10 22:21:30+00:00
- **Updated**: 2023-03-10 22:21:30+00:00
- **Authors**: Wojciech Zając, Jacek Tabor, Maciej Zięba, Przemysław Spurek
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional 3D face models are based on mesh representations with texture. One of the most important models is FLAME (Faces Learned with an Articulated Model and Expressions), which produces meshes of human faces that are fully controllable. Unfortunately, such models have problems with capturing geometric and appearance details. In contrast to mesh representation, the neural radiance field (NeRF) produces extremely sharp renders. But implicit methods are hard to animate and do not generalize well to unseen expressions. It is not trivial to effectively control NeRF models to obtain face manipulation. The present paper proposes a novel approach, named NeRFlame, which combines the strengths of both NeRF and FLAME methods. Our method enables high-quality rendering capabilities of NeRF while also offering complete control over the visual appearance, similar to FLAME. Unlike conventional NeRF-based architectures that utilize neural networks to model RGB colors and volume density, NeRFlame employs FLAME mesh as an explicit density volume. As a result, color values are non-zero only in the proximity of the FLAME mesh. This FLAME backbone is then integrated into the NeRF architecture to predict RGB colors, allowing NeRFlame to explicitly model volume density and implicitly model RGB colors.



### MCROOD: Multi-Class Radar Out-Of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.06232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2303.06232v1)
- **Published**: 2023-03-10 22:44:24+00:00
- **Updated**: 2023-03-10 22:44:24+00:00
- **Authors**: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach
- **Comment**: Accepted at ICASSP 2023
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45%, 92.13%, and 96.58% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is very suitable for real-time processing.



### Compressive Sensing with Tensorized Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2303.06235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.06235v1)
- **Published**: 2023-03-10 22:59:09+00:00
- **Updated**: 2023-03-10 22:59:09+00:00
- **Authors**: Rakib Hyder, M. Salman Asif
- **Comment**: None
- **Journal**: ICASSP 2023
- **Summary**: Deep networks can be trained to map images into a low-dimensional latent space. In many cases, different images in a collection are articulated versions of one another; for example, same object with different lighting, background, or pose. Furthermore, in many cases, parts of images can be corrupted by noise or missing entries. In this paper, our goal is to recover images without access to the ground-truth (clean) images using the articulations as structural prior of the data. Such recovery problems fall under the domain of compressive sensing. We propose to learn autoencoder with tensor ring factorization on the the embedding space to impose structural constraints on the data. In particular, we use a tensor ring structure in the bottleneck layer of the autoencoder that utilizes the soft labels of the structured dataset. We empirically demonstrate the effectiveness of the proposed approach for inpainting and denoising applications. The resulting method achieves better reconstruction quality compared to other generative prior-based self-supervised recovery approaches for compressive sensing.



### Do we need entire training data for adversarial training?
- **Arxiv ID**: http://arxiv.org/abs/2303.06241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.06241v2)
- **Published**: 2023-03-10 23:21:05+00:00
- **Updated**: 2023-04-05 00:07:46+00:00
- **Authors**: Vipul Gupta, Apurva Narayan
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.   We perform adversarial training on the adversarially-prone subset and mix it with vanilla training performed on the entire dataset. Our results show that when our method-agnostic approach is plugged into FGSM, we achieve a speedup of 3.52x on MNIST and 1.98x on the CIFAR-10 dataset with comparable robust accuracy. We also test our approach on state-of-the-art Free adversarial training and achieve a speedup of 1.2x in training time with a marginal drop in robust accuracy on the ImageNet dataset.



### HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations
- **Arxiv ID**: http://arxiv.org/abs/2303.06242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.06242v1)
- **Published**: 2023-03-10 23:22:41+00:00
- **Updated**: 2023-03-10 23:22:41+00:00
- **Authors**: Luca Franco, Paolo Mandica, Bharti Munjal, Fabio Galasso
- **Comment**: Accepted at ICLR 2023
- **Journal**: None
- **Summary**: Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the established Euclidean SSL framework counterparts. When tested on three established skeleton-based action recognition datasets, HYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3 downstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive pairs and bypasses therefore the complex and computationally-demanding mining procedures required for the negatives in contrastive techniques. Code is available at https://github.com/paolomandica/HYSP.



