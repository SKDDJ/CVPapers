# Arxiv Papers in cs.CV on 2023-03-03
### Feature Completion Transformer for Occluded Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2303.01656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01656v1)
- **Published**: 2023-03-03 01:12:57+00:00
- **Updated**: 2023-03-03 01:12:57+00:00
- **Authors**: Tao Wang, Hong Liu, Wenhao Li, Miaoju Ban, Tuanyu Guo, Yidi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Occluded person re-identification (Re-ID) is a challenging problem due to the destruction of occluders. Most existing methods focus on visible human body parts through some prior information. However, when complementary occlusions occur, features in occluded regions can interfere with matching, which affects performance severely. In this paper, different from most previous works that discard the occluded region, we propose a Feature Completion Transformer (FCFormer) to implicitly complement the semantic information of occluded parts in the feature space. Specifically, Occlusion Instance Augmentation (OIA) is proposed to simulates real and diverse occlusion situations on the holistic image. These augmented images not only enrich the amount of occlusion samples in the training set, but also form pairs with the holistic images. Subsequently, a dual-stream architecture with a shared encoder is proposed to learn paired discriminative features from pairs of inputs. Without additional semantic information, an occluded-holistic feature sample-label pair can be automatically created. Then, Feature Completion Decoder (FCD) is designed to complement the features of occluded regions by using learnable tokens to aggregate possible information from self-generated occluded features. Finally, we propose the Cross Hard Triplet (CHT) loss to further bridge the gap between complementing features and extracting features under the same ID. In addition, Feature Completion Consistency (FC$^2$) loss is introduced to help the generated completion feature distribution to be closer to the real holistic feature distribution. Extensive experiments over five challenging datasets demonstrate that the proposed FCFormer achieves superior performance and outperforms the state-of-the-art methods by significant margins on occluded datasets.



### Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems
- **Arxiv ID**: http://arxiv.org/abs/2303.01669v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.01669v2)
- **Published**: 2023-03-03 02:07:40+00:00
- **Updated**: 2023-07-27 06:40:49+00:00
- **Authors**: Yangyang Shu, Anton van den Hengel, Lingqiao Liu
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) strategies have demonstrated remarkable performance in various recognition tasks. However, both our preliminary investigation and recent studies suggest that they may be less effective in learning representations for fine-grained visual recognition (FGVR) since many features helpful for optimizing SSL objectives are not suitable for characterizing the subtle differences in FGVR. To overcome this issue, we propose learning an additional screening mechanism to identify discriminative clues commonly seen across instances and classes, dubbed as common rationales in this paper. Intuitively, common rationales tend to correspond to the discriminative patterns from the key parts of foreground objects. We show that a common rationale detector can be learned by simply exploiting the GradCAM induced from the SSL objective without using any pre-trained object parts or saliency detectors, making it seamlessly to be integrated with the existing SSL process. Specifically, we fit the GradCAM with a branch with limited fitting capacity, which allows the branch to capture the common rationales and discard the less common discriminative patterns. At the test stage, the branch generates a set of spatial weights to selectively aggregate features representing an instance. Extensive experimental results on four visual tasks demonstrate that the proposed method can lead to a significant improvement in different evaluation settings.



### Nonlinear ill-posed problem in low-dose dental cone-beam computed tomography
- **Arxiv ID**: http://arxiv.org/abs/2303.01678v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2303.01678v1)
- **Published**: 2023-03-03 02:46:15+00:00
- **Updated**: 2023-03-03 02:46:15+00:00
- **Authors**: Hyoung Suk Park, Chang Min Hyun, Jin Keun Seo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the mathematical structure of the ill-posed nonlinear inverse problem of low-dose dental cone-beam computed tomography (CBCT) and explains the advantages of a deep learning-based approach to the reconstruction of computed tomography images over conventional regularization methods. This paper explains the underlying reasons why dental CBCT is more ill-posed than standard computed tomography. Despite this severe ill-posedness, the demand for dental CBCT systems is rapidly growing because of their cost competitiveness and low radiation dose. We then describe the limitations of existing methods in the accurate restoration of the morphological structures of teeth using dental CBCT data severely damaged by metal implants. We further discuss the usefulness of panoramic images generated from CBCT data for accurate tooth segmentation. We also discuss the possibility of utilizing radiation-free intra-oral scan data as prior information in CBCT image reconstruction to compensate for the damage to data caused by metal implants.



### Dense Pixel-to-Pixel Harmonization via Continuous Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2303.01681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01681v1)
- **Published**: 2023-03-03 02:52:28+00:00
- **Updated**: 2023-03-03 02:52:28+00:00
- **Authors**: Jianqi Chen, Yilan Zhang, Zhengxia Zou, Keyan Chen, Zhenwei Shi
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution (HR) image harmonization is of great significance in real-world applications such as image synthesis and image editing. However, due to the high memory costs, existing dense pixel-to-pixel harmonization methods are mainly focusing on processing low-resolution (LR) images. Some recent works resort to combining with color-to-color transformations but are either limited to certain resolutions or heavily depend on hand-crafted image filters. In this work, we explore leveraging the implicit neural representation (INR) and propose a novel image Harmonization method based on Implicit neural Networks (HINet), which to the best of our knowledge, is the first dense pixel-to-pixel method applicable to HR images without any hand-crafted filter design. Inspired by the Retinex theory, we decouple the MLPs into two parts to respectively capture the content and environment of composite images. A Low-Resolution Image Prior (LRIP) network is designed to alleviate the Boundary Inconsistency problem, and we also propose new designs for the training and inference process. Extensive experiments have demonstrated the effectiveness of our method compared with state-of-the-art methods. Furthermore, some interesting and practical applications of the proposed method are explored. Our code will be available at https://github.com/WindVChen/INR-Harmonization.



### Multi-Scale Control Signal-Aware Transformer for Motion Synthesis without Phase
- **Arxiv ID**: http://arxiv.org/abs/2303.01685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.01685v1)
- **Published**: 2023-03-03 02:56:44+00:00
- **Updated**: 2023-03-03 02:56:44+00:00
- **Authors**: Lintao Wang, Kun Hu, Lei Bai, Yu Ding, Wanli Ouyang, Zhiyong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing controllable motion for a character using deep learning has been a promising approach due to its potential to learn a compact model without laborious feature engineering. To produce dynamic motion from weak control signals such as desired paths, existing methods often require auxiliary information such as phases for alleviating motion ambiguity, which limits their generalisation capability. As past poses often contain useful auxiliary hints, in this paper, we propose a task-agnostic deep learning method, namely Multi-scale Control Signal-aware Transformer (MCS-T), with an attention based encoder-decoder architecture to discover the auxiliary information implicitly for synthesizing controllable motion without explicitly requiring auxiliary information such as phase. Specifically, an encoder is devised to adaptively formulate the motion patterns of a character's past poses with multi-scale skeletons, and a decoder driven by control signals to further synthesize and predict the character's state by paying context-specialised attention to the encoded past motion patterns. As a result, it helps alleviate the issues of low responsiveness and slow transition which often happen in conventional methods not using auxiliary information. Both qualitative and quantitative experimental results on an existing biped locomotion dataset, which involves diverse types of motion transitions, demonstrate the effectiveness of our method. In particular, MCS-T is able to successfully generate motions comparable to those generated by the methods using auxiliary information.



### Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View
- **Arxiv ID**: http://arxiv.org/abs/2303.01686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01686v1)
- **Published**: 2023-03-03 02:59:13+00:00
- **Updated**: 2023-03-03 02:59:13+00:00
- **Authors**: Shuo Wang, Xinhai Zhao, Hai-Ming Xu, Zehui Chen, Dameng Yu, Jiahao Chang, Zhen Yang, Feng Zhao
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Multi-view 3D object detection (MV3D-Det) in Bird-Eye-View (BEV) has drawn extensive attention due to its low cost and high efficiency. Although new algorithms for camera-only 3D object detection have been continuously proposed, most of them may risk drastic performance degradation when the domain of input images differs from that of training. In this paper, we first analyze the causes of the domain gap for the MV3D-Det task. Based on the covariate shift assumption, we find that the gap mainly attributes to the feature distribution of BEV, which is determined by the quality of both depth estimation and 2D image's feature representation. To acquire a robust depth prediction, we propose to decouple the depth estimation from the intrinsic parameters of the camera (i.e. the focal length) through converting the prediction of metric depth to that of scale-invariant depth and perform dynamic perspective augmentation to increase the diversity of the extrinsic parameters (i.e. the camera poses) by utilizing homography. Moreover, we modify the focal length values to create multiple pseudo-domains and construct an adversarial training loss to encourage the feature representation to be more domain-agnostic. Without bells and whistles, our approach, namely DG-BEV, successfully alleviates the performance drop on the unseen target domain without impairing the accuracy of the source domain. Extensive experiments on various public datasets, including Waymo, nuScenes, and Lyft, demonstrate the generalization and effectiveness of our approach. To the best of our knowledge, this is the first systematic study to explore a domain generalization method for MV3D-Det.



### Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2303.01687v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01687v1)
- **Published**: 2023-03-03 03:00:49+00:00
- **Updated**: 2023-03-03 03:00:49+00:00
- **Authors**: Yilin Yang, Kamil Adamczewski, Danica J. Sutherland, Xiaoxiao Li, Mijung Park
- **Comment**: None
- **Journal**: None
- **Summary**: Maximum mean discrepancy (MMD) is a particularly useful distance metric for differentially private data generation: when used with finite-dimensional features it allows us to summarize and privatize the data distribution once, which we can repeatedly use during generator training without further privacy loss. An important question in this framework is, then, what features are useful to distinguish between real and synthetic data distributions, and whether those enable us to generate quality synthetic data. This work considers the using the features of $\textit{neural tangent kernels (NTKs)}$, more precisely $\textit{empirical}$ NTKs (e-NTKs). We find that, perhaps surprisingly, the expressiveness of the untrained e-NTK features is comparable to that of the features taken from pre-trained perceptual features using public data. As a result, our method improves the privacy-accuracy trade-off compared to other state-of-the-art methods, without relying on any public data, as demonstrated on several tabular and image benchmark datasets.



### Spatio-Temporal Structure Consistency for Semi-supervised Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.01707v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01707v1)
- **Published**: 2023-03-03 04:18:09+00:00
- **Updated**: 2023-03-03 04:18:09+00:00
- **Authors**: Wentao Lei, Lei Liu, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent medical diagnosis has shown remarkable progress based on the large-scale datasets with precise annotations. However, fewer labeled images are available due to significantly expensive cost for annotating data by experts. To fully exploit the easily available unlabeled data, we propose a novel Spatio-Temporal Structure Consistent (STSC) learning framework. Specifically, a gram matrix is derived to combine the spatial structure consistency and temporal structure consistency together. This gram matrix captures the structural similarity among the representations of different training samples. At the spatial level, our framework explicitly enforces the consistency of structural similarity among different samples under perturbations. At the temporal level, we consider the consistency of the structural similarity in different training iterations by digging out the stable sub-structures in a relation graph. Experiments on two medical image datasets (i.e., ISIC 2018 challenge and ChestX-ray14) show that our method outperforms state-of-the-art SSL methods. Furthermore, extensive qualitative analysis on the Gram matrices and heatmaps by Grad-CAM are presented to validate the effectiveness of our method.



### BayeSeg: Bayesian Modeling for Medical Image Segmentation with Interpretable Generalizability
- **Arxiv ID**: http://arxiv.org/abs/2303.01710v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2303.01710v1)
- **Published**: 2023-03-03 04:48:37+00:00
- **Updated**: 2023-03-03 04:48:37+00:00
- **Authors**: Shangqi Gao, Hangqi Zhou, Yibo Gao, Xiahai Zhuang
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Due to the cross-domain distribution shift aroused from diverse medical imaging systems, many deep learning segmentation methods fail to perform well on unseen data, which limits their real-world applicability. Recent works have shown the benefits of extracting domain-invariant representations on domain generalization. However, the interpretability of domain-invariant features remains a great challenge. To address this problem, we propose an interpretable Bayesian framework (BayeSeg) through Bayesian modeling of image and label statistics to enhance model generalizability for medical image segmentation. Specifically, we first decompose an image into a spatial-correlated variable and a spatial-variant variable, assigning hierarchical Bayesian priors to explicitly force them to model the domain-stable shape and domain-specific appearance information respectively. Then, we model the segmentation as a locally smooth variable only related to the shape. Finally, we develop a variational Bayesian framework to infer the posterior distributions of these explainable variables. The framework is implemented with neural networks, and thus is referred to as deep Bayesian segmentation. Quantitative and qualitative experimental results on prostate segmentation and cardiac segmentation tasks have shown the effectiveness of our proposed method. Moreover, we investigated the interpretability of BayeSeg by explaining the posteriors and analyzed certain factors that affect the generalization ability through further ablation studies. Our code will be released via https://zmiclab.github.io/projects.html, once the manuscript is accepted for publication.



### One-class Damage Detector Using Deeper Fully-Convolutional Data Descriptions for Civil Application
- **Arxiv ID**: http://arxiv.org/abs/2303.01732v3
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2303.01732v3)
- **Published**: 2023-03-03 06:27:15+00:00
- **Updated**: 2023-05-08 16:12:44+00:00
- **Authors**: Takato Yasuno, Masahiro Okano, Junichiro Fujii
- **Comment**: 11 pages, 9 figures, 3 tables
- **Journal**: Adv. Artif. Intell. Mach. Learn., 3 (2):996-1011 (Published On:
  02-May-2023)
- **Summary**: Infrastructure managers must maintain high standards to ensure user satisfaction during the lifecycle of infrastructures. Surveillance cameras and visual inspections have enabled progress in automating the detection of anomalous features and assessing the occurrence of deterioration. However, collecting damage data is typically time consuming and requires repeated inspections. The one-class damage detection approach has an advantage in that normal images can be used to optimize model parameters. Additionally, visual evaluation of heatmaps enables us to understand localized anomalous features. The authors highlight damage vision applications utilized in the robust property and localized damage explainability. First, we propose a civil-purpose application for automating one-class damage detection reproducing a fully convolutional data description (FCDD) as a baseline model. We have obtained accurate and explainable results demonstrating experimental studies on concrete damage and steel corrosion in civil engineering. Additionally, to develop a more robust application, we applied our method to another outdoor domain that contains complex and noisy backgrounds using natural disaster datasets collected using various devices. Furthermore, we propose a valuable solution of deeper FCDDs focusing on other powerful backbones to improve the performance of damage detection and implement ablation studies on disaster datasets. The key results indicate that the deeper FCDDs outperformed the baseline FCDD on datasets representing natural disaster damage caused by hurricanes, typhoons, earthquakes, and four-event disasters.



### AdvART: Adversarial Art for Camouflaged Object Detection Attacks
- **Arxiv ID**: http://arxiv.org/abs/2303.01734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2303.01734v1)
- **Published**: 2023-03-03 06:28:05+00:00
- **Updated**: 2023-03-03 06:28:05+00:00
- **Authors**: Amira Guesmi, Ioan Marius Bilasco, Muhammad Shafique, Ihsen Alouani
- **Comment**: None
- **Journal**: None
- **Summary**: A majority of existing physical attacks in the real world result in conspicuous and eye-catching patterns for generated patches, which made them identifiable/detectable by humans. To overcome this limitation, recent work has proposed several approaches that aim at generating naturalistic patches using generative adversarial networks (GANs), which may not catch human's attention. However, these approaches are computationally intensive and do not always converge to natural looking patterns. In this paper, we propose a novel lightweight framework that systematically generates naturalistic adversarial patches without using GANs. To illustrate the proposed approach, we generate adversarial art (AdvART), which are patches generated to look like artistic paintings while maintaining high attack efficiency. In fact, we redefine the optimization problem by introducing a new similarity objective. Specifically, we leverage similarity metrics to construct a similarity loss that is added to the optimized objective function. This component guides the patch to follow a predefined artistic patterns while maximizing the victim model's loss function. Our patch achieves high success rates with $12.53\%$ mean average precision (mAP) on YOLOv4tiny for INRIA dataset.



### Multi-Plane Neural Radiance Fields for Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2303.01736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01736v1)
- **Published**: 2023-03-03 06:32:55+00:00
- **Updated**: 2023-03-03 06:32:55+00:00
- **Authors**: Youssef Abdelkareem, Shady Shehata, Fakhri Karray
- **Comment**: ICDIPV 2023
- **Journal**: None
- **Summary**: Novel view synthesis is a long-standing problem that revolves around rendering frames of scenes from novel camera viewpoints. Volumetric approaches provide a solution for modeling occlusions through the explicit 3D representation of the camera frustum. Multi-plane Images (MPI) are volumetric methods that represent the scene using front-parallel planes at distinct depths but suffer from depth discretization leading to a 2.D scene representation. Another line of approach relies on implicit 3D scene representations. Neural Radiance Fields (NeRF) utilize neural networks for encapsulating the continuous 3D scene structure within the network weights achieving photorealistic synthesis results, however, methods are constrained to per-scene optimization settings which are inefficient in practice. Multi-plane Neural Radiance Fields (MINE) open the door for combining implicit and explicit scene representations. It enables continuous 3D scene representations, especially in the depth dimension, while utilizing the input image features to avoid per-scene optimization. The main drawback of the current literature work in this domain is being constrained to single-view input, limiting the synthesis ability to narrow viewpoint ranges. In this work, we thoroughly examine the performance, generalization, and efficiency of single-view multi-plane neural radiance fields. In addition, we propose a new multiplane NeRF architecture that accepts multiple views to improve the synthesis results and expand the viewing range. Features from the input source frames are effectively fused through a proposed attention-aware fusion module to highlight important information from different viewpoints. Experiments show the effectiveness of attention-based fusion and the promising outcomes of our proposed method when compared to multi-view NeRF and MPI techniques.



### Mover: Mask and Recovery based Facial Part Consistency Aware Method for Deepfake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.01740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.01740v2)
- **Published**: 2023-03-03 06:57:22+00:00
- **Updated**: 2023-05-06 02:23:25+00:00
- **Authors**: Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake techniques have been widely used for malicious purposes, prompting extensive research interest in developing Deepfake detection methods. Deepfake manipulations typically involve tampering with facial parts, which can result in inconsistencies across different parts of the face. For instance, Deepfake techniques may change smiling lips to an upset lip, while the eyes remain smiling. Existing detection methods depend on specific indicators of forgery, which tend to disappear as the forgery patterns are improved. To address the limitation, we propose Mover, a new Deepfake detection model that exploits unspecific facial part inconsistencies, which are inevitable weaknesses of Deepfake videos. Mover randomly masks regions of interest (ROIs) and recovers faces to learn unspecific features, which makes it difficult for fake faces to be recovered, while real faces can be easily recovered. Specifically, given a real face image, we first pretrain a masked autoencoder to learn facial part consistency by dividing faces into three parts and randomly masking ROIs, which are then recovered based on the unmasked facial parts. Furthermore, to maximize the discrepancy between real and fake videos, we propose a novel model with dual networks that utilize the pretrained encoder and masked autoencoder, respectively. 1) The pretrained encoder is finetuned for capturing the encoding of inconsistent information in the given video. 2) The pretrained masked autoencoder is utilized for mapping faces and distinguishing real and fake videos. Our extensive experiments on standard benchmarks demonstrate that Mover is highly effective.



### A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2303.01743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01743v1)
- **Published**: 2023-03-03 07:10:02+00:00
- **Updated**: 2023-03-03 07:10:02+00:00
- **Authors**: Yingda Yin, Yang Wang, He Wang, Baoquan Chen
- **Comment**: ICLR 2023 Spotlight (notable-top-25%)
- **Journal**: None
- **Summary**: Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. Probabilistic rotation regression has raised more and more attention with the benefit of expressing uncertainty information along with the prediction. Though modeling noise using Gaussian-resembling Bingham distribution and matrix Fisher distribution is natural, they are shown to be sensitive to outliers for the nature of quadratic punishment to deviations. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region, resulting in a better convergence. Our extensive experiments show that our proposed distribution achieves state-of-the-art performance for rotation regression tasks over both probabilistic and non-probabilistic baselines. Our project page is at https://pku-epic.github.io/RotationLaplace.



### Generative Diffusions in Augmented Spaces: A Complete Recipe
- **Arxiv ID**: http://arxiv.org/abs/2303.01748v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2303.01748v1)
- **Published**: 2023-03-03 07:20:58+00:00
- **Updated**: 2023-03-03 07:20:58+00:00
- **Authors**: Kushagra Pandey, Stephan Mandt
- **Comment**: None
- **Journal**: None
- **Summary**: Score-based Generative Models (SGMs) have achieved state-of-the-art synthesis results on diverse tasks. However, the current design space of the forward diffusion process is largely unexplored and often relies on physical intuition or simplifying assumptions. Leveraging results from the design of scalable Bayesian posterior samplers, we present a complete recipe for constructing forward processes in SGMs, all of which are guaranteed to converge to the target distribution of interest. We show that several existing SGMs can be cast as specific instantiations of this parameterization. Furthermore, building on this recipe, we construct a novel SGM: Phase Space Langevin Diffusion (PSLD), which performs score-based modeling in a space augmented with auxiliary variables akin to a physical phase space. We show that PSLD outperforms competing baselines in terms of sample quality and the speed-vs-quality tradeoff across different samplers on various standard image synthesis benchmarks. Moreover, we show that PSLD achieves sample quality comparable to state-of-the-art SGMs (FID: 2.10 on unconditional CIFAR-10 generation), providing an attractive alternative as an SGM backbone for further development. We will publish our code and model checkpoints for reproducibility at https://github.com/mandt-lab/PSLD.



### Area of interest adaption using feature importance
- **Arxiv ID**: http://arxiv.org/abs/2303.12744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.12744v1)
- **Published**: 2023-03-03 07:49:10+00:00
- **Updated**: 2023-03-03 07:49:10+00:00
- **Authors**: Wolfgang Fuhl, Susanne Zabel, Theresa Harbig, Julia Astrid Moldt, Teresa Festl Wiete, Anne Herrmann Werner, Kay Nieselt
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present two approaches and algorithms that adapt areas of interest (AOI) or regions of interest (ROI), respectively, to the eye tracking data quality and classification task. The first approach uses feature importance in a greedy way and grows or shrinks AOIs in all directions. The second approach is an extension of the first approach, which divides the AOIs into areas and calculates a direction of growth, i.e. a gradient. Both approaches improve the classification results considerably in the case of generalized AOIs, but can also be used for qualitative analysis. In qualitative analysis, the algorithms presented allow the AOIs to be adapted to the data, which means that errors and inaccuracies in eye tracking data can be better compensated for. A good application example is abstract art, where manual AOIs annotation is hardly possible, and data-driven approaches are mainly used for initial AOIs.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FAOIGradient&mode=list



### A temporally quantized distribution of pupil diameters as a new feature for cognitive load classification
- **Arxiv ID**: http://arxiv.org/abs/2303.12757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.12757v1)
- **Published**: 2023-03-03 07:52:16+00:00
- **Updated**: 2023-03-03 07:52:16+00:00
- **Authors**: Wolfgang Fuhl, Susanne Zabel, Theresa Harbig, Julia Astrid Moldt, Teresa Festl Wiete, Anne Herrmann Werner, Kay Nieselt
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new feature that can be used to classify cognitive load based on pupil information. The feature consists of a temporal segmentation of the eye tracking recordings. For each segment of the temporal partition, a probability distribution of pupil size is computed and stored. These probability distributions can then be used to classify the cognitive load. The presented feature significantly improves the classification accuracy of the cognitive load compared to other statistical values obtained from eye tracking data, which represent the state of the art in this field. The applications of determining Cognitive Load from pupil data are numerous and could lead, for example, to pre-warning systems for burnouts.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FCognitiveLoadFeature&mode=list



### Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2303.01765v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01765v2)
- **Published**: 2023-03-03 08:08:04+00:00
- **Updated**: 2023-03-21 02:50:41+00:00
- **Authors**: Xingqun Qi, Chen Liu, Muyi Sun, Lincheng Li, Changjie Fan, Xin Yu
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.



### Prior Information based Decomposition and Reconstruction Learning for Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.01776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01776v1)
- **Published**: 2023-03-03 08:34:28+00:00
- **Updated**: 2023-03-03 08:34:28+00:00
- **Authors**: Jinsheng Wei, Haoyu Chen, Guanming Lu, Jingjie Yan, Yue Xie, Guoying Zhao
- **Comment**: The article has been accepted by IEICE TRANS. Information and Systems
- **Journal**: None
- **Summary**: Micro-expression recognition (MER) draws intensive research interest as micro-expressions (MEs) can infer genuine emotions. Prior information can guide the model to learn discriminative ME features effectively. However, most works focus on researching the general models with a stronger representation ability to adaptively aggregate ME movement information in a holistic way, which may ignore the prior information and properties of MEs. To solve this issue, driven by the prior information that the category of ME can be inferred by the relationship between the actions of facial different components, this work designs a novel model that can conform to this prior information and learn ME movement features in an interpretable way. Specifically, this paper proposes a Decomposition and Reconstruction-based Graph Representation Learning (DeRe-GRL) model to effectively learn high-level ME features. DeRe-GRL includes two modules: Action Decomposition Module (ADM) and Relation Reconstruction Module (RRM), where ADM learns action features of facial key components and RRM explores the relationship between these action features. Based on facial key components, ADM divides the geometric movement features extracted by the graph model-based backbone into several sub-features, and learns the map matrix to map these sub-features into multiple action features; then, RRM learns weights to weight all action features to build the relationship between action features. The experimental results demonstrate the effectiveness of the proposed modules, and the proposed method achieves competitive performance.



### Benchmarking White Blood Cell Classification Under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2303.01777v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01777v2)
- **Published**: 2023-03-03 08:36:19+00:00
- **Updated**: 2023-05-19 17:52:34+00:00
- **Authors**: Satoshi Tsutsui, Zhengyang Su, Bihan Wen
- **Comment**: Accepted to the International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2023. More datasets are cited
- **Journal**: None
- **Summary**: Recognizing the types of white blood cells (WBCs) in microscopic images of human blood smears is a fundamental task in the fields of pathology and hematology. Although previous studies have made significant contributions to the development of methods and datasets, few papers have investigated benchmarks or baselines that others can easily refer to. For instance, we observed notable variations in the reported accuracies of the same Convolutional Neural Network (CNN) model across different studies, yet no public implementation exists to reproduce these results. In this paper, we establish a benchmark for WBC recognition. Our results indicate that CNN-based models achieve high accuracy when trained and tested under similar imaging conditions. However, their performance drops significantly when tested under different conditions. Moreover, the ResNet classifier, which has been widely employed in previous work, exhibits an unreasonably poor generalization ability under domain shifts due to batch normalization. We investigate this issue and suggest some alternative normalization techniques that can mitigate it. We make fully-reproducible code publicly available\footnote{\url{https://github.com/apple2373/wbc-benchmark}}.



### Meme Sentiment Analysis Enhanced with Multimodal Spatial Encoding and Facial Embedding
- **Arxiv ID**: http://arxiv.org/abs/2303.01781v1
- **DOI**: 10.1007/978-3-031-26438-2_25
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2303.01781v1)
- **Published**: 2023-03-03 08:44:20+00:00
- **Updated**: 2023-03-03 08:44:20+00:00
- **Authors**: Muzhaffar Hazman, Susan McKeever, Josephine Griffith
- **Comment**: Published as chapter in ISBN:978-3-031-26438-2
- **Journal**: In: Longo, L., OReilly, R. (eds) Artificial Intelligence and
  Cognitive Science. AICS 2022. Communications in Computer and Information
  Science, vol 1662. Springer, Cham
- **Summary**: Internet memes are characterised by the interspersing of text amongst visual elements. State-of-the-art multimodal meme classifiers do not account for the relative positions of these elements across the two modalities, despite the latent meaning associated with where text and visual elements are placed. Against two meme sentiment classification datasets, we systematically show performance gains from incorporating the spatial position of visual objects, faces, and text clusters extracted from memes. In addition, we also present facial embedding as an impactful enhancement to image representation in a multimodal meme classifier. Finally, we show that incorporating this spatial information allows our fully automated approaches to outperform their corresponding baselines that rely on additional human validation of OCR-extracted text.



### 3D Multi-Object Tracking Based on Uncertainty-Guided Data Association
- **Arxiv ID**: http://arxiv.org/abs/2303.01786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01786v1)
- **Published**: 2023-03-03 08:53:00+00:00
- **Updated**: 2023-03-03 08:53:00+00:00
- **Authors**: Jiawei He, Chunyun Fu, Xiyang Wang
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: In the existing literature, most 3D multi-object tracking algorithms based on the tracking-by-detection framework employed deterministic tracks and detections for similarity calculation in the data association stage. Namely, the inherent uncertainties existing in tracks and detections are overlooked. In this work, we discard the commonly used deterministic tracks and deterministic detections for data association, instead, we propose to model tracks and detections as random vectors in which uncertainties are taken into account. Then, based on the Jensen-Shannon divergence, the similarity between two multidimensional distributions, i.e. track and detection, is evaluated for data association purposes. Lastly, the level of track uncertainty is incorporated in our cost function design to guide the data association process. Comparative experiments have been conducted on two typical datasets, KITTI and nuScenes, and the results indicated that our proposed method outperformed the compared state-of-the-art 3D tracking algorithms. For the benefit of the community, our code has been made available at https://github.com/hejiawei2023/UG3DMOT.



### Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2303.01788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01788v1)
- **Published**: 2023-03-03 08:54:06+00:00
- **Updated**: 2023-03-03 08:54:06+00:00
- **Authors**: Xiwen Liang, Minzhe Niu, Jianhua Han, Hang Xu, Chunjing Xu, Xiaodan Liang
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Multi-task learning has emerged as a powerful paradigm to solve a range of tasks simultaneously with good efficiency in both computation resources and inference time. However, these algorithms are designed for different tasks mostly not within the scope of autonomous driving, thus making it hard to compare multi-task methods in autonomous driving. Aiming to enable the comprehensive evaluation of present multi-task learning methods in autonomous driving, we extensively investigate the performance of popular multi-task methods on the large-scale driving dataset, which covers four common perception tasks, i.e., object detection, semantic segmentation, drivable area segmentation, and lane detection. We provide an in-depth analysis of current multi-task learning methods under different common settings and find out that the existing methods make progress but there is still a large performance gap compared with single-task baselines. To alleviate this dilemma in autonomous driving, we present an effective multi-task framework, VE-Prompt, which introduces visual exemplars via task-specific prompting to guide the model toward learning high-quality task-specific representations. Specifically, we generate visual exemplars based on bounding boxes and color-based markers, which provide accurate visual appearances of target categories and further mitigate the performance gap. Furthermore, we bridge transformer-based encoders and convolutional layers for efficient and accurate unified perception in autonomous driving. Comprehensive experimental results on the diverse self-driving dataset BDD100K show that the VE-Prompt improves the multi-task baseline and further surpasses single-task models.



### Confidence-driven Bounding Box Localization for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.01803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01803v1)
- **Published**: 2023-03-03 09:19:08+00:00
- **Updated**: 2023-03-03 09:19:08+00:00
- **Authors**: Huixin Sun, Baochang Zhang, Yanjing Li, Xianbin Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Despite advancements in generic object detection, there remains a performance gap in detecting small objects compared to normal-scale objects. We for the first time observe that existing bounding box regression methods tend to produce distorted gradients for small objects and result in less accurate localization. To address this issue, we present a novel Confidence-driven Bounding Box Localization (C-BBL) method to rectify the gradients. C-BBL quantizes continuous labels into grids and formulates two-hot ground truth labels. In prediction, the bounding box head generates a confidence distribution over the grids. Unlike the bounding box regression paradigms in conventional detectors, we introduce a classification-based localization objective through cross entropy between ground truth and predicted confidence distribution, generating confidence-driven gradients. Additionally, C-BBL describes a uncertainty loss based on distribution entropy in labels and predictions to further reduce the uncertainty in small object localization. The method is evaluated on multiple detectors using three object detection benchmarks and consistently improves baseline detectors, achieving state-of-the-art performance. We also demonstrate the generalizability of C-BBL to different label systems and effectiveness for high resolution detection, which validates its prospect as a general solution.



### Are All Point Clouds Suitable for Completion? Weakly Supervised Quality Evaluation Network for Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2303.01804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.01804v1)
- **Published**: 2023-03-03 09:24:29+00:00
- **Updated**: 2023-03-03 09:24:29+00:00
- **Authors**: Jieqi Shi, Peiliang Li, Xiaozhi Chen, Shaojie Shen
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: In the practical application of point cloud completion tasks, real data quality is usually much worse than the CAD datasets used for training. A small amount of noisy data will usually significantly impact the overall system's accuracy. In this paper, we propose a quality evaluation network to score the point clouds and help judge the quality of the point cloud before applying the completion model. We believe our scoring method can help researchers select more appropriate point clouds for subsequent completion and reconstruction and avoid manual parameter adjustment. Moreover, our evaluation model is fast and straightforward and can be directly inserted into any model's training or use process to facilitate the automatic selection and post-processing of point clouds. We propose a complete dataset construction and model evaluation method based on ShapeNet. We verify our network using detection and flow estimation tasks on KITTI, a real-world dataset for autonomous driving. The experimental results show that our model can effectively distinguish the quality of point clouds and help in practical tasks.



### When does Privileged Information Explain Away Label Noise?
- **Arxiv ID**: http://arxiv.org/abs/2303.01806v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01806v2)
- **Published**: 2023-03-03 09:25:39+00:00
- **Updated**: 2023-06-01 08:00:42+00:00
- **Authors**: Guillermo Ortiz-Jimenez, Mark Collier, Anant Nawalgaria, Alexander D'Amour, Jesse Berent, Rodolphe Jenatton, Effrosyni Kokiopoulou
- **Comment**: Accepted ICML 2023, Honolulu
- **Journal**: None
- **Summary**: Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches with existing no-PI techniques designed to deal with label noise.



### Word-As-Image for Semantic Typography
- **Arxiv ID**: http://arxiv.org/abs/2303.01818v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.01818v2)
- **Published**: 2023-03-03 09:59:25+00:00
- **Updated**: 2023-03-06 16:34:15+00:00
- **Authors**: Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir
- **Comment**: None
- **Journal**: None
- **Summary**: A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques.



### Exploring Machine Learning Privacy/Utility trade-off from a hyperparameters Lens
- **Arxiv ID**: http://arxiv.org/abs/2303.01819v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01819v1)
- **Published**: 2023-03-03 09:59:42+00:00
- **Updated**: 2023-03-03 09:59:42+00:00
- **Authors**: Ayoub Arous, Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique
- **Comment**: None
- **Journal**: None
- **Summary**: Machine Learning (ML) architectures have been applied to several applications that involve sensitive data, where a guarantee of users' data privacy is required. Differentially Private Stochastic Gradient Descent (DPSGD) is the state-of-the-art method to train privacy-preserving models. However, DPSGD comes at a considerable accuracy loss leading to sub-optimal privacy/utility trade-offs. Towards investigating new ground for better privacy-utility trade-off, this work questions; (i) if models' hyperparameters have any inherent impact on ML models' privacy-preserving properties, and (ii) if models' hyperparameters have any impact on the privacy/utility trade-off of differentially private models. We propose a comprehensive design space exploration of different hyperparameters such as the choice of activation functions, the learning rate and the use of batch normalization. Interestingly, we found that utility can be improved by using Bounded RELU as activation functions with the same privacy-preserving characteristics. With a drop-in replacement of the activation function, we achieve new state-of-the-art accuracy on MNIST (96.02\%), FashionMnist (84.76\%), and CIFAR-10 (44.42\%) without any modification of the learning procedure fundamentals of DPSGD.



### A Hybrid Approach to Full-Scale Reconstruction of Renal Arterial Network
- **Arxiv ID**: http://arxiv.org/abs/2303.01837v1
- **DOI**: 10.1038/s41598-023-34739-y
- **Categories**: **cs.CE**, cs.CV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2303.01837v1)
- **Published**: 2023-03-03 10:39:25+00:00
- **Updated**: 2023-03-03 10:39:25+00:00
- **Authors**: Peidi Xu, Niels-Henrik Holstein-Rathlou, Stinne Byrholdt Søgaard, Carsten Gundlach, Charlotte Mehlin Sørensen, Kenny Erleben, Olga Sosnovtseva, Sune Darkner
- **Comment**: 19 pages, 5 figures (excluding references and supplementary)
  submitted to Communications Biology
- **Journal**: Sci Rep 13, 7569 (2023)
- **Summary**: The renal vasculature, acting as a resource distribution network, plays an important role in both the physiology and pathophysiology of the kidney. However, no imaging techniques allow an assessment of the structure and function of the renal vasculature due to limited spatial and temporal resolution. To develop realistic computer simulations of renal function, and to develop new image-based diagnostic methods based on artificial intelligence, it is necessary to have a realistic full-scale model of the renal vasculature. We propose a hybrid framework to build subject-specific models of the renal vascular network by using semi-automated segmentation of large arteries and estimation of cortex area from a micro-CT scan as a starting point, and by adopting the Global Constructive Optimization algorithm for generating smaller vessels. Our results show a statistical correspondence between the reconstructed data and existing anatomical data obtained from a rat kidney with respect to morphometric and hemodynamic parameters.



### Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models
- **Arxiv ID**: http://arxiv.org/abs/2303.01869v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01869v3)
- **Published**: 2023-03-03 11:52:21+00:00
- **Updated**: 2023-04-09 12:56:44+00:00
- **Authors**: Qu Tang, XiangYu Zhu, Zhen Lei, ZhaoXiang Zhang
- **Comment**: Accepted to Computer Vision and Pattern Recognition (CVPR)2023
- **Journal**: None
- **Summary**: The ability to discover abstract physical concepts and understand how they work in the world through observing lies at the core of human intelligence. The acquisition of this ability is based on compositionally perceiving the environment in terms of objects and relations in an unsupervised manner. Recent approaches learn object-centric representations and capture visually observable concepts of objects, e.g., shape, size, and location. In this paper, we take a step forward and try to discover and represent intrinsic physical concepts such as mass and charge. We introduce the PHYsical Concepts Inference NEtwork (PHYCINE), a system that infers physical concepts in different abstract levels without supervision. The key insights underlining PHYCINE are two-fold, commonsense knowledge emerges with prediction, and physical concepts of different abstract levels should be reasoned in a bottom-up fashion. Empirical evaluation demonstrates that variables inferred by our system work in accordance with the properties of the corresponding physical concepts. We also show that object representations containing the discovered physical concepts variables could help achieve better performance in causal reasoning tasks, i.e., ComPhy.



### Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models
- **Arxiv ID**: http://arxiv.org/abs/2303.01870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.01870v1)
- **Published**: 2023-03-03 11:53:01+00:00
- **Updated**: 2023-03-03 11:53:01+00:00
- **Authors**: Naman D Singh, Francesco Croce, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\ell_\infty$-threat model, but even more so improve generalization to unseen $\ell_1/\ell_2$-robustness. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust models across different ranges of model parameters and FLOPs.



### Attention-based Saliency Maps Improve Interpretability of Pneumothorax Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.01871v1
- **DOI**: 10.1148/ryai.220187
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01871v1)
- **Published**: 2023-03-03 12:05:41+00:00
- **Updated**: 2023-03-03 12:05:41+00:00
- **Authors**: Alessandro Wollek, Robert Graf, Saša Čečatka, Nicola Fink, Theresa Willem, Bastian O. Sabel, Tobias Lasser
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To investigate chest radiograph (CXR) classification performance of vision transformers (ViT) and interpretability of attention-based saliency using the example of pneumothorax classification.   Materials and Methods: In this retrospective study, ViTs were fine-tuned for lung disease classification using four public data sets: CheXpert, Chest X-Ray 14, MIMIC CXR, and VinBigData. Saliency maps were generated using transformer multimodal explainability and gradient-weighted class activation mapping (GradCAM). Classification performance was evaluated on the Chest X-Ray 14, VinBigData, and SIIM-ACR data sets using the area under the receiver operating characteristic curve analysis (AUC) and compared with convolutional neural networks (CNNs). The explainability methods were evaluated with positive/negative perturbation, sensitivity-n, effective heat ratio, intra-architecture repeatability and interarchitecture reproducibility. In the user study, three radiologists classified 160 CXRs with/without saliency maps for pneumothorax and rated their usefulness.   Results: ViTs had comparable CXR classification AUCs compared with state-of-the-art CNNs 0.95 (95% CI: 0.943, 0.950) versus 0.83 (95%, CI 0.826, 0.842) on Chest X-Ray 14, 0.84 (95% CI: 0.769, 0.912) versus 0.83 (95% CI: 0.760, 0.895) on VinBigData, and 0.85 (95% CI: 0.847, 0.861) versus 0.87 (95% CI: 0.868, 0.882) on SIIM ACR. Both saliency map methods unveiled a strong bias toward pneumothorax tubes in the models. Radiologists found 47% of the attention-based saliency maps useful and 39% of GradCAM. The attention-based methods outperformed GradCAM on all metrics.   Conclusion: ViTs performed similarly to CNNs in CXR classification, and their attention-based saliency maps were more useful to radiologists and outperformed GradCAM.



### AutoMatch: A Large-scale Audio Beat Matching Benchmark for Boosting Deep Learning Assistant Video Editing
- **Arxiv ID**: http://arxiv.org/abs/2303.01884v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2303.01884v1)
- **Published**: 2023-03-03 12:30:09+00:00
- **Updated**: 2023-03-03 12:30:09+00:00
- **Authors**: Sen Pei, Jingya Yu, Qi Chen, Wozhou He
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: The explosion of short videos has dramatically reshaped the manners people socialize, yielding a new trend for daily sharing and access to the latest information. These rich video resources, on the one hand, benefited from the popularization of portable devices with cameras, but on the other, they can not be independent of the valuable editing work contributed by numerous video creators. In this paper, we investigate a novel and practical problem, namely audio beat matching (ABM), which aims to recommend the proper transition time stamps based on the background music. This technique helps to ease the labor-intensive work during video editing, saving energy for creators so that they can focus more on the creativity of video content. We formally define the ABM problem and its evaluation protocol. Meanwhile, a large-scale audio dataset, i.e., the AutoMatch with over 87k finely annotated background music, is presented to facilitate this newly opened research direction. To further lay solid foundations for the following study, we also propose a novel model termed BeatX to tackle this challenging task. Alongside, we creatively present the concept of label scope, which eliminates the data imbalance issues and assigns adaptive weights for the ground truth during the training procedure in one stop. Though plentiful short video platforms have flourished for a long time, the relevant research concerning this scenario is not sufficient, and to the best of our knowledge, AutoMatch is the first large-scale dataset to tackle the audio beat matching problem. We hope the released dataset and our competitive baseline can encourage more attention to this line of research. The dataset and codes will be made publicly available.



### TRR360D: A dataset for 360 degree rotated rectangular box table detection
- **Arxiv ID**: http://arxiv.org/abs/2303.01894v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01894v3)
- **Published**: 2023-03-03 12:47:30+00:00
- **Updated**: 2023-03-08 11:23:18+00:00
- **Authors**: Wenxing Hu, Minglei Tong
- **Comment**: None
- **Journal**: None
- **Summary**: To address the problem of scarcity and high annotation costs of rotated image table detection datasets, this paper proposes a method for building a rotated image table detection dataset. Based on the ICDAR2019MTD modern table detection dataset, we refer to the annotation format of the DOTA dataset to create the TRR360D rotated table detection dataset. The training set contains 600 rotated images and 977 annotated instances, and the test set contains 240 rotated images and 499 annotated instances. The AP50(T<90) evaluation metric is defined, and this dataset is available for future researchers to study rotated table detection algorithms and promote the development of table detection technology. The TRR360D rotated table detection dataset was created by constraining the starting point and annotation direction, and is publicly available at https://github.com/vansin/TRR360D.



### Quantifying the LiDAR Sim-to-Real Domain Shift: A Detailed Investigation Using Object Detectors and Analyzing Point Clouds at Target-Level
- **Arxiv ID**: http://arxiv.org/abs/2303.01899v1
- **DOI**: 10.1109/TIV.2023.3251650
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01899v1)
- **Published**: 2023-03-03 12:52:01+00:00
- **Updated**: 2023-03-03 12:52:01+00:00
- **Authors**: Sebastian Huch, Luca Scalerandi, Esteban Rivera, Markus Lienkamp
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, 2023
- **Summary**: LiDAR object detection algorithms based on neural networks for autonomous driving require large amounts of data for training, validation, and testing. As real-world data collection and labeling are time-consuming and expensive, simulation-based synthetic data generation is a viable alternative. However, using simulated data for the training of neural networks leads to a domain shift of training and testing data due to differences in scenes, scenarios, and distributions. In this work, we quantify the sim-to-real domain shift by means of LiDAR object detectors trained with a new scenario-identical real-world and simulated dataset. In addition, we answer the questions of how well the simulated data resembles the real-world data and how well object detectors trained on simulated data perform on real-world data. Further, we analyze point clouds at the target-level by comparing real-world and simulated point clouds within the 3D bounding boxes of the targets. Our experiments show that a significant sim-to-real domain shift exists even for our scenario-identical datasets. This domain shift amounts to an average precision reduction of around 14 % for object detectors trained with simulated data. Additional experiments reveal that this domain shift can be lowered by introducing a simple noise model in simulation. We further show that a simple downsampling method to model real-world physics does not influence the performance of the object detectors.



### Unfinished Architectures: A Perspective from Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2303.12732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.12732v1)
- **Published**: 2023-03-03 13:05:10+00:00
- **Updated**: 2023-03-03 13:05:10+00:00
- **Authors**: Elena Merino-Gómez, Pedro Reviriego, Fernando Moral
- **Comment**: Accepted for publication in EGA
  (https://polipapers.upv.es/index.php/EGA)
- **Journal**: None
- **Summary**: Unfinished buildings are a constant throughout the history of architecture and have given rise to intense debates on the opportuneness of their completion, in addition to offering alibis for theorizing about the compositional possibilities in coherence with the finished parts. The development of Artificial Intelligence (AI) opens new avenues for the proposal of possibilities for the completion of unfinished architectures. Specifically, with the recent appearance of tools such as DALL-E, capable of completing images guided by a textual description, it is possible to count on the help of AI for architectural design tasks. In this article we explore the use of these new AI tools for the completion of unfinished facades of historical temples and analyse the still germinal stadium in the field of architectural graphic composition.



### Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2303.01903v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.01903v2)
- **Published**: 2023-03-03 13:05:15+00:00
- **Updated**: 2023-03-16 01:49:29+00:00
- **Authors**: Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu
- **Comment**: Accepted to CVPR 2023, code available at
  https://github.com/MILVLG/prophet
- **Journal**: None
- **Summary**: Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are encoded into the prompts to enable GPT-3 to better comprehend the task thus enhancing its capacity. Prophet significantly outperforms all existing state-of-the-art methods on two challenging knowledge-based VQA datasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their testing sets, respectively.



### EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization
- **Arxiv ID**: http://arxiv.org/abs/2303.01904v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01904v4)
- **Published**: 2023-03-03 13:05:30+00:00
- **Updated**: 2023-05-23 05:33:02+00:00
- **Authors**: Junha Song, Jungsoo Lee, In So Kweon, Sungha Choi
- **Comment**: Accepted to CVPR 2023, Project page:
  https://sites.google.com/view/junha/ecotta
- **Journal**: None
- **Summary**: This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is crucial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders applying TTA in real-world deployments. Our approach consists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel architecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropagation. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents error accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy outperforms other state-of-the-art methods on various benchmarks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA.



### Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.01906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01906v1)
- **Published**: 2023-03-03 13:07:14+00:00
- **Updated**: 2023-03-03 13:07:14+00:00
- **Authors**: Liwei Yang, Xiang Gu, Jian Sun
- **Comment**: 13 pages, 9 figures, accepted at AAAI 2023 (Oral Presentation)
- **Journal**: None
- **Summary**: Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model's generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets.



### Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment
- **Arxiv ID**: http://arxiv.org/abs/2303.01913v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01913v1)
- **Published**: 2023-03-03 13:27:00+00:00
- **Updated**: 2023-03-03 13:27:00+00:00
- **Authors**: Jong-Ryul Lee, Yong-Hyuk Moon
- **Comment**: Accepted in AAAI-2023
- **Journal**: None
- **Summary**: As deep learning models become popular, there is a lot of need for deploying them to diverse device environments. Because it is costly to develop and optimize a neural network for every single environment, there is a line of research to search neural networks for multiple target environments efficiently. However, existing works for such a situation still suffer from requiring many GPUs and expensive costs. Motivated by this, we propose a novel neural network optimization framework named Bespoke for low-cost deployment. Our framework searches for a lightweight model by replacing parts of an original model with randomly selected alternatives, each of which comes from a pretrained neural network or the original model. In the practical sense, Bespoke has two significant merits. One is that it requires near zero cost for designing the search space of neural networks. The other merit is that it exploits the sub-networks of public pretrained neural networks, so the total cost is minimal compared to the existing works. We conduct experiments exploring Bespoke's the merits, and the results show that it finds efficient models for multiple targets with meager cost.



### PPCR: Learning Pyramid Pixel Context Recalibration Module for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.01917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01917v2)
- **Published**: 2023-03-03 13:36:55+00:00
- **Updated**: 2023-03-10 12:14:01+00:00
- **Authors**: Xiaoqing Zhang, Zunjie Xiao, Xiao Wu, Jiansheng Fang, Junyong Shen, Yan Hu, Risa Higashita, Jiang Liu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Spatial attention mechanism has been widely incorporated into deep convolutional neural networks (CNNs) via long-range dependency capturing, significantly lifting the performance in computer vision, but it may perform poorly in medical imaging. Unfortunately, existing efforts are often unaware that long-range dependency capturing has limitations in highlighting subtle lesion regions, neglecting to exploit the potential of multi-scale pixel context information to improve the representational capability of CNNs. In this paper, we propose a practical yet lightweight architectural unit, Pyramid Pixel Context Recalibration (PPCR) module, which exploits multi-scale pixel context information to recalibrate pixel position in a pixel-independent manner adaptively. PPCR first designs a cross-channel pyramid pooling to aggregate multi-scale pixel context information, then eliminates the inconsistency among them by the well-designed pixel normalization, and finally estimates per pixel attention weight via a pixel context integration. PPCR can be flexibly plugged into modern CNNs with negligible overhead. Extensive experiments on five medical image datasets and CIFAR benchmarks empirically demonstrate the superiority and generalization of PPCR over state-of-the-art attention methods. The in-depth analyses explain the inherent behavior of PPCR in the decision-making process, improving the interpretability of CNNs.



### Robust Detection Outcome: A Metric for Pathology Detection in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2303.01920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01920v1)
- **Published**: 2023-03-03 13:45:13+00:00
- **Updated**: 2023-03-03 13:45:13+00:00
- **Authors**: Felix Meissen, Philip Müller, Georgios Kaissis, Daniel Rueckert
- **Comment**: Accepted at MIDL 2023
- **Journal**: None
- **Summary**: Detection of pathologies is a fundamental task in medical imaging and the evaluation of algorithms that can perform this task automatically is crucial. However, current object detection metrics for natural images do not reflect the specific clinical requirements in pathology detection sufficiently. To tackle this problem, we propose Robust Detection Outcome (RoDeO); a novel metric for evaluating algorithms for pathology detection in medical images, especially in chest X-rays. RoDeO evaluates different errors directly and individually, and reflects clinical needs better than current metrics. Extensive evaluation on the ChestX-ray8 dataset shows the superiority of our metrics compared to existing ones. We released the code at https://github.com/FeliMe/RoDeO and published RoDeO as pip package (rodeometric).



### MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2303.01932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.01932v2)
- **Published**: 2023-03-03 14:02:50+00:00
- **Updated**: 2023-03-09 13:08:22+00:00
- **Authors**: Kejie Li, Jia-Wang Bian, Robert Castle, Philip H. S. Torr, Victor Adrian Prisacariu
- **Comment**: To be appeared at CVPR 2023
- **Journal**: None
- **Summary**: High-quality 3D ground-truth shapes are critical for 3D object reconstruction evaluation. However, it is difficult to create a replica of an object in reality, and even 3D reconstructions generated by 3D scanners have artefacts that cause biases in evaluation. To address this issue, we introduce a novel multi-view RGBD dataset captured using a mobile device, which includes highly precise 3D ground-truth annotations for 153 object models featuring a diverse set of 3D structures. We obtain precise 3D ground-truth shape without relying on high-end 3D scanners by utilising LEGO models with known geometry as the 3D structures for image capture. The distinct data modality offered by high-resolution RGB images and low-resolution depth maps captured on a mobile device, when combined with precise 3D geometry annotations, presents a unique opportunity for future research on high-fidelity 3D reconstruction. Furthermore, we evaluate a range of 3D reconstruction algorithms on the proposed dataset. Project page: http://code.active.vision/MobileBrick/



### Retinal Image Restoration using Transformer and Cycle-Consistent Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2303.01939v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01939v1)
- **Published**: 2023-03-03 14:10:47+00:00
- **Updated**: 2023-03-03 14:10:47+00:00
- **Authors**: Alnur Alimanov, Md Baharul Islam
- **Comment**: 4 pages, 3 figures, The International Symposium on Intelligent Signal
  Processing and Communication Systems (ISPACS2022)
- **Journal**: None
- **Summary**: Medical imaging plays a significant role in detecting and treating various diseases. However, these images often happen to be of too poor quality, leading to decreased efficiency, extra expenses, and even incorrect diagnoses. Therefore, we propose a retinal image enhancement method using a vision transformer and convolutional neural network. It builds a cycle-consistent generative adversarial network that relies on unpaired datasets. It consists of two generators that translate images from one domain to another (e.g., low- to high-quality and vice versa), playing an adversarial game with two discriminators. Generators produce indistinguishable images for discriminators that predict the original images from generated ones. Generators are a combination of vision transformer (ViT) encoder and convolutional neural network (CNN) decoder. Discriminators include traditional CNN encoders. The resulting improved images have been tested quantitatively using such evaluation metrics as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and qualitatively, i.e., vessel segmentation. The proposed method successfully reduces the adverse effects of blurring, noise, illumination disturbances, and color distortions while significantly preserving structural and color information. Experimental results show the superiority of the proposed method. Our testing PSNR is 31.138 dB for the first and 27.798 dB for the second dataset. Testing SSIM is 0.919 and 0.904, respectively.



### Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo
- **Arxiv ID**: http://arxiv.org/abs/2303.01943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01943v1)
- **Published**: 2023-03-03 14:15:48+00:00
- **Updated**: 2023-03-03 14:15:48+00:00
- **Authors**: Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, Andrés Bruhn
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: While recent methods for motion and stereo estimation recover an unprecedented amount of details, such highly detailed structures are neither adequately reflected in the data of existing benchmarks nor their evaluation methodology. Hence, we introduce Spring $-$ a large, high-resolution, high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie "Spring", it provides photo-realistic HD datasets with state-of-the-art visual effects and ground truth training data. Furthermore, we provide a website to upload, analyze and compare results. Using a novel evaluation methodology based on a super-resolved UHD ground truth, our Spring benchmark can assess the quality of fine structures and provides further detailed performance statistics on different image regions. Regarding the number of ground truth frames, Spring is 60$\times$ larger than the only scene flow benchmark, KITTI 2015, and 15$\times$ larger than the well-established MPI Sintel optical flow benchmark. Initial results for recent methods on our benchmark show that estimating fine details is indeed challenging, as their accuracy leaves significant room for improvement. The Spring benchmark and the corresponding datasets are available at http://spring-benchmark.org.



### PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2303.01959v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.01959v1)
- **Published**: 2023-03-03 14:32:48+00:00
- **Updated**: 2023-03-03 14:32:48+00:00
- **Authors**: Jinghuai Zhang, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Point cloud classification is an essential component in many security-critical applications such as autonomous driving and augmented reality. However, point cloud classifiers are vulnerable to adversarially perturbed point clouds. Existing certified defenses against adversarial point clouds suffer from a key limitation: their certified robustness guarantees are probabilistic, i.e., they produce an incorrect certified robustness guarantee with some probability. In this work, we propose a general framework, namely PointCert, that can transform an arbitrary point cloud classifier to be certifiably robust against adversarial point clouds with deterministic guarantees. PointCert certifiably predicts the same label for a point cloud when the number of arbitrarily added, deleted, and/or modified points is less than a threshold. Moreover, we propose multiple methods to optimize the certified robustness guarantees of PointCert in three application scenarios. We systematically evaluate PointCert on ModelNet and ScanObjectNN benchmark datasets. Our results show that PointCert substantially outperforms state-of-the-art certified defenses even though their robustness guarantees are probabilistic.



### Unified Perception: Efficient Depth-Aware Video Panoptic Segmentation with Minimal Annotation Costs
- **Arxiv ID**: http://arxiv.org/abs/2303.01991v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.01991v2)
- **Published**: 2023-03-03 15:00:12+00:00
- **Updated**: 2023-04-02 17:25:52+00:00
- **Authors**: Kurt Stolle, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: Depth-aware video panoptic segmentation is a promising approach to camera based scene understanding. However, the current state-of-the-art methods require costly video annotations and use a complex training pipeline compared to their image-based equivalents. In this paper, we present a new approach titled Unified Perception that achieves state-of-the-art performance without requiring video-based training. Our method employs a simple two-stage cascaded tracking algorithm that (re)uses object embeddings computed in an image-based network. Experimental results on the Cityscapes-DVPS dataset demonstrate that our method achieves an overall DVPQ of 57.1, surpassing state-of-the-art methods. Furthermore, we show that our tracking strategies are effective for long-term object association on KITTI-STEP, achieving an STQ of 59.1 which exceeded the performance of state-of-the-art methods that employ the same backbone network.   Code is available at: https://tue-mps.github.io/unipercept



### Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly
- **Arxiv ID**: http://arxiv.org/abs/2303.01999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2303.01999v1)
- **Published**: 2023-03-03 15:11:36+00:00
- **Updated**: 2023-03-03 15:11:36+00:00
- **Authors**: Xianghao Xu, Paul Guerrero, Matthew Fisher, Siddhartha Chaudhuri, Daniel Ritchie
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Representing a 3D shape with a set of primitives can aid perception of structure, improve robotic object manipulation, and enable editing, stylization, and compression of 3D shapes. Existing methods either use simple parametric primitives or learn a generative shape space of parts. Both have limitations: parametric primitives lead to coarse approximations, while learned parts offer too little control over the decomposition. We instead propose to decompose shapes using a library of 3D parts provided by the user, giving full control over the choice of parts. The library can contain parts with high-quality geometry that are suitable for a given category, resulting in meaningful decompositions with clean geometry. The type of decomposition can also be controlled through the choice of parts in the library. Our method works via a self-supervised approach that iteratively retrieves parts from the library and refines their placements. We show that this approach gives higher reconstruction accuracy and more desirable decompositions than existing approaches. Additionally, we show how the decomposition can be controlled through the part library by using different part libraries to reconstruct the same shapes.



### BSH-Det3D: Improving 3D Object Detection with BEV Shape Heatmap
- **Arxiv ID**: http://arxiv.org/abs/2303.02000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02000v1)
- **Published**: 2023-03-03 15:13:11+00:00
- **Updated**: 2023-03-03 15:13:11+00:00
- **Authors**: You Shen, Yunzhou Zhang, Yanmin Wu, Zhenyu Wang, Linghao Yang, Sonya Coleman, Dermot Kerr
- **Comment**: None
- **Journal**: None
- **Summary**: The progress of LiDAR-based 3D object detection has significantly enhanced developments in autonomous driving and robotics. However, due to the limitations of LiDAR sensors, object shapes suffer from deterioration in occluded and distant areas, which creates a fundamental challenge to 3D perception. Existing methods estimate specific 3D shapes and achieve remarkable performance. However, these methods rely on extensive computation and memory, causing imbalances between accuracy and real-time performance. To tackle this challenge, we propose a novel LiDAR-based 3D object detection model named BSH-Det3D, which applies an effective way to enhance spatial features by estimating complete shapes from a bird's eye view (BEV). Specifically, we design the Pillar-based Shape Completion (PSC) module to predict the probability of occupancy whether a pillar contains object shapes. The PSC module generates a BEV shape heatmap for each scene. After integrating with heatmaps, BSH-Det3D can provide additional information in shape deterioration areas and generate high-quality 3D proposals. We also design an attention-based densification fusion module (ADF) to adaptively associate the sparse features with heatmaps and raw points. The ADF module integrates the advantages of points and shapes knowledge with negligible overheads. Extensive experiments on the KITTI benchmark achieve state-of-the-art (SOTA) performance in terms of accuracy and speed, demonstrating the efficiency and flexibility of BSH-Det3D. The source code is available on https://github.com/mystorm16/BSH-Det3D.



### Zero-shot Object Counting
- **Arxiv ID**: http://arxiv.org/abs/2303.02001v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02001v2)
- **Published**: 2023-03-03 15:14:36+00:00
- **Updated**: 2023-04-24 15:51:01+00:00
- **Authors**: Jingyi Xu, Hieu Le, Vu Nguyen, Viresh Ranjan, Dimitris Samaras
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Class-agnostic object counting aims to count object instances of an arbitrary class at test time. It is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs which are often unavailable for novel categories, especially for autonomous systems. Thus, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. Such a counting system does not require human annotators in the loop and can operate automatically. Starting from a class name, we propose a method that can accurately identify the optimal patches which can then be used as counting exemplars. Specifically, we first construct a class prototype to select the patches that are likely to contain the objects of interest, namely class-relevant patches. Furthermore, we introduce a model that can quantitatively measure how suitable an arbitrary patch is as a counting exemplar. By applying this model to all the candidate patches, we can select the most suitable patches as exemplars for counting. Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method. Code is available at https://github.com/cvlab-stonybrook/zero-shot-counting



### MAEVI: Motion Aware Event-Based Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2303.02025v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02025v1)
- **Published**: 2023-03-03 15:44:00+00:00
- **Updated**: 2023-03-03 15:44:00+00:00
- **Authors**: Ahmet Akman, Onur Selim Kılıç, A. Aydın Alatan
- **Comment**: Submitted to International Conference on Image Processing (ICIP) 2023
- **Journal**: None
- **Summary**: Utilization of event-based cameras is expected to improve the visual quality of video frame interpolation solutions. We introduce a learning-based method to exploit moving region boundaries in a video sequence to increase the overall interpolation quality.Event cameras allow us to determine moving areas precisely; and hence, better video frame interpolation quality can be achieved by emphasizing these regions using an appropriate loss function. The results show a notable average \textit{PSNR} improvement of $1.3$ dB for the tested data sets, as well as subjectively more pleasing visual results with less ghosting and blurry artifacts.



### Single-photon Image Super-resolution via Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.02033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02033v1)
- **Published**: 2023-03-03 15:52:01+00:00
- **Updated**: 2023-03-03 15:52:01+00:00
- **Authors**: Yiwei Chen, Chen Jiang, Yu Pan
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: Single-Photon Image Super-Resolution (SPISR) aims to recover a high-resolution volumetric photon counting cube from a noisy low-resolution one by computational imaging algorithms. In real-world scenarios, pairs of training samples are often expensive or impossible to obtain. By extending Equivariant Imaging (EI) to volumetric single-photon data, we propose a self-supervised learning framework for the SPISR task. Particularly, using the Poisson unbiased Kullback-Leibler risk estimator and equivariance, our method is able to learn from noisy measurements without ground truths. Comprehensive experiments on simulated and real-world dataset demonstrate that the proposed method achieves comparable performance with supervised learning and outperforms interpolation-based methods.



### Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies
- **Arxiv ID**: http://arxiv.org/abs/2303.02034v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2303.02034v2)
- **Published**: 2023-03-03 15:52:06+00:00
- **Updated**: 2023-07-25 00:19:04+00:00
- **Authors**: Hannah Pinson, Joeri Lenaerts, Vincent Ginis
- **Comment**: None
- **Journal**: None
- **Summary**: We here present a stepping stone towards a deeper understanding of convolutional neural networks (CNNs) in the form of a theory of learning in linear CNNs. Through analyzing the gradient descent equations, we discover that the evolution of the network during training is determined by the interplay between the dataset structure and the convolutional network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, ordered, stage-like transitions, and that the speed of discovery changes depending on the relationship between the dataset and the convolutional network structure. Moreover, we find that this interplay lies at the heart of what we call the ``dominant frequency bias'', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. We furthermore provide experiments that show how our theory relates to deep, non-linear CNNs used in practice. Our findings shed new light on the inner working of CNNs, and can help explain their shortcut learning and their tendency to rely on texture instead of shape.



### Unsupervised Deep Digital Staining For Microscopic Cell Images Via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2303.02057v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02057v1)
- **Published**: 2023-03-03 16:26:38+00:00
- **Updated**: 2023-03-03 16:26:38+00:00
- **Authors**: Ziwang Xu, Lanqing Guo, Shuyan Zhang, Alex C. Kot, Bihan Wen
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: Staining is critical to cell imaging and medical diagnosis, which is expensive, time-consuming, labor-intensive, and causes irreversible changes to cell tissues. Recent advances in deep learning enabled digital staining via supervised model training. However, it is difficult to obtain large-scale stained/unstained cell image pairs in practice, which need to be perfectly aligned with the supervision. In this work, we propose a novel unsupervised deep learning framework for the digital staining of cell images using knowledge distillation and generative adversarial networks (GANs). A teacher model is first trained mainly for the colorization of bright-field images. After that,a student GAN for staining is obtained by knowledge distillation with hybrid non-reference losses. We show that the proposed unsupervised deep staining method can generate stained images with more accurate positions and shapes of the cell targets. Compared with other unsupervised deep generative models for staining, our method achieves much more promising results both qualitatively and quantitatively.



### 3D-Aware Object Localization using Gaussian Implicit Occupancy Function
- **Arxiv ID**: http://arxiv.org/abs/2303.02058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02058v2)
- **Published**: 2023-03-03 16:28:22+00:00
- **Updated**: 2023-08-02 14:21:18+00:00
- **Authors**: Vincent Gaudillière, Leo Pauly, Arunkumar Rathinam, Albert Garcia Sanchez, Mohamed Adel Musallam, Djamila Aouada
- **Comment**: 6 pages, 5 figures, IROS 2023
- **Journal**: None
- **Summary**: To automatically localize a target object in an image is crucial for many computer vision applications. To represent the 2D object, ellipse labels have recently been identified as a promising alternative to axis-aligned bounding boxes. This paper further considers 3D-aware ellipse labels, \textit{i.e.}, ellipses which are projections of a 3D ellipsoidal approximation of the object, for 2D target localization. Indeed, projected ellipses carry more geometric information about the object geometry and pose (3D awareness) than traditional 3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal model allows for approximating known to coarsely known targets. We then propose to have a new look at ellipse regression and replace the discontinuous geometric ellipse parameters with the parameters of an implicit Gaussian distribution encoding object occupancy in the image. The models are trained to regress the values of this bivariate Gaussian distribution over the image pixels using a statistical loss function. We introduce a novel non-trainable differentiable layer, E-DSNT, to extract the distribution parameters. Also, we describe how to readily generate consistent 3D-aware Gaussian occupancy parameters using only coarse dimensions of the target and relative pose labels. We extend three existing spacecraft pose estimation datasets with 3D-aware Gaussian occupancy labels to validate our hypothesis. Labels and source code are publicly accessible here: https://cvi2.uni.lu/3d-aware-obj-loc/.



### Evaluation of Confidence-based Ensembling in Deep Learning Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.03185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.03185v1)
- **Published**: 2023-03-03 16:29:22+00:00
- **Updated**: 2023-03-03 16:29:22+00:00
- **Authors**: Rafael Rosales, Peter Popov, Michael Paulitsch
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembling is a successful technique to improve the performance of machine learning (ML) models.   Conf-Ensemble is an adaptation to Boosting to create ensembles based on model confidence instead of model errors to better classify difficult edge-cases. The key idea is to create successive model experts for samples that were difficult (not necessarily incorrectly classified) by the preceding model. This technique has been shown to provide better results than boosting in binary-classification with a small feature space (~80 features).   In this paper, we evaluate the Conf-Ensemble approach in the much more complex task of image classification with the ImageNet dataset (224x224x3 features with 1000 classes). Image classification is an important benchmark for AI-based perception and thus it helps to assess if this method can be used in safety-critical applications using ML ensembles.   Our experiments indicate that in a complex multi-label classification task, the expected benefit of specialization on complex input samples cannot be achieved with a small sample set, i.e., a good classifier seems to rely on very complex feature analysis that cannot be well trained on just a limited subset of "difficult samples".   We propose an improvement to Conf-Ensemble to increase the number of samples fed to successive ensemble members, and a three-member Conf-Ensemble using this improvement was able to surpass a single model in accuracy, although the amount is not significant. Our findings shed light on the limits of the approach and the non-triviality of harnessing big data.



### Unproportional mosaicing
- **Arxiv ID**: http://arxiv.org/abs/2303.02081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02081v2)
- **Published**: 2023-03-03 16:55:44+00:00
- **Updated**: 2023-03-06 08:58:43+00:00
- **Authors**: Vojtech Molek, Petr Hurtik, Pavel Vlasanek, David Adamczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Data shift is a gap between data distribution used for training and data distribution encountered in the real-world. Data augmentations help narrow the gap by generating new data samples, increasing data variability, and data space coverage. We present a new data augmentation: Unproportional mosaicing (Unprop). Our augmentation randomly splits an image into various-sized blocks and swaps its content (pixels) while maintaining block sizes. Our method achieves a lower error rate when combined with other state-of-the-art augmentations.



### Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization
- **Arxiv ID**: http://arxiv.org/abs/2303.03108v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.03108v3)
- **Published**: 2023-03-03 16:58:53+00:00
- **Updated**: 2023-07-04 04:17:43+00:00
- **Authors**: Xingxuan Zhang, Renzhe Xu, Han Yu, Hao Zou, Peng Cui
- **Comment**: CVPR2023 highlight paper
- **Journal**: None
- **Summary**: Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results show that GAM improves the generalization of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Furthermore, we show that GAM can help SAM find flatter minima and achieve better generalization.



### Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement
- **Arxiv ID**: http://arxiv.org/abs/2303.02091v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02091v2)
- **Published**: 2023-03-03 17:14:44+00:00
- **Updated**: 2023-08-19 09:42:39+00:00
- **Authors**: Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, Gang Zeng
- **Comment**: ICCV 2023 camera-ready, Project Page: https://me.kiui.moe/nerf2mesh
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in image-based 3D reconstruction. However, their implicit volumetric representations differ significantly from the widely-adopted polygonal meshes and lack support from common 3D software and hardware, making their rendering and manipulation inefficient. To overcome this limitation, we present a novel framework that generates textured surface meshes from images. Our approach begins by efficiently initializing the geometry and view-dependency decomposed appearance with a NeRF. Subsequently, a coarse mesh is extracted, and an iterative surface refining algorithm is developed to adaptively adjust both vertex positions and face density based on re-projected rendering errors. We jointly refine the appearance with geometry and bake it into texture images for real-time rendering. Extensive experiments demonstrate that our method achieves superior mesh quality and competitive rendering quality.



### Bi-parametric prostate MR image synthesis using pathology and sequence-conditioned stable diffusion
- **Arxiv ID**: http://arxiv.org/abs/2303.02094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02094v1)
- **Published**: 2023-03-03 17:24:39+00:00
- **Updated**: 2023-03-03 17:24:39+00:00
- **Authors**: Shaheer U. Saeed, Tom Syer, Wen Yan, Qianye Yang, Mark Emberton, Shonit Punwani, Matthew J. Clarkson, Dean C. Barratt, Yipeng Hu
- **Comment**: Accepted at MIDL 2023 (The Medical Imaging with Deep Learning
  conference, 2023)
- **Journal**: None
- **Summary**: We propose an image synthesis mechanism for multi-sequence prostate MR images conditioned on text, to control lesion presence and sequence, as well as to generate paired bi-parametric images conditioned on images e.g. for generating diffusion-weighted MR from T2-weighted MR for paired data, which are two challenging tasks in pathological image synthesis. Our proposed mechanism utilises and builds upon the recent stable diffusion model by proposing image-based conditioning for paired data generation. We validate our method using 2D image slices from real suspected prostate cancer patients. The realism of the synthesised images is validated by means of a blind expert evaluation for identifying real versus fake images, where a radiologist with 4 years experience reading urological MR only achieves 59.4% accuracy across all tested sequences (where chance is 50%). For the first time, we evaluate the realism of the generated pathology by blind expert identification of the presence of suspected lesions, where we find that the clinician performs similarly for both real and synthesised images, with a 2.9 percentage point difference in lesion identification accuracy between real and synthesised images, demonstrating the potentials in radiological training purposes. Furthermore, we also show that a machine learning model, trained for lesion identification, shows better performance (76.2% vs 70.4%, statistically significant improvement) when trained with real data augmented by synthesised data as opposed to training with only real images, demonstrating usefulness for model training.



### Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective
- **Arxiv ID**: http://arxiv.org/abs/2303.02095v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02095v2)
- **Published**: 2023-03-03 17:24:39+00:00
- **Updated**: 2023-03-10 08:04:01+00:00
- **Authors**: Animesh Gupta, Irtiza Hasan, Dilip K. Prasad, Deepak K. Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Coreset selection is among the most effective ways to reduce the training time of CNNs, however, only limited is known on how the resultant models will behave under variations of the coreset size, and choice of datasets and models. Moreover, given the recent paradigm shift towards transformer-based models, it is still an open question how coreset selection would impact their performance. There are several similar intriguing questions that need to be answered for a wide acceptance of coreset selection methods, and this paper attempts to answer some of these. We present a systematic benchmarking setup and perform a rigorous comparison of different coreset selection methods on CNNs and transformers. Our investigation reveals that under certain circumstances, random selection of subsets is more robust and stable when compared with the SOTA selection methods. We demonstrate that the conventional concept of uniform subset sampling across the various classes of the data is not the appropriate choice. Rather samples should be adaptively chosen based on the complexity of the data distribution for each class. Transformers are generally pretrained on large datasets, and we show that for certain target datasets, it helps to keep their performance stable at even very small coreset sizes. We further show that when no pretraining is done or when the pretrained transformer models are used with non-natural images (e.g. medical data), CNNs tend to generalize better than transformers at even very small coreset sizes. Lastly, we demonstrate that in the absence of the right pretraining, CNNs are better at learning the semantic coherence between spatially distant objects within an image, and these tend to outperform transformers at almost all choices of the coreset size.



### Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT
- **Arxiv ID**: http://arxiv.org/abs/2303.02110v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2303.02110v5)
- **Published**: 2023-03-03 17:51:08+00:00
- **Updated**: 2023-04-02 00:18:22+00:00
- **Authors**: Zitong Yu, Md Ashequr Rahman, Richard Laforest, Thomas H. Schindler, Robert J. Gropler, Richard L. Wahl, Barry A. Siegel, Abhinav K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoising myocardial perfusion SPECT (MPS) images was conducted. The impact of DL-based denoising was evaluated using fidelity-based FoMs and AUC, which quantified performance on detecting perfusion defects in MPS images as obtained using a model observer with anthropomorphic channels. Based on fidelity-based FoMs, denoising using the considered DL-based method led to significantly superior performance. However, based on ROC analysis, denoising did not improve, and in fact, often degraded detection-task performance. The results motivate the need for objective task-based evaluation of DL-based denoising approaches. Further, this study shows how VCTs provide a mechanism to conduct such evaluations using VCTs. Finally, our theoretical treatment reveals insights into the reasons for the limited performance of the denoising approach.



### Skeletal Point Representations with Geometric Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.02123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02123v1)
- **Published**: 2023-03-03 18:08:12+00:00
- **Updated**: 2023-03-03 18:08:12+00:00
- **Authors**: Ninad Khargonkar, Beatriz Paniagua, Jared Vicory
- **Comment**: 5 pages, 5 figures, 2 tables. Accepted to IEEE International
  Symposium on Biomedical Imaging (ISBI) 2023
- **Journal**: None
- **Summary**: Skeletonization has been a popular shape analysis technique that models both the interior and exterior of an object. Existing template-based calculations of skeletal models from anatomical structures are a time-consuming manual process. Recently, learning-based methods have been used to extract skeletons from 3D shapes. In this work, we propose novel additional geometric terms for calculating skeletal structures of objects. The results are similar to traditional fitted s-reps but but are produced much more quickly. Evaluation on real clinical data shows that the learned model predicts accurate skeletal representations and shows the impact of proposed geometric losses along with using s-reps as weak supervision.



### TRUSformer: Improving Prostate Cancer Detection from Micro-Ultrasound Using Attention and Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2303.02128v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02128v1)
- **Published**: 2023-03-03 18:12:46+00:00
- **Updated**: 2023-03-03 18:12:46+00:00
- **Authors**: Mahdi Gilany, Paul Wilson, Andrea Perera-Ortega, Amoon Jamzad, Minh Nguyen Nhat To, Fahimeh Fooladgar, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
- **Comment**: None
- **Journal**: None
- **Summary**: A large body of previous machine learning methods for ultrasound-based prostate cancer detection classify small regions of interest (ROIs) of ultrasound signals that lie within a larger needle trace corresponding to a prostate tissue biopsy (called biopsy core). These ROI-scale models suffer from weak labeling as histopathology results available for biopsy cores only approximate the distribution of cancer in the ROIs. ROI-scale models do not take advantage of contextual information that are normally considered by pathologists, i.e. they do not consider information about surrounding tissue and larger-scale trends when identifying cancer. We aim to improve cancer detection by taking a multi-scale, i.e. ROI-scale and biopsy core-scale, approach. Methods: Our multi-scale approach combines (i) an "ROI-scale" model trained using self-supervised learning to extract features from small ROIs and (ii) a "core-scale" transformer model that processes a collection of extracted features from multiple ROIs in the needle trace region to predict the tissue type of the corresponding core. Attention maps, as a byproduct, allow us to localize cancer at the ROI scale. We analyze this method using a dataset of micro-ultrasound acquired from 578 patients who underwent prostate biopsy, and compare our model to baseline models and other large-scale studies in the literature. Results and Conclusions: Our model shows consistent and substantial performance improvements compared to ROI-scale-only models. It achieves 80.3% AUROC, a statistically significant improvement over ROI-scale classification. We also compare our method to large studies on prostate cancer detection, using other imaging modalities. Our code is publicly available at www.github.com/med-i-lab/TRUSFormer



### Depth-based 6DoF Object Pose Estimation using Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2303.02133v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02133v2)
- **Published**: 2023-03-03 18:25:07+00:00
- **Updated**: 2023-04-27 18:07:40+00:00
- **Authors**: Zhujun Li, Ioannis Stamos
- **Comment**: 8 pages. We have submitted the paper to The IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2023) on March 1st 2023
- **Journal**: None
- **Summary**: Accurately estimating the 6D pose of objects is crucial for many applications, such as robotic grasping, autonomous driving, and augmented reality. However, this task becomes more challenging in poor lighting conditions or when dealing with textureless objects. To address this issue, depth images are becoming an increasingly popular choice due to their invariance to a scene's appearance and the implicit incorporation of essential geometric characteristics. However, fully leveraging depth information to improve the performance of pose estimation remains a difficult and under-investigated problem. To tackle this challenge, we propose a novel framework called SwinDePose, that uses only geometric information from depth images to achieve accurate 6D pose estimation. SwinDePose first calculates the angles between each normal vector defined in a depth image and the three coordinate axes in the camera coordinate system. The resulting angles are then formed into an image, which is encoded using Swin Transformer. Additionally, we apply RandLA-Net to learn the representations from point clouds. The resulting image and point clouds embeddings are concatenated and fed into a semantic segmentation module and a 3D keypoints localization module. Finally, we estimate 6D poses using a least-square fitting approach based on the target object's predicted semantic mask and 3D keypoints. In experiments on the LineMod and Occlusion LineMod datasets, SwinDePose outperforms existing state-of-the-art methods for 6D object pose estimation using depth images. This demonstrates the effectiveness of our approach and highlights its potential for improving performance in real-world scenarios. Our code is at https://github.com/zhujunli1993/SwinDePose.



### Toward Data-Driven Glare Classification and Prediction for Marine Megafauna Survey
- **Arxiv ID**: http://arxiv.org/abs/2303.12730v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.12730v1)
- **Published**: 2023-03-03 18:46:19+00:00
- **Updated**: 2023-03-03 18:46:19+00:00
- **Authors**: Joshua Power, Derek Jacoby, Marc-Antoine Drouin, Guillaume Durand, Yvonne Coady, Julian Meng
- **Comment**: 15 pages, 4 figures, 5th ICPR Workshop on Computer Vison for
  Automated Analysis of Underwater Imagery (CVAUI 2022)
- **Journal**: None
- **Summary**: Critically endangered species in Canadian North Atlantic waters are systematically surveyed to estimate species populations which influence governing policies. Due to its impact on policy, population accuracy is important. This paper lays the foundation towards a data-driven glare modelling system, which will allow surveyors to preemptively minimize glare. Surveyors use a detection function to estimate megafauna populations which are not explicitly seen. A goal of the research is to maximize useful imagery collected, to that end we will use our glare model to predict glare and optimize for glare-free data collection. To build this model, we leverage a small labelled dataset to perform semi-supervised learning. The large dataset is labelled with a Cascading Random Forest Model using a na\"ive pseudo-labelling approach. A reflectance model is used, which pinpoints features of interest, to populate our datasets which allows for context-aware machine learning models. The pseudo-labelled dataset is used on two models: a Multilayer Perceptron and a Recurrent Neural Network. With this paper, we lay the foundation for data-driven mission planning; a glare modelling system which allows surveyors to preemptively minimize glare and reduces survey reliance on the detection function as an estimator of whale populations during periods of poor subsurface visibility.



### Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners
- **Arxiv ID**: http://arxiv.org/abs/2303.02151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2303.02151v1)
- **Published**: 2023-03-03 18:58:16+00:00
- **Updated**: 2023-03-03 18:58:16+00:00
- **Authors**: Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao
- **Comment**: Accepted by CVPR 2023. Code is available at
  https://github.com/ZrrSkywalker/CaFo. arXiv admin note: substantial text
  overlap with arXiv:2209.12255
- **Journal**: None
- **Summary**: Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre-training paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.



### Unleashing Text-to-Image Diffusion Models for Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2303.02153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02153v1)
- **Published**: 2023-03-03 18:59:47+00:00
- **Updated**: 2023-03-03 18:59:47+00:00
- **Authors**: Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, Jiwen Lu
- **Comment**: project page: https://vpd.ivg-research.xyz
- **Journal**: None
- **Summary**: Diffusion models (DMs) have become the new trend of generative models and have demonstrated a powerful ability of conditional synthesis. Among those, text-to-image diffusion models pre-trained on large-scale image-text pairs are highly controllable by customizable prompts. Unlike the unconditional generative models that focus on low-level attributes and details, text-to-image diffusion models contain more high-level knowledge thanks to the vision-language pre-training. In this paper, we propose VPD (Visual Perception with a pre-trained Diffusion model), a new framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. Instead of using the pre-trained denoising autoencoder in a diffusion-based pipeline, we simply use it as a backbone and aim to study how to take full advantage of the learned knowledge. Specifically, we prompt the denoising decoder with proper textual inputs and refine the text features with an adapter, leading to a better alignment to the pre-trained stage and making the visual contents interact with the text prompts. We also propose to utilize the cross-attention maps between the visual features and the text features to provide explicit guidance. Compared with other pre-training methods, we show that vision-language pre-trained diffusion models can be faster adapted to downstream visual perception tasks using the proposed VPD. Extensive experiments on semantic segmentation, referring image segmentation and depth estimation demonstrates the effectiveness of our method. Notably, VPD attains 0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring image segmentation, establishing new records on these two benchmarks. Code is available at https://github.com/wl-zhao/VPD



### MixVPR: Feature Mixing for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2303.02190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02190v1)
- **Published**: 2023-03-03 19:24:03+00:00
- **Updated**: 2023-03-03 19:24:03+00:00
- **Authors**: Amar Ali-bey, Brahim Chaib-draa, Philippe Giguère
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE all while being orders of magnitude faster. Our code and trained models are available at https://github.com/amaralibey/MixVPR.



### R-TOSS: A Framework for Real-Time Object Detection using Semi-Structured Pruning
- **Arxiv ID**: http://arxiv.org/abs/2303.02191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02191v1)
- **Published**: 2023-03-03 19:26:08+00:00
- **Updated**: 2023-03-03 19:26:08+00:00
- **Authors**: Abhishek Balasubramaniam, Febin P Sunny, Sudeep Pasricha
- **Comment**: None
- **Journal**: None
- **Summary**: Object detectors used in autonomous vehicles can have high memory and computational overheads. In this paper, we introduce a novel semi-structured pruning framework called R-TOSS that overcomes the shortcomings of state-of-the-art model pruning techniques. Experimental results on the JetsonTX2 show that R-TOSS has a compression rate of 4.4x on the YOLOv5 object detector with a 2.15x speedup in inference time and 57.01% decrease in energy usage. R-TOSS also enables 2.89x compression on RetinaNet with a 1.86x speedup in inference time and 56.31% decrease in energy usage. We also demonstrate significant improvements compared to various state-of-the-art pruning techniques.



### X$^3$KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.02203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02203v1)
- **Published**: 2023-03-03 20:29:49+00:00
- **Updated**: 2023-03-03 20:29:49+00:00
- **Authors**: Marvin Klingner, Shubhankar Borse, Varun Ravi Kumar, Behnaz Rezaei, Venkatraman Narayanan, Senthil Yogamani, Fatih Porikli
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Recent advances in 3D object detection (3DOD) have obtained remarkably strong results for LiDAR-based models. In contrast, surround-view 3DOD models based on multiple camera images underperform due to the necessary view transformation of features from perspective view (PV) to a 3D world representation which is ambiguous due to missing depth information. This paper introduces X$^3$KD, a comprehensive knowledge distillation framework across different modalities, tasks, and stages for multi-camera 3DOD. Specifically, we propose cross-task distillation from an instance segmentation teacher (X-IS) in the PV feature extraction stage providing supervision without ambiguous error backpropagation through the view transformation. After the transformation, we apply cross-modal feature distillation (X-FD) and adversarial training (X-AT) to improve the 3D world representation of multi-camera features through the information contained in a LiDAR-based 3DOD teacher. Finally, we also employ this teacher for cross-modal output distillation (X-OD), providing dense supervision at the prediction stage. We perform extensive ablations of knowledge distillation at different stages of multi-camera 3DOD. Our final X$^3$KD model outperforms previous state-of-the-art approaches on the nuScenes and Waymo datasets and generalizes to RADAR-based 3DOD. Qualitative results video at https://youtu.be/1do9DPFmr38.



### Lightweight, Uncertainty-Aware Conformalized Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2303.02207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02207v1)
- **Published**: 2023-03-03 20:37:55+00:00
- **Updated**: 2023-03-03 20:37:55+00:00
- **Authors**: Alex C. Stutts, Danilo Erricolo, Theja Tulabandhula, Amit Ranjan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven visual odometry (VO) is a critical subroutine for autonomous edge robotics, and recent progress in the field has produced highly accurate point predictions in complex environments. However, emerging autonomous edge robotics devices like insect-scale drones and surgical robots lack a computationally efficient framework to estimate VO's predictive uncertainties. Meanwhile, as edge robotics continue to proliferate into mission-critical application spaces, awareness of model's the predictive uncertainties has become crucial for risk-aware decision-making. This paper addresses this challenge by presenting a novel, lightweight, and statistically robust framework that leverages conformal inference (CI) to extract VO's uncertainty bands. Our approach represents the uncertainties using flexible, adaptable, and adjustable prediction intervals that, on average, guarantee the inclusion of the ground truth across all degrees of freedom (DOF) of pose estimation. We discuss the architectures of generative deep neural networks for estimating multivariate uncertainty bands along with point (mean) prediction. We also present techniques to improve the uncertainty estimation accuracy, such as leveraging Monte Carlo dropout (MC-dropout) for data augmentation. Finally, we propose a novel training loss function that combines interval scoring and calibration loss with traditional training metrics--mean-squared error and KL-divergence--to improve uncertainty-aware learning. Our simulation results demonstrate that the presented framework consistently captures true uncertainty in pose estimations across different datasets, estimation models, and applied noise types, indicating its wide applicability.



### Building Floorspace in China: A Dataset and Learning Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2303.02230v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, econ.GN, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2303.02230v2)
- **Published**: 2023-03-03 21:45:36+00:00
- **Updated**: 2023-06-06 21:08:52+00:00
- **Authors**: Peter Egger, Susie Xi Rao, Sebastiano Papini
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a first milestone in measuring the floorspace of buildings (that is, building footprint and height) for 40 major Chinese cities. The intent is to maximize city coverage and, eventually provide longitudinal data. Doing so requires building on imagery that is of a medium-fine-grained granularity, as larger cross sections of cities and longer time series for them are only available in such format. We use a multi-task object segmenter approach to learn the building footprint and height in the same framework in parallel: (1) we determine the surface area is covered by any buildings (the square footage of occupied land); (2) we determine floorspace from multi-image representations of buildings from various angles to determine the height of buildings. We use Sentinel-1 and -2 satellite images as our main data source. The benefits of these data are their large cross-sectional and longitudinal scope plus their unrestricted accessibility. We provide a detailed description of our data, algorithms, and evaluations. In addition, we analyze the quality of reference data and their role for measuring the building floorspace with minimal error. We conduct extensive quantitative and qualitative analyses with Shenzhen as a case study using our multi-task learner. Finally, we conduct correlation studies between our results (on both pixel and aggregated urban area levels) and nightlight data to gauge the merits of our approach in studying urban development. Our data and codebase are publicly accessible under https://gitlab.ethz.ch/raox/urban-satellite-public-v2.



### Domain adaptation using optimal transport for invariant learning using histopathology datasets
- **Arxiv ID**: http://arxiv.org/abs/2303.02241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02241v1)
- **Published**: 2023-03-03 22:19:19+00:00
- **Updated**: 2023-03-03 22:19:19+00:00
- **Authors**: Kianoush Falahkheirkhah, Alex Lu, David Alvarez-Melis, Grace Huynh
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology is critical for the diagnosis of many diseases, including cancer. These protocols typically require pathologists to manually evaluate slides under a microscope, which is time-consuming and subjective, leading to interest in machine learning to automate analysis. However, computational techniques are limited by batch effects, where technical factors like differences in preparation protocol or scanners can alter the appearance of slides, causing models trained on one institution to fail when generalizing to others. Here, we propose a domain adaptation method that improves the generalization of histopathological models to data from unseen institutions, without the need for labels or retraining in these new settings. Our approach introduces an optimal transport (OT) loss, that extends adversarial methods that penalize models if images from different institutions can be distinguished in their representation space. Unlike previous methods, which operate on single samples, our loss accounts for distributional differences between batches of images. We show that on the Camelyon17 dataset, while both methods can adapt to global differences in color distribution, only our OT loss can reliably classify a cancer phenotype unseen during training. Together, our results suggest that OT improves generalization on rare but critical phenotypes that may only make up a small fraction of the total tiles and variation in a slide.



### Exploring Self-Supervised Representation Learning For Low-Resource Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2303.02245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02245v2)
- **Published**: 2023-03-03 22:26:17+00:00
- **Updated**: 2023-06-29 03:22:52+00:00
- **Authors**: Soumitri Chattopadhyay, Soham Ganguly, Sreejit Chaudhury, Sayan Nag, Samiran Chattopadhyay
- **Comment**: Accepted at IEEE ICIP 2023
- **Journal**: None
- **Summary**: The success of self-supervised learning (SSL) has mostly been attributed to the availability of unlabeled yet large-scale datasets. However, in a specialized domain such as medical imaging which is a lot different from natural images, the assumption of data availability is unrealistic and impractical, as the data itself is scanty and found in small databases, collected for specific prognosis tasks. To this end, we seek to investigate the applicability of self-supervised learning algorithms on small-scale medical imaging datasets. In particular, we evaluate $4$ state-of-the-art SSL methods on three publicly accessible \emph{small} medical imaging datasets. Our investigation reveals that in-domain low-resource SSL pre-training can yield competitive performance to transfer learning from large-scale datasets (such as ImageNet). Furthermore, we extensively analyse our empirical findings to provide valuable insights that can motivate for further research towards circumventing the need for pre-training on a large image corpus. To the best of our knowledge, this is the first attempt to holistically explore self-supervision on low-resource medical datasets.



### A Visual SLAM with Moving Object Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2303.02257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.02257v1)
- **Published**: 2023-03-03 23:12:43+00:00
- **Updated**: 2023-03-03 23:12:43+00:00
- **Authors**: Qi Zhang, Siyuan Gou, Wenbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Simultaneous Localization and Mapping (SLAM) has received significant attention in recent years due to its ability to estimate camera trajectory and create an environment map using visual data alone, making a substantial contribution to autonomous driving applications, in particular, a real-world scenario with moving crowds and vehicles. In this work, we propose a visual SLAM system that incorporates moving object trajectory tracking and prediction. We take into account the motion clues of the pedestrians to track and predict their movement, as long as mapping the environment. Such an integrated system solves the localization of the camera and other moving objects in the scene, and further creates a sparse map to support the potential navigation of the vehicle. In the experiment, we demonstrate the effectiveness and robustness of our approach through a comprehensive evaluation on both our simulation and real-world KITTI datasets.



### Learning to reason over visual objects
- **Arxiv ID**: http://arxiv.org/abs/2303.02260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2303.02260v1)
- **Published**: 2023-03-03 23:19:42+00:00
- **Updated**: 2023-03-03 23:19:42+00:00
- **Authors**: Shanka Subhra Mondal, Taylor Webb, Jonathan D. Cohen
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices). These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases.



