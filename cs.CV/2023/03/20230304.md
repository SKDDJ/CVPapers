# Arxiv Papers in cs.CV on 2023-03-04
### Learning Label Encodings for Deep Regression
- **Arxiv ID**: http://arxiv.org/abs/2303.02273v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02273v1)
- **Published**: 2023-03-04 00:11:34+00:00
- **Updated**: 2023-03-04 00:11:34+00:00
- **Authors**: Deval Shah, Tor M. Aamodt
- **Comment**: Published at ICLR 2023 (Notable top-25%)
- **Journal**: International Conference on Learning Representations 2023
  (https://openreview.net/pdf?id=k60XE_b0Ix6)
- **Summary**: Deep regression networks are widely used to tackle the problem of predicting a continuous value for a given input. Task-specialized approaches for training regression networks have shown significant improvement over generic approaches, such as direct regression. More recently, a generic approach based on regression by binary classification using binary-encoded labels has shown significant improvement over direct regression. The space of label encodings for regression is large. Lacking heretofore have been automated approaches to find a good label encoding for a given application. This paper introduces Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encoding. RLEL provides a generic approach for tackling regression. Underlying RLEL is our observation that the search space of label encodings can be constrained and efficiently explored by using a continuous search space of real-valued label encodings combined with a regularization function designed to encourage encodings with certain properties. These properties balance the probability of classification error in individual bits against error correction capability. Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings. Applying RLEL results in 10.9% and 12.4% improvement in Mean Absolute Error (MAE) over direct regression and multiclass classification, respectively. Our evaluation demonstrates that RLEL can be combined with off-the-shelf feature extractors and is suitable across different architectures, datasets, and tasks. Code is available at https://github.com/ubc-aamodt-group/RLEL_regression.



### OASIS: Automated Assessment of Urban Pedestrian Paths at Scale
- **Arxiv ID**: http://arxiv.org/abs/2303.02287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02287v2)
- **Published**: 2023-03-04 01:32:59+00:00
- **Updated**: 2023-05-04 20:03:14+00:00
- **Authors**: Yuxiang Zhang, Suresh Devalapalli, Sachin Mehta, Anat Caspi
- **Comment**: None
- **Journal**: None
- **Summary**: The inspection of the Public Right of Way (PROW) for accessibility barriers is necessary for monitoring and maintaining the built environment for communities' walkability, rollability, safety, active transportation, and sustainability. However, an inspection of the PROW, by surveyors or crowds, is laborious, inconsistent, costly, and unscalable. The core of smart city developments involves the application of information technologies toward municipal assets assessment and management. Sidewalks, in comparison to automobile roads, have not been regularly integrated into information systems to optimize or inform civic services. We develop an Open Automated Sidewalks Inspection System (OASIS), a free and open-source automated mapping system, to extract sidewalk network data using mobile physical devices. OASIS leverages advances in neural networks, image sensing, location-based methods, and compact hardware to perform sidewalk segmentation and mapping along with the identification of barriers to generate a GIS pedestrian transportation layer that is available for routing as well as analytic and operational reports. We describe a prototype system trained and tested with imagery collected in real-world settings, alongside human surveyors who are part of the local transit pathway review team. Pilots show promising precision and recall for path mapping (0.94, 0.98 respectively). Moreover, surveyor teams' functional efficiency increased in the field. By design, OASIS takes adoption aspects into consideration to ensure the system could be easily integrated with governmental pathway review teams' workflows, and that the outcome data would be interoperable with public data commons.



### Co-Speech Gesture Synthesis using Discrete Gesture Token Learning
- **Arxiv ID**: http://arxiv.org/abs/2303.12822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2303.12822v1)
- **Published**: 2023-03-04 01:42:09+00:00
- **Updated**: 2023-03-04 01:42:09+00:00
- **Authors**: Shuhong Lu, Youngwoo Yoon, Andrew Feng
- **Comment**: 8 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes conditioned on input speech context. Since the inference is formulated as token sampling, multiple gesture sequences could be generated given the same speech input using top-k sampling. The quantitative results and the user study showed the proposed method outperforms the previous methods and is able to generate realistic and diverse gesture motions.



### Visualizing Transferred Knowledge: An Interpretive Model of Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2303.02302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02302v1)
- **Published**: 2023-03-04 03:02:12+00:00
- **Updated**: 2023-03-04 03:02:12+00:00
- **Authors**: Wenxiao Xiao, Zhengming Ding, Hongfu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Many research efforts have been committed to unsupervised domain adaptation (DA) problems that transfer knowledge learned from a labeled source domain to an unlabeled target domain. Various DA methods have achieved remarkable results recently in terms of predicting ability, which implies the effectiveness of the aforementioned knowledge transferring. However, state-of-the-art methods rarely probe deeper into the transferred mechanism, leaving the true essence of such knowledge obscure. Recognizing its importance in the adaptation process, we propose an interpretive model of unsupervised domain adaptation, as the first attempt to visually unveil the mystery of transferred knowledge. Adapting the existing concept of the prototype from visual image interpretation to the DA task, our model similarly extracts shared information from the domain-invariant representations as prototype vectors. Furthermore, we extend the current prototype method with our novel prediction calibration and knowledge fidelity preservation modules, to orientate the learned prototypes to the actual transferred knowledge. By visualizing these prototypes, our method not only provides an intuitive explanation for the base model's predictions but also unveils transfer knowledge by matching the image patches with the same semantics across both source and target domains. Comprehensive experiments and in-depth explorations demonstrate the efficacy of our method in understanding the transferred mechanism and its potential in downstream tasks including model diagnosis.



### IKD+: Reliable Low Complexity Deep Models For Retinopathy Classification
- **Arxiv ID**: http://arxiv.org/abs/2303.02310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02310v1)
- **Published**: 2023-03-04 03:59:06+00:00
- **Updated**: 2023-03-04 03:59:06+00:00
- **Authors**: Shreyas Bhat Brahmavar, Rohit Rajesh, Tirtharaj Dash, Lovekesh Vig, Tanmay Tulsidas Verlekar, Md Mahmudul Hasan, Tariq Khan, Erik Meijering, Ashwin Srinivasan
- **Comment**: Submitted to IEEE International Conference on Image Processing (ICIP
  2023)
- **Journal**: None
- **Summary**: Deep neural network (DNN) models for retinopathy have estimated predictive accuracies in the mid-to-high 90%. However, the following aspects remain unaddressed: State-of-the-art models are complex and require substantial computational infrastructure to train and deploy; The reliability of predictions can vary widely. In this paper, we focus on these aspects and propose a form of iterative knowledge distillation(IKD), called IKD+ that incorporates a tradeoff between size, accuracy and reliability. We investigate the functioning of IKD+ using two widely used techniques for estimating model calibration (Platt-scaling and temperature-scaling), using the best-performing model available, which is an ensemble of EfficientNets with approximately 100M parameters. We demonstrate that IKD+ equipped with temperature-scaling results in models that show up to approximately 500-fold decreases in the number of parameters than the original ensemble without a significant loss in accuracy. In addition, calibration scores (reliability) for the IKD+ models are as good as or better than the base mode



### Virtual Sparse Convolution for Multimodal 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2303.02314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02314v1)
- **Published**: 2023-03-04 04:15:36+00:00
- **Updated**: 2023-03-04 04:15:36+00:00
- **Authors**: Hai Wu, Chenglu Wen, Shaoshuai Shi, Xin Li, Cheng Wang
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Recently, virtual/pseudo-point-based 3D object detection that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, introducing a huge amount of redundant computation during detection. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection precision. This paper proposes a fast yet effective backbone, termed VirConvNet, based on a new operator VirConv (Virtual Sparse Convolution), for virtual-point-based 3D object detection. VirConv consists of two key designs: (1) StVD (Stochastic Voxel Discard) and (2) NRConv (Noise-Resistant Submanifold Convolution). StVD alleviates the computation problem by discarding large amounts of nearby redundant voxels. NRConv tackles the noise problem by encoding voxel features in both 2D image and 3D LiDAR space. By integrating VirConv, we first develop an efficient pipeline VirConv-L based on an early fusion design. Then, we build a high-precision pipeline VirConv-T based on a transformed refinement scheme. Finally, we develop a semi-supervised pipeline VirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test leaderboard, our VirConv-L achieves 85% AP with a fast running speed of 56ms. Our VirConv-T and VirConv-S attains a high-precision of 86.3% and 87.2% AP, and currently rank 2nd and 1st, respectively. The code is available at https://github.com/hailanyi/VirConv.



### Real-Time Hand Gesture Identification in Thermal Images
- **Arxiv ID**: http://arxiv.org/abs/2303.02321v1
- **DOI**: 10.1007/978-3-031-06430-2_41
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.02321v1)
- **Published**: 2023-03-04 05:02:35+00:00
- **Updated**: 2023-03-04 05:02:35+00:00
- **Authors**: James Ballow, Soumyabrata Dey
- **Comment**: 21st International Conference on Image Analysis and Processing
- **Journal**: None
- **Summary**: Hand gesture-based human-computer interaction is an important problem that is well explored using color camera data. In this work we proposed a hand gesture detection system using thermal images. Our system is capable of handling multiple hand regions in a frame and process it fast for real-time applications. Our system performs a series of steps including background subtraction-based hand mask generation, k-means based hand region identification, hand segmentation to remove the forearm region, and a Convolutional Neural Network (CNN) based gesture classification. Our work introduces two novel algorithms, bubble growth and bubble search, for faster hand segmentation. We collected a new thermal image data set with 10 gestures and reported an end-to-end hand gesture recognition accuracy of 97%.



### APE: An Open and Shared Annotated Dataset for Learning Urban Pedestrian Path Networks
- **Arxiv ID**: http://arxiv.org/abs/2303.02323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02323v1)
- **Published**: 2023-03-04 05:08:36+00:00
- **Updated**: 2023-03-04 05:08:36+00:00
- **Authors**: Yuxiang Zhang, Nicholas Bolten, Sachin Mehta, Anat Caspi
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring the full transportation network, including sidewalks and cycleways, is crucial for many automated systems, including autonomous driving, multi-modal navigation, trip planning, mobility simulations, and freight management. Many transportation decisions can be informed based on an accurate pedestrian network, its interactions, and connectivity with the road networks of other modes of travel. A connected pedestrian path network is vital to transportation activities, as sidewalks and crossings connect pedestrians to other modes of transportation. However, information about these paths' location and connectivity is often missing or inaccurate in city planning systems and wayfinding applications, causing severe information gaps and errors for planners and pedestrians. This work begins to address this problem at scale by introducing a novel dataset of aerial satellite imagery, street map imagery, and rasterized annotations of sidewalks, crossings, and corner bulbs in urban cities. The dataset spans $2,700 km^2$ land area, covering select regions from $6$ different cities. It can be used for various learning tasks related to segmenting and understanding pedestrian environments. We also present an end-to-end process for inferring a connected pedestrian path network map using street network information and our proposed dataset. The process features the use of a multi-input segmentation network trained on our dataset to predict important classes in the pedestrian environment and then generate a connected pedestrian path network. Our results demonstrate that the dataset is sufficiently large to train common segmentation models yielding accurate, robust pedestrian path networks.



### Decompose, Adjust, Compose: Effective Normalization by Playing with Frequency for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2303.02328v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02328v3)
- **Published**: 2023-03-04 05:23:11+00:00
- **Updated**: 2023-03-15 12:39:19+00:00
- **Authors**: Sangrok Lee, Jongseong Bae, Ha Young Kim
- **Comment**: 10 pages,6 figures, Conference on Computer Vision and Pattern
  Recognition 2023
- **Journal**: None
- **Summary**: Domain generalization (DG) is a principal task to evaluate the robustness of computer vision models. Many previous studies have used normalization for DG. In normalization, statistics and normalized features are regarded as style and content, respectively. However, it has a content variation problem when removing style because the boundary between content and style is unclear. This study addresses this problem from the frequency domain perspective, where amplitude and phase are considered as style and content, respectively. First, we verify the quantitative phase variation of normalization through the mathematical derivation of the Fourier transform formula. Then, based on this, we propose a novel normalization method, PCNorm, which eliminates style only as the preserving content through spectral decomposition. Furthermore, we propose advanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of variations in content and style, respectively. Thus, they can learn domain-agnostic representations for DG. With the normalization methods, we propose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain gap. The proposed models outperform other recent DG methods. The DAC-SC achieves an average state-of-the-art performance of 65.6% on five datasets: PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.



### A Fast Training-Free Compression Framework for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2303.02331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02331v1)
- **Published**: 2023-03-04 05:34:25+00:00
- **Updated**: 2023-03-04 05:34:25+00:00
- **Authors**: Jung Hwan Heo, Arash Fayyazi, Mahdi Nazemi, Massoud Pedram
- **Comment**: Preprint. 13 pages, 9 Figures, 8 Tables
- **Journal**: None
- **Summary**: Token pruning has emerged as an effective solution to speed up the inference of large Transformer models. However, prior work on accelerating Vision Transformer (ViT) models requires training from scratch or fine-tuning with additional parameters, which prevents a simple plug-and-play. To avoid high training costs during the deployment stage, we present a fast training-free compression framework enabled by (i) a dense feature extractor in the initial layers; (ii) a sharpness-minimized model which is more compressible; and (iii) a local-global token merger that can exploit spatial relationships at various contexts. We applied our framework to various ViT and DeiT models and achieved up to 2x reduction in FLOPS and 1.8x speedup in inference throughput with <1% accuracy loss, while saving two orders of magnitude shorter training times than existing approaches. Code will be available at https://github.com/johnheo/fast-compress-vit



### Improving Audio-Visual Video Parsing with Pseudo Visual Labels
- **Arxiv ID**: http://arxiv.org/abs/2303.02344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.02344v1)
- **Published**: 2023-03-04 07:21:37+00:00
- **Updated**: 2023-03-04 07:21:37+00:00
- **Authors**: Jinxing Zhou, Dan Guo, Yiran Zhong, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Audio-Visual Video Parsing is a task to predict the events that occur in video segments for each modality. It often performs in a weakly supervised manner, where only video event labels are provided, i.e., the modalities and the timestamps of the labels are unknown. Due to the lack of densely annotated labels, recent work attempts to leverage pseudo labels to enrich the supervision. A commonly used strategy is to generate pseudo labels by categorizing the known event labels for each modality. However, the labels are still limited to the video level, and the temporal boundaries of event timestamps remain unlabeled. In this paper, we propose a new pseudo label generation strategy that can explicitly assign labels to each video segment by utilizing prior knowledge learned from the open world. Specifically, we exploit the CLIP model to estimate the events in each video segment based on visual modality to generate segment-level pseudo labels. A new loss function is proposed to regularize these labels by taking into account their category-richness and segmentrichness. A label denoising strategy is adopted to improve the pseudo labels by flipping them whenever high forward binary cross entropy loss occurs. We perform extensive experiments on the LLP dataset and demonstrate that our method can generate high-quality segment-level pseudo labels with the help of our newly proposed loss and the label denoising strategy. Our method achieves state-of-the-art audio-visual video parsing performance.



### MetaGrad: Adaptive Gradient Quantization with Hypernetworks
- **Arxiv ID**: http://arxiv.org/abs/2303.02347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02347v1)
- **Published**: 2023-03-04 07:26:34+00:00
- **Updated**: 2023-03-04 07:26:34+00:00
- **Authors**: Kaixin Xu, Alina Hui Xiu Lee, Ziyuan Zhao, Zhe Wang, Min Wu, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: A popular track of network compression approach is Quantization aware Training (QAT), which accelerates the forward pass during the neural network training and inference. However, not much prior efforts have been made to quantize and accelerate the backward pass during training, even though that contributes around half of the training time. This can be partly attributed to the fact that errors of low-precision gradients during backward cannot be amortized by the training objective as in the QAT setting. In this work, we propose to solve this problem by incorporating the gradients into the computation graph of the next training iteration via a hypernetwork. Various experiments on CIFAR-10 dataset with different CNN network architectures demonstrate that our hypernetwork-based approach can effectively reduce the negative effect of gradient quantization noise and successfully quantizes the gradients to INT4 with only 0.64 accuracy drop for VGG-16 on CIFAR-10.



### Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling
- **Arxiv ID**: http://arxiv.org/abs/2303.02353v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02353v2)
- **Published**: 2023-03-04 08:33:46+00:00
- **Updated**: 2023-03-11 19:53:30+00:00
- **Authors**: Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang
- **Comment**: Accepted by AAAI 2023. Code is available at
  https://github.com/yang-jin-hai/SAIN
- **Journal**: None
- **Summary**: High-resolution (HR) images are usually downscaled to low-resolution (LR) ones for better display and afterward upscaled back to the original size to recover details. Recent work in image rescaling formulates downscaling and upscaling as a unified task and learns a bijective mapping between HR and LR via invertible networks. However, in real-world applications (e.g., social media), most images are compressed for transmission. Lossy compression will lead to irreversible information loss on LR images, hence damaging the inverse upscaling procedure and degrading the reconstruction accuracy. In this paper, we propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware image rescaling. To tackle the distribution shift, we first develop an end-to-end asymmetric framework with two separate bijective mappings for high-quality and compressed LR images, respectively. Then, based on empirical analysis of this framework, we model the distribution of the lost information (including downscaling and compression) using isotropic Gaussian mixtures and propose the Enhanced Invertible Block to derive high-quality/compressed LR images in one forward pass. Besides, we design a set of losses to regularize the learned LR images and enhance the invertibility. Extensive experiments demonstrate the consistent improvements of SAIN across various image rescaling datasets in terms of both quantitative and qualitative evaluation under standard image compression formats (i.e., JPEG and WebP).



### Self-Supervised Learning for Place Representation Generalization across Appearance Changes
- **Arxiv ID**: http://arxiv.org/abs/2303.02370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02370v2)
- **Published**: 2023-03-04 10:14:47+00:00
- **Updated**: 2023-03-09 12:33:10+00:00
- **Authors**: Mohamed Adel Musallam, Vincent Gaudillière, Djamila Aouada
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Visual place recognition is a key to unlocking spatial navigation for animals, humans and robots. While state-of-the-art approaches are trained in a supervised manner and therefore hardly capture the information needed for generalizing to unusual conditions, we argue that self-supervised learning may help abstracting the place representation so that it can be foreseen, irrespective of the conditions. More precisely, in this paper, we investigate learning features that are robust to appearance modifications while sensitive to geometric transformations in a self-supervised manner. This dual-purpose training is made possible by combining the two self-supervision main paradigms, \textit{i.e.} contrastive and predictive learning. Our results on standard benchmarks reveal that jointly learning such appearance-robust and geometry-sensitive image descriptors leads to competitive visual place recognition results across adverse seasonal and illumination conditions, without requiring any human-annotated labels.



### NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2303.02375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02375v2)
- **Published**: 2023-03-04 10:38:41+00:00
- **Updated**: 2023-03-26 10:38:58+00:00
- **Authors**: Bowen Cai, Jinchi Huang, Rongfei Jia, Chengfei Lv, Huan Fu
- **Comment**: Accepted to CVPR 2023, project page:
  https://3d-front-future.github.io/neuda
- **Journal**: None
- **Summary**: This paper studies implicit surface reconstruction leveraging differentiable ray casting. Previous works such as IDR and NeuS overlook the spatial context in 3D space when predicting and rendering the surface, thereby may fail to capture sharp local topologies such as small holes and structures. To mitigate the limitation, we propose a flexible neural implicit representation leveraging hierarchical voxel grids, namely Neural Deformable Anchor (NeuDA), for high-fidelity surface reconstruction. NeuDA maintains the hierarchical anchor grids where each vertex stores a 3D position (or anchor) instead of the direct embedding (or feature). We optimize the anchor grids such that different local geometry structures can be adaptively encoded. Besides, we dig into the frequency encoding strategies and introduce a simple hierarchical positional encoding method for the hierarchical anchor structure to flexibly exploit the properties of high-frequency and low-frequency geometry and appearance. Experiments on both the DTU and BlendedMVS datasets demonstrate that NeuDA can produce promising mesh surfaces.



### Hierarchical Training of Deep Neural Networks Using Early Exiting
- **Arxiv ID**: http://arxiv.org/abs/2303.02384v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, eess.IV, I.2.6; I.2.10; I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2303.02384v3)
- **Published**: 2023-03-04 11:30:16+00:00
- **Updated**: 2023-04-23 11:59:54+00:00
- **Authors**: Yamin Sepehri, Pedram Pad, Ahmet Caner Yüzügüler, Pascal Frossard, L. Andrea Dunbar
- **Comment**: 11 pages, 9 figures, 1 Table
- **Journal**: None
- **Summary**: Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw input data with the cloud and does not require communication during the backward pass. Several simulations and on-device experiments for different neural network architectures demonstrate the effectiveness of this method. It is shown that the proposed method reduces the training runtime by 29% and 61% in CIFAR-10 classification experiment for VGG-16 and ResNet-18 when the communication with the cloud is done at a low bit rate channel. This gain in the runtime is achieved whilst the accuracy drop is negligible. This method is advantageous for online learning of high-accuracy deep neural networks on low-resource devices such as mobile phones or robots as a part of an edge-cloud system, making them more flexible in facing new tasks and classes of data.



### Graph-based Representation for Image based on Granular-ball
- **Arxiv ID**: http://arxiv.org/abs/2303.02388v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02388v1)
- **Published**: 2023-03-04 11:39:46+00:00
- **Updated**: 2023-03-04 11:39:46+00:00
- **Authors**: Xia Shuyin, Dai Dawei, Yang Long, Zhany Li, Lan Danf, Zhu hao, Wang Guoy
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Current image processing methods usually operate on the finest-granularity unit; that is, the pixel, which leads to challenges in terms of efficiency, robustness, and understandability in deep learning models. We present an improved granular-ball computing method to represent the image as a graph, in which each node expresses a structural block in the image and each edge represents the association between two nodes. Specifically:(1) We design a gradient-based strategy for the adaptive reorganization of all pixels in the image into numerous rectangular regions, each of which can be regarded as one node. (2) Each node has a connection edge with the nodes with which it shares regions. (3) We design a low-dimensional vector as the attribute of each node. All nodes and their corresponding edges form a graphical representation of a digital image. In the experiments, our proposed graph representation is applied to benchmark datasets for image classification tasks, and the efficiency and good understandability demonstrate that our proposed method offers significant potential in artificial intelligence theory and application.



### Few-Shot Defect Image Generation via Defect-Aware Feature Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2303.02389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02389v1)
- **Published**: 2023-03-04 11:43:08+00:00
- **Updated**: 2023-03-04 11:43:08+00:00
- **Authors**: Yuxuan Duan, Yan Hong, Li Niu, Liqing Zhang
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: The performances of defect inspection have been severely hindered by insufficient defect images in industries, which can be alleviated by generating more samples as data augmentation. We propose the first defect image generation method in the challenging few-shot cases. Given just a handful of defect images and relatively more defect-free ones, our goal is to augment the dataset with new defect images. Our method consists of two training stages. First, we train a data-efficient StyleGAN2 on defect-free images as the backbone. Second, we attach defect-aware residual blocks to the backbone, which learn to produce reasonable defect masks and accordingly manipulate the features within the masked regions by training the added modules on limited defect images. Extensive experiments on MVTec AD dataset not only validate the effectiveness of our method in generating realistic and diverse defect images, but also manifest the benefits it brings to downstream defect inspection tasks. Codes are available at https://github.com/Ldhlwh/DFMGAN.



### Audio-Visual Quality Assessment for User Generated Content: Database and Method
- **Arxiv ID**: http://arxiv.org/abs/2303.02392v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.02392v1)
- **Published**: 2023-03-04 11:49:42+00:00
- **Updated**: 2023-03-04 11:49:42+00:00
- **Authors**: Yuqin Cao, Xiongkuo Min, Wei Sun, Xiaoping Zhang, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: With the explosive increase of User Generated Content (UGC), UGC video quality assessment (VQA) becomes more and more important for improving users' Quality of Experience (QoE). However, most existing UGC VQA studies only focus on the visual distortions of videos, ignoring that the user's QoE also depends on the accompanying audio signals. In this paper, we conduct the first study to address the problem of UGC audio and video quality assessment (AVQA). Specifically, we construct the first UGC AVQA database named the SJTU-UAV database, which includes 520 in-the-wild UGC audio and video (A/V) sequences, and conduct a user study to obtain the mean opinion scores of the A/V sequences. The content of the SJTU-UAV database is then analyzed from both the audio and video aspects to show the database characteristics. We also design a family of AVQA models, which fuse the popular VQA methods and audio features via support vector regressor (SVR). We validate the effectiveness of the proposed models on the three databases. The experimental results show that with the help of audio signals, the VQA models can evaluate the perceptual quality more accurately. The database will be released to facilitate further research.



### Fine-Grained ImageNet Classification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2303.02400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2303.02400v1)
- **Published**: 2023-03-04 12:25:07+00:00
- **Updated**: 2023-03-04 12:25:07+00:00
- **Authors**: Maria Lymperaiou, Konstantinos Thomas, Giorgos Stamou
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification has been one of the most popular tasks in Deep Learning, seeing an abundance of impressive implementations each year. However, there is a lot of criticism tied to promoting complex architectures that continuously push performance metrics higher and higher. Robustness tests can uncover several vulnerabilities and biases which go unnoticed during the typical model evaluation stage. So far, model robustness under distribution shifts has mainly been examined within carefully curated datasets. Nevertheless, such approaches do not test the real response of classifiers in the wild, e.g. when uncurated web-crawled image data of corresponding classes are provided. In our work, we perform fine-grained classification on closely related categories, which are identified with the help of hierarchical knowledge. Extensive experimentation on a variety of convolutional and transformer-based architectures reveals model robustness in this novel setting. Finally, hierarchical knowledge is again employed to evaluate and explain misclassifications, providing an information-rich evaluation scheme adaptable to any classifier.



### Open-Vocabulary Affordance Detection in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2303.02401v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02401v5)
- **Published**: 2023-03-04 12:26:47+00:00
- **Updated**: 2023-07-23 08:31:15+00:00
- **Authors**: Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen, Thieu Vo, Ngan Le, Anh Nguyen
- **Comment**: Accepted at The 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)
- **Journal**: None
- **Summary**: Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real-world robotic applications with a fast inference speed (~100ms). Our project is available at https://openad2023.github.io.



### Fine-Grained Classification with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2303.02404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02404v1)
- **Published**: 2023-03-04 12:32:45+00:00
- **Updated**: 2023-03-04 12:32:45+00:00
- **Authors**: Qi Wei, Lei Feng, Haoliang Sun, Ren Wang, Chenhui Guo, Yilong Yin
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Learning with noisy labels (LNL) aims to ensure model generalization given a label-corrupted training set. In this work, we investigate a rarely studied scenario of LNL on fine-grained datasets (LNL-FG), which is more practical and challenging as large inter-class ambiguities among fine-grained classes cause more noisy labels. We empirically show that existing methods that work well for LNL fail to achieve satisfying performance for LNL-FG, arising the practical need of effective solutions for LNL-FG. To this end, we propose a novel framework called stochastic noise-tolerated supervised contrastive learning (SNSCL) that confronts label noise by encouraging distinguishable representation. Specifically, we design a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for noisy label correction and selectively updating momentum queue lists. By this mechanism, we mitigate the effects of noisy anchors and avoid inserting noisy labels into the momentum-updated queue. Besides, to avoid manually-defined augmentation strategies in contrastive learning, we propose an efficient stochastic module that samples feature embeddings from a generated distribution, which can also enhance the representation ability of deep models. SNSCL is general and compatible with prevailing robust LNL strategies to improve their performance for LNL-FG. Extensive experiments demonstrate the effectiveness of SNSCL.



### The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2303.02411v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02411v1)
- **Published**: 2023-03-04 13:12:18+00:00
- **Updated**: 2023-03-04 13:12:18+00:00
- **Authors**: Maria Lymperaiou, Giorgos Stamou
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in visiolinguistic (VL) learning have allowed the development of multiple models and techniques that offer several impressive implementations, able to currently resolve a variety of tasks that require the collaboration of vision and language. Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models. External knowledge sources such as knowledge graphs (KGs) and Large Language Models (LLMs) are able to cover such generalization gaps by filling in missing knowledge, resulting in the emergence of hybrid architectures. In the current survey, we analyze tasks that have benefited from such hybrid approaches. Moreover, we categorize existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.



### Improved Trajectory Reconstruction for Markerless Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2303.02413v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02413v2)
- **Published**: 2023-03-04 13:16:02+00:00
- **Updated**: 2023-03-08 12:17:04+00:00
- **Authors**: R. James Cotton, Anthony Cimorelli, Kunal Shah, Shawana Anarwala, Scott Uhlrich, Tasos Karakostas
- **Comment**: None
- **Journal**: None
- **Summary**: Markerless pose estimation allows reconstructing human movement from multiple synchronized and calibrated views, and has the potential to make movement analysis easy and quick, including gait analysis. This could enable much more frequent and quantitative characterization of gait impairments, allowing better monitoring of outcomes and responses to interventions. However, the impact of different keypoint detectors and reconstruction algorithms on markerless pose estimation accuracy has not been thoroughly evaluated. We tested these algorithmic choices on data acquired from a multicamera system from a heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We found that using a top-down keypoint detector and reconstructing trajectories with an implicit function enabled accurate, smooth and anatomically plausible trajectories, with a noise in the step width estimates compared to a GaitRite walkway of only 8mm.



### PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2303.02416v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02416v2)
- **Published**: 2023-03-04 13:38:51+00:00
- **Updated**: 2023-03-24 05:37:41+00:00
- **Authors**: Yuan Liu, Songyang Zhang, Jiacheng Chen, Kai Chen, Dahua Lin
- **Comment**: Update code link and add additional results
- **Journal**: None
- **Summary**: Masked Image Modeling (MIM) has achieved promising progress with the advent of Masked Autoencoders (MAE) and BEiT. However, subsequent works have complicated the framework with new auxiliary tasks or extra pre-trained models, inevitably increasing computational overhead. This paper undertakes a fundamental analysis of MIM from the perspective of pixel reconstruction, which examines the input image patches and reconstruction target, and highlights two critical but previously overlooked bottlenecks. Based on this analysis, we propose a remarkably simple and effective method, {\ourmethod}, that entails two strategies: 1) filtering the high-frequency components from the reconstruction target to de-emphasize the network's focus on texture-rich details and 2) adopting a conservative data transform strategy to alleviate the problem of missing foreground in MIM training. {\ourmethod} can be easily integrated into most existing pixel-based MIM approaches (\ie, using raw images as reconstruction target) with negligible additional computation. Without bells and whistles, our method consistently improves three MIM approaches, MAE, ConvMAE, and LSMAE, across various downstream tasks. We believe this effective plug-and-play method will serve as a strong baseline for self-supervised learning and provide insights for future improvements of the MIM framework. Code and models are available at \url{https://github.com/open-mmlab/mmselfsup/tree/dev-1.x/configs/selfsup/pixmim}.



### Improving the quality of dental crown using a Transformer-based method
- **Arxiv ID**: http://arxiv.org/abs/2303.02426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02426v1)
- **Published**: 2023-03-04 14:14:00+00:00
- **Updated**: 2023-03-04 14:14:00+00:00
- **Authors**: Golriz Hosseinimanesh, Farnoosh Ghadiri, Ammar Alsheghri, Ying Zhang, Julia Keren, Farida Cheriet, Francois Guibault
- **Comment**: None
- **Journal**: None
- **Summary**: Designing a synthetic crown is a time-consuming, inconsistent, and labor-intensive process. In this work, we present a fully automatic method that not only learns human design dental crowns, but also improves the consistency, functionality, and esthetic of the crowns. Following success in point cloud completion using the transformer-based network, we tackle the problem of the crown generation as a point-cloud completion around a prepared tooth. To this end, we use a geometry-aware transformer to generate dental crowns. Our main contribution is to add a margin line information to the network, as the accuracy of generating a precise margin line directly,determines whether the designed crown and prepared tooth are closely matched to allowappropriateadhesion.Using our ground truth crown, we can extract the margin line as a spline and sample the spline into 1000 points. We feed the obtained margin line along with two neighbor teeth of the prepared tooth and three closest teeth in the opposing jaw. We also add the margin line points to our ground truth crown to increase the resolution at the margin line. Our experimental results show an improvement in the quality of the designed crown when considering the actual context composed of the prepared tooth along with the margin line compared with a crown generated in an empty space as was done by other studies in the literature.



### ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing
- **Arxiv ID**: http://arxiv.org/abs/2303.02437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02437v2)
- **Published**: 2023-03-04 14:59:25+00:00
- **Updated**: 2023-03-09 13:03:37+00:00
- **Authors**: Zequn Zeng, Hao Zhang, Zhengjue Wang, Ruiying Lu, Dongsheng Wang, Bo Chen
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Zero-shot capability has been considered as a new revolution of deep learning, letting machines work on tasks without curated training data. As a good start and the only existing outcome of zero-shot image captioning (IC), ZeroCap abandons supervised training and sequentially searches every word in the caption using the knowledge of large-scale pretrained models. Though effective, its autoregressive generation and gradient-directed searching mechanism limit the diversity of captions and inference speed, respectively. Moreover, ZeroCap does not consider the controllability issue of zero-shot IC. To move forward, we propose a framework for Controllable Zero-shot IC, named ConZIC. The core of ConZIC is a novel sampling-based non-autoregressive language model named GibbsBERT, which can generate and continuously polish every word. Extensive quantitative and qualitative results demonstrate the superior performance of our proposed ConZIC for both zero-shot IC and controllable zero-shot IC. Especially, ConZIC achieves about 5x faster generation speed than ZeroCap, and about 1.5x higher diversity scores, with accurate generation given different control signals.



### Comparative Studies of Unsupervised and Supervised Learning Methods based on Multimedia Applications
- **Arxiv ID**: http://arxiv.org/abs/2303.02446v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2303.02446v1)
- **Published**: 2023-03-04 16:11:49+00:00
- **Updated**: 2023-03-04 16:11:49+00:00
- **Authors**: Amitesh Kumar Singam, Benny Lövström, Wlodek J. Kulesza
- **Comment**: None
- **Journal**: International Journal of Science and Research (IJSR), Vol 12,
  issue 3(2023), pp. 214-218
- **Summary**: In the mobile communication field, some of the video applications boosted the interest of robust methods for video quality assessment. Out of all existing methods, We Preferred, No Reference Video Quality Assessment is the one which is most needed in situations where the handiness of reference video is partially available. Our research interest lies in formulating and melding effective features into one model based on human visualizing characteristics. Our work explores comparative study between Supervised and unsupervised learning methods. Therefore, we implemented support vector regression algorithm as NR-based Video Quality Metric(VQM) for quality estimation with simplified input features. We concluded that our proposed model exhibited sparseness even after dimension reduction for objective scores of SSIM quality metric.



### Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2303.02449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02449v1)
- **Published**: 2023-03-04 16:16:47+00:00
- **Updated**: 2023-03-04 16:16:47+00:00
- **Authors**: Jiren Mai, Fei Zhang, Junjie Ye, Marcus Kalander, Xian Zhang, WanKou Yang, Tongliang Liu, Bo Han
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve breakthrough improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods.



### Understanding weight-magnitude hyperparameters in training binary networks
- **Arxiv ID**: http://arxiv.org/abs/2303.02452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2303.02452v1)
- **Published**: 2023-03-04 16:42:04+00:00
- **Updated**: 2023-03-04 16:42:04+00:00
- **Authors**: Joris Quist, Yunqiang Li, Jan van Gemert
- **Comment**: Conference: ICLR 2023
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) are compact and efficient by using binary weights instead of real-valued weights. Current BNNs use latent real-valued weights during training, where several training hyper-parameters are inherited from real-valued networks. The interpretation of several of these hyperparameters is based on the magnitude of the real-valued weights. For BNNs, however, the magnitude of binary weights is not meaningful, and thus it is unclear what these hyperparameters actually do. One example is weight-decay, which aims to keep the magnitude of real-valued weights small. Other examples are latent weight initialization, the learning rate, and learning rate decay, which influence the magnitude of the real-valued weights. The magnitude is interpretable for real-valued weights, but loses its meaning for binary weights. In this paper we offer a new interpretation of these magnitude-based hyperparameters based on higher-order gradient filtering during network optimization. Our analysis makes it possible to understand how magnitude-based hyperparameters influence the training of binary networks which allows for new optimization filters specifically designed for binary neural networks that are independent of their real-valued interpretation. Moreover, our improved understanding reduces the number of hyperparameters, which in turn eases the hyperparameter tuning effort which may lead to better hyperparameter values for improved accuracy. Code is available at https://github.com/jorisquist/Understanding-WM-HP-in-BNNs



### Exploiting Implicit Rigidity Constraints via Weight-Sharing Aggregation for Scene Flow Estimation from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2303.02454v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02454v2)
- **Published**: 2023-03-04 16:55:57+00:00
- **Updated**: 2023-04-01 09:01:55+00:00
- **Authors**: Yun Wang, Cheng Chi, Xin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Scene flow estimation, which predicts the 3D motion of scene points from point clouds, is a core task in autonomous driving and many other 3D vision applications. Existing methods either suffer from structure distortion due to ignorance of rigid motion consistency or require explicit pose estimation and 3D object segmentation. Errors of estimated poses and segmented objects would yield inaccurate rigidity constraints and in turn mislead scene flow estimation. In this paper, we propose a novel weight-sharing aggregation (WSA) method for feature and scene flow up-sampling. WSA does not rely on estimated poses and segmented objects, and can implicitly enforce rigidity constraints to avoid structure distortion in scene flow estimation. To further exploit geometric information and preserve local structure, we design a deformation degree module aim to keep the local region invariance. We modify the PointPWC-Net and integrate the proposed WSA and deformation degree module into the enhanced PointPWC-Net to derive an end-to-end scene flow estimation network, called WSAFlowNet. Extensive experimental results on the FlyingThings3D and KITTI datasets demonstrate that our WSAFlowNet achieves the state-of-the-art performance and outperforms previous methods by a large margin. We will release the source code at https://github.com/wangyunlhr/WSAFlowNet.git.



### DistilPose: Tokenized Pose Regression with Heatmap Distillation
- **Arxiv ID**: http://arxiv.org/abs/2303.02455v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02455v3)
- **Published**: 2023-03-04 16:56:29+00:00
- **Updated**: 2023-03-16 11:17:20+00:00
- **Authors**: Suhang Ye, Yingyi Zhang, Jie Hu, Liujuan Cao, Shengchuan Zhang, Lei Shen, Jun Wang, Shouhong Ding, Rongrong Ji
- **Comment**: accepted by CVPR2023
- **Journal**: None
- **Summary**: In the field of human pose estimation, regression-based methods have been dominated in terms of speed, while heatmap-based methods are far ahead in terms of performance. How to take advantage of both schemes remains a challenging problem. In this paper, we propose a novel human pose estimation framework termed DistilPose, which bridges the gaps between heatmap-based and regression-based methods. Specifically, DistilPose maximizes the transfer of knowledge from the teacher model (heatmap-based) to the student model (regression-based) through Token-distilling Encoder (TDE) and Simulated Heatmaps. TDE aligns the feature spaces of heatmap-based and regression-based models by introducing tokenization, while Simulated Heatmaps transfer explicit guidance (distribution and confidence) from teacher heatmaps into student models. Extensive experiments show that the proposed DistilPose can significantly improve the performance of the regression-based models while maintaining efficiency. Specifically, on the MSCOCO validation dataset, DistilPose-S obtains 71.6% mAP with 5.36M parameter, 2.38 GFLOPs and 40.2 FPS, which saves 12.95x, 7.16x computational cost and is 4.9x faster than its teacher model with only 0.9 points performance drop. Furthermore, DistilPose-L obtains 74.4% mAP on MSCOCO validation dataset, achieving a new state-of-the-art among predominant regression-based models.



### Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/2303.02460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02460v1)
- **Published**: 2023-03-04 17:35:24+00:00
- **Updated**: 2023-03-04 17:35:24+00:00
- **Authors**: Jing Wu, David Pichler, Daniel Marley, David Wilson, Naira Hovakimyan, Jennifer Hobbs
- **Comment**: Dataset:
  https://github.com/jingwu6/Extended-Agriculture-Vision-Dataset Video:
  https://youtu.be/2xaKxUpY4iQ
- **Journal**: Transactions on Machine Learning Research(TMLR2023)
- **Summary**: A key challenge for much of the machine learning work on remote sensing and earth observation data is the difficulty in acquiring large amounts of accurately labeled data. This is particularly true for semantic segmentation tasks, which are much less common in the remote sensing domain because of the incredible difficulty in collecting precise, accurate, pixel-level annotations at scale. Recent efforts have addressed these challenges both through the creation of supervised datasets as well as the application of self-supervised methods. We continue these efforts on both fronts. First, we generate and release an improved version of the Agriculture-Vision dataset (Chiu et al., 2020b) to include raw, full-field imagery for greater experimental flexibility. Second, we extend this dataset with the release of 3600 large, high-resolution (10cm/pixel), full-field, red-green-blue and near-infrared images for pre-training. Third, we incorporate the Pixel-to-Propagation Module Xie et al. (2021b) originally built on the SimCLR framework into the framework of MoCo-V2 Chen et al.(2020b). Finally, we demonstrate the usefulness of this data by benchmarking different contrastive learning approaches on both downstream classification and semantic segmentation tasks. We explore both CNN and Swin Transformer Liu et al. (2021a) architectures within different frameworks based on MoCo-V2. Together, these approaches enable us to better detect key agricultural patterns of interest across a field from aerial imagery so that farmers may be alerted to problematic areas in a timely fashion to inform their management decisions. Furthermore, the release of these datasets will support numerous avenues of research for computer vision in remote sensing for agriculture.



### ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure
- **Arxiv ID**: http://arxiv.org/abs/2303.02472v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02472v1)
- **Published**: 2023-03-04 18:06:36+00:00
- **Updated**: 2023-03-04 18:06:36+00:00
- **Authors**: Hee Suk Yoon, Joshua Tian Jin Tee, Eunseop Yoon, Sunjae Yoon, Gwangsu Kim, Yingzhen Li, Chang D. Yoo
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model calibration in various batch size settings without the need for internal hyperparameter tuning, (2) ESD yields the best-calibrated results compared with previous approaches, and (3) ESD drastically improves the computational costs required for calibration during training due to the absence of internal hyperparameter. The code is publicly accessible at https://github.com/hee-suk-yoon/ESD.



### FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks
- **Arxiv ID**: http://arxiv.org/abs/2303.02483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02483v1)
- **Published**: 2023-03-04 19:07:48+00:00
- **Updated**: 2023-03-04 19:07:48+00:00
- **Authors**: Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe Song, Tao Xiang
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: In the fashion domain, there exists a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning. They differ drastically in each individual input/output format and dataset size. It has been common to design a task-specific model and fine-tune it independently from a pre-trained V+L model (e.g., CLIP). This results in parameter inefficiency and inability to exploit inter-task relatedness. To address such issues, we propose a novel FAshion-focused Multi-task Efficient learning method for Vision-and-Language tasks (FAME-ViL) in this work. Compared with existing approaches, FAME-ViL applies a single model for multiple heterogeneous fashion tasks, therefore being much more parameter-efficient. It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer. Extensive experiments on four fashion tasks show that our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models. Code is available at https://github.com/BrandonHanx/FAME-ViL.



### Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries
- **Arxiv ID**: http://arxiv.org/abs/2303.02484v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02484v2)
- **Published**: 2023-03-04 19:11:54+00:00
- **Updated**: 2023-06-19 18:59:43+00:00
- **Authors**: Charlotte Loh, Seungwook Han, Shivchander Sudalairaj, Rumen Dangovski, Kai Xu, Florian Wenzel, Marin Soljacic, Akash Srivastava
- **Comment**: Camera Ready Revision. ICML 2023
- **Journal**: None
- **Summary**: Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses that is often required in large, diverse datasets like ImageNet. As a result of their inherent diversity, MSE improves classification performance, uncertainty quantification, and generalization across a series of transfer tasks.



### CapDet: Unifying Dense Captioning and Open-World Detection Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2303.02489v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02489v3)
- **Published**: 2023-03-04 19:53:00+00:00
- **Updated**: 2023-03-15 13:45:48+00:00
- **Authors**: Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, Xiaodan Liang
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Benefiting from large-scale vision-language pre-training on image-text pairs, open-world detection methods have shown superior generalization ability under the zero-shot or few-shot detection settings. However, a pre-defined category space is still required during the inference stage of existing methods and only the objects belonging to that space will be predicted. To introduce a "real" open-world detector, in this paper, we propose a novel method named CapDet to either predict under a given category list or directly generate the category of predicted bounding boxes. Specifically, we unify the open-world detection and dense caption tasks into a single yet effective framework by introducing an additional dense captioning head to generate the region-grounded captions. Besides, adding the captioning task will in turn benefit the generalization of detection performance since the captioning dataset covers more concepts. Experiment results show that by unifying the dense caption task, our CapDet has obtained significant performance improvements (e.g., +2.1% mAP on LVIS rare classes) over the baseline method on LVIS (1203 classes). Besides, our CapDet also achieves state-of-the-art performance on dense captioning tasks, e.g., 15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.



### Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later
- **Arxiv ID**: http://arxiv.org/abs/2303.02490v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.NE, F.2.2; I.3.3; I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2303.02490v1)
- **Published**: 2023-03-04 20:08:57+00:00
- **Updated**: 2023-03-04 20:08:57+00:00
- **Authors**: Binxu Wang, John J. Vastola
- **Comment**: 36 pages, 27 figures
- **Journal**: None
- **Summary**: How do diffusion generative models convert pure noise into meaningful images? We argue that generation involves first committing to an outline, and then to finer and finer details. The corresponding reverse diffusion process can be modeled by dynamics on a (time-dependent) high-dimensional landscape full of Gaussian-like modes, which makes the following predictions: (i) individual trajectories tend to be very low-dimensional; (ii) scene elements that vary more within training data tend to emerge earlier; and (iii) early perturbations substantially change image content more often than late perturbations. We show that the behavior of a variety of trained unconditional and conditional diffusion models like Stable Diffusion is consistent with these predictions. Finally, we use our theory to search for the latent image manifold of diffusion models, and propose a new way to generate interpretable image variations. Our viewpoint suggests generation by GANs and diffusion models have unexpected similarities.



### Prismer: A Vision-Language Model with An Ensemble of Experts
- **Arxiv ID**: http://arxiv.org/abs/2303.02506v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02506v2)
- **Published**: 2023-03-04 21:22:47+00:00
- **Updated**: 2023-03-12 02:30:16+00:00
- **Authors**: Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, Anima Anandkumar
- **Comment**: Tech Report. Project Page: https://shikun.io/projects/prismer Code:
  https://github.com/NVlabs/prismer v2: fixed incorrect training cost estimate
  and zero-shot NoCaps performance of SimVLM
- **Journal**: None
- **Summary**: Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.



### Visual Saliency-Guided Channel Pruning for Deep Visual Detectors in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2303.02512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2303.02512v1)
- **Published**: 2023-03-04 22:08:22+00:00
- **Updated**: 2023-03-04 22:08:22+00:00
- **Authors**: Jung Im Choi, Qing Tian
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural network (DNN) pruning has become a de facto component for deploying on resource-constrained devices since it can reduce memory requirements and computation costs during inference. In particular, channel pruning gained more popularity due to its structured nature and direct savings on general hardware. However, most existing pruning approaches utilize importance measures that are not directly related to the task utility. Moreover, few in the literature focus on visual detection models. To fill these gaps, we propose a novel gradient-based saliency measure for visual detection and use it to guide our channel pruning. Experiments on the KITTI and COCO traffic datasets demonstrate our pruning method's efficacy and superiority over state-of-the-art competing approaches. It can even achieve better performance with fewer parameters than the original model. Our pruning also demonstrates great potential in handling small-scale objects.



### Detection of the Arterial Input Function Using DSC-MRI Data
- **Arxiv ID**: http://arxiv.org/abs/2303.02516v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2303.02516v1)
- **Published**: 2023-03-04 22:30:38+00:00
- **Updated**: 2023-03-04 22:30:38+00:00
- **Authors**: Svitlana Alkhimova, Kateryna Sazonova
- **Comment**: 7 pages
- **Journal**: Proceedings of the V International Scientific and Practical
  Conference "Modern and global methods of the development of scientific
  thought", October 25-28, 2022, Florence, Italy. - Florence : International
  Science Group, 2022. - P. 541-547
- **Summary**: Accurate detection of arterial input function is a crucial step in obtaining perfusion hemodynamic parameters using dynamic susceptibility contrast-enhanced magnetic resonance imaging. It is required as input for perfusion quantification and has a great impact on the result of the deconvolution operation. To improve the reproducibility and reliability of arterial input function detection, several semi- or fully automatic methods have been proposed. This study provides an overview of the current state of the field of arterial input function detection. Methods most commonly used for semi- and fully automatic arterial input function detection are reviewed, and their advantages and disadvantages are listed.



